- analysis: '>'
  authors:
  - Nanjundan P.
  - Jaisingh W.
  - George J.P.
  citation_count: '0'
  description: 'Healthcare systems are increasingly seeking to match patients'' pace
    of life and be personalized, as they are demanding more advanced products and
    services. The only solution for collecting and analyzing health data in realtime
    is an edge computing (EC) environment, coupled with 5G speeds and modern computing
    techniques. The technology in healthcare is currently being used to develop smart
    systems that can expedite the diagnosis of disease and provide precise and timely
    treatment. The automated hospital monitoring system and medical diagnosis system
    enable doctors to monitor and diagnose patients from a variety of locations, including
    hospitals, workplaces, and homes and provide transportation options. As a result,
    overall doctor visits are reduced as well as patient care is improved. More than
    162 billion healthcare IoT devices are expected to be used worldwide by 2021 thanks
    to the internet of things (IoT) sensors and applications for general healthcare.
    With edge intelligence (EI), wearable devices with sensors, like smartwatches
    or smartphones, and gateway devices, such as microcontrollers, can form edge nodes:
    smart devices with sensors, as well as gateway devices with sensors, can act as
    edge nodes. Smart sensor devices are typically installed at a greater distance
    from personal computers (PCs) and servers, which can be utilized in fog computing
    (FC). In healthcare, EC and FC are used to deliver reliable, low-latency, and
    location-aware healthcare services by utilizing sensors located within users''
    reach. Recently, many researchers have proposed using hierarchical computing for
    the distribution and allocation of inference-based tasks among edge devices and
    fog nodes, which could lead to an increase in computing power and compute capability
    of edge devices. For disease prediction, this chapter discusses a variety of EC
    techniques.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Reconnoitering the Landscape of Edge Intelligence in Healthcare
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Edge computing for smart disease prediction treatment therapy
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu D.
  - Ding Y.
  - Yu G.
  - Zhong Z.
  - Song Y.
  citation_count: '0'
  description: A human-centered, sustainable development technological concept from
    industry 5.0 is rapidly sparking extensive discussions in the academic and professional
    communities. Massive data need to be outsourced to nearby fog nodes and remote
    cloud servers due to the constrained resource of terminals in industrial Internet
    of things (IIoT) in future industry 5.0. However, terminals are skeptical about
    the credibility of the outsourced data due to the loss of physical data possession.
    Existing data storage auditing schemes in cloud computing will be performed with
    high latency, which is not suitable for cloud-fog-assisted IIoT. To improve the
    computational efficiency and reduce the communication overhead, in this paper,
    a privacy-preserving dynamic auditing for regenerating code-based storage in cloud-fog-assisted
    IIoT is proposed. First, a generalized framework of exact reparation regenerating
    code is employed to encode the data file, which can improve the data storage security
    in cloud-fog-assisted IIoT. Then, the ZSS signature is used to generate the authentication
    tag for each encoded data segment. Moreover, a proper data structure is designed
    to store the encoded data and authenticators in storage servers based on the properties
    of exact reparation regenerating code, which can efficiently make full use of
    the fragmented storage space and significantly reduce the latency of data update.
    Security analysis shows that the proposed scheme provides the resistance of forgery
    attacks, replacing attacks, replaying attacks and ensures the data privacy. Performance
    analysis demonstrates that the proposed scheme can be performed with low computational
    cost and communication overhead that can be well used in cloud-fog-assisted IIoT.
  doi: 10.1016/j.iot.2024.101084
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Preliminaries 4.
    Problem statement 5. The proposed scheme 6. Evaluation 7. Conclusion Ethics approval
    CRediT authorship contribution statement Declaration of competing interest Funding
    Data availability References Show full outline Figures (9) Show 3 more figures
    Tables (2) Table 1 Table 2 Internet of Things Volume 25, April 2024, 101084 Research
    article Privacy-preserving dynamic auditing for regenerating code-based storage
    in cloud-fog-assisted IIoT Author links open overlay panel Dengzhi Liu a b c,
    Yongdong Ding a, Geng Yu a, Zhaoman Zhong a c, Yuanzhao Song d Show more Add to
    Mendeley Share Cite https://doi.org/10.1016/j.iot.2024.101084 Get rights and content
    Abstract A human-centered, sustainable development technological concept from
    industry 5.0 is rapidly sparking extensive discussions in the academic and professional
    communities. Massive data need to be outsourced to nearby fog nodes and remote
    cloud servers due to the constrained resource of terminals in industrial Internet
    of things (IIoT) in future industry 5.0. However, terminals are skeptical about
    the credibility of the outsourced data due to the loss of physical data possession.
    Existing data storage auditing schemes in cloud computing will be performed with
    high latency, which is not suitable for cloud-fog-assisted IIoT. To improve the
    computational efficiency and reduce the communication overhead, in this paper,
    a privacy-preserving dynamic auditing for regenerating code-based storage in cloud-fog-assisted
    IIoT is proposed. First, a generalized framework of exact reparation regenerating
    code is employed to encode the data file, which can improve the data storage security
    in cloud-fog-assisted IIoT. Then, the ZSS signature is used to generate the authentication
    tag for each encoded data segment. Moreover, a proper data structure is designed
    to store the encoded data and authenticators in storage servers based on the properties
    of exact reparation regenerating code, which can efficiently make full use of
    the fragmented storage space and significantly reduce the latency of data update.
    Security analysis shows that the proposed scheme provides the resistance of forgery
    attacks, replacing attacks, replaying attacks and ensures the data privacy. Performance
    analysis demonstrates that the proposed scheme can be performed with low computational
    cost and communication overhead that can be well used in cloud-fog-assisted IIoT.
    Previous article in issue Next article in issue Keywords Industry 5.0IIoTData
    storage auditingRegenerating codeZSS signature 1. Introduction Industry 5.0 (the
    Fifth Industrial Revolution) enhances the current Industry 4.0 framework by leveraging
    research and innovation to catalyze the shift towards a sustainable, human-centric
    and resilient European industry [1]. In particular, Industry 4.0 focuses more
    on the integration of digitalization and AI-driven technologies to increase the
    efficiency and flexibility of production other than the foundational principles
    of social equity and sustainability [2]. In contrast, industry 5.0 prioritizes
    the welfare of the worker as a core element in the production process, using new
    technologies to foster prosperity beyond jobs and growth while respecting the
    production limits of the Earth [1]. The core of Industry 5.0 is Human–Computer
    Interaction. In other words, Industry 5.0 enables the further integration of man
    and machine [3]. The emergent manufacturing paradigm of industry 5.0 requires
    frequent data exchange between production machinery and industrial information
    systems via networks [3]. The basic pillar of digital manufacturing of industrial
    Internet of things (IIoT) can meet the development requirement. IIoT connects
    all the industrial assets, including machines and control systems with the information
    systems and the business processes [4], [5]. With the rapid development of IIoT,
    it is imprudent to locally store all the continuous data in IIoT end devices in
    light of the constrained storage capacity of these devices [6]. Data outsourcing
    serves as a viable approach to handle the substantial amount of data generated
    by resource-constrained devices in industrial settings [7], [8]. Moreover, fog
    computing and cloud computing can facilitate the secure and efficient processing,
    storage, and management of the massive data in IIoT [6]. However, the utilization
    of outsourced storage raises a series of security challenges, including issues
    such as unauthorized data tampering and data exception [9], [10], [11], [12].
    Thus, ensuring the data integrity is essential before the aggregation of the data
    from terminals. Data integrity auditing is an optimistic approach to ensure the
    integrity, reliability and availability of the outsourced data [13], [14], [15],
    [16]. Many auditing schemes have been proposed to check the integrity of data
    stored in remote storage system. However, the existing auditing schemes have much
    computational cost in authenticators generation and proof checking. Moreover,
    the flexibility and the high latency in the integrity checking do not meet the
    practical requirements in cloud-fog-assisted IIoT in industry 5.0. In this paper,
    a privacy-preserving dynamic auditing scheme for distributed storage system based
    on exact regenerating code in Cloud-Fog-Assisted IIoT is proposed. The main contributions
    of the proposed scheme are summarized as follows. (1) A novel authenticator is
    designed based on a short signature of ZSS signature [17] that has lower time
    complexity in the auditing. Note that the designed authenticators in auditing
    generated by terminals’ secret keys and can be verified publicly. (2) A storage
    framework is constructed based on exact reparation regeneration code that each
    storage node only stores a segment of an encoded file. When a raw data is changed,
    the data block of same position in all segments will be changed simultaneously.
    It is worth noting that a segment is assigned an authenticator other than each
    block is signed individually, which can significantly reduce the time cost of
    the data pre-process and decrease extra storage cost for storage servers. (3)
    A high-performance data structure of a multi-level dynamic hash table (ML-DHT)
    is designed for storage nodes to store files and for the TPA to store auxiliary
    information of data. The designed data structure in this paper can effectively
    make full use of the storage space and improve the efficiency of data update.
    The remainder of this paper is constructed as follows. In Section 2, some related
    works of the proposed scheme are described. In Section 3, the necessary preliminaries
    are presented. In Section 4, the formal definition, system model and security
    requirements are described. In Section 5, the proposed scheme in this paper is
    introduced in detail. The correctness analysis, security analysis and performance
    analysis are presented in Section 6. Finally, the conclusion of this paper is
    provided. 2. Related work The integrity checking problem of remote data is initially
    analyzed by Deswarte et al. which illustrates the impracticality about checking
    all files downloading from storage servers and proposed two solutions based on
    challenge-response protocols [18]. Then two profound conceptions were put forward
    that are provable data possession (PDP) and proofs of retrievability (POR). In
    2007, Ateniese et al. determined a definition of PDP that offers probabilistic
    proof of a third party’s storage of a file [19]. Juels and Kaliski introduced
    a description of POR enabling storage party to produce a proof that user can retrieve
    a certain file by collecting sufficient file data as to recover original file
    [20]. Subsequently, massive literatures appeared around data integrity auditing
    based on PDP or POR model. In 2011, Wang et al. proposed an auditing scheme supporting
    data dynamics in cloud computing, where classic Merkle Hash Tree is manipulated
    for block tag authentication [21]. To decrease user’s online burden and data’s
    privacy-preserving, Wang et al. proposed a public auditing for cloud storage,
    where a TPA is introduced and the proposed scheme can support auditing for multiple
    users simultaneously [22]. In 2021, Li et al. proposed a lightweight integrity
    auditing approach for cache data replicas on edge servers, where an improved Merkle
    Hash Tree is designed to generate the integrity proof [23]. However, the existing
    schemes do not consider reparation of corrupted data in the auditing. To recover
    the corrupted data, Wang et al. proposed an integrity auditing scheme for distributed
    cloud storage based on erasure code [24]. Note that the time of verification in
    Wang et al.’s scheme is limited. In 2015, Shen et al. proposed a integrity auditing
    scheme for storage system based on erasure code [25]. To enable public auditability
    and data recovery, Ren et al. proposed a dynamic proof of retrievability scheme
    using network coding, erasure code and rb23Tree [26]. However, the above mentioned
    schemes do not consider the problem of trade-off between communication overhead
    and storage overhead during repair. To reduce the communication overhead, Chen
    et al. designed a remote data checking scheme for network-coding-based distributed
    storage system but client’s online burden was relatively heavy [27]. Liu et al.
    proposed a public auditing scheme for cloud storage based on functional reparation
    regenerating code but bring a considerable overhead in terms of tag re-computation
    [28]. However, the high computational cost and latency of the existing schemes
    cannot suit the practical usage in cloud-fog-assisted IIoT in industry 5.0. To
    improve the efficiency of data auditing and reduce the communication overhead,
    this paper proposes a privacy-preserving dynamic auditing scheme based on exact
    regenerating code and ZSS signature for cloud-fog-assisted IIoT, where the authentication
    tags in the auditing did not need to be re-computed after data repair. Moreover,
    the proposed scheme supports efficient dynamic data update and fast reparation
    with a low cost, which can be well performed for the data security assurance in
    cloud-fog-assisted IIoT in industry 5.0. 3. Preliminaries 3.1. Regenerating code
    Regenerating code can repair data of corrupted nodes in a distributed storage
    system while keeping a tradeoff between storage overhead of nodes and communication
    bandwidth [29]. Exact regenerating code is a branch of regenerating code, which
    can exactly repair the data of node server [30]. A product–matrix framework is
    presented in literature [31] that is a common framework underlying the exact regenerating
    code. Note that symbol is the total number of storage nodes in a distributed storage
    system (DSS) and is the number of the nodes to be communicated for recovering
    the whole data. The symbol of is the number the nodes to be connected by a failed
    node for repairing the data. Three main steps are included in the generalized
    framework, including data encoding, data reconstruction and exact regeneration.
    (1) Data encoding : The algorithm takes as input original file and outputs encoded
    file . A message matrix is encoded into a result matrix by multiplying a special
    encoding matrix , where is the number of the symbols in each encoded data segment.
    The specific formula is presented as Eq. (1) . In particular, each row of is an
    encoding vector of the corresponding th node where vector stored in th node is
    produced by . Note that each symbol of the above matrix belongs to finite field
    . (1) Then, each set of distributed storage nodes stores a row of the encoded
    data matrix . As shown in Fig. 1, the th row is sent to the th server separately,
    where . (2) Data reconstruction : The algorithm takes as input the encoded file
    and outputs the original file . Data reconstruction phase can recover the whole
    original file from symbols uniformly stored in nodes. First, arbitrary nodes of
    DSS respectively send encoded vectors to a trustworthy server. Note that is a
    row vector of encoded data and is a serial number of storage nodes. The symbol
    of is transpose of a vector or matrix. Then, the collector integrates the vectors
    into a matrix equaling to that rows of encoding matrix multiplies original data
    . The detailed operation is shown as Eq. (2). Finally, the collector recovers
    from Eq. (2) by utilizing the property of encoding matrix whose any determinant
    sub-matrix is invertible. (2) (3) Exact regeneration : The algorithm takes as
    input the data of the helper storage node of uncorrupted servers in a DSS, outputs
    the repair result of the corrupted node. Exact regenerating phase can exactly
    repair segments of failed nodes in a DSS without redundantly recovering the whole
    file, which is cost-effective and greatly decreases the communication overhead
    during repair. First, failed node resorts to helper nodes and each helper node
    sends back the product of multiplying by to failed node, where is a subset of
    elements of vector with a length of . Second, the failed node assembles the products
    from helper nodes and the assemble value is equal to , where is a inversible square
    matrix that is determined by the property of encoding matrix. The specific deduction
    process is shown as Eq. (3). Finally, simultaneously left multiply at two sides
    of Eq. (3) and get expected symbols about corrupted data. (3) Download : Download
    high-res image (223KB) Download : Download full-size image Fig. 1. Coded DSS.
    3.2. Elliptic curve discrete log problem (ECDLP) Given two points of and in Elliptic
    Curve , to find an integer such that is a hard problem, provided that such an
    integer exists [32]. 3.3. Bilinear pairing map Given an addictive cyclic groups
    and a multiplicative cyclic group of great prime order , a bilinear pairing such
    that the following properties: (1) Bilinear: . (2) Nondegeneracy: . (3) Computability:
    is computable by an existing algorithm. 3.4. k-CAA (collusion attack algorithm
    with k traitors) assumption For and a private , given , , , , , , where and is
    a cyclic group, to compute is hard within the polynomial time algorithm for some
    [33]. 4. Problem statement 4.1. Formal definition : The algorithm takes as input
    security parameter of , and as output the public system parameter . : The algorithm
    takes as input system parameter , and as output public key and secret key . :
    The algorithm takes as input secret key , side information and encoded file and
    as output signature set of the encoded file. : The algorithm takes as input side
    information of terminal’s data and as output . : The algorithm takes as input
    terminal’s encoded file and signatures set of the file and as output . : The algorithm
    takes as input side information of the th segment of the file to be checked and
    as output the challenge . : The algorithm takes as input the challenge and as
    output the proof . : The algorithm takes as input the proof and as output or .
    : The algorithm takes as input side information set of the challenged segment
    from different files of different terminals and outputs the challenge . : The
    algorithm takes as input responded from challenged servers and outputs batch verification
    result or . 4.2. System model The system model in this paper includes four entities,
    which are terminals, fog node, cloud and TPA (Third Party Auditor). Fig. 2 shows
    the system model. The detailed description of entities is shown as follows. •
    Terminals: Terminals are the generating source of original data in IIoT. The storage
    resource are constrained. Terminals can offload the computational and generated
    data to nearby servers. • Fog: Fog node serves as a service provider that offers
    diverse services to terminals through the Internet. Fog provides constrained computing
    capability and storage resources for terminals. Note that the data stored in fog
    nodes are the auditing targets in this paper. • Cloud: Cloud provides infinite
    resources of computing and storage for fog nodes. In this paper, cloud provides
    services for the data that need to be stored permanently. Moreover, the large-scale
    computing task that cannot be performed on fog nodes can also be outsourced to
    cloud. • TPA: In this paper, the TPA is introduced to audit the integrity of data
    stored in the fog nodes that can ensure the availability and reliability of the
    data stored in fog. Download : Download high-res image (243KB) Download : Download
    full-size image Fig. 2. System model. 4.3. Security model The proposed scheme
    in this paper is designed based on the following security assumptions. (1) Assume
    that the challenged fog node is not untrustworthy. The fog node takes all kinds
    of measures to cover the fact of data corruption resulting from inner exception
    or external attack. Hence, upon receiving the auditing challenges, the challenged
    node may forge the integrity proof of the challenged data to the TPA. (2) Assume
    that the TPA is honest-but-curious. That is to say, the TPA can honestly execute
    data integrity auditing algorithms but it may be curious about the content of
    the data. (3) The cloud servers are not fully trusted. In this paper, the cloud
    is introduced to backup the data and undertake the outsourced computing tasks
    from the fog nodes. Hence, the security of the cloud has no impact on the data
    auditing of the fog nodes in this paper. 4.4. Dynamic structure Data stored in
    storage nodes are not static but dynamic, which will be updated by the terminals
    at any time. Therefore, it is important for storage servers to have ability to
    timely update the stored data and verification metadata. To improve the efficiency
    of data auditing, it is essential to design an efficient data structure for storage
    servers. In this paper, a multi-level dynamic hash table (ML-DHT) for the storage
    nodes is designed, which is an evolution of DHT described in literature [34].
    The data structure of ML-DHT is shown in Fig. 3. In ML-DHT, three kinds of basic
    elements are included, which are terminal elements, file segment elements and
    block elements. The basic elements stored in an array-like structure. The terminal
    element is composed of a terminal identity and a pointer directing to the first
    element of file array of the corresponding terminal. The file segment element
    is composed of a file identifier and a pointer directing to the first element
    of block array of the file. Each block element consists of encoded data blocks
    and a segment-level authentication tag . According to the characteristics of the
    coding, the original file is encoded into segments and the number of blocks in
    each segment is same and constant. Thereby, it is undoubtedly logical to employ
    an array to store data blocks in convenience to search for the specified blocks.
    Moreover, the designed multi-level structure can effectively make full use of
    fragmented storage space. Download : Download high-res image (121KB) Download
    : Download full-size image Fig. 3. Data structure of ML-DHT. 5. The proposed scheme
    5.1. Overview In the proposed scheme, the TPA sends the auditing challenges to
    encoded data stored in fog nodes regularly or on-demand. The challenged storage
    nodes respond integrity proofs back to the TPA. Finally, the TPA verifies the
    received proofs. The aforementioned process represents a classical request–response
    process. 5.2. Specific algorithms This subsection details each algorithm of the
    proposed scheme. 5.2.1. Setup The setup phase initializes some public and private
    parameters of system and terminal, consisting of algorithm and algorithm. (1)
    : The algorithm of is run to generate related system parameters. First, randomly
    choose an addictive cyclic group and a multiplicative cyclic group of great prime
    order with a definition of a bilinear pairing . Second, randomly select an element
    from group . In this way, a pairing group is established. Third, select a one-way
    hash function . Finally, algorithm returns public parameters . (2) : The algorithm
    of is run to generate terminal-related keys. First, terminal randomly selects
    a real number as the secret key. Second, compute and take as the terminal’s public
    key. Therefore, the running result of algorithm is . 5.2.2. Data pre-processing
    The data preprocessing phase, composed of algorithm , and , generates a signature
    for each encoded segment to be outsourced and then sends data and the corresponding
    side information to server and the TPA separately. (1) : The algorithm of is run
    to generate the signatures of encoded data segments. Assume that the raw file
    is encoded into the file which are split simultaneously into segments based on
    the row vector of data matrix of file after executing algorithm, where each for
    contains blocks. In other words, . First, the terminal extracts the related side
    information of segments. Then, the terminal computes an authentication tag for
    each data segment . The formula of tag is computed as Eq. (4) . (4) Note that
    the symbol of is the identifier of the terminal, is the identifier of the file,
    and is serial number of the server node storing one segment, and is the block’s
    indices value in the segment. Therefore, the algorithm returns the signatures
    set . (2) : The algorithm of is run to send auxiliary information of segments
    to the TPA. After the data block is generated and encoded, the side information
    are also created at the same time. Then, terminal sends to the TPA. The TPA inserts
    once the side information is received. Finally, the algorithm returns 1, representing
    that the terminal successfully transmits related information of the outsourced
    data to the TPA. (3) : The algorithm of is run to upload necessary data to fog
    servers. First, the terminal extracts the signature of th segment from signature
    set and the data block set of th segment from the file , respectively. Second,
    the terminal sends and to th server for . Third, upon receiving , th server stores
    these data into the ML-DHT and sends back acknowledgment to the terminal. After
    receiving the response, terminal deletes the file and the signatures locally.
    Finally, the algorithm returns 1, representing that the terminal successfully
    transmits the outsourced data to servers. 5.2.3. Basic auditing In this phase,
    the TPA sends an auditing challenge to server for a segment of a certain file
    of a certain terminal. Then, the challenged server responds proof of this segment.
    Finally, the TPA checks the received proof. The basic auditing phase consists
    of three algorithms, which are , and . (1) : The algorithm of is run to generate
    and send auditing challenges to fog servers. For each challenged segment , the
    TPA chooses a random value for replay attack resistance, where is the serial number
    of server node in a DSS. Note that the random number is different from each other.
    Finally, the challenge is established and sent to th server by the TPA. After
    transmitting the challenge, the TPA computes ( and . The above generated values
    will be used in algorithm. (2) : The algorithm of is run to generate the proof
    of the challenged segment. Upon receiving the challenge , the th server analyzes
    identifier of challenged segment and locates corresponding segment and data blocks.
    After extracting data blocks in the challenged segment, th server computes the
    proof for th segment as Eq. (5). (5) Finally, the server responds to the TPA.
    (3) : The algorithm of is run to verify the proof of challenged segment for checking
    the integrity of the outsourced blocks. Upon receiving from the server, the TPA
    first needs to compute and if they are not computed in advance. Then, the TPA
    verifies the as Eq. (6). (6) If Eq. (6) holds, returns ; otherwise, returns .
    5.3. Batch auditing The proposed scheme in this paper can be extended to support
    batch auditing for multiple segments from same or different files of one or multiple
    terminals. Differ from basic auditing phase, the TPA transmits multiple challenges
    to the corresponding servers. Upon receiving proofs from the servers, the TPA
    verifies all signatures at once by using a checking equation. Note that the generation
    of every proof in batch auditing phase is similar to that in the basic auditing.
    The detailed description of the proof generation is omitted here. In this part,
    two main algorithms about batch auditing are described, which are and . (1) :
    The algorithm of is run to generate and send auditing challenges for a set consisting
    of segments from arbitrary files or terminals. For each challenged segment , the
    TPA chooses a random value , where is order number in the set . Moreover, there
    exist related identifier information attached with each . Finally, challenge is
    established and sent to the servers storing segment by the TPA. Then, the TPA
    pre-computes and for each , where is the serial number of storage node storing
    the challenge segment in DSS. (2) : The algorithm of is run to check the proofs
    of the challenged segments. Upon receiving the proof set from fog servers, the
    TPA checks the as Eq. (7). (7) If above equation holds, return ; otherwise, return
    . 5.4. Dynamic update When a file needs to be uploaded into a set of distributed
    servers, the file first can be encoded using exact-repair regenerating code. According
    to the properties of regenerating codes, each fog server can be assigned a segment
    at a time. Therefore, the following dynamic update is performed at segment level.
    (1) Addition: Assume that a new encoded file needs to be stored into the distributed
    servers, in particular, the th segment for of the file needs to be inserted to
    the th server. First, terminal signs each segment using its own private key and
    sends the uploading request to the TPA and fog nodes, respectively. Upon receiving
    the request, the TPA and fog nodes first need to check the legitimacy of the request
    and responds approval. At the same time, each node creates a -bit one-dimensional
    array used to store data segment as well as tag. Then, the fog node computes the
    location of the file segment element by using hash function and links the pointer
    of this element to the header of this array. Upon receiving approval, node transmits
    the side information to the TPA and data to the th server, where . Finally, the
    TPA stores received and the servers add their own segment data into in ML-DHT.
    (2) Deletion: Suppose that file needs to be deleted, terminal needs to send deletion
    request to the whole set of distributed servers successively. Then, the server
    deletes segments of the file . Meanwhile, a deletion request should be sent to
    the TPA. Finally, upon receiving the deletion request, the th server first locates
    the position of the file segment element pointing to the th segment according
    to . The server removes the pointer of this element and destroyed the block array.
    In addition, the TPA also deletes side information of file . (3) Modification:
    The modification of an original data can lead to the update of the corresponding
    column data in the encoding file. Therefore, the terminal needs to send modification
    requests to all servers in DSS for altering a data value of the segment, with
    the position index of the data block corresponding to the column number of the
    altered source data. Fig. 4 indicates the impact of the changed original data
    on the corresponding encoded data. Specifically, when the change of an th column
    data from the original file occurs, subsequent changes will occur within th encoded
    data blocks of all segment dispersed across servers. Suppose that the th column
    data of encoded file needs to be modified into . To update the corresponding data
    stored in the servers, terminal first generates the tags set of new segments according
    to Eq. (4) and distributes updating requests across servers. Upon receiving the
    request, th server locates position of the data with index in the ML-DHT and replaces
    the original block value with the new block value . In addition, the tag of th
    segment of a file is also updated by new tag . Any operation in the TPA does not
    occur at data modification stage. The reason is that the authentication tag in
    the proposed scheme is not related to version number and timestamp of data block.
    Download : Download high-res image (97KB) Download : Download full-size image
    Fig. 4. The impact of changes from original data. 6. Evaluation This section presents
    the correctness analysis, security analysis and performance analysis of the proposed
    scheme. 6.1. Correctness analysis Suppose that , , , , , , and are run honestly
    by the entities in the IIoT storage system, including terminals, the TPA and fog
    servers. In the basic auditing, the challenged server sends the correct proof
    to the TPA after running . After that, the TPA strictly executes the algorithm
    of ( , ) ( ) and generates the result of with a overwhelming probability. More
    precisely, given , the following computation procedure holds. Download : Download
    high-res image (118KB) Download : Download full-size image Similar to the correctness
    analysis of basic auditing, the result of is if , - , and are strictly executed
    by all entities. To be specific, given that , the following procedure will be
    held. Download : Download high-res image (123KB) Download : Download full-size
    image Hence, the correctness of the proposed data storage auditing in this paper
    is proved. 6.2. Security analysis Theorem 1 Unforgeability of ZSS-Based Signature
    The proposed scheme constructed based on ZSS signature is unforgeable if k-CAA
    assumption in bilinear group holds. Proof It means that forging a ZSS-based signature
    is computationally infeasible under the k-CAA assumption. In [17], Zhang et al.
    described the detailed proof. □ Theorem 2 Resistance to Forgery Attacks The verification
    cannot be passed by the TPA if the server responds the forged proof of incorrect
    block to the challenge for a specified segment. Proof The replaying-attack game
    can be designed as follows. The TPA sends an auditing challenge to the th server.
    Upon receiving the challenge, the server should respond the correct proof of segment
    , to the TPA without any mistakes. However, the incorrect proof of a segment contain
    forged data block . If the verification of the proof can be passed by the TPA,
    the server wins this game; otherwise, it fails. Given . If the server wins the
    game, the following Eq. (8) must be held. (8) However, for the correct proof,
    we have Eq. (9). (9) According to the properties of bilinear map, the following
    conclusion can be drawn. The equation of contradicts with the assumption of the
    prior forging-attack game that the forged block value is not equal to the real
    block value . Hence, the above forging-attack game are failed. That is to say,
    the proposed scheme provides the resistance to forgery attacks. □ Theorem 3 Resistance
    to Replacing Attacks The verification in the proposed scheme cannot be passed
    by the TPA if the challenged server responds the proof of another block to the
    challenge for a specified block. Proof Analogous to the proof of Theorem 2. □
    Theorem 4 Resistance to Replaying Attacks The verification cannot be passed by
    the TPA if the server responds the proof of a previous challenge for a specified
    segment. Proof The following forging-attack game is designed. The TPA sends the
    auditing request of to the th server. Upon receiving the challenge, the challenged
    server responds a previous proof of of the challenged segment to the TPA, where
    the value of is a random number for the previous challenge of the th block. Here,
    replacing value with . If the checking is passed, the server wins; otherwise,
    it fails. Similar to Theorem 2, Theorem 3, the conclusion of is inconsistent with
    the assumption of when the server succeeds in the replaying-attack game. Hence,
    the proposed scheme provides the resistance to replying attacks. □ Theorem 5 Privacy
    Preserving Any adversary cannot learn the terminal’s encoded data from the proof
    sent from the data storage server. Proof Assume that an adversary wants to extract
    the terminal’s data value of after obtaining the proof of a segment. According
    to elliptic curve discrete logarithm problem, the adversary cannot derive the
    data value from . Furthermore, it is difficult to deduct each block value from
    the linear combination of blocks. Hence, the proposed scheme provides the property
    of data privacy preserving. □ 6.3. Performance analysis 6.3.1. Experiment environment
    The proposed scheme in this paper is evaluated through experiments on a PC running
    windows 11 OS that is equipped with 2.50 GHz Intel Core (TM) i9-12900H CPU and
    16.0 GB RAM. The simulations are performed on Eclipse using Java language. All
    algorithms in integrity checking are run based on JPBC (Java Pairing-Based Cryptography)
    Library with version 2.0.0. Furthermore, the bilinear pairing chose here is type
    A, where the elliptic curve is over the field for prime with the embedding degree
    is . 6.3.2. Storage overhead Table 1 lists the storage overhead for each entity
    during auditing the segment of a file stored in a storage node. A terminal stores
    its private key that consists of an element in . The terminal needs to store bits.
    Note that distributed storage nodes respectively store checking tags of a file,
    one tag for each node. Each tag belongs to . Thus, the size of tag is . A storage
    server needs to additionally store bits related to a file, which significantly
    reduces extra storage overhead in comparison with the additional cost of bits
    in the scheme that a block of a data segment is assigned with a authentication
    tag. The TPA stores an auxiliary information element concerning one file that
    consists of two identifiers (UID and FID) at the length of . Hence, the TPA only
    needs to store bits for each file, which is apparently lower than the storage
    cost of the scheme whose TPA stores blocks’ side information of a segment including
    version number, timestamp and so forth. Overall, the proposed scheme in this paper
    brings relatively low storage overhead for all entities including terminal, storage
    node and TPA. Table 1. Storage overhead of data auditing. Entity Storage overhead
    Terminal Storage node TPA 6.3.3. Communication cost Table 2 lists the communication
    overhead in each phase of auditing a data segment. During the data segment pre-processing
    stage, the terminal sends an auxiliary information element to the TPA and transmits
    data block and a checking tag to one of storage servers in a DSS. Each data block
    is at the length of bits. Therefore, the terminal needs to transmit bits to the
    TPA and bits to the storage servers. During the , the TPA sends a challenge of
    a segment to one server that consists of two identifiers and one random number
    in . In other words, the TPA sends bits in total. The communication cost between
    the TPA and server is obviously less than the communication cost of similar schemes
    whose terminal sends the side information of blocks of a segment to the TPA including
    version number, timestamp, random number and so forth. In the proof generation
    phase, the challenged server responds a checking tag and intermediate value in
    to the TPA. Hence, the server sends to the TPA totally. Generally, the communication
    overhead between all entities is relatively low in the proposed scheme of this
    paper. Table 2. Communication overhead of data auditing. Phase Communication overhead
    Data preprocessing : : Challenge : Proof generation : 6.3.4. Computational cost
    analysis We compare the proposed scheme of the computational cost with Liu et
    al.’s scheme [28] in the data auditing. For ease of description, PAEC and PARC
    are used to denote the proposed scheme and Liu et al.’s scheme, respectively.
    (1) Fig. 5 shows the experimental results of the terminal’s signing time for different
    number of data blocks in a segment. Note that the simulation result is mean of
    100 trials. Two conclusions can be learned from Fig. 5. (1) The signature generation
    phase in the proposed scheme spends less time compare to that in PARC [28] under
    the condition of the same number of blocks in a segment. (2) With the increase
    of the number of blocks in a segment, time cost of tag generation in PARC [28]
    increase faster compared to that in our scheme. (2) Fig. 6 illustrates the trend
    of time consumption in auditing proof generation with the difference number of
    blocks in a segment. As the number of blocks increase, the time consumption of
    proof generation increases gradually in PARC [28]. However, the time consumption
    in our scheme is a constant with the increase of blocks. (3) Fig. 7 is the simulation
    result comparison of time cost in verification phase. As the figure depicts, PARC
    [28] incurs a higher time cost compared to our scheme. Moreover, the growth rate
    of time cost in PARC [28] is greater than that in our scheme. Hence, from the
    comparison result in Fig. 5, Fig. 6, Fig. 7, it can be concluded that the proposed
    scheme can be performed with high efficiency in data auditing compared to that
    in Liu et al.’s scheme [28]. Download : Download high-res image (134KB) Download
    : Download full-size image Fig. 5. Time cost of tag generation. Download : Download
    high-res image (142KB) Download : Download full-size image Fig. 6. Time cost of
    proof generation. Download : Download high-res image (143KB) Download : Download
    full-size image Fig. 7. Time cost of verification. 7. Conclusion This paper proposes
    a privacy-preserving auditing scheme for dynamic storage based on exact repair
    regenerating code in cloud-fog-assisted IIoT, where a TPA is delegated to execute
    the auditing tasks of data integrity. Note that a generalized framework of exact
    code is deployed to the distributed storage system in cloud-fog-assisted IIoT,
    which provides the stored data with the reparation. Moreover, to decrease the
    storage cost of storage servers and computational overhead of auditing, the ZSS-based
    authenticator is assigned to each encoded segment other than block that can be
    performed with high efficiency in the authenticator generation, proof generation
    and verification in auditing. In addition, a data structure of ML-DHT is designed
    for storage system that can realize fast data update. Security and performance
    analysis shows that the proposed scheme is secure and high-performance, which
    is suitable for data security assurance in practical cloud-fog-assisted IIoT in
    industry 5.0. Ethics approval The authors herewith do confirm that this manuscript
    has not been published elsewhere and is not also under consideration by the other
    journals. CRediT authorship contribution statement Dengzhi Liu: Writing – review
    & editing, Writing – original draft, Methodology, Investigation, Formal analysis,
    Conceptualization. Yongdong Ding: Writing – original draft, Validation, Methodology,
    Data curation. Geng Yu: Writing – review & editing, Validation, Methodology, Investigation.
    Zhaoman Zhong: Writing – review & editing, Supervision. Yuanzhao Song: Writing
    – review & editing, Supervision. Declaration of competing interest The authors
    declare that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Funding
    This work is supported by the National Science Foundation of China [grant numbers
    62102169, 72174079]; the Postgraduate Research & Practice Innovation Program of
    Jiangsu Ocean University [grant number KYCX2022-12]; the Excellent Teaching Team
    of “Qinglan Project, China ” in Jiangsu Province [grant number 2022-29]. Data
    availability No data was used for the research described in the article. References
    [1] European Commission and Directorate-General for Research and Innovation, Breque
    M., De Nul L., Petridis A. Industry 5.0 – Towards a Sustainable, Human-Centric
    and Resilient European Industry Publications Office of the European Union (2021),
    10.2777/308407 Google Scholar [2] Xu X., Lu Y., Vogel-Heuser B., Wang L. Industry
    4.0 and Industry 5.0—Inception, conception and perception J. Manuf. Syst., 61
    (2021), pp. 530-535, 10.1016/j.jmsy.2021.10.006 View PDFView articleView in ScopusGoogle
    Scholar [3] Zong L., Memon F.H., Li X., Wang H., Dev K. End-to-end transmission
    control for cross-regional industrial internet of things in Industry 5.0 IEEE
    Trans. Ind. Inform., 18 (6) (2022), pp. 4215-4223, 10.1109/TII.2021.3133885 View
    in ScopusGoogle Scholar [4] Lu J., Shen J., Vijayakumar P., Gupta B.B. Blockchain-based
    secure data storage protocol for sensors in the industrial Internet of Things
    IEEE Trans. Ind. Inform., 18 (8) (2022), pp. 5422-5431, 10.1109/TII.2021.3112601
    View in ScopusGoogle Scholar [5] Sisinni E., Saifullah A., Han S., Jennehag U.,
    Gidlund M. Industrial Internet of Things: Challenges, opportunities, and directions
    IEEE Trans. Ind. Inform., 14 (11) (2018), pp. 4724-4734, 10.1109/TII.2018.2852491
    View in ScopusGoogle Scholar [6] Fu J.-S., Liu Y., Chao H.-C., Bhargava B.K.,
    Zhang Z.-J. Secure data storage and searching for industrial IoT by integrating
    fog computing and cloud computing IEEE Trans. Ind. Inform., 14 (10) (2018), pp.
    4519-4528, 10.1109/TII.2018.2793350 View in ScopusGoogle Scholar [7] Dohare I.,
    Singh K., Ahmadian A., Mohan S., Kumar Reddy M P. Certificateless aggregated signcryption
    scheme (CLASS) for cloud-fog centric Industry 4.0 IEEE Trans. Ind. Inform., 18
    (9) (2022), pp. 6349-6357, 10.1109/TII.2022.3142306 View in ScopusGoogle Scholar
    [8] Liu D., Zhang Y., Wang W., Dev K., Khowaja S.A. Flexible data integrity checking
    with original data recovery in IoT-enabled maritime transportation systems IEEE
    Trans. Intell. Transp. Syst., 24 (2) (2023), pp. 2618-2629, 10.1109/TITS.2021.3125070
    View in ScopusGoogle Scholar [9] Li Y., Yu Y., Min G., Susilo W., Ni J., Choo
    K.-K.R. Fuzzy identity-based data integrity auditing for reliable cloud storage
    systems IEEE Trans. Dependable Secure Comput., 16 (1) (2019), pp. 72-83, 10.1109/TDSC.2017.2662216
    View in ScopusGoogle Scholar [10] Li J., Yan H., Zhang Y. Certificateless public
    integrity checking of group shared data on cloud storage IEEE Trans. Serv. Comput.,
    14 (1) (2021), pp. 71-81, 10.1109/TSC.2018.2789893 Google Scholar [11] Liu D.,
    Li Z., Jia D. Secure distributed data integrity auditing with high efficiency
    in 5G-enabled software-defined edge computing Cyber Secur. Appl., 1 (2022), Article
    100004, 10.1016/j.csa.2022.100004 Google Scholar [12] Zhang Y., Xu C., Liang X.,
    Li H., Mu Y., Zhang X. Efficient public verification of data integrity for cloud
    storage systems from indistinguishability obfuscation IEEE Trans. Inf. Forensics
    Secur., 12 (3) (2017), pp. 676-688, 10.1109/TIFS.2016.2631951 View in ScopusGoogle
    Scholar [13] Jin H., Jiang H., Zhou K. Dynamic and public auditing with fair arbitration
    for cloud data IEEE Trans. Cloud Comput., 6 (3) (2018), pp. 680-693, 10.1109/TCC.2016.2525998
    View in ScopusGoogle Scholar [14] Sookhak M., Yu F.R., Zomaya A.Y. Auditing big
    data storage in cloud computing using divide and conquer tables IEEE Trans. Parallel
    Distrib. Syst., 29 (5) (2018), pp. 999-1012, 10.1109/TPDS.2017.2784423 View in
    ScopusGoogle Scholar [15] Shen W., Qin J., Yu J., Hao R., Hu J. Enabling identity-based
    integrity auditing and data sharing with sensitive information hiding for secure
    cloud storage IEEE Trans. Inf. Forensics Secur., 14 (2) (2019), pp. 331-346, 10.1109/TIFS.2018.2850312
    Google Scholar [16] Liu D., Zhang Y., Jia D., Zhang Q., Zhao X., Rong H. Toward
    secure distributed data storage with error locating in blockchain enabled edge
    computing Comput. Stand. Interfaces, 79 (2022), Article 103560, 10.1016/j.csi.2021.103560
    View PDFView articleView in ScopusGoogle Scholar [17] Zhang F., Safavi-Naini R.,
    Susilo W. An efficient signature scheme from bilinear pairings and its applications
    Bao F., Deng R., Zhou J. (Eds.), Public Key Cryptography-PKC 2004, vol. 2947,
    Springer Berlin Heidelberg, Berlin, Heidelberg (2004), pp. 277-290, 10.1007/978-3-540-24632-9_20
    View in ScopusGoogle Scholar [18] Deswarte Y., Quisquater J.-J., Saïdane A. Remote
    integrity checking Integrity and Internal Control in Information Systems VI, vol.
    140, Springer US, Boston, MA (2004), pp. 1-11, 10.1007/1-4020-7901-X_1 View in
    ScopusGoogle Scholar [19] Ateniese G., Burns R., Curtmola R., Herring J., Kissner
    L., Peterson Z., Song D. Provable data possession at untrusted stores Proceedings
    of the 14th ACM Conference on Computer and Communications Security, Association
    for Computing Machinery, New York, NY, USA (2007), pp. 598-609, 10.1145/1315245.1315318
    Google Scholar [20] Juels A., Kaliski B.S. Pors: Proofs of retrievability for
    large files Proceedings of the 14th ACM Conference on Computer and Communications
    Security, Association for Computing Machinery, New York, NY, USA (2007), pp. 584-597,
    10.1145/1315245.1315317 View in ScopusGoogle Scholar [21] Wang Q., Wang C., Ren
    K., Lou W., Li J. Enabling public auditability and data dynamics for storage security
    in cloud computing IEEE Trans. Parallel Distrib. Syst., 22 (5) (2011), pp. 847-859,
    10.1109/TPDS.2010.183 View in ScopusGoogle Scholar [22] Wang C., Chow S.S., Wang
    Q., Ren K., Lou W. Privacy-preserving public auditing for secure cloud storage
    IEEE Trans. Comput., 62 (2) (2013), pp. 362-375, 10.1109/TC.2011.245 View in ScopusGoogle
    Scholar [23] Li B., He Q., Chen F., Jin H., Xiang Y., Yang Y. Auditing cache data
    integrity in the edge computing environment IEEE Trans. Parallel Distrib. Syst.,
    32 (5) (2021), pp. 1210-1223, 10.1109/TPDS.2020.3043755 View in ScopusGoogle Scholar
    [24] Wang C., Wang Q., Ren K., Cao N., Lou W. Toward secure and dependable storage
    services in cloud computing IEEE Trans. Serv. Comput., 5 (2) (2012), pp. 220-232,
    10.1109/TSC.2011.24 View in ScopusGoogle Scholar [25] Shen S.-T., Lin H.-Y., Tzeng
    W.-G. An effective integrity check scheme for secure erasure code-based storage
    systems IEEE Trans. Reliab., 64 (3) (2015), pp. 840-851, 10.1109/TR.2015.2423192
    View in ScopusGoogle Scholar [26] Ren Z., Wang L., Wang Q., Xu M. Dynamic proofs
    of retrievability for coded cloud storage systems IEEE Trans. Serv. Comput., 11
    (4) (2018), pp. 685-698, 10.1109/TSC.2015.2481880 View in ScopusGoogle Scholar
    [27] Chen B., Curtmola R., Ateniese G., Burns R. Remote data checking for network
    coding-based distributed storage systems Proceedings of the 2010 ACM Workshop
    on Cloud Computing Security Workshop, Association for Computing Machinery, New
    York, NY, USA (2010), pp. 31-42, 10.1145/1866835.1866842 Google Scholar [28] Liu
    J., Huang K., Rong H., Wang H., Xian M. Privacy-preserving public auditing for
    regenerating-code-based cloud storage IEEE Trans. Inf. Forensics Secur., 10 (7)
    (2015), pp. 1513-1528, 10.1109/TIFS.2015.2416688 View in ScopusGoogle Scholar
    [29] Dimakis A.G., Godfrey P.B., Wu Y., Wainwright M.J., Ramchandran K. Network
    coding for distributed storage systems IEEE Trans. Inform. Theory, 56 (9) (2010),
    pp. 4539-4551, 10.1109/TIT.2010.2054295 View in ScopusGoogle Scholar [30] Rashmi
    K.V., Shah N.B., Kumar P.V., Ramchandran K. Explicit construction of optimal exact
    regenerating codes for distributed storage 2009 47th Annual Allerton Conference
    on Communication, Control, and Computing, Allerton (2009), pp. 1243-1249, 10.1109/ALLERTON.2009.5394538
    View in ScopusGoogle Scholar [31] Rashmi K.V., Shah N.B., Kumar P.V. Optimal exact-regenerating
    codes for distributed storage at the MSR and MBR points via a product-matrix construction
    IEEE Trans. Inform. Theory, 57 (8) (2011), pp. 5227-5239, 10.1109/TIT.2011.2159049
    View in ScopusGoogle Scholar [32] Zheng Y., Imai H. How to construct efficient
    signcryption schemes on elliptic curves Inform. Process. Lett., 68 (5) (1998),
    pp. 227-233, 10.1016/S0020-0190(98)00167-7 View PDFView articleView in ScopusGoogle
    Scholar [33] Du H., Wen Q. Efficient and provably-secure certificateless short
    signature scheme from bilinear pairings Comput. Stand. Interfaces, 31 (2) (2009),
    pp. 390-394, 10.1016/j.csi.2008.05.013 View PDFView articleView in ScopusGoogle
    Scholar [34] Tian H., Chen Y., Chang C.-C., Jiang H., Huang Y., Chen Y., Liu J.
    Dynamic-hash-table based public auditing for secure cloud storage IEEE Trans.
    Serv. Comput., 10 (5) (2017), pp. 701-714, 10.1109/TSC.2015.2512589 View in ScopusGoogle
    Scholar Cited by (0) View Abstract © 2024 Elsevier B.V. All rights reserved. Part
    of special issue Data Security for Cloud-Fog-Assisted Industrial Internet of Things
    (IIoT) in Future Industry 5.0 Edited by Dr. Haowen Tan (Zhejiang Sci-Tech University,
    Hangzhou, , China), Dr. Max Hashem Eiza (Liverpool John Moores University, Liverpool,
    , United Kingdom), Dr. Kouichi Sakurai (National University Corporation Kyushu
    University, Fukuoka, , Japan), Dr. Sangman Moh (Chosun University, Gwangju, ,
    South Korea) View special issue Recommended articles Blockchain based secure Ownership
    Transfer Protocol for smart objects in the Internet of Things Internet of Things,
    Volume 25, 2024, Article 101002 Kiran M., …, Varun Yarehalli Chandrappa View PDF
    Unveiling the secrets of online consumer choice: A deep learning algorithmic approach
    to evaluate and predict purchase decisions through EEG responses Information Processing
    & Management, Volume 61, Issue 3, 2024, Article 103671 Yiran Li, …, Jia Wu View
    PDF Blockchain-assisted computation offloading collaboration: A hierarchical game
    to fortify IoT security and resilience Internet of Things, Volume 23, 2023, Article
    100880 Zitao Zhou, …, Sitong Zhang View PDF Show 3 more articles Article Metrics
    Captures Readers: 2 View details About ScienceDirect Remote access Shopping cart
    Advertise Contact and support Terms and conditions Privacy policy Cookies are
    used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Internet of Things (Netherlands)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Privacy-preserving dynamic auditing for regenerating code-based storage in
    cloud-fog-assisted IIoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lone K.
  - Sofi S.A.
  citation_count: '0'
  description: Smart devices are concerned about the processing and computation of
    tasks due to their tiny nature. They prefer to offload their tasks to the cloud
    for processing and computation. Due to the huge amount of data being generated
    by smart devices, the cloud becomes inefficient in terms of huge delay. Thus,
    Processing tasks in the cloud can add latency and finally needs to be addressed.
    Thus, fog computing is an alternative to the latency issue. The tasks are offloaded
    to fog instead of the cloud. In this paper, e-TOALB (enhanced task offloading
    and load balancing), a modified and enhanced nature-inspired and meta-heuristic
    ant colony optimization is used to offload tasks in a fog environment. The results
    obtained by the proposed method are compared with Particle swarm optimization
    (PSO), round robin (RR), and ant colony optimization. The numerical results clearly
    show an improvement in average response time and load sharing among all fog nodes.
    The results of the proposed model produce low response time, low average service
    time, and low standard deviation. The proposed scheme aims to find the best possible
    decision for offloading tasks to nearby fog devices and to find an optimal route
    for offloading with the least communication cost and average service time.
  doi: 10.1002/cpe.7951
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Concurrency and Computation: Practice and Experience RESEARCH ARTICLE
    Full Access e-TOALB: An efficient task offloading in IoT-fog networks Kalimullah
    Lone,  Shabir Ahmad Sofi First published: 08 November 2023 https://doi.org/10.1002/cpe.7951
    SECTIONS PDF TOOLS SHARE Summary Smart devices are concerned about the processing
    and computation of tasks due to their tiny nature. They prefer to offload their
    tasks to the cloud for processing and computation. Due to the huge amount of data
    being generated by smart devices, the cloud becomes inefficient in terms of huge
    delay. Thus, Processing tasks in the cloud can add latency and finally needs to
    be addressed. Thus, fog computing is an alternative to the latency issue. The
    tasks are offloaded to fog instead of the cloud. In this paper, e-TOALB (enhanced
    task offloading and load balancing), a modified and enhanced nature-inspired and
    meta-heuristic ant colony optimization is used to offload tasks in a fog environment.
    The results obtained by the proposed method are compared with Particle swarm optimization
    (PSO), round robin (RR), and ant colony optimization. The numerical results clearly
    show an improvement in average response time and load sharing among all fog nodes.
    The results of the proposed model produce low response time, low average service
    time, and low standard deviation. The proposed scheme aims to find the best possible
    decision for offloading tasks to nearby fog devices and to find an optimal route
    for offloading with the least communication cost and average service time. 1 INTRODUCTION
    One of the major developments in the field of technology is the rapid growth of
    the Internet of Things. Figure 1 shows the expected number of smart devices by
    the end of 2025.1 The devices [like smartphones, smart meters, smart vehicles,
    sensors, smart wearables, and so on are interconnected in such a way that various
    fields of life like home, healthcare, grid, city, and many more have made life
    dependent on such technologies.2 IoT has plentiful sensing devices scattered geographically.
    The nodes sense some attributes within the surroundings.3 The data can be collected
    with an aim to store and evaluate it for better decision-making.4 IoT devices
    can sense data but can not process and store the same due to its less power and
    less computation.5 There is a shortage of storage when such devices sense data
    for a long time. Battery drainage, low computation power, and less storage make
    them inefficient in storing and analyzing such data.6 There is a desperate need
    to process this data either locally or remotely. Local processing is limited due
    to the size and power of the device.7 A better way is to offload data to some
    remote location for processing purposes.8 Cloud comes out to be the best solution
    for the problem. There is a limitation in processing the task at cloud.9 It adds
    latency in terms of communication and computation.10 The process of collecting
    tasks from resource-constrained devices and processing the same at resource-rich
    devices like the cloud is known as task offloading.4 The end devices produce a
    lot of data and tasks; processing this data and tasks in the cloud can increase
    computation and communication time.11 There is a need to find a solution to this
    problem. Instead of offloading the tasks to the cloud, a better option is to offload
    the tasks to fog nodes.12 These fog nodes are close to the end users, having devices
    like gateways, routers, networking, and storage facilities.13 While performing
    computation and processing through offloading on fog devices, there are many challenges,
    which need to be addressed.14 First, we need to make sure which tasks are to be
    offloaded and which tasks are to be locally processed.12 An important issue is,
    where to offload the tasks. Second, we need to make sure about allocating communication
    and computation resources.15 FIGURE 1 Open in figure viewer PowerPoint Graphical
    abstract. The main aim is always to reduce energy consumption and task completion
    time. An important issue in IoT networks is quality of service, it has requirements
    like transmission rate, delay, throughput, and bit error rate.16 The problem with
    the IoT-fog environment is the exponential growth of smart devices. Thus, it is
    challenging for them to process this huge data through conventional greedy methods.17
    To overcome this problem and to achieve the objectives of this work, a modified
    and enhanced ant colony optimization technique, e-TOALB is proposed. The simulation
    results show an improvement when compared with round robin, particle swarm optimization,
    and ant colony optimization when average response time and load sharing between
    fog nodes are taken into consideration. Figure 2 shows the graphical abstract
    of the whole work. FIGURE 2 Open in figure viewer PowerPoint Expected IoT connected
    devices. The rest organization of this paper is as follows. Section 2 discusses
    related work. Section 3 discusses ant colony optimization. Section 4 discusses
    the system model of the work. Section 5 discusses the optimization problem formulation.
    Section 6 presents the proposed task offloading method. Section 9 explains the
    results and discussion while taking different scenarios into consideration. At
    last, section 17 concludes the paper. 2 RELATED WORK Fog computing in IoT-Fog-Cloud
    architectures developed strong bases for future computing. Authors in Reference
    11 have proposed a scheme of offloading while minimizing energy consumption and
    cost. The tasks of different sizes, different computational requirements, and
    peak tolerable delays are generated from multiple users and multiple fog servers
    are used. An optimal solution is generated from the Dinkelbach algorithm. The
    authors have used offloading probability and transmit power as optimization variables.
    The authors failed to address mobility and power optimization. Authors in Reference
    18 have modified the apriori algorithm and formulated an I-apriori method for
    task scheduling in fog environments. The main objective of the work is to minimize
    execution time and waiting time. The issues like power consumption, energy management,
    and cost are not addressed. Authors in Reference 19 have used game theory to offload
    tasks in vehicular edge networks. The task offloading problem is treated as a
    multi-user game problem. Nash equilibrium of the game is computed with the help
    of distributed computation offloading algorithm. The work is based on task offloading
    and has shown a decline in the computation overhead of nodes. The work presented
    in Reference 6 has addressed the problem of overload and minimizing delays at
    fog nodes. The authors have proposed a method that considers changing nature of
    mobile devices and lowers the turnaround time. Turnaround time is minimized using
    an integer programming model with an NP-hard behavior. A prediction mechanism
    is used to predict location, bandwidth, and processing power for better offloading
    decisions. The work has performed offloading of tasks and resource utilization
    to reduce latency and response time. The authors20 have proposed a task offloading
    method in software-defined networks where multi-hop access points connect IoT
    devices with fog nodes. The method provides an integer linear programming method
    to remove the nonlinear behavior of task offloading problems. The focus of the
    work is to offload tasks to fog for processing to reduce energy consumption and
    latency. However, other performance parameters are not taken into consideration.
    The work presented in Reference 21 has proposed an offloading mechanism based
    on user mobility, which reduces the failure rate while offloading tasks in heterogeneous
    networks. Offloading is done in planner and executor modes. The offloading planner
    has very less information about the whole system. It gathers information and passes
    it on to the offloading executor for learning. The learning policy is stored in
    both phases until both converge. The work deals with task offloading and offloading
    probability while minimizing congestion, cost, and latency. Authors in Reference
    22 have considered a multiple input multiple output mobile edge computing with
    a main focus on minimizing buffering delay and power consumption. Authors have
    used a decentralized offloading method to find the best possible policies for
    local and offloaded computations. A deep deterministic policy gradient method
    is adopted to understand such policies. The work has focused on cost, latency,
    and power. Reference 23 proposed a data collection method with privacy-preserving
    and offloading the tasks to the fog devices in an IoT environment assisted with
    Fog. A sampling-based encryption method is used to encrypt data. A joint optimization
    method is used while allocating CPU and channel bandwidth during offloading. The
    main aim of the work is to provide privacy and offloading of data from sensing
    devices to fog devices. Power consumption and latency issues have been solved
    and reduced to a significant level. Reference 12 proposed a deep learning offloading
    mechanism to utilize resources in an efficient way. Tasks generated by end devices
    are offloaded to resource-rich devices for computation and processing. Issues
    of energy consumption and transmission power are taken into consideration and
    are optimized to the best levels. However, response time and cost are not taken
    into account. The authors in Reference 24 proposed an algorithm that works to
    maximize energy efficiency. The framework has various modules which take care
    of communication between the devices. There is an offloading section that decides
    the strategy to offload tasks, with support from two other sections device and
    program profiler. The devices in the surroundings accept the offloaded tasks for
    processing. The devices with higher CPU energy are searched in the whole network.
    The lower energy devices among the list are the nodes that need to offload tasks
    to nodes with higher energy. The main focus is to offload tasks and consume energy
    in the best possible way. The authors in Reference 25 have proposed a prediction-based
    offloading scheme with an infrastructure part and a business logic part. The level
    at which mobile devices are connected is termed as a terminal layer, edge cloud
    as an access layer, and remote cloud as the network layer. The dynamic nature
    of the network produces heterogeneous and diverse data, which is properly handled
    by LSTM (Long Short Term Memory) through prediction. The model works on the offloading
    of tasks and data to a resource-rich device. The main focus of the work is the
    delay while offloading and this model has reduced delay to a tolerable range.
    The model has not addressed issues of energy consumption, power, and cost. The
    authors26 have proposed an offloading technique that works on the consumption
    of energy and execution time. Optimization problems with multi-objectives are
    handled by genetic algorithms. The execution of the concurrent workflow is separated
    into various tasks like scheduled, ready, and unready. The tasks are divided into
    two sets, scheduled and unscheduled. An algorithm has been proposed to address
    the schedule confirmation. The offloading strategy is employed through genetic
    algorithms where fitness functions define the optimal solutions. The authors in
    Reference 27 proposed a computational offloading strategy where many users and
    many offloading points have structured points and limited resources. Offloading
    is treated as a cost-minimization problem with a backtracking-based method to
    find an exact solution. There are various problems related to time complexity
    that is dealt with an improved genetic algorithm and a greedy algorithm. Authors
    in Reference 28 have proposed a task offloading model in an IoT-Fog environment
    with a main focus on minimizing delay. The scheme scales down the service delay
    of tasks produced by IoT devices that are offloaded to fog nodes. The authors
    gave set the upper limits in the form of a threshold in case of offloading. If
    the values reach beyond that threshold, the tasks are offloaded to the cloud.
    The authors in Reference 29 proposed a fruit-fly-based method for offloading that
    works on nominal energy consumption. Its main components are a device manager,
    broker, and task tracker. The device manager keeps track of the node capabilities
    in the cloudlet. The broker assigns tasks to the nodes based on their capability
    and the task tracker executes the tasks. Linear regression is used to keep track
    of client requests and predict wasted resources. The incoming request of the clients
    is either in the form of tuples or attributes. The classification of the jobs
    is done on the basis of a pipeline-based decision tree classifier. Fruit fly optimization
    is used for offloading, where tasks are treated as a fly and are offloaded to
    the cloud or fog. Authors in Reference 26 have used non dominated sorting genetic
    algorithm for task offloading in an edge cloud environment. The challenges of
    energy consumption and latency have been reduced in an optimized way. Tasks from
    sensing devices are offloaded to the fog nodes for processing and computation.
    However, other issues like cost and bandwidth are not addressed. The authors in
    Reference 4 have used a combination of genetic algorithm and particle swarm optimization
    to propose a sub-optimal algorithm to offload tasks to fog. The algorithm optimizes
    task offloading decisions, optimal fog node selection, and energy consumption.
    The authors have discussed energy consumption and latency but have not touched
    on issues like the utilization of resources and cost. The work in Reference 1
    has proposed a smart ant colony optimization technique for offloading tasks in
    fog environments. The authors have proposed a smart ant colony optimization while
    addressing task offloading and resource utilization. Parameters like latency and
    bandwidth are taken into consideration while the authors have not addressed energy,
    power, and cost. The authors in Reference 2 have proposed an ant colony optimization-based
    method to offload tasks to fog for processing and computation. The method has
    provided a scheduling mechanism for tasks and an offloading method for end-device
    tasks for execution at the fog layer. The performance parameters like average
    response time, standard deviation, and degree of imbalance are taken into consideration
    and enhanced as per the parameter demands. The authors in Reference 3 have proposed
    a smart ant Colony optimization to offload tasks to fog devices for computation
    and processing. The proposed work has improved the quality of service in the fog
    environment. The issues like latency, execution time, and degree of imbalance
    have been addressed and their efficiency has been improved. The summary of the
    related work is presented in Table 1. Table 1 includes performance parameters,
    optimization variables, and proposed techniques. TABLE 1. Summary of the related
    work. Paper author Performance parameters used Optimization variable Proposed
    technique 11 Latency, Energy Consumption, Cost Offloading Probability, Transmit
    Power Successive convex approximation, Dinkelbach algorithm 1 Latency Task Offloading,
    Bandwidth, Resource Utilization Smart Ant Colony Optimization 4 Energy consumption,
    Latency Channel Bandwidth, Transmit power, task offloading Combination of particle
    swarm optimization and genetic algorithms 18 Latency Task scheduling I-apriori,
    an enhancement to Apriori algorithm 5 Communication cost, Throughput, Latency
    Task offloading Distributed deep neural networks in IoT-Fog-Cloud environment
    19 Latency, cost Task offloading, bandwidth Distributed computation offloading
    6 Latency, Response Time computation Offloading, Resource Utilization Online assignment
    of mobile application to cloudlets using prediction 20 Energy Consumption, Latency
    Task offloading Dynamic Task offloading in Fog networks 21 Cost, Congestion, Latency
    Task Offloading, Offloading Probability Modified e-greedy and modified soft-max
    algorithms 26 Energy Consumption, Latency Task Offloading Non dominated sorting
    genetic algorithm 22 Cost, Latency, Power Task Offloading Deep deterministic policy
    gradient 23 Latency, Power Fog Data Encryption, Privacy, Task Offloading, Bandwidth
    Privacy enabled data collection and offloading 12 Energy Consumption, Transmission
    Power Resource Allocation, Task Offloading Deep learning based offloading and
    resource allocation 2 Average Response Time, Standard Deviation, Degree of Imbalance
    Scheduling, Task offloading Task offloading using ant colony optimization 1 Latency
    Time, Execution Time, Degree of Imbalance Quality of service, Task offloading
    Task offloading using smart ant colony optimization 3 Average Response Time, Standard
    Deviation, Degree of Imbalance Scheduling, Task offloading Computation offloading
    using artificial bee colony optimization 24 Energy Consumption Task offloading
    Mixed interger non linear programming 25 Dealay Task Prediction, Computation offloading
    Long Short Term Memory based prediction 29 Completion Time, Energy Consumption,
    Response Time Resource Allocation, Task Offloading Fruit fly optimization 26 Execution
    Time, Energy Consumption Computation Offloading Non-dominated Sorting genetic
    algorithm-III 27 Execution Time Task Offloading, Resource Utilization genetic
    algorithm 28 Delay Task Offloading Load sharing using collaboration policy In
    this paper, we will work on task offloading while taking response time and load
    sharing between nodes into consideration. The reason for using response time is
    that it processes tasks very fast and can be easily used in real-time applications.
    Load sharing between nodes uses resources of all the fog nodes in an optimized
    way and it will add more efficiency. Notation Description of the notation Total
    number of end devices used Total number of fog devices used Data rate (Mbps) generated
    by end device g Group of end devices opting a particular fog node Rate of arrival
    of data (Mbps) at fog node Data arrival rate (Mbps) of fog node from all end devices
    belonging to Network latency between end devices and fog nodes Bandwidth of local
    network Bandwidth of cloud network Network latency between fog node and cloud
    Rate at which data arrived (Mbps) at by end device Size of the task coming from
    IoT device Service rate (Mbps) of for data produced by IoT device Service rate
    (Mbps) of for a group of devices, g 3 ANT COLONY OPTIMIZATION Ant colony optimization
    is a technique of finding the shortest path between a food source and an ant colony
    in an optimized way to find the shortest route. Ants start their journey from
    colonies and move randomly to find the food source. They communicate with each
    other through a chemical called pheromones. The amount of pheromone released on
    a path depends on the quantitative and qualitative strength of the food source.
    In this way, ants follow the paths with higher pheromone levels. The ants move
    from one location to other iteratively to find the best possible route. After
    every iteration, ant, z, moves from location R to location Q from a set of un-visited
    locations . The location R denotes an end device location and Q denotes a fog
    node location. The end device wants to offload data and tasks to a particular
    fog node from location R to location Q. (1) (2) Where R,U is the pheromone concentration
    between location P and location Q. R,U is the length of the pheromone trial between
    location R and location Q. is the list of un-visited locations of the ant. q lies
    between [0 … 1] and is a random number distributed uniformly. lies between 0 <=
    <= 1 The ant moves from location R to location S with a probability: (3) and are
    heuristic constants, where , controls the impact of pheromone quantity and , finds
    the task offloading quality. is the quantity of the task offloading. (4) Taking
    quality of the outcome into consideration, pheromone trails must be updated on
    regular basis. While building the best possible solution, the local pheromone
    trail updating strategy is as follows: (5) Where is the pheromone trail quantity
    for an ant z in the iteration t. is the pheromone quantity for the ant z in the
    iteration (t+1). is the pheromone evaporation rate which lies in (0 < < 1). (6)
    Where (7) Where is the length of the global best tour from the start of the trail.
    is the global evaporation rate. 4 SYSTEM MODEL The architecture has three layers
    beginning from the bottom via the middle to the top as in Figure 3. The bottom
    layer has small data-generating IoT devices with minimal capacity and power. The
    top layer has powerful cloud servers with very high processing and storage. The
    middle layer is sand witched between cloud and IoT devices in the form of fog.
    The number of IoT devices and fog devices used are: FIGURE 3 Open in figure viewer
    PowerPoint IoT-fog-cloud system model. where d= 1,2,3 M and where e= 1,2,3 Q The
    nodes at each layer are scattered at different locations but within the range
    of this whole IoT-Fog-Cloud architecture. The small IoT devices are scattered
    in such a way that they can fall within the coverage area of many fog devices.
    These IoT devices can offload their computation and tasks to fog nodes for processing
    and storage in a wireless manner. The fog nodes are connected to each other in
    a meshed manner. Fog devices and cloud servers are mostly connected through wired
    fiber links. The tasks can be processed locally or can be offloaded to fog devices
    or the cloud depending on the IoT node capacity. Fog nodes can process the tasks
    or can offload further to the best possible adjacent fog node or cloud. Offloading
    tasks to the cloud can add delay due to multi-hop distance. We prefer to process
    tasks in the fog because we are taking only real-time tasks into consideration.
    However, the cloud is used to store data and process non-real-time tasks. In our
    case, we are using the cloud only to store task data and computation result data.
    There are various cost parameters that are required for task offloading from end
    devices to fog nodes. Overall communication cost, service time, and average response
    time calculation are explained below as in Reference 2: Cost in terms of communication
    between end device and fog node is calculated as: (8) Cost in terms of communication
    between fog node and cloud c is shown below: (9) Overall communication cost while
    offloading from end devices is: (10) Tasks generated by IoT devices ( ) are offloaded
    to the nearby fog devices ( ). A particular IoT device at any location can be
    in the vicinity of many fog devices. When any device is ready to offload its tasks
    to fog or cloud, it must first receive the availability and strength of all the
    nearby fog nodes. The fog nodes with the highest signal strength and availability
    must be chosen for offloading. We assume that the fog nodes will have an arrival
    rate in terms of Poisson distribution and an M/M/1 queuing model. The model will
    provide us with the waiting time at the fog node queue. If the calculated waiting
    time at a particular fog node is less than the highest tolerable delay of the
    task then node “e” will accept the task from that IoT device. While offloading
    tasks coming from end devices to fog nodes, the service time is found to be: (11)
    Where is the service rate (Mbps) and is the data arrival rate from end devices.
    When we have a large number of end devices, where end devices belong to a specific
    application group ''g''. The service rate is calculated as: (12) Where is the
    service rate (Mbps) at the fog node for specific application class belonging to
    group ''g'' and is the total arrival rate of data coming from end devices belonging
    to ''g''. It can be calculated as: (13) We can find the total utilization of a
    fog node by adding the utilization of offloaded tasks from all end devices as:
    (14) Where if the tasks are offloaded to the fog node. Total response time is
    the sum of end device to fog communication cost, fog to cloud communication cost,
    and average service time: (15) (16) While offloading tasks to a particular node,
    the response time can be calculated as: (17) Load on a particular fog with respect
    to other nodes is: (18) 5 OPTIMIZATION PROBLEM FORMULATION We formulate an optimization
    problem of average response time and load sharing among fog nodes. This work aims
    to minimize the average response time and load sharing among fog nodes by optimizing
    the offloading decision. (19) (20) (19,20A) (19,20B) (19,20C) (19,20D) (19,20E)
    Constraints (19,20A) and (19,20B) indicate the random number distributed uniformly
    and the value of lies between [0,1]. Constraint (19,20C) shows the impact of pheromone
    quantity. Constraint (19,20D) indicates the task offloading quantity. (19,20E)
    demonstrates the percentage of occupied resources, which lie in the range of [0,1].
    6 PROPOSED TASK OFFLOADING METHOD IoT-fog environment is a real-time sensitive
    application, where data is generated at the IoT level and processed at the fog
    nodes. The generated data must be balanced across the nodes keeping in mind the
    existing load on the fog nodes, network bandwidth, and latency due to the network.
    Thus the selection of a fog node must guarantee the completion of the task processing
    to fulfill the QoS constraints. When the number of IoT and fog devices increase,
    the problem takes an NP-hard form. Thus traditional methods to solve such problems
    is challenging. We propose a meta-heuristic approach using a modified ant colony
    optimization. Several NP-hard problems have been solved by ant colony optimization
    in an effective way. It has been used to balance the load in a better way and
    reduce the response time of the tasks. Thus our job is to minimize the response
    time of the tasks and balance the workload of fog nodes in an efficient way. We
    have to initialize the parameters , , , , , , , , , and . In our work, end devices
    or I devices are represented by ants as shown in Algorithm 1. The end devices
    generate tasks and they are to be processed at fog devices. The end devices do
    not have enough resources to process such tasks. Thus offloading is the only option
    for processing these tasks. The process of offloading adds communication cost
    and service time to such tasks. Equations (10) and (11) are used to find out the
    communication cost and service time respectively. The total response time is calculated
    by 16 and the total load on a fog node through 18. The quantity of the offloading
    is given by Equation (4). Algorithm 1. Task offloading We have used the Minkowski
    distance method, a general form of a distance metric in Euclidean space. It contains
    three spatial dimensions and a time dimension to find the exact position. This
    method will allow us to find the distance between two points with an added spatial
    dimension. This spatial dimension allows us to place an end device in the network
    coverage of the fog devices.30 When an end device wants to offload its tasks to
    a fog node. The probability with which an ant(end device) can choose a fog node
    is given by Equation (3). These steps find the pheromone levels of the fog nodes.
    The selection of fog nodes is shown in Algorithm 2. Algorithm 2. Fog node selection
    The data and tasks generated by an IoT device are processed by fog nodes. The
    selection of the fog node depends on the size of the tasks, the time required
    to process the tasks, the delay encountered, the processing power of the nodes,
    and the storage of the fog nodes. Considering all these factors, Steven''s power
    law can be used to find out the magnitude of the processing capability and availability
    of a node, as: (21) Where is the sensing quality constant, which in the case is
    the delay encountered due to previous tasks already in the fog node, is the computation
    power of the fog node, and is the percentage of occupied resources and which are
    in the range, [0-1]. We are using a switch probability method to increase the
    probability of finding a fog node that can handle the end device data and tasks
    efficiently. This method provides us with another fog node if the current node
    is unable to handle requests from the users.31 Global search is used to find the
    best possible fog node. It finds out the capability and availability of the nodes
    to keep a record of all the available resources. In local search, end devices
    will search randomly and will find fog devices. IoT devices can search the fog
    nodes for processing of the tasks on a random basis called a local search if the
    best possible node is not found. local search phases are represented as: (22)
    Where is the solution array, and are the jth and kth fog nodes from the solution
    array, r is a random number in [0-1]. When a lot of nodes are found, the magnitude
    of processing capability and availability of nodes are stored in an array. The
    nodes with the highest magnitude are selected among a group of nodes, we call
    it a global search. It is represented as: (23) Where is the current best solution.
    The method finds the best fog node among all the nodes. 7 RESULTS AND DISCUSSION
    The performance of the proposed model has been evaluated in this section. The
    simulations are performed on PySpark on a PC with 3.19 GHz, 16GB RAM Intel(R)
    Core(TM) i7 CPU 8700 @3.20GHz. We have used a total of 250–2000 sensing devices.
    The processing of these sensing devices is performed on 10–25 fog devices. The
    sensor nodes are scattered within an area of 150 to 350 m. The packet data size
    is 250 to 1024 Kb, the sensor data rate is 5–50, the fog data arrival rate is
    10–50, and the cloud data arrival rate is 20–75. The service rate of fog and cloud
    are 50–300 and 100–400 respectively. The maximum allowed delay of the tasks is
    100 ms. The bandwidth of the fog nodes and cloud is 12 and 300 MB/s. The values
    of heuristic constants and are 0.2 and 2 respectively. The evaporation rate is
    0.1. 7.1 Average response time versus number of IoT devices 7.1.1 Scenario 1 The
    proposed method is executed with a small number of fog nodes initially in this
    scenario, =10. The data rate of the sensing devices is is 30 and the service rate
    ( ) of fog nodes ( ) for data produced by sensor is 200. Figure 4 shows values
    of the average response time of the offloaded tasks at various numbers of sensing
    devices as calculated using Equation (17). The average response time of tasks
    is increasing with an increase in the number of sensing devices. The proposed
    method maintains a lower average response time when compared with RR, PSO, and
    ACO.2 FIGURE 4 Open in figure viewer PowerPoint Scenario 1: Average service time
    versus no of IoT devices. 7.1.2 Scenario 2 In this scenario, the number of fog
    devices are kept the same at = 10. There is a change in the service rate and data
    rate. The service rate ( ) for fog nodes is 150, and the data rate of the sensing
    devices is 40. The proposed model shows better results as compared with other
    methods. Figure 5 shows the graphical representation of the proposed model and
    others. FIGURE 5 Open in figure viewer PowerPoint Scenario 2: Average service
    time versus no of IoT devices. 7.1.3 Scenario 3 This scenario shows the behavior
    of the method with an increase in the number of fog devices from = 10 to = 20.
    Data rate is also varied from = 40 to =50. The service rate of fog nodes is kept
    at = 200. The proposed method shows a lower average service time when compared
    with all other methods as in Figure 6. FIGURE 6 Open in figure viewer PowerPoint
    Scenario 3: Average service time versus no of IoT devices. 7.2 Average service
    time versus iterations Figure 7 shows the values of average response time with
    respect to iterations. The convergence of the proposed method is faster than that
    of PSO, ACO, and RR. The best value of the proposed solution is reached after
    the iteration. FIGURE 7 Open in figure viewer PowerPoint Average service time
    versus iterations. 7.3 Standard deviation versus number of IoT devices The response
    times of the offloaded tasks are computed in the simulation work as shown in Table
    2. The behavior of the proposed method depicts a balance in terms of load sharing
    among nodes. This load sharing is calculated in terms of the standard deviation
    of the response time of the offloaded tasks. A larger value of standard deviation
    means the least balanced fog nodes while the lower values indicate highly balanced
    fog nodes. The standard deviation is calculated as: (24) TABLE 2. Average response
    time of the offloaded tasks with varying IoT devices. No. of IoT nodes RR PSO
    ACO Proposed Scenario 1 250 68 65 68 66 500 74 69 72 68 750 82 75 74 70 1000 88
    83 80 73 1250 97 86 82 77 1500 104 92 85 80 1750 116 94 88 82 2000 124 98 91 85
    Scenario 2 250 88 80 78 75 500 91 82 82 77 750 94 83 83 80 1000 97 88 86 82 1250
    99 93 90 84 1500 104 96 92 87 1750 106 98 96 90 2000 111 99 98 92 Scenario 3 250
    92 89 89 84 500 96 90 90 86 750 104 91 91 87 1000 108 94 92 89 1250 114 99 94
    90 1500 120 100 95 92 1750 127 104 99 94 2000 130 104 99 95 Figure 8 shows values
    of standard deviation with an increase in the number of IoT devices. It is clearly
    indicated that the values of standard deviation are the least among all, with
    the result the nodes are highly balanced in the proposed method using Equation
    (24). FIGURE 8 Open in figure viewer PowerPoint Standard deviation versus no of
    IoT devices. 7.4 Degree of imbalance versus number of IoT devices The proposed
    algorithm has evaluated the degree of imbalance of the load matrix also. It is
    the imbalance between the nodes in a fog environment and is measured as: (25)
    Where i = 1, 2, …, Q Figure 9 indicates the degree of imbalance of load among
    fog nodes and it clearly shows that the load is efficiently balanced among the
    fog nodes in the proposed algorithm. FIGURE 9 Open in figure viewer PowerPoint
    Degree of imbalance versus no of IoT devices. 8 CONCLUSION Resource-constrained
    IoT devices produce a lot of data, which is difficult for them to process. It
    is better to offload this data to the cloud for processing but it can introduce
    a lot of latency. There is a better option to offload tasks to fog to minimize
    latency. This paper presents a task offloading method to address the latency issue.
    A meta-heuristic and nature-inspired ant colony algorithm is modified to offload
    tasks to the fog nodes. The fog nodes process tasks in such a way to enhance average
    response time and balance tasks over fog nodes. The research work can be extended
    in a number of ways to enhance power consumption to reduce processing time and
    overall cost. We can enhance decision-making and better diagnosis in fields like
    healthcare, agriculture, and forecasting. CONFLICT OF INTEREST STATEMENT There
    is not any conflict of interest to mention. Open Research REFERENCES Volume36,
    Issue6 10 March 2024 e7951 Figures References Related Information Recommended
    Cloud and Fog Computing Secure Connected Objects, [1] Minimal channel cost‐based
    energy‐efficient resource allocation algorithm for task offloading under FoG computing
    environment Premalatha Baskar,  Prakasam Periasamy Concurrency and Computation:
    Practice and Experience Cloud and Fog Computing for the IoT Internet of Things:
    Architectures, Protocols and Standards, [1] Energy aware optimal routing model
    for wireless multimedia sensor networks using modified Voronoi assisted prioritized
    double deep Q‐learning Sellamuthu Suseela,  Ravi Krithiga,  Muthusamy Revathi,  Gajendran
    Sudhakaran,  Reddiyapalayam Murugeshan Bhavadharini Concurrency and Computation:
    Practice and Experience IoT/cloud‐enabled smart services: A review on QoS requirements
    in fog environment and a proposed approach based on priority classification technique
    Amel Ksentini,  Maha Jebalia,  Sami Tabbane International Journal of Communication
    Systems Download PDF Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy
    Terms of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies Developing World Access HELP & SUPPORT Contact Us Training
    and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies."'
  inline_citation: '>'
  journal: 'Concurrency and Computation: Practice and Experience'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'e-TOALB: An efficient task offloading in IoT-fog networks'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Khezri E.
  - Yahya R.O.
  - Hassanzadeh H.
  - Mohaidat M.
  - Ahmadi S.
  - Trik M.
  citation_count: '3'
  description: 'Problem statement: Nowadays, devices generate copious quantities of
    high-speed data streams due to Internet of Things (IoT) applications. For the
    most part, cloud computing platforms handle and manage all of these data and requests.
    However, for certain applications, the data transmission delay that comes with
    transferring data from edge devices to the cloud could be unbearable. When there
    are a lot of devices connected to the internet, the public network actually becomes
    a bottleneck for data transfer. In this setting, power management, data storage,
    resource management, and service management all necessitate more robust infrastructure
    and complex processes. More efficient use of network and cloud resources is achievable
    with fog computing''s “intelligent gateway” capability. Methodology: Planning
    and managing resources is one of the most important factors affecting system performance
    (especially latency) in a fog-cloud environment. Planning in an environment with
    fog and clouds is an NP-hard problem. This paper delves into the optimisation
    difficulty of longevity for data-intensive job scheduling in fog and cloud-based
    IoT systems. The issue is initially expressed as an optimisation model for integer
    linear programming (ILP). Next, we provide a heuristic algorithm known as DLJSF
    (Data-Locality Aware Job Scheduling in Fog-Cloud) that is based on the suggested
    formulation. Results: The results of the tests showed that the performance of
    the proposed algorithm is close to the results by an average of 87 %. Also, on
    average, it is 99.16 % better than the LP results obtained from the optimal solution
    obtained from the solver obtained from the solution that the data is processed
    locally. To check the efficiency of the simulation solution, it was repeated for
    tasks with different entry rates and data with different sizes. Conclusion: According
    to the obtained documents, the data transfer approach can be valuable and the
    proposed algorithm has not lost its performance in different conditions.'
  doi: 10.1016/j.rineng.2024.101780
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related works 3. Proposed
    method 4. Evaluation and discussion 5. Conclusions CRediT authorship contribution
    statement Declaration of competing interest Data availability References Show
    full outline Figures (10) Show 4 more figures Tables (8) Table 1 Table 2 Table
    Table 3 Table 4 Table 5 Show all tables Results in Engineering Volume 21, March
    2024, 101780 DLJSF: Data-Locality Aware Job Scheduling IoT tasks in fog-cloud
    computing environments Author links open overlay panel Edris Khezri a, Rebaz Othman
    Yahya b, Hiwa Hassanzadeh c, Mohsen Mohaidat d, Sina Ahmadi e, Mohammad Trik a
    Show more Share Cite https://doi.org/10.1016/j.rineng.2024.101780 Get rights and
    content Under a Creative Commons license open access Highlights • Presented here
    is a way for scheduling jobs in cloud-fog-based IoT systems that make use of data
    location knowledge by combining fog and cloud processing. • In order to reduce
    the overall quantity of makepan, the workplace in the fog system has been located
    through data replication and transmission between fogs, which goes against previous
    works. • Lessening the strain on public cloud resources and network bandwidth
    is the goal of the proposed method. Abstract Problem statement Nowadays, devices
    generate copious quantities of high-speed data streams due to Internet of Things
    (IoT) applications. For the most part, cloud computing platforms handle and manage
    all of these data and requests. However, for certain applications, the data transmission
    delay that comes with transferring data from edge devices to the cloud could be
    unbearable. When there are a lot of devices connected to the internet, the public
    network actually becomes a bottleneck for data transfer. In this setting, power
    management, data storage, resource management, and service management all necessitate
    more robust infrastructure and complex processes. More efficient use of network
    and cloud resources is achievable with fog computing''s “intelligent gateway”
    capability. Methodology Planning and managing resources is one of the most important
    factors affecting system performance (especially latency) in a fog-cloud environment.
    Planning in an environment with fog and clouds is an NP-hard problem. This paper
    delves into the optimisation difficulty of longevity for data-intensive job scheduling
    in fog and cloud-based IoT systems. The issue is initially expressed as an optimisation
    model for integer linear programming (ILP). Next, we provide a heuristic algorithm
    known as DLJSF (Data-Locality Aware Job Scheduling in Fog-Cloud) that is based
    on the suggested formulation. Results The results of the tests showed that the
    performance of the proposed algorithm is close to the results by an average of
    87 %. Also, on average, it is 99.16 % better than the LP results obtained from
    the optimal solution obtained from the solver obtained from the solution that
    the data is processed locally. To check the efficiency of the simulation solution,
    it was repeated for tasks with different entry rates and data with different sizes.
    Conclusion According to the obtained documents, the data transfer approach can
    be valuable and the proposed algorithm has not lost its performance in different
    conditions. Previous article in issue Next article in issue Keywords Locality-awareOptimisationCloud
    computingInternet of thingsFog computingScheduling 1. Introduction Over the past
    few decades, there have been notable advancements in the realm of information
    and communication technology, particularly in the domain of Internet of Things
    (IoT) [1,2]. These activities have resulted in the creation of sophisticated network
    infrastructure, as well as improved flexibility and efficiency in managing data
    and processes associated with different items [3]. An emerging and significant
    area of study is the task scheduling that considers the awareness of location
    in Internet of Things systems that rely on fog computing (also known as Edge Computing).
    As the latest smart items continue to rise in number, it is necessary to manage
    the position information of these objects with more precision [[4], [5], [6]].
    Each of these things, ranging from smart watches to environmental sensors, autonomously
    creates its own location information, which may be utilised to enhance a wide
    range of operations and services. Nevertheless, obstacles such as handling substantial
    amounts of data, restricted bandwidth, and the requirement for prompt response
    in some situations have formed the foundation for enhancing the organization of
    tasks via fog computing. Fog computing enables localized data processing and operations
    for these items, instead of transmitting all the data to remote cloud centres.
    This technique not only enhances the system''s efficiency but also safeguards
    the privacy and security of information. International data company IDC projects
    that by 2025, there will be 55 trillion networked devices, including 550 million
    current sensors, and 76 billion sensor-enabled items linked to the network (Fig.
    1). There are 215 billion sensor devices accessible in US factories, 5.5 billion
    sensors are linked to 120 million autos, and 1.3 million homes are connected to
    250 million sensors. It is projected that 238.2 million wearable devices will
    be in use by 2025. Furthermore, 180 zettabytes of data will be produced by these
    networked devices by 2025, predicts IDC [7,8]. Download : Download high-res image
    (279KB) Download : Download full-size image Fig. 1. Forecasting the device connection
    procedure for networked devices between 2015 and 2025. Although it is an affordable
    option for processing and storing large amounts of data, cloud computing has grown
    in popularity despite its drawbacks [[9], [10], [11]]. In order to manage and
    store data, resources, and energy, effective infrastructure and mechanisms are
    required. Cisco has suggested fog computing as a way to offset some of the drawbacks
    of cloud computing, including high end-to-end latency, traffic jams, and expensive
    communication expenses [12]. A new scheduling model is necessary for fog computing,
    a new computing paradigm [13,14]. Applications for fog computing are often installed
    on client mobile devices, occasionally on a cloud server, and on one of the possible
    fog nodes. These gadgets are powered by finite resources. Fog computing thus complicates
    the decision-making process for computational scheduling [15,16]. Many researchers
    are working to create models of scheduling today. Additionally, current data-location-based
    algorithms have greatly improved in terms of making appropriate use of computer
    and communication resources [17,18]; However, the models that adequately address
    these systems have not yet been provided in their entirety. Thus, a new scheduling
    strategy for the best use of fog and cloud resources is required to achieve the
    system''s intended performance while minimising response times to users and task
    completion times. This will ensure that the desired experience and service quality
    criteria are provided in accordance with the type of user request [19,20]. Knowing
    the location of the data users will help make the best judgements regarding resource
    allocation, as many schedules displayed in the cloud-fog environment do not account
    for their location. The productivity of the resources that are accessible will
    also be increased by taking into account the interaction between clouds that are
    adjacent to one another and the distribution of jobs inside connected clouds.
    This study proposes a way to schedule jobs in cloud-fog-based Internet of Things
    systems that collaborate with the cloud to analyse data and know where the data
    is located. Unlike previous approaches, we employ data duplication and transfer
    between fogs in our method to determine the optimal task placement inside the
    fog system in order to minimise the overall makespan value. Linear programming
    is used to formulate the makespan optimisation issue. The optimisation model has
    been examined in two modes. In the first mode, dubbed LP-Locality, the tasks have
    to be located in one of the three locations where the data copies are placed;
    moving the data is not an option. The LP solver is used to solve this model. The
    LP-Migration mode is the second one. To achieve a faster makespan time, this model
    trades off local execution and gearbox. The LP solver is also used to solve this
    model. In addition to the mathematical model, a low-complexity heuristic algorithm
    known as DLJSF has been suggested and simulated for this scenario. • Motivation
    Since the fog bed has less processing, storage, and communication capacity than
    the cloud, one of the key problems that greatly impacts system performance in
    the fog-cloud context is resource management. The following are some obstacles
    and research questions: a) How can a massive volume of data produced across many
    geographic locations, with processing and storage demands exceeding the capabilities
    of current computer systems, be adequately examined and processed with the least
    amount of delay time? b) In what location should the data set that the linked
    devices generated be kept? In fog or cloud sources? c) What components ought to
    be included in the framework created to address the needs for processing geographic
    data? d) When a user switches to a different fog node, is a user-related process
    on that node interrupted? When resources in fog and cloud are scheduled appropriately
    to meet application needs, resource productivity and efficiency increase, allowing
    for faster user response times and eventually increased profitability. Placing
    data-oriented jobs in a distributed cloud system is a challenging topic. To reduce
    needless data transfers via the Internet platform and boost network traffic, an
    effective algorithm is needed to assign the NP-tasks of complex fog and other
    issues to the processing resources. Many researchers are working to create models
    of scheduling today. Additionally, data location-based algorithms already in use
    have greatly improved in terms of making appropriate use of computational and
    communication resources; yet, thorough presentations of models that properly address
    these systems are still lacking. Therefore, a new scheduling strategy for the
    best use of fog and cloud resources is required in order to achieve the system''s
    intended performance, minimise response times to users, and ensure that the quality
    of the desired experience and service quality are provided in accordance with
    the type of user request. Knowing the location of the data users will help make
    the best judgments regarding resource allocation, as many schedules displayed
    in the cloud-fog environment do not account for their location. The productivity
    of the resources that are accessible will also be increased by taking into account
    the interaction between clouds that are adjacent to one another and the distribution
    of jobs inside connected clouds. The suggested method''s strategy is to lower
    network traffic and the pressure on public clouds. Local data processing offers
    performance benefits like quick response times and minimal latency. Potential
    benefactors of this research include cloud service providers, fog service providers,
    and even users requiring emergency and low-latency services. In conclusion, the
    authors'' contributions and innovations to this study are summarised as follows.
    • Presented here is a way for scheduling jobs in cloud-fog-based IoT systems that
    make use of data location knowledge by combining fog and cloud processing. • In
    order to reduce the overall quantity of makepan, the workplace in the fog system
    has been located through data replication and transmission between fogs, which
    goes against previous works. • Lessening the strain on public cloud resources
    and network bandwidth is the goal of the proposed method. This is how the remainder
    of the paper is structured. A summary of the research conducted on task scheduling
    in the cloud-fog platform is covered in section 2. Section 3 begins with an explanation
    of the architecture of the suggested scheduling system in the fog-cloud platform.
    Next, the problem is expressed as a linear programming problem, and finally, the
    suggested heuristic method, DLJSF, for job distribution scheduling in this platform
    is covered. The performance evaluation of the suggested method is provided in
    section 4, along with an analysis of the evaluation findings. Section 5 concludes
    with a list of future projects and conclusions. 2. Related works Fog computing
    has arisen as a means to fulfil the needs of IoT applications that are not adequately
    addressed by current technologies. Several measures have been implemented to promote
    the progress of fog development, and significant efforts have been made to enhance
    specific elements. Nevertheless, a comprehensive examination of the various methods,
    elucidating their potential integration and application to address specific requirements,
    is necessary. Bellavista et al. [4] have introduced a singular architectural model
    and a novel categorization by conducting a comprehensive comparison of several
    solutions. Ultimately, they formulate definitive conclusions and provide explicit
    guidance for the development of fog-based IoT applications. Zhao et al. (2018)
    work aims to enhance current cloud services by taking into account application
    performance when scheduling next-generation containers. Concretely, this study
    introduces and examines a novel paradigm that prioritises load balancing and application
    performance. In contrast to other research, their model consolidates the conflict
    between load balancing and application performance into a single optimisation
    problem and subsequently employs a statistical approach to efficiently resolve
    it. Cloud computing frameworks are new models designed to enhance the existing
    architectures of the Internet of Things (IoT). Task scheduling is crucial in frameworks
    like these, and optimising the scheduling of IoT task requests can enhance system
    performance and productivity. Abedinzadeh & Akyol [21]. devised a novel approach
    for scheduling IoT queries in a cloud fog environment using Artificial Ecosystem
    Based Optimisation (AEO), which they named AEOSSA. The proposed change involves
    the utilisation of Salp Swarming Algorithm (SSA) operators to enhance the exploitability
    of the Artificial Ecosystem Optimisation (AEO) method in the search for the most
    optimal solution to the problem. The efficacy of the AEOSSA methodology, developed
    to address the task scheduling dilemma, is assessed by employing several synthetic
    and real-world datasets of varying magnitudes. Furthermore, a thorough evaluation
    is conducted to assess the efficacy of AEOSSA in comparison to other established
    meta-heuristic methods. Zhang et al. [22] have proposed a novel data scheduling
    optimisation algorithm called SCC-DSO to tackle the problems of data matching
    deviation and load imbalance in the Internet of Things. This algorithm aims to
    address these issues during the data scheduling process. Initially, based on the
    processing capability of the IoT work node, a data placement algorithm is devised
    to logically allocate the input data for the job. Furthermore, the data scheduling
    queue is optimised by considering the information about the storage location of
    data blocks, hence minimising the occurrence of data scheduling that involves
    non-local implementation. Liu et al. [23] offer an integrated architecture that
    combines IoT, fog, and cloud layers to enhance availability, resource utilisation,
    and response time for time-sensitive applications. The suggested architecture
    consists of two primary components: clustering and scheduling. The clustering
    process entails executing dynamic and completely decentralised clustering of IoT
    nodes to enhance the controllability of mobile IoT nodes, diminish energy consumption
    in nodes with limited resources, and enhance network traffic throughput. The clustering
    flow incorporates a novel multi-objective decentralised clustering algorithm that
    leverages fuzzy logic to facilitate efficient execution on IoT nodes with constrained
    computational capabilities. The scheduling of big data tasks in cloud computing
    environments has garnered significant interest in recent years, primarily due
    to the rapid increase in the number of enterprises that depend on cloud-based
    infrastructure as the foundation for storing and analysing large volumes of data.
    The primary obstacle in scheduling big data services in cloud-based systems is
    to guarantee minimal production time while simultaneously minimising resource
    use. Various strategies have been suggested in an effort to address this obstacle.
    The primary constraint of these methods arises from their disregard for the trust
    levels of virtual machines (VMs), thereby jeopardising the overall quality of
    service (QoS) of the big data analysis process. This compromise extends beyond
    just heartbeat frequency ratios and resource consumption, encompassing security
    challenges such as intrusion detection, access control, authentication, and more.
    In order to address this constraint, Rjoub et al. (2020) introduced a trust-based
    scheduling approach named BigTrustScheduling. This approach consists of three
    main steps: evaluating the trust level of virtual machines, establishing the priority
    level of tasks, and implementing trust-based scheduling. Zhao et al. [24] have
    introduced a method for efficiently organising and scheduling independent tasks
    based on the knowledge of the data storage location. The system replicates each
    data block three times. A single block is stored in the remote repository, whereas
    two blocks are stored in the local cluster. Task scheduling considers the makespan
    and the degree of processor connectivity as two performance criteria. This method
    aims to prevent the transmission of information by carefully considering the data''s
    location. The developers of this approach are seeking a comprehensive scheduling
    strategy rather than an optimal plan. The authors, Sun et al. [25]; have considered
    the placement of data storage inside a network of geographically distributed data
    centres. The term “system input” refers to a collection of tasks that are brought
    into the system for the purpose of scheduling. Tasks require data that has been
    replicated thrice in different places. The goal is to provide a method that will
    reduce the total duration of the system''s operation. Low latency is a critical
    component of a fog computing platform. As mentioned earlier, most of the methods
    used for fog applications aim to offer users quicker reaction times and reduced
    connection expenses. Table 1 compares and highlights the differences between different
    approaches. These methods prioritise calculations and do not consider the location
    of data generation when discussing the jobs. This project has focused on the examination
    and development of fog computing applications, particularly the processing of
    Internet of Things data at the edge. The proposed methodology primarily emphasises
    data-related tasks. Furthermore, it has been proven that maintaining data''s location
    will alleviate the burden on the network and the public cloud. Table 1. Prior
    research and the present investigation are compared. Ref. target environment Resource
    Application type Type of task or work Goals Wang et al. [26] Fog-cloud computer
    power Task Massive Computer-Heavy discover the sweet spot between the makespan
    and the monetary expense of utilising cloud resources Ali et al. [27] Small Cloud
    Processor Task Massive Computer-Heavy Reduce power use for Executing each user''s
    responsibilities Lakhan et al. [28] D2D combined with fog Processor Workflow Massive
    Computer-Heavy Streamlining the processing of user requests Fakhri et al. [29]
    Fog CPU Workflow Intensive delay tolerant of delays An optimal ratio of CPU runtime
    to user-allocated memory Karabulut et al. [30] Fog-cloud Processor Set of tasks
    Massive Computer-Heavy Reduce power use for Executing each user''s responsibilities
    Shrisha et al. [31] cloud computer power Job computational An optimal ratio of
    CPU runtime to user-allocated memory Razaque et al. [32] Fog Data storage and
    processing power Set of tasks Intensive delay tolerant of delays Lowering the
    expense of the network and processing time Sun et al. [25] Edge Fog Data storage
    and processing power Set of tasks Intensive delay tolerant of delays Reducing
    processing time and enhancing the user interface Zhao et al. [33] Fog memory Set
    of tasks Intensive delay tolerant of delays Reducing processing time and enhancing
    the user interface Hedayati et al. [34] Fog Data storage and processing power
    Job Intensive delay tolerant of delays An optimal ratio of CPU runtime to user-allocated
    memory Trik et al. [35] D2D fogging VM Job computational Reduce power use for
    Executing each user''s responsibilities This Work Fog-cloud Data storage and processing
    power and Computational Task aData-driven Municipal cloud and lowering network
    traffic, Knowing where the data is located allows one to plan data-oriented actions
    in the fog-cloud platform. a Data-Driven is a term in the field of decision-making
    and includes a wide range of things including collecting data, extracting patterns
    and facts and using these data in decision-making. 3. Proposed method 3.1. Planning
    for an IoT system hosted in the cloud This research investigated the cloud-fog
    system''s data processing activity scheduling problem. Think of it as a generic
    system whose fogs and clouds are distributed throughout different parts of the
    world. The ability to collaborate and complete the duties given by the timetable
    are assumed capabilities of these centres. Each cloud and fog centre has its own
    unique set of computing resources, which reveal its processing power. • Data of
    model: Sensors and users in many parts of the world contribute to this system''s
    data pool. It is common practice to save the produced data to the closest cloud
    storage provider. In order to avoid losing important data, it is usual practice
    to replicate it in many systems. The data can be duplicated and kept in different
    locations at different costs, depending on the resources that are available and
    the current state of the business. Although there would be complication and cost
    associated with repeating data across sources, data is often repeated to a certain
    degree. Assuming it is the input to the problem, the location of the data is crucial.
    • Work model: Batch jobs are unique from interactive ones since they usually do
    not call for user involvement. Data processing applications that are input into
    the system in batches are the focus of this research. Processes using large amounts
    of data constitute one category of such applications. The quantity of processed
    data dictates the number of tasks that are combined into one. • Task of model:
    Depending on their processing model, applications can be categorised into several
    tasks. Each job takes some of the incoming data and runs it through an appropriate
    and accessible computer resource. It follows that these duties are considered
    to be separate and distinct. Similar computer resources have the same execution
    time for a certain activity. But data is required to initiate a task, and that
    data must be kept at the task processing location. Also, the time it takes to
    do a task is based on how long it took to finish the final part of that work.
    • Scheduling of model: Work schedulers in distributed cloud computing platforms
    are primarily responsible for allocating tasks to available computer centres and
    coordinating their execution. The dispatcher is in charge of deciding which system
    resource will complete the job. The coordinator decides the order in which tasks
    are executed in these data centres. 3.2. Architecture of the system As shown in
    Fig. 2, the system under investigation consists of three hierarchical levels.
    First level: the IoT devices that feed data and requests into the system. Layer
    two, next to the terminal nodes, is the fog sources layer. With little communication
    latency, this layer offers local computation, storage, and network services. The
    tier below: The third level contains the geographically dispersed cloud resources,
    which are physically far from the data generators and users but nonetheless have
    ample processing and storage capabilities. Various regions house the system''s
    sources, which encompass both cloud and fog. Download : Download high-res image
    (445KB) Download : Download full-size image Fig. 2. The fog-cloud system''s three-layer
    architecture from a broad perspective. In the proposed system, there is a central
    broker. The broker has a cohesive view of the system resources. Fig. 3 illustrates
    the expected resource coordination scenario for the layered fog computing architecture.
    They are shown as resources in accordance with their qualities and capabilities.
    Traditional clouds are at the top layer, and fog sources are at the perimeter
    [37]. As we go up the layers, the privacy is more secured, the security is stronger,
    and the resources are more potent [38]. It is clear from the graphic that the
    resources are physically arranged into several tiers. For scheduling, resources
    need to be logically integrated. All of these resources are managed holistically
    by the resource manager or broker. Resource abstraction can be accomplished through
    virtualization. Download : Download high-res image (305KB) Download : Download
    full-size image Fig. 3. The fog-cloud system''s layered architecture as seen from
    the sources [36]. Regardless of the chosen mechanism, the following conditions
    must be satisfied: (a) A thorough familiarity with all of the system''s available
    resources for optimally satisfying client requests. b) Oversight and administration
    of all assets, regardless of who owns them or where they are located. b) The method
    for classifying resources and how they are classified when services are requested.
    d) Depending on its capabilities, choose the source. e) Effective management of
    resources. As part of the proposed system, a broker is responsible for handling
    all user requests, allocating resources from the cloud and fog nodes, and selecting
    the most suitable resource for each workload. At the central broker, all incoming
    jobs are put into a FIFO queue. Applications that process information from IoT
    devices, social media platforms, and online retailers often save data to the most
    convenient location. These records might be stored in multiple data centres, depending
    on the cost, availability, and needs of the company. The data''s location is fed
    into the suggested system as input to solve the problem. 3.3. Problem of definition
    Here we explain what the problem is and show how to solve it using the linear
    programming (LP) solver on the fog-cloud platform by expressing the scheduling
    of jobs as a problem. According to Wang et al. [26]; this work utilised the GLPK
    linear solver, which stands for GNU Linear Programming Kit. • Problem input: The
    proposed scheduling problem contains input arguments and decision variables, as
    shown in Table 2. Table 2. Challenge parameters and decision factors. • System
    environment description: A set of F·C = {1, …, M} is presented, representing the
    total number of fog and cloud in the fog-cloud distributed system, which is (m)
    to fog and (n) to cloud. Each fog and cloud''s total number of virtual machines
    is represented by a set V = {1, …, w}. Every virtual machine has a finite amount
    of computational capacity. The processing power of each virtual machine (w) in
    the centre (j) is determined by the number of processors and memory of each virtual
    machine (w), which are represented as and , respectively. Each processor''s (mips)
    of that processor determines its execution speed. The resource''s processing capacity
    in millions of instructions per second is determined by this value. and represent
    the bandwidth and propagation delay of two centres, (j) and (y), respectively.
    • Job description: The tasks J = {1, …, E} are entered into the general scheduler
    to be scheduled. An independent task is represented by the set when a job is divisible
    by (y). is the calculation value for each job . The value of is determined in
    this way: (1) Parameter Description F·C = {1, …, M} The system''s accessible set
    of fog and cloud centres J = {1, …, E} There are tasks in system E that need to
    be planned. w = {1, …, W} Cloud and fog centres'' inventory of virtual machines
    Each job''s set of responsibilities Information files kept by the system Distance
    in bits between two data centres y and j Time elapsed between the use of two computer
    resources, y and j Resource processor count (w) in centre j The needed number
    of processors Calculating task duration data arrangement decision variable Total
    execution time on resource v at centre i Makespan the entire system 3.4. Formulation
    of problems using linear programming The variable is the decision variable of
    the problem if and only if is placed in the source (w) from the centre (j) and
    its data is available through the centre (y) is equal to one; otherwise, it will
    be zero, in accordance with the entries given in the previous section [39,40].
    Put another way, a scheduler must determine which data is accessible for a given
    task and where it should be executed for each task that is part of the system''s
    set of tasks. In this study, the first option is called the data placement option,
    while the second option is called the data access option. The goal is to determine
    the optimal schedule with the shortest consumption time. In this work, “makepan”
    refers to the point at which the final task on all resources was finished. When
    represents the overall execution time on resource v at centre i, the system''s
    makespan is displayed by . As a result, the following is how our scheduling problem
    is expressed in linear programming: (2) (3) (4) (5) (6) (7) Each task''s total
    execution time that has been completed on a resource equals the resource''s total
    execution time. A task''s execution time is made up of three components: the task
    processing time, the time needed to transfer the necessary data, and the link
    release delay time if the data is coming from a remote source. As a result, in
    the first limit, the processing time of task on resource v in centre j is represented
    by the value of . The data transmission time required by task at its execution
    site is indicated by the value of . Equation (2) constraint demonstrates that
    makespan sets a time limit on how long each resource can take to complete. There
    can only be one data centre assigned to each work, according to equation (3)''s
    constraint. Equation (4)''s constraint suggests that only one copy of the data
    is available for use. Put otherwise, a task cannot be delegated to several sources,
    nor can the data required for it be transferred from various storage sources.
    According to the constraint in equation (5), S is a binary variable. The constraints
    of equations (6), (7) suggest that the chosen resource ought to have sufficient
    memory and processing power to complete a task. When the constraint from equation
    (8) is added, the task must be completed at the data''s location. (8) 3.5. DLJSF
    algorithm for heuristics Data-oriented task placement in cloud-fog distributed
    systems is a challenging and NP-hard problem. To allocate the jobs to the computer
    resources and stop needless data transfers over the Internet platform, an effective
    algorithm with minimal temporal complexity is required. Even though the current
    algorithms based on data location have significantly improved how communication
    and computing resources are used, focusing solely on the location of the data
    can lead to load imbalance in computing resources, place the data in a long processing
    queue, and make it unsuitable for requirements requiring a quick response time.
    As a result, we have introduced the DLJSF algorithm, which allows for data transmission
    to enhance system performance. Algorithm (1): DLJSF 1: procedure DLJSF 2:Input:
    Bipartite graph V; T;J; S;FC; 3:Output: Allocation , Makespan 4:Smax MaxWorkloadf
    5://Step 1: Sort jobs based on how long they should take to complete, then distribute
    the workload. 6: ←0, 7: currently ← ; 8: For j = 1 to E//E represents the total
    number of jobs; 9: For j = 1 to L//L indicates how many tasks there are for the
    job j; 10: For j = 1 to M//The number of Fog/Cloud resources is indicated by M.
    11: For y = 1 to M 12: while t ≤ tmax do 13: Determine Xi fitness value (Fi) 14:
    Update X1 using Eq. (5). 15: If  = 1and (The cloud resource''s memory and processor
    fog is sufficient) then 16://Ki, lj location of data for 17: Find min | currently
    _  +  | 18: target = j; 19: currently _ptarget = currently _ptarget 20: currently
    _pmax = currently _pmax 21:   = | currently _pmax | 22: Jobs are sorted in ascending
    order by . 23://Step 2: put jobs on the least loaded Fog/Cloud resource. 24: For
    i = 1 to E 25: For l = 1 to L 26: For j = 1 to M 27: For y = 1 to M 28: If  =
    1and (Fog/Cloud resource J''s memory and processor are sufficient) then 29: Find
    min | currently _  +  | 30:  _(ptarget) min Workload {Ks(ptarget)} 31: target
    = j; 32: ptarget = ptarget 33: Return P//updated queue for the schedule The following
    three logics are used in the suggested DLJSF algorithm, which is created with
    a check on these three algorithmic logics. The first rationale states that since
    it is easier to do duties at the data location, it is advisable to pay close attention
    to it. Moving data, however, can enhance performance. It starts with the tiniest
    thing in order to stop the migration of large elements. In the event that data
    needs to be moved, it is done so using the quickest route possible between the
    location of the data and the task execution site. There are two steps in the suggested
    algorithm. We choose which work should be scheduled first in the first phase.
    The first stage yields a sorted list based on the job that is the shortest in
    length. Lines 5 through 20 of logic 1, which forms the basis of this phase, are
    included. In order to minimise the makespan of each task, we balance the tasks
    of a single task over various resources from line 6 to line 20. The value of ,
    which is initialised to zero for all tasks in line 6, represents the makespan
    time of each job, which is defined as the completion time of the final task of
    a task on resources. In line 11, the task is chosen to be sent if the conditions
    of having the data in the centre (y) and having the source''s memory and processor
    in the centre (j) are met. The task t li is added to the chosen source''s queue
    in line 16. We arrange the jobs in row 17 according to their value. In the second
    stage, jobs are successfully assigned to resources using the two logics that follow.
    This stage distributes the workload among the resources with the least load, encompassing
    lines 18 through 30. The second rationale states that tasks should be distributed
    evenly across available resources. The imbalance of tasks inside a task will result
    in a longer reaction time since the makespan of a task is defined as the last
    task performed on a resource. We will be able to accomplish the third logic''s
    objectives if we balance the workload on available resources. The third line of
    reasoning holds that dividing up the entire burden across the resources will help
    to shorten the makespan, as each resource''s maximum load determines how long
    a job can take to complete. However, the amount of resource used is decided by
    the job scheduling algorithm. The scheduling algorithm must be aware of the load
    on each resource in order to execute load balancing, and it must route a work
    to the resource with the lowest load at each stage. 4. Evaluation and discussion
    This section contains an evaluation of the algorithm''s performance that was presented.
    The amount of communication between various centres and makespan are the parameters
    that this research evaluates. Additionally, the last task in the system''s completion
    time is known as makespan, and the quantity of data exchanged between system centres
    is known as communication. These evaluations aim to show the algorithm''s capabilities
    while accounting for data location and transport to enhance the performance of
    the recommended scheduling technique. In particular, Fig. 5 (a, b, and c) shows
    the overall assessment of the DLJSF approach. Fig. 5 (a, b, and c) compares the
    performance of the Central approach with the DLJSF algorithm. Fig. 6(a, b, and
    c) illustrates the DLJSF algorithm''s data transfer pattern. Fig. 6 presents an
    evaluation of the effect of data repetition in selecting the location for task
    execution. 4.1. Evaluation approaches The following is a quick discussion of three
    methods that have been suggested for work assignment and data access. • LP-Locality:
    The first method, known as LP-Locality, is derived in the modelling section by
    adding equation (8) to equation (2) through (7). Tasks must be positioned in one
    of the three locations where the data copy is located in order to satisfy this
    equation. Put another way, data cannot be moved and actions can only be completed
    in these three locations. This model is obtained using its LP solver and is modelled
    as a linear programming problem. • LP-Migration: The second strategy is known
    as “LP-Migration.” Equation (2) through (7) from section 3 problem modelling section
    are included in this method. This method allows for the transfer of data in addition
    to considering its location in order to improve performance. In order to improve
    makespan time, there is a trade-off between local execution and transmission in
    this method. This method is also solved using an LP solver and modelled as linear
    programming. For this method, a heuristic algorithm has been suggested and simulated
    in addition to the mathematical model. 4.2. Fog-cloud configuration parameters
    The eight fog centres and one cloud centre in the suggested system are dispersed
    throughout various geographic regions. Cloud computing centres are situated farther
    away from the final user than they are from users and items that generate data.
    Tasks are carried out using a variety of virtual and physical resources that are
    available to each of these centres, including cloud and fog centres. Table 3 contains
    the physical resource specs, while Table 4 contains the virtual resource specifications.
    Table 3. Details of the distributed fog-cloud system''s physical resources. Empty
    Cell Memory (GB) Storage (TB) Number of core Bandwidth (mbps) Processor Speed
    (mips) Number of VM Fog 1 14 2 16 200 5230 3 Fog 2 18 2 18 200 5230 5 Fog 3 20
    2 20 200 2900 7 Fog 4 24 2 24 200 2900 6 Fog 5 18 2 18 200 3950 5 Fog 6 16 2 16
    200 3950 3 Fog 7 20 2 20 200 2900 4 Fog 8 20 2 20 200 2900 5 DataCenter1 300 200
    300 20,000 45,990 15 Table 4. Details of the cloud-distributed system''s virtual
    resources. Empty Cell VMs of Fog and Cloud Memory (GB) Number of core Fog 1 VM1-VM2
    8 3 Fog 2 VM1-VM4 5 5 Fog 3 VM1-VM3 3 3 VM1-VM6 5 5 Fog 4 VM1-VM4 5 5 VM5 3 3
    Fog 5 VM1-VM4 5 5 Fog 6 VM1 7 7 VM2 9 9 Fog 7 VM1-VM3 7 7 Fog 8 VM1-VM3 5 5 VM4
    7 7 DataCenter1 VM1-VM12 17 17 VM13 9 9 As previously stated, the location of
    the data was chosen using the traditional four-iteration strategy. The necessary
    data are randomly stored in repeated fogs using a zipf-like distribution with
    a value of a = [0.002–40]. Applications involving data processing are simulated
    using a non-uniform range produced by the zipf-like distribution. There have been
    three data sets used, with averages of 23, 130, and 676 MB. The system under evaluation
    has established a complete connection between the fog and cloud centres. Table
    5 displays the available bandwidth values between the centres, while Table 6 displays
    the link propagation latency between the centres. The table does not specify how
    the virtual computers inside the fog and cloud are connected. Due to the fact
    that we have assumed that each fog and cloud centre has local storage for data
    storage, as well as that there is no internal propagation delay or transmission
    delay. We have assumed a Poisson distribution with a range of 1–16 tasks per task,
    and an average of 9. This scenario, which is comparable to the one covered in
    source [41,42], is typically utilised for data analysis in the context of distant
    data centres. • Configurations for modelling and simulation Table 5. Bandwidth
    (Mbps) in between several geographic centres. Empty Cell Fog_1 Fog _2 Fog _3 Fog
    _4 Fog _5 Fog _6 Fog_7 Fog_8 DataCenter1 Fog 1 – 944 786 834 989 688 592 942 384
    Fog 2 942 – 683 418 392 946 692 546 96 Fog 3 786 685 – 632 832 392 641 692 184
    Fog 4 834 419 639 – 548 291 391 419 158 Fog 5 996 392 833 553 – 382 833 688 254
    Fog 6 687 943 392 245 379 – 549 392 84 Fog 7 594 694 641 392 834 543 – 428 142
    Fog 8 946 546 692 419 688 392 416 – 71 DataCenter1 946 94 184 160 254 91 148 123
    – Table 6. Propagation time (in milliseconds) between various geographic centres.
    Empty Cell Fog_1 Fog _2 Fog _3 Fog _4 Fog _5 Fog _6 Fog_7 Fog_8 DataCenter1 Fog
    1 1 5 4.5 5.5 5 7 6 5.5 13 Fog 2 5 1 4 3.5 4.5 6.5 5.5 4 19 Fog 3 4.5 4 2 5 4
    5.5 7 6 12 Fog 4 5.5 3.5 5 2 3.5 4.5 4 3 14 Fog 5 5 4.5 4 3.5 2 2 3 4 16 Fog 6
    7 6.5 5.5 4.5 2 2 4 3 11 Fog 7 6 5.5 7 4 3 4 2 5 15 Fog 8 5.5 4 6 3 4 3 5 2 14
    DataCenter1 13 19 12 14 16 11 15 14 2 A simulator called ClodSim3.06 [43] and
    the NetBeans IDE 7.0.3 were used for algorithm simulation. Data centres, virtual
    machines, applications, users, computing resources, and policies for managing
    different aspects of a system (such as scheduling) can be defined with the help
    of classes in the free and open-source framework known as Cloudsim, which allows
    users to simulate cloud computing scenarios. The linear programming model is solved
    using GLPK6.75. A server running Windows 11 with 8 cores and 4 GB RAM is used
    to solve the linear programming model. Programmes can be solved using GLPK, a
    library for linear programming. Applications include large-scale mixed-integer
    and linear programming, among others. Table 7 shows other simulation parameters.
    Table 7. Rules for the simulated work load. Parameter Value Task count (cloudlets)
    300, 2000 Task duration 1500, 25,000 Data size (350, 650) mb Final product dimensions
    (350, 650) mb 4.3. Examination of outcomes and assessment of performance The three
    strategies that were presented in section 4.1 will be assessed and contrasted
    in this part. For the heuristic algorithm, the average number of executions is
    25, while for the linear programming model, it is 6. 4.3.1. Evaluation of DLJSF''s
    overall effectiveness in cutting makespan This section will compare the LP solver''s
    output with the overall performance of the DLJSF heuristic algorithm to minimise
    the makespan. Three data sets—discussed in section 4.2 with varying average data
    volumes and default configurations have been taken into consideration for this
    purpose. The acquired values are divided by the DLJSF value to normalise the results
    shown in Fig. 4(a), (b), and (c). The above procedure will be applied to all graphs
    that are drawn in terms of makespan in the following in order to normalise the
    outcomes. Download : Download high-res image (430KB) Download : Download full-size
    image Fig. 4. Comparison of the DLJSF algorithm''s overall performance across
    three distinct datasets in the fog-cloud system to minimise the makespan. a) data
    set weighing 23 MB on average. b) a data set weighing 131 MB on average. b) a
    data collection weighing 676 MB on average. Download : Download high-res image
    (369KB) Download : Download full-size image Fig. 5. Comparing the DLJSF algorithm''s
    overall performance in a fog-cloud system using the Central mode to minimise makespan
    over three distinct data sets. a) A 23 MB data collection on average. b) A dataset
    that weighs 128 MB on average. b) A data collection weighing 676 MB on average.
    Download : Download high-res image (415KB) Download : Download full-size image
    Fig. 6. The DLJSF algorithm''s data exchange pattern for three distinct data sets.
    a) a dataset weighing 23 MB on average. b) a dataset weighing 128 MB on average.
    c) The data set, which has a 676 MB average. Fig. 4A, B, and 4C, respectively,
    compare the makespan algorithm for data sizes with averages of 23, 128 MB, and
    676 MB. As previously stated, using the LP solver to solve the problem will yield
    the best outcome among the options mentioned. LP-Locality yields the greatest
    results when tasks are sent only to the centres where the data is located. This
    ensures that the data is executed locally and prevents data from being moved between
    computing centres. Based on a comparison between the DLJSF algorithm and the LP-Locality
    model results, it is concluded that the data localization strategy has resulted
    in a longer task execution length. The suggested algorithm performs better than
    the ideal mode, which solely emphasises the location of data storage. Furthermore,
    it is evident that data-driven decision-making significantly reduces the overall
    execution time. For instance, LP-Migration, LP-Locality, and DLJSF have values
    of 1.71, 0.932, and 1, respectively, in Fig. 4(a). This outcome demonstrates that
    the DLJSF algorithm performs 0.68 % better than LP-Locality. This is the outcome
    of DLJSF''s reduction of scheduling duration through the placement of jobs in
    idle resources and the transfer of information to them. This analysis yields two
    conclusions. The first factor that has a considerable impact on makespan time
    is the local task execution or local data localization policy that is applied.
    Second, the suggested approach has a good chance of lowering the makespan value
    when the data is transported between different centres. The results in this section
    show that, on average, DLJSF is 87 % close to the LP solver. Jobs log in at varied
    rates throughout the day, just as it is observed in the real world. In order to
    do this, the experiment has been repeated with varying workloads—that is, with
    varying numbers of distinct tasks and the algorithm''s performance has been assessed.
    Various inputs have been employed to assess the scheduling algorithm''s resilience.
    How many jobs are available in the system for scheduling is indicated by the number
    of jobs. Increased input volume signifies a high rate of data entry into the system.
    By looking at the numbers, it can be said that adding more tasks to the system
    hasn''t made the algorithm any less efficient. 4.3.2. A comparison between the
    suggested system''s makespan and the centralised mode This section evaluates two
    different Central scenarios: the first, where data is sent to the cloud for processing
    tasks and there is only one cloud; the second, where tasks are distributed between
    fogs and the proposed system model, where the cloud and fog centres can work together.
    Fig. 5(a), (b), and (c) display the evaluation''s findings. It is evident that
    the DLJSF algorithm in the system model that is being given outperforms the centralised
    mode by a factor of 2.6–6. Stated differently, this strategy completes the entire
    job earlier than the Central mode. The delay between the cloud sources and the
    area where the data is generated is significant since the cloud sources are located
    distant from the sources of the data. Additionally, there are a lot of users on
    the communication link between fog and cloud, which means that the bandwidth allotted
    to them is less than the total bandwidth of fog and cloud. This evaluation has
    demonstrated the impact of the transmission delay and high release. 4.3.3. Analysing
    the costs of data transfer between centres Reducing the quantity of communication
    between computing resources is a crucial challenge in data-oriented applications.
    Our objective is to handle every task in the system and carry it out using the
    data''s location. This method seeks to limit the amount of data that is transferred
    across the open communication channel. Time and data mobility need to be traded
    off, which is the problem. Since we meet the ideal total execution time and no
    resource is left idle, communication takes place. Here, the volume of communication
    and data transfer is computed. The task''s needed data size is denoted by . The
    total number of non-local tasks multiplied by the size of data required for each
    task equals the amount of communication. Tasks classified as non-local are those
    that are not handled by one of the data centres'' sources. Equation (9), used
    to obtain this value, is utilised. (9) All data is moved from the network platform
    to the public cloud and processed centrally there under the third strategy described
    in the previous section. The network bandwidth will be consumed by this. As a
    result, all of the data has been transferred, and 143 % of it has been transferred.
    We won''t transmit any data and this percentage will be 0.01 % if we rely solely
    on local processing. The quantity of communication or data transmission in the
    heuristic method that is being given is determined using formula (9). The data
    transfer pattern diagram for the DLJSF algorithm is depicted in Fig. 6. For instance,
    Fig. 6(a) shows that, for 25 jobs, 87 % of the data is processed locally, 7 %
    of the data is transferred across the network link in cloud centres, and 8 % of
    the data is transferred between cloud centres. The network transfers 16 % of its
    total data, and based on these figures, we can infer that the suggested algorithm
    has greatly decreased the network and cloud load as compared to the centralised
    option. To test the algorithm''s resilience to the volume of data and the variety
    of jobs, the data transfer quantity has been measured. Figures (a), (b), and (p)
    illustrate how much data is transferred to the cloud for each mode. For mode (a),
    it ranges from 4 % to 10 %; for mode (b), it ranges from 9 % to 12 %; and for
    mode (p), it ranges from 8 % to 13 %. The DLJSF algorithm has a good chance of
    lowering network and public cloud load, according to this analysis. 4.3.4. The
    impact of data repetition frequency on system efficiency Data regarding resources
    are typically somewhat repeated in distributed systems. There are two benefits
    to doing this again. It increases processing reliability first. It also lessens
    the quantity of communication during processing time. Since three data repeats
    are typically employed in classical settings, we have determined that this is
    the number of repetitions. We alter the number of versions r and examine its impact
    on makespan in order to examine the impact of data repetition on system performance.
    The following outcomes were attained when the trials were conducted again for
    r = 3 to r = 7. By dividing the value obtained in each of the several modes by
    the default mode taken into consideration for the proposed system (r = 4), these
    findings are produced. As can be seen in Fig. 7, the makespan value for the scenario
    with three repeats is equal to 1.35 when compared to the case with four repetitions.
    In this way, as the number of repetitions increases, the makespan value decreases
    as well. These observations can be explained by the fact that more jobs can be
    assigned to the locations of these copies when the data is distributed throughout
    more centres. Consequently, the makespan value likewise drops. Download : Download
    high-res image (172KB) Download : Download full-size image Fig. 7. A comparison
    of how system performance is affected by the quantity of data repetitions for
    141 tasks with an average data size of 676 MB. 4.3.5. The impact of data repetition
    on data access patterns Our objective in this part is to assess how data repetition
    affects the data access pattern. As was previously indicated, a scheduler must
    determine which of the three data iterations is accessible for a given task and
    where to execute each task that is part of the set of tasks in the system. In
    this study, the first decision is called “data placement,” while the second is
    called “data access decision.” Fig. 8 illustrates the data access result. In this
    instance, the 54 % repetition factor means that each data point is distributed
    over 51 % of the system''s centres. In this assessment, we only consider LP-Migration
    and DLJSF. Since these algorithms do not account for migration, they are excluded
    from the analysis. Download : Download high-res image (299KB) Download : Download
    full-size image Fig. 8. The impact of repetitive data on the pattern of data access
    for 150 activities with 676 MB of average data size. The original data is presumed
    to be the initial copy in this case. Here, an attempt is made to determine the
    percentage of processed data that are original, the percentage of copied data,
    and the percentage of data that have moved and undergone processing at distant
    centres. Overall, both algorithms show that when the number of copies rises, more
    jobs are scheduled locally. These observations result from the fact that by spreading
    the data across multiple centres, additional work can be assigned to the locations
    of these copies. Consequently, the makespan value likewise drops. For instance,
    when comparing the 51 % data repetition scenario to the 39 % case, which is a
    total of 72 %, it can be observed that 53 % of the selected data are copy data
    and 80 % of the data are implemented locally. It has worked better if they were
    processed locally. 4.3.6. Comparative analysis Our work is contrasted with AEOSSA
    architecture, HEFT methods, and MobMBER. First, we contrast DLJSF with MobMBER,
    the only method for healthcare Cloud-Fog IoT architecture that is currently available
    in the literature [44]. MobMBER''s task scheduling algorithm aims to minimise
    both the network burden and the duration of the schedule. The emergency factor
    and the task''s data size are the two criteria that MobMBER uses to determine
    a task''s priority. Priority determination in DLJSF, however, is dependent on
    task classification and maximum response time. MobMBER does not take patient movement
    into account while assigning five tasks to each fog device. Conversely, our suggested
    strategy addresses this mobility by putting DLJSF into practice [3]. Second, we
    contrast HEFT with DLJSF. A well-known scheduling heuristic technique called HEFT
    aims to minimise the Makespan for the jobs that are scheduled. As an algorithm,
    HEFT is used in the Cloud-Fog IoT architecture, which was suggested in Ref. [[45],
    [46]]. Our method focuses on latency, whereas the authors of [45] suggest a Makespan-monetary
    cost trade-off for task scheduling on cloud and fog computing devices. Because
    the tasks related to healthcare are time-sensitive. All of the tasks in this approach
    are forwarded to cloud devices for processing. We decide to use the first-come,
    first-served (FCFS) scheduling technique to arrange the workloads. This is accomplished
    by assigning the next job to the ready queue after rotating the cloud nodes in
    sequential sequence. Fig. 7 shows how changing the number of jobs affects Makespan,
    network load, and energy consumption if 52 % of the sink devices are mobile throughout
    the full 29-min simulation period, resulting in 182 consecutive allocations in
    total. To see the differences between the four techniques, we set the y axis in
    Fig. 9(a) and (c) to logarithmic scale. As can be seen in Fig. 9(a), DLJSF performs
    better than the other methods. When there are 150 jobs, the Makespan in MobMBER
    is reported to be more than 16 min (1000 s), whereas it only approaches 47 s in
    DLJSF and HEFT, and approximately 900 s (14 min) for AEOSSA. The Makespan for
    each strategy increases as the number of tasks increases. Even though DLJSF and
    HEFT display comparable Makespan values for tasks with 150 and 150, respectively,
    HEFT has a greater Makespan beginning with 250 tasks, increasing by 110 s at 250
    tasks and reaching approximately 450 s (9 min) at 370 tasks. There are several
    factors that contribute to DLJSF''s superior performance over the other methods.
    First off, in contrast to the AEOSSA architecture, DLJSF uses fog devices in addition
    to cloud devices as processing nodes throughout the allocation process. As a result,
    additional transmission expenses to the distant cloud datacenters are reduced.
    Second, the location of the data stored in the gateway devices is taken into account
    by DLJSF. This reduces the additional expense resulting from the transmission
    delay to distant processing nodes. HEFT and MobMBER, however, make scheduling
    decisions without taking data locality into account. Because of its dual purpose
    optimisation function, which aims to minimise network traffic burden at the price
    of Makespan, it is observed that MobMBER exhibits the poorest Makespan. Furthermore,
    MobMBER decides, independent of the fog device''s capabilities or the task''s
    requirements, to assign the tasks locally on the fog device (by placement in the
    edge-queue) when their emergency level surpasses a certain threshold. Please take
    note that in order to distinguish between the four ways, we have set the y axis
    to a logarithmic scale. It is important to note that for every task, DLJSF''s
    Makespan values are consistently less than the MaxResponse time of 1800 s. The
    network traffic burden for each of the four ways is plotted against the number
    of tasks in Fig. 9(b). It can be demonstrated that because of its dual purpose
    optimisation function, MobMBER has the lowest value for network load. However,
    for varying numbers of jobs, especially when the number of tasks increases, the
    network traffic burden in DLJSF is lower than in both HEFT and AEOSSA approaches.
    Even if the values between HEFT and AEOSSA are comparable, AEOSSA has a larger
    network load since it is dependent on additional processing nodes, or fog nodes,
    in addition to the cloud nodes. The energy consumption of the sink and fog devices
    is plotted against the total number of tasks for MobMBER, HEFT, and DLJSF in Fig.
    9(c). The energy consumption values for DLJSF and HEFT at 150 and 250 jobs are
    displayed in Fig. 9(c). Because DLJSF aims to reduce Makespan latency and use
    more fog devices, it exhibits a greater value when the number of jobs exceeds
    certain thresholds. Even if MobMBER makes an effort to reduce schedule duration
    and traffic burden, it could assign more tasks to fog devices in order to lighten
    the network load required for cloud transmission. Because fog devices have less
    processing power than cloud devices, this may result in a decrease in networking
    energy but a significant rise in computing energy. An rise in the quantity of
    tasks is observed for all three techniques. Download : Download high-res image
    (590KB) Download : Download full-size image Fig. 9. Different amounts of tasks.
    4.3.7. Analysis at runtime This section evaluates the execution times of the algorithms
    used in the experiment. When scheduling, make sure there are no idle data centres
    by setting the check-in rate to 0.2 ms. We monitored the time it took to obtain
    the data and compute the jobs, and in all there are 1300 scheduled jobs. The DLJSF
    and LP solver solution times are shown in Fig. 10. In this figure, the recommended
    method performs significantly better than the LP solver. Compared to DLJSF, the
    LP solution takes an extra 4 min to run on average for a workload of 6 jobs. It
    is also apparent that there is a little difference between LP_Migration and LP_Locality.
    It makes reasonable that computing decisions on-site would become more challenging
    with increasing constraints. Thus, imposing a second constraint on data locality
    results in an increase in the size of the solution matrix and a longer calculation
    time for the solution. This demonstrates the efficacy of DLJSF and our goal of
    developing the programme within a reasonable timeframe. Download : Download high-res
    image (269KB) Download : Download full-size image Fig. 10. Comparison of the time
    to solution in the analysis of time complexity. According to the shown parameters,
    the number of tasks for each of the algorithms has been evaluated in the following
    work, the analysis showed that the proposed algorithm has shown the optimal time
    for the tasks according to the size of the data and the distribution of the tasks.
    In general, it is shown in Fig. 10 that the proposed algorithm was able to reduce
    the complexity time and provide an opportunity to find optimal or near-optimal
    solutions. 5. Conclusions This study offered a method for setting up data-oriented
    job scheduling in a distributed cloud environment. The location of data storage
    has been considered in the suggested approach. Public and cluster cloud systems
    typically utilise location-aware scheduling to avoid consuming network bandwidth.
    However, load balancing is an issue when scheduling jobs locally. Consequently,
    data can be shifted to the fog and cloud centres in order to balance the network''s
    load. While avoiding needless data movement is the goal, data migration is sometimes
    necessary to expedite processing. The simulation results have led to the following
    deductions: While data must be processed locally in a subset of location-based
    techniques, data mobility between computer centres is permitted throughout the
    optimisation phase. Due to the fact that these results'' observation indicated
    how poor the network connection is. For instance, both strategies function nearly
    identically as the data transfer time grows. The observations suggest that the
    data transfer strategy may have some value. The truth is that solving problems
    involving only local data processing takes longer. Due to the poor quality of
    the resources found in the fog, it forces jobs to be queued up for a long time.
    We demonstrated through simulation that the suggested approach performs 87 % closer
    on average to the output of the LP solver''s optimal solution. Additionally, it
    performs 78 % better on average than the solution where the data is handled locally.
    The simulation solution was run again with varying arrival rates and data sizes
    to verify its effectiveness. The results gathered demonstrated that the suggested
    technique continues to function well under many circumstances. Regarding future
    research, the tasks included in this study were made up of a sequence of independent
    tasks; however, a task''s tasks can also be regarded as non-independent, or as
    a work flow where dependent activities cannot be completed simultaneously. Certain
    data that have a high emergency level can be given a priority level so that they
    can be handled more quickly. Although there were no rules for the data migration
    in this study, there are several that can be established. Only the makespan criterion
    was taken into account for optimisation in this study; however, additional factors,
    such as energy, cost, etc., may be included to determine where tasks should be
    executed. CRediT authorship contribution statement Edris Khezri: Conceptualization,
    Data curation. Rebaz Othman Yahya: Investigation, Project administration. Hiwa
    Hassanzadeh: Conceptualization, Resources, Supervision. Mohsen Mohaidat: Project
    administration, Resources. Sina Ahmadi: Funding acquisition, Project administration.
    Mohammad Trik: Methodology, Software, Writing – original draft, Writing – review
    & editing. Declaration of competing interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. Data availability No data
    was used for the research described in the article. References [1] Z. Hao, E.
    Novak, S. Yi, Q. Li Challenges and software architecture for fog computing IEEE
    Internet Comput., 21 (2) (2017), pp. 44-53 View in ScopusGoogle Scholar [2] A.
    AlZailaa, H.R. Chi, A. Radwan, R. Aguiar Low-latency task classification and scheduling
    in fog/cloud based critical e-health applications ICC 2021-IEEE International
    Conference on Communications, IEEE (2021, June), pp. 1-6 CrossRefGoogle Scholar
    [3] M. Abd Elaziz, L. Abualigah, I. Attiya Advanced optimization technique for
    scheduling IoT tasks in cloud-fog computing environments Future Generat. Comput.
    Syst., 124 (2021), pp. 142-154 View PDFView articleView in ScopusGoogle Scholar
    [4] P. Bellavista, J. Berrocal, A. Corradi, S.K. Das, L. Foschini, A. Zanni A
    survey on fog computing for the Internet of Things Pervasive Mob. Comput., 52
    (2019), pp. 71-99 View PDFView articleView in ScopusGoogle Scholar [5] D.H. Kadir
    Statistical evaluation of main extraction parameters in twenty plant extracts
    for obtaining their optimum total phenolic content and its relation to antioxidant
    and antibacterial activities Food Sci. Nutr., 9 (7) (2021), pp. 3491-3499 CrossRefView
    in ScopusGoogle Scholar [6] A.W. Omer, H.T. Blbas, D.H. Kadir A Comparison between
    Brown''s and Holt''s Double Exponential Smoothing for Forecasting Applied Generation
    Electrical Energies in Kurdistan Region (2021) Google Scholar [7] E. Mohammadian,
    M.E. Dastgerdi, A.K. Manshad, A.H. Mohammadi, B. Liu, S. Iglauer, A. Keshavarz
    Application of Underbalanced Tubing Conveyed Perforation in Horizontal Wells:
    A Case Study of Perforation Optimization in a Giant Oil Field in Southwest Iran
    (2022) Google Scholar [8] D. Kadir Bayesian Inference of Autoregressive Models
    Doctoral dissertation, University of Sheffield (2018) Google Scholar [9] D.M.
    Saleh, D.H. Kadir, D.I. Jamil A comparison between some penalized methods for
    estimating parameters: simulation study Qalaai Zanist J., 8 (1) (2023), pp. 1122-1134
    Google Scholar [10] M. Trik, A.M.N.G. Molk, F. Ghasemi, P. Pouryeganeh A hybrid
    selection strategy based on traffic analysis for improving performance in networks
    on chip J. Sens., 2022 (2022), pp. 1-19 CrossRefGoogle Scholar [11] S. Yue, B.
    Niu, H. Wang, L. Zhang, A.M. Ahmad Hierarchical sliding mode-based adaptive fuzzy
    control for uncertain switched under-actuated nonlinear systems with input saturation
    and dead-zone Robot. Intell. Automat., 43 (5) (2023), pp. 523-536 CrossRefView
    in ScopusGoogle Scholar [12] R. Mehta, J. Sahni, K. Khanna Task scheduling for
    improved response time of latency sensitive applications in fog integrated cloud
    environment Multimed. Tool. Appl. (2023), pp. 1-24 View in ScopusGoogle Scholar
    [13] M. Samiei, A. Hassani, S. Sarspy, I.E. Komari, M. Trik, F. Hassanpour Classification
    of skin cancer stages using a AHP fuzzy technique within the context of big data
    healthcare J. Cancer Res. Clin. Oncol. (2023), pp. 1-15 CrossRefGoogle Scholar
    [14] P. Kuppusamy, N.M.J. Kumari, W.Y. Alghamdi, H. Alyami, R. Ramalingam, A.R.
    Javed, M. Rashid Job scheduling problem in fog-cloud-based environment using reinforced
    social spider optimization J. Cloud Comput., 11 (1) (2022), p. 99 View in ScopusGoogle
    Scholar [15] J. Li, Y. Chen, Y. Chen, W. Zhang, Z. Liu A smart energy IoT model
    based on the Itsuku PoW technology Res. Eng., 18 (2023), Article 101147 View PDFView
    articleView in ScopusGoogle Scholar [16] B. Xue, Q. Yang, Y. Jin, Q. Zhu, J. Lan,
    Y. Lin, …, X. Zhou Genotoxicity assessment of haloacetaldehyde disinfection byproducts
    via a simplified yeast-based toxicogenomics assay Environ. Sci. Technol., 57 (44)
    (2023), pp. 16823-16833 CrossRefView in ScopusGoogle Scholar [17] P. Kumar, R.
    Kumar Issues and challenges of load balancing techniques in cloud computing: a
    survey ACM Comput. Surv., 51 (6) (2019), pp. 1-35 Google Scholar [18] C. Cao,
    J. Wang, D. Kwok, F. Cui, Z. Zhang, D. Zhao, …, Q. Zou webTWAS: a resource for
    disease candidate susceptibility genes identified by transcriptome-wide association
    study Nucleic Acids Res., 50 (D1) (2022), pp. D1123-D1130 CrossRefView in ScopusGoogle
    Scholar [19] S.M. Sajadi, D.H. Kadir, S.M. Balaky, E.M. Perot An Eco-friendly
    nanocatalyst for removal of some poisonous environmental pollution and statistically
    evaluation of its performance Surface. Interfac., 23 (2021), Article 100908 View
    PDFView articleView in ScopusGoogle Scholar [20] H. Blbas, D.H. Kadir An application
    of factor analysis to identify the most effective reasons that university students
    hate to read books Int. J. Innov. Creativ. Change, 6 (2) (2019), pp. 251-265 Google
    Scholar [21] M.H. Abedinzadeh, E. Akyol A multidimensional opinion evolution model
    with confirmation bias 2023 59th Annual Allerton Conference on Communication,
    Control, and Computing (Allerton), IEEE (2023, September), pp. 1-8 CrossRefGoogle
    Scholar [22] H. Zhang, Q. Zou, Y. Ju, C. Song, D. Chen Distance-based support
    vector machine to predict DNA N6-methyladenine modification Curr. Bioinf., 17
    (5) (2022), pp. 473-482 CrossRefView in ScopusGoogle Scholar [23] Z. Liu, J. Zhang,
    Y. Li, L. Bai, Y. Ji Joint jobs scheduling and lightpath provisioning in fog computing
    micro datacenter networks J. Opt. Commun. Netw., 10 (7) (2018), pp. B152-B163
    CrossRefView in ScopusGoogle Scholar [24] Y. Zhao, H. Liang, G. Zong, H. Wang
    Event-based distributed finite-horizon $ H_\\infty $ consensus control for constrained
    nonlinear multiagent systems IEEE Syst. J., 32 (12) (2023), pp. 82-98 2023 View
    in ScopusGoogle Scholar [25] J. Sun, Y. Zhang, M. Trik PBPHS: a profile-based
    predictive handover strategy for 5G networks Cybern. Syst., 53 (6) (2022), pp.
    1-22 Google Scholar [26] Z. Wang, Z. Jin, Z. Yang, W. Zhao, M. Trik Increasing
    efficiency for routing in internet of things using binary gray wolf optimization
    and fuzzy logic J. King Saud Univ.-Comput. Inf. Sci., 35 (9) (2023), Article 101732
    View PDFView articleView in ScopusGoogle Scholar [27] I.M. Ali, K.M. Sallam, N.
    Moustafa, R. Chakraborty, M. Ryan, K.K.R. Choo An automated task scheduling model
    using non-dominated sorting genetic algorithm II for fog-cloud systems IEEE Trans.
    Cloud Comput., 10 (4) (2020), pp. 2294-2308 Google Scholar [28] A. Lakhan, Q.U.A.
    Mastoi, M. Elhoseny, M.S. Memon, M.A. Mohammed Deep neural network-based application
    partitioning and scheduling for hospitals and medical enterprises using IoT assisted
    mobile fog cloud Enterprise Inf. Syst., 16 (7) (2022), Article 1883122 View in
    ScopusGoogle Scholar [29] P.S. Fakhri, O. Asghari, S. Sarspy, M.B. Marand, P.
    Moshaver, M. Trik A fuzzy decision-making system for video tracking with multiple
    objects in non-stationary conditions Heliyon, 9 (11) (2023), pp. 230-249 Google
    Scholar [30] E. Karabulut, F. Gholizadeh, R. Akhavan-Tabatabaei The value of adaptive
    menu sizes in peer-to-peer platforms Transport. Res. C Emerg. Technol., 145 (2022),
    Article 103948 View PDFView articleView in ScopusGoogle Scholar [31] H.S. Shrisha,
    U. Boregowda An energy efficient and scalable endpoint linked green content caching
    for Named Data Network based Internet of Things Res. Eng., 18 (7) (2022), pp.
    125-142 13 (2022) 100345 Google Scholar [32] A. Razaque, Y. Jararweh, B. Alotaibi,
    M. Alotaibi, S. Hariri, M. Almiani Energy-efficient and secure mobile fog-based
    cloud for the Internet of Things Future Generat. Comput. Syst., 127 (2022), pp.
    1-13 View PDFView articleView in ScopusGoogle Scholar [33] H. Zhao, G. Zong, H.
    Wang, X. Zhao, N. Xu Zero-sum game-based hierarchical sliding-mode fault-tolerant
    tracking control for interconnected nonlinear systems via adaptive critic design
    IEEE Trans. Autom. Sci. Eng. (2023) Google Scholar [34] S. Hedayati, N. Maleki,
    T. Olsson, F. Ahlgren, M. Seyednezhad, K. Berahmand MapReduce scheduling algorithms
    in Hadoop: a systematic study J. Cloud Comput., 12 (1) (2023), p. 143 View in
    ScopusGoogle Scholar [35] M. Trik, H. Akhavan, A.M. Bidgoli, A.M.N.G. Molk, H.
    Vashani, S.P. Mozaffari A new adaptive selection strategy for reducing latency
    in networks on chip Integration, 89 (2023), pp. 9-24 View PDFView articleView
    in ScopusGoogle Scholar [36] G. Wang, J. Wu, M. Trik A Novel Approach to Reduce
    Video Traffic Based on Understanding User Demand and D2D Communication in 5G Networks
    IETE Journal of Research (2023), pp. 1-17 Google Scholar [37] R. Turaka, S.R.
    Chand, R. Anitha, R.A. Prasath, S. Ramani, H. Kumar, …, Y. Farhaoui A novel approach
    for design energy efficient inexact reverse carry select adders for IoT applications
    Res. Eng., 18 (2023), Article 101127 View PDFView articleView in ScopusGoogle
    Scholar [38] P. Morsali, S. Dey, A. Mallik, A. Akturk Switching modulation optimization
    for efficiency maximization in a single-stage series resonant DAB-based DC-AC
    converter IEEE J. Emerg. Sel. Top. Power Electron., 36 (10) (2023), pp. 532-554
    2023 Google Scholar [39] L. Xiao, Y. Cao, Y. Gai, E. Khezri, J. Liu, M. Yang Recognizing
    sports activities from video frames using deformable convolution and adaptive
    multiscale features J. Cloud Comput., 12 (1) (2023), pp. 1-20 CrossRefGoogle Scholar
    [40] X. Ding, R. Yao, E. Khezri An efficient algorithm for optimal route node
    sensing in smart tourism Urban traffic based on priority constraints Wireless
    Network (2023), pp. 1-18 Google Scholar [41] H. Zhao, G. Zong, X. Zhao, H. Wang,
    N. Xu, N. Zhao Hierarchical sliding-mode surface-based adaptive critic tracking
    control for nonlinear multiplayer zero-sum games via generalized fuzzy hyperbolic
    models IEEE Trans. Fuzzy Syst., 94 (5) (2023), pp. 420-439 2023 View in ScopusGoogle
    Scholar [42] A. Mousavi, H. Arefanjazi, M. Sadeghi, A.M. Ghahfarokhi, F. Beheshtinejad,
    M.M. Masouleh Comparison of feature extraction with PCA and LTP methods and investigating
    the effect of dimensionality reduction in the bat algorithm for face recognition
    Int. J. Robot. Control Syst., 3 (3) (2023), pp. 501-509 CrossRefGoogle Scholar
    [43] A.S. Abohamama, A. El-Ghamry, E. Hamouda Real-time task scheduling algorithm
    for IoT-based applications in the cloud–fog environment J. Netw. Syst. Manag.,
    30 (4) (2022), p. 54 View in ScopusGoogle Scholar [44] M. Khalafi, D. Boob Accelerated
    primal-dual methods for convex-strongly-concave saddle point problems International
    Conference on Machine Learning, PMLR (2023, July), pp. 16250-16270 View in ScopusGoogle
    Scholar [45] R.M. Abdelmoneem, A. Benslimane, E. Shaaban Mobility-aware task scheduling
    in cloud-Fog IoT-based healthcare architectures Comput. Network., 179 (2020),
    Article 107348 View PDFView articleView in ScopusGoogle Scholar [46] G. Wang,
    J. Wu, M. Trik A novel approach to reduce video traffic based on understanding
    user demand and D2D communication in 5G networks IETE J. Res. (2023), pp. 1-17
    Google Scholar Cited by (0) © 2024 The Authors. Published by Elsevier B.V. Recommended
    articles HEPGA: A new effective hybrid algorithm for scientific workflow scheduling
    in cloud computing environment Simulation Modelling Practice and Theory, Volume
    130, 2024, Article 102864 Hind Mikram, …, Youssef Saadi View PDF Experimental
    study on the effect of using phase change materials to prevent of increasing the
    temperature of the interfacial between the hand and the computer mouse Ain Shams
    Engineering Journal, 2024, Article 102626 Fei Zhang, …, Majid Zarringhalam View
    PDF Energy-efficient resource allocation in cloud infrastructure using L3F-MGA
    and E-ANFIS Measurement: Sensors, Volume 31, 2024, Article 100965 S. Prathiba,
    Sharmila Sankar View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 2 Captures Readers: 4 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Results in Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'DLJSF: Data-Locality Aware Job Scheduling IoT tasks in fog-cloud computing
    environments'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gowda N.C.
  - Manvi S.S.
  - Malakreddy A.B.
  - Buyya R.
  citation_count: '0'
  description: A mechanism of fog computing environment is employed in order to enhance
    the cloud computing services toward the edge devices in a range of locations with
    low latency. A fog computing environment is effective when compared to cloud computing
    for providing communication between various edge devices such as smart devices
    and mobile devices used by users in the same location. Even though fog servicing
    extends the best services of cloud computing, it also suffers from a set of security
    threats like authentication, key management, data privacy and trust management.
    Authentication with effective key management between edge devices is the most
    pressing security issue in fog computing. This paper proposes an effective two-way
    authentication between edge devices with key management in fog computing environments
    (TAKM-FC). The edge nodes are the user’s mobile devices and set of smart devices
    controlled by the fog server. To improve the proposed authentication system, we
    have made use of techniques like fuzzy extractor and one-way hash with cryptographic
    primitives. The proposed TAKM-FC scheme is validated mathematically based on the
    ROR model and then verified using the ProVerif tool. The TAKM-FC scheme has been
    evaluated using iFogSim to measure the performance parameters like throughput,
    end-to-end delay, packet loss, energy consumption and network usage. The overhead
    analysis of the proposed scheme is carried out and shows that the computation
    cost, communication cost and storage cost are improved by 11–21%, 8–19% and 6–13%,
    respectively, compared to existing schemes.
  doi: 10.1007/s11227-023-05712-3
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home The Journal of Supercomputing Article
    TAKM-FC: Two-way Authentication with efficient Key Management in Fog Computing
    Environments Published: 30 October 2023 Volume 80, pages 6855–6890, (2024) Cite
    this article Download PDF Access provided by University of Nebraska-Lincoln The
    Journal of Supercomputing Aims and scope Submit manuscript Naveen Chandra Gowda,
    Sunilkumar S. Manvi, A. Bharathi Malakreddy & Rajkumar Buyya  202 Accesses Explore
    all metrics Abstract A mechanism of fog computing environment is employed in order
    to enhance the cloud computing services toward the edge devices in a range of
    locations with low latency. A fog computing environment is effective when compared
    to cloud computing for providing communication between various edge devices such
    as smart devices and mobile devices used by users in the same location. Even though
    fog servicing extends the best services of cloud computing, it also suffers from
    a set of security threats like authentication, key management, data privacy and
    trust management. Authentication with effective key management between edge devices
    is the most pressing security issue in fog computing. This paper proposes an effective
    two-way authentication between edge devices with key management in fog computing
    environments (TAKM-FC). The edge nodes are the user’s mobile devices and set of
    smart devices controlled by the fog server. To improve the proposed authentication
    system, we have made use of techniques like fuzzy extractor and one-way hash with
    cryptographic primitives. The proposed TAKM-FC scheme is validated mathematically
    based on the ROR model and then verified using the ProVerif tool. The TAKM-FC
    scheme has been evaluated using iFogSim to measure the performance parameters
    like throughput, end-to-end delay, packet loss, energy consumption and network
    usage. The overhead analysis of the proposed scheme is carried out and shows that
    the computation cost, communication cost and storage cost are improved by 11–21%,
    8–19% and 6–13%, respectively, compared to existing schemes. Similar content being
    viewed by others A Review of Secure Authentication Techniques in Fog Computing
    Chapter © 2023 Mitigating Security Problems in Fog Computing System Chapter ©
    2022 Unboxing fog security: a review of fog security and authentication mechanisms
    Article 15 August 2023 1 Introduction Cloud computing offers well-organized resource
    allocation and access based on a request from a set of clients/users [1]. Even
    though cloud computing makes our lives easier and more efficient, it is plagued
    by several challenges that include increasing data transfers, insufficiency in
    data capacity, low adaptation to mobility rate, excessive latency, security and
    privacy. The most important of them is cloud computing security and protection,
    which should be approached with caution in practice [2]. Cisco designed the fog
    computing environment to address these challenges, and then expanded its use in
    2012 [3]. Distributed edge devices are used in the fog computing environment to
    offer storage, communication, and computing capabilities. IoT data processing
    has three layers of computing that include the edge layer, the fog layer, and
    the cloud layer. The edge layer includes a substantial number of IoT (Internet
    of Things) or other smart devices and it is the foundation of a fog computing
    system. The Fog layer at an intermediate level contains various fog servers with
    computational and storage devices, whereas the Cloud layer has various service-providing
    devices and cloud servers [4]. Fog computing can be used in conjunction with cloud
    computing, but it is not a replacement for it. Fog computing services are used
    in various applications such as smart cities, smart vehicles, smart grids, automotive
    systems, healthcare and other IoT services mechanisms [4]. Fog computing, on the
    other hand, is an innovative and expanded service of cloud computing services,
    thus it suffers from similar problems such as security concerns, hardware issues,
    and storage safeguards. Among such issues in the fog computing environment, the
    most significant one is security-related problems [5] such as authentication,
    privacy, integrity, trust maintenance, and other security concerns are most important
    to consider. To accomplish safe authentication, effective key management with
    low computation and transmission costs is one of the most careful security concerns.
    To provide security, various mechanisms have been suggested for key exchange and
    authentication in fog computing [6, 7]. However, in dispersed fog and cloud computing
    settings, these must be enhanced to combat all types of security threats. Consider
    the fog server will be acting as a middle party between the edge devices such
    as mobile devices and a set of smart devices. The edge devices communicate with
    each other through the fog server using an insecure channel as it is public. A
    scheme must be designed so that only the authorized user must access avoiding
    any illegal access to the smart device. This is considered to be the authentication
    problem. At the same time the required keys must not be generated only based on
    the store credentials in order to avoid the respective attacks. This is considered
    to be key management issue. So it requires a secured mechanism that can secure
    the connections so that authorized user can only be given access to smart device.
    This paper proposes an effective two-way mutual and multi-level authentication
    between the user’s mobile device and smart device with efficient key management
    controlled by fog server. 1.1 Motivation and problem statement Data flow between
    the edge devices via fog server is insecure as the channel is public and easily
    accessed by malicious users in the vicinity. This raises an issue of authentication
    between edge devices and also with the fog server. The authentication can be considered
    to be a device authentication and message authentication which ensures the legitimacy
    of the device and the exchanged message. Device authentication is all about the
    validation of identity and other credentials of the device, whereas message authentication
    is to maintain the integrity of communication. An authentication scheme must satisfy
    certain requirements such as: (i) computation, communication and storage overhead
    must be low. (ii) the authentication and key management must be strong and scalable.
    (iii) there must be provision for re-authentication and revocation. The most popular
    authentication schemes proposed for fog and cloud computing using symmetric and
    asymmetric cryptographic use lengthy certificates and tripartite algorithms for
    key exchange and authentication. Also, the session key generation used to be carried
    out by fog servers instead of edge devices which lead to heavy network overhead
    such as computation, communication, and storage overheads. They also face too
    many security attacks and are unsuitable for multi-level authentication in a decentralized
    computing environment. Considering all these, this work aims to propose an effective
    multi-level and Two-way Authentication between edge devices with Key Management
    in Fog Computing environments (TAKM-FC). It must ensure device authentication
    and optimizes communication and computation overheads and mitigates several attacks.
    1.2 Contributions in the paper The major contributions of TAKM-FC are highlighted
    as: 1. Proposed a fog-controlled mechanism to provide a two-way mutual, multi-level
    authentication between the smart device and user’s mobile device through session
    key agreement. 2. Mathematical analysis of the TAKM-FC is discussed based on ROR
    model and theoretical analysis is discussed by considering a set of related attacks
    and compared with existing schemes. 3. The overhead analysis of the proposed scheme
    is analyzed and shows that the computation cost, communication cost and storage
    cost are improved by 11–21%, 8–19% and 6–13%, respectively, compared to existing
    schemes. 4. The TAKM-FC has been implemented and evaluated using iFogSim and measured
    throughput, end-to-end delay, packet loss, energy consumption, and network usage
    for different cases. 1.3 Organization of Paper The rest of the paper is organized
    as follows: Sect. 2 elaborates and distinguishes the existing works and the background
    required for the work is given in Sect. 3. The detailed discussion of each phase
    in the proposed work is highlighted in Sect. 4. The security analysis including
    formal and informal security analysis is carried out with a comparison with existing
    works described in Sect. 5. The implementation of TAKM-FC in iFogSim and a discussion
    of the performance analysis considering computation, communication, and storage
    costs are given in Sect. 6. Finally, Sect. 7 provides the conclusion of the paper
    with future enhancements. 2 Related work Bonomi et al. originally defined fog
    computing [3] in 2012, by describing the features and roles of fog nodes in computation.
    They have also demonstrated how fog engineering may be used in other areas, including
    the internet of vehicles, internet of everything, remote sensors, hospital management,
    smart networks, mechanical industries, and actuator systems. In [8], Stojmenovic
    et al. go into further detail on the motivation and benefits of fog computing
    as well as security concerns. Most of the researchers have proposed various schemes
    to achieve authentication and key management for fog and cloud computing environments
    [9, 10]. Those schemes are broadly classified based on the mechanism such as (i)
    Cryptography (ii) Signature (iii) Verification. Cryptographic schemes can be symmetric
    (message authentication codes, hash function, etc.), asymmetric (public key infrastructure,
    elliptic curve cryptography, etc.) and other ID-based cryptographic schemes. The
    signature-based schemes are single-user signature and group signature-based schemes.
    Verification-based schemes are batch verification and cooperative message authentication-based
    schemes. Several authors have proposed authentication systems with mutual authentication
    in the literature [11, 12]. The focus was on the user with a fog server and the
    user with smart device authentication, in which the users may use their mobile
    devices as end devices with much more limited resource usage than the fog server
    as a multi-factor authentication technique [13, 14]. P. Kumar et al. [15] provided
    a mutual authentication technique with untraceability using symmetric key-based
    functions. In [16], Braeken et al. described a system that uses a Trusted Third
    Party (TTP) to perform chaos-based operations. Later many additional mutual identity-based
    authentication techniques have been suggested utilizing TTP [17, 18]. But here
    the TTP must be remained active for complete key agreement phase in all of these
    schemes, but this is not possible for all applications. Further authors Al Hamid
    et al. [19] came up with the idea of using TTP to drive the key management. The
    disadvantage of these techniques is that the fog server is in charge of creating
    the session key, leaving the other two entities inactive while the key is generated.
    As a result, these systems are extremely sensitive to key generation and management
    attacks. As a result, C. Ke et al. [20] and Wu. TY et al. [21] proposed a way
    to create the session key based on the shared smart cards at end nodes itself
    rather than at the fog server. However, it may also lead to numerous ephemeral
    secret leaking attacks. To address this, authors C.-M. Chen et al. in [22] offered
    a secret key exchange, and D. Tiwari et al. in [23] expanded the scheme to include
    a multi-server mechanism. By taking into account the computational and storage
    costs, the technique suggested withstanding numerous attacks in a multi-server
    scenario [24]. Furthermore, because the tasks are executed on end nodes and fog
    nodes, which are publicly accessible and often active in the face of attacks,
    this is an essential assumption [25]. In terms of secure identity-based schemes,
    such as the one given in [26], which are intended for use in mobile computing
    environments where the end nodes are connected to an authentication server for
    getting the access rights. Authors Wu. TY et al. in [27] says that without the
    use of TTP, authentication using dynamic ID exchange cannot offer total accountability.
    In [28], Alsahlani et al. expanded mutual authentication to include proof of a
    link to retain responsibility, although this needed a separate memory for security.
    However, M. Wazid et al. in [29] addresses the problem by just utilizing TTP for
    the deployment step, with the remaining stages running without it. Its main goal
    is to cut down on the computation costs. Yadhav et al. in [30] proposed the symmetric
    key authentication to provide anonymity and unlinkability for fog-powered smart
    devices, which was later enhanced in [31]. They claim that requiring physical
    credentials at IoT devices for authentication is ineffective, therefore he advocated
    employing the physical unclonable feature to provide mutual authentication without
    the need for physical credentials. M. Wazid et al. in [32] focuses on the internet
    of vehicle deployment and could achieve some of the known attacks. H.S. Ali et
    al. has combined the methods of fuzzy-verifiers and honeywords to design and proven
    a secured three factor authentication mechanism based on extended chaotic maps
    utilizing the AVISPA tool [33]. According to S. Lu et al. in [34], their protocol
    allows an authentication mechanisms and produces a session key. It may be upgraded
    to include a meter reading and a fog node. The method is secure because the learning
    parity with a noise problem is NP-complete, which implies it can withstand quantum
    computing attacks. The method is resistant to quantum assaults since it uses hash
    chains and authorized encryption. Furthermore, entities do not need a permanent
    master key and session keys are generated using a hash function. They looked at
    a smart emergency system in which an edge application sends out alert signals
    to pre-determined locations. In [36], U. Chatterjee et al. have considered an
    effective security solution for a smart health operations system involving a cloud
    using an Elliptic Curve Cryptography framework claiming that their framework preserves
    security and privacy features and qualities seen in other frameworks in the same
    context. The overview of the survey carried out on various existing works is shown
    in Table 1. Table 1 Summary of related works carried out Full size table The major
    research gaps identified upon the literature of existing works as follows: 1.
    All the methods do not give privacy to unknowns, nor do they need a pairing procedure
    on the user’s device. Forward privacy cannot be accomplished because the session
    key produced by the protocol mechanism is constant. 2. Most of the existing schemes
    use the tripartite algorithm for key exchange, and they are also extremely sensitive
    to adversarial attacks. 3. The session key generation must be carried at the edge
    device level instead of depending on fog server. 4. Most of the existing mechanisms
    will consume high overhead due to lengthy certificates and are not suitable for
    decentralized computing environment. To address these gaps, we propose a mechanism
    for secure authentication with a key agreement for fog-driven smart device access
    to users, which does not suffer from the drawbacks found in the current schemes.
    3 System model and security considerations The system model of the proposed TAKM-FC
    is introduced here with a related threat model. Then we provide all the possible
    security attacks related to authentication and key management. Finally introducing
    the preliminary background to the proposed scheme. 3.1 System model Figure 1 shows
    the system model of the proposed TAKM-FC approach. The TAKM-FC has five entities:
    Trusted Third Party (TTP), User (Uj), Mobile Device (MDj), Fog Server (FS), and
    Smart Device (SDi). A TTP is a central entity that is responsible for deploying
    the Uj, FS, and SDi. The FS provides services as an intermediary between the Uj
    and SDi. SDi is a collection of sensors or smart nodes under a specific fog server
    that accumulates real-time data and stored in FS which can be accessed by a user
    upon request. The Uj is a user using the mobile device (MDj) that will connect
    to a fog server to obtain data collected by a smart device. Fig. 1 System model
    Full size image The case study considered here is that the smart devices (data
    producers) connected to the fog server will collect real-time data and send it
    to fog server for storage. When any user (data consumer) wishes to access the
    data of the smart device, the user must be granted permission to view and access
    the data. To achieve this requirement, we need to provide a two-way, multi-level
    authentication between the smart device and the user’s mobile device. 3.2 Threat
    model We have considered a threat model based on the Dolev-Yao [37] in the proposed
    work. It is elaborated as: The channel used between the entities is not fully
    encrypted so an adversary can eavesdrop and fetch the sensitive data. The user
    messages are sent through public channels where an adversary can perform unauthorized
    access, update and delete the data. An adversary can easily access and use the
    data from mobile device when it is lost by the owner or stolen by an opponent.
    An adversary can compromise the edge devices and fetch the session keys. An adversary
    can compromise the fog server and fetch the client information and shared keys.
    3.3 Security requirements Because of the security prerequisites in a fog computing
    environment, some of the attacks need to be forestalled in the authentication
    protocol are elaborated as: Forward/Backward secrecy After the session gets over,
    no further messages/instructions will be considered from the respective user.
    Meantime, when a new user joins the existing communication group, the new user
    must not be open to previously communicated messages/instructions. Replay attack
    An unauthorized third party holds the record of blocked data in the computational
    system, wherein an attacker tries to mislead the other authorized user. Impersonation
    attack A successful impersonation attack in which an attacker assumes the legal
    identity of any of the authorized parties in the computing system environment.
    Offline Password guessing attack An unauthorized adversary can make use of previous
    messages transferred and the credentials stored in the mobile device (lost/stolen)
    and guess the unique password. Ephemeral secret leakage attack The secret session
    key of edge devices can be retrieved by the intruder when it is not generated
    based on both long-term (credentials) and short-term (random numbers). Man-in-the-middle
    attack: An attacker acts as a middle man, it receive a messages transmitted from
    an authorized sender, modifies and forwards them to other authorized recipients.
    The adversary may also delete the messages. Insider attack: An authorized user
    of a computing environment or system, tries to misuse and mislead access to the
    authorized system environment. Mobile stole/lost attack When a mobile device is
    lost or stolen by an attacker, he can access all the credentials and other data
    in the local storage of the mobile device. Un-traceability An adversary must not
    be able to track the different activities carried out by any devices in a computing
    environment during communications. User anonymity An attacker must be unaware
    of the real and unique identity of any communicating device in the environment
    while communicating. 3.4 Preliminaries This section discusses the basic preliminary
    mechanisms used in the proposed TAKM-FC scheme. 3.4.1 Cryptographic primitives
    Consider G as a multiplicative subgroup of Zp*, p being the large prime value,
    the identity element could be, f = 1 and g, g being the generator value of G.
    The assumption is made as discrete logarithms in g ∈ G are computationally infeasible.
    The G is the base point among a set of points on the elliptic curve. Consider
    an elliptic curve ECCp(x, y) over a prime Galois field GF(p) with (x,y) are being
    constants. The bivariate polynomial function of degree t in a finite Galois field
    (GF(p) = Zp*) is calculated using Eq. 1. $$F\\left( {x,y} \\right) = \\mathop
    \\sum \\limits_{{m,n = 0}}^{t} \\left( {a_{{m,n}} ~x^{m} } \\right)~y^{n}$$ (1)
    The coefficients for the function are selected from the set GF(p) and are symmetric,
    so F(x,y) = F(y,x) [38]. We have made use of the symmetric polynomial function
    at smart device and fog server for generating the storing the mutual symmetry.
    3.4.2 Fuzzy extractor function The fuzzy extractor function is the most used method
    for handling the user biometric effectively [39]. The fuzzy extractor functions
    include two operations as Generation and Reproduction operations. Generation (Gen)
    It is used to generate the bio-key(α) for a specific user biometric with the help
    of public reproduction parameter (β). The Gen(.) reads the biometric (BIO) as
    input, process and produces the bio-key using Eq. 2. $$(\\alpha , \\beta ) =\\mathrm{
    Gen}(\\mathrm{BIO})$$ (2) Reproduction (Rep) It is the operation used for the
    verification process by extracting or recovering the bio-key based on current
    user biometrics and the same public reproduction parameter. The Rep(.) reads the
    user biometric (BIO′) and reproduction parameter (β), it recovers the bio-key
    using Eq. 3. $$\\alpha \\mathrm{^{\\prime}}=\\mathrm{Rep}(\\mathrm{BIO{^\\prime}},
    \\beta )$$ (3) We have used the fuzzy extractor function for user registration
    and login process in the proposed work. 4 Proposed TAKM-FC scheme The proposed
    TAKM-FC scheme uses a fuzzy extractor function, bitwise XOR operations, a one-way
    hash function, and other cryptographic primitives. The fuzzy extractor function
    has been used for user login and authentication to the mobile device with cryptographic
    primitives. Later the bitwise XOR operations, a one-way hash function, and other
    cryptographic primitives are used for communication between the mobile device
    and smart device through the fog server. The TAKM-FC scheme has three phases:
    Deployment and registration, Secret key management, and Authentication with session
    key generation. Figure 2 shows the overview of the TAKM-FC. Fig. 2 Overview of
    the proposed scheme Full size image Phase 1 The fog server, smart device and user’s
    mobile device must be registered with a trusted third party before they start
    their transactions. Then they can identify each other without the intervention
    of a third party. Phase 2 The smart device and fog server mutually accept the
    channel by generating the common secret key at both ends. Phase 3 The process
    of authentication will begin with a user login to the registered mobile device.
    The request will be sent to the fog server, upon successful authentication the
    request is forwarded to a smart device. The smart device will authenticate and
    generate the session key, then send it to a respective mobile device for successful
    authentication. We have also considered 2 optional phases in the scheme.(1) Password
    Update based on new biometric: When a registered user wishes to change the password,
    it can be done without interacting with TTP based on his biometric and current
    password. (2) New Mobile Device Updating Phase: When a registered user wishes
    to change his/her mobile device or when it is lost, the new mobile device needs
    to be updated with TTP and then with FS. Each phase is discussed in detail in
    the following sections. Table 2 lists the notations used in the description of
    the proposed TAKM-FC scheme. Table 2 Notations and descriptions Full size table
    4.1 Phase 1: deployment and user registration The fog server, smart devices, and
    mobile devices must be deployed by a trusted third party. The user must also register
    using mobile device based on his or her credentials. The deployment process begins
    with the assignment of the appropriate ID and temporary certificates by TTP. 4.1.1
    Deployment of Fog server and smart devices The deployment of fog server and smart
    device is given in Fig. 3. The TTP chooses a unique ID and generates a temporary
    certificate for FS and generate a polynomial bivariate for FS as: \\(F(\\mathrm{TFID},y)={\\sum
    }_{m,n=0}^{t}\\left({a}_{m,n} {(\\mathrm{TFID})}^{m}\\right) {y}^{n}\\) using
    Eq. 1 as discussed in Sect. 3.4.1. Finally the FS is deployed by storing \\(<\\mathrm{TFID},\\mathrm{
    TC}, F(\\mathrm{TFID}, y)>\\) in its storage. Fig. 3 Deployment of fog server
    and smart device Full size image The TTP chooses a unique ID and generates a temporary
    certificate for all SDi, i = 1,2,3,….n based on the current requesting timestamp.
    The TTP generate a polynomial bivariate for SDi as: \\(F(\\mathrm{TSID}i,y)={\\sum
    }_{m,n=0}^{t}\\left({a}_{m,n} {(\\mathrm{TSID}i)}^{m}\\right) {y}^{n}\\) using
    Eq. 1 as discussed in Sect. 3.4.1. Finally the SDi is deployed by storing \\(<\\mathrm{TSID}i
    , \\mathrm{TC}i , F(\\mathrm{TSID}i , y)>\\) in its storage. 4.1.2 User registration
    with mobile device and deployment of mobile device A user at the edge layer must
    register with a mobile device and in turn, the mobile device must be deployed
    by TTP. The user registration with a mobile device is using a set of user credentials
    such as user id with password and user biometric. The biometric is processed using
    a fuzzy extractor function using Eqs. 2 and 3 as discussed in Sect. 3.4. Later
    the mobile device will be deployed by TTP based on the user credentials. Any number
    of users Uj can be registered with multiple mobile devices MDj, where j = 1,2,3….,x
    which can be deployed by the TTP. Figure 4 illustrates the user registration and
    mobile device deployment process. The MDj reads the user credentials as input
    such as user-id, password, biometric and private number (UIDj, PWDj, BIOj, r).
    MDj generate the bio key and hashed ID based on private number then sends a registration
    request to TTP. The TTP in turn generates the temporary certificate and shares
    with MDj, also updates the user information with FS. The MDj will update the related
    credentials and store in its local storage as \\(<\\mathrm{HUID}j*, \\mathrm{TC}j*,
    \\mathrm{PPWD}j, (\\mathrm{TFID}, \\mathrm{TC}*), \\beta j, R >\\) and deletes
    the other original parameters < r, HUIDj, TCj, h(TC) > . The FS updates its memory
    as \\(<\\mathrm{TFID}, \\mathrm{TC}, F(\\mathrm{TFID}, y), \\mathrm{HUID}j, R>\\)
    having the registered user data. Fig. 4 User registration and deployment of mobile
    device Full size image 4.2 Phase 2: secret key management Once the SDi and FS
    are deployed separately, then they must also accept each other for further communications.
    Here, we discuss the mutual acceptance and a secret key (SecK) is generated at
    both ends (SDi and FS) upon acceptance. As shown in Fig. 5 the SDi sends a request
    having its TSID and a current random number to FS. The FS will extract the received
    vector and generate the secret key based on received and stored credentials. FS
    will also produce the temporary variables based on the secret key to be shared
    with SDi for verification. Once the SDi receives the credentials from FS, it will
    generate the secret key by itself. The SDi will also generate the temporary variable
    using its secret key. The agreement between SDi and FS is considered to be successful
    if the generated and received temporary values (T2 = T2*) are same. Then the SecK
    is stored at both ends (SDi and FS) for further communications. Fig. 5 Secret
    key management at fog server and smart device Full size image 4.3 Phase 3: authentication
    with session key generation The process of authentication begins only after the
    entities (SDi, FS, MDj) are deployed successfully. A registered user may request
    for accessing the smart device which requires the mutual authentication. A mutual
    authentication between the MDj and SDi will happen through the FS. Upon the successful
    two-way mutual authentication, a session key will be generated at MDj and SDi.
    The process of authentication with session key generation is given in Fig. 6.
    A registered user Uj will provide the required credentials and login to the MDj
    to reach the fog server. The mobile device validates the Uj based on current user
    credentials like user id, password and bio-metric before forwarding the request
    to FS. Upon successful login, the MDj will prepare the message having the hashed-id
    of user, temporary credentials and send the requesting message to FS. Fig. 6 Authentication
    with session key management Full size image The FS will verify the received request
    based on arrival time and validate the Uj based on the generated and stored credentials.
    Upon successful authentication of Uj, the FS will forward the request to SDi having
    the hashed-id of Uj, temporary credentials. The SDi will verify the received request
    based on arrival time and validate the user based on the generated and received
    credentials. The smart device will prepare the message having the hashed-id of
    SDi, temporary credentials and send the message to MDj. The MDj will verify the
    received request based on arrival time and validate the smart device based on
    the generated and received credentials. The mutual authentication is successful
    when the session keys (SKij and SKji) generated at both ends (SDi and MDj) are
    to be same. 4.4 Password update based on new biometric phase The updating of user
    password is carried out for two reasons when the user may not be willing to use
    the same password and bio-key for a long time or based on security concerns. The
    registered user (Uj) will be updating them in a registered mobile device (MDj)
    without the intervention of TTP or FS. As illustrated in Fig. 7, an Uj has to
    provide his/her original credentials. The MDj will verify the user access by generating
    the protected password and comparing it with the stored one. If the request is
    from authorized Uj, then MDj reads the new credentials from the user and generates
    the protected password based on the current new credentials. Finally, MDj will
    update its database by deleting the original data. Fig. 7 User password update
    in mobile device Full size image 4.5 New mobile device updating phase When the
    mobile device is stolen or lost, then there is a necessity for the User (Uj) to
    register to Fog Server (FS) with a New Mobile Device (nMDj). The new mobile device
    can be updated only by the legitimate user. As shown in Fig. 8 the legitimate
    user will have to provide the credentials, which need to be verified at MDj then
    the request will be forwarded to TTP. The TTP will generate the new certificate
    for the new MDj and deploy the new MDj. The information about the new MDj is also
    shared with FS for further communication. The MDj will update its storage as \\(<\\mathrm{HUID}j*{\\prime},
    \\mathrm{TC}j*{\\prime},\\mathrm{ PPWD}j{\\prime}, (\\mathrm{TFID}, \\mathrm{TC}*),
    \\beta j, R{\\prime} >\\) and deletes the other original parameters. The FS updates
    its memory as \\(<\\mathrm{TFID}, \\mathrm{TC}, F(\\mathrm{TFID}, y), \\mathrm{HUID}j{\\prime},
    R{\\prime}>\\). Fig. 8 New mobile device update with TTP Full size image 5 Security
    analysis Here we discuss about the security analysis of TAKM-FC including the
    formal and informal security analysis. The formal analysis and verification are
    based on the mathematical model suitable for authentication schemes. D. Wang et
    al. in [40] have used the RO (Random Oracle) model and BAN (Burrows–Abadi–Needham)
    [41] for verification of authentication schemes and they have proved as they could
    not be used effectively as they will not consider the syntactical issues in the
    scheme. M. Wazid et al. [42]. and L. Wu et al. [43] have proved that the effective
    verification of an authentication scheme can be achieved based on ROR (Real-Or-Random)
    model. So the robustness of the TAKM-FC is analyzed using ROR model. 5.1 ROR model-based
    formal analysis The security analysis of proposed work is carried out based on
    ROR model. As per the ROR model, the analysis is a game between adversary A and
    challenger C. The elements used in the analysis are as follows. Participants Fog
    Server (FS), Smart Device (SDi), Mobile Device (MDj) are the participants in the
    proposed scheme. \\({I}_{\\mathrm{FS}}^{t1}\\), \\({I}_{\\mathrm{SD}i}^{t2}\\)
    and \\({I}_{\\mathrm{MD}j}^{t3}\\) are the instances of FS, SDi and MDj at t1,
    t2, and t3 which are considered to be as oracles for the analysis. Accepted state
    Messages being exchanged between the participants must be in order and delivered
    at the instance It for current session, then it can be considered as an acceptable
    state. The session identification number (sid) will be assigned to such an accepted
    state. Partnering Two instances (It1, It2) are said to be partners only when they
    both are in an acceptable state and also share the common sid upon mutual acceptance.
    Freshness The instances \\({I}_{\\mathrm{SD}i}^{t2}\\) and \\({I}_{\\mathrm{MD}j}^{t3}\\)
    are considered as fresh when the session key (SK) generated at SDi and MDj is
    kept private. Adversary The ROR for the TAKM-FC is analyzed based on DY model
    [37]. An adversary is one who can have control on the interactions between the
    end participants, an adversary can also update or delete the content in the message.
    The adversary can perform the following operations: Execute (It1, It2): An adversary
    executes this to read the exchanged messages between the end participants FS,
    SDi and MDj. This is to apply the eavesdropping attack in analysis. Send (It,
    M): An adversary can send a message M to the end participant at active instance
    (It) and expect a reply from it. This is to apply the reply attack in analysis.
    Reveal (It): An adversary tries to capture the session key (SK) generated at SDi
    and MDj at instance It. Test(It): An adversary runs the test queries for multiple
    times to verify whether the session key shared is real or random. Random Oracle:
    The analysis considers a one-way hash function h(.) as a random oracle. The end
    participants can and also an adversary can access the h(.). So an adversary can
    also create the hash like genuine participants and use it. Theorem At a polynomial
    time Pt, adversary A attempts to steal the secret session key (SK) established
    between SDi and MDj. The advantage of adversary A in compromise and breaking the
    security of TAKM-FC by fetching the session key (SK) can be estimated in Eq. 4.
    $${\\mathrm{Adv}}_{A}^{\\mathrm{TAKM}-\\mathrm{FC}} \\left(Pt\\right)\\le \\left(\\frac{{qh}^{2}}{\\left|\\mathrm{Hash}\\right|}
    \\right)+(2. {\\mathrm{Adv}}_{A}^{\\mathrm{ECDDHP}}\\left(Pt\\right))$$ (4) where
    qh is number of hash queries, |Hash| is the range space of (.) and \\({\\mathrm{Adv}}_{A}^{\\mathrm{TAKM}-\\mathrm{FC}}\\)
    is the advantage of A in breaking the ECDDHP. Proof Series of games are used between
    challenger C and adversary A to prove the theorem [42, 43]. We have considered
    three games, Gamei (i = 0,1,2), the advantage of A winning the game is given in
    Eq. 5. $${\\mathrm{Adv}}_{A, {\\mathrm{Game}}_{i}}^{\\mathrm{TAKM}-\\mathrm{FC}}=\\mathrm{Pr}({\\mathrm{Succ}}_{{\\mathrm{Game}}_{i}}^{A})$$
    (5) Here Succ is the event when A tries to guess the bit c in the Gamei. Each
    game is discussed in detail as follows: \\({Game}_{0}^{A}\\) Adversary A launches
    an attack on the TAKM-FC by selecting a random bit b before the beginning of Game0A
    based on which the security definition is given in Eq. 6. $${\\mathrm{Adv}}_{A}^{\\mathrm{TAKM}-\\mathrm{FC}}\\left(Pt\\right)\\le
    |2 {\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{0}}^{\\mathrm{TAKM}-\\mathrm{FC}}-1|$$
    (6) \\({Game}_{1}^{A}\\) Adversary performs the eavesdropping attack by using
    execute and test queries. An adversary executes the queries to retrieve the session
    key and verify whether it is real or random. The session key being generated as
    \\(\\mathrm{SK}ji = h(h(\\mathrm{SR}||h(\\mathrm{TC}||ts3)||h(\\mathrm{HUID}j))||h(\\mathrm{TC}i||n5)||ts5)\\)
    is based on the credentials exchanged between the participants during the authentication
    phase. The messages exchanged in authentication phase of TAKM-FC are < HUIDj′,
    N3, T3, T4, ts3 > , < HUIDj ∗ , HSIDi ∗ , T5, T6, N4, ts4 > , and < HSIDi ∗  ∗ ,
    T7, T8, N4, ts5 > which has short term credentials (SR, ts3, n5, ts5) and long-term
    credentials (TC, HUIDj, TCi) are protected using a collision resistant one-way
    hash function. So even after the adversary fetches the messages being exchanged,
    it is difficult to find the session key and probability of winning the Game1 is
    not increased. So Game1 is not different than Game0 as shown in Eq. 7. $${\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{1}}^{\\mathrm{TAKM}-\\mathrm{FC}}=
    {\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{0}}^{\\mathrm{TAKM}-\\mathrm{FC}}$$ (7) \\({Game}_{2}^{A}\\)
    Adversary uses send and random oracle queries and try to change the messages being
    exchanged during login and authentication phase. The messages Msg1, Msg2 and Msg3
    are secured through one-way hash function and they are employed using random number
    at current timestamps. So it is difficult for adversary to steal and change the
    content of messages using send and random oracles as shown in Eq. 8. $$|{\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{1}}^{\\mathrm{TAKM}-\\mathrm{FC}}-
    {\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{2}}^{\\mathrm{TAKM}-\\mathrm{FC}}|\\le \\left(\\frac{{qh}^{2}}{2.\\left|\\mathrm{Hash}\\right|}
    \\right)$$ (8) An adversary try to break the security of TAKM-FC by simulating
    the hash queries and solve ECDDHP. Adversary A must be aware about all the credentials
    required to generate the session key, but those are highly secured using one-way
    hash. So adversary must solve ECDDHP to resolve the secured session key that is
    impossible. Generation of hash collision and solving the ECDDHP to break the security
    of TAKM-FC is defined in Eq. 9: $$|{\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{1}}^{\\mathrm{TAKM}-\\mathrm{FC}}-
    {\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{2}}^{\\mathrm{TAKM}-\\mathrm{FC}}|\\le \\left(\\frac{{qh}^{2}}{2.\\left|\\mathrm{Hash}\\right|}
    \\right)+ {\\mathrm{Adv}}_{A}^{\\mathrm{ECDDHP}}\\left(Pt\\right)$$ (9) Adversary
    executes the game for guessing attack as in Eq. 10. $${\\mathrm{Adv}}_{A,{\\mathrm{Game}}_{2}}^{\\mathrm{TAKM}-\\mathrm{FC}}=\\left(\\frac{1}{2}
    \\right)$$ (10) e application of triangular inequality based on previous equations
    lead to the following derivation: \\(\\begin{aligned} \\left( \\frac{1}{2} \\right).{\\text{Adv}}_{A}^{{{\\text{TAKM}}
    - {\\text{FC}}}} \\left( {Pt} \\right) = & |{\\text{Adv}}_{{A,{\\text{Game}}_{0}
    }}^{{{\\text{TAKM}} - {\\text{FC}}}} - \\left( \\frac{1}{2} \\right)\\left| {
    = |{\\text{Adv}}_{{A,{\\text{Game}}_{0} }}^{{{\\text{TAKM}} - {\\text{FC}}}} -
    {\\text{Adv}}_{{A,{\\text{Game}}_{2} }}^{{{\\text{TAKM}} - {\\text{FC}}}} } \\right|
    \\\\ = & |{\\text{Adv}}_{{A,{\\text{Game}}_{1} }}^{{{\\text{TAKM}} - {\\text{FC}}}}
    - {\\text{Adv}}_{{A,{\\text{Game}}_{2} }}^{{{\\text{TAKM}} - {\\text{FC}}}} |
    = \\left( {\\frac{{qh^{2} }}{{2.\\left| {{\\text{Hash}}} \\right|}} } \\right)
    + {\\text{Adv}}_{A}^{{{\\text{ECDDHP}}}} \\left( {Pt} \\right) \\\\ \\end{aligned}\\).
    Finally, multiply by factor of 2 on both sides fetching Eq. 11: $${\\mathrm{Adv}}_{A}^{\\mathrm{TAKM}-\\mathrm{FC}}
    \\left(Pt\\right)\\le \\left(\\frac{{qh}^{2}}{\\left|\\mathrm{Hash}\\right|} \\right)+(2.
    {\\mathrm{Adv}}_{A}^{\\mathrm{ECDDHP}}\\left(Pt\\right))$$ (11) 5.2 Informal security
    analysis The informal security analysis is made by deliberating some known attacks
    which need to be resisted. The different security features considered are C1:
    Replay attack, C2: Offline Password guessing attack, C3: Impersonation attack,
    C4: Ephemeral secret leakage attack, C5: Man-in-the-middle attack, C6: Mobile
    stolen/lost attack, C7: User anonymity, C8: un-traceability attack, C9: Forward
    Secrecy attack, C10: Insider attack. Table 3 presents a comparison of all considered
    security features in the proposed scheme with other related schemes [33:CredAuth][34:QuantAuth][35:LAMAS][36:ECCAuth]
    and it demonstrates that the proposed mechanism satisfies all the said security
    concerns. Table 3 Comparison of security features Full size table Replay attack
    We have used two key points to resist replay attacks. They are random numbers
    with the current timestamp in each sent data segment. As a result, obtaining the
    random numbers n1 to n5 and ts1 to ts5 from the public channel is extremely difficult
    for the opponent. As a result, the adversary will be unable to repeat the messages
    without knowing the random number; hence, the suggested technique will be able
    to counteract the replay attack. User Impersonation attack The adversary may try
    to log in and send the message to FS acting on behalf of authorized user Uj. An
    adversary may try to create the message < HUIDj′, N3, T3, T4, ts3 > to send to
    FS. But to create this message an adversary must have the original details as
    user biometric based which itself you generate all other credentials. Also, an
    adversary must know the private value, r to generate R based on a random number.
    Without all these details an adversary cannot create the expected message, so
    the proposed scheme eliminates the user impersonation attack. Fog Server Impersonation
    attack An adversary may try to send a message < HUIDj*, HSIDi*, T5, T6, N4, ts4 > to
    a smart device on behalf of the fog server. The adversary may also assume some
    random numbers and current timestamps, which are used to generate the temporary
    variables random secret number of the message. But it also needs the hashed identifier
    of the user and smart device, and importantly the secret key agreed by both FS
    and SDi, which is hard to find or assume by an adversary. So without these, an
    adversary cannot create a valid message to send to SDi. So the proposed scheme
    eliminates the fog server impersonation attack. Smart Device Impersonation attack
    An adversary may try to a message < HSIDi**, T7, T8, N4, ts5 > to the user’s mobile
    device on behalf of the smart device. An adversary may also assume some random
    numbers and current timestamps, which are used to generate the temporary variables
    random secret number of the message. But it also needs the hashed identifier of
    the user and smart device, and importantly the secret key agreed by both FS and
    SDi which is hard to find or assume by an adversary. So without these, an adversary
    cannot create a valid message to send to MDj. So the proposed scheme eliminates
    the smart device impersonation attack. Offline Password Guessing attack Once an
    adversary has stolen the mobile device MDj of registered authorized user Uj, he
    can retrieve all the credentials stored in the device using a power analysis attack.
    That is he can access < HUIDj*, T Cj*, P PW Dj,(T F ID, T C*), βj, R > but he
    cannot know or try to guess the deleted parameters. Without r, there is no use
    of R as R is generated by TTP using r. For an adversary it is hard to guess the
    bio-key used further, he may also fetch the bio-key but it is hard to guess the
    original user id, UID and PWD, which are not stored. So in the proposed scheme,
    it is difficult for adversaries to guess the password. Ephemeral secret leakage
    attack If an adversary thinks of obtaining the session key (SK), first he needs
    to obtain the ephemeral secret key (SKij or SKji) shared between FS and SDi. For
    finding the secret key he must know the random numbers (n1–n5) used. If we assume
    the adversary can obtain these random numbers by chance, even then the adversary
    must find the hashed IDs used. But to find the hashed id’s, he must get the original
    id of Uj, SDi, and FS, which is impossible to get from the public channel. This
    indicates that the suggested method may withstand an ephemeral secret leakage
    attack. Man-in-the-middle attack Suppose an adversary wants to act in between
    the Uj and FS. An adversary may receive the message from a user as < HUIDj′, N3,
    T3, T4, ts3 > , but he cannot modify his identity as if the user (HUIDj) because
    to do so an adversary must have the private value of the user r. Also, the HUIDj,
    in turn, will be updated as HUIDj’ by using the bio-key αj. So the value r and
    αj are not traceable and modifiable by any middle person. So that any message
    sent to a middle person other than the actual one, the FS can easily detect and
    terminate the connection. So the proposed scheme overcomes the man-in-the-middle
    attack. User anonymity and un-traceability attack: In the login and authentication
    phases, we utilized random numbers (n1–n5) and timestamps (ts1 to ts5), both of
    which change from session to session. As a result, the adversary is unable to
    deduce the true identities of Uj, SDi, and FS. Because of this, the suggested
    system may be adaptable to protect the user’s privacy. Furthermore, the identity
    of fog servers and users is obscured by the random number, which is also unique
    between sessions, making this method safe against untraceability attacks. Mobile
    device stolen/lost attack As explained in the password guessing attack, when the
    MDj is stolen or lost, an adversary can fetch all the credentials stored in it.
    But it is hard to guess the actual PWDj, UIDj, and BIOj. Unless an adversary knows
    these parameters, he cannot initiate any communication with FS or SDi. So no use
    even an adversary collects the MDj or no worries for a registered user for further
    communications as the registered user can re-initiate using a new mobile device
    as discussed in 4.6. 5.3 Formal security verification based on ProVerif The ProVerif
    [44] is used to perform the security analysis of cryptographic protocols. We used
    this tool to assess the security characteristics of authentication mechanism in
    a formal way. ProVerif is an automated cryptographic protocol validator that employs
    Horn clauses to express the protocol. It is commonly used for verification of
    the exchange of security keys and achieve mutual authentication, which includes
    both asymmetric and symmetric encryption methods, also hash functions, digital
    signature algorithms and other methods. The security analysis is made based on
    the DY model [37] as enumerated in the Sect. 3.2. To analyze the security of the
    proposed scheme, we have created two channels (private and public). A private
    channel is used at the time of the registration phase between the entities and
    TTP. A public channel denotes interface media between the entities at the authentication
    and key generation phase. Multiple events have been considered at authenticating
    entities, to generate the secret key and session key. The overall results are
    shown in Fig. 9. As a consequence of the aforementioned verification result, we
    may infer that the proposed mechanism accomplishes an effective mutual authentication
    and also maintain the secrecy of the session key, so that an adversary A is unable
    to intrude and break the process or get the session key. Fig. 9 Simulation results
    using ProVerif Full size image 6 Performance evaluation Here, we present the overall
    performance analysis of the TAKM-FC scheme. First, calculate the overhead metrics
    of TAKM-FC and then analyze by comparing the results of TAKM-FC with existing
    schemes [33:CredAuth][34:QuantAuth][35:LAMAS][36:ECCAuth]. These papers have been
    considered for comparison as they have proposed authentication using different
    cryptographic primitives such as credential-based, ECC-based and quantum-based
    mechanisms. Later we present the implementation and result analysis using iFogSim.
    The experimentation of the TAKM-FC is carried out in the system having a configuration
    as 2.4GHz CPU, Intel i7, 16 GB RAM and 1 TB hard disk. 6.1 Overhead analysis of
    the proposed TAKM-FC scheme The overhead analysis of the proposed scheme is presented
    based on computation, communication and storage cost. Later the result analysis
    is carried out by comparing with the existing schemes. The cryptographic operations
    used in the scheme have been implemented using Python 3.8 using the respective
    libraries such as NumPy and pycrypto to find the execution time of each function.
    The experiment is conducted for 10 times and recorded the average time consumed
    in milliseconds. Table 4 shows the time consumption for each cryptographic primitive.
    Table 4 Computation cost parameters Full size table 6.1.1 Computation cost It
    is the total amount of time consumed to execute a task in the device, the device
    can be an edge device, fog server, or cloud server. Any task in the device is
    compiled with a set of cryptographic primitives and other mathematical operations
    to attain the results. Among the phases of TAKM-FC, the authentication with key
    management phase is considered for computation cost analysis by leaving other
    phases as they are not performed regularly. The authentication with key management
    phase accumulated with various functional primitives such as concatenation, XOR
    operation, fuzzy extractor and one-way hash function. Here we have considered
    only the fuzzy extractor and one-way hash function for computation cost evaluation,
    as other primitives consume very least processing time. Considering the execution
    time of each primitive as shown in Table 4, a computation cost of the proposed
    scheme is computed as: 1Tfe + 33Th = 0.0171 + 33(0.0026) = 0.1029 s. The analysis
    results show that the TAKM-FC has achieved 18.46%, 21.74%, 11.90% and 13.38% lesser
    computation overhead compared with existing schemes [33,34,35,36], respectively.
    The comparison is shown in Fig. 10. The analysis of computation overhead is carried
    out by varying the number of edge devices from 5 to 30 and results are recorded.
    Figure 11 shows the comparison of computation overhead. The computation cost overhead
    is minimal when compared to other schemes, even after it increases with an increase
    in the number of edge devices. Fig. 10 Computation cost comparison Full size image
    Fig. 11 Analysis of computation overhead Full size image 6.1.2 Communication cost
    It is the number of bits consumed in the process of communication between the
    entities. In the authentication with key management process, we share three messages
    between MDj, FS, and SDi. Parameters considered for evaluating the communication
    overhead are shown in Table 5. The messages are evaluated with overhead as, the
    message from MDj to FS: < HUIDj′, N 3, T 3, T 4, ts3 >  =  < 160 + 320 + 160 + 160 + 32 >  = 832
    bits. The message from FS to SDi: < HUIDj, HSIDi, T 5, T 6, N 4, ts4 >  =  < 160 + 160 + 160 + 160 + 320 + 32 >  = 992
    bits. The message from SDi to MDj: < HSIDi ∗  ∗ , T 7, T 8, N 4, ts5 >  =  < 160 + 160 + 160 + 320 + 32 >  = 832
    bits. So the total number of bits shared between the entities are: 832 + 992 + 832 = 2656
    bits. Table 5 Communication cost parameters Full size table The results show that
    the proposed TAKM-FC achieved 19.29%, 11.17%, 8.79% and 15.44% lesser communication
    overhead compared with existing schemes [33,34,35,36], respectively. The comparison
    is shown in Fig. 12 and shows as it consumes less communication overhead between
    edge devices and fog server. The analysis of communication overhead is carried
    out by varying the number of edge devices from 5 to 30 and results are recorded.
    Figure 13 shows the comparison of communication overhead. The communication cost
    overhead is minimal when compared to other schemes, even after it increases with
    an increase in the number of edge devices. Fig. 12 Communication cost comparison
    Full size image Fig. 13 Analysis of communication overhead Full size image 6.1.3
    Storage cost It is considered to be the total bits stored in the local storage
    of any device consisting of generated or received credentials useful for communication.
    Here, we have considered the storage of the smart device, mobile device and fog
    server for analysis. The size of parameters in each device is based on Table 5.
    The MDj will be storing the credentials as \\(<\\mathrm{HUID}j, \\mathrm{TC}j,
    \\mathrm{PPWD}j, (\\mathrm{TFID}, \\mathrm{TC}), \\beta j, R>\\) at the registration
    phase and SKij at the authentication phase. So storage bits at mobile device is < 160 + 160 + 128 + 160 + 160 + 320 >  = 1088 + 160 = 1248
    bits. The SDi will be storing < TSIDi, HSIDi, TCi, F (TSIDi, y), SecK > at the
    registration phase and SKji at the authentication phase. So total storage bits
    in the smart device is < 160 + 160 + 160 + 160 + 160 >  = 800 + 160 = 960 bits.
    The FS stores < TFID, HFID, TC, F (TFID, y), HUIDj, R, SecKi >  =  < 160 + 160 + 160 + 160 + 320 + 160 >  = 1120
    bits. The storage cost of the TAKM-FC is 3328 bits and compared with other schemes
    as shown in Fig. 14. The analysis results show that TAKM-FC has achieved 13.87%,
    10.44%, 6.62% and 7.91% lesser storage overhead compared with existing schemes
    [33,34,35,36], respectively. It is proven to be the mobile device, smart device
    and fog server in the proposed scheme consume very less storage. The analysis
    of storage overhead is carried out by varying the number of edge devices from
    5 to 30 and results are recorded. The comparison of computation overhead is shown
    in Fig. 15. Fig. 14 Storage cost comparison Full size image Fig. 15 Analysis of
    storage overhead Full size image 6.2 Performance evaluation using iFogSim The
    proposed TAKM-FC is implemented in the iFogSim simulator [45] for evaluating the
    performance of the work. The performance metrics considered for analysis are throughput,
    end-to-end delay, rate of packet loss, energy consumption and network usage. The
    topology in the simulation is created by having a cloud server and one fog server
    with set of mobile devices and a set of smart devices. The number of smart devices
    and a number of user mobile devices are considered in three scenarios having 20,
    30 and 40 smart devices with variable user mobile devices from 1 to 5. The simulation
    parameters used in iFogSim are given in Table 6. Table 6 Parameter definitions
    Full size table For all the considered scenarios, we deliberate the simulation
    performance metrics: throughput, end-to-end delay, rate of packet loss, energy
    consumption and network usage. Throughput Is the number of units of work processed
    or a number of requests made per second from the concurrent users who are using
    the same application system at the same time. It is found using Eq. 12. $$\\mathrm{Throughput}
    (bps)=\\frac{\\mathrm{Total\\; number\\; of \\;packets }\\times \\mathrm{ size\\;
    of \\;each\\; packet }(\\mathrm{bits})}{\\mathrm{Total\\; time\\; taken}(\\mathrm{s})}$$
    (12) It is observed in Fig. 16 that an increase in the number of devices in communication
    increases the total number of messages being exchanged and increases the throughput.
    It was observed that there is a variation in throughput of about 11.26–11.80%
    for 1 user with 10–30 smart devices, 7.0–6.2% for 2 users with 10–30 smart devices,
    6.14–6.43% for 3 users with 10–30 smart devices, 12.36–11.00% for 4 users with
    10–30 smart devices and 20.69–17.22% for 5 users with 10–30 smart devices. Fig.
    16 Throughput versus number of users Full size image End-to-End Delay It is an
    average time taken for communications to reach their destination from the respective
    source node. It is calculated using Eq. 13. $$\\mathrm{Delay}=\\frac{\\mathrm{Packet
    \\;Arrival\\; Time}-\\mathrm{Packet\\; Sent \\;Time}}{\\mathrm{Total\\; messages}}$$
    (13) This is required to build a session key between the communicating parties
    and is crucial to monitor for authentication with the key management method. For
    a more efficient authentication method with a smaller number of users, a delay
    should have a lower value. As shown in Fig. 17, the value of delay increases when
    increase in the number of users, as it turn they increases the number of messages
    exchanged. It was observed that there is an variation in delay of about 2.7–5.12%
    for 1 user with 10–30 smart devices, 16.6–18.6% for 2 users with 10–30 smart devices,
    22.3–18.2% for 3 users with 10–30 smart devices, 20.7–17.8% for 4 users with 10
    to 30 smart devices and 20.2–16.4% for 5 users with 10–30 smart devices. Fig.
    17 End-to-end delay versus number of users Full size image Packet Loss It is the
    number of packets that get dropped while two devices were communicating. It is
    calculated using an Eq. 14. $$\\mathrm{Packet \\;Loss Rate}=\\frac{\\mathrm{Total\\;
    packets \\;sent}-\\mathrm{ Total\\; packets\\; recived}}{\\mathrm{Total\\; packets
    \\;sent}}$$ (14) As shown in Fig. 18, the packet loss rate rises when the increase
    in the number of users, owing to the fact that increase in congestion as more
    messages are exchanged. It was observed that there is a variation in packet loss
    rate of about 11.11–50.0% for 1 user with 10–30 smart devices, 38.0–52.5% for
    2 users with 10–30 smart devices, 45.9–49.33% for 3 users with 10–30 smart devices,
    60.5–70.2% for 4 users with 10–30 smart devices and 72.8–75.7% for 5 users with
    10–30 smart devices. Fig. 18 Rate of packet loss versus number of users Full size
    image Energy Consumption It is the total energy utilized based on the power supply
    to the entire environment including cloud, fog, and edge devices. As shown in
    Fig. 19, the energy consumption increases when increase in the number of smart
    and mobile devices. It was observed that there is a variation in energy consumption
    of about 33.8–63.6% for 1 user with 10–30 smart devices, 34.3–63.8% for 2 users
    with 10–30 smart devices, 34.7–63.9% for 3 users with 10–30 smart devices, 35.1–64.08%
    for 4 users with 10–30 smart devices and 35.7–64.3% for 5 users with 10–30 smart
    devices. Fig. 19 Overall energy consumption Full size image Network Usage Network
    usage is the utilization of nodes and the occurrence of traffic while communicating
    between the nodes in the network. It is depicted in Fig. 20. The traffic at the
    edge layer increases when increase in the number of smart and mobile devices,
    in turn the traffic at fog server also increases relatively. It was observed that
    there is a variation in network usage of about 84.4–42.5% for 1 user with 10–30
    smart devices, 78.2–33.09% for 2 users with 10–30 smart devices, 80.4–39.8% for
    3 users with 10–30 smart devices, 81.04–64.7% for 4 users with 10–30 smart devices
    and 81.1% to 67.9% for 5 users with 10 to 30 smart devices. Fig. 20 Network usage
    Full size image 7 Conclusions and future work The fog computing environment is
    more prone to security issues when the number of edge devices is increasing in
    digital communications. The insecurity activities can be controlled by providing
    multi-level and mutual authentication between the edge devices. This paper proposes
    a multi-level and two-way authentication with effective key management between
    the smart device and mobile device. A final session key is generated at both edge
    devices after the successful authentication between them. The proposed work made
    use of fuzzy extractor function for efficient user login to the mobile device,
    cryptographic primitives are used during key management and authentication. Security
    analysis is carried out considering the mathematical ROR model and theoretical
    information analysis and also TAKM-FC is verified for some of the known attacks
    by using the security verification tool ProVerif. We have also illustrated how
    the proposed scheme is efficient compared to other schemes by discussing the computation,
    communication, and storage cost overhead. The proposed scheme has been implemented
    in iFogSim and verified throughput, end-to-end delay, packet loss, energy consumption,
    and network usage. In future, the work will be extended with blockchain technology
    to increase the efficiency in key management between edge devices. The blockchain
    provides a distributed platform to store and manage the key pair that achieves
    anonymity, authenticity, nonframeability, and unforgeability. Also, the semantic-based
    method of access control extended to the next level of security by having the
    history of the user''s request and by designing fine-grained policies based on
    heterogeneous devices in a real social network is needed to study other factors
    such as trust. Data availability Data sharing is not applicable to this article
    as no datasets were generated during the current study. References Namasudra S,
    Roy P (2018) PpBAC. J Organ End User Comput 30:14–31. https://doi.org/10.4018/joeuc.2018100102
    Article   Google Scholar   Xiong H, Wang Y, Li W, Chen C-M (2019) Flexible, efficient,
    and secure access delegation in cloud computing. ACM Trans Manag Inf Syst 10:1–20.
    https://doi.org/10.1145/3318212 Article   Google Scholar   Bonomi F, Milito R,
    Zhu J, Addepalli S (2012) Fog computing and its role in the internet of things.
    In: Proceedings of the first edition of the MCC workshop on mobile cloud computing-MCC’12.
    https://doi.org/10.1145/2342509.2342513 Singh SP, Nayyar A, Kumar R (2019) Sharma
    A, Fog computing: from architecture to edge computing and big data processing.
    J Supercomput 75:2070–2105. https://doi.org/10.1007/s11227-018-2701-2 Article   Google
    Scholar   Manvi SS, Gowda NC (2019) Trust management in fog computing. Appl Integr
    Tech Methods Distrib Syst Technol. https://doi.org/10.4018/978-1-5225-8295-3.ch002
    Article   Google Scholar   Murtaza MH, Tahir H, Tahir S, Alizai ZA, Riaz Q, Hussain
    M (2022) A portable hardware security module and cryptographic key generator.
    J Inf Secur Appl 70:103332. https://doi.org/10.1016/j.jisa.2022.103332 Article   Google
    Scholar   Mehdi M, Ajani MT, Tahir H, Tahir S, Alizai Z, Khan F, Riaz Q, Hussain
    M (2021) PUF-based key generation scheme for secure group communication using
    MEMS. Electronics 10:1691. https://doi.org/10.3390/electronics10141691 Article   Google
    Scholar   Stojmenovic I, Wen S (2014) The fog computing paradigm scenarios and
    security issues. Ann Comput Sci Inf Syst. https://doi.org/10.15439/2014f503 Article   Google
    Scholar   Kaliya N, Pawar D (2023) Unboxing fog security: a review of fog security
    and authentication mechanisms. Computing. https://doi.org/10.1007/s00607-023-01208-3
    Article   Google Scholar   Al-Mekhlafi ZG, Al-Shareeda MA, Manickam S, Mohammed
    BA, Alreshidi A, Alazmi M, Alshudukhi JS, Alsaffar M, Rassem TH (2023) Efficient
    authentication scheme for 5G-enabled vehicular networks using fog computing. Sensors
    23:3543. https://doi.org/10.3390/s23073543 Article   ADS   PubMed   PubMed Central   Google
    Scholar   Luqman M, Faridi AR (2023) Authentication of fog-assisted IoT networks
    using advanced encryption credibility approach with modified Diffie–Hellman encryption.
    Concurr Comput. https://doi.org/10.1002/cpe.7742 Article   Google Scholar   Saravanakumar
    S, Saravanan T (2023) Secure personal authentication in fog devices via multimodal
    rank-level fusion. Concurr Comput. https://doi.org/10.1002/cpe.7673 Article   Google
    Scholar   Loffi L, Westphall CM, Grudtner LD, Westphall CB (2021) Mutual authentication
    with multi-factor in IoT-Fog-Cloud environment. J Netw Comput Appl 176:102932.
    https://doi.org/10.1016/j.jnca.2020.102932 Article   Google Scholar   Mo J, Hu
    Z, Chen H, Shen W (2019) An efficient and provably secure anonymous user authentication
    and key agreement for mobile cloud computing. Wirel Commun Mob Comput 2019:1–12.
    https://doi.org/10.1155/2019/4520685 Article   Google Scholar   Kumar P, Braeken
    A, Gurtov A, Iinatti J, Ha PH (2017) Anonymous secure framework in connected smart
    home environments. IEEE Trans Inform Forensic Secur 12:968–979. https://doi.org/10.1109/tifs.2016.2647225
    Article   Google Scholar   Braeken A, Kumar P, Liyanage M, Hue TTK (2017) An efficient
    anonymous authentication protocol in multiple server communication networks (EAAM).
    J Supercomput 74:1695–1714. https://doi.org/10.1007/s11227-017-2190-8 Article   Google
    Scholar   Odelu V, Das AK, Wazid M, Conti M (2016) Provably Secure authenticated
    key agreement scheme for smart grid. IEEE Trans Smart Grid. https://doi.org/10.1109/tsg.2016.2602282
    Article   Google Scholar   Guo J, Du Y, Zhang Y, Li M (2021) A provably secure
    ECC-based access and handover authentication protocol for space information networks.
    J Netw Comput Appl 193:103183. https://doi.org/10.1016/j.jnca.2021.103183 Article   Google
    Scholar   Al Hamid HA, Rahman SMM, Hossain MS, Almogren A, Alamri A (2017) A security
    model for preserving the privacy of medical big data in a healthcare cloud using
    a fog computing facility with pairing-based cryptography. IEEE Access 5:22313–22328.
    https://doi.org/10.1109/access.2017.2757844 Article   Google Scholar   Ke C, Zhu
    Z, Xiao F, Huang Z, Meng Y (2022) SDN-based privacy and functional authentication
    scheme for fog nodes of smart healthcare. IEEE Internet Things J. https://doi.org/10.1109/jiot.2022.3161935
    Article   Google Scholar   Wu TY, Lee Z, Yang L (2021) Provably secure authentication
    key exchange scheme using fog nodes in vehicular ad hoc networks. J Supercomput
    77:6992–7020. https://doi.org/10.1007/s11227-020-03548-9 Article   Google Scholar   Chen
    C-M, Huang Y, Wang K-H, Kumari S, Wu M-E (2020) A secure authenticated and key
    exchange scheme for fog computing. Enterprise Information Systems 15:1200–1215.
    https://doi.org/10.1080/17517575.2020.1712746 Article   ADS   Google Scholar   Tiwari
    D, Chaturvedi GK, Gangadharan GR (2019) ACDAS: Authenticated controlled data access
    and sharing scheme for cloud storage. Int J Commun Syst 32:e4072. https://doi.org/10.1002/dac.4072
    Article   Google Scholar   Akram MA, Ghaffar Z, Mahmood K, Kumari S, Agarwal K,
    Chen C-M (2020) An anonymous authenticated key-agreement scheme for multi-server
    infrastructure. Hum Cent Comput Inf Sci. https://doi.org/10.1186/s13673-020-00227-9
    Article   Google Scholar   Liu C-L, Tsai W-J, Chang T-Y, Liu T-M (2018) Ephemeral-secret-leakage
    secure ID based three-party authenticated key agreement protocol for mobile distributed
    computing environments. Symmetry 10:84. https://doi.org/10.3390/sym10040084 Article   ADS   Google
    Scholar   Patonico S, Braeken A, Steenhaut K (2019) Identity-based and anonymous
    key agreement protocol for fog computing resistant in the Canetti–Krawczyk security
    model. Wireless Netw. https://doi.org/10.1007/s11276-019-02084-6 Article   Google
    Scholar   Wu TY, Meng Q, Yang L, Guo X, Kumari S (2022) A provably secure lightweight
    authentication protocol in mobile edge computing environments. J Supercomput 78:13893–13914.
    https://doi.org/10.1007/s11227-022-04411-9 Article   Google Scholar   Alsahlani
    AYF, Popa A (2021) LMAAS-IoT: Lightweight multi-factor authentication and authorization
    scheme for real-time data access in IoT cloud-based environment. J Netw Comput
    Appl 192:103177. https://doi.org/10.1016/j.jnca.2021.103177 Article   Google Scholar   Wazid
    M, Das AK, Kumar N, Vasilakos AV (2019) Design of secure key management and user
    authentication scheme for fog computing services. Futur Gener Comput Syst 91:475–492.
    https://doi.org/10.1016/j.future.2018.09.017 Article   Google Scholar   Yadav
    AK, Braeken A, Misra M (2023) Symmetric key-based authentication and key agreement
    scheme resistant against semi-trusted third party for fog and dew computing. J
    Supercomput. https://doi.org/10.1007/s11227-023-05064-y Article   Google Scholar   Yan
    X, Ma M (2021) A lightweight and secure handover authentication scheme for 5G
    network using neighbour base stations. J Netw Comput Appl 193:103204. https://doi.org/10.1016/j.jnca.2021.103204
    Article   Google Scholar   Wazid M, Bagga P, Das AK, Shetty S, Rodrigues JJPC,
    Park Y (2019) AKMIoV: authenticated key management protocol in fog computing-based
    internet of vehicles deployment. IEEE Internet Things J 6:8804–8817. https://doi.org/10.1109/jiot.2019.2923611
    Article   Google Scholar   Ali HS, Sridevi R (2022) Credential-based authentication
    mechanism for IoT devices in fog-cloud computing. ICT Anal Appl. https://doi.org/10.1007/978-981-16-5655-2_30
    Article   Google Scholar   Lu S, Li X (2021) Quantum-resistant lightweight authentication
    and key agreement protocol for fog-based microgrids. IEEE Access 9:27588–27600.
    https://doi.org/10.1109/access.2021.3058180 Article   Google Scholar   Hamada
    M, Salem SA, Salem FM (2022) LAMAS: Lightweight anonymous mutual authentication
    scheme for securing fog computing environments. Ain Shams Eng J 13:101752. https://doi.org/10.1016/j.asej.2022.101752
    Article   Google Scholar   Chatterjee U, Ray S, Khan MK, Dasgupta M, Chen C-M
    (2022) An ECC based lightweight remote user authentication and key management
    scheme for IoT communication in context of fog computing. Computing 104:1359–1395.
    https://doi.org/10.1007/s00607-022-01055-8 Article   Google Scholar   Dolev D,
    Yao A (1983) On the security of public key protocols. IEEE Trans Inform Theory
    29:198–208. https://doi.org/10.1109/tit.1983.1056650 Article   MathSciNet   Google
    Scholar   Das AK, Sengupta I (2008) An effective group-based key establishment
    scheme for large-scale wireless sensor networks using bivariate polynomials. In:
    2008 3rd International Conference on Communication Systems Software and Middleware
    and Workshops (COMSWARE ’08). https://doi.org/10.1109/comswa.2008.4554370 Dodis
    Y, Reyzin L, Smith A (2004) Fuzzy extractors: how to generate strong keys from
    biometrics and other noisy data. In: Proceedings of the advances in cryptology
    (Eurocrypt’04), LNCS, vol 3027 Wang D, He D, Wang P, Chu C-H (2015) Anonymous
    two-factor authentication in distributed systems: certain goals are beyond attainment.
    IEEE Trans Dependable Secure Comput 12:428–442. https://doi.org/10.1109/tdsc.2014.2355850
    Article   Google Scholar   Burrows M, Abadi M, Needham R (1990) A logic of authentication.
    ACM Trans Comput Syst 8:18–36. https://doi.org/10.1145/77648.77649 Article   Google
    Scholar   Wazid M, Das AK, Odelu V, Kumar N, Susilo W (2020) Secure remote user
    authenticated key establishment protocol for smart home environment. IEEE Trans
    Dependable Secure Comput 17:391–406. https://doi.org/10.1109/tdsc.2017.2764083
    Article   Google Scholar   Wu L, Wang J, Choo K-KR, He D (2019) Secure key agreement
    and key protection for mobile device user authentication. IEEE Trans Inform Forensics
    Secur 14:319–330. https://doi.org/10.1109/tifs.2018.2850299 Article   Google Scholar   Blanchet
    B, Smyth B, Cheval V, Sylvestre M (2018) ProVerif 2.00: automatic cryptographic
    protocol verifier, user manual and tutorial. p 05–16 Awaisi KS, Abbas A, Khan
    SU, Mahmud R, Buyya R (2021) Simulating fog computing applications using iFogSim
    toolkit. Mob Edge Comput. https://doi.org/10.1007/978-3-030-69893-5_22 Article   Google
    Scholar   Download references Funding The authors did not receive any funds, grants,
    or other support for conducting this study, preparation of this manuscript, and
    submitting the work. Author information Authors and Affiliations BMS Institute
    of Technology and Management, Bengaluru, India Naveen Chandra Gowda School of
    Computer Science and Engineering, REVA University, Bengaluru, India Naveen Chandra
    Gowda & Sunilkumar S. Manvi Department of AI and ML, BMS Institute of Technology
    and Management, Bengaluru, India A. Bharathi Malakreddy School of Computing and
    Information Systems, University of Melbourne, Melbourne, VIC, 3010, Australia
    Rajkumar Buyya Contributions NCG-Conceptualization, Methodology, Data curation,
    Experimentation, Writing—Original draft preparation. SSM-Conceptualization, Methodology,
    Visualization, Investigation, Validation, Supervision. BMA-Methodology, Investigation,
    Validation, Writing—Reviewing and Editing, Supervision. RB-Validation, Writing—Reviewing
    and Editing. Corresponding author Correspondence to Naveen Chandra Gowda. Ethics
    declarations Conflict of interest All authors certify that they have no affiliations
    with or involvement in any organization or entity with any financial interest
    or non-financial interest in the subject matter or materials discussed in this
    manuscript. The authors have no conflict of interest to declare that are relevant
    to the content of this article. Ethical approval Ethical approval is not required
    and not applicable to publish this paper. Additional information Publisher''s
    Note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Springer Nature or
    its licensor (e.g. a society or other partner) holds exclusive rights to this
    article under a publishing agreement with the author(s) or other rightsholder(s);
    author self-archiving of the accepted manuscript version of this article is solely
    governed by the terms of such publishing agreement and applicable law. Reprints
    and permissions About this article Cite this article Gowda, N.C., Manvi, S.S.,
    Malakreddy, A.B. et al. TAKM-FC: Two-way Authentication with efficient Key Management
    in Fog Computing Environments. J Supercomput 80, 6855–6890 (2024). https://doi.org/10.1007/s11227-023-05712-3
    Download citation Accepted 06 October 2023 Published 30 October 2023 Issue Date
    March 2024 DOI https://doi.org/10.1007/s11227-023-05712-3 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Cloud computing Fog computing Authentication Key management iFogSim Use our pre-submission
    checklist Avoid common mistakes on your manuscript. Sections Figures References
    Abstract Introduction Related work System model and security considerations Proposed
    TAKM-FC scheme Security analysis Performance evaluation Conclusions and future
    work Data availability References Funding Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Supercomputing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'TAKM-FC: Two-way Authentication with efficient Key Management in Fog Computing
    Environments'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ullah A.
  - Yasin S.
  - Alam T.
  citation_count: '0'
  description: Numerous gadgets are linked together globally by the Internet of Things
    (IoT). Health checking, exercise programmers and remote medical assistance are
    a few examples of emerging areas in the healthcare system. Implementing cloud
    computing functionality on edge devices is the constant goal of fog computing.
    The approach is anticipated to surpass the minimum latencies requirement when
    used with Internet of Things (IoT) medical equipment. IoT devices produce different
    amounts of healthcare data. Due to the enormous volume of data produced, networks
    get overloaded, increasing delay. Traditional cloud servers are unable to meet
    the low latency requirements of IoT medical equipment and consumers. IoT data
    transfer, it is therefore vital to reduce network latency, computation delay,
    and energy consumption. Using FC, data can be stored, processed, and analyzed.
    Cloud computing data is located at a network edge to reduce high latency. Here,
    a novel resolution to the problem mentioned earlier is proposed. It combines an
    analytical model with a hybrid fuzzy-based reinforced learning technique in an
    FC setting. The objective is to reduce energy usage and cloud server latency for
    health-care IoT. The Internet of Things-FC context is selected and placed by the
    proposed smart FC analysis technique and algorithm using a fuzzy inference system,
    optimization techniques, and development approaches. The results showed that our
    suggested strategy reduced latency by 1.2% in comparison to other techniques.
  doi: 10.1007/s11042-023-16899-1
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Multimedia Tools and Applications
    Article Latency aware smart health care system using edge and fog computing Published:
    25 September 2023 Volume 83, pages 34055–34081, (2024) Cite this article Download
    PDF Access provided by University of Nebraska-Lincoln Multimedia Tools and Applications
    Aims and scope Submit manuscript Arif Ullah, Saman Yasin & Tanweer Alam  211 Accesses
    2 Citations Explore all metrics Abstract Numerous gadgets are linked together
    globally by the Internet of Things (IoT). Health checking, exercise programmers
    and remote medical assistance are a few examples of emerging areas in the healthcare
    system. Implementing cloud computing functionality on edge devices is the constant
    goal of fog computing. The approach is anticipated to surpass the minimum latencies
    requirement when used with Internet of Things (IoT) medical equipment. IoT devices
    produce different amounts of healthcare data. Due to the enormous volume of data
    produced, networks get overloaded, increasing delay. Traditional cloud servers
    are unable to meet the low latency requirements of IoT medical equipment and consumers.
    IoT data transfer, it is therefore vital to reduce network latency, computation
    delay, and energy consumption. Using FC, data can be stored, processed, and analyzed.
    Cloud computing data is located at a network edge to reduce high latency. Here,
    a novel resolution to the problem mentioned earlier is proposed. It combines an
    analytical model with a hybrid fuzzy-based reinforced learning technique in an
    FC setting. The objective is to reduce energy usage and cloud server latency for
    health-care IoT. The Internet of Things-FC context is selected and placed by the
    proposed smart FC analysis technique and algorithm using a fuzzy inference system,
    optimization techniques, and development approaches. The results showed that our
    suggested strategy reduced latency by 1.2% in comparison to other techniques.
    Similar content being viewed by others Integrating IoT and Fog Computing for Healthcare
    Service Delivery Chapter © 2017 Smart Healthcare Systems: An IoT with Fog Computing
    based Solution for Healthcared Chapter © 2023 Internet of Health Things (IoHT)
    for personalized health care using integrated edge-fog-cloud network Article 08
    June 2020 1 Introduction Despite the fact that e-health is still a relatively
    new concept globally, it has gained popularity over the past few decades in a
    number of different countries. Information technology has recently drawn the interest
    of many academics to the digital transformation of medical data. In many healthcare
    institutions, electronic health records contain enormous volumes of diverse medical
    data. It is essential to efficiently integrate, clean, store, analyses, and comprehend
    data in order to guarantee correctness and save access times. In order to make
    informed decisions, healthcare systems must carefully examine the massive amounts
    of real-time data that sensors and wearable technology have collected. The results
    of these decisions significantly affect patient health [1, 2]. Researchers are
    examining signals that could indicate physical ailments using wearable computers
    connected to the internet of things (IoT). On the other hand, because of their
    superior performance, computational methods are growing in popularity. Descriptive,
    clinical, predictive, and operational analytics are the four main categories of
    healthcare data. Determine a patient''s current status and generate data in the
    form of reports, charts, and histograms are the two main objectives of descriptive
    analytics. Numerous analytical tools could be used to do this type of analysis.
    The diagnostic analysis combines decision trees and clustering approaches to investigate
    the occurrence of events and their causes in order to comprehend the causes of
    the recurrence of some specific diseases in individual patients [3]. The core
    of predictive analytics, which use various machine learning approaches to foresee
    unknown events, is building an adequate prediction model. Prescriptive analytics
    seeks to make the best decisions by advising realistic actions that lead to appropriate
    patient treatments. Computing architectures over the past five years, cloud computing
    has dominated every sector of the economy. A cloud-based architecture frequently
    has two levels: the cloud layer is made up of cloud servers with strong storage
    and robust computing capabilities. They link through the core optical network,
    which has incredibly high bandwidth, on the end-user layer which is made up of
    end-user devices. These devices are connected by wired and wireless connections,
    as well as links to the cloud layer through which they communicate [4]. Cloud
    computing infrastructure, platforms, and applications can all be made available
    as services through cloud computing. Due to its durability, dependability, and
    massive processing power, cloud computing is most likely to become the main form
    of computer technology in the future. One of the main problems with cloud computing
    is the rising service average latency. Therefore, it is useless frequently used
    for delay-sensitive real-time IoT applications in many different industries, such
    as in emergencies and heavy traffic [5]. Fog computing and edge computing, two
    emerging technologies, have been suggested as potential solutions to these problems.
    The distance between the database and the end user is frequently narrowed by edge
    and fog technologies. One of the key differences between the two computer technologies
    is performance. Edge computing installs computing servers at the network''s entrance,
    as opposed to fog computing, which embeds servers into the LAN. Fog, on the other
    hand, provides the edge of the network with access to cloud services. The notion
    is the edge, and fog serves as the standard for nature. Only the quality of the
    input can tell fog nodes and cloud-based apps apart [6]. Fogging, another name
    for cloud computing that involves putting hardware in between the cloud and the
    people who actually create the data, has a stronger decentralization goal than
    traditional cloud computing. By moving virtualized resources to the same network
    edge, fog computing promises to speed up user access to applications and connectivity.
    As a result, cloud solutions are made available for extremely mobile technologies.
    Machine learning typically uses a complex network architecture to connect the
    devices that are a part of the target. Fog computing system was created to fill
    any service gaps that may have existed, not to replace cloud computing. So, according
    to Cisco, fog computing is a virtual setting that gives access to infrastructure
    for processing, storage, and communication as well as cloud-based data centers
    that aren''t really situated at the network edge. Unlike clouds, which hinder
    it from providing the services and effectively meeting customer expectations,
    fog is advantageous [7]. In contrast to cloud computing, which transmits compute,
    control, and storage data to a centralized cloud, fog computing balances local
    and remote processing, storage, and network management. A distributed computing
    system known as \"fog computing\" is one in which both virtualized and non-virtualized
    edge devices handle the majority of the work. It shares several traits with the
    cloud, such as processing that isn''t latency-aware and the capacity to keep important
    data for a very long period by serving as a firewall between users and the cloud
    environment. Fog computing shares a core structure with cloud computing, but its
    lower levels have special components that can be used to spot unusual temporal
    behaviors. Many various domains, including the health care sector, traffic patterns,
    parking services, and more, are managed and improved through fog computing. It
    grants the technology the freedom to handle interactions and tasks in the manner
    that it sees fit. Fog computing basically describes a focus layer that sits between
    the hardware and the cloud and has boosted data processing, research, and capacity.
    This is made feasible by minimizing the amount of data that must be transmitted
    to the cloud. An enhanced administrative workflow and a seamless client experience
    are provided by fog computing. The ability to detect, control, and analyses data
    with extraordinarily low latency is often produced through the combination of
    hardware and software [8]. Furthermore, fog computing lacks persistent storage.
    By removing unnecessary data from its computational storage, it lowers costs while
    lightening the load on the cloud. Fog devices are used in slightly different locations
    and situations than cloud devices. The nature of heterogeneity. Overview of edge
    computing over the past few years, there has been a shift in the computing trend,
    and this shift is moving cloud services closer to the edge of networks. As a result,
    the network''s core is becoming the network''s peripheral for computing operations
    and services [9]. This cutting-edge technology is known as edge computing. Data
    processing is distinct from fog computing technology in terms of the actual location
    of the machine. Programmable interface controllers (PACs) and other intelligent
    devices perform the function of processors. With the high performance and energy
    consumption of cloud computing, machine learning has evolved in reaction to crises.
    Due to these, there is now a chance to provide reduced computational offloading
    services for over the past few years, there has been a shift in the computing
    trend, and this shift is moving cloud services closer to the edge of networks.
    Because of this, computing operations and Internet of Things (IoT) software and
    hardware have limited resources. Additionally, it offers benefits for information
    buffering and storage that aid in managing heavy network traffic. The following
    are a few crucial factors that affect system performance right away and help to
    realize the full potential of computer technologies: Latency Between two network
    nodes, it is the typical time it takes to transport a packet [10]. The system
    performance and transmission capacity are reflected in the latency cite service
    response these monitor the typical amount of time it takes for a user to hear
    back after filing a service request. The beneficial service reaction rate of IoT
    services indicates reliability, consistency, and real-time rapid response. Bandwidth
    the average amount of time required to send a packet between two network nodes
    is referred to as bandwidth. The transmission capacity and system performance
    are taken into account while determining the required storage [11]. The data storing
    capability of each solution is reflected in this criterion. Server overhead the
    average computing cost of each solution for each request is reflected in these
    criteria. Network congestion the possibility of the packets colliding is reflected
    in these criteria. Depicts the general structure of the Fog-based health care
    system using IoT the three components of this architecture are the thing, fog,
    and cloud layers. The thing layer is made up of consumer electronics such as sensors,
    wearable internet of things devices, microcontroller chips, heart rate monitor
    actuators, etc. The fog layer is composed of neighborhood routers, fog servers,
    and internet gateways [12]. This layer enables communication and data exchange
    between the cloud and thing levels. Using datasets from deep learning, fog computing,
    distributed learning, and the inner layer, it produces judgments for emergency
    medical circumstances. With the aid of computers in the cloud layer that have
    incredibly large storage, processing, and analytical capacities, healthcare practitioners
    can choose long-term therapy for patients. Fog computing undoubtedly rank highly
    due to its extraordinarily quick service response times among the best options
    for applications involving internet-connected healthcare things [13]. 2 Related
    work In an IoT scenario, computing for cloud systems was proposed to handle large
    or complex healthcare statistics. Methods for cloud computing that make use of
    national cloud data hubs. Information gathering is essential for IoT strategy.
    Major issues with the internet of things and cloud technologies include potential,
    bandwidth, slowness in reacting while simultaneously defending, and privacy. The
    edge that clouded node concepts of authenticity overwhelmed these creations. Some
    of the arrangements suggested in the paper''s sensor structure ideal designs,
    similar prototype, problems and challenges in the field of cloud computing, and
    finally presentation are computed by using the iFog. IoT and cloud-based systems
    for data storage, management, and configuration. By passing internet of things
    boundaries made easier by cloud computing. The distribution of sources discussed
    in this article is the primary demand for IoT policy. Since users can access cloud
    facilities from any location using a little laptop, cloud computing divides and
    maximizes resources on its site. By looking at the assembled data, cloud computing
    provides a mechanism to locate patients, retain histories, and effectively treat
    illnesses. The advantage of the cloud is that healthcare facilities are admitted
    to the company and clinic assets really decrease. Reduced healthcare costs, a
    positive patient experience, and fast removal of patient condition services in
    the event of a disaster are also desirable [14]. While the internet of things
    and cloud computing solve various experiments and additional problems calculated
    by a combination of those tools, cloud computing in the healthcare industry ends
    up first documenting and naming unhealthy objects or illnesses. The knowledge
    design process for this network system requires time and information. Cloud usage
    shouldn''t be used for important purposes. Applications based on the cloud have
    numerous challenges relating to the need for large amounts of information, sporadic
    outages, protection, and security concerns. A time of observation is necessary
    before applying health care. Cloud is unable to meet the standards for the while
    term. The data is moved to the cloud and then returned to the appliance using
    the bandwidth. These concerns are essential to healthcare whenever a precise and
    suitable response is needed to prevent the loss of a life [15]. Systems based
    on the cloud allow knowledge following entirely altered establishments in tactics
    to be written and efficiency is once again referred to as the necessary plan,
    causing the reaction to break and requiring a large amount of information to handle
    massive data. The key issues are also user privacy and security understanding.
    Measuring the reasons why people utilize the cloud to measure uncertainly. Scientists
    have suggested many computing models that correspond to fog computing, such as
    the cloud of things, mist computing, and edge computing, and cloudlets, in order
    to solve these concerns [16]. Related work due to connection bandwidth, fog computing
    has become the newest capacity for transferring cloud tenders to actual IoT devices
    more swiftly. When working on a project with an uncertain background, the primary
    task that must be accomplished in the event of any unforeseen circumstance typically
    be delayed searching. Fog node settling is necessary to lessen the disruptions
    inside the package contact layer in order to achieve the goal [17]. However, as
    opposed to a normal fog computing design or a widely accepted notion of fog nodes,
    and a location to install fog nodes or even though cables the web of things (IoT)
    package influencing. Switches, routers, gateways, mobile phones, intelligent,
    automobiles, and other edge plans are among the candidates for setting up fog
    nodes, albeit the process varies depending on the application. We have access
    during work hours as competitors for the fog node preparation. The entrance gathers
    information from practical tools but lacks some pre-processing or decision-making
    abilities. Since the entrance is built with knowledge of fog techniques, it is
    known as a fog-sensible entrance (FSG). IoT transportation is designated for maintenance
    by virtual machines (VMs), which are sped up by circulating fog nodes. To get
    ready to reduce the total iatrogenic delay caused by movement accumulation and
    process, we tend to increase the amount of fog nodes. Our findings show that optimum
    fog node readiness within the IoT web may result in a reduction in latency associated
    with the development of IoT understanding of a fairly common cloud system [18].
    The use of fog hubs to organize administrations in accordance with the fog computing
    strategy. In addition to using fog computing to calculate material, a fog hub
    is useful. The actual device known as the Fog hub has completed sending the fog
    computing data according to the schedule. A framework for implementing IoT services
    can be provided via fog hub. Any device''s network has the potential to transform
    into a fog hub because it supports virtualization, stockpiling, computation, and
    inquiry. For fog hubs, fog computing has a number of contenders that resemble
    switches, corridors, switches, devoted attendants, or doors. It provides the structure
    needed to finish IoT administrations. A gadget called a channel is used to collect
    data from the key hubs. Everywhere continue to be certain conditions while the
    entrance continues to be expected near ensure some sort of pre handling before
    interrelated projects that can''t be done assuming that is independent. Fog is
    necessary, all things considered. As a result, the entryways must be enhanced
    with fog capabilities. Known as a fog shrewd entrance (FSG), this type of door
    [19]. An FSG considers further background information and attacks the foundation
    of the applications'' criticism. In this way, it functions intelligently by providing
    information only when necessary. With the aid of a jointly developed fog organization,
    the additional acting skills are merged. Not always are the basic hubs and organizations
    present. Additional requirements created for diverse administrations include efficient
    instrument hubs and essential sensor linkages. Basically, under the watchful eye
    of an FSG, temporary capacity, pre-handling, information security and protection,
    and other such endeavors should be achievable successfully and even more proficiently.
    In response to 12 criticisms, an application entryway should select the type of
    information that controlled and referred to. The recovering use of would be advantageous
    to FSG assets in the cloud and organization. They support enable convention change
    across several organizational divisions, making them least intense in comparison
    to other needs and necessities [20]. The disclosure of fog hubs, information reserving,
    information offloading, administration coordination, and so forth are some of
    the major challenges in the planning of fog-driven administrations for IoT. The
    mechanism for delivering distress signals to and testing stays to fog hubs. Increasing
    the number of fog hubs is primarily done to increase information frequency and
    reduce idleness. Additionally, improving understanding and effectiveness of fog
    computing engineering is the discovery optimal figure of fog hubs determination.
    Despite various possibly cunning passageways within a structure that might be
    relocated up to fog hubs. In order to entirely obscure the IoT passageways, we
    would like to bring numerous fog hubs to IoT enterprises. Like the office area
    issue, which was taken into consideration, the FSGs sending issue for enhancement
    issues an NP-hard problem. Planning a calculation to produce close optimal arrangements
    quickly for this problem is a laborious task. Additionally, the vast majority
    of appropriate items that are offered in the medical services sector are used
    in fixed care options. Due to cost pressure, emergency clinics, clinical guidance
    units, or other patient consideration offices have challenges including having
    fewer financial resources [21]. As a result, doing the right thing in a fixed
    setting becomes motivated by a decrease in labours expenses. Two-thirds (66%)
    of all clinics under study have an internal remote system, which has seen a growth
    of twelve offer centers in the last two years. The majority (81%) of emergency
    rooms in the EU are online. Furthermore, the vast majority of pre-owned IT benefits
    are handled internally, therefore qualified IT staff are typically accessible
    [22]. As a result, doing the right thing in a fixed setting becomes motivated
    by a decrease in labours expenses. Two-thirds (66%) of all clinics under study
    have an internal remote system, which has seen a growth of twelve offer centers
    in the last two years. The majority (81%) of emergency rooms in the EU are online.
    Furthermore, the vast majority of pre-owned IT benefits are handled internally,
    therefore qualified IT staff are typically accessible [23]. Table 1 show the model
    of smart health care system using edge and fog computing. Table 1 All methods
    and datasets are applied through these techniques Full size table 3 Working of
    system model The proposed new set of rules is in charge of switching from fog
    nodes in the framework inside the device and setting up the information allocation
    issue inside an MDP. This problem allows fog nodes to pass dispersed information
    envelopes, apply various statistical techniques, and implement switches aimed
    at end-users. To get Q-Learning knowledge of version, future fuzzy-system-based
    models, which different from present fog network approaches that merely depend
    on the main aspect, are used. First, magnetic environments are primarily concerned
    with the end-users'' time for critical information that is transmitted to them
    in a single packet from a specific kind of fog node. When nearby fog nodes work
    together, the internal movement choices are changed. Decision-making mechanism
    can then be selected in order to minimize carrier latency. Visitors of the community
    website modify the discussion of performance in-facts to enable users to swiftly
    and easily apply the charge to facts, distribution to a dispersed packet of fog
    nodes. In this kind of dynamic information packet allocation architecture, the
    fog node offers the most accurate information that may be required by customers
    or time-sensitive. The term fog nodes are used in our proposed technique as a
    server with communication capabilities that may demonstrate processing and computing
    capability. The online (internet source) UCI device learning repository has been
    used to gather healthcare ECG sensor data as a middleman system for machine learning
    devices and intelligent process devices comprises 14 components and 303 instances.
    But the particular dataset on coronary heart disease from the UCI library joins
    76 different variables. The dataset that we use in our simulation is information
    that has been uniformly sampled. The suggested list of algorithmic guidelines
    was assessed using recordings from a patient whose ECG was part of a dataset.
    This patient passed away from cardiac disease brought on by high blood pressure,
    high blood sugar, and high cholesterol. The numbers have been carefully kept track
    of. A total of 303 patients'' ECG strips and records have been acquired by Leads
    (one channel). The fitness dataset''s distinguishing features include genuine,
    fully integer-based, and express [29]. In our simulation, the ECG sensor. The
    proposed new set of rules is in charge of switching from fog nodes in the framework
    inside the device and setting up the information allocation issue inside an MDP.
    This problem allows fog nodes to pass dispersed information envelopes, apply various
    statistical techniques, and implement switches aimed at end-users. To get Q-Learning
    knowledge of version, future fuzzy-system-based models, which differ from present
    fog network approaches that merely depend on the main aspect, are used. First,
    magnetic environments are primarily concerned with the end-users'' time for critical
    information that is transmitted to them in a single packet from a specific kind
    of fog node. When nearby fog nodes work together, the internal movement choices
    are changed. Decision-making mechanism can then be selected in order to minimize
    carrier latency. Visitors of the community website modify the discussion of performance
    in-facts to enable users to swiftly and easily apply the charge to facts, distribution
    to a dispersed packet of fog nodes. In this kind of dynamic information packet
    allocation architecture, the fog node offers the most accurate information that
    may be required by customers or time-sensitive [30]. The term fog nodes are used
    in our proposed technique as a server with communication capabilities that may
    demonstrate processing and computing capability. The online (internet source)
    UCI device learning repository has been used to gather healthcare ECG sensor data
    as a middleman system for machine learning devices and intelligent process devices.
    In our simulation, the ECG sensor data contains of comprises 14 components and
    303 instances. But the particular dataset on coronary heart disease from the UCI
    library joins 76 different variables. The dataset that we use in our simulation
    is information that has been uniformly sampled. The suggested list of algorithmic
    guidelines was assessed using recordings from a patient whose ECG was part of
    a dataset. This patient passed away from cardiac disease brought on by high blood
    pressure, high blood sugar, and high cholesterol. The numbers have been carefully
    kept track of. A total of 303 patients'' ECG strips and records have been acquired
    by Leads (one channel). The fitness dataset''s distinguishing features include
    genuine, fully integer-based, and express [31]. The Markov decision process (MDP)
    technique in Q-learning was used to minimize the constraint to obtain the lowest
    computation latency, conversation latency, and network latency by distributing
    information packets to specific processors of digital machines. Data collection
    and observation are described mathematically using the Q-learning algorithm MDP,
    which builds up feedback from experience in a dynamic environment. The recommended
    method calls for a Q-learning algorithm for MDP to take into consideration the
    dynamic behavior of the IoT fog-cloud device [32]. The IoT fog-cloud device was
    unable to take advantage of the transition possibilities and rewards due to fog
    nodes dynamic packet conversion of incoming processing requests. The usage of
    Q-mastering MDP provides a mechanism for making decisions that reduces the issues
    brought on by unique data packet requirements from unique clients at unusual times
    and the computational capabilities of fog nodes. The Q-mastering algorithm resolves
    the MDP with unknown enormous rewards and transition characteristics by utilizing
    and examining the system''s astounding states [33]. Figure 1 healthcare environment
    by using IoT, and computing methods of fog. Additionally, it raises appreciation
    for the employment of very effective action by the IoT fog-cloud gadget. Using
    a set of fuzzy-based rules, real-time monitoring of PHD (patient healthcare data)
    is carried out (RL). Fig. 1 Healthcare environment by using IoT and edge computing
    Full size image The traits of IoT in healthcare require RL to provide a quick
    indication of the patient''s past fitness status [34]. Figure 2 present design
    and IoT engineering of fog processing. Fig. 2 Design and IoT engineering of fog
    processing Full size image The selection of statistics packets for calculations
    specific to fog nodes is carried out using RL and a NN. This reduced the strain
    on a few nodes that were under pressure to deliver the records to end users as
    soon as possible. Making the most of the available resources is made possible
    by RL, which permits the delivery of more data to processors in the form of packets
    while abiding by QoS restrictions for critical jobs that are running late. The
    FIS was used to process the decision to set guidelines. It was designed to draw
    conclusions from the patient''s prior medical history [35]. The suggested method
    is shown to have less excessive delay. Figure 2 shows the healthcare IoT records
    transmission model. It provides a data packet to fog nodes via the IoT. The fog
    nodes swiftly transmit the records packet to end users. A grasp fog controller
    controls the selection and data transfer of the fog node by interacting with a
    cloud server similarly to a cloud server [36]. The suggested approach enables
    fog nodes to select appropriate data sent by IoT devices. Various fog nodes, and
    delivered to live clients. This solution aims to eliminate excessive latency or
    total latency among IoT in order to avoid losing customers. Distributed smart
    decision-making is necessary for the distribution of data packets to various fog
    nodes for compute. This packet release of information relates to the by giving
    up nodes, the need for information in the quickest possible time is met. When
    distributing information packets optimally, communication postpones, extended
    computation postpones, and community postpones are all taken into consideration.
    Nodes may face delays since a wide variety of information packets are being transmitted
    around the network. Important choices must be made about the quantity of records
    packets that must be uploaded and migrated, (ii) the scaling of records packets
    to fog nodes, and (iii) the data packets that must be assigned to fog nodes for
    calculation and given to them in real time. It''s interesting to note that in
    a fog environment, the key structures stress the coordinated migration and overall
    load offset. Studies to improve device understanding with the application of smart
    FC There is no longer any way to reduce the overall latency between IoT, cloud
    servers, and give-up users [37]. Figure 3 present IoT information transmission
    model. Fig. 3 Expert fog regulator, end-client and cloud server make up the medical-care
    IoT information transmission model Full size image In Fig. 3 the fiber channel
    protocol is used to show the IoT framework model for healthcare services. The
    information sent by Internet of Things (IoT) devices for medical services is divided
    into categories of acceptable, typical, and high-risk by means of a FIS (Fuzzy
    Interface System) categorization technique. Patient’s healthcare data is distributed
    over numerous virtual machines on fog servers using reinforcement learning. The
    dissemination of the contaminated information to end users inside the damaged
    asset is preferable via an artificial neural network. In virtualization, fog hubs
    are used to send information packets between various hubs and end users. By using
    end hubs that are connected to cloud hubs, data recovery can be moved there [38].
    Figure 4 present healthcare IoT devices, classified PHD systems, fog gateways,
    fog servers. Fig. 4 Healthcare IoT devices, classified PHD systems, fog gateways,
    fog servers, and virtual computers make up the healthcare IoT system model (VM).
    Full size image It employs a skilled Fog hub switch regulator with layout data
    for information parcel designation and propagation. Within the organizational
    framework of the company, hubs connect each center to the professional cloud hub.
    The fog node calculation-based parcel designation approach is examined in this
    study as relevant dynamic data in an AI environment. The Fog can move data parcels
    to several hubs in order to decrease latency and control traffic. Information
    packages on a line on this CPU can be utilized as effective traffic files on hubs
    and have an average impact on reaction times. Every fog node hub has the ability
    to compile data, make simple judgments, support the end hubs, and provide details
    on acquiring traffic data and line location. Expert visit views the data as distinct
    [39]. From numerous centers and builds a table of organization. The knowledgeable
    cloud hub sends requirements in order to decide whether the trustworthy fog node
    needs to transfer the necessary data. If so, information is being sent to the
    hub next door, where decisions are made as a result of time passing and the availability
    of commanding information. This study''s primary objective is to choose time-sensitive
    data while minimizing latency and system traffic [40]. A calculation with a presentation
    guarantee is suggested for the MDP of idleness minimization in medical care IoT
    and the cloud, which was previously identified. The Markov Decision Process (MDP)
    is composed of a fog hub acting as a decision expert that continuously analyses
    the instant states of the controlled framework, selects individuals who have given
    their express consent (an Ai(s)), determines the motion in alternative states′,
    and rewards r in order to switch its subsequent decisions. A specialist in markov
    decision processes chooses a state-produced activity [41]. This reveal the new
    state and award progress probability distribution. Our approach states that the
    expert mist hub chooses a task as a part of the current status and takes taking
    into consideration the leadership displayed in all hubs and related states. The
    Markov decision process is represented by a set of four tuples. Si, Ai, Pi, and
    Ri, where is the state space, is the number of discrete data packages to be distributed
    to end clients, and nl N (1 NL N) is the number of fog nodes that hold the data
    bundles., Q = Q1,, QN | Qi 0, 1,, Qi, max is the number of information bundles
    currently remaining in the node hub''s line. The activity space is also where
    nf N (1 nf N, nf NL) is described as the adjacent hub inside the fog network that
    is being designated with information packages provided by hub NL is the quantity
    of data bundles transferred away from the neighboring cloud hub nf. Allow Ai(s)
    Ai is a group of tasks that can be finished on different states. As a result of
    Ai(s) being so well-described, hub nl can swap out the statistics package with
    the hub after it, providing the client with the same amount of information packages
    or fewer than are now required. Activity a determines the total amount of information
    bundles that processed privately or chosen for the open line space of hub NL.
    The shifting chance of moving Pi (s, a) from one state to another when an action
    is carried out is [42]. The medal when action is Si, Ai, and Ri when the framework
    is in states, and is executed. The framework''s major goal is to make a unique
    information bundle designation method visible on each hub in order to boost utility
    while lowering the risk of information parcel underdevelopment and dispersion.
    As a result, for a particular action and at the states, the deciding framework
    offers the following description of the immediate reward capacity. $${R}_i\\left(s,a\\right)={U}_i\\left(s,a\\right)-\\left({L}_i^{FOG}\\left(s,a\\right)+O\\left(s,a\\right)\\right)$$
    (1) where Ui(s, a), \\({L}_i^{FOG}\\left(s,a\\right)\\) and O(s, a) denotes the
    immediate effectiveness, direct latency, and statistics of packets as a distribution
    probability function in a combination of respectively. The prepared value is computed
    as $${U}_i\\left(s,a\\right)={r}_{il}\\log \\left(1+{d}_f^l+{d}_f^p\\right)$$
    (2) where Riu is the return value. $${L}_i^{FOG}\\left(s,a\\right)=\\frac{\\left({\\chi}_l\\cdot
    {C}_L^{FOG}+{C}_{PL}^{FOG}+{N}_L^{FOG}\\right)}{\\left({d}_f^l+{d}_f^p\\right)}$$
    (3) where χl is the latency burden. Now, \\({C}_L^{FOG}\\) is the message in potential,
    \\({N}_L^{FOG}\\) the calculation of the network latency, and \\({C}_{PL}^{FOG}\\)
    the latency. It is not certain how long it takes for an information package to
    go from an end-client fog hub (a wearable IoT device) to a mist hub, a fog node
    hub, and back to an end-client hub [43]. $${C}_L^{FOG}={C}_L^{FOG}\\left(\\ \\mathrm{Request}\\
    \\right)+\\Big)+{C}_L^{FOG}\\left(\\ \\mathrm{Response}\\ \\right)\\ {C}_L^{FOG}\\left(\\mathrm{Request}\\right)=\\frac{D_P^S}{v_{n_e,{n}_f}^S}\\cdot
    {d}_l^p$$ (4) $${C}_L^{FOG}\\left(\\ \\mathrm{Response}\\ \\right)=\\frac{D_P^S}{v_{n_f,{n}_e}}\\cdot
    {d}_f^p{C}_L^{FOG}=\\frac{D_P^S}{v_{n_e,{n}_f}}\\cdot {d}_l^p+\\frac{D_P^S}{v_{n_f,{n}_e}^S}\\cdot
    {d}_f^p\\kern0.75em \\mathrm{where}$$ (5) $${v}_{n_e,{n}_f}={B}_{e_w}\\cdot \\log
    \\left(1+\\frac{g_{n_e,{n}_f}\\cdot {P}_{t_x{n}_e}}{B_{e_w}\\cdot {N}_0^e}\\right)\\kern0.5em
    \\mathrm{and}\\kern0.5em {v}_{n_f,{n}_e}={B}_{f_w}\\cdot \\log \\left(1+\\frac{g_{n_f,{n}_e}\\cdot
    {P}_{t_n{n}_f}}{B_{f_w}\\cdot {N}_0^f}\\right){C}_L^{FOG}$$ (6) between the end-user
    node ne and adjacent fog node nf is resolute, as $${C}_L^{FOG}={D}_P^S\\cdot \\left(\\frac{d_i^p}{v_{n_e,
    nf}}+\\frac{d_f^p}{v_{n_f,{n}_e}}\\right)$$ (7) between fog nodes NL and nf is
    communicated as $${C}_L^{FOG}={D}_P^S\\cdot \\left(\\frac{d_f^l}{v_{n_l,{n}_f}^l}+\\frac{d_f^p}{v_{n_f,{n}_l}}\\right)\\kern0.5em
    \\mathrm{where},\\kern0.5em {v}_{n_l,{n}_f}={B}_{l_w}\\cdot \\log \\left(1+\\frac{g_{n_l,{n}_f}\\cdot
    {P}_{t_x{n}_l}}{B_{l_w}\\cdot {N}_0^l}\\right)$$ (8) and $${v}_{n_f,{n}_l}={B}_{f_w}\\cdot
    \\log \\left(1+\\frac{g_{n_f,{n}_l}\\cdot {P}_{t_K{n}_f}}{B_{f_w}\\cdot {N}_0^f}\\right){C}_L^{FOG}$$
    (9) between node ne and node NL is firm as [44] $${C}_L^{FOG}={D}_P^S\\cdot \\left(\\frac{d_l^p}{v_{n_e,{n}_l}}+\\frac{d_f^l}{v_{n_l,{n}_e}}\\right)$$
    (10) where $${v}_{n_l,{n}_e}={B}_{l_w}\\cdot \\log \\left(1+\\frac{g_{n_J{n}_e}\\cdot
    {P}_{t_x{n}_l}}{B_{l_w}\\cdot {N}_0^l}\\right)\\kern0.75em \\mathrm{and}$$ (11)
    $${v}_{e_e,{v}_l}={B}_{e_w}\\cdot \\log \\left(1+\\frac{g_{n_e,{n}_i}\\cdot {P}_{t_x{n}_e}}{B_{e_n}\\cdot
    {N}_0{n}^e}\\right)$$ (12) Here, \\({d}_l^p\\) is the quantity of information
    bundles sent by ne to nf and ne, \\({d}_f^p\\) the quantity of information parcels
    sent by nf to ne and nl, \\({d}_f^l\\) the amount of data packets sent by nl to
    ne and nf, \\({D}_P^S\\) the size of data packets, \\({v}_{n_e,{n}_l}\\) IoT scheme
    program service rate from ne to nf, \\({v}_{n_f,{n}_e}\\) the fog diffusion service
    frequency from nf to ne, \\({v}_{n_e,{n}_l}\\) the broadcast service proportion
    from ne to nl,  \\({v}_{n_l,{n}_e}\\) the fog transmission service rate from nl
    to ne, \\({v}_{n_l,{n}_f}^n\\) the illustration service rate from nl to nf, \\({v}_{n_f,{n}_l}\\)
    the transmission package speed from nf to nl, \\({B}_{e_{\\ell }}\\), \\({B}_{f_n}\\),
    and \\({B}_{l_{l_l}}\\) the bandwidths per a node ne, nf and nl, \\({g}_{n_e,{n}_f}\\triangleq
    {\\beta}_1{d}_{n_e,{n}_f}-{\\beta}_2\\), \\({g}_{n_f,{n}_e}\\triangleq {\\beta}_3{d}_{n_f,{n}_e}-{\\beta}_4,{g}_{n_j,{n}_e}\\triangleq
    {\\beta}_5{d}_{n_j,{n}_e}-{\\beta}_6\\), \\({g}_{n_e,{n}_l}\\triangleq {\\beta}_7{d}_{n_e,{n}_l}-{\\beta}_8,{g}_{n_f,{n}_f}\\triangleq
    {\\beta}_9{d}_{n_f,{n}_f}-{\\beta}_{10}\\) and \\({g}_{n_f,{n}_l}\\triangleq {\\beta}_{11}{d}_{n_f,{n}_l}-{\\beta}_{12}\\)
    the channel advantages for \\({v}_{n_e,{n}_f},{v}_{n_f,{n}_e},{v}_{n_l,{n}_e},{v}_{n_e,{n}_l}\\),
    \\({v}_{n_l,{n}_f}^n\\) and \\({v}_{n_f,{n}_l}\\), \\({d}_{n_{e, nf}}\\) distance
    in which nodes have ne and nf, \\({d}_{n_{l, ne}}\\) the distance in between nodes
    nl and ne, \\({d}_{n_{i,n\\in g}}\\ the\\) distance between hub and have nodes
    nl and nf, \\({P}_{t_x{n}_e}\\), \\({P}_{t_x{n}_f}\\) and \\({P}_{t_x{m}_l}\\)
    the transmission controls of nodes ne, nf and nl, \\({N}_o^e\\) the noise power
    thickness for transmission service rate from ne to nf and nl, \\({N}_o^f\\) the
    noise power concentration for transmission service rate from nf to ne and nl,
    \\({N}_o^l\\) the noise power mass for conduction service rate from nl to ne and
    nf, β1, β3, β5, β7, β9 and β11 symbolizes the route damage continuous, and β2,
    β4, β6, β8, β10 and β12 signifies the track cost exponent, respectively. The number
    of packets transferred from the end-user node ne to the fog node NL, NL to the
    fog node nf, and nf to me determine the network latency, which is represented
    by the following formula: The organization''s inactivity relies on the all-out
    packages delivered from the end-client hub ne to the fog hub NL, NL to the fog
    hub nf, and from nf to me; the organization''s latency is communicated as [44]
    $${N}_L^{FOG}=\\frac{l_n{H}_C{n}_l+{l}_n{H}_C{n}_f+{l}_n{H}_C{n}_e}{T_P},{N}_L^{FOG}=\\frac{l_n{H}_C\\cdot
    \\left({n}_l+{n}_f+{n}_e\\right)}{T_P},$$ (13) Here \\({T}_P={d}_l^p+{d}_f^l+{d}_f^p\\)
    where HC is the number of stage totals, TP the total facts of packets sent, and
    ln the entity journey delayed. The calculation latency (waiting time and administration
    time) can be expressed as by anticipating an analysis framework and preventing
    parcel information loss with the parcel appearance rate and administration rate
    for the fog hub. $${C}_{pl}^{FOG}=\\frac{N_l\\cdot CP{U}_I\\cdot {d}_f^l}{c_s^l}+\\frac{N_I\\cdot
    CP{U}_I\\cdot {d}_f^p}{c_s^f}+\\frac{1}{v_{n_{i,j},{n}_e}-{\\lambda}_e}+\\frac{1}{v_{n_{i,j{v}_f}}-{\\lambda}_f}+\\frac{1}{v_{v_f,{n}_l}-{\\lambda}_l}+\\frac{1}{v_{v_{j,{n}_e}}-{\\lambda}_{el}}$$
    (14) where NI is the entire sum of information per statics of packet, CPUI is
    the CPU cycle per training, λe, λl, λe′, and λf is the statics of packet arrival
    charges at nodes NL and nf, \\({c}_s^l\\) and \\({c}_s^f\\) the CPU speediness
    of nodes NL and nf. The records packet distribution possibility O(s, a) is considered
    as [45] $$O\\left(s,a\\right)=\\frac{\\chi_i\\left({d}_f^l\\cdot {P}_{\\mathrm{allocation},l}+{d}_f^p\\cdot
    {P}_{\\mathrm{allocation},f}\\right)}{d_f^l+{d}_f^p}$$ (15) $${P}_{\\mathrm{aflecorasej}}=\\frac{\\mathit{\\max}\\left(0,{\\lambda}_i-\\left({Q}_{i,\\mathit{\\max}}-{Q}_i^{\\prime}\\right)\\right)}{\\lambda_i},$$
    (16) $${Q}_i^{\\prime }=\\kern0.5em \\mathit{\\min}\\left(\\mathit{\\max}\\left(0,{Q}_i-{v}_i\\right)+{d}_{fi}^l,{Q}_{i,\\mathit{\\max}}\\right).$$
    χ i is the documents packet distribution weightiness, vi the service speed of
    a node ni, \\({d}_{f_i}^l\\) the overall computation of files packets to be nearby
    handled at node ni, and λi is the figures packet form frequency at node ni. \\({Q}_i^{\\prime
    }\\) denotes the next file state, i.e., residual documents packages of a node
    ni in state’s when an accomplishment is achieved. The complete latency is communicated
    as \\({T}_L={C}_L^{FOG}+{N}_L^{FOG}+{C}_{PL}^{FOG}\\)  [46]. $${\\displaystyle
    \\begin{array}{c}{T}_L={D}_P^S\\cdot \\left(\\frac{d_l^p}{v_{n_e,{n}_l}}+\\frac{d_f^l}{v_{n,{n}_e}^l}\\right)+{D}_P^S\\cdot
    \\left(\\frac{d_f^l}{v_{n_l,{n}_f}^l}+\\frac{d_f^p}{v_{n_j,{n}_l}^p}\\right)+{D}_P^S\\cdot
    \\left(\\frac{d_l^p}{v_{n_e,{n}_f}}+\\frac{d_f^p}{v_{n_{j,},{n}_e}}\\right)+\\\\
    {}\\frac{N_l\\cdot CP{U}_I\\cdot {d}_f^l}{c_s^l}+\\frac{N_I\\cdot CP{U}_I\\cdot
    {d}_f^p}{c_s^f}+\\frac{1}{v_{n_{i,j},{n}_e}-{\\lambda}_e}+\\frac{1}{v_{n_{i,j{v}_f}}-{\\lambda}_f}+\\frac{1}{v_{v_f,{n}_l}-{\\lambda}_l}\\end{array}}$$
    (17) To an end-user node ne via a stage 1 transmission link, fog hubs NL and nf
    are to send information about the frequency of traffic. It is necessary to confirm
    the QoS (latency constraint) for end users. Multiple interruptions, such as calculation
    delay (interruption in files on nodes), correspondence latency, and network latency,
    affect end-clients as a result of huge information transfer and significant information
    flow. The objective of the suggested approach is to allow for modification while
    reducing latency. Prior to implementing the framework, Pi and reward Ri are chosen.
    The best long-term reward for any state activity is described as a sequence that
    starts with the current condition and ends with the activity that yields that
    state''s typical foreseeable modern future alternatives. The minute reward obtained
    after k further time steps is then worth twice \\({\\gamma}_i^{\\mathcal{K}-1}\\),
    where γi is named as a markdown factor (0 < γi < 1). The Bellman optimality equation
    is satisfied since the firm is the primary value purpose [47]. $${\\displaystyle
    \\begin{array}{c}{v}^{\\ast }(s)=\\underset{a}{\\mathit{\\max}}\\kern0.1em \\mathrm{E}\\left({R}_{i_{t+1}}+{\\gamma}_i{v}^{\\ast}\\left({S}_{i_{t+1}}\\right)\\mid
    {S}_{i_t}=s,{A}_{i_t}=a\\right)\\\\ {}=\\underset{a}{\\mathit{\\max}}\\kern0.1em
    \\sum_{s^{\\prime },r}\\kern0.1em {p}_l\\left({s}^{\\prime },r\\mid s,a\\right)\\left[r+{\\gamma}_i{v}^{\\ast}\\left({s}^{\\prime}\\right)\\right]\\end{array}}$$
    (18) In all save the most extreme cases, the framework cannot dependably predict
    the likelihood of Pi and reward Ri because those restrictions are subject to change.
    It is advised that RL speak with someone about this restriction. Reinforcement
    Learning insufficiency of sensitive knowledge is overcome by identifying the source
    of sensitivities. The classical dynamic computation has restricted functionality
    in R since it assumes an ideal model and has a high evaluation value. It is common
    practice to employ Q-learning, a recognized technique without a model, to determine
    the ideal state-activity methodology for any Markov Decision Process. For the
    proposed system, the learning primary fog nodes serve as a regulator by continuously
    identifying the current conditions with an action that is followed by a change.
    Consequently, it names the various States \"s\" and the prize \"r.\" With the
    help of these discoveries, it succeeds and resumes its Q-learning projections
    to the point where approaching active following is developed [48]. $$Q\\left(s,a\\right)\\leftarrow
    \\left(1-{\\alpha}_i\\right)Q\\left(s,a\\right)+{\\alpha}_i\\left[{R}_i\\left(s,a\\right)+{\\gamma}_i\\underset{a^{\\prime}\\in
    {A}_{is}}{\\mathit{\\max}}\\kern0.1em Q\\left({s}^{\\prime },{a}^{\\prime}\\right)\\right],$$
    (19) where αi (0 < αi < 1) is the learning rate here, αi balances the heaviness
    of the old assessment with the weight of the new evaluation and perception. In
    eq. 3.21, a prototype MDP, Q(s′, a′) is the Q function for modifying state’s′
    and activity a′, and Q represents the environment of activity a and on state’s.
    The condition fixes the problem with progress statuses and awards for the IoT-fog
    cloud architecture for medical services. The main fog node hub functions as a
    regulator to monitor the current state and activities. The fog node continues
    to collect information on new states (s′) and rewards (r). The Q-function is updated
    after the process is complete, as shown in Eq 18. Specifically, this condition
    resolves the problem of advancement in the change possibility proficiency rewards
    while using the model RL technique. MDP for Q-learning Gathering the single activity
    with the highest rough rate, or voracious choice (at arg max Qt(a)), is how to
    determine the understandable activity decision rate [49]. This leads to the covetous
    activity choice rate, which is a crucial component of the Q-learning -greedy algorithm
    calculation, reliably acquiring the current knowledge to mislead the ongoing award.
    While incorporating a limited chance that neglectfully prefers against the whole
    open activities during a similar number of possibilities, the algorithm behaves
    greedily for a greater projected number of terms. The hungry decision of manipulation
    and the inquiry approach are requested by RL. They also ask for the unquenchable
    determination and chance of an irregular option. While exploration can produce
    the most extreme generally communicating long-term gain, exploitation is an appropriate
    activity to overestimate the important advantage at a stage. One application of
    the - greedy algorithm computation at this point confirms that Q (s, a) is the
    ideal cost by increasing the tiny part of the fraction that is being moved and
    the entire opposite infinite part of the entire distance. The suggested approach
    determines the measured reward ability by using Eq (19). In order to describe
    its three components, the state’s′ is obtained. Although the state''s border is
    a movable unit, the contiguous mist fog node has the ability to send data packets
    to other fog node hubs for designation. The data is not set at the fog hub after
    the appearance of the information packets. Patients'' and end-clients'' interest
    in localized and focused data administration high [50]. The FC technique is used
    because end clients cannot retrieve time-basic limited information from cloud
    servers. Mist hubs send the nearby processing offices to the client''s end based
    on the need. Fog hubs, which have rapid local connections, deliver stored cloud
    information to mobile clients. Equipment switches, switches, IP camcorders, and
    other devices are examples of mist gadgets. A fog server could be a light cloud
    server or a virtualized figuring framework. Additionally, and separately show
    the j-th fog server''s current storage size length and CPU recurrence in Hz. If
    the j-th information parcel credits to the j-th mist fog server, the j-th information
    bundle''s delay at the time of distribution to the fog servers is described as
    eq (20) By selecting a fog server from the predicted information parcel for distribution,
    long-term rearrangement is achieved. When the fog server has completed calculating
    the information [51]. $${\\displaystyle \\begin{array}{c}\\mathrm{P}(t)\\triangleq
    \\mathit{\\min}\\sum_{i=1}^{\\Omega (t)}\\kern0.1em \\sum_{j=1}^{\\Psi}\\kern0.1em
    {y}_{ij}{l}_{ij}\\\\ {}\\sum_{i=1}^{\\Omega (t)}\\kern0.1em {y}_{ij}=1,\\forall
    j\\in \\Psi, \\kern3.5em \\\\ {}{l}_{ij}\\le {\\tau}_i^d,\\forall j\\in \\Psi
    \\kern6.25em \\\\ {}{y}_{ij}\\in \\left\\{0,1\\right\\},\\forall i\\in \\Omega
    (t),\\forall j\\in \\Psi \\end{array}}$$ (20) Where Ω(t) and ψ are the sets of
    data packets and fog server, respectively. Therefore, the long-term latency reduction
    function (fΔ) is given by $$\\left({f}_{\\Delta}\\right)\\underset{t\\to \\infty
    }{\\mathit{\\lim}}\\kern0.1em \\frac{1}{t}\\sum\\nolimits_{i=1}^t\\kern0.1em \\mathrm{P}(i)$$
    (21) To completely handle the transferred information parcel, the calculation
    idleness of the framework becomes maximal between the circulated fog servers.
    The choice is made utilizing the greedy technique, which reduces the framework
    idleness when information parcels are transferred [52]. 4 Data collection Using
    established verification processes, data collecting is a strategy for gathering,
    estimating, and interpreting appropriate discernment for research activity. Based
    on the information gathered, the hypothesis can be stretched. To obtain a thorough
    and accurate illustration of the subject, data is gathered from a variety of sources.
    It also brings a methodical approach to measurement. Data gathering aids in estimating
    production, resolving research-related questions, and forming predictions regarding
    future developments and trends. For commercial decisions that must be made, for
    maintaining the quality of research and for ensuring quality, accurate data collecting
    is crucial. Data may be compiled from internet resources like GitHub fig share
    Kaggle and many more [53]. Data preprocessing is very important in database management
    systems. Data is always dirty. Data preprocessing in Machine Learning is a crucial
    step that helps enhance the quality of data to promote the extraction of meaningful
    insights from the data. Data preprocessing in machine learning refers to the technique
    of preparing (cleaning and organizing) the raw data to make it suitable for building
    and training machine learning models. In simple words, data preprocessing in Machine
    Learning is a data mining technique that transforms raw data into an understandable
    and readable format [54]. There are several stages or steps in data pre-processing.
    By performing these steps the data is managed in a valid form to add to the database
    for avoiding any type of inconvenience. The proposed method used three different
    steps to normalize the data. The steps of the proposed method are given below:
    Treat with Missing Values for data preprocessing, treating with missing values
    is very important. After treating with missing values get accurate results. Missing
    values means that any data is not present in their existing area. Splitting the
    dataset is the next step in data preprocessing. Every dataset for the machine
    learning model must be split into two separate sets training set and a test set.
    The training set denotes the subset of a dataset that is used for training the
    machine learning model. A test set, on the other hand, is the subset of the dataset
    that is used for testing the machine learning model. The ML model uses the test
    set to predict outcomes. Usually, the dataset is split into 70:30 ratios or 80:20
    ratios. This means that it either takes 70% or 80% of the data for training the
    model while leaving out the rest 30% or 20%. The splitting process varies according
    to the shape and size of the dataset in question. In the proposed method used
    80:20 ratios [55]. Feature engineering is the process that takes raw data and
    transforms it into features that can be used to create a predictive model. Feature
    engineering aims to prepare an input data set that best fits the machine learning
    algorithm as well as to enhance the performance of machine learning models. In
    proposed mode used the feature engineering method for data preprocessing [56].
    A Dataset is a set or collection of data. This set is normally presented in a
    tabular pattern. Every column describes a particular variable. And each row corresponds
    to a given member of the data set, as per the given question. This is a part of
    data management. Data sets describe values for each variable for unknown quantities
    such as fault detection and identification time, temperature, probability etc.
    of an object or values of random numbers. The values in this set are known as
    a datum. The data set consists of data of one or more members corresponding to
    each row. For conducting this research dataset is collected from fig share web
    site. Data is in numerical and alphabetically format. The numerical data set is
    a data set, where the data are expressed in numbers rather than natural language.
    The numerical data is sometimes called quantitative data. Input scenarios for
    faulted and fault-free conditions represented in the data [57]. All data sets
    use condition-based definition for ground truth. All use 1-minute measurement
    frequency so the data sets can be converted into input samples of any time horizon
    larger than 1 minute. 5 Results and discussions The internet of things (IoT) has
    significantly expanded and now permeates many aspects of daily life. Smart health
    and smart education are two examples of IoT environments that are expanding in
    popularity and attention over the past several years. These IoT applications provide
    users with practical services using sensors and actuators. IoT devices have limited
    processing capabilities and are lightweight, making them susceptible to severe
    environments, which frequently results in device failure. In general, IoT experience
    a variety of problems brought on by hardware issues, short battery life, or human
    error, latency and energy consumption. These IoT flaws are more serious and potentially
    dangerous in health care with fog computing setups with integration. Such research,
    however, are inappropriate for real-world IoT in smart health in Fog computing
    setups. An effective approach was proposed to address the problem of latency and
    energy consumption in smart health care system using IoT and fog computing. This
    study suggests some random different methods for fault detection and identification.
    Below, the excellent results of the suggested strategy are discussed. 5.1 Evaluation
    tested Two separate parameters, including time consumption and latency during
    the transactions, are used to assess the suggested approach. In this experiment,
    four end nodes are deployed, including two Raspberry P is that function as IoT
    devices, one laptop that serves as a clinic node and one desktop that serves as
    a hospital node. Other devices function as followers and remote monitoring tools,
    while desktops operate as RPC servers. Details of the evaluation tested for the
    suggested strategy are shown in Table 2. Table 2 Details of the Assessment Tested
    for the Proposed Method Full size table 5.2 Throughput and average latency Two
    crucial factors that aid in more precise evaluation of the blockchain network
    are throughput and latency. For the IoT devices in this study, these parameters
    are determined (Raspberry-Pi). The quantity of successful transactions that are
    committed by the blockchain network is known as transaction throughput. Transactions
    per second (TPS) is the unit used to describe transaction rates. Transaction Throughput
    (TPS) = Total Number of Successful Transaction. The time elapsed between a successful
    transaction and the time of transaction deployment is known as latency. According
    to Equation (5), latency in a transaction is the interval between the successful
    transaction time (k2) and the deployed transaction time (k1). Average Latency
    = Successful Transaction time (k2) − Deployed Transaction Time (k1). The throughput
    and latency of the network have a direct impact on scalability. Scalability is
    immediately impacted by any variance in hardware setup, network speed, and size.
    Because the RPC server is installed in device 1 and 2 and there is no trailing
    data on the same system, just the throughput and latency of IoT devices are calculated
    in this study. To calculate both throughput and the transaction, there are four
    rounds which are mention in Tables 3 and 4 and Fig. 5. Table 3 Time Consumption
    for Data Registered (IoT nodes) in Smart Health System Full size table Table 4
    Time Consumption for Data Request (clinic and hospital nodes) Full size table
    Fig. 5 Time Consumption for Data Registered (IoT nodes) in Smart Health System
    Full size image The estimated time required for data registering IoT devices is
    shown in Table 5. At the first, fiftieth, one hundredth, and hundredth fifty transactions,
    the time taken by the devices is measured, and the average transaction time is
    computed. The average transaction times for devices 1 and 2 are 11.2534 and 12.167
    respectively. Because device 2 has less processing power than device 1, it takes
    longer to complete tasks. In Table 5 at the first, fiftieth, one hundredth, and
    hundredth fifty transactions, the time taken by the devices is measured, and the
    average transaction time is computed. The average transaction times for devices
    3 and 4 are 23.345 and 26.67 respectively. These result are also mentioned in
    Figs. 6 and 4. Table 5 Energy Consumption for the Generation of the Requests Full
    size table Fig. 6 Time Consumption for Data Request (Clinic and Hospital Nodes
    Full size image The time required to process data requests for the clinic and
    hospital nodes is shown in Table 5 Similar to Fig. 7, four separate transaction
    points are used to gauge how much time is spent on data queries. When compared
    to device 1, which takes 11.2534 ms, device 2 takes 12.167 ms less time device
    3, which takes 23.345, device 4, which takes 26.67.The difference in compute power
    is what accounts for the decreased time consumption. In both situations, the computing
    power of the device is directly related to how long it takes to process data requests
    or data that has been registered on the smart health system. On the both condition
    latency base on throughput and time consumption are taken in the given network.
    For devices 1, 2, 3, and 4, the average transaction power is 26.98 mW for the
    first, 50th, 100th, and 150th transactions, respectively (see Tables 3, 4 and
    5). The USB Tester79 and RAPL measurement tool are used to measure the energy
    consumption of devices 1 and 2. IPPET81 and RAPL are used to measure the energy
    consumption of devices 3 and 4. To determine the power consumption in IoT devices,
    the difference between the power consumed by the IoT device at the time of a data
    request and the power consumed by the IoT node in the ideal state is taken into
    account (Fig. 8). Fig. 7 Total Energy Consumption for the Generation of the Requests
    by the Smart Health Full size image Fig. 8 Energy Consumption for the Generation
    of the Requests Full size image 5.3 Energy consumption The bare minimum of time
    required by an IoT node to generate a request for affiliation with the network
    is referred to as energy consumption. The device sends data and is recorded on
    the smart health with the help of fog computing is a successful association which
    is known as energy consumption. Table 6 show the energy consumption for the generation
    of the requests. Table 6 Energy Consumption for the Generation of the Requests
    (2) Full size table Result comparison the proposed technique with the exacting
    technique energy consumption or power consumption in the IoT nodes (Raspberry
    Pi’s), clinic, and hospital node, are considered at the time of generation of
    the data requests. Energy consumption of the device is also measured at the four
    different device and section which are mention in Table 7 and Fig. 9. Table 7
    Fault Detection Rate of Proposed and Existing Technique Full size table Fig. 9
    Fault Detection Rate of Proposed and Existing Technique Full size image The experiment
    was conducted using the raw material that was collected. Data of a 250 Kb size.
    Two methods are used to process the gathered data: an edge gateway and a direct
    connection to the fog server through the small gateway. Both times, the delay
    is calculated while 250 Kb of raw data samples are being transmitted. In the first
    instance, the latency time is quantified as soon as cloud-based patient data is
    accessed. In the second scenario, an edge server that has an embedded socket server
    for local storage installed, and latency then be measured. A different scenario
    is used to measure the latency reduction: the types of data samples, the network
    load, and the quantity of edge devices each resultant table includes a summary
    of the experimental setup and values used to test the results under specific conditions.
    Tables 2, 3, 4, 5, 6, 7 shows that while different kinds of data samples are being
    collected, fog computing can lower the latency between gateways and servers. The
    calculated findings showed that, for data on oxygen saturation, the proposed model''s
    delay is decreased by up to 1.97%. Different network loads and conditions are
    taken into account in the second scenario. The internet there are three categories
    for load/conditions: high load, medium load, and low load. Table 8 provides a
    summary of the observed outcomes. The obtained data showed that a heavy load on
    the network reduces latency by 28.98 medium network load results in measured delay
    of .5% while low network load results in observed latency of 1.2%. Finally, changes
    in the number of edge devices in the edge computing layer are used to measure
    delay. The results shown in Table 8 and Fig. 10 showed that the systems latency
    is decreased by an increase of fog nodes due to their quick processing for IoT
    devices. Table 8 Overall Result of Proposed and Existing Technique Full size table
    Fig. 10 Overall Result of Proposed and Existing Technique Full size image 6 Conclusion
    In a culture where digital technology is dominant, the Internet of Things (IoT)
    is essential to our daily life and also important for smart health care system.
    We are surrounded by a loT of different objects or stuff, which is the underlying
    basis of this technology i.e., edge and fog computing. The Internet of Things
    has developed into a platform where devices could become smarter and more information-efficient.
    IoT has several applications. The smart health care system is one of the most
    important and essential applications of the Internet of Things. One of the main
    problems is locating the flaw in Internet of Things (IoT) devices. It is crucial
    to correctly and promptly use energy and improve the latency of the entire system
    in the given smart health system using cloud and fog system. The suggested structure
    consists of two processes, lunacy and energy consumption. In the first phase,
    latency improve using different parches. The energy consumption consists of different
    parameter. The suggested approach has some drawbacks. Algorithm suggested could
    not perform accurately on a large dataset. Try to fix the issues moving forward.
    Limitations the Internet of Things (IoT) has significantly expanded and now permeates
    many aspects of daily life. Over the past few years, IoT environments like smart
    health and smart cities have gained popularity and increased attention. These
    IoT applications provide users with practical services using sensors and actuators.
    Internet of Things has numerous advantages, but it also has certain drawbacks.
    Similar to our suggested strategy, it has several drawbacks. First, the suggested
    algorithm fails when used to huge data sets. A huge dataset must be used for the
    proposed approach to function effectively. Work with large data sets in the future
    and that we can also use for predication and accuracy measurement used. Moreover,
    smart health need more security and data storage because these two section are
    ongoing issue in smart health system taking different technigoies like cloud and
    fog. References Ahmad M, Amin MB, Hussain S, Kang BH, Cheong T, Lee S (2016) Health
    fog: a novel framework for health and wellness applications. J Supercomput 72(10):3677–3695
    Article   Google Scholar   Al-Anzi FS et al (2014) New proposed robust, scalable
    and secure network cloud computing storage architecture 7(05):347 Aburukba RO,
    AliKarrar M, Landolsi T, El-Fakih K (2020) Scheduling Internet of Things requests
    to minimize latency in hybrid Fog–Cloud computing. Futur Gener Comput Syst 111:539–551
    Article   Google Scholar   Breivold HP, Sandström K (2015) Internet of things
    for industrial automation--challenges and technical solutions. in 2015 IEEE International
    Conference on Data Science and Data Intensive Systems. IEEE Biswas AR, Giaffreda
    R (2014) IoT and cloud convergence: Opportunities and challenges. in 2014 IEEE
    World Forum on Internet of Things (WF-IoT). IEEE Currie M, Philip LJ, Roberts
    AJBhsr (2015) Attitudes towards the use and acceptance of eHealth technologies:
    a case study of older adults living with chronic pain and implications for rural
    healthcare 15(1):1–12 Colomo-Palacios R, Fernandes E, Sabbagh M, de Amescua Seco
    A (2012) Human and intellectual capital management in the cloud: software vendor
    perspective. J Univ Comput Sci 18(11):1544–1557 Google Scholar   Dubey H, Yang
    J, Constant N, Amiri AM, Yang Q, Makodiya K (2015) Fog data: Enhancing telehealth
    big data through fog computing. In Proceedings of the ASE bigdata & socialinformatics
    2015 (pp 1–6) Dolui K, Datta SK (2017) Comparison of edge computing implementations:
    Fog computing, cloudlet and mobile edge computing. in 2017 Global Internet of
    Things Summit (GIoTS). IEEE Dzombeta S, Stantchev V, Colomo-Palacios R, Brandis
    K, Haufe K (2014) Governance of cloud computing services for the life sciences.
    IT Professional 16(4):30–37.Farahani, B., et al., Towards fog-driven IoT eHealth:
    Promises and challenges of IoT in medicine and healthcare. 2018. 78:659–676 Ejaz
    M, Kumar T, Kovacevic I, Ylianttila M, Harjula E (2021) Health-blockedge: Blockchain-edge
    framework for reliable low-latency digital healthcare applications. Sensors 21(7):2502
    Article   ADS   PubMed   PubMed Central   Google Scholar   Greco L, Percannella
    G, Ritrovato P, Tortorella F, Vento M (2020) Trends in IoT based solutions for
    health care: Moving AI to the edge. Pattern recognition letters, 135, 346–353.Greenberg,
    A., et al., The cost of a cloud: research problems in data center networks. 2008,
    ACM New York, NY, USA. p 68–73 Gazis V, Goertz M, Huber M, Leonardi A, Mathioudakis
    K, Wiesmaier A, Zeiger F (2015) Short paper: IoT: Challenges, projects, architectures.
    In 2015 18th international conference on intelligence in next generation networks
    (pp 145–147). IEEE Hayyolalam V, Aloqaily M, Ozkasap O, Guizani M (2021) Edge
    intelligence for empowering IoT-based healthcare systems. arXiv preprint arXiv:2103.12144
    He D, Zeadally SJIiotj (2014) An analysis of RFID authentication schemes for internet
    of things in healthcare environment using elliptic curve cryptography 2(1):72–83
    Hou X, Li Y, Chen M, Wu D, Jin D, Chen S (2016) Vehicular fog computing: A viewpoint
    of vehicles as the infrastructures. IEEE Trans Veh Technol 65(6):3860–3873 Article   Google
    Scholar   Henke C, Stantchev V (2009) Human aspects in clinical ambient intelligence
    scenarios. in 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence
    and Intelligent Agent Technology. IEEE Ho KF, Hirai HW, Kuo YH, Meng HM, Tsoi
    KK (2015) Indoor air monitoring platform and personal health reporting system:
    big data analytics for public health research. In 2015 IEEE International Congress
    on Big Data (pp 309–312). IEEE Indrawan-Santiago M (2020) Proceedings of the 22nd
    International Conference on Information Integration and Web-based Applications
    & Services. 2020: Association for Computing Machinery Kraemer FA, Braten AE, Tamkittikhun
    N, Palma D (2017) Fog computing in healthcare–a review and discussion. IEEE Access
    5:9206–9222 Article   Google Scholar   Kumari A, Tanwar S, Tyagi S, Kumar N (2018)
    Fog computing for Healthcare 4.0 environment: Opportunities and challenges. Comput
    Electr Eng 72:1–13 Article   Google Scholar   Krallmann H, Schröpfer C, Stantchev
    V, Offermann P (2008) Enabling autonomous self-optimisation in service-oriented
    systems. In Autonomous systems–self-organization, management, and control (pp
    127–134). Springer, Dordrecht Kosta S, Aucinas A, Hui P, Mortier R, Zhang X (2012)
    Thinkair: Dynamic resource allocation and parallel execution in the cloud for
    mobile code offloading. In 2012 Proceedings IEEE Infocom (pp 945–953). IEEE Kadhim
    QK, Yusof R, Mahdi HS, Al-Shami SSA, Selamat SR (2018) A review study on cloud
    computing issues. In Journal of Physics: Conference Series (Vol. 1018, No. 1,
    p 012006). IOP Publishing Li J, Cai J, Khan F, Rehman AU, Balasubramaniam V, Sun
    J, Venu P (2020) A secured framework for sdn-based edge computing in IOT-enabled
    healthcare system. IEEE Access 8:135479–135490 Article   Google Scholar   Li Y,
    Wang W (2014) Can mobile cloudlets support mobile applications? in IEEE INFOCOM
    2014-IEEE Conference on Computer Communications. IEEE Maiti P, Apat HK, Sahoo
    B, Turuk AK (2019) An effective approach of latency-aware fog smart gateways deployment
    for IoT services. Internet of Things 8:100091 Article   Google Scholar   Marín-Tordera
    E, Masip-Bruin X, García-Almiñana J, Jukan A, Ren GJ, Zhu J (2017) Do we all really
    know what a fog node is? Current trends towards an open definition. Comput Commun
    109:117–130 Article   Google Scholar   Masip-Bruin X, Marín-Tordera E, Alonso
    A, Garcia J (2016) Fog-to-cloud Computing (F2C): The key technology enabler for
    dependable e-health services deployment. In 2016 Mediterranean ad hoc networking
    workshop (Med-Hoc-Net) (pp 1–5). IEEE Mahmud R, Ramamohanarao K, Buyya RJAToIT
    (2018) Latency-aware application module management for fog computing environments
    19(1):1–21 Mell P, Grance T (2011) The NIST definition of cloud computing Mao
    Y, You C, Zhang J, Huang K, Letaief KB (2017) Mobile edge computing: Survey and
    research outlook. arXiv preprint arXiv:1701.01090 Medina V, García  JMJACS (2014)
    A survey of migration mechanisms of virtual machines 46(3):1–33 Maier MV (2016)
    The Internet of Things (IoT): what is the potential of Internet of Things applications
    for consumer marketing?, University of Twente Monteiro A, Dubey H, Mahler L, Yang
    Q, Mankodiya K (2016) Fit: A fog computing device for speech tele-treatments.
    In 2016 IEEE international conference on smart computing (SMARTCOMP) (pp 1–3).
    IEEE Ngu AH, Gutierrez M, Metsis V, Nepal S, Sheng QZ (2016) IoT middleware: A
    survey on issues and enabling technologies. IEEE Internet Things J 4(1):1–20 Article   Google
    Scholar   Obaid W, Farag MM, Hamid AK (2022) Smart Information Recognition on
    COVID-19 APPs for User Health Identification. in 2022 Advances in Science and
    Engineering Technology International Conferences (ASET). IEEE Pareek K, Tiwari
    PK, Bhatnagar V (2021) Fog Computing in Healthcare: A Review. in IOP Conference
    Series: Materials Science and Engineering. IOP Publishing Porter ME, Heppelmann
    JEJHbr (2014) How smart, connected products are transforming competition 92(11):64–88
    Petruch K, Stantchev V, Tamm G (2011) A survey on IT-governance aspects of cloud
    computing. Int J Web Grid Serv 7(3):268–303 Article   Google Scholar   Rahmani
    AM, Gia TN, Negash B, Anzanpour A, Azimi I, Jiang M, Liljeberg P (2018) Exploiting
    smart e-Health gateways at the edge of healthcare Internet-of-Things: A fog computing
    approach. Futur Gener Comput Syst 78:641–658 Article   Google Scholar   Shukla
    S, Hassan MF, Khan MK, Jung LT, Awang A (2019) An analytical model to minimize
    the latency in healthcare internet-of-things in fog computing environment. PLoS
    ONE 14(11) Article   CAS   PubMed   PubMed Central   Google Scholar   Singh A,
    Chatterjee K (2021) Securing Smart Healthcare System with Edge Computing. Computers
    & Security 102353 Shi W, Dustdar SJC (2016) The promise of edge computing 49(5):78–81
    Stantchev V, Malek M (2009) Translucent replication for service level assurance,
    in High assurance services computing. Springer. p 1–18 Stantchev V, Schröpfer
    C (2009) Negotiating and enforcing qos and slas in grid and cloud computing. in
    International Conference on Grid and Pervasive Computing. Springer Shi W, Cao
    J, Zhang Q, Li Y, Xu L (2016) Edge computing: Vision and challenges. IEEE Internet
    Things J 3(5):637–646 Article   Google Scholar   Stantchev V (2009) Performance
    evaluation of cloud computing offerings. in 2009 Third International Conference
    on Advanced Engineering Computing and Applications in Sciences. IEEE Stantchev
    VJICSI (2008) Berkeley, California, Effects of replication on web service performance
    in WebSphere. 94704: p 2008–03 Stantchev V, Malek M (2008) Addressing web service
    performance by replication at the operating system level. in 2008 Third International
    Conference on Internet and Web Applications and Services. IEEE Sabaté E, Sabaté
    E (2003) Adherence to long-term therapies: evidence for action. World Health Organization
    Alam T, Gupta R, Qamar S, Ullah A (2022) Recent applications of Artificial Intelligence
    for Sustainable Development in smart cities. In Recent Innovations in Artificial
    Intelligence and Smart Applications (pp 135–154). Cham: Springer International
    Publishing Ullah A, Chakir A (2022) Improvement for tasks allocation system in
    VM for cloud datacenter using modified bat algorithm. Multimedia Tools and Applications
    81(20):29443–29457 Article   PubMed   PubMed Central   Google Scholar   Ullah
    A, Nawi NM (2021) An improved in tasks allocation system for virtual machines
    in cloud computing using HBAC algorithm. Journal of Ambient Intelligence and Humanized
    Computing 1–14 Ouhame S, Hadi Y, Ullah A (2021) An efficient forecasting approach
    for resource utilization in cloud data center using CNN-LSTM model. Neural Comput
    Appl 33:10043–10055 Article   Google Scholar   Ouhame S, Hadi Y (2020) A Hybrid
    Grey Wolf Optimizer and Artificial Bee Colony Algorithm Used for Improvement in
    Resource Allocation System for Cloud Technology. Intl J Online Biomed Eng 16(14)
    Ogbuke N, Yusuf YY, Gunasekaran A, Colton N, Kovvuri D (2023) Data-driven technologies
    for global healthcare practices and COVID-19: opportunities and challenges. Ann
    Operations Res 1–36 Nasralla MM, Khattak SBA, Ur Rehman I, Iqbal M (2023) Exploring
    the Role of 6G Technology in Enhancing Quality of Experience for m-Health Multimedia
    Applications: A Comprehensive Survey. Sensors 23(13):5882 Article   ADS   PubMed   PubMed
    Central   Google Scholar   Chaudhury S, Dhabliya D, Madan S, Chakrabarti S (2023)
    Blockchain Technology: A Global Provider of Digital Technology and Services. In
    Building Secure Business Models Through Blockchain Technology: Tactics, Methods,
    Limitations, and Performance (pp. 168–193). IGI Global Velciu M, Spiru L, Dan
    Marzan M, Reithner E, Geli S, Borgogni B et al (2023) How Technology-Based Interventions
    Can Sustain Ageing Well in the New Decade through the User-Driven Approach. Sustainability
    15(13):10330 Article   Google Scholar   Kumar P, Chauhan S, Awasthi LK (2023)
    Artificial intelligence in healthcare: review, ethics, trust challenges & future
    research directions. Eng Appl Artif Intell 120:105894 Article   Google Scholar   Bonomi
    F, Milito R, Zhu J, Addepalli S (2012) Fog computing and its role in the internet
    of things. In Proceedings of the first edition of the MCC workshop on Mobile cloud
    computing (pp 13–16) Li F, Vögler M, Claeßens M, Dustdar S (2013) Efficient and
    scalable IoT service delivery on cloud. In 2013 IEEE sixth international conference
    on cloud computing (pp. 740–747). IEEE Cao Y, Hou P, Brown D, Wang J, Chen S (2015)
    Distributed analytics and edge intelligence: Pervasive health monitoring at the
    era of fog computing. In Proceedings of the 2015 Workshop on Mobile Big Data (pp.
    43–48) Chen M et al (2018) Edge cognitive computing based smart healthcare system
    86:403–411 Ren J, He Y, Yu G, Li GY (2019) Joint communication and computation
    resource allocation for cloud-edge collaborative system. In 2019 IEEE Wireless
    Communications and Networking Conference (WCNC) (pp 1–6). IEEE Download references
    Author information Authors and Affiliations Department of Computer Science Faculty
    of Computing and Artificial Intelligent Air University, Islamabad, Pakistan Arif
    Ullah School of Computing, Riphah International University, Islamabad, Pakistan
    Saman Yasin Faculty of Computer and Information Systems, Islamic University of
    Madinah, Medina, Saudi Arabia Tanweer Alam Corresponding author Correspondence
    to Arif Ullah. Ethics declarations Competing interests The authors declare that
    they have no known competing financial interests or personal relationships that
    could have appeared to influence the work reported in, this paper. Additional
    information Publisher’s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive
    rights to this article under a publishing agreement with the author(s) or other
    rightsholder(s); author self-archiving of the accepted manuscript version of this
    article is solely governed by the terms of such publishing agreement and applicable
    law. Reprints and permissions About this article Cite this article Ullah, A.,
    Yasin, S. & Alam, T. Latency aware smart health care system using edge and fog
    computing. Multimed Tools Appl 83, 34055–34081 (2024). https://doi.org/10.1007/s11042-023-16899-1
    Download citation Received 21 February 2023 Revised 17 July 2023 Accepted 01 September
    2023 Published 25 September 2023 Issue Date March 2024 DOI https://doi.org/10.1007/s11042-023-16899-1
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Fog Edge computing IoT applications Neural network Healthcare
    applications Use our pre-submission checklist Avoid common mistakes on your manuscript.
    Associated Content Part of a collection: Track 6: Computer Vision for Multimedia
    Applications Sections Figures References Abstract Introduction Related work Working
    of system model Data collection Results and discussions Conclusion References
    Author information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Multimedia Tools and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Latency aware smart health care system using edge and fog computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Pan S.
  - Huang C.
  - Fan J.
  - Shi Z.
  - Tong J.
  - Wang H.
  citation_count: '0'
  description: In the era of continuous development in Internet of Things (IoT) technology,
    smart services are penetrating various facets of societal life, leading to a growing
    demand for interconnected devices. Many contemporary devices are no longer mere
    data producers but also consumers of data. As a result, massive amounts of data
    are transmitted to the cloud, but the latency generated in edge-to-cloud communication
    is unacceptable for many tasks. In response to this, this paper introduces a novel
    contribution—a layered computing network built on the principles of fog computing,
    accompanied by a newly devised algorithm designed to optimize user tasks and allocate
    computing resources within rechargeable networks. The proposed algorithm, a synergy
    of Lyapunov-based, dynamic Long Short-Term Memory (LSTM) networks, and Particle
    Swarm Optimization (PSO), allows for predictive task allocation. The fog servers
    dynamically train LSTM networks to effectively forecast the data features of user
    tasks, facilitating proper unload decisions based on task priorities. In response
    to the challenge of slower hardware upgrades in edge devices compared to user
    demands, the algorithm optimizes the utilization of low-power devices and addresses
    performance limitations. Additionally, this paper considers the unique characteristics
    of rechargeable networks, where computing nodes acquire energy through charging.
    Utilizing Lyapunov functions for dynamic resource control enables nodes with abundant
    resources to maximize their potential, significantly reducing energy consumption
    and enhancing overall performance. The simulation results demonstrate that our
    algorithm surpasses traditional methods in terms of energy efficiency and resource
    allocation optimization. Despite the limitations of prediction accuracy in Fog
    Servers (FS), the proposed results significantly promote overall performance.
    The proposed approach improves the efficiency and the user experience of Internet
    of Things systems in terms of latency and energy consumption.
  doi: 10.3390/s24041165
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 24 Issue 4 10.3390/s24041165 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editor Charith Perera
    Subscribe SciFeed Recommended Articles Related Info Links More by Authors Links
    Article Views 440 Table of Contents Abstract Introduction Related Works System
    Model Problem Formulation Algorithm Design and Numerical Results Conclusions and
    Future Works Author Contributions Funding Data Availability Statement Conflicts
    of Interest References share Share announcement Help format_quote Cite question_answer
    Discuss in SciProfiles thumb_up Endorse textsms Comment first_page settings Order
    Article Reprints Open AccessArticle Optimizing Internet of Things Fog Computing:
    Through Lyapunov-Based Long Short-Term Memory Particle Swarm Optimization Algorithm
    for Energy Consumption Optimization † by Sheng Pan ‡, Chenbin Huang ‡, Jiajia
    Fan , Zheyan Shi , Junjie Tong and Hui Wang * School of Computer Science and Technology,
    Zhejiang Normal University, Jinhua 321004, China * Author to whom correspondence
    should be addressed. † This paper is an extension of work originally presented
    in the 17th China Conference on Internet of Things (Wireless Sensor Network) (CWSN),
    Dalian, China, 13–15 October 2023. ‡ These authors contributed equally to this
    work. Sensors 2024, 24(4), 1165; https://doi.org/10.3390/s24041165 Submission
    received: 31 December 2023 / Revised: 28 January 2024 / Accepted: 8 February 2024
    / Published: 10 February 2024 (This article belongs to the Special Issue Smart
    Internet of Things (IoT)) Download keyboard_arrow_down     Browse Figures Versions
    Notes Abstract In the era of continuous development in Internet of Things (IoT)
    technology, smart services are penetrating various facets of societal life, leading
    to a growing demand for interconnected devices. Many contemporary devices are
    no longer mere data producers but also consumers of data. As a result, massive
    amounts of data are transmitted to the cloud, but the latency generated in edge-to-cloud
    communication is unacceptable for many tasks. In response to this, this paper
    introduces a novel contribution—a layered computing network built on the principles
    of fog computing, accompanied by a newly devised algorithm designed to optimize
    user tasks and allocate computing resources within rechargeable networks. The
    proposed algorithm, a synergy of Lyapunov-based, dynamic Long Short-Term Memory
    (LSTM) networks, and Particle Swarm Optimization (PSO), allows for predictive
    task allocation. The fog servers dynamically train LSTM networks to effectively
    forecast the data features of user tasks, facilitating proper unload decisions
    based on task priorities. In response to the challenge of slower hardware upgrades
    in edge devices compared to user demands, the algorithm optimizes the utilization
    of low-power devices and addresses performance limitations. Additionally, this
    paper considers the unique characteristics of rechargeable networks, where computing
    nodes acquire energy through charging. Utilizing Lyapunov functions for dynamic
    resource control enables nodes with abundant resources to maximize their potential,
    significantly reducing energy consumption and enhancing overall performance. The
    simulation results demonstrate that our algorithm surpasses traditional methods
    in terms of energy efficiency and resource allocation optimization. Despite the
    limitations of prediction accuracy in Fog Servers (FS), the proposed results significantly
    promote overall performance. The proposed approach improves the efficiency and
    the user experience of Internet of Things systems in terms of latency and energy
    consumption. Keywords: predictive allocation; fog computing; internet of things
    (IoT); system stability; Lyapunov; LSTM; PSO 1. Introduction In recent years,
    the proliferation of the Internet of Things (IoT) has led to a surge in smart
    mobile devices [1]. The advent of IPv6 technology has made everything being network-accessible
    a reality, and the commercialization of 5G technology further accelerated the
    increase in edge devices. With every grain of sand potentially having its own
    IP address, the deployment of numerous sensors to perceive the environment has
    become feasible. IoT analytics data show a growth from 6.1 billion global IoT
    device connections in 2017 to 14.4 billion in 2022, with an 18% increase in 2022
    alone. Shockingly, by 2027, IoT connections might exceed 29 billion. However,
    this rapid expansion puts immense pressure on IoT infrastructure. The massive
    amount of data generated by numerous terminal sensing devices surpasses the processing
    capabilities of end-user devices. While cloud computing significantly eases this
    situation, its latency still fails to meet current user demands. Compared to the
    growth rate of terminal devices, the backbone network’s transmission speed has
    grown less than 10% annually [2]. Hence, public cloud server latency often exceeds
    100 milliseconds [3], which is disadvantageous for delay-sensitive applications.
    Recent advancements in fog computing underscore its revolutionary impact on traditional
    cloud computing. By redistributing computing, storage, and network resources to
    the edge and vicinity of IoT devices, fog computing effectively addresses the
    evolving demands of various applications. This paradigm shift is feasible because
    not all tasks necessitate the potent computing capabilities offered by cloud computing.
    Edge-based data processing and analysis have emerged as critical components of
    fog computing, furnishing real-time decision support and intelligence for IoT
    applications while optimizing latency and bandwidth.Simultaneously, much of the
    current research in fog computing revolves around real-time and near-real-time
    requirements [4]. The widespread adoption of this environment underscores the
    urgency for immediate data processing, emphasizing more than just response time.
    In autonomous driving systems, fog computing substantially reduces the transmission
    time between vehicles and remote cloud servers, achieving an ultra-low latency
    of less than 1 millisecond, as opposed to the 150 milliseconds latency observed
    in cloud computing [5,6]. This highlights the clear advantage of fog computing
    in enhancing the efficiency and responsiveness of real-time applications. Furthermore,
    fog computing, by elevating the Age of Information (AoI), significantly enhances
    the user experience in areas such as online gaming and Virtual Reality (VR). These
    applications fully exploit ultra-low latency, ensuring a seamless and immersive
    experience [7,8]. Consequently, future endeavors will concentrate on designing
    and modeling fog computing environments to achieve near-real-time response times,
    further aligning with the requirements of time-sensitive applications. Task offloading
    involves transferring computational tasks from terminal devices or edge nodes
    to more powerful edge or cloud servers. This occurs when terminal devices or edge
    nodes lack sufficient computational resources or cannot complete specific tasks
    on time. The system must decide whether to transfer these tasks and related data
    to other devices or nodes for processing. While task offloading is a decision
    for the producer or the subsequent handler, resource allocation is a core concern
    for upper-level managers in distributed systems. They must assess the demands
    of various tasks and resources, considering the current state of available resources.
    However, while fog computing enhances task processing capabilities, it also faces
    potential latency increases due to the added burden from connecting more devices.
    In the pursuit of performance and speed, this can lead to device idleness and
    resource wastage. In general models, total energy consumption for data processing
    includes both data transmission and processing energy. Data transmission energy
    depends on the channel status, which is influenced by the transmission time, power,
    and available bandwidth [9]. Higher transmission rates can be achieved by increasing
    power and reducing time but this increases the energy consumption. Balancing transmission
    power and time is key for minimal energy consumption, adjusting rates based on
    task latency requirements [10]. Computing energy is influenced by the computational
    capacity: increased resource input leads to exponential energy growth. Thus, allocating
    tasks to appropriate nodes, ensuring minimal resource input per unit while meeting
    latency requirements, is vital for energy efficiency [11]. Fog computing systems
    enhance decision-making and services by collecting and analyzing data from various
    devices, integrating contextual information. Even if some nodes or links fail,
    other nodes can continue operating. The focus in fog systems is on optimizing
    task processing through fog offloading and fog node resource allocation. This
    approach aims to improve overall resource utilization, thereby enhancing system
    performance and end-user experience. The three key considerations distilled from
    this are as follows: (1) Effective Resource Management in Fog Nodes or Cloud Servers:
    Proper resource scheduling is crucial due to the inevitable presence of idle or
    underutilized nodes. Effective management can significantly amplify the impact
    of fog nodes. (2) Task Time Sensitivity and Priority: Determining task priority
    is vital in current applications. Reasonable decision-making is necessary to prevent
    task stacking and enhance processing efficiency. (3) Dynamic Resource Demand in
    Fog Environments: Fog systems often face dynamic changes in resource demand, which
    can cause instability. Appropriate systemic interventions help reduce overall
    energy consumption and ensure long-term stability. This paper presents our perspectives
    on several key factors in fog computing, with the main contributions summarized
    as follows: Dynamic Fog Federation Formation: Considering the heterogeneity of
    fog device resources and the slower hardware update cycles compared to software,
    we propose a novel approach to establish dynamic fog federations. Utilizing heuristic
    algorithms, specifically Particle Swarm Optimization (PSO), we rapidly sense fog
    processor resources to form federations dynamically. These federations are tailored
    to each round of tasks, focusing on devices with high data correlation and low
    data heterogeneity. Predictive Resource Allocation: We introduce the concept of
    predictive resource allocation, driven by the unique data characteristics of end
    users. This approach dynamically adjusts model training based on outcomes to optimize
    resource allocation for task demands. It not only fulfills the need for tailored
    offloading solutions for different user profiles but also enhances overall performance
    through optimized prediction effectiveness. System Stability and User Experience:
    In consideration of long-term system stability, we analyzed the overall system’s
    stable conditions to enhance the user experience and quality on edge devices,
    thereby optimizing total energy consumption. Model Implementation and Task Prioritization:
    The model, depicted in Figure 1, illustrates how geographically proximate fog
    nodes form clusters. These clusters, possessing varied resources, are unified
    through virtualization technology and managed by a central, high-performance fog
    server. The central server orchestrates the workload distribution between fog
    servers and nodes, prioritizing tasks based on latency constraints. High-priority
    tasks are allocated to more capable fog servers, while medium-priority tasks are
    assigned later. To this end, we propose the Lyapunov-based Dynamic Long Short-Term
    Memory Predictive PSO Algorithm (LDLPSO) for the efficient management and prediction
    of task and resource allocation. Figure 1. System architecture diagram. The structure
    of the paper is organized as follows. Section 2 introduces related work and research
    motivations. Section 3 describes the system model, including the transmission
    model, prediction model, priority assessment, energy consumption model, and algorithm
    design. Section 4 discusses the reasoning and feasibility of system energy consumption
    optimization. Section 5 presents the simulation results, analyzing the performance
    of the proposed allocation protocol compared to other allocation protocols. Section
    6 outlines the conclusions of this paper. 2. Related Works Since its inception
    in 2012 and formal definition in 2018 [12], fog computing has been the subject
    of extensive research efforts focused on enhancing system performance. The core
    of these efforts, as indicated in references [13,14,15,16], centers on optimizing
    task offloading and resource scheduling within fog computing systems. As demonstrated
    by references [10,17,18,19,20,21,22,23,24,25], these studies revolve around the
    stability of systems with limited computational resources and energy. Subsequently,
    within [1,2], there is a shift from passive to proactive, attempting to anticipate
    task request volumes and preparing for them in advance based on responses. Although
    these studies were published at different times, the direction of fog computing
    research is becoming more refined. 2.1. Works Focused on Optimizing Task Offloading
    J. Flinn proposed that task offloading decisions should hinge on the resources
    accessible on mobile devices and the resources required for task execution [13].
    Mainak Adhikari and his team’s research in fog computing, focusing on efficient
    IoT task scheduling and processing, inspired our work. They prioritized tasks
    based on deadlines and utilized multi-level feedback queues for appropriate device
    allocation [14]. Shichao Guan and colleagues developed an active hybrid offloading
    model for sustainable and heterogeneous offloading management in active cloudlets.
    This model, aimed at energy and QoS-aware heterogeneous offloading and resource
    allocation, adapts to various task and resource types [15]. It collaborates with
    cloud resources for partition and migration-based offloading, guided by task load
    and QoS demands. Xu Chen et al. investigated decentralized computing offloading
    games in mobile cloud computing environments, aiming for effective dispersed offloading
    via game-theoretic methods. Their approach focuses on the coordination of task
    offloading among mobile cloud users to enhance system performance and efficiency,
    reducing centralized management complexity and enabling self-organized decision-making
    among users [16]. The methods mentioned above revolve around task offloading and
    resource allocation. Multi-level feedback queues, lateral migration offloading,
    and decentralized management all provide references and inspiration for our subsequent
    work. However, their methods are limited by static resource availability, do not
    consider energy consumption, and are not suitable for delay-sensitive applications.
    On our study, we re-designed the way fog nodes provide computing resource services.
    The task-centric combination of fog computing allows for the dynamic adjustment
    of computing resources, considering low-power fog processors, thereby reducing
    the overall energy consumption. 2.2. Works Focused on Optimizing Energy Consumption
    With the improvement in fog computing system performance, the issue of energy
    constraints on fog devices cannot be overlooked. Guowei Zhang and colleagues proposed
    an energy-minimizing task offloading algorithm centered on fairness. This approach
    focuses on three main aspects: the energy consumption of task offloading, the
    historical average energy consumption of fog nodes, and node priority [17]. They
    aim for fairness and minimized energy consumption in fog computing IoT environments
    by optimizing target fog nodes, transmission power, and sub-task sizes. Jianbo
    Du and colleagues developed the CORA algorithm, addressing computation offloading
    and resource allocation in hybrid fog–cloud computing systems. This algorithm
    aims to minimize the maximum weighted delay and energy consumption cost among
    user devices, ensuring user fairness and tolerable delay [18]. CORA coordinates
    offloading decisions and resource allocation, like computational resources, transmission
    power, and wireless bandwidth. Yu Qiu and colleagues focused on cost-effective
    optimization in FogC-IoMT (Fog Computing-based Internet of Medical Things) for
    healthcare monitoring [19]. They addressed challenges like time sensitivity, energy
    limitations, quality of service, and wireless constraints, breaking the problem
    into three sub-problems: medical task offloading, sub-channel allocation, and
    power distribution. They developed a sub-optimal, low-complexity algorithm to
    reduce energy consumption and transmission delay. Arash Bozorgchenani et al. proposed
    a method for multi-objective computation sharing in an energy- and delay-constrained
    mobile edge computing environment. By designing an evolutionary algorithm (NSGA2),
    they efficiently found the optimal balance between energy consumption and task
    processing delay, achieving minimal energy consumption and minimal processing
    delay for tasks [20]. The inspiration from the above works was to decompose complex
    problems into smaller ones, considering the joint optimization of energy consumption
    and task processing delay. However, their research did not take into account the
    performance differences between fog devices and overlooked the dynamic nature
    of fog computing systems. Of course, to design a more robust system, it is necessary
    to consider the queue stability of joint optimization. Therefore, in our subsequent
    related work, we studied Lyapunov system control theory. Maganti Venkatesh et
    al. proposed an innovative deep learning mechanism, focusing on addressing workload
    balancing issues in fog computing [21]. By appropriately allocating workloads,
    they aimed to meet the QoS requirements of delay-sensitive IoT applications as
    much as possible. They considered workload distribution issues between fog nodes
    and the cloud, analyzing the stability of the IoT–fog–cloud queue scheme using
    Lyapunov drift and penalty theory. Karimiafshar et al. proposed the use of Lyapunov
    optimization techniques to ensure system stability while minimizing energy consumption
    and deadline misses in the deployment of fog computing resources in industrial
    IoT networks [10]. Yang Cai and Llorca, among others, made significant contributions
    to the efficient delivery of emerging distributed cloud architectures (such as
    fog and mobile edge computing) in real-time stream processing applications [22].
    We also previously conducted a study on related topics. Huang et al. proposed
    a heuristic particle swarm optimization algorithm based on the Lyapunov optimization
    framework for resource scheduling and energy consumption optimization in fog computing
    [23]. They designed a novel queuing system that allows scheduling packets based
    on the current destination set, and developed the first fully decentralized, throughput,
    and cost-optimal multi-cast flow control algorithm using Lyapunov drift and penalty
    control theory. Their methods reflect considering the fog computing system from
    the perspective of system dynamics, which is achieved by controlling the stability
    of each optimization target queue. However, their research still deals with passive
    systems and can only be considered quasi real-time. Subsequently, in order to
    improve the accuracy and scientific validity of our experiments, we scrutinized
    some experimental models, power consumption calculations, and fog server mode
    properties. Minghong Lin’s team addressed energy cost issues in data centers,
    proposing a model for dynamically adjusting the data center size to save costs.
    This model, based on an online algorithm for dynamic right-sizing, suggests shutting
    down servers during low-load periods to achieve significant cost savings, assuming
    perfect future workload prediction, which is a challenge in practical applications
    [24]. Y. Kim’s team used Monsoon power monitors to measure the power consumption
    of Galaxy Note smartphones with LTE chipsets across various mobile scenarios and
    Korean network operators. They fitted parameters to a typical CPU energy model,
    providing a basis for computational energy consumption and simulation in fog federations,
    highlighting the complexity and sensitivity to dynamic network states in their
    proposed strategy for balancing cost and latency in mobile edge computing environments
    [25]. 2.3. Works Focused on Prediction In order to continue to improve the speed
    in processing latency, people find another way toward predictive algorithm research,
    trying to analyze historical data to predict the amount of task data, and prepare
    for resource allocation in advance. Zhibo Li and colleagues developed a load prediction
    method for hybrid Mobile Edge Computing (MEC) servers using Bidirectional Long
    Short-Term Memory Networks (BILSTM). Known for its high predictive accuracy and
    minimal parameter configuration, this method is employed to optimize computing
    task offloading decisions, aiming to reduce task response times [2]. Despite enhanced
    predictive performance, their approach does not fully consider the time dependency
    of MEC server performance indicators and load levels, nor the resource energy
    consumption in practical applications. This suggests that, while improving prediction
    accuracy is crucial, balancing performance and resource efficiency in real-world
    deployment is also essential. Xin Gao and colleagues focused on dynamic offloading
    and resource allocation in multi-tier fog computing systems, based on traffic
    prediction. They introduced a stochastic network optimization problem aimed at
    minimizing the time-average power consumption while maintaining system stability
    [1]. Although their design reduced the average delay via a dual-layer fog structure,
    it did not fully address the general settings, task priorities, or resource availability,
    nor did it elaborate on how the prediction mechanism enhances system performance.
    The high dependency on prediction outcomes suggests a potential direction for
    future research. Addressing the limitations identified in previous studies, this
    paper introduces the LDLPSO algorithm, designed to maximize the benefits of low-performance
    devices in a layered fog structure. It dynamically learns from historical data
    and assesses current states to predict data flows and prepare responses in advance,
    ensuring real-time task processing even under resource constraints. The optimized
    heuristic algorithm enables rapid real-time resource sensing and efficient resource
    allocation. It also reduces the overall energy consumption by integrating and
    utilizing resources effectively. Overall, there is still relatively little research
    on predictive offloading in fog computing systems. We compare this paper with
    the works in Table 1. Table 1. Comparisons of related works. 3. System Model In
    this chapter, we will introduce the system’s topology, environment, and the technologies
    implemented to meet the required specifications. In Section 3.1, we discuss the
    system’s physical model and structure. In Section 3.2, we describe in detail the
    transmission model and the overall queueing model for tasks during transmission,
    including the evolution of predicted and actual queues. In addition, details related
    to the uplink delay are shown. Section 3.3 explains the implementation principles
    of the prediction model and its evolution in the system. Finally, in Section 3.4,
    we provide a detailed description of the energy consumption calculation details
    for the entire system. Of course, some related formula symbols can be analyzed
    in detail in Table 2. Table 2. List of symbols. Among these, the FS (Fog Server)
    is a core component of this system, and our algorithm will be deployed at this
    structural level. The FS is not only responsible for predicting the demand for
    computing resources but also for transmitting received data to the corresponding
    Fog Processor Nodes (CFCN) cluster. The following are the three tasks that the
    FS needs to perform: (1) FS records this data as both the training and testing
    datasets for predictive functions.The dataset continuously updates with the ongoing
    reception by the FS. Therefore, during the dynamic process of predicting results,
    the model iterates gradually; (2) Based on predicted values, we use the Particle
    Swarm Optimization algorithm (PSO) to rapidly search for suitable nodes or clusters
    of nodes and formulate a solution in advance; (3) Finally, by comparing predicted
    results with actual results, adjustments to the allocation plan continue for those
    not in line with real values, while data that are in line are directly distributed
    to the corresponding Fog Processor Nodes (CFCN) cluster according to the plan.
    In conclusion, the FS plays a crucial role in the model, dynamically predicting
    upcoming task loads in real time and allocating idle resources reasonably to cope
    with these tasks. 3.1. System Basic Elements In the proposed system architecture,
    the fog computing layer adopts a two-layer structure to meet the data processing
    needs from K edge devices (represented as 𝐼 1 , 𝐼 2 ,…, 𝐼 𝑘 ). Edge devices generate
    data and transmit them to the fog layer wirelessly. The results are then subsequently
    obtained from it. (1) Fog Layer 1: Comprising Fog Servers (FS), it is responsible
    for coordinating task assignment and scheduling. FS manages the status of each
    Fog Computing Node (FCN) in the next layer, recording the power consumption and
    computing capacity. When data from IoT devices reach this layer, the FS generates
    a cluster of Fog Processor Nodes (CFCN) specific to the tasks and allocates data
    to them for processing. The FS dynamically predicts the task load in real time
    and designs the allocated resource quantity; (2) Fog Layer 2: Comprising N different
    FCNs, each represented as 𝐹 𝑛 with n ranging from 1 to N. These nodes, arranged
    according to the fog processor 𝐹 𝑠 , dynamically combine into a cluster of Fog
    Processor Nodes (CFCN) to ensure a low power consumption and high-quality task
    completion. It is crucial to emphasize that the Fog Server (FS) serves as a critical
    link between edge devices and fog processors, which are typically connected through
    wired connections. In summary, the Fog Server (FS) serves as a crucial link between
    edge devices and fog processors, which are typically connected through wired connections.
    The FS comprehensively understands the state of the fog layer, determining the
    optimal allocation of tasks among CFCNs to ensure the efficient utilization of
    computing resources and energy. Although we considered the cloud, this design
    did not allow us to study much about the offloading strategy between cloud and
    fog systems. The specific system model is illustrated in Figure 1. 3.2. Transmission
    Model and Task Queue To enhance the realism of the transmission channel emulation,
    we incorporated a channel capacity calculation based on the Shannon capacity formula.
    This calculation quantifies the channel capacity between IoT devices and servers
    in Fog Layer 1. The length of each time slot for the wireless channel is denoted
    as 𝜏 , which remains constant within time slot t and may vary across different
    time slots. The wireless channel’s gain experiences decay over time, with decay
    power represented by 𝑆 𝑘 (𝑡) . The transmission delay C 𝑘,𝑠 (𝑡) is determined
    by the following formula [23]: C 𝑘,𝑠 (𝑡)=𝑊𝜏 log 2 (1+ 𝑃 𝑘,𝑠(𝑡) 𝑆 𝑘,𝑠(𝑡) 𝑊𝜎 ) (1)
    where W signifies the wireless bandwidth; 𝜏 denotes the time slot length; 𝑃 𝑘,𝑠
    (𝑡) represents the transmitted power from IoT device k to 𝐹 𝑆 𝑠 at time slot t;
    𝑆 𝑘,𝑠 (𝑡) indicates the fading gain of the wireless channel used by IoT device
    k to 𝐹 𝑆 𝑠 ; and 𝜎 signifies the noise power. In our system, a many-to-one relationship
    exists between user devices and fog servers. Task request intervals from user
    devices follow a random exponential distribution, consequently also making the
    service time provided by fog servers to user devices random and follow an exponential
    distribution. For example, when an IoT device sends a data packet to a fog server
    with no ongoing tasks, the server immediately handles the task. Fog server nodes
    primarily handle allocation and management, where task arrivals and processing
    are independent, adhering to the queuing theory M/M/1 model [26]. The queuing
    delay 𝑊 𝑞 can be calculated using the following formula: 𝑊 𝑞 = 𝜆 𝜇(𝜇−𝜆) (2) where
    𝜆 represents the task arrival rate and 𝜇 represents the service rate. Considering
    transmission delay further, we propose the transmission delay formula 𝜏 𝑐 [23]:
    𝜏 𝑐 = 1 𝐶 𝑘,𝑠 (𝑡) 𝛼 𝑘 (𝑡) 𝐷 𝑘 (𝑡) − 𝛼 𝑘 (𝑡) 𝜆 𝑘 (𝑡) (3) where 𝜏 𝑐 represents transmission
    delay; 𝛼 𝑘 (𝑡) represents the offloading ratio; 𝐷 𝑘 (𝑡) represents the average
    data size of device k tasks; and 𝐶 𝑘,𝑠 (𝑡) represents transmission capacity. This
    model comprehensively considers the impact of the offloading ratio and task data
    size on the transmission delay, providing crucial insights for task scheduling
    and transmission decisions in the system. Between time slots t, we assume 𝐼 𝑖
    (𝑡) ( T i (t)≤ 𝑇𝑚𝑎𝑥 for some constant 𝑇𝑚𝑎𝑥 ) for the IoT device 𝐼 𝑘 arriving at
    𝐹 𝑠 . Tasks usually arrive at different time slots with varying processing sizes.
    Based on this, 𝐹 𝑠 records the information sent by the task and employs an LSTM
    model algorithm to predict the future workload within a prediction window. 𝐹 𝑠
    deploys assigned tasks in advance based on the prediction, resulting in two types
    of queues on 𝐹 𝑠 : (1) Prediction data: P i, W k (t) ; (2) Arrival data: 𝜆 𝑠 𝑟
    (𝑡) ; (3) Offload data: 𝜇 𝑠 𝑟 (𝑡) . Actual tasks arriving at 𝐹 𝑠 are arranged
    in the arrival queue and forwarded to the fog processor for task processing. Local
    processing resources are prioritized for the management of CFCNs. Prediction Queue:
    A k,w (𝑡+1)= A k,w (𝑡)+ 𝜆 𝑠 𝑟 (𝑡) (4) Actual Queue: Q (𝑅) (𝑡+1)=max{ Q (𝑅) (𝑡)+
    𝜆 𝑠 𝑟 (𝑡)− 𝜇 𝑟 (𝑡)− U 𝑟 (𝑡),0} (5) 3.3. Prediction Model Based on the characteristics
    of user devices in heterogeneous networks and data analysis, it appears that a
    single device will periodically deliver packets, while delay-insensitive tasks
    do not require a high quality of service and only need to be completed within
    a given time frame. In this paper, we enhance LSTM by using sliding event windows
    for predicting and estimating resource requests of IoT devices to reduce energy
    loss while ensuring task completion. Long Short-Term Memory (LSTM) networks are
    a variant of Recurrent Neural Networks (RNNs) for processing sequential data,
    designed to address the long-term dependency problem prevalent in RNNs, all of
    which have a recurrent neural network module in the form of chains. In standard
    RNNs, this recurrent structural module only has a fairly common structure, such
    as a tanh layer. As the data are transformed while traversing the RNN, some information
    will be discarded at each time slot. After a period of time, the state of the
    RNN is almost devoid of any trace of the initial input. Therefore, when the conventional
    neurons are replaced by memory units, the LSTM will retain long-term information
    to some extent and solve the gradient explosion problem. The memory unit consists
    of three main controllers: the forgetting gate, input gate, and output gate. The
    model of LSTM is shown in Figure 2. Figure 2. The structure of LSTM. Data predictability
    typically requires a discernible pattern or trend, enabling statistical analysis
    and forecasting. Historical data are crucial for understanding system behavior,
    with data quality determining predictive model accuracy. Adequate computational
    resources are necessary for processing and analyzing large datasets. Stable systems
    or processes, with consistent behavior patterns, are more predictable. However,
    such stability is not always present in IoT edge computing. Certain sensors may
    operate stably over time, like those in environmental monitoring or surveillance.
    Encapsulating data formats and using dynamic LSTM models can stabilize predictions,
    optimizing performance and resource consumption. For constantly changing scenarios,
    a sliding window approach (initially set as Wk) reduces interference from large
    content variations. This dynamic training method sacrifices some training time
    for improved outcomes, applying predictions to resource pre-allocation when success
    rates reach a certain threshold. 3.4. Energy Model The total power consumption
    P(t) of fog tiers in time slot t consists of the processing power consumption
    and wireless transmit power consumption, where the processing work number, in
    turn, contains the distribution power consumption of the fog server and the computational
    power consumption of the fog processor, given a local CPU with frequency f. In
    the previous section, the task queue in the energy consumption model for task
    processing follows the M/M/1 pattern. The local computational delay 𝜏 𝑖 can be
    expressed using the following formula: 𝜏 i = 𝜆 𝑖 (𝑡)( 𝜎 F 𝑖 2 + 𝒟 2 𝑖 ) 2( ℱ 2
    𝑖 − 𝜆 𝑖 (𝑡) ℱ 𝑖 𝐷 𝑖 ) + 𝒟 𝑖 ℱ 𝑖 (6) where 𝜆 denotes the probability of task arrival;
    𝜎 represents the standard deviation of the task size; and 𝐹 𝑖 denotes the computational
    resources of i. The total energy consumed in performing a task is, therefore,
    represented as 𝑃 𝑙 𝑖 : 𝑃 𝑙 𝑖 = ∑ 𝑖=1 𝑁 𝑝 𝑖 (𝑡) (7) Therefore, in our system, delay-sensitive
    tasks are allocated processing based on computational requirements: a perception
    of the current local resources is made, and if there is the capacity to handle
    them, computational offloading is not considered. Otherwise, the task data that
    need to be processed are allocated according to the current resource status of
    the fog node by the control node, and offloaded to the fog processor layer for
    processing, as shown in Figure 1. The total energy consumption of delay tasks
    can be expressed as: P(𝑡)= ∑ 𝑠=1 𝑆 𝑝 𝑠 (𝑡)+ ∑ 𝑖=1 𝑁 𝑝 𝑐 𝑖,𝑗 (𝑡)+ ∑ 𝑖=1 𝑁 (1−𝛼)
    𝑝 𝑖 (𝑡) (8) where 𝑝 𝑖 (𝑡) represents the energy consumed by fog node s. Y. Kim
    et al. measured the power consumption of a Galaxy Note smartphone equipped with
    an LTE chipset using a Monsoon power monitor for three network operators and various
    mobile scenarios in South Korea by fitting the parameters ( 𝜅 , 𝜙 , 𝜌 ) to a typical
    CPU energy model in [16], where 𝛿 t denotes the duration of one time slot in the
    15th second, as shown below [25]. 𝐸 𝑠 ( 𝑠 𝑖 (𝑡))=(𝜅 ( 𝑠 𝑖 (𝑡)) 𝜑 +𝜌)Δ𝑡. (9) The
    measured energy consumption of the LTE and Wi-Fi networks was 2605 mJ/s and 1225
    mJ/s, respectively, and the CPU energy parameter was ( 𝜅 , 𝜙 , 𝜌 ) = (0.33, 3.00,
    0.10) [24]. So, as described in [27], 𝑝 𝑖 (𝑡) can be expressed as 𝑝 𝑠 (𝑡)= 𝜏 𝑠
    𝜁 ( 𝐹 𝑠 (𝑡)) 3 (10) where 𝜁 is a parameter depending on the deployed hardware
    and is measurable in practice. In this paper, 𝑃 𝑐 𝑖,𝑗 (𝑡)= 𝜏 𝑐 𝑝 𝑐 𝑖 (𝑡) is defined
    as the energy consumption of the device i transmitting tasks to j. M represents
    the set of IoT nodes, and N represents the set of fog nodes. 4. Problem Formulation
    The optimization of total energy consumption is a key concern in this study. We
    adopt the Lyapunov approach to optimize total energy consumption. Ensuring the
    condition that the queue is stable, the entire optimization problem can be transformed
    into a Lyapunov optimization problem. That is, to guarantee the minimum average
    energy consumption per time slot, it can be represented as: p1:& lim 𝑇→∞ 1 𝑇 ∑
    𝑡=0 𝑇−1 P 𝑎 (𝑡) (11) 𝑠.𝑡. lim 𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 ∑ 𝑖=1 𝑁 ℱ 𝑖,𝑠 (𝑡)< ℱ 𝑠 (12) lim
    𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 ∑ 𝑖=1 𝑁 𝒞 𝑖,𝑗 (𝑡)< 𝒞 𝑗 (13) 0≤ 𝑝 𝑖 (𝑡)≤ 𝑃 𝑏 𝑖 (14) 0≤ ℱ 𝑖𝑗 (𝑡)≤
    ℱ 𝑠 (15) 0≤ 𝒞 𝑖𝑗 (𝑡)≤ 𝒞 𝑗 (16) The objective of P1 is to reduce the long-term
    average energy consumption across all IoT devices [28], necessitating full task
    offloading to computing nodes in the fog network, as denoted by 𝐹 𝑎 . Decision
    variables include the partition of computing resources for offloading 𝛼 𝑖 (𝑡),
    𝐹 𝑖 (𝑡) , transmission power per time slot 𝑃 𝑖 (𝑡) , and computational resources
    𝐹 𝑠 of the computing layer nodes, with 𝑖∈𝑁 . These variables are influenced by
    the task size, the required processing latency, and the objective function, under
    constraints of the channel bandwidth, computational resources (P1 and 𝐹 𝑠 ), and
    IoT nodes’ transmission power. It is assumed that all time slots are equal in
    length. P1 also considers limitations on the computing resources allocated by
    device i (15) and device i’s transmission speed (16). Over time, the allocated
    resources must be less than the device’s capacity, which is constrained by 𝐹 𝑠
    . Considering the device’s transmission capability, the bandwidth occupied by
    tasks should be less than the total channel bandwidth, ensuring energy usage remains
    below the battery’s total capacity. Local processing of tasks only accounts for
    energy consumption. 4.1. Setting Up Lyapunov Virtual Pairs of Columns In Lyapunov
    optimization, the satisfaction of long-term average constraints is equated to
    the rate stability of the virtual queue. To be more precise, a virtual queue is
    introduced to replace the computation resource constraint at edge node (11), and
    𝐺(𝑡) represents the random process of the virtual queue length at time slot t.
    The channel constraint is denoted by 𝐶(𝑡) , and the explicit form of 𝐺(𝑡) and
    𝐶(𝑡) can be represented as follows: 𝒢(𝑡+1)−𝒢(𝑡)=max ⎛ ⎝ ⎜ ⎜ ∑ 𝑖=1 𝑁 ℱ 𝑖,𝑠 (𝑡)−
    ℱ 𝑠 ,−𝒢(𝑡) ⎞ ⎠ ⎟ ⎟ (17) ℋ(𝑡+1)=max ⎛ ⎝ ⎜ ⎜ ℋ(𝑡)+ ∑ 𝑖=1 𝑁 𝒞 𝑖,𝑗 (𝑡)− 𝒞 𝑗 ,0 ⎞ ⎠
    ⎟ ⎟ (18) This study assumes a stable environment, with a consistent transmission
    power from the charging device. The estimated value of the charging amount 𝑃 𝑜𝑝𝑡
    𝑖 (𝑡) can be expressed as: 𝑃 𝑜𝑝𝑡 𝑖 (𝑡)= 1− 𝑒 −𝜓(𝑡)𝑑 (𝜓(𝑡)𝑑) 2 (19) where d denotes
    the distance between the sender and the receiver, and 𝜓 denotes the decay factor,
    often determined by the signal frequency and medium characteristics. If the virtual
    queue is rate-stable, only lim 𝑇→∞ A(T) T can meet constraint (11) according to
    the definition in [29]. Similarly, we can derive channel backlog virtual queue
    B, delay backlog virtual queue C, and edge node energy backlog virtual queue D.
    The proof is as follows: First, we prove the stability of the virtual queue: 𝒢(𝑡+1)−𝒢(𝑡)=max
    ⎛ ⎝ ⎜ ⎜ ∑ 𝑖=1 𝑁 ℱ 𝑖,𝑠 (𝑡)− ℱ 𝑠 ,−𝒢(𝑡) ⎞ ⎠ ⎟ ⎟ (20) For 𝔱∈(0,1,2,…,T−1) , there
    exists lim 𝒯→∞ 𝒢(𝑇)−𝒢(0) 𝑇 ≥ lim 𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 ∑ 𝑖=1 𝑁 ℱ 𝑖,𝑠 (𝑡)− ℱ 𝑠 (21)
    If A(0) = 0, then lim 𝖳→∞ 𝒢(𝑇) 𝑇 =0 (22) Hence, there is: lim 𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1
    ∑ 𝑖=1 𝑁 ℱ 𝑖,𝑗 (𝑡)≤ ℱ 𝑗 (23) Therefore, queue A is stable, and similarly, virtual
    queue B(t) is stable, yielding: lim 𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 ∑ 𝑖=1 𝑁 𝐶 𝑖,𝑗 (𝑡)≤ 𝐶 𝑗 (24)
    After proving the above, we can translate problem P1 into problem P2. 4.2. Constructing
    the Lyapunov Function In this subsection, the specific Lyapunov derivation process
    and the translation of problem P1 into problem P2 will be shown, as represented
    by the following equations: 𝑃2: lim 𝑇→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 𝒫 𝑎 (𝑡) s.t. 𝐺(𝑡) is rate stable
    𝐻(𝑡) is rate stable (14)–(16) (25) Equations (14)–(16) set up the virtual queue
    vector, with the system state represented as 𝜃(𝑡) : Θ(𝑡)=[𝒢(𝑡),ℋ(𝑡)] (26) ℒ(𝜃(𝑡))
    = 1 2 ∑ 𝑖=1 𝑄 𝑖 (𝑡) 2 = 1 2 (𝒢 (𝑡) 2 +ℋ (𝑡) 2 ) (27) Δ𝜃(𝑡)=𝐸(𝐿(𝜃(𝑡+1))−𝐿(𝜃(𝑡))∣𝜃(𝑡))
    (28) When the rate in each queue is stable, problem P2 can be transformed into
    problem P3, because the original problem with long-term average objectives and
    constraints can be approximately transformed into a problem with drift plus penalty.
    In this paper, it is assumed that: 𝑃(𝑡)= ∑ 𝑡=0 𝑇−1 𝑃 𝑎 (𝑡) (29) Based on the Lyapunov
    drift plus penalty algorithm, it can be converted into problem P3 as shown below:
    P3: 𝑠.𝑡. minΔ𝜃(𝑡)+𝑉𝐸(𝑃(𝑡)∣𝜃(𝑡)) (12),(13) (30) where V represents the weight of
    the objective function, and Δ𝜃(𝑡) represents the drift of the queue, i.e., the
    stability of the queue. The following shows the specific form of A to prepare
    for the upper bound of P4. First, we show 𝒢 (𝑡+1) 2 −𝒢 (𝑡) 2 : 𝖦 (𝑡+1) 2 −𝖦 (𝑡)
    2 =max ⎛ ⎝ ⎜ ⎜ G(𝑡)+ ∑ 𝑖=1 𝑁 F 𝑖,𝑠 (𝑡)− F 𝑠 ,0 ⎞ ⎠ ⎟ ⎟ 2 −G (𝑡) 2 ≤2G(𝑡) ⎛ ⎝ ⎜
    ⎜ ∑ 𝑖=1 𝑁 F 𝑖,𝑠 (𝑡)− F 𝑠 ⎞ ⎠ ⎟ ⎟ + ⎛ ⎝ ⎜ ⎜ ∑ 𝑖=1 𝑁 F 𝑖,𝑠 (𝑡)− F 𝑠 ⎞ ⎠ ⎟ ⎟ 2 ≤2𝖦(𝑡)
    ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡)−2𝖦(𝑡) 𝖥 𝑠 + ⎛ ⎝ ⎜ ⎜ ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡) ⎞ ⎠ ⎟ ⎟ 2 −2 𝖥 𝑠 ∑ 𝑖=1
    𝑁 𝖥 𝑖,𝑠 (𝑡)+ 𝖥 2 𝑠 ≤2(𝖦(𝑡)− 𝖥 𝑠 ) ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡)+ ⎛ ⎝ ⎜ ⎜ ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡)
    ⎞ ⎠ ⎟ ⎟ 2 + 𝖥 2 𝑠 ≤2(𝖦(𝑡)− 𝖥 𝑠 ) ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡)+(𝑁+1) 𝖥 𝑠 2 = 𝒟 1 +2(𝖦(𝑡)−
    𝖥 𝑠 ) ∑ 𝑖=1 𝑁 𝖥 𝑖,𝑠 (𝑡) (31) Here, the fixed value is represented by 𝒟 1 : 𝒟 1
    =(𝒩+1) ℱ 2 𝑠 (32) Similarly, the virtual queue B under bandwidth constraints can
    be represented as: ℋ (𝑡+1) 2 −ℋ (𝑡) 2 = 𝒟 2 +2(ℋ(𝑡)− 𝒞 𝑗 ) ∑ 𝑖=1 𝑁 𝐶 𝑖,𝑗 (𝑡) (33)
    where 𝒟 2 represents a fixed constant and is expressed by the following formula:
    𝒟 2 =(𝒩+1) 𝒞 2 𝑗 (34) Problem P3 can be expanded to problem P4: P4: min𝑉𝑃(𝑡)+(G(𝑡)−
    F 𝑠 ) ∑ 𝑖=1 𝑁 F 𝑖,𝑠 (𝑡)+(H(𝑡)− 𝐶 𝑗 ) ∑ 𝑖=1 𝑁 𝐶 𝑖,𝑗 (𝑡) s.t. (14),(15),(16) (35)
    To prove that virtual queues G and H are stable within the context of problem
    P4, where P4 has an upper bound constant C, the proof involves demonstrating that
    these queues do not grow indefinitely over time. This is typically achieved by
    showing that the long-term average inflow rate to each queue is less than or equal
    to its outflow rate. By establishing this condition, it can be inferred that the
    queues will remain bounded and thus stable, ensuring that the system operates
    within its defined constraints. The proof is as follows: ℒ(𝜃(𝑡+1))−ℒ(𝜃(𝑡))+𝑉𝑃(𝑡)≤𝐶
    ℒ(𝜃(𝑡+1))−ℒ(𝜃(𝑡)) ℒ(𝜃(𝑇))−ℒ(𝜃(0)) ≤𝐶 ≤𝑇𝐶 (36) This is obtained by making ℒ(𝜃(0))=0
    : 1 2 (𝒢 (𝑡) 2 +ℋ (𝑡) 2 ) G (𝑡) 2 G(𝑡) ≤𝑇𝐶 ≤2𝑇𝐶 ≤ 2𝑇𝐶 − − − − √ (37) Thus, continuing
    the derivation yields: lim 𝑇→∞ 𝒢(𝑇) 𝑇 ≤ lim 𝑇→∞ 2𝑇𝐶 − − − − √ 𝑇 =0 (38) The following
    conclusions can then be drawn: lim 𝑇→∞ 𝒢(𝑇) 𝑇 =0 (39) Thus, we can assume that
    the queue is stable, and we will use an approximate analysis to explore the upper
    bound of problem P4. 4.3. Upper Bound Analysis The reasoning in the previous section
    proved that problem P4 to be solved has an upper bound, and we will then solve
    it for the exact value of this upper bound. First, it is necessary to prove the
    upper bound by substituting the computational drift into Equation (40): lim 𝑇→∞
    1 𝑇 ∑ 𝑡=0 𝑇−1 𝑃 ′ (𝑡)≤𝑃*+𝜁 (40) where the specific deviation 𝜁 is as follows:
    𝜁= ( 𝑁 2 +1)( 𝐹 𝑗 2 + 𝐶 𝑗 2 )+2𝑁 G max 𝐹 𝑗 +2𝑁H max 𝐶 𝑗 2𝑉 (41) According to the
    process of solving queuing network problems using the Lyapunov optimization method,
    the queue length is usually regarded as a random process. Then, the Lyapunov function
    is constructed to estimate the expected growth rate of the queue length, and the
    queue length is controlled in a certain range. When the constructed function meets
    certain conditions, it can be proved by stability theory that the queue length
    can be controlled within a certain range, and the optimal solution can be obtained.
    Of course, these results are subject to deviation. For the problem of deviation
    range, a penalty term is introduced into the Lyapunov function to limit the fluctuation
    of the queue length to a certain range, so that the problem can obtain an optimal
    solution within a certain deviation range. This approach is called the Lyapunov
    drift-plus-penalty method [30]. The specific proof is as follows: Δ Θ ′ (𝑡)+𝑉𝐸(
    𝑃 ′ (𝑡)|Θ(𝑡)) ≤ 𝐷 1 + 𝐷 2 2 +𝑉 𝑃 ′ (𝑡)+(G (𝑡)−F 𝑗 ) ∑ 𝑖=1 𝑁 F ′ 𝑖,𝑠 (𝑡) +(H (𝑡)−
    𝐶 𝑗 ) ∑ 𝑖=1 𝑁 𝐶 ′ 𝑖,𝑗 (𝑡) ≤ 𝐷 1 + 𝐷 2 2 +𝑉𝑃*+𝑁( 𝒢 max ℱ 𝑗 + ℋ max 𝐶 𝑗 ) (42) Subtracting
    Δ Θ ′ (𝑡) from both sides and dividing by V at the same time reduces the original
    equation to: lim T→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 𝑃 ′ (𝑡) ≤ 𝐷 1 + 𝐷 2 2𝑉 +𝑃* + 𝑁( 𝒢 max ℱ 𝑗 +
    ℋ max 𝐶 𝑗 ) 𝑉 − Δ Θ ′ (𝑡) 𝑉 (43) The next expansion of Δ Θ ′ (𝑡) yields the upper
    bound 𝜁 lim T→∞ 1 𝑇 ∑ 𝑡=0 𝑇−1 𝑃 ′ (𝑡) ≤ 𝐷 1 + 𝐷 2 2𝑉 +𝑃* + 𝑁( 𝒢 max ℱ 𝑗 + ℋ max
    𝐶 𝑗 ) 𝑉 − Δ Θ ′ (𝑡) 𝑉 = 𝐷 1 + 𝐷 2 2𝑉 +𝑃*+ 𝑁( 𝒢 max ℱ 𝑗 + ℋ max 𝐶 𝑗 ) 𝑉 − lim 𝑇→∞
    ℒ( 𝜃 ′ (𝑇−1))−ℒ( 𝜃 ′ (0)) 𝑉𝑇 =𝑃*+ (𝑁+1) 2 ℱ 𝑗 2 + (𝑁+1) 2 𝐶 𝑗 2 2𝑉 + 𝑁 𝒢 max ℱ
    𝑗 𝑉 =𝑃*+ ( 𝑁 2 +1) F 𝑗 2 + 𝐶 𝑗 2 )+2𝑁 G max F 𝑗 +2𝑁 H max 𝐶 𝑗 2𝑉 (44) Overall,
    we initially precisely defined the problem, aiming to optimize the total energy
    consumption of the Internet of Things (IoT) system using the Lyapunov method.
    Subsequently, we systematically established the Lyapunov function, transforming
    problem P1 into P3 and introducing a virtual queue that encompasses aspects such
    as computing resources, channels, delays, and energy. By penalizing the Lyapunov
    drift, we conducted an upper-bound analysis of problem P4, providing crucial insights
    for the theoretical analysis of problem solutions. This establishes a theoretical
    foundation for the algorithm design, the construction of the fitness function,
    and the long-term operation of the system in the subsequent fifth section. 5.
    Algorithm Design and Numerical Results This section outlines and analyzes the
    algorithm. The Fog Servers (FS) pre-processed data from edge or IoT devices, integrating
    types, volumes, and pre-processing times. A predictive model was then built using
    Pytorch, trained on 90% of the data, with the remaining 10% being used for model
    validation. A key consideration was selecting data volumes, and setting Wk to
    4–20% of the total time slot. The predictive window size was adjusted based on
    feedback from each prediction, optimizing the results without affecting the allocation.
    At Algorithm 1, The algorithm details the construction of a sliding prediction
    model, predicting offloading data, and adjusting the prediction accuracy by using
    some training costs as a trade-off. PSO-based resource allocation is described
    in the Algorithm 2, also addressing system stability requirements. Algorithm 1
    Prediction and Allocation Algorithm using LSTM and PSO. Require:  𝑊 𝑘 ,𝑄(𝑡), 𝜆
    𝑠 𝑟 (𝑡), U 𝑟 (t), 𝑃 𝑐 , 𝑃 𝑖 , 𝑃 𝑠 , 𝑇 𝑀𝐴𝑋 Ensure:  0<𝑡< 𝑇 𝑀𝐴𝑋 1: Initialize LSTM
    model with parameters 2: Initialize PSO algorithm for processor allocation 3:
    for  𝑡=0 to 𝑇 𝑀𝐴𝑋  do 4:     Update { 𝜆 𝑠 𝑟 (𝑡)} , { 𝐴 𝑤 (𝑡)} 5:     𝑄(𝑡)=𝑄(𝑡−1)+
    𝜆 𝑠 𝑟 (t)− 𝜇 𝑟 (t)− U 𝑟 (t)       ▹ update task queue 6:     if  𝑡≥ 𝑊 𝑘  then
    7:         𝐴 𝑤 (𝑡)= LSTM Prediction (𝑄(𝑡− 𝑊 𝑘 :𝑡−1)) ▹ predict using LSTM 8:     end
    if 9:     U 𝑟 (t) ′ = PSO Allocation ( 𝐴 𝑤 (𝑡)) ▹ PSO based allocation 10:     if  𝐴
    𝑤 (𝑡)≈ 𝜆 𝑠 𝑟 (t)  then 11:         U 𝑟 (t)= U 𝑟 (t) ′            ▹ use predicted
    allocation 12:     else 13:         U 𝑟 (t)= PSO Allocation ( 𝜆 𝑠 𝑟 (𝑡))     ▹
    reallocate based on actual data 14:     end if 15:     Execute allocation based
    on U 𝑟 (t) 16:     Update prediction accuracy 𝐸 𝑟 17:     if  ( 𝐸 𝑟 <60%) and
    ( 𝑊 𝑘 <20%× 𝑇 𝑚𝑎𝑥 )  then 18:         𝑊 𝑘 +=5 ▹ increase window size if accuracy
    low 19:     else if  ( 𝐸 𝑟 ≥60%) and ( 𝑊 𝑘 >5%× 𝑇 𝑚𝑎𝑥 )  then 20:         𝑊 𝑘
    *=0.9 ▹ decrease window size if accuracy high 21:     end if 22: end for Algorithm
    2 Allocation Algorithm Based on Lya-PSO. Require:  𝐼 𝑘 , 𝐴 𝑘 (𝑡),𝑃(𝑡),𝐹(𝑘),𝐹(𝑗),𝐹(𝑛),
    𝑃 𝑘 , 𝑃 𝑐 , 𝑃 𝑛 , U 𝑟 (t) Ensure:  0<𝐹(𝑘)<𝐹 (𝑘) 𝑀𝐴𝑋 ,0<𝐹(𝑛)<𝐹 (𝑛) 𝑀𝐴𝑋 ,0< 𝐶 𝑘,𝑛
    < 𝐶 𝑘,𝑛 𝑀𝐴𝑋 1: // Initialize task allocation process 2: for  𝑖=1 to K do 3:     Initialize
    position 𝑋 𝑘 and velocity 𝑉 𝑘 for particle k 4:     Define objective function
    min𝑉𝑃(𝑡)+(G(𝑡)− F 𝑠 ) ∑ 𝑁 𝑘=1 F 𝑘,𝑠 (𝑡)+(H(𝑡)− 𝐶 𝑗 ) ∑ 𝑁 𝑘=1 𝐶 𝑘,𝑗 (𝑡) 5:     Evaluate
    particle k and set 𝑝𝐵𝑒𝑠𝑡 6:     Update parameters = 𝑋 𝑘 7: end for 8: // Verification
    and Allocation 9: if  𝑋 𝑘 ==null and 𝐹(𝑗)≠null  then 10:     for  𝑛=1 to N do
    11:         if  𝐹 𝑛 is free then 12:            Allocate an idle fog processor
    13:            Recursive: 𝐴 𝑘 (𝑡)+=Allocation algorithm( 𝐼 𝑘 )  ▹ Find the best
    matching fog processor group 14:         end if 15:     end for 16: end if 17:
    return  𝑃*(𝑡), 𝐴 ∗ 𝑘 (𝑡)           ▹ Return the optimal allocation In this section,
    the performance of the proposed LLPSO algorithm is evaluated through numerical
    simulations. The test was conducted on a Windows 10 system with a 2.3 GHz Intel
    Core i5-8300H processor and 24 GB 2667-MHz DDR4 memory. The simulation environment
    was built using Python 3.10. The section begins with the basic setup of the simulation
    environment and then presents results and analyses of the algorithm’s performance
    under different latency requirements for tasks. 5.1. Basic Settings In the simulation,
    a fog computing system with 20 users, 50 computing nodes, and one control node
    was modeled. Each control node corresponded to computing nodes, all active with
    varying total resources. Each node had different available resources, and available
    memory and CPU for dynamically processing change when offloading tasks. Twenty
    users were randomly simulated to send task requests over an hour. The bandwidth
    between users and fog control nodes was set at 10 Mb/s, with wired connections
    to fog computing nodes. Tasks were uploaded to edge devices, which decided on
    local processing or offloading. Users could also connect directly to fog servers.
    Users and edge devices, which were treated as users, had initial CPU capacities
    of 500 Mhz and 500 M of memory. Fog server nodes had higher bandwidths of 2 GHz
    and 4 G RAM. Processor nodes started with 300 Mhz CPUs and 300 M RAM. The communication
    interference and distances between users and computing nodes were constant, while
    noise between users and the cloud varied. Task sizes ranged from 1–10 M, and the
    algorithm’s inertia weight (w) was set at 0.6. 5.2. Comparative Analysis of Prediction
    Algorithms Firstly, we present a partial data comparison graph between the optimized
    prediction algorithm and real data. Prediction, as a task, is inherently challenging,
    considering it deals with unknown events. The effectiveness of predictions becomes
    more evident when the data exhibit certain patterns that can be predicted. In
    this experiment, we initially conducted a comparison of predictions involving
    standalone LSTM, LSTM with a fixed sliding window mode, and LSTM with a dynamic
    sliding window. Firstly, in Figure 3, you can see a comparative graph of the prediction
    accuracy for each time slot among the three prediction methods. In general cases,
    the prediction started with failure, resulting in a starting point at 0 and 1.
    At this point, the simulation represented extreme conditions where user data were
    continuously changing, and this data variation was influenced by the bandwidth,
    transmission rate, and processing latency. According to the evaluation criteria
    in this paper, it was observed that the dynamic window, WK, which adjusted its
    accuracy gradually with model training, exhibited a relatively higher precision.
    Figure 3. Accuracy trend at each time slot. Figure 4 illustrates the training
    time comparison for the three algorithms in each time slot. Firstly, the general
    LSTM model could be used directly after training; thus, it did not consume a significant
    amount of training time. The training time for LSTM optimized under the dynamic
    sliding window was initially faster than that optimized using the static window
    training, but later it took longer. The comparison between dynamic and static
    arose because the algorithm adjusted during continuous operation, discovering
    that investing some training time appropriately could enhance accuracy. Secondly,
    the training time was determined by the channel rate as the lower limit and when
    to use these data as the upper limit. Figure 4. Training time under each time
    slot. In summary, in this design, the prediction method of LSTM optimized under
    the dynamic sliding window, compared to LSTM optimized under the static window,
    improved our accuracy by approximately 3.28 times and the precision by over 18.02%.
    Compared to the regular LSTM prediction method, our accuracy increased by 1.422
    times, and the precision improved by over 7.72%. 5.3. Evaluation with Different
    Delay Requirement Task We evaluated the performance of various algorithms for
    tasks with different latency requirements and also presented a comparison of delays
    after incorporating prediction algorithms. Additionally, we assessed the system
    performance, including the impact of node expansion on system performance, tasks
    with the same number of nodes but different arrival rates, and a comparison of
    error rates for tasks with different arrival rates. In Figure 5, we can observe
    the total energy consumption variation for different algorithms in each time slot,
    and Figure 6 illustrates the average energy consumption variation in different
    time slots. Among them, the improved LDLPSO algorithm performed the best, followed
    by the LPSO algorithm, the original PSO algorithm, the First-Come-First-Served
    (FiFs) algorithm, and the Greedy algorithm. The reason for this phenomenon is
    that our proposed LDLPSO algorithm takes into account the stability of the entire
    system queue, making its line relatively clear with smaller fluctuations. Although
    the graph appears relatively stable, the actual numerical changes are significant.
    After comparison, it can be concluded that the LDLPSO algorithm saves an average
    of 9.44% in energy consumption compared to the LPSO algorithm and 32.73% compared
    to the most energy-consuming algorithm, i.e., the Greedy algorithm. Figure 5.
    Total time consumption of various tasks. Figure 6. Average energy consumption
    graph of time slots. In Figure 7 and Figure 8, we further compare the energy consumption
    and latency between the LDLPSO and LPSO algorithms for additional research. On
    one hand, although the Full System (FS) energy consumption increased, the overall
    average energy consumption did not, thanks to its dynamic combination fog processor
    scheme, which was designed for tasks. Simultaneously, due to the effect of pre-allocation,
    our allocation speed increased by 1.25 times compared to the original speed. On
    the other hand, with the increase in the number of nodes, the variation in our
    average energy consumption remained relatively stable. As for the other strategies,
    the First-Come-First-Served (FiFs) strategy led to queue congestion and increased
    task delays, requiring more computational and communication resources to meet
    the deadline and consequently increasing energy consumption. Finally, the Greedy
    algorithm consumed more energy in an attempt to complete all tasks as quickly
    as possible. Figure 7. Performance of two algorithms: average energy consumption
    of FS with increasing number of nodes. Figure 8. Performance of two algorithms:
    round averaging latency distribution. In Figure 9, we can observe that, even with
    the same arrival rate, our proposed LLPSO algorithm still performed the best.
    This is because the combination of Lyapunov-based system control and LSTM-based
    task prediction, while maintaining queue stability, significantly reduces the
    queue time and lowers system energy consumption. In contrast, the other three
    algorithms focus solely on resource allocation without dynamic optimization. Additionally,
    we compared the performance of different algorithms under different task arrival
    rates with the same number of user nodes, as shown in Figure 10. In these comparisons,
    the Greedy algorithm outperformed the LDLPSO algorithm. This is because the Greedy
    algorithm allocates a significant amount of resources and energy, thereby increasing
    the task completion rate. On the other hand, compared to LDLPSO, the performance
    of the other two algorithms in terms of task completion rate is relatively poor.
    LDLPSO can effectively utilize available resources, prioritize optimizing task
    allocation, and ensure tasks are completed on time. However, the other two algorithms
    lack flexibility in resource utilization and task allocation, thus they cannot
    fully leverage the available resources to meet task demands, resulting in a lower
    task completion rate. Figure 9. The energy consumption chart for different algorithms
    under different arrival rates. Figure 10. The task-miss rate for different algorithms
    under different arrival rates. In addition, we also compared the performance of
    different algorithms under different task arrival rates with the same number of
    user nodes. In these comparisons, the Greedy algorithm outperformed the LLPSO
    algorithm. This is because the Greedy algorithm invests significant resources
    and energy, resulting in a higher task completion rate. On the other hand, the
    other two algorithms performed relatively poorly in terms of the task completion
    rate compared to LDLPSO. LDLPSO utilizes the available resources efficiently and
    prioritizes optimal task allocation to ensure timely task completion. It allocates
    a substantial amount of resources and energy to task execution, thereby improving
    the task completion rate. However, the other two algorithms are less flexible
    in resource utilization and task allocation, thus they are unable to fully utilize
    the available resources to meet task demands, leading to a lower task completion
    rate. 6. Conclusions and Future Works This paper introduces a predictive offloading
    algorithm, LDLPSO, based on the Lyapunov approach, optimized for charging scenarios.
    Even in environments with highly random data leading to a relatively low prediction
    success rate, comparative experiments revealed that the LDLPSO algorithm still
    outperformed the other four algorithms in terms of the task completion rate and
    energy consumption. Specifically, compared to the LPSO algorithm, we achieved
    an improvement of approximately 25% in response speed at the cost of sacrificing
    some Fog Server (FS) performance. The overall average energy consumption was reduced
    by approximately 9.44%, demonstrating good scalability. However, there are still
    some limitations and areas for improvement. We did not consider the mobility of
    user nodes or adequately address issues related to cloud collaboration. In real
    scenarios, user nodes may move to different locations, and the frequent changes
    in users may impact the prediction, allocation, and resource utilization of tasks.
    Therefore, in future research, we need to consider how to adapt to the mobility
    of user nodes to enhance the algorithm’s performance further, making it better
    suited for diverse system heterogeneity and data heterogeneity in device scenarios.
    In conclusion, although the LDLPSO algorithm in this paper achieved effective
    optimization results, further improvements are still necessary. Our future efforts
    will focus on researching the system heterogeneity and data heterogeneity of users,
    edge devices, and fog devices, leveraging the advantages of wireless charging
    networks. We aim to further enhance resource utilization efficiency, advance research
    in this field, and improve the performance and applicability of the algorithm.
    Author Contributions Conceptualization, S.P.; Methodology, S.P. and C.H.; Software,
    S.P. and J.T.; Validation, S.P. and J.T.; Formal analysis, S.P., J.F. and Z.S.;
    Investigation, S.P., C.H. and J.F.; Resources, H.W.; Data curation, C.H.; Writing—original
    draft, S.P., C.H. and Z.S.; Writing—review & editing, S.P., J.F., Z.S. and J.T.;
    Visualization, S.P.; Supervision, Z.S., J.T. and H.W.; Project administration,
    J.F. and H.W.; Funding acquisition, H.W. All authors have read and agreed to the
    published version of the manuscript. Funding This research was supported by the
    National Natural Science Foundation of China under Grant Nos. 62171413. The public
    welfare research project of Jinhua City of Zhejiang Province of China under Grant
    2022-4-063. The name of our project fund is Research on the Fundamental Theory
    and Application of Compressed Data Collection for Large-scale Wireless Sensor
    Networks in Defective Multi-modal Sampling Scenarios. Data Availability Statement
    Data are simulation realizations, already described in the text, no additional
    datasets available. Conflicts of Interest The authors declare no conflicts of
    interest. References Gao, X.; Huang, X.; Bian, S.; Shao, Z.; Yang, Y. PORA: Predictive
    Offloading and Resource Allocation in Dynamic Fog Computing Systems. IEEE Internet
    Things J. 2020, 7, 72–87. [Google Scholar] [CrossRef] Li, Z.; Qian, Y.; Tang,
    F.; Zhao, M.; Zhu, Y. H-BILSTM: A Novel Bidirectional Long Short Term Memory Network
    Based Intelligent Early Warning Scheme in Mobile Edge Computing (MEC). IEEE Trans.
    Emerg. Top. Comput. 2023, 11, 253–264. [Google Scholar] [CrossRef] Gao, Y.; Tang,
    W.; Wu, M.; Yang, P.; Dan, L. Dynamic social-aware computation offload-ing for
    low-latency communications in IoT. IEEE Internet Things J. 2019, 6, 7864–7877.
    [Google Scholar] [CrossRef] Gomes, E.; Costa, F.; De Rolt, C.; Plentz, P.; Dantas,
    M. A Survey from Real-Time to Near Real-Time Applications in Fog Computing Environments.
    Telecom 2021, 2, 489–517. [Google Scholar] [CrossRef] Sasaki, K.; Suzuki, N.;
    Makido, S.; Nakao, A. Vehicle control system coordinated between cloud and mobile
    edge computing. In Proceedings of the 2016 55th Annual Conference of the Society
    of Instrument and Control Engineers of Japan (SICE), Tsukuba, Japan, 20–23 September
    2016; pp. 1122–1127. [Google Scholar] Alliance, N. 5G White Paper. Next Generation
    Mobile Networks; White Paper; NGMN: Frankfurt, Germany, 2015; Volume 1. [Google
    Scholar] Du, J.; Zou, Z.; Shi, Y.; Zhao, D. Zero latency: Real-time synchronization
    of BIM data in virtual reality for collaborative decisionmaking. Autom. Constr.
    2018, 85, 51–64. [Google Scholar] [CrossRef] Lv, H.; Zheng, Z.; Wu, F.; Chen,
    G. Strategy-Proof Online Mechanisms for Weighted AoI Minimization in Edge Computing.
    IEEE J. Sel. Areas Commun. 2021, 39, 1277–1292. [Google Scholar] [CrossRef] Dong,
    Y.; Guo, S.; Wang, Q.; Yu, S.; Yang, Y. Content caching-enhanced computation offloading
    in mobile edge service networks. IEEE Trans. Veh. Technol. 2021, 71, 872–886.
    [Google Scholar] [CrossRef] Karimiafshar, A.; Hashemi, M.R.; Heidarpour, M.R.;
    Toosi, A.N. An energy-conservative dispatcher for fog-enabled IIoT systems: When
    stability and timeliness matter. IEEE Trans. Serv. Comput. 2021, 16, 80–94. [Google
    Scholar] [CrossRef] Hazra, A.; Adhikari, M.; Amgoth, T.; Srirama, S.N. Joint computation
    offloading and scheduling optimization of IoT applications in fog networks. IEEE
    Trans. Netw. Sci. Eng. 2020, 7, 3266–3278. [Google Scholar] [CrossRef] Bonomi,
    F.; Milito, R.A.; Zhu, J.; Addepalli, S. Fog computing and its role in the internet
    of things. In Proceedings of the First Edition of the MCC Workshop on Mobile Cloud
    Computing, Helsinki, Finland, 17 August 2012; pp. 13–16. [Google Scholar] Flinn,
    J. Cyber Foraging: Bridging Mobile and Cloud Computing; Synthesis Lectures on
    Mobile & Pervasive Computing; Springer Nature: Berlin/Heidelberg, Germany, 2012;
    Volume 7, pp. 1–103. [Google Scholar] Adhikari, M.; Mukherjee, M.; Srirama, S.N.
    DPTO: A Deadline and Priority-Aware Task Offloading in Fog Computing Framework
    Leveraging Multilevel Feedback Queueing. IEEE Internet Things J. 2020, 7, 5773–5782.
    [Google Scholar] [CrossRef] Guan, S.; Boukerche, A.; Loureiro, A. Novel Sustainable
    and Heterogeneous Offloading Management Techniques in Proactive Cloudlets. IEEE
    Trans. Sustain. Comput. 2021, 6, 334–346. [Google Scholar] [CrossRef] Chen, X.
    Decentralized computation offloading game for mobile cloud computing. IEEE Trans.
    Parallel Distrib. Syst. 2015, 26, 974–983. [Google Scholar] [CrossRef] Zhang,
    G.; Shen, F.; Liu, Z.; Yang, Y.; Wang, K.; Zhou, M.-T. FEMTO: Fair and Energy-Minimized
    Task Offloading for Fog-Enabled IoT Networks. IEEE Internet Things J. 2019, 6,
    4388–4400. [Google Scholar] [CrossRef] Du, J.; Zhao, L.; Feng, J.; Chu, X. Computation
    Offloading and Resource Allocation in Mixed Fog/Cloud Computing Systems with Min-Max
    Fairness Guarantee. IEEE Trans. Commun. 2018, 66, 1594–1608. [Google Scholar]
    [CrossRef] Qiu, Y.; Zhang, H.; Long, K. Computation Offloading and Wireless Resource
    Management for Healthcare Monitoring in Fog-Computing-Based Internet of Medical
    Things. IEEE Internet Things J. 2021, 8, 15875–15883. [Google Scholar] [CrossRef]
    Bozorgchenani, A.; Mashhadi, F.; Tarchi, D.; Monroy, S.A.S. Multi-Objective Computation
    Sharing in Energy and Delay Constrained Mobile Edge Computing Environments. IEEE
    Trans. Mob. Comput. 2021, 20, 2992–3005. [Google Scholar] [CrossRef] Venkatesh,
    M.; Polisetty, S.N.K.; Srilakshmi, C.H.; Satpathy, R.; Neelima, P. A Novel Deep
    Learning Mechanism for Workload Balancing in Fog Computing. In Proceedings of
    the 2022 International Conference on Automation, Computing and Renewable Systems
    (ICACRS), Pudukkottai, India, 13–15 December 2022; pp. 515–519. [Google Scholar]
    [CrossRef] Cai, Y.; Llorca, J.; Tulino, A.M.; Molisch, A.F. Decentralized Control
    of Distributed Cloud Networks with Generalized Network Flows. IEEE Trans. Commun.
    2023, 71, 256–268. [Google Scholar] [CrossRef] Huang, C.; Wang, H.; Zeng, L.;
    Li, T. Resource Scheduling and Energy Consumption Optimization Based on Lyapunov
    Optimization in Fog Computing. Sensors 2022, 22, 3527. [Google Scholar] [CrossRef]
    [PubMed] Lin, M.; Wierman, A.; Andrew, L.; Thereska, E. Dynamic right-sizing for
    power-proportional data centers. IEEE/ACM Trans. Netw. 2013, 21, 1378–1391. [Google
    Scholar] [CrossRef] Kim, Y.; Kwak, J.; Chong, S. Dual-side optimization for cost-delay
    tradeoff in mobile edge computing. IEEE Trans. Veh. Technol. 2018, 67, 1765–1781.
    [Google Scholar] [CrossRef] Zukerman, M. Introduction to queueing theory and stochastic
    teletraffic models. arXiv 2013, arXiv:1307.2968. [Google Scholar] Jiang, Z.; Mao,
    S. Energy delay tradeoff in cloud offloading for multi-core mobile devices. IEEE
    Access 2015, 3, 2306–2316. [Google Scholar] [CrossRef] Lin, R.; Xie, T.; Luo,
    S.; Zhang, X.; Xiao, Y.; Moran, B.; Zukerman, M. Energy-Efficient Computation
    Offloading in Collaborative Edge Computing. IEEE Internet Things J. 2022, 9, 21305–21322.
    [Google Scholar] [CrossRef] Lin, R.; Zhou, Z.; Luo, S.; Xiao, Y.; Wang, X.; Wang,
    S.; Zukerman, M. Distributed Optimization for Computation Offloading in Edge Computing.
    IEEE Trans. Wirel. Commun. 2020, 19, 8179–8194. [Google Scholar] [CrossRef] Neely,
    M.J.; Huang, L. Dynamic product assembly and inventory control for maximum profit.
    In Proceedings of the 49th IEEE Conference on Decision and Control (CDC), Atlanta,
    GA, USA, 15–17 December 2010; pp. 2805–2812. [Google Scholar] [CrossRef] Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Pan, S.; Huang, C.; Fan, J.; Shi, Z.; Tong,
    J.; Wang, H. Optimizing Internet of Things Fog Computing: Through Lyapunov-Based
    Long Short-Term Memory Particle Swarm Optimization Algorithm for Energy Consumption
    Optimization. Sensors 2024, 24, 1165. https://doi.org/10.3390/s24041165 AMA Style
    Pan S, Huang C, Fan J, Shi Z, Tong J, Wang H. Optimizing Internet of Things Fog
    Computing: Through Lyapunov-Based Long Short-Term Memory Particle Swarm Optimization
    Algorithm for Energy Consumption Optimization. Sensors. 2024; 24(4):1165. https://doi.org/10.3390/s24041165
    Chicago/Turabian Style Pan, Sheng, Chenbin Huang, Jiajia Fan, Zheyan Shi, Junjie
    Tong, and Hui Wang. 2024. \"Optimizing Internet of Things Fog Computing: Through
    Lyapunov-Based Long Short-Term Memory Particle Swarm Optimization Algorithm for
    Energy Consumption Optimization\" Sensors 24, no. 4: 1165. https://doi.org/10.3390/s24041165
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 10. Feb 15. Feb 20. Feb 25.
    Feb 1. Mar 6. Mar 11. Mar 16. Mar 21. Mar 26. Mar 0 100 200 300 400 500 For more
    information on the journal statistics, click here. Multiple requests from the
    same IP address are counted as one view.   Sensors, EISSN 1424-8220, Published
    by MDPI RSS Content Alert Further Information Article Processing Charges Pay an
    Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For
    Reviewers For Editors For Librarians For Publishers For Societies For Conference
    Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles
    Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe
    to receive issue release notifications and newsletters from MDPI journals Select
    options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated
    Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Optimizing Internet of Things Fog Computing: Through Lyapunov-Based Long
    Short-Term Memory Particle Swarm Optimization Algorithm for Energy Consumption
    Optimization †'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ramesh D.
  - Rizvi N.
  - Rao P.C.S.
  - Sundararajan E.A.
  - Mondal K.
  - Srivastava G.
  - Qi L.
  citation_count: '0'
  description: 'With the advent of Internet of Things (IoT) applications, smart IoT
    devices are ubiquitous. Executing these devices on cloud data centers can lead
    to network congestion and transmission delay that causes failure instances in
    cloud architecture. Therefore, fog computing is derived to satisfy the low latency,
    location awareness, and mobility requirements of massive IoT applications near
    end users. Concerned with the high requirements of compute-intensive business
    applications, fog computing structure becomes complex due to limited computing
    capacity, and therefore these applications are offloaded to the cloud. Therefore,
    to overcome the offloading problem of workflow tasks in this work, a novel method
    named: Improved Chemical Reaction Optimization for Workflow Scheduling in a Hybrid
    Environment (ICRO-WSHE) has been proposed. The proposed algorithm aims at minimizing
    the execution cost of workflow applications under the defined deadline constraints.
    The algorithm is based on CRO, which has fewer parameters and the fastest convergence
    speed. However, the existing CRO has drawbacks in terms of getting stuck in the
    local optimum and having difficulty obtaining real optimal solutions. Therefore,
    the proposed mechanism modifies the existing CRO algorithm and includes the suitable
    features of PSO (Particle Swarm Optimization) and FQR (Fitness-based Quasi Reflection)
    methods to prevent the shortcomings of the CRO method. Further, a double-point
    synthesis operation is incorporated to increase the exploration rate and improve
    the working of the proposed algorithm. The result of the simulation experiment
    based on different types of workflows shows that the proposed method outperforms
    existing algorithms and proves to be a practical approach.'
  doi: 10.1109/TNSM.2023.3299358
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Network ...
    >Volume: 21 Issue: 1 Improved Chemical Reaction Optimization With Fitness-Based
    Quasi-Reflection Method for Scheduling in Hybrid Cloud-Fog Environment Publisher:
    IEEE Cite This PDF Dharavath Ramesh; Naela Rizvi; P. C. Srinivasa Rao; Elankovan
    A. Sundararajan; Koushik Mondal; Gautam Srivastava; Lianyong Qi All Authors 103
    Full Text Views Abstract Document Sections I. Introduction II. Related Work III.
    System Architecture and Problem Statement IV. Proposed Work V. Result and Experimental
    Analysis Show Full Outline Authors Figures References Keywords Metrics Abstract:
    With the advent of Internet of Things (IoT) applications, smart IoT devices are
    ubiquitous. Executing these devices on cloud data centers can lead to network
    congestion and transmission delay that causes failure instances in cloud architecture.
    Therefore, fog computing is derived to satisfy the low latency, location awareness,
    and mobility requirements of massive IoT applications near end users. Concerned
    with the high requirements of compute-intensive business applications, fog computing
    structure becomes complex due to limited computing capacity, and therefore these
    applications are offloaded to the cloud. Therefore, to overcome the offloading
    problem of workflow tasks in this work, a novel method named: Improved Chemical
    Reaction Optimization for Workflow Scheduling in a Hybrid Environment (ICRO-WSHE)
    has been proposed. The proposed algorithm aims at minimizing the execution cost
    of workflow applications under the defined deadline constraints. The algorithm
    is based on CRO, which has fewer parameters and the fastest convergence speed.
    However, the existing CRO has drawbacks in terms of getting stuck in the local
    optimum and having difficulty obtaining real optimal solutions. Therefore, the
    proposed mechanism modifies the existing CRO algorithm and includes the suitable
    features of PSO (Particle Swarm Optimization) and FQR (Fitness-based Quasi Reflection)
    methods to prevent the shortcomings of the CRO method. Further, a double-point
    synthesis operation is incorporated to increase the exploration rate and improve
    the working of the proposed algorithm. The result of the simulation experiment
    based on different types of workflows shows that the proposed method outperforms
    existing algorithms and proves to be a practical approach. Published in: IEEE
    Transactions on Network and Service Management ( Volume: 21, Issue: 1, February
    2024) Page(s): 653 - 669 Date of Publication: 28 July 2023 ISSN Information: DOI:
    10.1109/TNSM.2023.3299358 Publisher: IEEE Funding Agency: SECTION I. Introduction
    The evolution of the Internet of Things (IoT) encompasses the gamut of intelligent
    devices and sensors. These devices are ubiquitous and generate an unprecedented
    volume of data that needs to be processed within the strict timeline in a real-time
    manner [1], [2]. Generally, IoT applications are processed by real-time tasks
    comprising tasks with precedence constraints in the workflow. Earlier, the cloud
    paradigm emerged as an eminent one offering a pool of infinite resources in computation,
    storage, and network [3] to execute the workflows. The flexibility to scale the
    cloud resources of varying configurations on-demand empowers the users to develop
    innovative scheduling techniques to utilize the resources efficiently. However,
    with the advent of the Internet of Things (IoT), the architecture of the cloud
    has become incompetent and is associated with significant challenges [4]. The
    Internet of Things (IoT) is based on self-configuring devices interconnected in
    a dynamic environment. It represents pervasive computing where Internet connectivity
    is extended beyond smart devices to everyday objects to perform various services
    [5]. These devices are located at the network’s edge and require data processing
    with location awareness, low transmission delay, and mobility requirements. Fog
    computing has emerged as a popular paradigm to satisfy these requirements and
    can effectively compensate for the problem of communication latency in the cloud
    [6]. The fog nodes, with their local computation, storage, and network, are installed
    at the edge of networks. Placing resources at the network’s end allows fog nodes
    to perform low-latency transmission, location awareness, mobility support, and
    real-time interaction. Therefore, fog computing proves to be more efficient regarding
    service latency, network traffic, power consumption, content distribution, etc.
    In this context, as compared to cloud computing, fog computing can better meet
    the IoT application’s requirements. In contrast, the cloud’s dynamically scalable
    and virtualized resources offered over the Internet may complement IoT. The flexibility
    and dynamic scaling of the resources provided by the cloud can alleviate the limited
    capacity of fog nodes to satisfy the increasing demands of compute-intensive applications.
    Therefore, it can be concluded that fog computing is not aimed to replace the
    cloud but to complement each other, leading to the new cloud-fog computing paradigm
    [7]. Generally, most offloaded applications consist of several tasks that can
    be independent or mutually dependent. The application consisting of independent
    tasks can be offloaded simultaneously and processed in parallel. Whereas the dependent
    tasks have dependency constraints, and parallel offloading is impossible. These
    applications are modeled as workflow and represented as Direct Acyclic Graph (DAG).
    Although various advantages have been associated with fog computing, it still
    faces innumerable challenges. For instance, scheduling workflows involve real-time
    execution and movement of data from fog and cloud environments that result in
    a complex execution environment. Another challenge is the uncertain allocation
    of resources and execution of workflows on these resources. Further, resource
    allocation is flexible for some tasks, while for other tasks, resource allocation
    is not flexible due to dependency constraints. Therefore, there is a need for
    an effective scheduling algorithm for efficient resource allocation such that
    the execution time and cost are minimized. This work focuses on workflow scheduling
    algorithms in a hybrid distributed computing environment based on collaboration
    between fog and cloud. The proposed work aims to minimize the execution cost of
    deadline-constrained workflow applications. Being an NP -complete problem, the
    workflow scheduling problem has been widely exploited for decades in the distributed
    environment of the grid and cloud [8], [9], [10]. Several algorithms have been
    designed to find a suitable schedule for satisfying the QoS requirements. Most
    of the approaches are designed for the grid and cloud environment with the aim
    of makespan and cost minimization [11], [12]. The classical scheduling algorithms
    can be categorized into two types: heuristic algorithms and meta-heuristic algorithms.
    Heuristic approaches are simple to implement and work efficiently. For instance,
    the HEFT [13] algorithm is a list-based algorithm designed for the grid environment
    with the aim of makespan minimization. Similarly, IC-PCP [14], BHEFT [15], FCFS
    [16] etc are some well-known heuristic algorithms. However, because of the simple
    strategy, the performance of heuristic algorithms is not up to the mark. Therefore,
    to enhance the quality of workflow scheduling in a heterogeneous environment,
    meta-heuristic approaches have been deployed to achieve near-optimal solutions
    [17]. Particle swarm optimization (PSO) [18], Genetic Algorithm (GA) [19], Ant
    colony optimization (ACO) [20] and many more have been used for the workflow scheduling
    problem. This work utilizes chemical reaction optimization (CRO), a recent enhancement
    in the meta-heuristic method [21]. CRO has been applied to various applications
    and achieved wider success [22]. The reason for the success is the ability of
    CRO to combine the natural chemical reaction properties with mathematical optimization
    and has fewer tunable parameters. Unlike the other meta-heuristic techniques,
    in the initial phase, CRO does not require many parameters but needs only the
    initial reactants for the implementation. Thus, fewer iterations are required
    for obtaining the optimal result, leading to a reduction in completion time. Although
    CRO has various advantages, there are some shortcomings like other meta-heuristic
    algorithms. For example, CRO can fall into the local optimum and have difficulties
    obtaining real optimal solutions. Therefore, to enhance the performance of the
    CRO algorithm, in this work, we have incorporated good features of the PSO algorithm
    and the opposition-based learning (OBL) method for improving exploration and exploitation
    mechanisms. Based on the above improvement aspects, a new approach named: Improved
    chemical reaction optimization for workflow scheduling in a hybrid environment
    (ICRO-WSHE) has been proposed and applied to scheduling deadline-constrained workflows
    in a hybrid cloud-fog environment. The proposed system combines chemical reaction
    optimization, particle swarm optimization, and the fitness-based quasi-reflection
    method (FQR) to obtain an efficient scheduling strategy. A. Our Contributions
    The main contribution of the work is illustrated as follows: A novel workflow
    scheduling strategy named ICRO-WSHE has been proposed for the complex cloud-fog
    environment. An improved CRO which is a hybrid of CRO, PSO, and FQR-based strategy
    has been proposed to improve the performance of the existing CRO method. A novel
    double-point synthesis operation has been proposed. Various benchmark workflows
    have been exploited to verify the efficacy of the proposed approach. Also, experimental
    evaluation is demonstrated in proving the superiority of the proposed algorithm
    compared with the state-of-art algorithms. The structure of the rest of the paper
    is described as follows. In Section II, the related work of workflow scheduling
    in the cloud-fog environment has been discussed. Section III describes the system
    model and problem definition. The proposed approach is described in Section IV,
    followed by results and discussion in Section V. Finally, the work is concluded
    in Section VI. SECTION II. Related Work The workflow scheduling problem in the
    distributed computed environment is an NP -hard problem [23]. Therefore, several
    studies have been proposed related to workflow scheduling problems in the heterogeneous
    grid and cloud computing [24], [25]. However, with the emergence of IoT applications,
    the workflow scheduling problem becomes more challenging and requires the innovation
    of scheduling strategies in the hybrid cloud-fog environment. In the current scenario,
    very few works related to workflow scheduling problems have been formulated to
    support real-time requirements in the cloud-fog environment. We have divided the
    related work into two parts for better readability. In the first part, we will
    discuss the heuristic techniques for workflow scheduling in a cloud-fog environment.
    In the second part, different meta-heuristic algorithms are discussed. A. Heuristic
    Algorithms for Workflow Scheduling in Cloud-Fog Environment Pham and Huh [26]
    proposed a heuristic algorithm to balance the makespan and cost of executing large-scale
    offloading applications in the collaborated fog and cloud environment. Similarly,
    Pham et al. [27] proposed another work named Cost-Makespan aware Scheduling (CMaS)
    algorithm. CMaS is also a heuristic approach to balance the trade-off between
    the makespan and the cost of execution. Further, this work is extended by adding
    an efficient task reassignment policy to satisfy the defined deadline. Ijaz et
    al. [28] proposed an Energy Makespan Multi-objective Optimization algorithm in
    fog-cloud environments. The algorithm has two phases. The first phase models workflow
    scheduling problems as multi-objective optimization problems and computes trade-offs
    between considered objectives while assigning tasks to the cloud or fog nodes.
    In the second phase, the deadline-aware step-wise frequency scaling approach has
    been adopted to reduce energy consumption further. However, the algorithm did
    not consider the cost of execution, which is also one of the most crucial QoS
    parameters. Deng et al. [29] proposed a workload allocation methodology in a fog-cloud
    environment. The trade-off between power consumption and transmission delay in
    two-tier architecture is investigated. The proposed approach determines an efficient
    allocation strategy between the fog and cloud paradigm. From the experimental
    analysis, it has been observed that fog computing has significantly improved the
    performance of the cloud paradigm. However, the proposed mechanism, which has
    dependency constraints, has not been designed for workflow. Stavrinides and Karatza
    [30] proposed a heuristic for IoT workflow scheduling in the hybrid cloud-fog
    environment. The proposed approach schedules compute-intensive tasks having low
    communication requirements on the cloud while communication-intensive tasks with
    low computation demand are scheduled on the fog nodes. In addition, the data transfer
    cost from IoT devices in the terminal layer to the fog layer is also considered.
    However, the algorithm only considers the communication cost from the IoT layer
    to the fog layer and ignores the communication cost from the fog layer to the
    cloud layer. Fellir et al. [31] designed a multi-agent system for executing bag-of-tasks
    in the fog-cloud environment. The model considers the task priority, waiting time,
    and response time. The algorithm proves better for resource utilization. However,
    a detailed explanation of the multi-agent system is missing. Similarly, another
    work based on a multi-agent system is proposed in [32], where the healthcare applications
    are scheduled in a fog-cloud environment. The proposed system consists of mapping
    between three decision tables for critical scheduling tasks. The first step decides
    whether the task can be executed locally or not. If not, selecting a suitable
    neighboring fog node is considered. If capable fog nodes are not found, the task
    is executed on the cloud. However, the algorithm does not consider the fundamentals
    of multi-agent systems like communication, negotiation, and collaboration. Mouradian
    et al. [33] proposed a Tabu Search-based Component Placement (TSCP) algorithm
    to find the sub-optimal placement. In addition to this, a random waypoint mobility
    model has been used to calculate the expected makespan and cost of execution.
    Davami et al. [34] proposed the Critical Path Extraction Algorithm (CPEA) based
    on critical path extraction in the hybrid fog-cloud computing system. It is a
    multi-criteria decision-making algorithm that derives the critical path of multiple
    workflows. Maio and Kimovski [35] proposed a novel Pareto-based approach for offloading
    tasks to fog nodes named Multi-Objective Workflow Offloading (MOWO). MOWO focuses
    on response time, reliability, and financial cost. Bisht and Vampugani [36] proposed
    an improved version of the min-min algorithm for workflow scheduling which considers
    the minimization of cost, time, energy, and load balancing. B. Meta-Heuristic
    Algorithms for Workflow Scheduling in Cloud-Fog Environment Mokini et al. [37]
    utilize the genetic algorithm to design the multi-agent system (MAS-GA). The system
    allows the modeling of IoT applications as a multi-objective optimization problem
    considering cost, response time, and makespan. Binh et al. [38] proposed the TCaS
    algorithm, which is an evolutionary algorithm to deal with workflow scheduling
    problems in the cloud-fog environment. The algorithm is designed to have an optimal
    trade-off between cost and execution time. Xu et al. [39] proposed an improved
    particle swarm optimization with a non-linear decreasing function of inertia weight
    for workflow scheduling problems. Xie et al. [40] utilized the PSO algorithm for
    workflow scheduling in a fog-cloud environment. The author proposed a directional
    and non-local convergent particle swarm optimization (DNCPSO) which can reduce
    makespan and cost and obtain a promising result. Tuli et al. [41] proposed a workflow
    scheduling algorithm called Monte Carlo Learning using Deep Surrogate Models (MCDS)
    in a mobile-fog-cloud environment. MCDS is based on Artificial Intelligence that
    uses a tree-based search strategy. Further, a deep neural network-based surrogate
    model estimates long-term QoS impact for immediate actions on scheduling decisions.
    Ahmed et al. [42] proposed a methodology based on differential optimization and
    Moth-Flame optimization for scientific workflow scheduling in a fog environment.
    The proposed DMFO-DE algorithm has less computation time than the MFO algorithm
    and converges faster. DMFO-DE is used for solving DVFS-based scientific workflow
    scheduling problems in fog computing. The proposed scheduling algorithm utilizes
    the HEFT algorithm to decide the priority of tasks and uses the DMFO-DE algorithm
    to assign tasks to the appropriate VM. The algorithm aims to minimize the makespan
    of workflow and energy consumption. The algorithm ignores the cost of execution,
    which is the most crucial QoS parameter. Najafizadeh et al. [43] proposed a multi-objective
    simulated annealing algorithm (MOSA) to allocate tasks securely on cloud and fog
    nodes under the defined deadline constraints. Rizvi et al. [44] proposed an intelligent
    scheduler for offloading tasks to cloud and fog nodes. The proposed Intelligent
    salp swarm scheduler (ISSS-FQR) aims to reduce execution costs under the defined
    deadline constraints. The proposed intelligent fuzzy scheduler exploits the existing
    salp swarm algorithm to learn and optimize the fuzzy task-resource allocation
    rule. Further, a fitness-based quasi-reflection method has been incorporated to
    overcome the problem of the salp swarm algorithm. However, the algorithm is complex
    and has a high time complexity. Abohamama et al. [45] proposed a semi-dynamic
    real-time task scheduling algorithm in a cloud-fog environment. The proposed algorithm
    formulates task scheduling as a permutation-based optimization problem. A modified
    version of the genetic algorithm has been used to allocate tasks to different
    nodes, minimizing the makespan and cost. However, the algorithm is designed for
    independent tasks and not for workflows. Jangu and Raza [46] proposed an Improved
    Jellyfish Algorithm (IJFA). The work has an efficient two-step scheduling algorithm
    comprising a Bi-factor classification task phase based on the deadline and priority
    and a scheduling phase using an enhanced artificial Jellyfish Search Optimizer
    (JS). The proposed work aims to minimize the makespan, lower execution costs,
    and higher resource utilization. SECTION III. System Architecture and Problem
    Statement This section is divided into two subsections. Section III-A introduces
    the application and system models used in this paper, whereas Section III-B discusses
    the preliminaries. A. Application and System Model IoT applications can be modeled
    as directed acyclic graphs (DAG). Typically, in the DAG model, every task and
    dependencies between the tasks can be represented as nodes and edges, as shown
    in Fig. 1. Here, the DAG is defined as G(N,E) , where N={ n 1 , n 2 , n 3 ,… n
    m } is the number of tasks, and E denotes the directed and weighted edges between
    the tasks. Generally, each task n i ϵN has its computational weight (c w i ) regarding
    the number of instructions. Further, each edge e i,j ϵE represents the dependencies
    between task n i and n j , i.e., task n j cannot be executed before task n i .
    And e i,j has the non-negative communication value (c d i,j ) representing the
    data transfer time from task n i to n j . The task with no direct predecessor
    is the entry task ( n entry ) , while the task with no immediate successor is
    the exit task ( n exit ) . Each task has been assumed to be non-preemptive and
    cannot be executed until all the immediate parent tasks are completed. In addition
    to this, each workflow has been associated with the deadline constraint ( D w
    ) . Fig. 1. A DAG with 11 tasks. Show All The fog architecture comprises three
    layers shown in Fig. 2. The bottom layer is the terminal layer, consisting of
    smart home appliances, smartphones, tablets, sensors, and many more IoT devices.
    Requests were sent to the upper layer for application execution. The middle layer
    is the fog layer consists of fog devices having local computation, network, and
    storage. The gateway, routers, and switches are examples of fog nodes deployed
    near the end-users devices to receive and process the user’s workload in minimum
    time. At the same time, fog devices are connected to the cloud to benefit from
    the infinite pool of resources. The uppermost layer is the cloud layer that provides
    resources to the offloaded compute-intensive tasks sent from the fog layer. Fig.
    2. Architecture of cloud-fog Layer. Show All B. Preliminaries The proposed work
    is based on the Chemical Reaction Optimization (CRO), Particle Swarm Optimization
    (PSO), and Opposition Based Learning (OBL) methods. Therefore, a brief description
    of these methods has been described in the following sub-sections. 1) Chemical
    Reaction Optimization: CRO is a meta-heuristic algorithm based on chemical reactions
    proposed by Lam and Li [47]. The concept of CRO combines mathematical optimization
    techniques with natural chemical reactions. A chemical reaction is a method to
    convert unstable substances into stable ones. Initially, the unstable molecules
    having excessive energy are selected as the reactant. During the chemical reaction,
    the molecules interact through many reactions, and lastly, these molecules are
    converted with minimum energy. CRO includes two major components: i) Molecules
    as an agent and ii) Chemical reactions as the search operators. Molecules: Molecules
    represent the individual in an optimization technique. Each molecule is associated
    with energy in the form of Potential Energy (PoE) and Kinetic Energy (KiE) , a
    total number of collisions (NHit) , and minimum collision required (MHit) . The
    transformation of the molecule ω into ω ′ is possible only Po E ω ′ ≤Po E ω +Ki
    E ω . In CRO, inter-conversion of KiE and PoE among molecules is possible only
    through some chemical operations search operation. Chemical reactions: CRO uses
    chemical reactions as search operations. Different search operators are applied
    in CRO to balance exploitation and exploration rate. Two types of reactions are
    possible among the molecules i) uni-molecular (one molecule involves in a reaction)
    ii) inter-molecular (two or more molecules involves in the reaction). The detailed
    discussion related to different search operations in CRO is discussed as follows:
    On-wall ineffective collision: In this reaction, a small perturbation in the molecule
    occurs while hitting the wall of the container. Suppose molecule ω transformed
    into ω ′ after collision if Eq. (1) is satisfied. Po E ω +Ki E ω ≥Po E ω ′ (1)
    View Source Decomposition: In decomposition, the original molecule collides with
    the container wall and splits into two or more molecules. The reaction causes
    the generation of new molecules which are quite different from the original version.
    Let ω ′ and ω ′′ be the two decomposed molecules of ω . If the original molecule
    and the resultant molecules fulfill the condition defined in Eq. (2) then only
    decomposition is processed. Po E ω +Ki E ω ≥Po E ω ′ +Po E ω ′′ (2) View Source
    Inter-molecular ineffective collision: In this method, the collision between two
    or more molecules occurs, leading to some perturbations and energy transfer among
    them. This process is possible only if the Eq. (3) is satisfied. Let ω ′ and ω
    ′′ be the resultant solution for the original molecules ω 1 and ω 2 . Po E ω 1
    +Ki E ω 1 +Po E ω 2 +Ki E ω 2 ≥Po E ω ′ +Po E ω ′′ (3) View Source Synthesis:
    A new molecule ω ′ is synthesized when two molecules ( ω 1 and ω 2 ) collide with
    each other, and the resultant is significantly different from the original molecules.
    Po E ω 1 +Ki E ω 1 +Po E ω 2 +Ki E ω 2 ≥Po E ω ′ . (4) View Source 2) Particle
    Swarm Optimization: The PSO algorithm is an intelligent computing methodology
    derived from birds’ information circulation and social behavior [48]. In this
    algorithm, a swarm represents a group of N particles, where each particle P i
    represents the complete solution. The individual particle P i is represented in
    the y th dimension of a D -dimensional search space, with position X i and velocity
    V i shown in Eq. (5) and Eq. (6). In the initialization phase, the position and
    the velocity are randomly initialized. Afterward, the particle searches for the
    local best solution P bes t i and global best solution G bes t i in each iteration.
    The velocity V i,y and position X i,y in y th dimension are updated using Eq.
    (7) and Eq. (8) and then local best position and global best position are obtained.
    V i = X i = V i,y (t+1)= X i,y (t+1)= ( V i1 , V i2 … V i,y ) ( X i1 , X i2 …
    X i,y ) η∗ V i,y (t)+ b 1 ∗ r 1 ( X p bes t i,y − X i,y (t)) + b 2 ∗ r 2 ∗( X
    G best − X i,y (t)) X i,y (t)+ V i,y (t+1) (5) (6) (7) (8) View Source where,
    η (0≤η≤1) , is the inertia weight, r 1 and r 2 are the random numbers generated
    between 0 and 1, b 1 and b 2  (0≤ b 1 , b 2 ≤2) are the acceleration coefficient.
    X i,y (t+1) and V i,y (t+1) are the updated position and updated velocity of particle
    P i . The position and velocity are repeatedly updated until the best position
    is updated or the maximum iteration is reached. 3) Opposition Based Learning:
    From the previous mathematical analysis and empirical studies, it has been proved
    that the shortcoming of evolutionary algorithms can be removed by using opposition-based
    learning (OBL) [49]. Evolutionary algorithms (EAs) are based on natural processes,
    which means they are modeled after very slow processes. Therefore, to cope with
    the limitations of EAs, OBL methods are used. Several OBL methods exist as; fitness-based
    opposition, quasi-reflection, fitness-based quasi-reflection method, super-opposition,
    and many more. Out of these, this work has utilized the fitness-based quasi-reflection
    method (FQR). The reason for the selection is among all the different OBL methods,
    FQR has the highest probability of being closer to the optimization solution [49].
    The fitness-based quasi-reflected method hybridizes the quasi-reflected method
    and fitness-based opposition. The fitness-based opposition method and quasi-reflected
    point are derived from the theory of quasi-opposition point. Fig. 3 illustrates
    the concept of the opposite ( Y ^ 0 ) , quasi-opposite ( Y ^ q 0 ) , quasi-reflected
    point ( Y ^ qr ) . The quasi-reflected point is uniformly distributed between
    the center point (c) and the point Y ^ 0 . On the other hand, the fitness-based
    Opposition (FBO) is determined by including reflected weight between the current
    population and the median of the population. The reflected weight determines the
    amount of reflection based on the population’s fitness. FBO of a point Y ^ 0 is
    defined in Eq. (9). Y fbo ={ Y+(median−Y)k, median+(Y−median)k,   if Y<median   otherwise
    (9) View Source where kϵ[0,1] is the random number. Fig. 3. Opposite points, quasi
    opposite, and quasi-reflected points defined in the domain [a b]. Show All FQR
    ( Y ^ Kr ) is derived from the quasi-reflection and fitness-based opposition method
    that utilizes the fitness of the solution candidate to generate new solutions.
    In this method, the amount of reflection is controlled by the fitness of the solution.
    The solution with higher fitness is reflected by the least amount and vice versa.
    FQR has been proposed by Ergezer [49] and is computed as: Y ^ Kr ={ Y ^ +(c− y
    ^ )K, c+( Y ^ −c)(1−K),   if  Y ^ ≤c   if  Y ^ >c (10) View Source where, Kϵ[0,1]
    represents the reflection weight. C. Design of Objective Function For the design
    of an efficient scheduling strategy, the makespan and the cost of execution of
    workflows are taken into consideration. Makespan is the total execution time of
    workflow applications, reflecting the maximum completion time for the execution
    of tasks on assigned resources. The proposed work considers two types of computing
    resources, cloud servers, and fog servers. Based on the scheduling strategy, the
    workflow tasks are executed on the cloud or fog node. The processing time of the
    task allocated to any server is determined as; w i,k = c w i ϕ k (11) View Source
    where, w i,k is the execution time of n i on k th server, and ϕ k is the processing
    speed of k th server. Let c t i,j be the communication time for the data transferred
    from task n i to n j that can be computed as follows. c t i,j = c d i,j β (12)
    View Source where β is the bandwidth between two servers. When a task ( n j )
    is allocated on a server k , it is associated with two parameters, ST and FT ,
    computed from Eq. (13) and Eq. (14). S T n j is the start time of a task n j assigned
    to a server and F T n j is the finish time of a task n j . S T n j = F T n j =
    max{F T n i +c t i,j , n i ϵpred( n j )} S T n j + w j,k (13) (14) View Source
    where, pred( n j ) is the predecessor of the task n j . The makespan ( M w ) of
    a workflow can be computed as: M w =max{F T n j , n j ϵT} (15) View Source Another
    QoS parameter is the monetary cost, which is of two types in the hybrid cloud-fog
    paradigm. The first type is the computational cost (c p k ) associated with using
    the computational power of the server k while executing task n j shown in Eq.
    (16). c p k =pb p k ∗(F T n j −S T n j ) (16) View Source where, pb p k is the
    processing base price of the k th server. The next considered cost is the communication
    cost (c c k i,j ) incurred while transferring data from task n i to n j computed
    as: c c k i,j =cb p k ∗c t i,j (17) View Source where, cb p k is the processing
    base price of the k th server. The total cost of execution is the summation of
    computation cost and communication cost derived from Eq. (18). T C w = ∑ k=1 m
    ∑ i=1 n c p k i + ∑ k=1 m ∑ i,j=1 n c c k i,j (18) View Source Based on the above
    computation of makespan and cost, the objective of the proposed work is defined.
    This work aims to reduce the execution cost such that the defined deadline constraints
    are satisfied. The considered objective function related to makespan and the cost
    is shown as: Minimize T C w (subject to  M w ≤ D w ). SECTION IV. Proposed Work
    In this section, the working of a proposed methodology called Improved Chemical
    Reaction Optimization for workflow scheduling problems in a hybrid environment
    has been proposed. The proposed method has been depicted in Fig. 4, and the pseudo-code
    for the same is discussed in Algorithm 1. The detailed characterization of ICRO-WSHE
    has been illustrated in the following sub-sections. Fig. 4. Flow chart of proposed
    ICRO-WSHE. Show All SECTION Algorithm 1. ICRO-WSHE Input: DAG, N, D w ,V M POOL
    , max iter Output: Optimized Schedule ( ω ) 1: Initialize parameter α , Population
    size = N 2: for Every molecule do 3: Initialize each molecule ω i =[ x 1 , x 2
    , x 3 ,… x D ],1≤i≤ N w , x j ∈V M POOL , 4: Compute POE of each molecule from
    5: end for 6: while iter≤ max iter do 7: Generate random number r 1 8: if r 1
    <c then 9: Select a molecule ( ω ) randomly from the population 10: if |NHit−MHit|>α
    then 11: Decompose ( ω ) into ω ′ and ω ′′ 12: if Po E ω ′ <Po E ω &&Po E ω ′′
    <Po E ω then 13: Add ω ′ and ω ′′ into the population 14: Remove ω from the population
    15: end if 16: else 17: Call on-wall ineffective collision ( ω ) using PSO equation
    18: if Po E ω ′ <Po E ω then 19: Add ω ′ into the population 20: Remove ω from
    the population 21: end if 22: end if 23: else 24: Select two molecules ( ω 1 and
    ω 2 ) from the population 25: if M ω 1 < D w && M ω 2 < D w then 26: Perform Synthesis
    ( ω 1 and ω 2 ) into ω ′ 27: if Po E ω ′ <Po E ω 1 &&Po E ω ′ <Po E ω 2 then 28:
    Add ω ′ into the population 29: Remove ω 1 and ω 2 from the population 30: end
    if 31: else 32: Perform Inter-molecular ineffective collision ( ω 1 and ω 2 )
    using PSO equations 33: if Po E ω ′ <P E ω 1 &&Po E ω ′′ <Po E ω 2 then 34: Add
    ω ′ and ω ′′ into the population 35: Remove ω 1 and ω 2 from the population 36:
    end if 37: end if 38: end if 39: end while 40: return optimal molecule ( ω ) from
    the solution A. CRO Modelling For modeling a CRO problem, two key steps are required.
    First, how the problem is encoded in the form of the molecule, and the second
    is how to estimate the fitness (in terms of PoE) of each molecule. In the workflow
    scheduling scenario, each molecule (ω) consists of two parts: The first part contains
    the set of workflow tasks, and the second part comprises the VMs on which these
    tasks are assigned. Fig. 5 represents the molecule’s structure (one complete solution
    for the considered workflow shown in Fig. 1. From Fig. 5, the DAG consists of
    11 tasks, which is the dimension of the molecule, and the value of the molecule
    in each dimension represents the servers (cloud or fog server). For the considered
    example, it has been assumed that servers (0–2) are cloud servers and servers
    (3–5) are fog servers. For the considered example, the execution time of each
    task is shown in Table I, and the pricing detail in Table II. The Makespan ( M
    ω ) obtained for the schedule shown in Fig. 5 is 28 and the total execution cost
    (T C ω ) is 20.08. TABLE I Execution Time of Tasks on Different Servers TABLE
    II Pricing of Considered VM Instances Fig. 5. A molecule representation (ω) .
    Show All Each molecule is associated with Potential Energy (PoE) (i.e., determines
    the fitness of the molecule) and Kinetic Energy (KiE) . In our work, we only consider
    PoE to avoid the complexity of the problem. Po E i =T C ω (19) View Source CRO
    is not designed to solve the constraint workflow scheduling problem. Therefore,
    to address the problem, this work utilizes the strategy proposed by Deb et al.
    [50]. In this work, a single QoS constraint (deadline) is considered. Eq. (19)
    shows that the solution satisfying the deadline with minimum execution cost has
    the minimum potential energy. There can be a situation where both schedules satisfy
    the given deadline, then the solution with minimum execution cost will have the
    minimum potential energy and will be preferred. On the other hand, if one solution
    is viable and the other is not, then a viable solution is considered. There can
    be a situation when none of the solutions satisfies the deadline constraint. In
    that case, the solution having minimum deadline violation will be preferred. After
    initializing molecules and evaluating fitness function, each molecule undergoes
    four chemical reaction operations: on-wall, decomposition, inter-molecular, and
    synthesis. The detailed description of these operations is discussed as follows:
    On-wall ineffective collision: This is a uni-molecular operation, where one molecule
    (ω) collides with the container wall and forms the new molecule ( ω ′ ) . CRO
    is good at local search but not much effective in exploring the search phase,
    which results in slow convergence and low-quality solutions. In contrast, PSO
    can converge faster; therefore, the advantages of PSO have been utilized and applied
    as a neighborhood operator in the on-wall ineffective collision. The inclusion
    of PSO as a neighborhood operator may generate better results than basic CRO has
    been illustrated in [51]. PSO is a well-known algorithm for fast convergence due
    to the efficient exploration of solution search spaces. In PSO, the update of
    a solution is based on the velocity equation; the parameters involved in the equation
    are inertia weight and social acceleration coefficients which support the efficient
    exploring of solution search spaces. It has a stronger ability to extend the solution
    search space due to an efficient global search algorithm. We take this advantage
    of PSO for the quick convergence of CRO-based workflow scheduling. The usage of
    the PSO equation to improve global search in the on-wall ineffective collision
    is illustrated as follows: Let X i,x (t)=[1 2 0 3 4 3 2 0 4] , P best =[ 3 5 2
    1 3 3 0 1 0 3 5] , and the initial velocity as V i,x =[0.2 0.6 1.4−2.9 0.5 0.3
    1 2.1 0.2−1 1] , and the other parameters as η=0.73 , r 1 =0.4 , r 2 =0.5 , b
    1 =1.5 , b 2 =1.5 , and G best as [4 4 2 1 4 3 3 2 1 4 3]. Now, the updated velocity
    can be computed as: V i,x (t+1)=0.73∗[0.2,0.6,1.4,−2.9,0.5,0.3,1,2.1,0.2,−1,1]+1.5∗0.4∗([3
    5 2 1 3 3 0 1 0 3 5]−[0 Now, round up the values and store them in a variable
    temp=[3 5 1 0 1 0 0 1 1 0 3] . We consider the lower or upper range for the value
    less than or greater than the range. For instance, –1.07 can be rounded to 0.
    For the workflow tasks, a random task n i is selected from the considered schedule,
    and then the whole schedule is scanned forward and backward until a predecessor
    or successor task is met. Then, a random position is selected, and n i is inserted.
    Let us consider task n 3 , the immediate predecessor and successor of n 3 are
    n entry and n 6 . The random position between n entry and n 6 is chosen as 6,
    and then n 3 is shifted to position 6. The new sequence still follows the topological
    sort and maintains the task dependency. Afterwards, a new molecule ( ω ′ ) is
    created, whose PoE( ω ′ ) is calculated. If PoE( ω ′ ) is less than PoE(ω) , then
    only the new molecule is accepted and stored in the population pool. Otherwise,
    it will be rejected. Fig. 6 illustrates the new molecule ( ω ′ ) structure generated
    from the on-wall ineffective collision. Fig. 6. Molecule generated from on-wall
    ineffective collision (ω) . Show All Decomposition: Like on-wall ineffective collision,
    decomposition is also a uni-molecular operation. Decomposition of a molecule ω
    results in two new molecules ω ′ and ω ′′ . The first molecule is generated using
    the OBL method named: FQR and the second molecule is generated using the PSO method.
    The OBL method has been widely adopted to enhance the potential of intelligent
    algorithms. OBL method accelerates the convergence speed by comparing the fitness
    of the solution with its opposite and considering the fitter one. The illustration
    of deriving the opposite of molecule (ω) is discussed in the following manner.
    Suppose the size of the population is 10, and all the molecules are assigned with
    reflection weight K . The molecule with the highest fitness has the least reflection
    weight, whereas molecules with less fitness have a high reflection weight. Here,
    the opposite of the molecule presented in Fig. 5 is computed. Let the rank of
    the considered molecule be 5 th in the current population, then the reflected
    weight (K) obtained is 0.2. The parameters for FQR, i.e., a= 0 and b= 5, are considered.
    From Eq. (10), FQR for the considered molecule is obtained and illustrated in
    Fig. 7. The illustration of the FQR method is provided as follows: Fig. 7. Molecules
    generated from FQR method. Show All y ^ 1 =1+(2.5−1)∗0.2=1.3=ceil(1.3)=2 , y ^
    2 =1+(2.5−1)∗0.2=1.3=2 , y ^ 3 =2+(2.5−2)∗0.2=2.1=3 , y ^ 4 =0+(2.5−0)∗0.2=0.5=1
    , y ^ 5 =2.5+(3−2.5)∗0.8=5 , y ^ 6 =2.5+(4−2.5)∗0.8=3.7=4 , y ^ 8 =2.5+(5−2.5)∗0.8=4.5=5
    The second molecule is generated from the PSO equation, which we have already
    discussed in an on-wall ineffective collision. Inter-molecular in-effective collision:
    In this operation, two new molecules are generated from the existing two molecules.
    We have again utilized PSO-based equations for the construction of two new molecules.
    Afterward, if the newly generated molecules satisfy the conditions defined in
    eq. (3), then only the newly generated molecules are accepted. Synthesis: Synthesis
    operation involves two input molecules ( ω 1 and ω 2 ) that collide with each
    other to generate a single molecule ( ω ′ ) . In this method, two molecules are
    randomly selected. Afterward, the fitness of these molecules is determined, and
    based on the fitness, the molecules are categorized as best molecule ( ω b ) and
    normal molecule ( ω n ) . Two random points are chosen between (0,n−1) , and the
    molecules are cut into three segments. Algorithm 2 illustrates the working of
    double point synthesis operation. From steps [2]–[8], both molecules are categorized
    as best molecules and normal molecules. In step 9, two random points ( P 1 and
    P 2 ) are selected such that 0< P 1 < P 2 <n−1 . Subsequently, the size of the
    schedule from P 1 to P 2 is determined and compared with half the size of the
    schedule. If the value obtained is less than half the size of the schedule, then
    the task from the molecule is selected and stored in variable temp in step 12.
    Then, a sub-schedule S1 of length | P 2 − P 1 +1| is formed where the tasks from
    temp are arranged according to the tasks in ω n . S1 has the task, and the VMs
    sequenced according to the ω n (Steps [14]–[16]). | P 2 − P 1 +1|<n/2 then, from
    index 0 to P 1 −1 and P 2 to n−1 , the schedule from ω b is copied to the new
    molecule ω ′ (steps [17]–[19] & steps [21]–[23]). Otherwise, the schedule from
    index 0 to P 1 −1 and P 2 to n−1 of ω n is copied. Algorithm 2. Double Point Synthesis
    Operator Input: ω 1 , ω 2 Output: ω ′ 1: Compute f( ω 1 ) and f( ω 2 ) 2: if f(
    ω 1 )>f( ω 2 ) then 3: ω b =f( ω 1 ) 4: ω n =f( ω 2 ) 5: else 6: ω b =f( ω 2 )
    7: ω n =f( ω 1 ) 8: end if 9: P 1 P 2 =rand(1,n−1) 10: if | P 2 − P 1 +1|<n/2
    then 11: for i= P 1 to P 2 do 12: temp← ω b .Task[i] 13: end for 14: Generate
    sub-schedule S1 15: S1.Task← sort temp according to task of ω n 16: S1.VM← ω n
    .VM 17: for i=0,i< P 1 ,i++ do 18: ω ′ [i]← ω b [i] 19: end for 20: Append S1
    at the end of ω ′ 21: for i= P 2 +1,i<n−1,i++ do 22: ω ′ [i]← ω b [i] 23: end
    for 24: else 25: for i= P 1  to  P 2 do 26: temp← ω n .Task[i] 27: end for 28:
    Generate sub-schedule S1 29: S1.Task← sort temp according to task of ω b 30: S1.VM←
    ω b .VM 31: for i=0,i< P 1 ,i++ do 32: ω ′ [i]← ω n [i] 33: end for 34: Append
    S1 at the end of ω ′ 35: for i= P 2 +1,i<n−1,i++ do 36: ω ′ [i]← ω n [i] 37: end
    for 38: end if 39: return ω ′ Further, the desired sub-schedule from (step 14
    or 28) is appended to ω ′ from the position P 1 to P 2 . Finally, a new molecule
    ( ω ′ ) is generated in step 39. An example to demonstrate the working of the
    proposed methodology is presented in Fig. 8. For illustration, consider two molecules
    ( ω 1 and ω 2 ) with an assumption that the fitness of ω 1 is greater than the
    fitness of ω 2 . Let P 1 =3 and P 2 =6 be the randomly selected point, and the
    value of | P 2 − P 1 +1| is less than 5.5. Now, store the task from P 1 to P 2
    from ω 1 in temp , temp←{ n 3 , n 4 , n 5 , n 6 } . Next, a sub-schedule S1 with
    the same task of temp but sequenced according to the ω 2 along with the mapping
    is generated. Further, the tasks and their corresponding VMs from the molecule
    ω 1 are copied to the S1 . Then, sub-schedule S1 containing tasks { n 5 , n 3
    , n 6 , n 4 } and VMs {v m 5 ,v m 2 ,v m 0 ,v m 5 } is appended at index P 1 to
    P 2 of ω ′ . Lastly, again the schedule of ω 1 from position P 2 +1 to n−1 is
    copied to ω ′ . Fig. 8. Molecule generated from double point synthesis. Show All
    SECTION V. Result and Experimental Analysis In this section, the performance of
    the proposed algorithm ICRO-WSHE is demonstrated. The algorithm is utilized over
    the hybrid fog-cloud environment to solve workflow scheduling problems with minimum
    execution cost while satisfying the deadline constraints. A detailed description
    regarding the types of scientific workflow, simulation setup, and comparisons
    of performance with existing state-of-the-art algorithms is illustrated as follows.
    A. Scientific Workflow Model Pegasus workflow generator used the information gathered
    from the real scientific workflows to approximate the workflows used in real scientific
    applications. Pegasus has published several workflows, including Cybershake, Montage,
    Sipht, Epigenomic, and Inspiral [52]. These workflows have been widely investigated
    to measure the performance of workflow scheduling algorithms. Therefore, this
    work utilizes the same workflows for comparison and evaluation. The Cybershake
    workflow is used by Southern California Earthquake Center (SCEC) to characterize
    earthquake hazards. A montage workflow combines multiple input images to create
    a custom mosaic of the sky. Epigenomic is used for genome sequencing, and Sipht
    is used to automate the search of untranslated RNA. On the other hand, LIGO Inspiral
    generates and analyzes gravitational waves. Further, the detailed structure of
    each workflow is shown in Fig. 9 [49]. Fig. 9. Structure of real-world workflows.
    Show All For evaluation, the workflow is categorized as small size (100 tasks),
    medium size (500 tasks), and large size (1000 tasks). The experimental results
    are evaluated using four different deadlines. The deadlines were evaluated such
    that the deadline value lies between the minimum and maximum runtime. These runtimes
    are calculated based on two implementation policies. In the first policy, a schedule
    with the slowest execution time is obtained by executing all the tasks on a single
    cheapest VM. The second one calculates the schedule with the fastest execution
    time, where one fastest VM for each task is leased. Afterward, to get the values
    for the deadline, the difference between minimum execution time and maximum execution
    is divided by five, and an interval size is obtained. To get the first deadline
    interval, one interval size is added to the fastest execution time. Similarly,
    to get the second interval, two interval sizes are added, and so on. In this way,
    the algorithm is analyzed as the value of the deadline varies from strict to relaxed.
    B. Simulation Setup The simulation and the experiments were conducted on the Windows
    64-bit, CPU core i7 with 8GB RAM using the CloudSim toolkit [45]. For simulation,
    four types of fog and five types of cloud servers are considered, and each server
    has different characteristics. The detailed characterization of each server is
    shown in Table III. The total number of cloud nodes is determined by the size
    of the workflow. The number of nodes is assumed to be the number of tasks that
    can be scheduled in parallel in a workflow multiplied by the types of nodes. For
    instance, if P tasks can be scheduled in parallel and there are n types of cloud
    nodes considered, we have a set of |P∗n| resources. Whereas, for the fog nodes,
    we have assumed a fixed number of nodes. TABLE III Characteristics of Considered
    VM Instances To perform and validate the experiments, some essential parameters
    need to be initialized. For a fair comparison, the population size of every algorithm
    is considered to be 50 and the maximum number of iterations to be performed is
    100. All the algorithms are executed ten times for every case such that the uncertainties
    are avoided and the average result is obtained. For DNCPSO and ICRO-WSHE, the
    learning factor b 1 and b 2 is assumed to be 2 and η= 0.5. For ICRO-WSHE, CR-AC,
    and CRO α is assumed to be 5. C. Comparison of ICRO-WSHE With Other Existing Algorithms
    To date, very few workflow scheduling algorithms in the hybrid cloud-fog environment
    have been designed. For measuring the performance of the proposed ICRO-WSHE, it
    has been compared with Directional and Non-local Convergent PSO [39], ISSS-FQR,
    hybrid chemical reaction optimization, and ant colony optimization (CR-AC) [53]
    and the basic Chemical Reaction Optimization (CRO). DNCPSO and ISSS-FQR are designed
    for the hybrid cloud-fog environment and therefore have been selected for comparison.
    On the other hand, CR-AC is designed to solve workflow scheduling problems only
    for the cloud environment. However, the CR-AC is the latest algorithm based on
    Chemical Reaction Optimization to minimize the execution cost of deadline-constraint
    workflows. Therefore, it has been used in this work for comparison. The brief
    working of these algorithms has been illustrated as follows: DNCPSO is based on
    the particle swarm optimization method. The algorithm has non-linear inertia weight,
    which uses directional search spaces for the selection and mutation operations.
    DNCPSO is designed to overcome the shortcoming of the existing PSO algorithm that
    traps local optimum solutions and prevents converging at optimal solutions. The
    PSO has been improved by following three aspects. First, the parameter adjustment
    for the non-local convergent is analyzed, and the non-linear inertia weight is
    proposed to balance local and global search strategies. Second, the search strategy
    of DNCPSO has been improved using auxiliary operations. The velocity and position
    information has been updated with some additional procedures to decide the direction
    of the optimal solution. Next, DNCPSO includes the selection and the mutation
    operation that enhances the local search operation. These three improvement aspects
    can optimize the local and global search abilities to evolve the population more
    quickly towards the convergence to optimal solutions and avoid falling into the
    local extremum. From the simulation result, it has been proved that DNCPSO can
    achieve better performance than the rest of the classical algorithms. However,
    DNCPSO has not considered the workflows having QoS constraints in terms of deadline,
    budget, and security. CR-AC is a cost-effective algorithm designed for workflow
    scheduling problems under deadline constraints in the cloud environment. CR-AC
    is the hybridization of the Chemical reaction optimization and Ant colony optimization
    (ACO) method. The algorithm comprises two phases: First, modified CRO is applied,
    and the ten best solutions are selected. While in the second phase, modified ACO
    is applied to the selected solutions for a specific time to minimize the cost.
    Unlike the traditional methodology, CR-AC enhances not only one solution but ten
    solutions and thus provides a large search space. However, the algorithm ignores
    the data transfer time and is not suitable for data-intensive applications. Further,
    it has utilized expensive cloud resources that cause an increase in monetary cost.
    ISSS-FQR is our recently proposed algorithm designed to solve complex and uncertain
    computation offloading problems in a fog-cloud environment. The proposed algorithm
    includes an intelligent fuzzy scheduler to offload the uncertain task to the appropriate
    resources. ISSS-FQR utilized a recently developed salp swarm algorithm to learn
    and optimize the task-resource allocation rules. In addition to this, an opposition-based
    learning method named: FQR is also incorporated to overcome the shortcomings of
    the salp swarm algorithm. From the simulation result, it has been proved that
    ISSS-FQR outperforms the rest of the existing algorithms by generating the most
    efficient schedules. However, ISSS-FQR is more complex and has a higher time complexity
    than ICRO-WSHE. D. Complexity Analysis Regarding the time complexity of ICRO-WSHE,
    in each iteration, various operations in terms of collision, decomposition, or
    synthesis have been performed. The time complexity of the synthesis algorithm
    proposed in Algorithm 2 is O(T) , where T is the number of tasks. For a given
    DAG, the number of edges could be T 2 . Therefore the overall time complexity
    is O( T 2 ∗V∗N∗ma x iter ) , where V is the total resources, N is the population
    size, and ma x iter is the maximum number of iterations. DNCPSO, CRO, and ICRO-WSHE
    have the same time complexity. While ISSS-FQR has the time complexity of O(N∗
    T 2 ∗V∗ma x iter +T) and CR-AC has O(N∗ T 2 ∗V+N∗ T 2 ∗V∗ma x iter ) . E. Result
    and Discussions 1) Deadline Constraint Evaluation: The algorithms are evaluated
    based on different deadline constraints. Fig. 10 illustrates the percentage of
    deadlines meet for every deadline interval for the given workflow. For Cybershake
    workflow (Fig. 10(a)) at deadline interval 1, DNCPSO and CRO fail to meet the
    strict constraint while ICRO-WSHE achieves 65% of the deadline met. ICRO-WSHE
    reaches 85.25% of overall success, while ISSS-FQR has 81.25% of success which
    is slightly less than the ICRO-WSHE. DNCPSO and CRO have the worst performance
    for all the deadline intervals. In Fig. 10(b), the result for the Montage workflow
    is shown for the considered range of deadlines. For the strict interval 1, the
    performance of the proposed ICRO-WSHE is not up to the mark because most of the
    schedules generated violate the strict deadline. However, ICRO-WSHE improves its
    performance when constraints are relaxed. Again, DNCPSO shows underperformance
    by achieving only 30% of overall success. While CRO shows 100% failure at the
    strict constraint of 1 and 2. For deadline interval 4, ICRO-WSHE and ISSS-FQR
    show 100% success while CR-AC has 90% success. Fig. 10. Percentage of deadline
    meets of different workflows. Show All For Epigenomic workflow (Fig. 10(c)), at
    the strict constraint of deadline interval 1 the proposed ICRO-WSHE and ISSS-FQR
    show similar performance. Further, the performance of the ICRO-WSHE is improved
    for the deadline interval 2. In the case of DNCPSO, its performance is enhanced
    for the deadline interval 1 and 3 and exhibits similar performance to CR-AC. Overall
    the percentage of deadlines met for ICRO-WSHE is 67.5%, ISSS-FQR has 66.25%, CR-AC
    has 60.75%, DNCPSO has 58% and CRO has 56.5%. Fig. 10(d) illustrates the performance
    of algorithms for Sipht workflow. At deadline interval 1, ISSS-FQR has the highest
    percentage of deadline meet followed by the proposed ICRO-WSHE. On the other hand,
    DNCPSO and CRO show less than 10% of success. For the relaxed interval of 4, both
    ICRO-WSHE and ISSS-FQR show 100% of success while CR-AC has near about 95% success,
    DNCPSO has 90% success and CRO has 85% of success. The result for the percentage
    of deadlines met for LIGO has been shown in Fig. 10(e). From Fig. 10(e), it has
    been demonstrated that ISSS-FQR delivers remarkable performance at strict intervals.
    The proposed approach has better performance than CR-AC, DNCPSO, and CRO for most
    of the range of deadline intervals. For the strict constraint of intervals 1 and
    2, DNCPSO and CRO show 100% failure. However, their performance has been improved
    when constraints are relaxed. Overall, DNCPSO is outperformed by all the algorithms
    and achieves the lowest percentage of deadlines met. The reason for the underperformance
    of DNCPSO is that it has not considered QoS constraints and uses a fixed number
    of cloud resources. For CR-AC, the average percentage of deadline meetings is
    only 62.875%. The reason for the failure of CR-AC is that it has only utilized
    cloud resources, which may lead to data transmission delay as well as increased
    cost of execution. The ISSS-FQR and the proposed ICRO-WSHE show significant performance
    because of a good balance between cloud and fog resources. However, in most cases,
    the performance of ICRO-WSHE is slightly better than ISSS-FQR and thus proves
    to be the most efficient algorithm. 2) Makespan Evaluation: The normalized makespan
    obtained for each workflow is displayed in Fig. 11. The normalized makespan can
    be computed from Eq. (20). Normalized Makespan= D w M w (20) View Source A schedule
    is successful if its makespan is less than the defined deadline. The schedule
    has a normalized makespan greater than one is said to be successful, which means
    the considered schedule has met the defined deadline constraints. Otherwise, the
    schedule that violates the defined constraints is the failed schedule. Boxplots
    have been constructed at each deadline interval for comparative studies of normalized
    makespan obtained for each workflow. For Cybershake workflow (Fig. 11(a)) at the
    strict deadline interval 1, the proposed ICRO-WSHE shows remarkable performance
    by obtaining a higher normalized makespan value than the other algorithms. The
    average normalized makespan of ICRO-WSHE is above 1, which means that most of
    the schedules generated are valid. The ISSS-FQR performs better than all algorithms
    for every range of deadlines. When the constraints are relaxed to 3 and 4, ICRO-WSHE
    and ISSS-FQR generate 100% valid schedules. In the case of CR-AC, DNCPSO, and
    CRO it has been observed that even at relaxed constraints, they are generating
    some schedules of longer makespan than the deadline. Fig. 11. Makespan evaluation
    at different deadline constraints. Show All The result for the Montage workflow
    is shown in Fig. 11(b). At the strict interval of 1, CR-AC exhibits exceptional
    performance. CR-AC has the highest median value than the rest of the algorithm
    for interval 1. Most of the schedules generated at strict intervals by all the
    algorithms are not valid. However, the performance of algorithms gets improved
    when constraints are relaxed. For intervals 2 and 3, ICRO-WSHE has a median value
    higher than the other algorithms and has 80% successful schedules. On the other
    hand, DNCPSO and CRO have a median value lower than one for most of the deadline
    constraints, which means failed schedules are generated most of the time. For
    interval 4, it has been observed that ISSS-FQR has a higher normalized value than
    all the algorithms and is more time-efficient. In contrast, the proposed ICRO-WSHO
    has generated schedules within the deadline with minimum cost. Fig. 11(c) shows
    the comparative studies of epigenomic workflow. For the epigenomic workflow, ISSS-FQR
    proves to be a more efficient algorithm in generating schedules of minimum makespan
    for all the ranges of deadlines. On the other hand, ICRO-WSHE satisfies most deadline
    constraints and generates cost-efficient schedules. CR-AC, DNCPSO, and CRO fail
    to meet the goal by generating a makespan longer than the defined deadline at
    interval 1. For the relaxed interval of 3 and 4, CR-AC, DNCPSO, and CRO generate
    most of the schedules that satisfy the deadline, but CR-AC is more time-efficient
    than DNCPSO and CRO. For the Sipht workflow (Fig. 11(d)), most of the schedules
    generated by ICRO-WSHE and ISSS-FQR have a median greater than 1, which means
    successful schedules are generated most of the time. For interval 1, ISSS-FQR
    achieves a higher percentage of success than ICRO-WSHE. However, when the constraint
    is relaxed to 2, ICRO-WSHE performs similarly to ISSS-FQR. Further, when constraints
    are more relaxed, ICRO-WSHE has a lower makespan than all the algorithms considered.
    Again DNCPSO and CRO fail to generate a 100% successful schedule even at the relaxed
    constraint of interval 4. From Fig. 11(e), ISSS-FQR generates schedules of minimum
    makespan than the rest of the algorithms at the deadline interval 1. However,
    most of the schedules generated at deadline constraint 1 violate the strict deadline.
    When the constraints are relaxed to 2, the ICRO-WSHE improves the performance
    and proves to be the most time-efficient algorithm. However, for constraints 3
    and 4, ISSS-FQR is more time-efficient, whereas CRO is the least time-efficient
    algorithm. Overall from the above comparative analysis, it can be concluded that
    the performance of ISSS-FQR and ICRO-WSHE have satisfied most of the deadline
    constraints and proven to be efficient algorithms. DNCPSO and CRO are not efficient
    in meeting the deadlines while the other algorithms are. In most cases, CR-AC
    has the least difference between its median value and the median value of the
    best-performing algorithm. The exceptional performance of CR-AC has also been
    observed for the Montage workflow at strict deadline constraints. On the other
    hand, DNCPSO and CRO mostly produce makespan longer than the deadline and have
    a significant difference between their median value and the median value of others.
    3) Cost Evaluation: The average execution cost of each workflow is shown at different
    deadline intervals from Fig. 12 to Fig. 16. The reason for showing an average
    makespan is that cost-efficient schedules are generated but not at the expense
    of violating the deadline. A dotted reference line has been shown in each bar
    graph that represents the deadline constraint. The algorithm satisfying the given
    deadline constraint with minimum execution cost will be considered the best algorithm.
    However, there can be a situation when none of the algorithms satisfy the given
    constraint. Then, in that case, the algorithm with minimum deadline violation,
    irrespective of the cost, will be preferred because generating the cheapest schedule
    but not meeting the deadline is of no use. Fig. 12. Cost evaluation of Cybershake.
    Show All Fig. 13. Cost evaluation of Montage. Show All Fig. 14. Cost evaluation
    of Epigenomic. Show All Fig. 15. Cost evaluation of SIPHT. Show All Fig. 16. Cost
    evaluation of LIGO. Show All Fig. 12 illustrates the cost and makespan evaluation
    concerning different deadline intervals for the Cybershake workflow. At the strict
    deadline ( D w =1) , only ICRO-WSHE satisfies the given deadline constraint, while
    CR-AC, ISSS-FQR, DNCPSO, and CRO violate the constraint. ISSS-FQR proves to be
    more cost-efficient at interval one but at the expense of deadline violation.
    In addition, all the algorithms have shown similar performance at ( D w =2) .
    However, the performance of ISSS-FQR has been improved at the interval ( D w =3,
    D w =4) and proves to be the second-best performing algorithm at relaxed constraints.
    Most of the time, schedules generated by CR-AC are expensive because of the utilization
    of only cloud resources. The result for the Montage workflow is shown in Fig.
    13. From Fig. 13, it has been demonstrated that for interval 1, all the algorithms
    violate the strict deadline constraint. However, CR-AC has a minimum difference
    between the achieved average makespan and the deadline and proves to be a time-efficient
    algorithm. CR-AC utilizes a higher execution cost than the rest of the algorithm,
    while DNCPSO has a minimum execution cost. At intervals 2 and 3, ICRO-WSHE and
    ISSS-FQR improve their performance and generate most of the valid schedules. ISSS-FQR
    proves to be more time-efficient than ICRO-WSHE at intervals 2 and 3, but when
    constraints are relaxed further, ICRO-WSHE proves to be more efficient. For the
    Epigenomic workflow (Fig. 14), all the algorithms failed to generate a valid schedule
    at interval 1. CRO has a higher average makespan than the rest of the algorithm
    for all the deadline intervals. Generally, CR-AC is not much cost-efficient algorithm
    because it has utilized only expensive cloud resources. When the constraints are
    relaxed, the proposed ICRO-WSHE proves the cost-efficient algorithm. For the Sipht
    workflow (Fig. 15), ICRO-WSHE and ISSS-FQR generate most of the valid schedule
    by achieving an average makespan below the deadline reference line. However, ICRO-WSHE
    is more cost-efficient than ISSS-FQR. DNCPO generates the cheapest schedule for
    the deadline interval (Dw=1, Dw=2) but with a makespan longer than the defined
    deadline. For all the deadline intervals, ISSS-FQR achieves a minimum average
    makespan but with a higher execution cost. On the other hand, ICRO-WSHE is the
    most significant algorithm in terms of minimum execution cost and the number of
    deadline violations. For LIGO workflow (Fig. 16), at interval 1, ICRO-WSHE has
    a makespan near the defined deadline and generates a minimum cost schedule. DNCPSO
    and CRO prove to be cost-efficient algorithms but at the expense of violating
    deadlines. On the other hand, ISSS-FQR has an average makespan less than the defined
    deadline for all the deadline intervals but at the expense of higher execution
    costs. Overall from the above analysis, it can be concluded that CR-AC, DNCPSO,
    and CRO are capable of generating valid schedules when the constraints are relaxed.
    For most workflows, DNCPSO is cost-efficient but at the expense of deadline violation.
    On the other hand, CR-AC generates schedules with a makespan lower than the defined
    deadline but at the expense of very high execution costs. ICRO-WSHE shows remarkable
    performance for Cybershake, Epigenomic, and LIGO, where ICRO-WSHE satisfies most
    of the deadlines and generates the cheapest schedule. Indeed, in some cases, ISSS-FQR
    is more time-efficient, but the main objective is to satisfy the deadline and
    minimize costs. Therefore, in this context, the proposed ICRO-WSHE is more restricted
    towards the concerned goal of reducing total execution costs while meeting the
    deadline. SECTION VI. Conclusion and Future Work The complexity and uncertainty
    in cloud-fog environments enhance the difficulties of the traditional scheduling
    algorithms. To deal with such issues, this paper proposes a novel algorithm called
    the Improved Chemical Reaction Optimization algorithm for workflow scheduling
    in a hybrid environment (ICRO-WSHE). Although the existing CRO technique has various
    advantages, there are some shortcomings, like other meta-heuristic algorithms.
    For example, CRO can fall into the local optimum and have difficulties obtaining
    optimal solutions. Therefore, to enhance the performance of the CRO algorithm,
    in this work, we have incorporated good features of the PSO algorithm and the
    opposition-based learning (OBL) method for improving exploration and exploitation
    mechanisms. Based on the above improvement aspects, a new approach, ICRO-WSHE,
    has been proposed and applied to scheduling deadline-constrained workflows in
    a hybrid cloud-fog environment. The proposed system combines chemical reaction
    optimization, particle swarm optimization, and fitness-based quasi-reflection
    method (FQR) to obtain an efficient scheduling strategy. Further, the double point
    synthesis operation is also proposed to increase the exploration rate. Extensive
    simulations have been performed over the five different types of workflows. Extensive
    simulations have been performed over the five different types of workflows. Comparisons
    are performed on various performance metrics, including the percentage of deadlines
    met, efficiency in terms of makespan, and execution cost. The obtained result
    shows that the overall percentage of deadline meeting for ICRO-WSHE is 75.15%,
    ISSS-FQR is 74.9%, CR-AC has 61.4% DNCPSO has 42.95%, and CRO has 41.45%. From
    the result analysis, it has been illustrated that the proposed ICRO-WSHE is the
    most significant algorithm for satisfying the deadline and generating cost-efficient
    schedules. More investigations are intended to be carried out for multi-objective
    optimization for workflow scheduling problems in a hybrid cloud-fog environment
    for future work. ACKNOWLEDGMENT The authors wish to express their gratitude and
    heartiest thanks to the Department of Computer Science & Engineering, Indian Institute
    of Technology (Indian School of Mines), Dhanbad, India, for providing their continuous
    research support. Authors Figures References Keywords Metrics More Like This Distributed
    Task Scheduling in Serverless Edge Computing Networks for the Internet of Things:
    A Learning Approach IEEE Internet of Things Journal Published: 2022 A Cost-Driven
    Intelligence Scheduling Approach for Deadline-Constrained IoT Workflow Applications
    in Cloud Computing IEEE Internet of Things Journal Published: 2024 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Network and Service Management
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Improved Chemical Reaction Optimization With Fitness-Based Quasi-Reflection
    Method for Scheduling in Hybrid Cloud-Fog Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Diro A.A.
  - Reda H.T.
  - Chilamkurti N.
  citation_count: '14'
  description: Technological advancements in wireless communications and electronics
    have enabled the rapid evolution of smart things connected to the Internet. Traditional
    networks face the challenges of scalability, real-time data delivery, programmability
    and mobility to support these smart objects collectively known as Internet of
    Things (IoT). To solve these issues, the integration of two emerging network technologies,
    namely, software defined networking (SDN) and Fog computing have gained a momentum
    as a novel model that support IoT architecture for manageability and low latency.
    SDN has a logically centralized network control plane, which is used for implementing
    sophisticated mechanisms of traffic control and resource management. On the other
    hand, Fog computing enables IoT devices’ data to be processed and managed at the
    network edge, thus providing support for applications that require very low and
    predictable latency. Though the communication latency is substantially reduced
    by the adoption of distributed fog layer closer to IoT ends, the latency overhead
    in the IoT/fog network is not only because of long distance between IoT devices
    and the cloud, but it is also caused by flow entry installation delay, which comes
    from limitations in data and control space designs. Traditional fog networks lack
    priority based fine-grain control over allocation of flows, and this incurs unnecessary
    delay for critical packets. The impact of packet blocking on QoS delivery could
    be reduced if the programmability power of SDN approach is employed in IoT applications
    for priority oriented flow space management in fog networks. In this paper, we
    propose a converged SDN and IoT/fog architecture which employs differential flow
    space allocation for heterogeneous IoT applications per flow classes to satisfy
    priority based quality of service requirements. Our analytical results demonstrate
    that urgent flow classes are served more efficiently than Naïve approach without
    compromising fairness of allocation for normal flow classes.
  doi: 10.1007/s12652-017-0677-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Ambient Intelligence and
    Humanized Computing Article Differential flow space allocation scheme in SDN based
    fog computing for IoT applications Original Research Published: 17 January 2018
    Volume 15, pages 1353–1363, (2024) Cite this article Download PDF Access provided
    by University of Nebraska-Lincoln Journal of Ambient Intelligence and Humanized
    Computing Aims and scope Submit manuscript Abebe Abeshu Diro, Haftu Tasew Reda
    & Naveen Chilamkurti   743 Accesses 22 Citations Explore all metrics Abstract
    Technological advancements in wireless communications and electronics have enabled
    the rapid evolution of smart things connected to the Internet. Traditional networks
    face the challenges of scalability, real-time data delivery, programmability and
    mobility to support these smart objects collectively known as Internet of Things
    (IoT). To solve these issues, the integration of two emerging network technologies,
    namely, software defined networking (SDN) and Fog computing have gained a momentum
    as a novel model that support IoT architecture for manageability and low latency.
    SDN has a logically centralized network control plane, which is used for implementing
    sophisticated mechanisms of traffic control and resource management. On the other
    hand, Fog computing enables IoT devices’ data to be processed and managed at the
    network edge, thus providing support for applications that require very low and
    predictable latency. Though the communication latency is substantially reduced
    by the adoption of distributed fog layer closer to IoT ends, the latency overhead
    in the IoT/fog network is not only because of long distance between IoT devices
    and the cloud, but it is also caused by flow entry installation delay, which comes
    from limitations in data and control space designs. Traditional fog networks lack
    priority based fine-grain control over allocation of flows, and this incurs unnecessary
    delay for critical packets. The impact of packet blocking on QoS delivery could
    be reduced if the programmability power of SDN approach is employed in IoT applications
    for priority oriented flow space management in fog networks. In this paper, we
    propose a converged SDN and IoT/fog architecture which employs differential flow
    space allocation for heterogeneous IoT applications per flow classes to satisfy
    priority based quality of service requirements. Our analytical results demonstrate
    that urgent flow classes are served more efficiently than Naïve approach without
    compromising fairness of allocation for normal flow classes. Similar content being
    viewed by others Software-Defined Dew, Roof, Fog and Cloud (SD-DRFC) Framework
    for IoT Ecosystem: The Journey, Novel Framework Architecture, Simulation, and
    Use Cases Article 22 March 2021 Software-Defined Fog Network Architecture for
    IoT Article 24 October 2016 An SDN Based Distributed IoT Network with NFV Implementation
    for Smart Cities Chapter © 2020 1 Introduction BY the end of this decade, the
    exponential growth of connected smart things, known as Internet of Things (IoT),
    is estimated to be about six times the population of the world (Diro et al. 2016).
    The adoption speed of these smart devices is unprecedentedly about five folds
    of the adoption of electricity and telephony altogether. The global connection
    of daily used devices (refrigerators, fans), smart city applications (connected
    cars, smart traffic lights, smart grids, and smart water utilities), etc. are
    the main factors for this growth. Technological advancement in wireless communications
    and electronics is another reason that has enabled the rapid evolution of smart
    things connected to the Internet. This massive interconnection, where edge nodes
    such as LTE/5G base stations, Wi-Fi access point, road side units, etc are communicating
    with IoT devices, is supported by what is known as fog computing (FC). FC is an
    expansion of cloud computing into the physical world of smart things designed
    to process events and data closer to the source (Mukherjee et al. 2017). Apart
    from proximity of data processing, storage, control and communication, the overwhelming
    variety of traffic generated by IoT devices need flexible and scalable management
    of network resources due to the underlying heterogeneity, flow space capacity
    limitations and the need for differential quality of services (QoS) requirements.
    Traditional fog networks face the challenges of manageability and centralized
    control, and programmability. The emergence of SDN and network function virtualization
    is profoundly transforming the way networks are managed. SDN has a logically centralized
    network control plane, which is used for implementing sophisticated mechanisms
    for resource optimization (Al Shayokh et al. 2016). This includes resource allocation
    mechanisms such as virtual machine migration, traffic monitoring, application-aware
    control, flow space allocation, etc. Fig. 1 shows the general architecture of
    SDN. On the other hand, FC enables data from IoT devices to be managed at the
    network edge, thus providing support for applications that require very low and
    predictable latency. Additionally, FC can complement the SDN by providing scalability,
    and mobility to support IoT devices. However, the low and predictable latency
    promises of FC could be jeopardized unless the limited resources such as flow
    space are managed properly by the programmability of SDN. To solve these issues,
    we propose an integration of emerging network technologies: software defined networking
    (SDN) and FC, as a novel model of IoT architecture for manageability, programmability,
    mobility and low latency. In this study, we present detailed insight into alleviating
    a flow space contention in heterogeneous IoT applications. In particular, we propose
    a differential flow space allocation (DFSA) algorithm in a converged SDN and IoT/fog
    networking to ensure their QoS requirements and satisfy performance measures such
    as reducing packet delay and probability of lost packets during flow space contention,
    and maximize throughput. Fig. 1 SDN general architecture (Tomovic et al. 2017)
    Full size image The focus of our study is a differential and priority driven flow
    space allocation that is designed to favor urgent and critical flows and considering
    fairness among the normal packet flows in the fog-to-things communication paradigm
    using the programmability of SDN approach. In this paper, our main contributions
    include: 1) Study of current state-of-the-art SDN based fog/IoT networks 2) Propose
    a novel architecture integrating SDN/OpenFlow and IoT/fog networks in a scalable
    and manageable way 3) Propose a novel DFSA algorithm for distributed architecture
    of SDN based FC that supports QoS requirements for IoT environments 4) Evaluate
    the performance of our system in comparison with non-QoS SDN systems in terms
    of performance measures such as packet blocking probability, throughput, and utilization.
    The remaining of this paper is organized as follows. We first briefly discuss
    problem statement in Sect. 2. Section 3 provides an architectural overview of
    SDN for fog/IoT computing. We present existing works in Sect. 4. Then, we comprehensively
    discuss our proposed system and performance analysis in Sects. 5 and 6 respectively.
    Finally, we explain performance analysis in Sect. 6 and conclude our paper in
    Sect. 7. 2 Problem statement The resource constraints of IoT devices are supposed
    to be solved by the advantage of services offered by the cloud for data storage
    and processing. However, cloud services cannot fulfil the promises (Mavromoustakis
    et al. 2017) because they are originally designed for data centers which have
    different requirement from the edge network. In other words, the emerging IoT
    applications require real-time communications and mobility support (e.g. smart
    traffic lights), which requires a low network latency. Therefore, a distributed
    intelligence which extends the cloud closer to IoT devices, known as FC, plays
    a fundamental role to support edge communications and services. Through the fog-to-things
    communication, FC architecture can solve the bandwidth, latency, and communications
    challenges associated with next generation networks that will utilize IoT. In
    addition, FC saves network resources and response time because transmitting data
    to any central point in the network such as cloud consumes a large bandwidth,
    and such unavoidably leads to large reaction time which may not be tolerated by
    some real-time applications (Sharkh et al. 2013). Though the communication latency
    is substantially reduced by the adoption of distributed fog layer closer to IoT
    ends, flow space management at the fog nodes also plays significant role in decreasing
    latency. The latency overhead in the IoT/fog network is not only because of the
    large distance between IoT devices and the cloud, but it is caused due to queuing
    delay, which comes from control space overhead and flow table buffer limitations.
    Due to its static nature of configuration and limited manageability, it is not
    possible to manage network traffic as per queue requirements of traditional fog
    networks, and therefore, packet blocking or dropping is inevitable. In other words,
    traditional fog networks lack fine-grain control over allocation of flow space
    for urgent flows, and this incurs unnecessary delay for critical packets. The
    impact of packet delay because of packet dropping or blocking could be reduced
    if the flexibility of management and innovation of SDN approach is employed for
    IoT applications for flow space management in fog networks. While SDN provides
    all the necessary flexibility for managing traffic, currently it treats all packets
    with equal priority for flow space allocation. In classical SDN/Openflow (Fig.
    2), the incoming packets are matched against flow entries in the flow table. If
    the flow information is not found in the flow table, the data plane sends the
    packet to the controller so that the controller reactively installs the flow information
    for the packet. As SDN was designed for data networks and WANs, the size of the
    flow table and controller capacity were not an issue in SDN. However, flow tables
    in edge networks are so limited in size that some critical packets might be blocked
    or dropped because of flow table capacity limitations, and burdens in the communication
    overhead. The consumption of flow space has grown exponentially as the result
    of the increment of matching fields from layer 2 to Layer 4 or beyond for fine-grain
    traffic management. This situation is due to more frequent message exchange between
    data and control planes to evict the existing flows and replace with the incoming
    flows, which causes the delay or drop of flow installation. In addition, the expensive
    and power consumption nature of Ternary Content Addressable Memory (CAM) technology
    has hindered to increase its capacity. The practical aspect is that applications
    differ in urgency, and need to be served differently to maintain their QoS requirements.
    This can be solved by partitioning flow space per QoS classes of incoming packets
    on the routing nodes in the fog network. Thus, the differential flow space management
    of heterogeneous IoT applications is inevitable at the network edge such as base
    stations, which is viably managed and supported by global SDN controller. Fig.
    2 Traditional flow processing in OpenFlow Full size image 3 Leveraging SDN for
    IoT/Fog computing In this section, we briefly discuss the basics of SDN and FC
    paradigms, and then present their envisioned combination together to propose a
    new architecture and service model. 3.1 Software defined networking In traditional
    networking, the control and data plane are tightly coupled with a network device
    such as router. The function of the control plane is controlling the behavior
    of a network by signaling, routing, system configuration and management. The data
    plane is tasked only with forwarding packets to the next hop. Network management
    functions are performed at very low level, and the defined network policies for
    network devices are updated manually. Thus, it lacks dynamic optimization of network
    resources. However, SDN is a new networking paradigm which separates and abstracts
    data plane and control plane in a network. Network intelligence is logically centralized
    in a controller layer and abstracted from the underlying physical network. The
    network functions such as route calculation, gathering topology information and
    link discovery are moving to control plane leaving only packet forwarding to the
    data plane. SDN has more efficient network management, more flexibility in responding
    to demands and faster innovation than the traditional networking paradigms. OpenFlow
    is an open communication protocol which connects the controller layer and the
    infrastructure layer of the SDN architecture. SDN controller and switch interacts
    via secure channel using the OpenFlow protocol. Network control application program
    interface or north-bound interfaces are exposed to network administrators for
    developing network management applications to more effectively run their networks.
    OpenFlow introduces new features that enable us to create and manage networks
    which are not possible to create and manage with internet protocol and Ethernet
    protocols (McKeown et al. 2008; Shahmir 2013). As the most appealing technology
    for future networks, SDN and its popular standard OpenFlow are attracting huge
    interests [8]. Although it is drawing tremendous attention in the research community
    for its promising innovation, SDN has got inherent problems such as the bottleneck
    of the controller and the flow table while exchanging messages with data plane
    during flow entry installation process. This flow entry installation bottleneck
    limits the implementation of SDN/OpenFlow networks for delay intolerant packets
    which need urgent and guaranteed service (Hu et al. 2014; Sharma et al. 2013,
    Tomovic et al.2014). 3.2 IoT/Fog computing Cloud computing can be used for global
    and long-term data analytics for distributed IoT devices as it provides advantages
    of data storage and processing capabilities (Fig. 3). However, these myriads of
    devices cannot afford the bandwidth and power requirements to be served by the
    cloud. Most of the IoT applications require real-time interaction and mobility
    support, which makes network latency a critical factor to consider. FC is an emerging
    IoT supporting architecture that brings data processing, storage, analytics, and
    communication closer to the network edge. It is an extended cloud closer to the
    ground because of its proximity to end-users, distribution, context-awareness
    and its support for mobility. The fog layer complements the cloud by providing
    things-to-cloud continuum services by introducing a new intermediate layer (Fig.
    4). Fig. 3 Traditional IoT architecture (Tomovic et al. 2017) Full size image
    Fig. 4 The role of fog layer in IoT architecture (Mukherjee et al. 2017) Full
    size image This intermediate layer consists of dedicated or virtualized nodes
    equipped with communication interface acting as a router. The data generated or
    collected by IoT devices is processed locally on fog nodes for faster service.
    The challenge is the resource constraints of fog nodes, which needs special management
    for incoming packets. There is a need to integrate FC with SDN for managing the
    flow space allocations at the fog nodes to prioritize critical packets. The integration
    enhances QoS for many heterogeneous IoT applications, and it also significantly
    reduces bandwidth consumption in the network. Thus, applications benefit from
    the reduced service costs. The integration of SDN and IoT is achieved by a fog
    node acting as a router with centralized controller. The importance of integrating
    SDN into IoT/fog network lies on the combined benefits of both technologies: programmability,
    easy to manage, and low latency. Even though FC brings predictable low delays
    for IoT applications, it is still crucial to have a global controller which has
    the knowledge of the entire network to differentiate incoming massive network
    traffic of IoT into different classes based on their QoS requirements. To deploy
    SDN based FC, an orchestration is required to manage resources and services across
    edge computing. 4 Related works In this section, we present recent related works
    on SDN and FC techniques in the IoT environment with integrated resource allocation
    schemes. Previous and existing works of flow entry installation for SDN networks
    typically assume that the flow space of each node can handle an infinite number
    of flow entries, which makes the controller easy to design (Curtis et al. 2011;
    Yu et al. 2010). Moreover, in the traditional SDN architecture the whole capacity
    of flow space is shared among the different applications. In practice, however,
    this assumption is not practical, and SDN nodes’ flow space capacity can become
    a significant bottleneck for scaling and guaranteeing QoS in SDN networks. Few
    related works (Tomovic et al. 2014, Kim 2010) have been conducted on QoS issues
    in the data plane of SDN for forwarding using mechanisms such as rate shaping
    and priority queueing. These works seem to offload flow processing functions from
    the control plane by restricting the packets to data plane. However, implementing
    flow management mechanism in the data plane alone violates OpenFlow principles
    (Vishnoi et al. 2014), and could lead to performance bottleneck and QoS degradation
    in the data plane. Studies in (Hassas Yeganeh and Ganjali 2012; Kyung et al. 2014)
    considered multi-controller based flow management and load balancing as a mechanism
    of guaranteeing QoS requirements. The works ignored to consider the incoming packets
    to control plane, which can create performance burdens and signaling overhead
    on the controllers. At the same time, employing timeout based flow replacement
    algorithms in SDN based IoT applications is not suitable on control plane as it
    does not guarantee the QoS of real-time packets of IoT devices. This is due to
    the fact that the abundance of normal flows in IoT/fog networks can cause urgent
    flows to be evicted from the flow table. Therefore, energy and computationally
    efficient flow space allocation scheme which handles both control plane and data
    plane is necessary for real-time IoT applications supported by SDN and fog computing.
    To the best of our knowledge, there is no previous work which integrates SDN and
    fog computing based flow space allocation for guaranteeing QoS requirements. Unlike
    the existing researches that have been conducted on traditional networks, our
    scheme employed differential flow space allocation that handles both control and
    data planes. While traditional SDN and other extensions focus on FCFS based flow
    entry management, our scheme proposes priority class oriented flow space provision.
    Table 1 summarizes the comparison of previous works and the proposed scheme. The
    details of our work is discussed in section V. Table 1 Comparisons of proposed
    and previous works Full size table 5 The proposed system The flow space allocation
    could be divided into complete flow space partitioning (CFSP), complete flow space
    sharing (CFSS), sharing flow space with maximum capacity(SFSMa), sharing flow
    space with minimum capacity (SFSMi) and the combinations. CFSP is the scheme in
    which a flow space is partitioned among N classes of flows with S servers. The
    success of this scheme depends on the fair and the optimal division of the flow
    space, but it is difficult to decide how much percentage of the flow space a specific
    class of packet shares. The CFSS scheme, as opposed to the CFSP, is a sharing
    scheme in which packets are admitted to the flow space if the space is free, without
    partitioning. This scheme is more efficient than complete partitioning under undifferentiated
    traffic admissions and balanced rate of packets admissions, i.e. packets with
    the same rate, under the unloaded system. However, it fails if the packets of
    multiple classes of different input rates because it inclines to the most frequent
    and higher input rates than packets which need urgent services. Most of the flow
    space allocation schemes of SDN employ CFSS mechanism using a timeout mechanism.
    The consequence of this allocation makes the control channel and flow table busy
    of serving non-urgent, but frequent packets and blocks the urgent ones, especially
    when the input rate is nearly equal to the service rate (saturation point) (Kamoun
    et al. 1980). The SFSMa allocation imposes some limitation in sharing of flow
    space of a node at a given time. The scheme seems better than the previous for
    fair treatment of packets, but doesn’t guarantee the full utilization of a given
    flow space under loaded system conditions. The previous schemes are not suitable
    for priority based flow space allocation, and is out of scope of this paper. The
    SFSMi allocation scheme solves the problem of SFSMa by reserving minimum flow
    space for each packet class, in addition to the space shared by all classes. Our
    algorithm is an enhancement of SFSMi scheme using priority classes of SDN services.
    In the traditional flow processing for matching against a flow table, all incoming
    packets are treated equally irrespective of their importance or urgency. This
    can lead to dropping or blocking of critical packets, which in turn degrades QoS
    due to high delay. In this Section, we propose a novel IoT architecture based
    on two emerging technologies: SDN and FC, and deferential flow allocation scheme
    for IoT applications as per their QoS requirements. 6 Architecture Our high-level
    system architecture is shown in Fig. 5 consisting of IoT and SDN/Fog computing.
    In this sub-section, we discuss the combined architecture of SDN and fog networks
    for IoT support. Fig. 5 Proposed architecture of SDN based IoT/Fog computing Full
    size image As IoT devices are highly distributed with constrained resources, a
    distant centralized cloud suffers from scalability and high delay. However, the
    integration of SDN and FC can open a new era for IoT applications. The distributed
    fog networks solve the problem of delay and jitter as they are closer to the IoT
    applications while SDN can solve the dynamic allocation of scarce resources such
    as flow space. One of the challenging aspect of using FC for IoT applications
    is the lack of global network knowledge for application QoS requirements (Diro
    et al. (2016, 2017), Shayokh 2016). The massive number of IoT applications needs
    a QoS guarantee (Yeganeh 2012, Kyung 2014, Batalla et al. 2017; Gajewski et al.
    2017) by allocating sufficient flow spaces for critical applications to avoid
    dropping or blocking packets. This limitation of FC could be solved by SDN as
    it is feasible to implement a custom resource allocation system in the local and
    the global SDN controller, as in Fig. 5. The innovation and manageability of SDN
    technology can be used to allocate flow spaces per their QoS levels for IoT applications.
    To achieve this, we propose logical partitioning of flow spaces per flow classes,
    which will be discussed in the next section. We assume that the communication
    technologies such as base stations, Access points, etc. in the fog networks are
    connected to each other by OpenFlow router. The openflow switch of the fog network
    has the components listed in the Fig. 6. The local SDN controllers are used for
    managing local resources such as flow space allocation for competing IoT applications.
    The global SDN controller, which is not invoked frequently, handles the fog services
    orchestration for mobility of IoT devices. Fig. 6 Structure of fog node Full size
    image 6.1 Differential flow space allocation (DFSA) scheme Our system assumes
    a model of single fog node in which urgent flows possess exclusively owned flow
    space, and shared flow space. Figure 7a shows the DFSA scheme in which flow space
    is hierarchically shared between classes. The flow table allocation problem of
    SDN node may be modeled as a set of R M/M/s queueing systems with s servers and
    share a finite space V under a mechanism, like router buffer allocation schemes.
    Flow space allocation system with number of allocators S (S = 1, …, R) could be
    a Poisson process with a rate λi and an exponential service time of mean l/ (Ciµ);
    Ci is the channel capacity of the controller and l/ µ is the average service rate.
    Flow classes to be allocated to their corresponding flow space by allocator i
    are class i flows, where each type of flow is served by its allocator in first
    come first served (FCFS) order in their corresponding flow space. Because of flow
    space sharing between our flow classes, we assume that the total system is birth–death
    process whose state is ri = (r1, …, rR) where ri is the number of class i flows.
    The flow space allocation system in steady state behaves like product form, as
    given in the Eq. (1). Fig. 7 a DFSA scheme. b DFSA scheme flowchart Full size
    image $$~P\\left( {{r_1},~ \\ldots ,~{r_R}} \\right)=~P\\left( {{r_i}} \\right)\\;=\\;\\left\\{
    {\\begin{array}{*{20}{c}} {\\pi \\left( 0 \\right)\\frac{{\\lambda _{i}^{{{r_i}}}}}{{{C_i}}},~~r
    \\in S} \\\\ {0,~~{\\text{otherwise}}} \\end{array}} \\right.$$ (1) where \\(\\pi\\)(0)
    refers to the probability when the system is empty and is given as (2) $$~\\frac{1}{{\\pi
    \\left( 0 \\right)}}=\\mathop \\sum \\limits_{S} \\frac{{\\lambda _{1}^{{r1}}}}{{{C_1}\\mu
    }}\\frac{{\\lambda _{2}^{{r2}}}}{{{C_2}\\mu }} \\cdots \\frac{{\\lambda _{R}^{{rR}}}}{{{C_R}\\mu
    }}$$ (2) Before considering our scheme, we need to model the naïve approach of
    flow space allocation in SDN network. The traditional approach seems complete
    sharing with common flow table space size of V for all flow classes whose scheduling
    principle is FCFS order. At any time, the flow space state is given by. Suppose
    all flow classes have the same average arrival rate of λ, average service time
    of µ, and channel capacity of C, and $$~F\\left( m \\right)=~\\mathop \\sum \\limits_{{0
    \\leq i \\leq \\mathop \\sum \\limits_{i} {r_i}=m}} {(\\frac{\\lambda }{C})^{\\mathop
    \\sum \\limits_{i} {r_i}}}$$ (3) then \\(\\frac{1}{{\\pi \\left( 0 \\right)}}=~\\mathop
    \\sum \\limits_{{m=0}}^{V} F\\left( m \\right)\\). In M/M/1 queuing system with
    C channel capacity the traffic intensity (\\({\\sigma _C}\\)) is given by (4)
    and we can apply to our model. $$~{\\sigma _C}=~\\frac{\\lambda }{C}$$ (4) Applying
    Algebra on (2), (3), and (4) we get. $$~\\frac{1}{{\\pi (0)}}\\;=\\;\\sum\\nolimits_{{i=1}}^{R}
    {{A_i}} \\left( {\\frac{{1 - {{({\\sigma _i})}^{V+1}}}}{{1 - {\\sigma _i}}}} \\right)$$
    (5) In this work, the total flow space is divided into two overlapping concentric
    circular flow spaces. The total flow space C is divided between two kinds of flows,
    namely urgent, and normal flows. The outer white layer C1 is allocated solely
    to the urgent flows. Figure 7 shows the algorithmic flowchart of our proposed
    differential flow space allocation scheme. From our algorithmic flowchart the
    arriving request will be checked if its flow table matches in; if so the packet
    will be forwarded. Otherwise, the classifier will identify the packets as either
    urgent class (UC) or normal class (NC) based on their QoS requirement. If the
    packet matches urgent flow class and if it has sufficient spaces, the flow entries
    are installed into the data plane without any blocking and delay. However, if
    there is no sufficient flow space in its own area, urgent flow competes for flow
    space C2 which it shares with normal flow classes. The urgent flow classes are
    blocked only if the space is not available in both flow table space categories.
    The normal flow class will be similarly handled for using space C2 based on FCFS.
    Thus the system deals with a trade-off between fairness among normal packets and
    efficiency for the urgent packets. The flow classifier assorts the incoming packets
    and assigns the corresponding flow space based on their QoS requirements. The
    packets are divided into P priority classes based on their QoS requirements. The
    incoming packets share the corresponding flow spaces based on FCFS resource scheduling
    principle. Each server (controller space) handles two kinds of customers (packets).
    Before the controller forwards the packet, it should enter the top-priority (i.e.
    UC) and the low-priority (i.e. NC). We considered a multi-server M/M/s queuing
    system characterized by a Poisson distribution based interarrival times (λi),
    and with an exponentially distributed service times (\\({\\mu _i}\\)) where i\\(~
    \\in ~\\)each class of packets. For simplicity, we will consider each group of
    class will have equal average interarrival times (λu for UC and λu for NC) and
    equal average service time of the packets in the system. Our system follows Kendall’s
    famous notation where the packet arrivals form a single queue (Smith 2002). The
    queuing model used for our system and illustration of space sharing for each class
    is depicted in Fig. 8. Fig. 8 a Queuing model, b C1 flow space sharing (by urgent
    packets), c C2 flow space sharing by normal and urgent packets Full size image
    We assume the total capacity C of the system is enough to serve the incoming packets.
    Moreover, for the sake of system stability the average arrival rate of customers
    is presumably less than their service rate in the system although multiple servers
    are considered. In what follows, we consider both cases for the two classes and
    discuss the impact of arrivals of UC packets on to flow space C2. The arrival
    packets of the two flow classes are independent and identically distributed to
    each other. The control flow state space is given by the set S = {0, 1, 2, …}.
    The Markov state of the M/M/s queue model is shown in (1). The transition state
    p(k;m) can be taken as the rate from the state k to the state m, and manipulated
    as (6). $$\\begin{gathered} p\\left( {k;k+1} \\right)=p\\left( k \\right)\\left(
    {{\\lambda _u}+\\;{\\lambda _n}} \\right),\\;{\\text{for}}\\;0 \\leq k \\leq {C_2}
    \\hfill \\\\ p\\left( {k;k+1} \\right)=p\\left( k \\right)\\left( {{\\lambda _u}}
    \\right),\\;{\\text{for}}\\;{C_2} \\leq k \\leq C \\hfill \\\\ p\\left( {k;k -
    1} \\right)=~\\mu,\\;{\\text{for}}\\;1 \\leq k<C \\hfill \\\\ \\end{gathered}$$
    (6) From Fig. 8, state k shows the admission of k packets in the given control
    space. P(k) indicates the probability of assignment of a packet to a control space
    when there are k packets in the system. Assuming a finite state space process
    and following from the queuing theories for π(0) as mentioned previously we can
    compute π(0) for our system (7). $$\\pi \\left( 0 \\right)={\\left( {\\mathop
    \\sum \\limits_{{k=0}}^{{{C_2}}} \\left( {{{\\left( {\\frac{{{\\lambda _u}+\\;{\\lambda
    _n}}}{\\mu }} \\right)}^k}} \\right)} \\right)^{ - 1}}+~{\\left( {\\mathop \\sum
    \\limits_{{k={C_2}+1}}^{C} \\left( {{{\\left( {\\frac{{{\\lambda _u}}}{\\mu }}
    \\right)}^{{C_2}}}\\cdot{{\\left( {\\frac{{{\\lambda _u}}}{\\mu }} \\right)}^k}}
    \\right)} \\right)^{ - 1}}$$ (7) It follows from (7) the probability of the mean
    busy period of the flow space is 1 − π(0). So the steady state probability (π(k))
    that k flows can be accommodated in the given control space is given (8) $${{\\varvec{\\uppi}}}\\left(
    {\\text{k}} \\right)=~\\left\\{ {\\begin{array}{*{20}{c}} {\\left( {\\frac{{{\\lambda
    _u}+\\;{\\lambda _n}}}{\\mu }} \\right){~^k}~.~~\\pi \\left( 0 \\right),\\;{\\text{for}}\\;~0
    \\leq k \\leq {C_2}~} \\\\ {\\left( {\\frac{{{\\lambda _u}+\\;{\\lambda _n}}}{\\mu
    }} \\right){~^{{C_2}}}~.~~\\left( {\\frac{{{\\lambda _u}}}{\\mu }} \\right){~^k}~.~~\\pi
    \\left( 0 \\right),\\;{\\text{for}}~\\;{C_{2+1}} \\leq k \\leq C} \\end{array}}
    \\right.$$ (8) By utilizing the above equations, the packet blocking probabilities
    for the UC and NC as a result of a control space bottleneck are: $$~P_{B}^{{NC}}=~\\mathop
    \\sum \\limits_{{k=0}}^{{{C_2}}} \\pi \\left( k \\right)~~~$$ (9) $$~P_{B}^{{UC}}=~\\mathop
    \\sum \\limits_{{k=0}}^{C} \\pi \\left( k \\right)~$$ (10) Substituting (7) and
    (8) in (9) and (10) we get (11) and (12) as closed form expression for the packet
    blocking probabilities for the two classes: UC and NC. $$P_{B}^{{NC}}=\\mathop
    \\sum \\limits_{{k=0}}^{{{C_2}}} \\left( {\\left( {\\frac{{{\\lambda _u}\\;+\\;{\\lambda
    _n}}}{\\mu }} \\right){~^k}.~\\frac{1}{{\\mathop \\sum \\nolimits_{{k=0}}^{{{C_2}}}
    \\left( {{{\\left( {\\frac{{{\\lambda _u}\\;+\\;{\\lambda _n}}}{\\mu }} \\right)}^k}}
    \\right)}}} \\right),$$ (11) $$\\begin{gathered} P_{B}^{{UC}}=\\mathop \\sum \\limits_{{k=0}}^{{{C_2}}}
    \\left( {\\left( {\\frac{{{\\lambda _u}+{\\lambda _n}}}{\\mu }} \\right){~^k}.~\\frac{1}{{\\mathop
    \\sum \\nolimits_{{k=0}}^{{{C_2}}} \\left( {{{\\left( {\\frac{{{\\lambda _u}\\;+\\;{\\lambda
    _n}}}{\\mu }} \\right)}^k}} \\right)}}} \\right)~ \\hfill \\\\ \\cup \\mathop
    \\sum \\limits_{{k={C_2}~+1}}^{C} \\left( {\\left( {\\frac{{{\\lambda _u}\\;+\\;{\\lambda
    _n}}}{\\mu }} \\right){~^{{C_2}}}~.~~\\left( {\\frac{{{\\lambda _u}}}{\\mu }}
    \\right){~^k}~\\left( {\\frac{1}{{\\left( {{{\\left( {\\frac{{{\\lambda _u}}}{\\mu
    }} \\right)}^{{C_2}}}.{{\\left( {\\frac{{{\\lambda _u}}}{\\mu }} \\right)}^k}}
    \\right)}}} \\right)} \\right) \\hfill \\\\ \\end{gathered}$$ (12) Another important
    fact in queuing theory is finding out the occupancy rate of the available resources
    in the system (Kendall 1953, Chen and Whitt 1993) and so is in SDN controller
    resource allocation. In particular, we are interested in computing the utilization
    and throughput of control space. The SDN controller space utilization is bound
    to the packet traffic intensity (\\({\\sigma _C}\\)) illustrated previously. Using
    (4) for \\({\\sigma _C}\\) and considering the two classes of packets (13). Note
    that for the stability of the system \\({\\sigma _C}\\) should be less than 1,
    otherwise when the system reaches its saturation the new arriving packets might
    be lost. The discussion of the computation of packet loss due to this reason is
    out of the scope of our study. $${\\sigma _C}=\\left\\{ {\\begin{array}{*{20}{c}}
    {\\frac{{{\\lambda _u}}}{{{C_1}\\mu }},~\\;{\\text{for~}}\\;{C_2} \\leq k \\leq
    C} \\\\ {\\frac{{{\\lambda _u}+\\;\\lambda {~_n}}}{{\\mu {C_{2~~}}}},\\;{\\text{~for}}\\;~0
    \\leq k \\leq {C_2}} \\end{array}} \\right.$$ (13) If \\({U^{{C_1}}}\\) and \\({U^{{C_2}}}\\)
    are utilizations of control spaces C1 and C2 so we can estimate the expected values
    as in (14) for C1, and (15) and (16) for C2 depending on the condition specified
    below. $$~E\\left( {{U^{{C_1}}}} \\right)\\;=\\;\\left( {{\\sigma _{{C_1}}}} \\right)\\left(
    {1 - P_{B}^{{UC}}} \\right){\\text{~}}$$ (14) Considering C2 we have two cases:
    Case 1: when there is a probability that only NC are served in C2 $$~E\\left(
    {{U^{{C_2}}}} \\right)=~\\left( {{\\sigma _{{C_2}}}} \\right)\\left( {1 - P_{B}^{{NC}}}
    \\right)~~$$ (15) Case 2: when there is a probability that both NC and UC are
    served in C2 $$E\\left( {{U^{{C_2}}}} \\right)=~\\left( {\\frac{{{\\lambda _u}+\\;\\lambda
    {~_n}}}{{\\mu {C_{2~~}}}}} \\right)\\left( {\\left( {1 - P_{B}^{{UC}}} \\right)
    \\cup \\left( {1 - P_{B}^{{NC}}} \\right)} \\right)~~$$ (16) Then throughput of
    each of the flow space classes is determined by the probability of forwarded packets
    and based on a no probability of blocking. Considering multiple servers of each
    class we compute the throughput of each of forwarded f packets where f ∈ the set
    of requesting packets k. For the packets forwarded in the flow space C1: $$~R_{{{C_1}}}^{f}={~_u}~\\left(
    {1 - P_{B}^{{UC}}} \\right)~$$ (17) And for the packets forwarded in the flow
    space C2 we consider the above two cases, and for each case gives (18) and (19)
    respectively. $$~R_{{{C_2}}}^{f}\\;=\\;{\\lambda _n}(1 - P_{B}^{{NC}})$$ (18)
    $$~R_{{{C_2}}}^{f}=~{(\\lambda_u}{+\\lambda_n})~\\left( {1 - P_{B}^{{NC}}} \\right)
    \\cup \\left( {1 - P_{B}^{{UC}}} \\right)~$$ (19) 7 Performance analysis In this
    section, we evaluate the performance of our system in comparison with non-QoS
    SDN system in terms of blocking probability, average waiting time, throughput
    and utilization. For simulation, 0.5 fractions of the total flow space have been
    allocated as C2 while 0.2 fractions of the system were given to C1 after repeated
    random experiment. We used fixed average service rate for both NC and UC and applied
    variable arrival rates for all types of flows. Based on the mathematical analysis
    of our DFSA scheme we computed the packet blocking probabilities, utilization
    of the flow spaces, and the throughput of the packets in the respective flow spaces.
    As it can be seen from Fig. 9, the probability of blocking of urgent packets is
    lower than the complete sharing scheme of conventional SDN networks. This is since
    packets are admitted differently according to their QoS requirements. However,
    the normal packets’ blocking probability is higher than the traditional system
    on the cost of priority admission of higher urgency packets. Figure 10 shows the
    normalized throughput of the different classes of packet arrivals. The UC has
    a better throughput because of less blocking probability as compared to the traditional
    approach. This could be attributed to larger flow space allocation for the UC,
    and sharing with NC when it is necessary. In addition, the throughput of NC surpassed
    that of traditional SDN due to the allocation of private space which is rarely
    shared. Figure 11 shows the SDN’s flow space utilization of each class of packets
    compared with the traditional system. As we can see from the figure both UC and
    NC have more efficient flow space utilization as compared to the traditional methods.
    The argument is similar to that of throughput. Fig. 9 Packet blocking probability
    versus packet arrivals Full size image Fig. 10 Throughput of our algorithm Full
    size image Fig. 11 Flow space utilization Full size image 8 Conclusion This paper
    has discussed differential flow space allocation scheme for SDN/Openflow networks.
    The packets admitted to the control system are classified per their QoS requirements
    for assignment to flow spaces with different capacity. Simulation results show
    that our flow space allocation scheme guarantees urgent packets with lower probability
    less blocking when compared to traditional systems of SDN. Our system also provides
    better flow space utilization and throughput than traditional OpenFlow system.
    These explorations give us a deeper insight of SDN/OpenFlow flow space limitations
    and serve as a blueprint for future improvements of SDN/OpenFlow in delay sensitive
    networks for which QoS is essential. In our future works, we will extend our system
    with multiple/virtualized controllers, and consider for other resource allocation
    for OpenFlow QoS mechanism. References Al Shayokh M, Abeshu A, Satrya GB, Nugroho
    MA (2016) Efficient and secure data delivery in software defined WBAN for virtual
    hospital. In Control, Electronics, Renewable Energy and Communications (ICCEREC),
    2016 International Conference on IEEE, pp. 12–16 Batalla JM, Vasilakos A, Gajewski
    M (2017) Secure smart homes: opportunities and challenges. ACM Comput Surv 50(5):75
    Google Scholar   Chen H, Whitt W (1993) Diffusion approximations for open queueing
    networks with service interruptions. Queueing Syst 13(4):335–359 Article   MathSciNet   Google
    Scholar   Curtis AR, Mogul JC, Tourrilhes J, Yalagandula P, Sharma P, Banerjee
    S (2011) DevoFlow: scaling flow management for high-performance networks. ACM
    SIGCOMM Comput Commun Rev 41(4):254–265 Article   Google Scholar   Diro AA, Chilamkurti
    N, 2017. Distributed attack detection scheme using deep learning approach for
    Internet of Things. Future Generation Computer Systems Diro AA, Chilamkurti N,
    Veeraraghavan P (2016) Elliptic Curve Based Cybersecurity Schemes for Publish-Subscribe
    Internet of Things. In International Conference on Heterogeneous Networking for
    Quality, Reliability, Security and Robustness. Springer, Cham, pp. 258–268 Diro
    AA, Chilamkurti N, Kumar N, 2017. Lightweight cybersecurity schemes using elliptic
    curve cryptography in publish-subscribe fog computing. Mobile Networks and Applications,
    pp 1–11 Gajewski M, Batalla JM, Mastorakis G, Mavromoustakis CX, 2017. A distributed
    IDS architecture model for Smart Home systems. Cluster Computing, pp 1–11 Hassas
    Yeganeh S, Ganjali Y (2012) August. Kandoo: a framework for efficient and scalable
    offloading of control applications. In: Proceedings of the first workshop on Hot
    topics in software defined networks. ACM, pp. 19–24 Hu F, Hao Q, Bao K (2014)
    A survey on software-defined network and openflow: from concept to implementation.
    IEEE Commun Surveys Tutorials 16(4):2181–2206 Article   Google Scholar   Kamoun
    F, Kleinrock L (1980) Analysis of shared finite storage in a computer network
    node environment under general traffic conditions. IEEE Trans Commun 28(7):992–1003
    Article   MathSciNet   Google Scholar   Kendall DG (1959) Stochastic processes
    occurring in the theory of queues and their analysis by the method of the imbedded
    Markov chain. Matematika 3(6):97–112 Google Scholar   Kim W, Sharma P, Lee J,
    Banerjee S, Tourrilhes J, Lee SJ, Yalagandula P (2010) Automated and scalable
    QoS control for network convergence. INM/WREN 10(1):1–1 Google Scholar   Kyung
    Y, Yim T, Kim T, Nguyen TM, Park J (2014) A QoS-aware differential processing
    control scheme for openflow-based mobile networks. IEICE TRANSACTIONS Inf Syst
    97(8):2178–2181 Article   ADS   Google Scholar   Mavromoustakis CX, Mastorakis
    G, Batalla JM, Chatzimisios P (2017) Social-oriented Mobile Cloud Offload processing
    with delay constraints for efficient energy conservation. In Communications (ICC),
    2017 IEEE International Conference on IEEE, pp. 1–7 McKeown N, Anderson T, Balakrishnan
    H, Parulkar G, Peterson L, Rexford J, Shenker S, Turner J (2008) OpenFlow: enabling
    innovation in campus networks. ACM SIGCOMM Comput Commun Rev 38(2):69–74 Article   Google
    Scholar   Mukherjee M, Matam R, Shu L, Maglaras L, Ferrag MA, Choudhury N, Kumar
    V (2017) Security and privacy in fog computing: challenges. IEEE Access 5:19293–19304
    Article   Google Scholar   Shahmir K, 2013. Stochastic switching using OpenFlow.
    Master of Telematics-Communication Networks and Networked Services Sharkh MA,
    Jammal M, Shami A, Ouda A (2013) Resource allocation in a network-based cloud
    computing environment: design challenges. IEEE Commun Mag 51(11):46–52 Article   Google
    Scholar   Sharma P, Banerjee S, Tandel S, Aguiar R, Amorim R, Pinheiro D (2013)
    Enhancing network management frameworks with SDN-like control. In Integrated Network
    Management (IM 2013), 2013 IFIP/IEEE International Symposium on IEEE, pp. 688–691
    Smith DK (2002) Calculation of steady-state probabilities of M/M queues: further
    approaches. Adv Decision Sci 6(1):43–50 MathSciNet   Google Scholar   Tomovic
    S, Yoshigoe K, Maljevic I, Pejanovic-Djurisic M, Radusinovic I (2014) SDN-based
    concept of QoS aware heterogeneous wireless network operation. Telecommunications
    Forum Telfor (TELFOR), 2014 22nd IEEE, pp 27–30 Tomovic S, Yoshigoe K, Maljevic
    I, Radusinovic I (2017) Software-defined fog Network architecture for IoT. Wireless
    Pers Commun 92(1):181–196 Article   Google Scholar   Vishnoi A, Poddar R, Mann
    V, Bhattacharya S (2014) Effective switch memory management in OpenFlow networks.
    In Proceedings of the 8th ACM International Conference on Distributed Event-Based
    Systems ACM, pp. 177–188 Yu M, Rexford J, Freedman MJ, Wang J (2010) Scalable
    flow-based networking with DIFANE. ACM SIGCOMM Comput Commun Rev 40(4):351–362
    Article   Google Scholar   Download references Author information Authors and
    Affiliations Department of Computer Science and IT, La Trobe University, Melbourne,
    Australia Abebe Abeshu Diro & Naveen Chilamkurti Ethio Telecom, Churchill Avenue,
    Addis Ababa, Ethiopia Haftu Tasew Reda Corresponding author Correspondence to
    Naveen Chilamkurti. Rights and permissions Reprints and permissions About this
    article Cite this article Diro, A.A., Reda, H.T. & Chilamkurti, N. Differential
    flow space allocation scheme in SDN based fog computing for IoT applications.
    J Ambient Intell Human Comput 15, 1353–1363 (2024). https://doi.org/10.1007/s12652-017-0677-z
    Download citation Received 30 September 2017 Accepted 28 December 2017 Published
    17 January 2018 Issue Date February 2024 DOI https://doi.org/10.1007/s12652-017-0677-z
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Software defined networking OpenFlow Internet of things Quality
    of services Differential flow space allocation Fog computing Wireless communication
    Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections
    Figures References Abstract Introduction Problem statement Leveraging SDN for
    IoT/Fog computing Related works The proposed system Architecture Performance analysis
    Conclusion References Author information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Ambient Intelligence and Humanized Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Differential flow space allocation scheme in SDN based fog computing for
    IoT applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chauhan S.
  - Swain C.K.
  - Behera L.
  citation_count: '0'
  description: The Internet of Things (IoT) has led to the adoption of fog computing
    for data-intensive applications that require low-latency processing. Fog computing
    uses distributed nodes near the network edge to reduce delays and save bandwidth.
    To optimize resource usage and achieve goals in fog computing, task scheduling
    plays a crucial role. To tackle this challenge, we propose an approach based on
    genetic algorithms, inspired by genetics and natural selection. We evaluate GA
    algorithm based on their ability to complete tasks and prioritize them effectively.
    The results show that the genetic algorithm performs exceptionally well, completing
    more tasks and handling priorities efficiently. Its adaptability to changing situations
    enables optimal resource usage and task completion. Our findings underscore the
    significance of genetic algorithms in fog computing systems. The insights gained
    from this study contribute to the advancement of fog computing systems, benefiting
    IoT applications and services.
  doi: 10.1007/978-981-99-8129-8_25
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on MAchine
    inTelligence for Research & Innovations MAiTRI 2023: Machine Intelligence for
    Research and Innovations pp 295–307Cite as Home Machine Intelligence for Research
    and Innovations Conference paper An Efficient Fog Computing Platform Through Genetic
    Algorithm-Based Scheduling Shivam Chauhan, Chinmaya Kumar Swain & Lalatendu Behera  Conference
    paper First Online: 03 March 2024 12 Accesses Part of the book series: Lecture
    Notes in Networks and Systems ((LNNS,volume 832)) Abstract The Internet of Things
    (IoT) has led to the adoption of fog computing for data-intensive applications
    that require low-latency processing. Fog computing uses distributed nodes near
    the network edge to reduce delays and save bandwidth. To optimize resource usage
    and achieve goals in fog computing, task scheduling plays a crucial role. To tackle
    this challenge, we propose an approach based on genetic algorithms, inspired by
    genetics and natural selection. We evaluate GA algorithm based on their ability
    to complete tasks and prioritize them effectively. The results show that the genetic
    algorithm performs exceptionally well, completing more tasks and handling priorities
    efficiently. Its adaptability to changing situations enables optimal resource
    usage and task completion. Our findings underscore the significance of genetic
    algorithms in fog computing systems. The insights gained from this study contribute
    to the advancement of fog computing systems, benefiting IoT applications and services.
    Keywords FCFS NP-EDF RTA TGR PGR IoT Fog Genetic algorithm Task scheduling Access
    provided by University of Nebraska-Lincoln. Download conference paper PDF 1 Introduction
    As we know, expansion in the field of communication and hardware technology, in
    which IoT devices played a vital role in many applications with real-time latency
    sensitivity. To host an IoT application, cloud can be used as an infrastructure.
    However, it can be difficult for cloud applications to support a large number
    of geographically dispersed IoT devices. Due to this, the network became congested
    and experienced high latency. Therefore, to overcome these issues, fog computing
    was introduced [1]. Fog computing is a model for hierarchically distributed computing
    which act as a bridge between cloud and IoT devices as shown in Fig. 1. The fog
    computing ecosystem provides a platform and architecture for a variety of software
    services. It improves QoS by decreasing service delivery delays and freeing up
    network resources. Fig. 1 Hierarchy of fog computing Full size image Networking
    devices like routers, gateway routers, hubs, and proxy servers are commonly called
    fog nodes which have computing capabilities and can be used as fog nodes [1].
    Fog nodes typically have a diverse range of resource capacities and environments
    for running applications. Due to their fundamental physical structure, most fog
    nodes, unlike cloud centers are severely resource-constrained and can be distributed
    around the edge. Large applications are represented as a set of lightweight, independent
    Application modules to align them in such fog computing environments. An application
    module carries out an operation according to input to produce appropriate output.
    Afterward, the result is provided to another module as input based on dependencies.
    Each module needs a specific number of resources, such as computational power,
    memory, and other resources to perform operations on input in a predetermined
    amount of time. 2 System Model In fog computing, there is a hierarchy of nodes
    where lower level nodes are close to IoT devices and higher level nodes are far
    away, and they communicate with each other through some designated protocol. Lower
    level nodes have fewer resources, but they process data faster as they are closer
    to IoT devices and vice versa to high-level nodes [2]. It also forms a cluster
    of nodes so that there is no delay in processing the data. Fog infrastructure
    provides a special networking technique to ensure proper communication between
    the nodes without any delays or interruptions (Fig. 2). Fig. 2 Organization of
    fog Full size image Fog Node Architecture—For managing applications running on
    fog nodes, we consider fog nodes to consist of three main components: the Controller
    Component, the Computational Component, and the Communication Component [1]. Computational
    Component: It is where the fog node does its actual work. It is further divided
    into Micro Computing Instances (MCIs), which are individual parts of the fog node
    that can be used to run each application module. Communication Component: If there
    are no MCIs, the computational Component is turned off and the node helps in forwarding
    the data packets as the only communication component. Controller Component: It
    is responsible for managing the operations of the Computational and Communication
    Components. If the load on the fog node’s applications grows, the computational
    component can be restarted to manage the additional work. 2.1 System Environment
    The system environment is the fog computing system that consists of multiple fog
    nodes and IoT devices. The fog nodes are organized into clusters based on their
    proximity to the IoT devices, and each set has a Master Node that coordinates
    the processing of data among the fog nodes in the set [3]. The fog nodes provide
    resources, such as computing, storage, and networking capabilities, to execute
    Application Modules, which are programs that process the data from the IoT devices.
    The fog nodes communicate with each other and with the IoT devices using communication
    protocols. The system environment also includes a Controller Component, which
    is responsible for monitoring and managing the operations of the Computational
    and Communication Components of the fog nodes. The Controller Component ensures
    that the fog nodes are operating efficiently, and that the data is being processed
    on time [1]. 2.2 Application Environment In the fog computing system, the application
    environment involves data processing from IoT devices. Fog computing, which extends
    the capabilities of cloud computing by placing computing resources closer to the
    data source, is utilized [4, 5]. In this application environment, data signals
    generated by the IoT devices are transmitted to fog nodes for processing [6].
    However, IoT devices cannot carry out the necessary processing themselves due
    to resource and energy limitations. As a result, fog computing serves as an intermediary
    layer between the Internet of Things-connected devices and the cloud. The fog
    nodes are organized into clusters based on their proximity to the IoT devices,
    and each cluster is overseen by a Master Node responsible for coordinating data
    processing among the fog nodes within the cluster [2]. The fog nodes offer computing,
    storage, and networking capabilities to execute Application Modules, which consist
    of programs responsible for processing the data. 2.3 Optimization Goal The main
    focus is to minimize the latency (delay) in processing data from IoT devices in
    a fog computing environment. This paper proposes a scheduling strategy for managing
    Application Modules on fog nodes in a way that minimizes the time it takes for
    the data to be processed. The optimization goal of this paper is by assigning
    the data to the fog nodes that can process it with the lowest latency. The paper
    proposes a mathematical model to optimize the allocation of data to fog nodes,
    taking into account the processing time, communication time, and latency requirements
    of the Application Modules. The main goal is to cut down on the time it takes
    to process data from IoT devices. This is important for real-time applications
    like self-driving cars and industrial control systems. This will enhance the performance
    and efficiency of fog computing environments [5]. 3 Methodology When processing
    tasks need to be distributed among multiple fog nodes, as they are in fog computing,
    effective scheduling strategies are crucial to ensuring optimal system performance.
    FCFS, NP-EDF, and Priority-based Scheduling are the three methods we selected
    for comparison with Genetic Algorithm as a scheduler, to evaluate these algorithms
    based on their ability to complete tasks and prioritize them effectively. 3.1
    Genetic Algorithm-Based Scheduler. Initiate Population: The method “initiate population”
    is used to start the genetic algorithm’s first population. It makes a group of
    people, also called genes or solutions, stand in for possible solutions to the
    problem at hand. Most of the time, each person is shown as a set of genes or factors.
    Evaluate: The evaluate function measures the fitness of each solution in the group.
    The fitness number shows how well a problem is solved or how close the answer
    is to what would be the best answer. As part of the review process, each solution
    takes a fitness test, which compares their performance to standards or goals that
    have already been set (Fig. 3). Fig. 3 Flowchart of genetic algorithm Full size
    image Selection: Selection in genetic algorithms means choosing chromosomes from
    the present population to be the parents of future generations. During this process,
    chromosomes with higher fitness values are often chosen because they are thought
    to offer better answers. Most of the time, the selection process relies on the
    fitness of the solution. A solution with a higher fitness value is more likely
    to be chosen since it has a better chance of passing on good traits to its offspring.
    Crossover: The crossover function joins the genes of two parents’ solutions to
    make children’s solutions for the next generation. In spontaneous reproduction,
    the idea of recombining genes is copied. Crossover often involves the swap of
    genetic material (genes or segments) between two parent solutions. This is done
    so that one or more children’s solutions have a mix of their parents’ solution
    traits. Mutation: Mutation is another form of a genetic operator that can be utilized
    to explore new regions of the search space by introducing random, minor chromosomal
    changes. Through the process of mutation, chromosomal segments can endure random
    modification or transformation. The rate at which mutations occur determines the
    probability that a particular gene or chromosome will become mutated. 4 Experimental
    Evaluation 4.1 Dataset Description Dataset comprises numerical values and is structured
    into five columns, each serving a specific purpose: Task ID: Each data point in
    the dataset has a unique number that makes up the ID column. These IDs help identify
    and label each entry in a way that is unique, making it easier to find and manage
    specific data points. Arrival Time: In the Arrival Time column, numbers are used
    to show when different tasks or data points come into the system to be processed.
    This information helps you find out how tasks are received and put in a queue
    to be done. Execution Time: In the Execution Time column, there are numbers that
    show how long it took to finish processing each task or data point. It shows how
    long it takes to complete a certain task from start to finish. Deadline: In the
    Deadline column, numbers are written down to show the time by which each task
    or job should be done. This value sets the longest amount of time a task can take
    to complete without missing its deadline. Priority: The dataset contains a Priority
    column, which holds numerical values that assign a priority level to each task
    or job. These values indicate the relative importance or urgency of processing
    specific tasks compared to others. Tasks with higher priority values are those
    that need to be given precedence in execution. 4.2 Performance Metrics In this
    study, we conducted an analysis of the proposed scheduling approach “genetic algorithm
    as scheduler” using our dataset. The simulation environment we used accepts a
    continuous task and allows the configuration of the virtual processor as input.
    To compare the performance of the Genetic algorithm as a scheduler with other
    scheduling approaches, we implemented three scheduling modules: FCFS, NP-EDF,
    and Priority comparison. The task parameters we used contain, ID(tid), arrival
    time(ai), execution time(ei), deadline(di), and priority(pi). For our simulations,
    we set the task’s deadline to be di > ai + ei, and we used a priority range from
    1 to 20 as well. This process was performed in Algorithm 1. To evaluate the performance
    of the different scheduling algorithms, we measured the task job when jobs were
    running concurrently scheduled on different virtual processors. We found that
    the Genetic algorithm outperformed the other methods in terms of Task Guarantee
    Ratio (TGR) [7, 8] and Priority Guarantee Ratio (PGR) [7, 8]. The improvement
    in TGR and PGR for Genetics was around 97% and 98%, respectively, compared to
    the other approaches. 4.2.1 Task Guarantee Ratio Figure 4 represents the results
    of the Priority Guaranteed Ratio from the scheduling algorithm for a set of 200
    tasks and 3 processors as shown in Fig. 4. Fig. 4 Task guarantee ratio Full size
    image In our study, we examined the TGR values, as shown in Fig. 4, to determine
    the effectiveness of four task scheduling algorithms: FCFS, Priority, NP-EDF,
    and Genetic. The TGR indicates how many tasks were completed before their deadline.
    The FCFS algorithm follows a simple approach of executing tasks in the order they
    arrive. However, this approach does not consider deadlines or priorities, leading
    to a relatively lower TGR. As the number of tasks increases, the TGR improves
    slightly, but it remains relatively low compared to other algorithms. The Priority
    algorithm assigns tasks based on their priority, giving higher priority tasks
    precedence. This approach significantly improves the TGR compared to FCFS by 39.28%,
    as critical tasks are prioritized for timely completion. As the number of tasks
    increases, a higher TGR is achieved, and the algorithm’s ability to meet deadlines
    remains relatively consistent. NP-EDF assigns tasks based on their deadlines,
    ensuring that tasks with earlier deadlines are executed first. This optimization
    based on deadlines improves the TGR significantly compared to both FCFS and Priority.
    NP-EDF showed a slight advantage over Priority by approximately 0.98%. As the
    number of tasks increases, NP-EDF continues to perform well, meeting deadlines
    efficiently and leading to a higher TGR. The Genetic Algorithm employs an evolutionary
    optimization approach, allowing it to adapt and optimize task schedules over generations.
    This flexibility and adaptability make it highly effective in meeting deadlines
    and minimizing task completion times. As the number of tasks increases, the Genetic
    Algorithm consistently outperforms other algorithms, indicating its robustness
    and ability to handle varying workloads effectively. The Genetic Algorithm outperformed
    all other algorithms by a significant margin, with an average improvement of about
    42.98% over FCFS, Priority, and NP-EDF algorithms. 4.2.2 Priority Guarantee Ratio
    The PGR metric measured their effectiveness in meeting deadlines as shown in Fig.
    5. Fig. 5 Priority guarantee ratio Full size image The four task scheduling algorithms
    whose PGR values were revealed in the analysis of the PGR graph (Figure) were
    FCFS, Priority, NP-EDF, and the Genetic Algorithm. PGR calculates the percentage
    of all priorities that were completed before their due dates. This metric reveals
    how well each algorithm performs in terms of meeting project deadlines [7]. Among
    the four scheduling algorithms evaluated in the study, the Genetic Algorithm emerged
    as the top performer in meeting task deadlines. The Genetic Algorithm achieved
    an impressive Task Guaranteed Ratio (TGR) of 0.974, indicating that approximately
    97.4% of the jobs were completed before their due dates. This outstanding performance
    can be attributed to the Genetic Algorithm’s adaptive and evolutionary nature.
    On the other hand, the other three algorithms, namely FCFS, Priority, and NP-EDF,
    also demonstrated respectable performance in meeting task deadlines but fell short
    compared to the Genetic Algorithm. They achieved TGR values of 0.377, 0.8985,
    and 0.9, respectively. The Priority algorithm prioritizes high-priority tasks
    for timely completion, and the NP-EDF algorithm optimizes resource use by focusing
    on task deadlines. These characteristics allowed them to outperform the simple
    FCFS algorithm, which schedules tasks based on their arrival time. However, their
    performance was still not as robust as the Genetic Algorithm’s. 4.2.3 Total Time
    Spent In our research, to measure the total time spent is the total quantity of
    time between when an assignment is submitted and when it is completed. The results
    showed significant differences between the algorithms. The FCFS algorithm, which
    schedules tasks in the order of their arrival, had a total time spent of 15,093
    units. The Priority algorithm, which assigns tasks based on priority, took 15,230
    units longer than FCFS. While it outperformed FCFS, it did not perform as well
    as other algorithms. On the other hand, the NP-EDF algorithm used the earliest
    deadline for task distribution and produced 15,291 units. While it optimizes resource
    use based on deadlines, it may not always be the most suitable approach for minimizing
    total time spent. The Genetic Algorithm, with a total time spent of 11,639 units,
    proved to be the most efficient scheduler as shown in Fig. 6. Inspired by natural
    selection, its adaptive and flexible nature allowed for iterative optimization
    of task schedules in dynamic and unpredictable fog computing environments. As
    a result, the Genetic Algorithm efficiently assigned jobs to processors, leading
    to improved efficiency and completion times. Fig. 6 Total time spent Full size
    image 5 Conclusion and Future Work This study considered four scheduling algorithms:
    FCFS, NP-EDF, Priority-based scheduling, and the Genetic Algorithm. We used the
    ratio of guaranteed jobs and the ratio of guaranteed priority to find out how
    well these algorithms worked. In both of these ways of measuring how well a scheduling
    method works, the results showed that the Genetic Algorithm did better than the
    other methods. It worked better than FCFS, NP-EDF, and Priority-based scheduling
    at getting more tasks done and meeting priority requirements. One of the best
    things about the Genetic Algorithm is that it can change and improve scheduling
    results as the system’s needs and limits change. Using an evolutionary process,
    the algorithm can find almost perfect solutions. This makes it very good at handling
    difficult scheduling situations. While this study indicated that the Genetic algorithm
    achieved some promising results, there is still much to learn and improve in the
    field of scheduling algorithms. Here are some ideas for further research: (a)
    Performance optimization: While the Genetic algorithm did better at balancing
    jobs and priorities, researchers can improve its overall performance by fine-tuning
    its parameters and experimenting with different methods. Investigating alternative
    genetic operators, fitness functions, and selection techniques may help to enhance
    scheduling algorithms. (b) Hybrid approaches: It might be interesting to look
    into hybrid scheduling approaches that combine the best characteristics of multiple
    algorithms. Adding machine learning approaches to the Genetic algorithm and integrating
    it with other well-known scheduling algorithms should produce improved results.
    (c) Real-world case studies and real-world implementation: It would aid in confirming
    the study’s conclusions. Implementing the Genetic algorithm or other improved
    scheduling algorithms in real-world systems and evaluating how well they function
    under different conditions can yield real-world insights and demonstrate how well
    they perform. References Intharawijitr K, Iida K, Koga H (2016) Analysis of fog
    model considering computing and communication latency in 5G cellular networks.
    2016 IEEE international conference on pervasive computing and communication workshops
    (PerCom workshops). IEEE Google Scholar   Takouna I et al (2013) Communication-aware
    and energy-efficient scheduling for parallel applications in virtualized data
    centers. 2013 IEEE/ACM 6th international conference on utility and cloud computing.
    IEEE Google Scholar   Selvarani S, Sudha Sadhasivam G (2010) Improved cost-based
    algorithm for task scheduling in cloud computing. 2010 IEEE international conference
    on computational intelligence and computing research. IEEE Google Scholar   Swain
    CK, Sahu A (2022) Reliability-ensured efficient scheduling with replication in
    cloud environment. IEEE Syst J 16(2):2729–2740 Article   Google Scholar   Kang
    Y, Zheng Z, Lyu MR (2012) A latency-aware co-deployment mechanism for cloud-based
    services. 2012 IEEE Fifth international conference on cloud computing. IEEE Google
    Scholar   Cheng L et al (2022) Cost-aware real-time job scheduling for hybrid
    cloud using deep reinforcement learning. Neural Comput Appl 34(21):18579–18593
    Google Scholar   Swain, Kumar C, Sahu A (2022) Interference aware workload scheduling
    for latency sensitive tasks in cloud environment. Computing 104(4):925–950 Google
    Scholar   Swain CK, Sahu A (2018) Interference aware scheduling of real time tasks
    in cloud environment. 2018 IEEE 20th international conference on high performance
    computing and communications; Exeter, UK, pp 974–979 Google Scholar   Schwiegelshohn,
    Uwe, Yahyapour R (1998) Analysis of first-come-first-serve parallel job scheduling.
    SODA 98 Google Scholar   Panda, Kumar S, Nanda SS, Bhoi SK (2022) A pair-based
    task scheduling algorithm for cloud computing environment. J King Saud Univ Comput
    Inf Sci 34(1):1434–1445 Google Scholar   Andersson, Björn, Baruah S, Jonsson J
    (2001) Static-priority scheduling on multiprocessors. Proceedings 22nd IEEE real-time
    systems symposium (RTSS 2001)(Cat. No. 01PR1420). IEEE Google Scholar   Lehoczky
    JP (1990) Fixed priority scheduling of periodic task sets with arbitrary deadlines.
    [1990] Proceedings 11th real-time systems symposium. IEEE Google Scholar   Lee
    J, Shin KG (2012) Preempt a job or not in EDF scheduling of uniprocessor systems.
    IEEE Trans Comput 63(5):1197–1206 MathSciNet   Google Scholar   Guevara JC, da
    Fonseca NLS (2021) Task scheduling in cloud-fog computing systems. Peer-to-peer
    networking and applications 14(2):962–977 Google Scholar   Soltani N, Soleimani
    B, Barekatain B (2017) Heuristic algorithms for task scheduling in cloud computing:
    a survey. Int J Comput Netw Inf Security 11(8):16 Google Scholar   Gupta I, Kumar
    MS, Jana PK (2016) Transfer time-aware workflow scheduling for multi-cloud environment.
    2016 international conference on computing, communication and automation (ICCCA).
    IEEE Google Scholar   Aazam M, Huh E-N (2015) Dynamic resource provisioning through
    fog micro datacenter. 2015 IEEE international conference on pervasive computing
    and communication workshops (PerCom workshops). IEEE Google Scholar   Fan Y et
    al (2018) Energy-efficient and latency-aware data placement for geo-distributed
    cloud data centers. Communications and Networking: 11th EAI international conference,
    ChinaCom 2016 Chongqing, China, September 24–26, 2016, Proceedings, Part II 11.
    Springer International Publishing Google Scholar   Download references Author
    information Authors and Affiliations Department of Computer Science and Engineering,
    Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, India Shivam Chauhan
    & Lalatendu Behera Department of Computer Science and Engineering, SRM University,
    Amaravati, Andhra Pradesh, India Chinmaya Kumar Swain Corresponding author Correspondence
    to Shivam Chauhan . Editor information Editors and Affiliations Dept of Instrumentation
    and Control Engg, Dr. B. R. Ambedkar National Institute of, Jalandhar, Punjab,
    India Om Prakash Verma School of Electrical and Electronics Engineering, Nanyang
    Technological University, Singapore, Singapore Lipo Wang Department of Electrical
    Engineering, Malaviya National Institute of Technolog, Jaipur, Rajasthan, India
    Rajesh Kumar Department of Mathematics, DR BR Ambedkar NIT Jalandhar, Jalandhar,
    Punjab, India Anupam Yadav Rights and permissions Reprints and permissions Copyright
    information © 2024 The Author(s), under exclusive license to Springer Nature Singapore
    Pte Ltd. About this paper Cite this paper Chauhan, S., Swain, C.K., Behera, L.
    (2024). An Efficient Fog Computing Platform Through Genetic Algorithm-Based Scheduling.
    In: Verma, O.P., Wang, L., Kumar, R., Yadav, A. (eds) Machine Intelligence for
    Research and Innovations. MAiTRI 2023. Lecture Notes in Networks and Systems,
    vol 832. Springer, Singapore. https://doi.org/10.1007/978-981-99-8129-8_25 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-8129-8_25 Published
    03 March 2024 Publisher Name Springer, Singapore Print ISBN 978-981-99-8128-1
    Online ISBN 978-981-99-8129-8 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Download book PDF Download book EPUB Sections Figures References Abstract
    Introduction System Model Methodology Experimental Evaluation Conclusion and Future
    Work References Author information Editor information Rights and permissions Copyright
    information About this paper Publish with us Discover content Journals A-Z Books
    A-Z Publish with us Publish your research Open access publishing Products and
    services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Efficient Fog Computing Platform Through Genetic Algorithm-Based Scheduling
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gupta A.
  - Gupta S.K.
  citation_count: '0'
  description: In this paper, we study a UAV-based fog or edge computing network in
    which UAVs and fog/edge nodes work together intelligently to provide numerous
    benefits in reduced latency, data offloading, storage, coverage, high throughput,
    fast computation, and rapid responses. In an existing UAV-based computing network,
    the users send continuous requests to offload their data from the ground users
    to UAV–fog nodes and vice versa, which causes high congestion in the whole network.
    However, the UAV-based networks for real-time applications require low-latency
    networks during the offloading of large volumes of data. Thus, the QoS is compromised
    in such networks when communicating in real-time emergencies. To handle this problem,
    we aim to minimize the latency during offloading large amounts of data, take less
    computing time, and provide better throughput. First, this paper proposed the
    four-tier architecture of the UAVs–fog collaborative network in which local UAVs
    and UAV–fog nodes do smart task offloading with low latency. In this network,
    the UAVs act as a fog server to compute data with the collaboration of local UAVs
    and offload their data efficiently to the ground devices. Next, we considered
    the Q-learning Markov decision process (QLMDP) based on the optimal path to handle
    the massive data requests from ground devices and optimize the overall delay in
    the UAV-based fog computing network. The simulation results show that this proposed
    collaborative network achieves high throughput, reduces average latency up to
    0.2, and takes less computing time compared with UAV-based networks and UAV-based
    MEC networks; thus, it can achieve high QoS.
  doi: 10.1002/dac.5759
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register International Journal
    of Communication Systems RESEARCH ARTICLE Full Access Unmanned aerial vehicles
    in collaboration with fog computing network for improving quality of service Akshita
    Gupta,  Sachin Kumar Gupta First published: 12 March 2024 https://doi.org/10.1002/dac.5759
    SECTIONS PDF TOOLS SHARE Summary In this paper, we study a UAV-based fog or edge
    computing network in which UAVs and fog/edge nodes work together intelligently
    to provide numerous benefits in reduced latency, data offloading, storage, coverage,
    high throughput, fast computation, and rapid responses. In an existing UAV-based
    computing network, the users send continuous requests to offload their data from
    the ground users to UAV–fog nodes and vice versa, which causes high congestion
    in the whole network. However, the UAV-based networks for real-time applications
    require low-latency networks during the offloading of large volumes of data. Thus,
    the QoS is compromised in such networks when communicating in real-time emergencies.
    To handle this problem, we aim to minimize the latency during offloading large
    amounts of data, take less computing time, and provide better throughput. First,
    this paper proposed the four-tier architecture of the UAVs–fog collaborative network
    in which local UAVs and UAV–fog nodes do smart task offloading with low latency.
    In this network, the UAVs act as a fog server to compute data with the collaboration
    of local UAVs and offload their data efficiently to the ground devices. Next,
    we considered the Q-learning Markov decision process (QLMDP) based on the optimal
    path to handle the massive data requests from ground devices and optimize the
    overall delay in the UAV-based fog computing network. The simulation results show
    that this proposed collaborative network achieves high throughput, reduces average
    latency up to 0.2, and takes less computing time compared with UAV-based networks
    and UAV-based MEC networks; thus, it can achieve high QoS. 1 INTRODUCTION With
    the advancement in mobile communication technologies, UAVs are extensively used
    as a type of multifunctional autonomous smart vehicle. The wide support of UAVs
    has previously been seen in the military as well as civilian applications. However,
    UAVs have been seen as facilitators for other applications, including military,
    surveillance and monitoring, telecommunications, Internet of Things (IoT), medical
    aid services, disaster management, and search and rescue missions. These motivating
    applications were primarily robotics or military in nature, as such traditional
    UAV-centric studies have typically concentrated on challenges of navigation, control,
    and autonomy.1 Meanwhile, the continuous demand for data from users puts a strain
    on continuous revolutionary progress in communication technologies to fulfill
    future wireless demands. The evolution of fifth generation (5G), beyond fifth
    generation (B5G), and sixth generation (6G) are in progress and massively used
    in current real-time applications. High-tech advancements and application needs
    have encouraged continual innovation in real-time applications. To efficiently
    guarantee the gathering of high-value facts from real-time applications and assure
    the correctness and efficacy of services is crucial, especially in today''s fast
    adoption of artificial intelligence approaches represented by machine learning
    and deep learning.2, 3 UAV-assisted communication systems are suitable for on-demand
    coverage or unforeseen crises because of their quick and flexible configuration.
    UAV-assisted paradigm has several benefits, including balancing loads, quick deployment,
    enormous capacity, mobility, and backhaul connectivity. Deploying UAVs in critical
    applications involves several challenges, despite the apparent advantages of UAV-assisted
    networks in the IoTs. UAV-assisted networks are used for complex real-time operations,
    and they can further raise significant challenges in terms of computational power,
    delay, offloading of data, and energy preservation.4 The limited onboard energy
    of a UAV is one of the main constraints in a UAV-assisted system. The energy of
    UAVs requires backup and the conservation of energy and this is possible if some
    of the nonparticipating UAVs go to sleep for some time and wake up when required.
    The backup can be provided by extra energy chargeable batteries and a power bank
    in this UAV-assisted paradigm.5 On the other hand, to handle the data and energy
    inefficiencies in wireless ground networks, UAVs support a lot by using the multidimensional
    space to serve several services efficiently.6 UAVs are popularly working with
    advanced technologies to revolutionize real-time networking support. With the
    demand for IT sectors as well as public safety, UAVs are used as heroic agents
    to interact with ground devices as well as other multiagents to fulfill their
    demands in terms of enhanced capacity, coverage, data processing, low latency,
    task offloading, fast computing, storage, and security. UAVs are widely used as
    flying base stations or relays to provide prolonged communication with ground
    devices.7 UAVs with cloud computing can offload sophisticated computing tasks
    and alleviate the problem of mobile terminal resource limitations. However, due
    to the centralized cloud''s distant location, communication bandwidth, and capacity
    limitations, latency-critical operations cannot complete the computation and communication
    in the optimal time.8 As well, edge computing also solves the problem of latency
    and response time as edge nodes put computing capabilities at the edge of the
    network. But still, the offloading requests from users put a challenge for edge
    devices due to their limited functionalities and not being able to transmit a
    large amount of data at the same time with security. If we compare fog computing
    with the edge computing paradigm, both are quite similar but fog processes all
    the data at the same while edge does not.9 Another major difference is that edge
    computing has weak security because the data are being processed across multiple
    edge devices; thus, security has become an increasingly complex issue. To cope
    with the challenges and complexities raised by edge computing, fog computing works
    better in UAV-based networks. Fog computing is a concept that incorporates network,
    computing, security, storage, and intelligence services on the network''s edge,
    which is physically near the data source. Fog computing maintains security as
    it is one hop away from ground devices while the edge has weak security. The collaboration
    of UAVs with fog computing can solve all these problems as well as ease the offloading
    of complex tasks and improve the quality of services (QoS) including delay, energy
    consumption, processing capabilities, resource allocation, and network capacity.10,
    11 The collaborative UAV-assisted fog network supports a large number of applications,
    but some major real-time applications such as public security, smart cities, and
    disaster management take ample benefit of the UAV–fog network. In public security,
    UAV–fog nodes are equipped with video cameras for public places, parking zones,
    and residential buildings to ensure safety and security. Despite the presence
    of numerous cellular networks with extensive coverage in smart cities, the limited
    capacity and peak bandwidth to adequately serve the demands of the current population
    is a challenge. So, the implementation of a collaborative UAV–fog network enables
    fog nodes to provide local storage and processing capabilities. This, in turn,
    leads to the optimization of network usage and a more efficient smart city infrastructure.
    UAV–fog network helps to control and estimate the risk of disaster and handles
    post-disaster recoveries.12 Moreover, the collaboration of UAVs and fog computing
    is a challenge because of the intricacy of the working environment, the unpredictability
    of terminal user distribution, and the restrictions of the UAV''s energy. Thus,
    it is required to plan an optimal path for UAVs when employing collaboratively
    with fog computing to address these challenges. UAVs work collaboratively with
    the fog computing paradigm to employ computing capabilities and computation offloading
    to handle the latency and complex tasks generated via UAV applications. On the
    other hand, the Q-learning-based Markov decision model is further employed to
    handle offloading decisions as well as maintain the energy consumption of UAV
    and fog nodes. The prime contributions of this article are pointed out as follows:
    First, we investigate the collaboration of UAV and fog computing and UAV and mobile
    edge computing (MEC) to distinguish the research gap for the novelty of this article.
    Second, we designed a four-tier architecture of a UAVs–fog collaborative network
    for real-time applications, where each UAV and UAV–fog node works intelligently
    to serve ground users. We also suggest a Q-learning-based Markov decision process
    (QLMDP) for UAV-enabled fog architecture, where UAV–fog nodes work to serve smart
    things which includes terminal devices (TDs) and computing devices (CDs) with
    ultralow latency, whereas UAV–fog nodes can provide smart offloading of data to
    TDs and maintain the decisions of multiple UAV nodes in the network. Further,
    we perform extensive simulation experiments to analyze the QoS of a UAV-enabled
    fog network, where UAV and fog nodes during the communication control the decisions
    intelligently and improve the overall performance of the network. The results
    have shown the effectiveness and feasibility of this collaborative network. The
    rest of the paper is organized as follows: Section 2 presents the related work,
    Section 5 shows the system model of the UAV–fog collaborative network, and Section
    9 discusses the Q-learning Markov decision process, an approach used for collaborative
    UAV–fog network. The simulation results and discussions are in Section 10. The
    limitations of the UAV–fog network are highlighted in Section 11, and lastly,
    the conclusion and future work are in Section 12. Moreover, Table 1 provides a
    list of all possible abbreviations used throughout the article. TABLE 1. List
    of abbreviations. 5G Fifth generation AOI Age of information B5G Beyond fifth
    generation CAL Computational access point 6G Sixth generation RL Reinforcement
    learning IoT Internet of Things ML Machine learning IIoT Industrial Internet of
    Things PL Path loss IOE Internet of Everything ARE Adaptive and random exploration
    scheme UEs User equipments WEM Weighted expectation maximization TDs Terminal
    devices RWPM Random way point mobility CDs Computing devices DRL Deep reinforcement
    learning QoS Quality of service MADRL Multiagent deep reinforcement learning RMs
    Rescue members SINR Signal-to-interference noise ratio UGV Unmanned ground vehicle
    QLMDP Q-learning Markov decision process UAV Unmanned aerial vehicles LAP Low
    altitude platform MEC Mobile edge computing LoS Line of site FC Fog computing
    MBS Mobile base station 2 RELATED WORK UAVs and fog computing integration for
    future technological developments are gaining attraction among research academicians.
    The collaboration of both technologies brings a revolution in the modern era.
    The technical functionalities of both technologies are used to deploy quick networks
    in emergencies. This collaboration achieves high functionality, high computing
    capabilities, storage, offloading, low latency, and enhanced coverage to improve
    the network''s QoS. Some of the researchers'' contributions in this area are explored
    to identify the gaps and find the limitations to provide novelty in our work.
    2.1 Collaboration of UAV and fog computing In the article,13 the authors use UAVs
    and fog computing in disaster management and recovery. The sensors in IoE-based
    systems perceive their surroundings and transmit data for processing. On the other
    hand, UAVs are used to explore locations that are impossible for humans to access
    (e.g., underwater exploration), and machine learning techniques are employed to
    identify, analyze, and learn from information gathered over the fog and edge computing
    nodes. In the article,14 the authors have a suggested UAV–fog-based network for
    the IoTs. They discussed the benefits and characteristics of both technologies,
    fog computing and UAVs, to successfully serve various IoT applications. In this
    paper, the author discusses the advantages, disadvantages, and functionalities
    of both technologies for IoT. However, they have not simulated the scenario to
    analyze the results. Benefits like improved line of sight (LoS), greater coverage
    and scalable, and on-demand deployment make UAVs distinctive in rescue and disaster
    scenarios. In this paper, the authors suggested the hybrid fog computing system,
    which combines UAVs and vehicular fog computing to serve in disaster scenarios.
    This system poses to minimize latency and task offloading in post-disaster scenarios.15
    In this paper, the author concentrates on three issues, that is, mobility, energy
    consumption, and communicated range, and proposes an energy-efficient UAV intelligent
    network that is interconnected, helping rescue members (RMs) and giving them long-term
    communication coverage. The authors used deep reinforcement learning to understand
    the varying environments and determine the optimal trajectories in post-disasters.16
    Unmanned ground vehicle (UGV) serves as a mobile charging station, tracking and
    supporting a UAV information collector that is deployed to support multiple heterogeneous
    IoT devices. In this paper, the author proposed the multiagent deep Q-network
    to minimize the problem of the age of information (AoI) and use UGV to optimize
    the trajectory and energy usage.17 The data generated by sensors and IoT devices
    can result in a variety of computing activities that cannot be performed locally.
    The offloading of such activities to external systems with stronger processing
    and storage capacity is often offered by centralized servers in distant clouds
    or on the edge via the fog computing platform. This approach can execute tasks
    with minimum power consumption and low latency. In this paper, the authors cover
    energy consumption which is somehow reduced but latency is still an issue.18 2.2
    Collaboration of UAVs and MEC A lot of previous work has been done on UAVs with
    MEC systems. The MEC provides computation and storage services to the mobile network''s
    edge, allowing it to handle high-demand applications while maintaining overall
    network latency. In the article,19 the author surveyed the MEC system to specify
    its constraints and work on it. The delay in network services can be managed by
    properly meeting strict delay requirements. In this article, the author illustrates
    the MEC applicability and existing models for integrating MEC functionalities
    into wireless services. This article focuses on computational offloading, allocation
    of resources, and mobility in MEC systems. In the article,20 the authors optimize
    and maintain the QoS and path planning for the MEC using the reinforcement learning
    platform. To guarantee a better QoS, the authors also used a sigmoid function
    to observe the demand of the terminal users. Further, the author considers some
    considerations of user demand, distance, and risk factors to ensure high service
    quality and better performance of the network. The results demonstration shows
    this network will able to provide high QoS and feasible network. In the article,21
    the authors examine smart computing for UAV-enabled MEC systems, in which multiple
    devices perform some complex computations with the assistance of a single computational
    access point (CAP) while being jammed by a UAV attack. The author employs reinforcement
    learning (RL) and other learning algorithms to improve system performance by minimizing
    latency and energy consumption. The above-discussed work clearly depicts that
    the high potential of using UAVs with fog or edge computing is driving up the
    scope of research in this area. Most of the researcher''s ideas and endeavors
    in this area are obviously motivated. To decrease latency and improve performance,
    the researchers leverage edge and fog technology. However, our objective is to
    improve the overall performance in terms of latency, throughput, and computing
    capabilities to provide services intelligently with high QoS at the network''s
    edge. 3 UAV–FOG COLLABORATIVE NETWORK: A SYSTEM MODEL Figure 1 shows the proposed
    scenario of the UAV–fog collaborative network which comprises four tiers, that
    is, smart things, local UAVs, UAV–fog, and cloud server. In tier 1, the ground
    zone is divided into three zones to cover a larger area with uniform connectivity.
    The ground devices share device-to-device (D2D) communication and are connected
    to aerial nodes via a 5G network. Tier 2 provides communication to ground devices
    via a request–response mechanism. Local UAVs are connected via communication links
    to communicate valid information. In tier 3, the UAV–fog is a server node processing
    all the data from the local UAVs and ground users and providing services to ground
    users at the end. In tier 4, the cloud server is used as a backup to store processed
    data from UAV–fog nodes and accessed via applications in real-time systems. The
    intelligent four-tier architecture of the UAV–fog network has been designed to
    depict the real scenario and its functionality, which is illustrated in Figure
    2. This architecture consists of smart things (including TDs and CDs), local UAVs,
    UAV–fog, and cloud servers. The data processing of terminal devices and storage
    is managed by UAV–fog nodes, whereas local UAVs preface the data requests from
    the smart things. This section further discussed the important characteristics
    of this system model, environment modeling, and the QLMDP. FIGURE 1 Open in figure
    viewer PowerPoint Proposed scenario of UAV–fog collaborative network. FIGURE 2
    Open in figure viewer PowerPoint Intelligent four-tier UAV–fog network. Characteristics
    of system model The collaboration of UAVs and fog computing as the fog node is
    new and yet to be explored on a wide scale. The system architecture layered detail
    is given as follows: The lower layer of this network model is a pool of smart
    things that use smart technologies and consume less energy and bandwidth. It mainly
    sends requests to the nearer UAVs. Local UAVs are commonly used and act as a relay
    to get a request from the smart things and further send it to UAV–fog nodes. UAV–fog
    nodes are the main heart of this network model as they process the whole information,
    manage resources, and send it back as a response to the end users. For example,
    in an emergency, there is a complete shutdown in the whole network, and these
    nodes still offload data to ground users. The upper layer consists of a cloud
    server, that is, a centralized data server that controls the whole network database
    and directly sends it through applications. 3.1 Environment modeling We considered
    a scenario consisting of multiple heterogeneous smart things, UAVs, and UAV–fog
    nodes and servers. These nodes are randomly deployed in a 1500 * 1500 m2 area
    at t = 0. MatLab R2021a is used to create and simulate this environment. Let''s
    suppose a set as the ground plane consists of mobile base stations (MBSs) maintaining
    downlink communication with a pool of smart things, that is, TDs and CDs, in a
    geographical area 1500 * 1500 represented as . Here, each BS set serves an area
    such that the and such that . The nodes are deployed randomly and and are the
    spatial distribution of TDs and CDs w.r.t particular BSs. The expected position
    of random nodes would be , whereas the UAVs are positioned at a height varied
    from the ground station, that is, 50 to 100 m. The LoS and NLoS communication
    is used for direct and indirect users. The set as an aerial plane consists of
    multiple UAVs, where local UAVs denoted by and UAV–fog as serve the base station
    Q. The X plane and Y plane work on different frequencies. The UAVs are equipped
    with an omnidirectional antenna that helps to send and receive signals from ground
    BS to flying nodes and vice versa. The Euclidean distance between the local UAV
    and UAV–fog can be expressed as (1) The distance between the local UAV , UAV–fog
    , and MBS can be expressed as (2) In this paper, we deployed TDs randomly in considered
    geographical areas . The distance between the TDs ( ) and local UAV and UAV–fog
    can be expressed as (3) To cover large geographical areas, connection establishment
    between aerial nodes and ground devices can be done using the ROS toolbox in MatLab.
    To maintain the quality of the network, the path loss of the network is calculated
    to decide the optimal path for UAVs to deliver continuous data between nodes.
    The total path loss from aerial plane to ground plane can be given by Equations
    (4)–(6). (4) (5) (6) Here, in Equation (6), the total path loss has been formulated
    to calculate the path loss between UAVs (Tx) and TDs (Rx). Whereas is the channel
    frequency, c is the speed of the light, is the distance between UAV and TDs, and
    is the excess path loss including LoS for direct communications and NLoS for indirect
    communication. The total power received to the total power transmitted from UAVs
    and UAV–fog to TDs is given by Equation (7) as (7) Equation (8) represents the
    power received by the TDs (Rx) with respect to the UAVs (Tx) in terms of path
    loss and distance. (8) where is the power received by TDs (Rx) as a function of
    distance between UAVs (Tx) and TDs (Rx), is the power transmitted by UAVs (Tx),
    is the gains of Tx and Rx antennas in dBs, is the wavelength of the signal, and
    is the total path loss between UAVs (Tx) and TDs (Rx). From Equation (8), the
    transmitted power from UAVs and UAV–fog to TDs are used in signal-to-interference
    noise ratio (SINR). Therefore, the SINR ( ) is calculated from Equation (9) as
    (9) where is the total path loss, is the noise power, and is the interference
    from external sources. The maximum transmission data rate ( ) for UAVs from positioned
    at to TD located at . Similarly, UAV–fog from positioned at to TD located at can
    be expressed in Equations (10) and (11) as (10) (11) where is the bandwidth, is
    the noise power, and is the interference from external sources. 3.2 Computing
    model In an intelligent UAV-enabled fog network, the local UAVs and UAV–fog nodes
    work together to serve intelligent services at every corner of the network with
    low latency and better QoS. The local UAVs work as a simple aerial base station
    and UAV–fog nodes are equipped with a fog server as an intelligent base station.
    The aerial plane consists of local UAVs set, which is denoted as , and UAV–fog
    as a flying server, which is denoted as . The UAVs are flying as aerial base stations
    above the ground devices, that is, TDs. The ground plane consists of MBS ; the
    number of IoT smart things set is denoted as where active TDs are denoted as and
    nonactive TDs are denoted as . The local UAV and UAV–fog nodes are scheduled to
    gather and offload device tasks collaboratively. The task can be offloaded from
    UAVs and UAV–fog to TDs in particular area . The data size of tasks can be denoted
    as . 3.2.1 Task offloading from TDs to flying servers UAV–fog nodes Assume all
    the tasks can be offloaded from active TDs to UAV–fog server nodes . The tasks
    to be offloaded are listed as a set . Each task has some certain set of attributes
    . Here, is the server address, is the size of the task, is the computation time
    during offloading, and is the number of iterations. The transmission time for
    task assigned at UAV and UAV–fog nodes can be calculated by Equation (12): (12)
    where is the size of the task taken during tasks offloaded from UAVs and UAV–fog
    nodes while is the maximum data transmission rate from positioned UAVs and UAV–fog
    nodes to TDs. The UAV–fog server node is assumed to offload tasks in an order
    of first come, first serve to active TDs. The overall transmission time in area
    when the UAV and UAV–fog nodes fly at a certain position can be calculated by
    Equation (13): (13) Similarly, the computation time for task assigned at UAV and
    UAV–fog nodes can be calculated by Equation (14): (14) where is the processing
    time taken by task and is the processing capacity of UAV for processing assigned
    task. The UAV–fog server node is assumed to offload tasks in an order of first
    come, first serve to active TDs. The overall computation time in area when the
    UAV and UAV–fog nodes fly at a certain position can be calculated by Equation
    (15): (15) The delay ( ) during offloading of tasks from TDs to UAVs and UAV–fog
    can be calculated by service delay ( ), transmission delay ( ), and computation
    delay ( ). These delays are included to manage the low latency in the network.
    It is represented from Equation (16) as (16) The total latency for data services
    to be sent from aerial nodes to TDs can be computed by including the processing
    time ( ) during uploading the service requests and response time ( ) to respond
    to the requests from TDs and queue time ( ) to wait for the request in the queue
    to avoid the congestion in the network. 4 Q-LEARNING MARKOV DECISION PROCESS:
    AN APPROACH In the collaboration of the UAV–fog model, we considered the learning
    model, that is, QLMDP, for quick modeling and decision-making in critical situations
    where random states and responses of the nodes make decisions. QLMDP is used in
    this model for balancing load and offloading to minimize latency between UAV–fog,
    UAVs, and end users. It works to find the optimal decision for offloading with
    requests and responses. Figure 3 details the QLMDP for the UAV–fog collaboration
    model. This methodology is used to minimize latency and make optimal decisions.
    The QLMDP process consists of the following: : A finite set of end user''s states,
    that is, requests; : Set of actions α from an agent; : Policies of transition
    state to state , at time ; and : Reward function, that is, responses, w.r.t action
    α. FIGURE 3 Open in figure viewer PowerPoint Q-learning Markov decision process
    for UAV–fog collaboration. Consideration 1. is a set of finite smart things denoted
    by , … . Due to the mobility of end users, the speed of devices may vary according
    to their random walk. The Q-learning Markov chain is the chain sequence of random
    nodes that defines their future stage based on the previous stage, that is, the
    Markov state defined by a set of tuples . Smart devices provide a frequent request
    to local UAVs or UAV–fog nodes. Smart devices are denoted by which is defined
    as a finite set in Equation (17) as follows: (17) where is node IP address; is
    node request; is node previous position; is node current position; is node direction
    (it defines to show the direction of the node); and is frequency at which node
    communicates. Consideration 2. is a set of local UAVs denoted by … . Local UAVs
    work as relays to cover large geographical areas and manage resources at a lower
    level before sending them to UAV–fog nodes. It depends on the local nodes whether
    the requests from the smart devices are either served or rejected. Local UAVs
    are defined by in Equation (18) as follows: (18) where is local UAV node IP address;
    is local UAV node optimal position; is the action taken by local UAV node, that
    is, either serve or reject requests; is acknowledgment sent to the end users;
    and is the frequency of local UAVs. Consideration 3. is a finite set of UAV–fog
    nodes denoted by … . UAV–fog collaboratively works as a fog node to show intelligence
    by fast processing and balancing the load with the management of the UAV–fog coordinator.
    UAV–fog nodes are defined by a set named as in Equation (19): (19) where is UAV–fog
    node IP address; is UAV–fog node optimal position; is the services provided by
    UAV–fog nodes; is UAV–fog coordinator that can manage the resources; is the reward
    function, that is, responses sent back by the UAV–fog nodes to end users; and
    is the frequency of UAV–fog nodes. Consideration 4. is a set of cloud server nodes
    denoted by … . The parameters of the cloud server are defined in Equation (20)
    as follows: (20) where is cloud server node location; is cloud public or private
    key; and is cloud server services, that is, storage and applications. The basic
    approach is set to communicate information via links through local UAVs and UAV–fog
    nodes in full-duplex mode. The communication links between the nodes are denoted
    by . When the user as an agent is moved from one state to another, the probability
    is called transition probability which is defined by Markov property in Equation
    (21). (21) Here, represents the present state of the user device and Nt + 1 represents
    the next state. Equation (21) defines the transition from one state to is independent
    of the previous state. Further, the notations used in this system model are mentioned
    in Table 2. The overall performance of the collaboration of UAV–fog networks can
    be evaluated by measuring various parameters such as service latency, throughput,
    computational offloading, and computing time. Table 3 discusses the technical
    specifications for UAV–fog network architecture. TABLE 2. Notations used in system
    model. Value Description Value Description Node IP address Local UAV frequency
    Node request UAV–fog IP address Node previous position UAV–fog position Node current
    position UAV–fog services Node direction UAV–fog coordinator Node frequency UAV–fog
    request Local UAV IP address UAV–fog frequency Local UAV position Cloud server
    location Local UAV action Cloud server key Local UAV acknowledgment Cloud server
    services TABLE 3. Technical specifications for UAV–fog network architecture. Tiers
    Parameter specification Value Considerations Smart devices Communication technology
    2.4 GHz The smart devices on the ground use Bluetooth and Wi-Fi connected by D2D
    communications Communication links 802.11b The two types of communication links
    formed: smart devices to UAVs and smart devices to MBS Topology used Hybrid The
    topology used between smart devices is point-to-point, star, mesh, or hybrid Local
    UAVs Maximum area 1500 * 1500 m2 However, it depends on the user''s choice As
    a basic model, the geographical area is taken to be in 1500 * 1500 m2 above sea
    level UAVs height from ground devices Varied up to 2 km The possible height of
    UAVs can be up to 2 km, but practically, the optimal height of UAVs is considered
    in this network as 200 m UAV flight endurance time Up to 1 h As we considered
    LAP local UAVs in this architecture, so the UAV flight endurance time is up to
    1 h, and batteries should be replaceable Transmission range 50 to 150 m The transmission
    range of local UAVs considered in the network is 50 to 150 m as a basic model
    Channel type Wireless The channel type is purely wireless in this tier Frequency
    spectrum 2.4 GHz as S-band and 5 GHz as C-band The frequency spectrum used in
    this network is unlicensed bands 2.4 and 5 GHz for indoor and outdoor use Path
    loss (L [dB]) 3 dB The total path loss calculated is 3 dB UAV modeling decision
    model Q-learning Markov decision process (QLMDP) QLMDP-based decision model is
    used in this architecture to do efficient offloading and balance load to reduce
    latency UAV altitude platform LAP The LAP UAVs are used in this network, and it
    uses LoS communication Antenna type and antenna gain Omnidirectional, 10 dB The
    antenna type is omnidirectional, and the gain is 10 dB Mobility Random way point
    model (RWPM) The mobility of UAVs is very important as it provides coverage and
    connectivity to the whole area. The mobility model used in local UAVs is RWPM
    (it may also be application dependent) UAV–fog The connection between UAV–fog
    and UAV nodes Master–slave and point-to-point The two types of connections are
    used in UAV–fog nodes, that is, master–slave and point-to-point. The UAV–fog coordinator
    works as a master node and handles all the information gathered by UAVs, and all
    other nodes act as a slave, whereas other UAV–fog nodes use the point-to-point
    connection Maximum UAV–fog nodes in a subset Six nodes per subset The maximum
    UAV–fog node used in this network is six per subset as basic consideration Power
    of UAV–fog nodes 5.1 to 1.9 W The power used by UAV–fog nodes is 5.1 to 1.9 W
    Bandwidth 800 MHz The bandwidth used by UAV–fog is 800 MHz and a data rate of
    800 Mbps Frequency 28 GHz The frequency used in operating UAV–fog nodes is 28 GHz
    Network latency Ultralow The UAV–fog node has the capability to provide ultralow
    latency services Noise power ( ) −150 dBm The calculated noise power is −150 dBm
    Transmission rate ( ) 280 Mbps The calculated transmission rate is 280 Mbps Real-time
    applications Support The collaboration of the UAV–fog network supports real-time
    applications Cloud server Storage capacity >1 GB In the basic model, the storage
    capacity of the cloud server is greater than 1 GB Network management High It manages
    IP addresses, security, traffic management, and load balancing of the whole network
    to ensure high performance of the system Security Public/private key security
    The security used in the database cloud server is a public/private key 5 SIMULATION
    RESULTS AND DISCUSSIONS For simulations, we considered the MatLab R2021a to perform
    the simulations and analyze the performance of the UAV–fog computing network.
    The UAV–fog networks operate at 5 GHz frequency for aerial-to-ground communications.
    We setup the scenario consisting of multiple heterogeneous smart things, UAVs,
    and UAV–fog nodes and servers. These nodes are randomly deployed in a 1500 * 1500 m2
    area at t = 0. The bandwidth required to operate each UAV is 200 MHz, whereas
    the noise power is calculated as −120 dbm/Hz. The transmission range of UAVs could
    be varied as 50, 100, 150, and 200 m. The antenna used in UAVs and UAV–fog is
    omnidirectional and antenna gain is 10 dB. The average energy required to move
    one UAV is μ = 0.1 J/m. The UAVs and UAV–fog nodes are placed at optimal positioned
    at height of 100–200 m and varied accordingly. The distance between UAVs and ground
    users and UAVs and the base station is calculated by Euclidian distance. The path
    loss is calculated to get the optimal path to offload data from that path. Further,
    the SINR ( ) and transmission data rate ( ) are calculated. A comparison analysis
    is conducted with UAVs-based network, UAV–MEC network, and collaboration of UAV
    and UAV–fog network to achieve better QoS. Figure 4 shows the SINR versus coverage
    rate probability. The performance of achieved SINR per maximum coverage with collaboration
    should be better as the power received at each UAV, and UAV–fog node is higher.
    It is seen in the first instance (collaborative UAV–fog network), from −20 to
    30 dB SINR, the coverage rate increases from 0.2 to 1.5 which means the more the
    users accommodate, the more the coverage rate of probability with a higher SINR
    ratio. The reason for achieving high SINR and maximum coverage is the offloading
    of data via an optimal path, even if more users or TDs participate in communication,
    which results in better network performance. Similarly, in other cases, the SINR
    coverage probabilities are 1.3 and 1.2, relatively lower than the collaborative
    UAV–fog networks. The low coverage probability is because more users participate
    in active communication and more bandwidth is allocated to all users in communication.
    The more the TDs participate, the more the SINR coverage reduces; however, in
    our proposed collaborative network with QLMDP, the computing capabilities of UAV–fog
    nodes with local UAVs and smart offloading increase the network''s performance.
    FIGURE 4 Open in figure viewer PowerPoint SINR versus coverage rate probability.
    Figure 5 shows the analysis of the average achieved throughput with UAVs, UAV–MEC,
    and collaboration of local UAVs and UAV–fog network. The average throughput is
    defined as the total throughput as the product of the offloaded tasks from all
    UAVs to ground TDs and the number of bits per task . It is observed from Figure
    5 that the proposed collaboration network achieves high throughput as the packet
    drop is less at the optimal path. UAVs-based network and UAV–MEC network may not
    be able to handle a lot of data traffic congestion effectively. That is why there
    is a drop in peak at 22 and 27 nodes. The reason behind the packet drop is the
    congestion caused by the increase in several requests by ground users. Thus, loss
    in packets is high. FIGURE 5 Open in figure viewer PowerPoint Analysis of average
    throughput. Figure 6 shows the analysis of average latency with UAVs, UAV–MEC,
    and collaboration of UAV–fog and local UAV nodes. It is observed that the proposed
    collaborative network achieves low latency even if the number of TDs and ground
    user''s increases. The maximum latency peak is 0.02 ms, which decreases due to
    the offloading of data from UAV–fog and local UAV nodes to users and vice versa.
    However, the requests and responses are managed locally via UAV–fog nodes using
    the QLMDP process at UAV–fog nodes to balance the decisions made by UAV–fog nodes
    and local UAV nodes. The low latency during offloading is due to the shorter response
    time while executing resource scheduling tasks from UAV fog nodes to TDs from
    the optimal path. Further, the QLMDP optimizes the resource allocation of UAV–fog
    nodes and takes optimal decisions to handle large volumes of data sequentially.
    This way, the minimum latency is achieved even if the number of TDs increases.
    In addition, in the UAVs-based and UAV–MEC networks, the latency is much higher
    compared with the proposed collaborative network. Therefore, from the above comparison,
    it is concluded that the collaboration of the UAV–fog network with QLMDP provides
    better latency and offloads large amounts of data optimally. In Figure 7, it is
    observed that the proposed collaborative network takes less computing time even
    if the number of task increases. In the proposed network, local UAVs acting as
    relays take the longest amount of time to calculate numerous jobs; thus, the task
    assignment enables UAVs–fog nodes to handle the most requests with the least amount
    of processing time. This way, the UAV–fog nodes help to reduce time and delay
    in the network. The figure shows the peak value of the proposed collaborative
    network achieves 0.1, while the UAVs-based network and UAV–MEC network take 0.29
    and 0.4, respectively, computing time to process and compute the tasks. Therefore,
    it is evaluated that even if there is an increase in the number of tasks, the
    UAV–fog nodes handle it efficiently with very little processing time. This is
    possible with the collaboration of local UAVs with UAV–fog nodes where the computing
    of data is handled via fog nodes and processing of tasks performs with local UAVs
    nodes. Table 4 shows the tabular comparative analysis: UAV-based ground, UAV–MEC,
    and proposed collaborative UAV–fog network; whereas Table 5 shows the comparative
    analysis of UAV–fog without QLMDP and collaborative UAV–fog with QLMDP network.
    From Table 5, it is evident that the collaborative UAV–fog network with QLMDP
    performs better as it provides very low latency and high throughput and takes
    less computing time than without QLMDP in UAV–fog network. Further, Table 6 shows
    the comparative result analysis and discussions of our proposed network with state-of-the-art
    schemes. FIGURE 6 Open in figure viewer PowerPoint Analysis of average latency.
    FIGURE 7 Open in figure viewer PowerPoint Analysis of number of tasks versus computing
    time. TABLE 4. Tabular comparative analysis: UAV-based ground, UAV–MEC, and proposed
    collaborative UAV–fog network. QoS Average latency Computing time Throughput Iterations
    UAV-based ground UAV–MEC Collaborative UAV–fog (QLMDP) UAV-based ground UAV–MEC
    Collaborative UAV–fog (QLMDP) UAV-based ground UAV–MEC Collaborative UAV–fog (QLMDP)
    I1 0.019156 0.000156 0.000101 0.03 0.01 0.01 2500 1500 3000 I2 0.021132 0.011132
    0.001194 0.17 0.13 0.026 1500 2000 2500 I3 0.043161 0.023161 0.011186 0.29 0.26
    0.037 1200 2568 2700 I4 0.050504 0.030156 0.021142 0.3 0.29 0.1 2000 3000 3400
    I5 0.007655 0.006678 0.005841 0.4 0.23 0.028 2698 2500 3500 I6 0.016035 0.004605
    0.003905 0.36 0.2 0.019 3500 500 3912 I7 0.017083 0.004124 0.003114 0.25 0.1 0.02
    200 4265 4000 I8 0.028446 0.007263 0.005926 0.19 0.17 0.07 4000 4267 4290 TABLE
    5. Comparative analysis of UAV–fog without QLMDP and collaborative UAV–fog with
    QLMDP network. QoS Average latency Computing time Throughput Iterations UAV–fog
    (without QLMDP) Collaborative UAV–fog (with QLMDP) UAV–fog (without QLMDP) Collaborative
    UAV–fog (with QLMDP) UAV–fog (without QLMDP) Collaborative UAV–fog (with QLMDP)
    I1 0.000286 0.000101 0.03 0.01 2800 3000 I2 0.020132 0.001194 0.16 0.026 1930
    2500 I3 0.030156 0.011186 0.28 0.037 2349 2700 I4 0.042189 0.021142 0.39 0.1 2560
    3400 I5 0.052678 0.005841 0.46 0.028 2698 3500 I6 0.036005 0.003905 0.29 0.019
    3216 3912 I7 0.002124 0.003114 0.11 0.02 200 4000 I8 0.001263 0.005926 0.19 0.07
    3997 4290 TABLE 6. Comparative result analysis and discussions of our proposed
    network with state-of-the-art schemes. Reference, year Key techniques Proposed
    scheme Outcomes UAV Fog Edge 5G Learning Yijing et al.,22 2017 ✓ ✗ ✗ ✗ Q-learning
    Adaptive and random exploration schemes (ARE) are used to navigate and search
    obstacles from the UAV path and their avoidance ARE scheme results in online path
    learning in an unknown environment. The results show satisfactory in terms of
    speed, accuracy, and obstacle avoidance Zhang et al.,23 2020 ✓ ✗ ✗ ✗ Machine learning
    (ML) The user distribution and downstream demand for traffic are projected using
    the weighted expectation maximization (WEM) scheme Contract theory to truthful
    exchange of data The results show the higher prediction accuracy is produced by
    WEM, especially when the cellular system''s traffic congestion becomes spatially
    uneven Chen et al.,11 2020 ✓ ✓ ✗ ✗ ✗ Multilevel mobile fog offloading scheme The
    results show a betterment in users'' QoS, reliability, and communication efficiency
    in the network Xue et al.,24 2022 ✓ ✗ ✗ ✓ Deep reinforcement learning (DRL) The
    author proposed multiagent deep reinforcement learning (MADRL) to provide better
    performance Considered 60 GHz mm-wave for high-speed communications The results
    analysis shows the proposed network supports high-speed networking and better
    performance in comparison with traditional networking Present study ✓ ✓ ✗ ✓ QLMDP
    Collaboration of local UAVs and UAV–fog nodes to provide better QoS Considered
    QLMDP to provide intelligent networking with low latency and high efficiency The
    simulation results show the comparison between the existing UAV-based network,
    the UAV–MEC network, and the proposed network. It shows that the collaboration
    of the UAV–fog network achieves low latency and better efficiency 6 LIMITATIONS
    OF UAV–FOG NETWORK This section provides the limitations of the proposed UAV–fog
    network in terms of implementation and complexity. Further, also highlights the
    direction to understand and work on these limitations are as follows: Energy consumption
    The limited energy of UAVs puts a strain on the UAV–fog network to work in emergencies.
    In UAV–fog-assisted ground networks, UAVs use much power even when no data are
    exchanged from UAV server nodes to ground nodes. Conserving power in the absence
    of a transmission-reception mode is challenging. Controlling a node''s communication
    capability or keeping some inactive nodes to sleep are two ways to reduce energy
    consumption. These methods will only reduce energy consumption so the network
    can function without the affected nodes. Trajectory design In the UAV–fog network,
    offloading data from different trajectories also consumes a lot of energy. The
    UAV follows a predetermined path across several regions to process the computing
    tasks that have been offloaded and deliver the data. Therefore, the trajectory
    of the UAVs should be optimized to reduce energy consumption. Delay Our proposal
    has reduced the delay to some extent. However, still, there is a challenge of
    handling extensive data traveling from the UAV–fog ground network, and minute
    delays will arise; while sending data from smart devices to local UAVs, local
    UAVs to UAV–fog nodes, and UAV–fog nodes to cloud servers can lead to propagation
    and transmission delays, extensive computing processing may place a strain on
    a single cloud server, resulting in processing and queuing delays. Our proposed
    network focuses on delay and handles it with the collaboration of local UAVs and
    UAV–fog nodes by optimizing path loss in the network. Moreover, using QLMDP in
    scheduling and offloading data from optimal paths minimized the overall delay
    in the proposed network. Coverage and connectivity Coverage is a significant characteristic
    of a wireless network that UAVs can consider across vast geographic areas. Another
    major problem is to sustain connectivity over an extended period while covering
    a broad geographic area. Collaborative multiple low altitude platform (LAP) UAVs
    cover a wide geographical area. The most effective way to maximize coverage with
    ongoing connectivity is to use a variety of approaches, including ant colony optimization,
    genetic algorithms, optimal path planning, density-based clustering, area mapping,
    and cost-based neural functions. Signal degradation The frameworks for the present
    UAV–fog data transmission and storage models lack the necessary components for
    possible complicated and large-scale applications in 5G and 6G networks. UAV movement
    and environmental disruptions are to blame for this. When a signal travels from
    a UAV–fog to ground users, it degrades the signal quality because of multiple
    hop propagation. Additionally, the bandwidth of the UAV–fog network may be consumed
    and clogged if all the data collected by the smart gadgets are sent directly to
    the cloud server. Also, interference from neighboring UAV–fog or local UAV nodes
    may affect the quality of signal in the UAV–fog ground network. The possibility
    of continuous sensing of the neighboring node data may affect the signal quality.
    Heterogeneity The UAV–fog ground network is composed of heterogeneous network
    with different node service capacity and capability. It is quite complex to coordinate
    the whole network. In collaboration of UAV–fog nodes (decentralized nodes) and
    local UAVs nodes while interacting with ground users, the fog computing has ability
    to manage the heterogeneity of nodes. Network complexity The proposed network
    is simulated with MATLAB R2021a. The environment modeling of multiple UAVs and
    fog nodes and then simulating the environment model with QLMDP is quite complex
    because of the unavailability of some advanced inbuilt models. Software freeze
    Another complexity with UAV–fog infrastructure is software freeze and disruptions.
    The source fog node and its program components crash if a UAV–fog node does not
    provide underlying services to other nodes for an extended period or is idle for
    a while. The cause of this is that the application may freeze or fail if the server
    nodes are not in communication throughout the application interval. 7 CONCLUSION
    AND FUTURE SCOPE This paper proposes a four-tier architecture of UAVs–fog collaborative
    network and QLMDP for smart offloading for real-time applications. To handle the
    problem of data congestion during offloading large amounts of data, we have formulated
    an optimal path for smart data offloading with minimum latency based on QLMDP.
    In this network, we considered a QLMDP, where the local UAVs act as a relay that
    accepts the requests from TDs for preprocessing, and UAV–fog nodes do optimal
    computing to serve smart things (TDs and ground users) with ultralow latency and
    high throughput. Furthermore, UAV–fog nodes can also do smart offloading of data
    tasks to TDs with less computing time and balancing the load caused by multiple
    nodes in the network. The comparison is made between UAV-based networks, UAV–MEC
    networks, and collaborative UAV–fog networks. The analysis of simulation results
    shows the proposed network improves QoS, including low latency 0.02 ms and high
    throughput. In the future, the work will be more inclined toward optimizing the
    energy consumption of UAV–fog nodes. The federated learning-based UAV–fog networking
    will be investigated, and the secure offline offloading of data through an optimized
    path will also be explored. Open Research REFERENCES Early View Online Version
    of Record before inclusion in an issue e5759 Figures References Related Information
    Recommended Quality of service‐aware approaches in fog computing Mostafa Haghi
    Kashani,  Amir Masoud Rahmani,  Nima Jafari Navimipour International Journal of
    Communication Systems Cloud and Fog Computing Secure Connected Objects, [1] A
    job scheduling algorithm for delay and performance optimization in fog computing
    Bushra Jamil,  Mohammad Shojafar,  Israr Ahmed,  Atta Ullah,  Kashif Munir,  Humaira
    Ijaz Concurrency and Computation: Practice and Experience Dual wideband high‐efficient
    reflective linear‐to‐dual‐circular dual‐rotational polarization converter with
    two complementary twin‐split square‐ring resonators Reza Zaker International Journal
    of Communication Systems Asynchronous multiuser detection for sparse code multiple
    access with unknown delays: A compressed sensing approach Dylan Wheeler,  Erin
    E. Tripp,  Balasubramaniam Natarajan International Journal of Communication Systems
    Download PDF Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms
    of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies Developing World Access HELP & SUPPORT Contact Us Training
    and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies."'
  inline_citation: '>'
  journal: International Journal of Communication Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Unmanned aerial vehicles in collaboration with fog computing network for
    improving quality of service
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Venkata Subbamma T.
  - Arvind C.
  citation_count: '0'
  description: A new paradigm in fog computing may develop shortly of the confluence
    of cyber-physical systems (CPS), industrial digital twins (DT), and cutting-edge
    NextGen wireless networks in the world of digital communication. The research
    highlights the growing necessity for effective uplink transmission in the context
    of sustainable big MIMO systems. It also demonstrates the need to utilise digital
    twins and the internet of things to collect data. It is not possible to find any
    research on a method that optimises spectrum efficiency while also taking into
    account latency, energy consumption, channel gain, signal-to-noise ratio (SNR),
    bit error rate (BER), and network throughput. The recent focus of this article
    is on enhancing the spectral efficiency of massive Multiple Input Multiple Output
    (MIMO) systems. It recommends utilising a framework for digital twins in conjunction
    with data gathered from the Internet of Things (IoT) and nonlinear precoding that
    is based on the Deep Radial Basis Function (DRBF). The DRBF performs calculations
    on precoding matrices in order to maximise the efficiency of the transmission
    process. The research took into consideration a number of significant characteristics,
    including energy consumption, latency, signal-to-noise ratio (SNR), network throughput,
    and utilisation of spectral efficiency. The spectral efficiency has shown a considerable
    improvement, which is indicative of an increase in the output of data. Reduced
    latency values place an emphasis on greater real-time responsiveness, which is
    particularly important for time-sensitive applications. Following the completion
    of the network throughput test, the findings indicated how effectively the method
    functioned when transferring a greater quantity of data. When the BER number was
    lower and the SNR value was greater, it showed that the data was more accurate.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Journal of Environmental Protection and Ecology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: DIGITAL TWIN-BASED UPLINK TRANSMISSION OPTIMISATION USING DEEP RADIAL BASIS
    FUNCTION PRECODING FOR SPECTRAL EFFICIENCY IN SUSTAINABLE MASSIVE MIMO SYSTEMS
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Saad M.
  - Enam R.N.
  - Qureshi R.
  citation_count: '0'
  description: As the volume and velocity of Big Data continue to grow, traditional
    cloud computing approaches struggle to meet the demands of real-time processing
    and low latency. Fog computing, with its distributed network of edge devices,
    emerges as a compelling solution. However, efficient task scheduling in fog computing
    remains a challenge due to its inherently multi-objective nature, balancing factors
    like execution time, response time, and resource utilization. This paper proposes
    a hybrid Genetic Algorithm (GA)-Particle Swarm Optimization (PSO) algorithm to
    optimize multi-objective task scheduling in fog computing environments. The hybrid
    approach combines the strengths of GA and PSO, achieving effective exploration
    and exploitation of the search space, leading to improved performance compared
    to traditional single-algorithm approaches. The proposed hybrid algorithm results
    improved the execution time by 85.68% when compared with GA algorithm, by 84%
    when compared with Hybrid PWOA and by 51.03% when compared with PSO algorithm
    as well as it improved the response time by 67.28% when compared with GA algorithm,
    by 54.24% when compared with Hybrid PWOA and by 75.40% when compared with PSO
    algorithm as well as it improved the completion time by 68.69% when compared with
    GA algorithm, by 98.91% when compared with Hybrid PWOA and by 75.90% when compared
    with PSO algorithm when various tasks inputs are given. The proposed hybrid algorithm
    results also improved the execution time by 84.87% when compared with GA algorithm,
    by 88.64% when compared with Hybrid PWOA and by 85.07% when compared with PSO
    algorithm it improved the response time by 65.92% when compared with GA algorithm,
    by 80.51% when compared with Hybrid PWOA and by 85.26% when compared with PSO
    algorithm as well as it improved the completion time by 67.60% when compared with
    GA algorithm, by 81.34% when compared with Hybrid PWOA and by 85.23% when compared
    with PSO algorithm when various fog nodes are given.
  doi: 10.3389/fdata.2024.1358486
  full_citation: '>'
  full_text: '>

    "Top bar navigation About us All journals All articles Submit your research Search
    Login Frontiers in Big Data Sections Articles Research Topics Editorial Board
    About journal Download Article 420 Total views 119 Downloads View article impact
    View altmetric score SHARE ON Edited by Carson Leung University of Manitoba, Canada
    Reviewed by MOHIT KUMAR Dr. B. R. Ambedkar National Institute of Technology Jalandhar,
    India Mitsuo Gen Fuzzy Logic Systems Institute, Japan TABLE OF CONTENTS Abstract
    1 Introduction 2 Literature review 3 Problem statement 4 The proposed algorithm
    5 Results and discussion 6 Conclusion Data availability statement Author contributions
    Funding Conflict of interest Publisher''s note References Export citation Check
    for updates People also looked at Machine learning-driven task scheduling with
    dynamic K-means based clustering algorithm using fuzzy logic in FOG environment
    Muhammad Saad Sheikh, Rabia Noor Enam and Rehan Inam Qureshi Implementation of
    a smart energy meter using blockchain and Internet of Things: A step toward energy
    conservation Muhammad Tahir, Najma Ismat, Huma Hasan Rizvi, Asma Zaffar, Syed
    Muhammad Nabeel Mustafa and Affan Ahmed Khan Hybridization of long short-term
    memory neural network in fractional time series modeling of inflation Erman Arif,
    Elin Herlinawati, Dodi Devianto, Mutia Yollanda and Dony Permana The impact of
    comorbidities and economic inequality on COVID-19 mortality in Mexico: a machine
    learning approach Jorge Méndez-Astudillo Predicting risk of preterm birth in singleton
    pregnancies using machine learning algorithms Qiu-Yan Yu, Ying Lin, Yu-Run Zhou,
    Xin-Jun Yang and Joris Hemelaar METHODS article Front. Big Data, 21 February 2024
    Sec. Big Data Networks Volume 7 - 2024 | https://doi.org/10.3389/fdata.2024.1358486
    Optimizing multi-objective task scheduling in fog computing with GA-PSO algorithm
    for big data application Muhammad Saad1,2* Rabia Noor Enam1 Rehan Qureshi3 1Computer
    Engineering Department, Sir Syed University of Engineering and Technology, Karachi,
    Pakistan 2Software Engineering Department, Sir Syed University of Engineering
    and Technology, Karachi, Pakistan 3University of Southern Queensland, Springfield,
    QLS, Australia As the volume and velocity of Big Data continue to grow, traditional
    cloud computing approaches struggle to meet the demands of real-time processing
    and low latency. Fog computing, with its distributed network of edge devices,
    emerges as a compelling solution. However, efficient task scheduling in fog computing
    remains a challenge due to its inherently multi-objective nature, balancing factors
    like execution time, response time, and resource utilization. This paper proposes
    a hybrid Genetic Algorithm (GA)-Particle Swarm Optimization (PSO) algorithm to
    optimize multi-objective task scheduling in fog computing environments. The hybrid
    approach combines the strengths of GA and PSO, achieving effective exploration
    and exploitation of the search space, leading to improved performance compared
    to traditional single-algorithm approaches. The proposed hybrid algorithm results
    improved the execution time by 85.68% when compared with GA algorithm, by 84%
    when compared with Hybrid PWOA and by 51.03% when compared with PSO algorithm
    as well as it improved the response time by 67.28% when compared with GA algorithm,
    by 54.24% when compared with Hybrid PWOA and by 75.40% when compared with PSO
    algorithm as well as it improved the completion time by 68.69% when compared with
    GA algorithm, by 98.91% when compared with Hybrid PWOA and by 75.90% when compared
    with PSO algorithm when various tasks inputs are given. The proposed hybrid algorithm
    results also improved the execution time by 84.87% when compared with GA algorithm,
    by 88.64% when compared with Hybrid PWOA and by 85.07% when compared with PSO
    algorithm it improved the response time by 65.92% when compared with GA algorithm,
    by 80.51% when compared with Hybrid PWOA and by 85.26% when compared with PSO
    algorithm as well as it improved the completion time by 67.60% when compared with
    GA algorithm, by 81.34% when compared with Hybrid PWOA and by 85.23% when compared
    with PSO algorithm when various fog nodes are given. 1 Introduction 1.1 A new
    era of big data and its impact on computing The pace of data generation is increasing
    rapidly due to the widespread use of IoT sensors and devices, as well as people''s
    increasing reliance on online services and social media (Chandra and Verma, 2023;
    Mortaheb and Jankowski, 2023). The phenomenon known as Big Data has had a significant
    impact on several aspects of modern life, such as healthcare, finance, and transportation
    (Mohanty, 2015; Fanelli et al., 2023). 1.2 Challenges and opportunities of big
    data There are substantial obstacles to overcome, despite the fact that Big Data
    has enormous potential for insight and innovation: Data Volume: Innovative storage
    methods and scalable computer infrastructure are necessary for managing and storing
    huge datasets (Kim et al., 2023; Wang and Yin, 2023). Data Velocity: In order
    to derive useful insights from data streams in real-time as they are being created,
    high-performance computing skills are required (Bharany, 2023; Olawoyin et al.,
    2023). Data Variety: Flexible processing methods and sophisticated analytics approaches
    are required due to the different nature of data sources, which might range from
    structured databases to unstructured social media postings (Khang et al., 2023;
    Qi et al., 2023). 1.3 The emergence of fog computing: a paradigm shift in data
    processing Big Data is highly challenging for the conventional cloud computing
    architecture, which relies on centralizing data and processing it in distant data
    centers, to effectively manage (Rosati et al., 2023; Sheng et al., 2023). A unique
    method to data processing is necessary because of the excessive latency, poor
    scalability, and security concerns that have been raised against it. At this time,
    fog computing, which is a paradigm switcher, comes into being. Computing in the
    fog pushes the computational capabilities of the cloud closer to the edge of the
    network (Bebortta et al., 2023; Hornik et al., 2023). Fog computing is used when
    data is produced and consumed at the network''s peripheral. The ability of fog
    computing to process data locally on a network of devices that have restricted
    capabilities (fog nodes) is one of the many benefits that fog computing provides.
    Some of these advantages includes the amount of latency that an application experiences
    is significantly reduced when it processes data locally as opposed to forwarding
    it to faraway cloud servers (Hussein et al., 2023). This is because the amount
    of delay that an application experiences is reduced greatly. This is a very important
    piece of information, particularly for real-time applications that need rapid
    replies and opportunities for decision. The Internet of Things (IoT) is able to
    effectively handle enormous amounts of data and expand without encountering any
    challenges as the network grows. This is made possible by distributed processing,
    which is made possible by fog computing (Fazel et al., 2023). The processing of
    sensitive data locally decreases the quantity of data that is transferred to distant
    servers, which in turn promotes both security and privacy (Raj, 2023). This results
    in enhanced security. Furthermore, this is particularly helpful for applications
    that deal with information that is of crucial importance. Offloading workloads
    to fog nodes may help you decrease the operating expenses that are connected with
    cloud infrastructure and maximize the utilization of other resources (Das and
    Inuwa, 2023; Mohamed et al., 2023). When you do this, you can also lower the costs
    associated with cloud infrastructure. 1.4 Multi-objective task scheduling: orchestrating
    tasks in the fog with big data considerations Despite the fact that fog computing
    offers a number of benefits for the processing of large amounts of data, one of
    the most significant challenges is still the discovery of an efficient method
    to schedule operations within the network. This involves allocating work to appropriate
    fog nodes, taking into consideration the many objectives that are being pursued
    is Minimizing Execution Time. Rapid response and customer satisfaction are of
    the utmost importance in real-time applications due to the potentially disastrous
    effects of delays. Finding the most efficient way to do things is the key to achieving
    this goal. Customers anticipate prompt responses while interacting with apps.
    Making the most efficient use of time is crucial. Particularly in interactive
    situations, minimizing reaction time is crucial for ensuring a satisfying user
    experience and avoiding disappointment. An important factor in assessing the system''s
    overall efficacy and efficiency is the sum of all job completion times. Reducing
    the completion time is of the utmost importance. Finding the sweet spot for job
    completion times is critical for avoiding network bottlenecks and ensuring effective
    task processing. By reducing the amount of data transported over unsecure networks,
    processing data near to its origin helps to alleviate network congestion and enhances
    data privacy. To do this, data is used to its fullest extent when it is located
    close to its point of origin. The vast majority of the time, these goals are diametrically
    opposed. A job that is prioritized based on execution time may see an increase
    in response time owing to network congestion or queueing if it is assigned to
    a strong fog node. The purpose of this is to show how the execution time may be
    prioritized. Finding the sweet spot for task schedule optimization inside the
    Big Data framework requires a multi-objective strategy. Using this approach well
    requires thinking about all of these goals at once and finding the best middle
    ground. For efficient Big Data management in fog, a hybrid GA-PSO algorithm seems
    to be a viable option. This is because it overcomes the shortcomings of previous
    methods and effectively deals with Big Data. Effective multi-objective task scheduling
    in fog computing settings is the goal of this approach, which combines the strengths
    of genetic algorithm (GA) with particle swarm optimization (PSO). The two methods
    work together to achieve this goal. The fog computing environment''s task scheduling
    mechanism is shown in Figure 1. figure 1 Figure 1. The task scheduling architecture
    in the fog environment. This study aims to enhance task scheduling in fog computing
    for big data applications, motivated by the need to overcome resource constraints
    in fog settings while meeting the rigorous demands of large data processing. We
    provide a hybrid GA-PSO method that utilizes the collective advantages of global
    exploration and rapid convergence to effectively manage the inherent trade-offs
    in multi-objective optimization. Our objective is to explore the fundamental element
    of fog computing in order to fully use its potential in various applications,
    improve user satisfaction, and optimize energy consumption, eventually creating
    a more robust and environmentally friendly large data processing environment.
    The remaining portions of the article are divided into the following sections
    for your convenience: In Section 2, we will conduct a review of the most current
    research that has been conducted in this area. In the third section, the structure
    of the algorithm that is used to schedule jobs as well as its definitions are
    discussed. The performance assessment of the multi-objective task scheduling problem
    in a fog environment is discussed in Section 4, which has a wealth of information
    on the subject. In addition to the data obtained from the experiment, this section
    also includes a summary of the results obtained from the experiment. In Section
    5, a synopsis of the work that will be carried out in the future is offered. 2
    Literature review Several researchers have proposed various approaches to address
    the multi-objective task scheduling problem in fog computing for Big Data applications.
    These approaches can be broadly categorized as follows: 2.1 Heuristic-based algorithms
    These rely on domain-specific knowledge and heuristics to assign tasks to fog
    nodes. Simple and easy to implement, but often struggle to adapt to diverse scenarios
    and may not find optimal solutions. Examples: Greedy algorithm, First-Come First-Serve
    (FCFS). 2.2 Single-objective optimization algorithms These focus on optimizing
    only one objective, typically execution time or resource utilization. May not
    consider other important objectives and may not lead to optimal solutions when
    considering multiple conflicting objectives. Examples: Shortest Job First (SJF),
    Round-Robin scheduling. 2.3 Metaheuristic algorithms These are population-based
    optimization algorithms inspired by natural phenomena. Can handle complex search
    spaces and explore diverse solutions effectively. Examples: Genetic Algorithm
    (GA), Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO). 2.4 Hybrid
    algorithms Combine the strengths of different optimization algorithms to achieve
    better performance. Can leverage the exploration capabilities of GA and the exploitation
    strengths of PSO for a more balanced and comprehensive search. Examples: PSOWOA.
    Here we provide a literature review of job scheduling algorithms that make use
    of one of these methods. A Multi-Objectives Grey Wolf Optimizer (MGWO) algorithm
    hosted on the fog broker to optimize both delay and energy consumption was presented
    by Saif et al. (2023). Simulation results show MGWO''s effectiveness compared
    to existing algorithms in reducing both objectives. Combining fog computing with
    cloud computing allows processing near data production sites for faster speeds
    and reduced bandwidth needs, especially beneficial for real-time IoT applications.
    However, limited resources in fog nodes necessitate efficient task scheduling
    to meet dynamic demands. This survey analyzes existing techniques, categorized
    into machine learning, heuristic, metaheuristic, and deterministic approaches,
    evaluating them based on execution time, resource utilization, and various other
    parameters is presented by Hosseinzadeh et al. (2023). It reveals that metaheuristic-based
    methods are most common (38%), followed by heuristic (30%), machine learning (23%),
    and deterministic (9%). Energy consumption is the most addressed objective (19%).
    A number of future options for enhancing fog computing job scheduling are highlighted
    in the conclusion of the paper. These future avenues include responding to dynamic
    situations, adding security and privacy, and leveraging improvements in artificial
    intelligence and machine learning applications. An approach known as a hybrid
    meta-heuristic optimization algorithm (HMOA) was published by Jakwa et al. (2023)
    for the purpose of scheduling tasks in fog computing in an energy-efficient manner.
    HMOA combines MPSO with deterministic spanning tree to overcome the drawbacks
    of separate methods. The hybrid MPSO-SPT allocates and manages resources, while
    MPSO schedules user tasks across fog devices. HMOA uses resources and energy better
    than state-of-the-art algorithms. The usage and evaluation of iFogSim permitted
    this. Future hybrid experiments will investigate execution time. This article
    addresses multi-objective scheduling in fog computing, where remote resources
    and unexpected demands are prevalent. Dai et al. (2023) design a multi-objective
    optimization model that maximizes time delay and energy use after proposing a
    dynamic priority adjustment methodology for task offloading. A model that optimizes
    priority adjustment follows. Due to its ability to manage complicated Pareto fronts
    and reduce reaction time and energy consumption, the M-E-AWA algorithm (MOEA/D
    with adaptive weight adjustment) may be useful for fog task scheduling. The goal
    of this project is to schedule scientific activities in fog computing while conserving
    energy. HDSOS-GOA is a new hybrid approach proposed in this study. Symbiotic Organisms
    Search (SOS) and Grasshopper Optimization Algorithm (GOA) algorithms are used
    with learning automata for dynamic selection. Mohammadzadeh et al. (2023) use
    the HEFT heuristic and DVFS methodology to optimize energy usage and job scheduling.
    Provided this way HDSOS-GOA surpasses rival scheduling algorithms in energy use,
    makespan, and completion time, according to extensive testing. Thus, it may be
    a low-power fog computing option. To efficiently manage requests from the Internet
    of Things (IoT) in a cloudy environment, this study proposes the Electric Earthworm
    Optimization technique (EEOA), a novel multi-objective job scheduling approach.
    EEOA, a hybrid of the Earthworm Optimization Algorithm (EOA), and the Electric
    Fish Optimization Algorithm (EFO), enhances EFO''s exploitation capabilities.
    Kumar and Karri (2023) implement EEOA, which is an attractive approach to cloud-fog
    job scheduling that is both efficient and economical. Significant gains in contrast
    to current techniques are shown in simulations utilizing real-world workloads
    like CEA-CURIE and HPC2N. Gains in efficiency of up to 89%, reductions in energy
    usage of 94%, and savings of 87% in overall cost are all part of these enhancements.
    The explosive growth of IoT data creates a burden on the cloud, hindering QoS.
    To address this, fog computing extends computing capabilities to the network edge.
    Scheduling tasks in fog environments faces challenges due to heterogeneous fog
    nodes and dynamic user demands. This article presents a comprehensive literature
    review, classifying task scheduling algorithms by approach, highlighting frequently
    used parameters, comparing available simulation tools, and identifying open issues
    and challenges to guide future research efforts in fog computing task scheduling
    is written by Saad et al. (2023). Due to the explosive growth of IoT and its high
    data demand, cloud computing struggles to support real-time applications with
    low latency. Fog computing emerges as a solution, enabling data processing near
    edge devices for faster response times. Scheduling tasks effectively on fog nodes
    with limited resources is crucial. This article proposes EETSPSO, an energy-efficient
    task scheduling algorithm based on Particle Swarm Optimization. EETSPSO outperforms
    existing algorithms like BLA and MPSO by minimizing makespan (6.39%−4.71%), reducing
    energy consumption (9.12%−11.47%), and decreasing execution time (9.83%−6.32%)
    and this is work is carried by Vispute and Vashisht (2023). Fog computing helps
    improve the quality of service (QoS) for IoT applications by enabling task offloading
    near data sources. However, reducing delay remains challenging due to resource
    limitations and workload imbalance. To address these issues, this article proposes
    a dynamic collaborative task offloading (DCTO) approach that dynamically derives
    the offloading policy based on fog device resources is presented by Tran-Dang
    and Kim (2023). Tasks can be executed by one or multiple fog devices through parallel
    subtask execution to reduce delay. Extensive simulations show that DCTO significantly
    reduces average delay in systems with high request rates and heterogeneous fog
    environments compared to existing solutions. Additionally, DCTO''s low computational
    complexity allows for online implementation. Extending cloud computing, fog computing
    offers lower latency and resource utilization improvements for IoT devices. Task
    scheduling in fog maps tasks to appropriate fog nodes to optimize resource usage
    and reduce IoT device costs. This article focuses on scheduling offloaded tasks
    for multiple users. Formulating the problem as a combinatorial optimization, the
    article proposes an improved integer particle swarm optimization method for efficient
    solution. Compared to the traversal method, the He and Bai (2023) proposed algorithm
    achieves 90% faster runtime while still finding an approximate optimal solution,
    demonstrating its effectiveness for fog computing task scheduling optimization.
    This article addresses the challenge of dynamic workflow scheduling in fog computing,
    where existing methods only consider cloud servers and ignore mobile and edge
    devices. A new problem model and simulator are presented that consider all three
    types of devices as a single system for task execution. A novel Multi-Tree Genetic
    Programming (MTGP) method is proposed by Xu et al. (2023) to automatically evolve
    scheduling heuristics for real-time decision-making at routing and sequencing
    points. Experiments show that MTGP significantly outperforms existing methods,
    achieving up to 50% reduction in makespan across different scenarios. The increasing
    number of IoT devices using cloud services has led to latency issues. Fog computing,
    which places processing resources closer to users at the network edge, emerges
    as a solution to reduce latency and improve user experience. However, minimizing
    latency without increasing energy consumption requires a powerful scheduling solution.
    In order to strike a compromise between energy usage and response time, Khiat
    et al. (2024) introduce GAMMR, a new genetic-based method for job scheduling in
    fog-cloud situations. Simulations on different datasets show that GAMMR achieves
    an average 3.4% improvement over the traditional genetic method. Workflow scheduling
    in cloud-fog settings is the difficult subject of this study. Bansal and Aggarwal
    (2023) designed a novel hybrid approach called the Particle Whale Optimization
    Algorithm (PWOA) to overcome the drawbacks of earlier algorithms like PSO and
    WOA. Combining the best features of PSO and WOA, PWOA enhances exploration and
    exploitation capabilities. Simulation results demonstrate that, across a range
    of scientific processes, PWOA minimizes both the total execution time and cost
    more effectively than PSO and WOA. It might be a good substitute for efficient
    workflow scheduling because of this. For application in fog-cloud computing settings,
    this article introduces a novel scheduling method called PGA. When optimizing
    task execution, the PGA algorithm considers not only energy consumption and deadline
    compliance but also the processing capability of the cloud and the resource limits
    of fog nodes. To achieve this goal, it uses a genetic algorithm in conjunction
    with job prioritization to distribute tasks to the right nodes in the cloud or
    fog. Hoseiny et al. (2021) conduct thorough simulations to show that PGA outperforms
    current techniques. Several researchers have used varying numbers of goals in
    an effort to solve the multi-objective optimization issue in workflow applications.
    For efficient resource allocation in workflows, this research suggests a hybrid
    meta-heuristic GA-PSO method. The suggested strategy leverages the characteristics
    of diverse jobs and nodes to achieve three objectives: reduce execution time,
    minimize reaction time, and lower overall completion time. The article by Walia
    et al. (2023) provides a thorough examination of the most advanced solutions available,
    including both AI and non-AI approaches. It includes detailed discussions on related
    quality of service measures, datasets, constraints, and issues. Each categorized
    resource management problem is accompanied with mathematical formulas, which introduce
    a quantitative aspect to the examination. The study concludes by highlighting
    potential areas for future research, advocating for the incorporation of advanced
    technologies such as Serverless computing, 5G, Industrial IoT (IIoT), blockchain,
    digital twins, quantum computing, and Software-Defined Networking (SDN) into existing
    Fog/Edge paradigms. This integration aims to improve business intelligence and
    analytics in IoT-based applications. In summary, the article provides a thorough
    examination, analysis, and plan for tackling obstacles in managing Fog/Edge resources
    for IoT applications. Kumar et al. (2023a) discuss the shortcomings of the cloud
    model in fulfilling the latency requirements of Industrial IoT (IIoT) applications.
    This article presents a new AI-based framework designed to optimize a multi-layered
    integrated cloud-fog environment. The system especially focuses on making real-time
    choices on offloading tasks. The framework integrates a fuzzy-based offloading
    controller and employs an AI-based Whale Optimization Algorithm (WOA) to increase
    decision-making for enhanced Quality-of-Service (QoS) parameters. The experimental
    findings reveal substantial improvements, such as a 37.17% drop in the time taken
    to complete a task, a 27.32% reduction in the amount of energy used, and a 13.36%
    decrease in the cost of executing the process, when compared to the standard reference
    methods. In summary, the proposed framework demonstrates the capacity of AI-driven
    solutions to enhance resource management in IIoT applications inside a fog computing
    environment. Kumar et al. (2021) provide ARPS, a system that enables autonomous
    allocation and organization of resources on cloud platforms. ARPS is a system
    that aims to effectively allocate cloud services to fulfill the Quality of Service
    (QoS) needs of different end-users. It focuses on optimizing both execution time
    and cost at the same time. By using the Spider Monkey Optimization (SMO) method,
    this system tries to address a multi-objective optimization issue. Through rigorous
    simulation study using Cloudsim, it has been shown to outperform four other current
    mechanisms. On summary, ARPS offers a proficient approach to enhance resource
    allocation and scheduling on cloud platforms. Kumar et al. (2023b) presents a
    system for predicting workload and allocating resources in fog-enabled Industrial
    Internet of Things (IoT). By using a sophisticated autoencoder model, the system
    predicts workloads and adjusts the number of fog nodes (FNs) accordingly. Additionally,
    it incorporates the crow search algorithm (CSA) to find the most effective FN.
    By conducting simulation study, the suggested scheme demonstrates superior performance
    compared to current models in terms of execution cost, request rejection ratio,
    throughput, and response time. This architecture provides a very effective approach
    for strategically positioning FNs to execute dynamic industrial IoT workloads
    with maximum efficiency. The study focuses on the pressing issue of resource scheduling
    in cloud computing, which arises from the increasing need for on-demand services
    and the diverse characteristics of cloud resources (Kumar et al., 2019). Cloud
    services encounter inefficiencies when scheduling mechanisms are unable to effectively
    balance resource utilization, resulting in either a decline in service quality
    or resource waste. The main objective of scheduling algorithms is to evenly divide
    various and intricate jobs across cloud resources, while minimizing imbalance
    and optimizing crucial performance characteristics like as reaction time, makespan
    time, dependability, availability, energy consumption, cost, and resource utilization.
    The literature review categorizes current scheduling algorithms, including heuristic,
    meta-heuristic, and hybrid methods, emphasizing their strengths and weaknesses.
    The work seeks to function as a methodical and all-encompassing reference for
    novice researchers in the cloud computing domain, promoting more advancement in
    scheduling methodologies. This article tackles the complex scheduling of real-time
    tasks on multiprocessors using a new algorithm called mohGA (Yoo and Gen, 2007).
    Combining GA and SA''s strengths, mohGA efficiently finds near-optimal schedules
    while optimizing conflicting objectives like tardiness and completion time. Simulation
    results show mohGA outperforms existing methods, making it a promising approach
    for real-time task scheduling in various applications. “Network Models and Optimization:
    Multiobjective Genetic Algorithm Approach” provides a thorough and up-to-date
    examination of the use of multi-objective genetic algorithms in solving diverse
    network optimization issues in numerous fields (Gen et al., 2008). The extensive
    scope of algorithms and applications covered in this resource, which includes
    basic principles such as shortest route issues and complex situations like airline
    fleet assignment, is suitable for both individuals seeking knowledge and professionals
    in the field. This book is a wonderful resource for anyone who want to comprehend
    and apply sophisticated network optimization approaches employing multi-objective
    genetic algorithms. The author in his book (Gen and Yoo, 2008) examines the use
    of Genetic Algorithms (GA) in scheduling soft real-time jobs on multiprocessors.
    The objective is to minimize tardiness while reducing the complexity associated
    with conventional approaches. The benefit of GA stems from its multifaceted approach
    that integrates principles and heuristics, providing more intricate solutions
    for this NP-hard issue. The chapter offers a complete method to effective job
    scheduling with decreased complexity by addressing continuous, homogeneous, and
    heterogeneous real-time systems. The outcomes produced by algorithms based on
    the GA algorithm are, in essence, preferable to those produced by other algorithms
    when the total number of process repetitions exceeds a specifically defined threshold.
    Conversely, as the number of iterations of the process increases, the GA-based
    meta-heuristic algorithm will require an extended duration to generate the optimal
    solution. In addition, PSO-based meta-heuristic algorithms exhibit superior performance
    in a reduced time period in comparison to alternative approaches. Due to the rapid
    convergence of PSO-based algorithms toward a solution, however, the results''
    dependability may be compromised. This rapid convergence could cause the algorithms
    to become stuck in the solution that is locally optimal. As a result, the suggested
    algorithm differentiates itself by using the qualities that are associated with
    both the GA and the PSO algorithms. When compared to previous algorithms with
    the same goals, it is anticipated that the Hybrid GA-PSO method would function
    much quicker with a wider variety of workflow application sizes. In addition,
    the Hybrid GA-PSO method does not necessarily become stuck in the locally optimum
    solution since it makes use of the hybrid approach as it combine the GA with other
    optimization algorithms, such as local search methods (PSO), to leverage their
    respective strengths, which increases the precision of the answers provided and
    stops the algorithm from becoming trapped in the solution that is optimum for
    the local environment. 3 Problem statement Current scheduling methods often fail
    to adequately handle important factors in fog computing, such as response time,
    energy cost, and resource utilization. We need a more advanced and adaptable task
    scheduling solution since fog settings are dynamic, with different workloads and
    different hardware capabilities. The two main ideas in genetic algorithms (GAs)
    that stand for the harmony between finding new solutions and improving old ones
    are exploration and exploitation (Gen and Lin, 2023a). In order to find superior
    answers in other areas, exploration must extensively explore the search space.
    Taking use of what is already known about potential solutions allows you to boost
    their efficiency. An algorithm that puts too much emphasis on exploration runs
    the risk of squandering time and energy looking in fruitless corners of the search
    space. On the other side, if the algorithm starts focusing on exploitation too
    soon, it might end up convergent to a local optimum before it ever starts looking
    for a better solution. In extreme cases of natural selection, it is advantageous
    to reproduce individuals with high fitness levels. This means other, perhaps more
    productive areas of the search space can be overlooked when the space rapidly
    converges on a local limit. The current state of the population should inform
    how the selection pressure is adjusted. Here we may find a happy medium between
    exploiting and exploring. There are a few drawbacks to be mindful of, such premature
    convergence, but the genetic algorithm (GA) has proved effective in solving many
    optimization problems. The GA stops looking for potential productive regions of
    the search space because it becomes stuck at a local optimum. Genetic algorithms''
    (GAs) primary drawback is the possibility of over-convergence, which prevents
    them from exploring the search space for optimal solutions and instead keeps them
    stuck in a local optimum. The GA''s effectiveness is diminished, leading to frequent
    poor results. It is possible to avoid early convergence in GA by using hybrid
    techniques. To get the most out of GA, try combining it with other optimization
    techniques, including local search approaches. Both the GA and local search algorithms
    can work together to improve GA-found solutions; the GA can even provide new genetic
    material to sidestep local optima. The benefits of GA-PSO, a combination of genetic
    and particle swarm optimization, outweigh those of employing either approach alone.
    An improved search process and maybe superior solutions are the results of the
    hybrid strategy, which merges GA''s exploration skills with PSO''s exploitation
    characteristics (Shami et al., 2023). In order to effectively explore the search
    space, GA makes use of its mutation and crossover operators. Based to its information-sharing
    mechanism and velocity updates, PSO efficiently uses promising portions of the
    search space to quickly converge toward locally optimum solutions. 4 The proposed
    algorithm 4.1 Key components of the proposed algorithm Chromosome representation:
    incorporate critical data like task priority, execution timings, fog node assignments,
    and other relevant information into an efficient chromosomal representation that
    encodes task scheduling solutions. Genetic algorithm (GA) components: utilize
    genetic operators such as mutation and crossover to provide a diverse set of solutions
    for job assignment. Build a comprehensive fitness function that considers all
    relevant metrics, including resource utilization, energy usage, response time,
    and more. Particle swarm optimization (PSO) components: use particles to represent
    potential solutions to the assignment''s challenge. Using location and velocity
    updates, you may guide particles toward the optimal solutions in the solution
    space. Hybridization strategy: by combining the strengths of GA and PSO, you can
    speed up the exploration process and get closer to the best possible answers.
    Dynamic adaptation: it may be difficult for the traditional task scheduling approaches
    to adapt the dynamic and diverse distributed systems in the fog computing settings.
    Create dynamic methods that the algorithm may use to adapt to changes in the fog
    environment, such changes in workload, network condition, and resource availability.
    Expected outcomes: the proposed Blended GA-PSO algorithm anticipates providing
    a highly adaptive and efficient task scheduling solution for fog computing environments.
    The outcomes are expected to include improved system performance, reduced response
    times, and enhanced resource utilization. Figure 2 illustrates the primary operations
    involved in the GAPSO algorithm. The GA-PSO method begins by producing a random
    population (Mortaheb and Jankowski, 2023), and one of the parameters of the algorithm
    is a certain number of iterations that must be completed before the process may
    proceed. The population demonstrates that there is more than one approach to solving
    the issue of workflow tasks, and each approach is a method for allocating all
    of the workflow jobs to the VMs that are now accessible. The GA algorithm is run
    on the initialized population for the initial number of iterations. The total
    iterations are set to (n), then the GA algorithm and PSO algorithm will be executed
    half times of n. Iteration of the form (n/2) was chosen since it makes the suggested
    algorithm to improved performance compared to using either GA or PSO alone. GA
    excels at exploration, while PSO shines at exploitation. By allocating equal time,
    the hybrid algorithm leverages both capabilities effectively simpler to grasp,
    which is why it was used. figure 2 Figure 2. Workflow of GA-PSO Algorithm. Experiments
    demonstrated that the GA-PSO algorithm functioned most effectively when the number
    of iterations was divided between the GA algorithm and the PSO algorithm in an
    equal manner. Additionally, it is well knowledge that the GA and the PSO need
    a great deal of function evaluations. This is due to the fact that each has to
    consider the objective of each individual participant in the population represented
    by the current illustration. As a result, reducing the population size in a genetic
    algorithm or particle swarm optimization is a popular technique for keeping the
    GA or PSO''s performance from deteriorating in terms of how accurate the findings
    are and how quickly the rate of reduction is. Pareto optimality is a state in
    which a group of solutions exists where no individual solution can be enhanced
    without causing a deterioration in another solution (Gen and Cheng, 2000; Zhang
    et al., 2022; Gen and Lin, 2023b). Task scheduling involves the identification
    of schedules that optimize many goals concurrently, such as minimizing makespan
    and resource utilization. The proposed Hybrid GAPSO algorithm can be customized
    to identify Pareto optimum solutions using the following methods: Creating several
    fitness functions that reflect distinct aims, Employing multi-objective selection
    procedures that prioritize solutions that provide compromises among goals. Within
    the framework of the genetic algorithm (GA), also known as the genetic algorithm,
    the solutions are referred to as chromosomes. These chromosomes are enhanced with
    each iteration of the algorithm by making use of GA operators including selection,
    crossover, and mutation (Gen and Lin, 2023a). These GA operators are all part
    of the genetic algorithm. The generated chromosomes are what is sent into the
    PSO algorithm after the first half of the set number of iterations has been finished.
    In the PSO method, chromosomes are referred to as particles, and with each iteration
    of the PSO algorithm (Shami et al., 2023), these particles go through a process
    that results in a little but noticeable improvement. In order to accurately reflect
    the answer to the workflow task issue, the particle that has the lowest fitness
    value has been chosen. 4.2 Initializing population The search for a solution at
    the beginning of the iteration is carried out in a random fashion. Upon completion
    of the first cycle, a number of new populations are generated, and then those
    populations are recursively improved by making use of the previously found solutions
    to provide a collection of proposed solutions. Chromosome is the term used to
    refer to the population in the GA method. The number of workflow jobs is proportional
    to the length of the chromosomes, and the genes that make up each chromosome are
    meant to stand in for the various virtual machines. The chromosomes that were
    created at random serve as the input for the GA-PSO method that was just developed.
    These solutions will be chosen based on the performance of the GA algorithm. Algorithm
    1 provides a visual representation of the whole initialization population phase.
    algorithm 1 Algorithm 1. The initializing population. 4.3 Utilizing the GA algorithm
    Applying the GA to the whole population that has been created for (n/2) of the
    specified iterations is the first stage in the procedure. In order to solve the
    scheduling issue, it is required to develop the best answer possible from all
    of the alternatives. For the next (n/2) rounds of the specified procedure, the
    PSO is applied to the whole population that the GA algorithm generates. The PSO
    approach, which retains both the best and worst solutions in memory, may be useful
    for attaining quick convergence on the optimum solution when the GA algorithm
    yields inadequate outcomes. The total number of workflow jobs is equivalent to
    the number of chromosomes that are utilized as symbols for the answers provided
    by the genetic algorithm (GA), which define the scheduling solution for our issue.
    Each chromosome has a number of genes that stand in for the hosts'' virtual machines
    (VMs). During each cycle of the GA, the chromosomes are passed via three distinct
    operators: the crossover, mutation, and selection operators. The selection operator
    is the GA algorithm''s initial operator. In order for the algorithm to generate
    the succeeding generation of chromosomes, this operator is in responsible of picking
    various options from the pool of already-existing chromosomes in order to solve
    the problem. 4.4 Selection operator In the genetic algorithm known as the GA,
    each iteration does not include the evolution of all of the produced chromosomes
    via the GA operators. As a result, the chromosomes go through a process known
    as the tournament selection in order to choose the most advantageous chromosome
    from among a set of chromosomes. After the completion of a number of competitions
    involving a small number of chromosomes, the function chooses an id at random.
    The chosen ids each stand for an index that refers to one particular chromosome
    out of a larger collection of chromosomes. According to Algorithm 2, the best
    chromosome in the group is chosen to be the crossover operator. This decision
    is based on the chromosome''s overall fitness. algorithm 2 Algorithm 2. The selection
    operator (tournament selection). 4.5 Fitness function The primary objective of
    task scheduling in a fog computing environment is to minimize the amount of time
    required to complete a given activity, respond to a task, and finish a task. Algorithm
    3 provides a visual representation of the whole fitness function. algorithm 3
    Algorithm 3. The fitness function. The task execution time, shows how long it
    takes task i to run on virtual machine j, based on the following Equation 1. E
    ct (i,j)= t length(i,j) v m comp(i,j)     (1) Where tlength(i,j) is the duration
    of the job necessary to execute the instruction, vm represents the computational
    capacity of the vmcomp(i,j) virtual machine based on the following Equation 2.
    v m comp(i,j) =v m ∗ pesNumber(i,j) v m mips(i,j)     (2) Where vmpesNumber(i,j)
    is the number of CPUs in virtual machine j, vmmips(i,j) represents the processing
    power of virtual machine j. RT(i,j)=FT(i,j)−ST(i,j)     (3) Where RT(i, j) is
    the response time of all jobs i executing on resources j. FT(i, j) is the task''s
    completion time and ST(i, j) is its beginning time as shown in Equation 3. The
    total amount of time it takes to finish a job FT(i, j) is the addition of the
    amount of time it takes to transfer the task RT(i, j) and the amount of time it
    takes to run the task on the virtual machine Ect(i,j) as shown in Equation 4.
    FT(i,j)=RT(i,j)+ E ct (i,j)     (4) 4.6 The crossover operator The crossover operator
    seeks to build new chromosomes by rearranging the genes on every set of chromosomes
    in a different order. A number denoting the stage at which each chromosome is
    split in half is chosen at random from the range of the total number of genes
    on each chromosome in the crossover approach. A child chromosome with two sections
    containing the genes, or VMs, of both parents'' chromosomes is the result of the
    crossover. The first group of virtual machines (VMs) use the first chromosome
    up to the random number-determined index. The second chromosome is home to the
    second set of virtual machines, which start at the randomly selected index and
    go all the way to the end of the chromosome. An example of how the crossover strategy
    could be used is shown in Algorithm 4. algorithm 4 Algorithm 4. Cross over operator.
    4.7 The mutation operator The mutation operator''s goal is to induce unexpected
    mutations into the new chromosomes created by the preceding crossover operator.
    These newly formed chromosomes ought to be more fit than the ones that are currently
    there. The mutation rate variable decides whether or not a mutation will really
    occur. The mutation operator operates on the chromosome that was returned by the
    selection procedure. The process of mutation begins with a number that is created
    at random in such a way that it is either lower than or the same as the mutation
    rate. It is necessary to determine whether or whether two genes, known as VMs,
    located on the same chromosome are distinct from one another. If they are identical,
    their places will be altered in order to build a new chromosome, indicating a
    different work allocation across the available virtual machines (VMs). The chromosome
    that has been created is then sent on to the subsequent step of the process. Algorithm
    5 provides an illustration of the method''s implementation, which is known as
    mutation. algorithm 5 Algorithm 5. Mutation operator. 4.8 Implementing the PSO
    algorithm The solutions obtained by the GA algorithm are then put into the PSO
    algorithm, together with the remaining iterations indicated, to select the optimal
    solution from among the solutions generated by the GA technique. This is done
    to identify which of the solutions produced by the GA technique is the best solution.
    The solutions are referred to as particles in the PSO approach, the persons who
    make up each particle stand in for the VMs, and the index of each VM stands in
    for a workflow job. The PSO algorithm is divided into many sections, all of which
    will be explained in this article. 4.9 Evolve (gbest) and (pbest) of the particles
    Each iteration results in the production of a new generation of the particles
    by basing that generation''s velocity and location on the results of the previous
    iteration. The values of (gbest) and (pbest) are created during each iteration;
    the changes in particle velocity and placement are reliant on these values. Algorithm
    6 shows how to implement obtaining the best gbest and best pbest values of the
    particles. The values of (gbest) and (pbest), which are dynamically modified over
    the algorithm''s iterations, determine the forward movement of the particles in
    the PSO algorithm. These values are always shifting. The solutions that are generated
    by the GA method are identical to the values of pbest[k] for the very first iteration,
    where k is the variable that is utilized to distinguish one solution from another.
    This holds true for all of the iterations that follow. The alternative that results
    in the highest possible score (gbest) is the one that has the lowest possible
    fitness value. algorithm 6 Algorithm 6. Evolve (gbest) and (pbest) of the particles.
    In addition, throughout each iteration, a comparison is made between the particles
    that were previously formed and the particles that are now being generated, and
    this comparison is based on the fitness value. The location that has the particle
    with the highest value of fitness is (pbest). The (gbest) saves the most suitable
    particle from the entire production of particles in every repetition by contrasting
    their fitness value to the value of the particle with the best value in the previous
    iteration (Pbest). The comparison procedure guarantees that all of the particles
    are progressing toward the solution that is considered to be the best in order
    to arrive at the solution that is considered to be the optimum one. The pbest
    and gbest are crucial in guiding the particles toward promising regions of the
    solution space. The pbest allows each particle to remember its own historical
    best position, while the gbest guides the entire swarm toward the overall best
    position discovered by any particle. The process of updating pbest and gbest is
    straightforward: 4.9.1 Update individual best (pbesti) If the fitness (or objective
    value) of the current position xi(t) is better than the fitness of pbesti, update
    pbesti to xi(t) as shown in Equation 5. pbesti={ xi(t)   if fitness(xi(t))<fitness(pbesti)
    pbesti                otherwise                (5) 4.9.2 Update global best (gbest):
    If the fitness of pbesti is better than the fitness of gbest, update gbest to
    pbesti as shown in Equation 6. pbesti={ pbesti   if fitness(pbesti)<fitness(gbest)
    gbest                  otherwise                (6) These updates ensure that
    the particles converge toward optimal solutions in the search space over iterations,
    leveraging both personal and swarm-wide historical information. 4.9.3 Update the
    velocity and position matrix In particle swarm optimization (PSO), the evolution
    of particles is governed by updating their positions and velocities based on the
    best historical positions, both individual (pbest) and global (gbest). Let''s
    denote xi as the position vector of particle i, vi as the velocity vector of particle
    i, pbesti as the best historical position of particle i, and gbest as the best
    position among all particles in the entire swarm. The equations for updating the
    velocity and position of each particle in a basic form of PSO are as follows:
    4.9.4 Velocity update (vi) vi(t+1)=w⋅vi(t)+c1⋅rand1⋅(pbesti−xi(t))+c2⋅rand2⋅ (gbest−xi(t))     (7)
    * w is the inertia weight, controlling the impact of the previous velocity. *
    c1 and c2 are the acceleration coefficients for personal and global influence,
    respectively. * rand1 and rand2 are random values between 0 and 1. 4.9.5 Position
    update (xi) xi(t+1)=xi(t)+vi(t+1)     (8) These Equations 7, 8 reflect the collaborative
    exploration of the solution space by particles. The first term in the velocity
    update equation represents the inertia of the particle, the second term is the
    attraction to the particle''s own best position (exploitation), and the third
    term is the attraction to the swarm''s best position (exploration). The velocity
    of each particle is altered proportionally throughout each iteration after the
    random creation of the beginning values for the velocity and position of the particles,
    as well as the computation of both (pbest) and (gbest). An example of how the
    update procedure for the velocity matrix should be implemented is shown in Algorithm
    7, which can be found here. algorithm 7 Algorithm 7. Update velocity and position
    matrix. The method, which includes changing the speed of the particles, tries
    to produce another type of particle from the different portions of the VMs that
    is more suited than the previous one. This new generation will be produced as
    a result of the procedure that involves updating the velocity of the particles.
    A comparison is made between the value of each individual included inside the
    particles and its value from the prior iteration of the procedure, which was indicated
    by the notation pbest. When both individuals in (pbest) and the particle have
    the same velocity, the velocity value for each person is reduced; otherwise, the
    velocity amount is increased. In a similar manner, a comparison is being made
    between each individual inside the particles and the values that they had during
    the last iteration of the process (gbest). When both persons in (gbest) and the
    particle have the same velocity, the value of the individual''s velocity is lowered;
    otherwise, the value of the individual''s velocity is raised. Therefore, the location
    of the virtual machines (VMs) that make up each particle is altered in accordance
    with the modified values of the velocity. Two virtual machines (VMs) with maximum
    velocity values are switched within each particle in the newly created population.
    The word “maximum” serves as a symbol for the GAPSO algorithm''s termination criteria,
    which is the condition that must be met before the method can be considered complete.
    When all of the termination conditions have been satisfied, the scheduling solution
    for the workflow application will be changed to the one that was devised during
    the most recent iteration of the process and will have the lowest fittness value
    among the population. If this is not the case, the values of (gbest) and (pbest)
    continue to iteratively change until the termination condition is met. Repeat
    all steps until a termination criterion is met (e.g., maximum iterations). Return
    the best chromosome in the population as the optimal task allocation solution.
    Algorithm 8 provides a visual representation of the whole GA-PSO method. algorithm
    8 Algorithm 8. The proposed algorithm. The above proposed algorithm can be mathematically
    expressed as shown below: Sets: W = (N, E) : Workflow represented as a directed
    graph with node set N and edge set E VM = {VM1, VM2, ..., VMm} : Set of m virtual
    machines Parameters: p ε N : Population size, a positive integer n ε N : Number
    of iterations, a positive integer Variables: X_i: Solution (chromosome) i, represented
    as a vector of task-to-VM assignments V_i: Velocity of particle i in PSO gbest:
    Global best solution Pbest_i: Personal best solution of particle i Initialization:
    X_i ← random_initialization(W, VM) for i = 1 to p n_half ← 0 Genetic Algorithm
    Phase: While n_half < n/2: For each pair of chromosomes X_j, X_i ← population:
    P1, P2 ← tournament_selection(population) offspring_X_j ← crossover(P1, P2) new_X_j
    ← mutation(offspring_X_j) n_half ← n_half + 1 Particle Swarm Optimization Phase:
    X_i ← random_initialization(W, VM), V_i ← random_vector() for i = 1 to p Calculate
    gbest, Pbest_i for each particle While n_half < n: For each particle j: V_j ←
    update_velocity(V_j, Pbest_j, gbest) X_j ← update_position(X_j, V_j) Recalculate
    gbest, Pbest_i if necessary Output: Return gbest Encoding Algorithm: The encoding
    scheme defines how tasks and resources are represented in the chromosomes (solutions)
    used by the hybrid GA-PSO algorithm. Here''s an approach: • Chromosome structure:
    Each chromosome is an array of N genes, where N is the number of tasks in the
    workflow. • Gene representation: Each gene represents the assigned resource for
    a specific task. This can be achieved via direct encoding in which the gene value
    directly corresponds to the resource ID (VM1, VM2, etc.). Decoding Algorithm:
    The decoding algorithm interprets the encoded chromosomes back into actual task
    allocations on fog nodes. Here''s the process: - Iterate over each chromosome.
    - For each gene i in the chromosome: - Based on encoding scheme: - Direct Encoding:
    Look up the resource ID from the gene value (i). - Assign the task i to the identified
    resource. Sample Example: Workflow W: 4 tasks (T1, T2, T3, T4). Fog Nodes: 3 nodes
    (VM1, VM2, VM3). Chromosome (Direct Encoding): (Chandra and Verma, 2023; Fanelli
    et al., 2023; Mortaheb and Jankowski, 2023) Decoding Process: T1 is assigned to
    VM2 (gene value 2). T2 is assigned to VM1 (gene value 1). T3 is assigned to VM3
    (gene value 3). T4 is assigned to VM2 (gene value 2). The effectiveness of the
    proposed GA-PSO method for task scheduling in fog computing is confirmed by extensive
    tests carried out utilizing the iFogSim simulator. The studies conducted a performance
    comparison between GA-PSO and well-known algorithms like as GA, PSO, and PWOA.
    This comparison was carried out across multiple situations, including diverse
    data inputs and fog node configurations. The primary findings validate the feasibility
    of GA-PSO in several aspects: Superior performance: GA-PSO shown superior performance
    compared to all other algorithms in terms of minimizing execution time, reaction
    time, and completion time for jobs. This shows its efficacy in identifying the
    most effective work assignments inside the fog environment. Effectiveness with
    heterogeneity: GA-PSO demonstrated exceptional performance even in situations
    where there was a wide range of job complexity and various capabilities of fog
    nodes. This illustrates its capacity to adapt to real-world circumstances when
    resources and workloads are not always homogeneous. Dynamic adaptation: the algorithm
    has an inherent mechanism for dynamic adaptation, enabling it to modify its behavior
    in response to changing variables in the fog environment, including changes in
    workload, network availability, and resource availability. This functional characteristic
    guarantees consistent and efficient operation even under changing circumstances.
    In summary, the experimental findings strongly support the notion that the GA-PSO
    algorithm, as described, is both theoretically robust and practically successful
    in real-world fog computing environments. The exceptional performance, versatility,
    and efficacy across many workloads establish its potential as a key tool for task
    scheduling in fog computing applications. 4.10 Performance evaluation and results
    In order to test the proposed algorithm, the ifogsim (Gen and Lin, 2023a) simulator
    was used to implement the proposed algorithm. In order to determine how well the
    proposed hybrid GA-PSO algorithm performs, two experiments have been conducted
    and its obtained results have been compared with those of other task scheduling
    algorithms already in use, such as the GA proposed in Reddy et al. (2020), the
    PSO algorithm proposed in Jabour and Al-Libawy (2021) and PWOA algorithm (Bansal
    and Aggarwal, 2023). In order to assess the efficiency of the Hybrid GA-PSO algorithm
    that was proposed, this action was taken. The experimental parameters are clearly
    outlined in Table 1. For the first experiment, we came up with five scenarios
    in which the number of tasks was incremented by 100 in each scenario. The number
    of fog nodes remained constant at 55 throughout the entire experiment, and the
    obtained results are presented in Table 4. In the same way, we came up with five
    scenarios for experiment number two, with the number of fog nodes varying by 15
    and the number of tasks remaining constant at 200 throughout the entire experiment.
    The outcomes are detailed in Table 5. These results are in depth discussed in
    results and discussion section. table 1 Table 1. Experiment parameters. 4.11 Simulation
    setup In order to compare the proposed method to existing algorithms regarding
    the scheduling issue of workflows, we performed detailed tests on an efficient
    car parking real-world workflow application using the simulation settings (Gupta
    et al., 2017) listed in Tables 2, 3. The simulation parameters included the number
    of data inputs and the characteristics of fog nodes. In determining the attributes
    of the fog node and workflow application employed in the study, these factors
    were considered. table 2 Table 2. Task characteristics. table 3 Table 3. Fog node
    characteristics. We conceived of a scenario in which parking spaces are captured
    on film by intelligent, high-definition cameras. The images are subsequently transmitted
    to the fog node. The fog node evaluates the state of the parking space by analyzing
    the images and presents visual representations of the parking space through a
    Wi-Fi-enabled smart LED that is affixed to the fog node (Gupta et al., 2017).
    Through the use of a proxy server, communication is established between the fog
    nodes and the cloud server. We established variables for parking spaces and the
    quantity of cameras within the simulation. As part of our experimental setup,
    five parking lots were established. Initially, between one hundred and five hundred
    cameras were installed in each parking lot for the purpose of capturing images
    of the parking area. It is essential to note that a minimum of one fog node was
    generated for each distinct region. Subsequently, the number of fog nodes was
    expanded to facilitate the analysis of results obtained from various configurations.
    By virtue of their intelligent nature (equipped with WiFi capabilities) and microcontroller
    connectivity, we successfully implemented the cameras within the simulation environment
    and classified them as sensors in accordance with the guidelines outlined in Sheikh
    et al. (2023). We augmented the quantity of cameras in order to conduct an analysis
    of the data collected across diverse configurations and to assess the impacts
    on the fog node''s execution time, response time, and completion time. 5 Results
    and discussion 5.1 Experiment one (number of tasks trade off) In Table 4 we have
    presented the outcomes of hybrid GA-PSO algorithm, GA algorithm, Hybrid PWOA algorithm,
    and PSO algorithm in terms of execution time, response time, and completion time
    when various data input are given. Figure 3 indicates the proposed hybrid algorithm
    results improved the execution time by 85.68% when compared with GA algorithm,
    by 84% when compared with Hybrid PWOA and by 51.03% when compared with PSO algorithm.
    Figure 4 indicates the hybrid GA-PSO algorithm results improved the response time
    by 67.28% when compared with GA algorithm, by 54.24% when compared with Hybrid
    PWOA and by 75.40% when compared with PSO algorithm. Figure 5 indicates the hybrid
    GA-PSO algorithm results improved the completion time by 68.69% when compared
    with GA algorithm, by 98.91% when compared with Hybrid PWOA and by 75.90% when
    compared with PSO algorithm. table 4 Table 4. The result of experiment number
    one. figure 3 Figure 3. Comparison of Execution time (in second) of proposed Hybrid
    algorithm with conventional GA, Hybrid PWOA and PSO with different number of data
    input (cameras). figure 4 Figure 4. Comparison of Response time (in second) of
    proposed Hybrid algorithm with conventional GA, Hybrid PWOA and PSO with different
    number of data input (cameras). figure 5 Figure 5. Comparison of Completion time
    (in second) of proposed Hybrid algorithm with conventional GA, Hybrid PWOA and
    PSO with different number of data input (cameras). Therefore, GA-PSO is the most
    efficient algorithm in terms of execution time, response time and completion time,
    followed by PSO, Hybrid PWOA, and GA is the least efficient algorithm. 5.2 Experiment
    two (fog node trade off) In Table 5 we have presented the outcomes of the proposed
    hybrid algorithm, GA algorithm, Hybrid PWOA algorithm and PSO algorithm in terms
    of execution time, response time and completion time when various fog nodes are
    given. Figure 6 indicates the proposed hybrid algorithm results improved the execution
    time by 84.87% when compared with GA algorithm, by 88.64% when compared with Hybrid
    PWOA and by 85.07% when compared with PSO algorithm. Figure 7 indicates the proposed
    hybrid algorithm results improved the response time by 65.92% when compared with
    GA algorithm, by 80.51% when compared with Hybrid PWOA and by 85.26% when compared
    with PSO algorithm. Figure 8 indicates the proposed hybrid algorithm results improved
    the completion time by 67.60% when compared with GA algorithm, by 81.34% when
    compared with Hybrid PWOA and by 85.23% when compared with PSO algorithm. table
    5 Table 5. The result of experiments number two. figure 6 Figure 6. Comparison
    of Execution time (in second) of proposed Hybrid algorithm with conventional GA,
    Hybrid PWOA and PSO with different number of fog nodes. figure 7 Figure 7. Comparison
    of Response time (in second) of proposed Hybrid algorithm with conventional GA,
    Hybrid PWOA and PSO with different number of fog nodes. figure 8 Figure 8. Comparison
    of Response time (in second) of proposed Hybrid algorithm with conventional GA,
    Hybrid PWOA and PSO with different number of fog nodes. Therefore, GA-PSO is the
    most efficient algorithm in terms of execution time, response time and completion
    time, followed by PSO, Hybrid PWOA and GA is the least efficient algorithm. 6
    Conclusion In this study, both the problems of task scheduling in fog computing
    environment and the solution to it are discussed by proposing a multi objective
    hybrid GA-PSO optimization algorithm. The experimental result shows that the proposed
    Hybrid GA-PSO algorithm-based task allocation give optimal results when compared
    with the GA, Hybrid PWOA and PSO algorithm in terms of both heterogeneity of fog
    nodes and number of data input respectively. Furthermore, the Hybrid GA-PSO technique
    does not always get trapped in the locally optimal solution due to GA''s exploration
    skills with PSO''s exploitation characteristics which increases the accuracy of
    the solution. We conducted two experiments and compare both algorithm outputs
    in terms of execution, response and completion time. In experiment one we fix
    the number of fog nodes and change number of data inputs and compare the results.
    We found that as the number of data input increases proposed Hybrid GA-PSO algorithm
    improves the execution time, response time, and completion time when compared
    with Hybrid PWOA, GA and PSO algorithm. In experiment two this time we fix the
    number of data input and change no of fog nodes in every scenario and compare
    the results. We found that as the number of fog nodes increases proposed Hybrid
    GA-PSO algorithm improves the execution time, response time, and completion time
    when compared with Hybrid PWOA, GA, and PSO algorithm. By combining the Genetic
    algorithm and particle swarm optimization algorithm, the performance of the multi
    objective task scheduling in fog computing environment is improved. In future,
    researchers can push the boundaries of hybrid optimization and develop even more
    powerful and versatile algorithms for solving complex real-world problems. The
    issue of uncertainty in allocating tasks to fog nodes must be resolved. Enhancing
    security and privacy protocols to handle sensitive large data in fog situations
    may need more refinement. Data availability statement The original contributions
    presented in the study are included in the article/supplementary material, further
    inquiries can be directed to the corresponding author. Author contributions MS:
    Writing – original draft, Writing – review & editing. RE: Formal analysis, Writing
    – review & editing. RQ: Formal analysis, Writing – review & editing. Funding The
    author(s) declare that no financial support was received for the research, authorship,
    and/or publication of this article. Conflict of interest The authors declare that
    the research was conducted in the absence of any commercial or financial relationships
    that could be construed as a potential conflict of interest. Publisher''s note
    All claims expressed in this article are solely those of the authors and do not
    necessarily represent those of their affiliated organizations, or those of the
    publisher, the editors and the reviewers. Any product that may be evaluated in
    this article, or claim that may be made by its manufacturer, is not guaranteed
    or endorsed by the publisher. References Bansal, S., and Aggarwal, H. (2023).
    A hybrid particle whale optimization algorithm with application to workflow scheduling
    in cloud–fog environment. Decis. Analyt. J. 9:100361. doi: 10.1016/j.dajour.2023.100361
    Crossref Full Text | Google Scholar Bebortta, S., Tripathy, S. S., Modibbo, U.
    M., and Ali, I. (2023). An optimal fog-cloud offloading framework for big data
    optimization in heterogeneous IoT networks. Decis. Analyt. J. 8:100295. doi: 10.1016/j.dajour.2023.100295
    Crossref Full Text | Google Scholar Bharany, S., et al. (2023). “A Comprehensive
    Review on Big Data Challenges,” in 2023 International Conference on Business Analytics
    for Technology and Security (ICBATS) (IEEE). doi: 10.1109/ICBATS57792.2023.10111375
    Crossref Full Text | Google Scholar Chandra, S., and Verma, S. (2023). Big data
    and sustainable consumption: a review and research agenda. Vision 27, 11–23. doi:
    10.1177/09722629211022520 Crossref Full Text | Google Scholar Dai, Z., Ding, W.,
    Min, Q., Gu, C., Yao, B., and Shen, X. (2023). ME-AWA: a novel task scheduling
    approach based on weight vector adaptive updating for fog computing. Processes
    11:1053. doi: 10.3390/pr11041053 Crossref Full Text | Google Scholar Das, R.,
    and Inuwa, M. M. (2023). A review on fog computing: issues, characteristics, challenges,
    and potential applications. Telem. Inform. Rep. 10:100049. doi: 10.1016/j.teler.2023.100049
    Crossref Full Text | Google Scholar Fanelli, S., Pratici, L., Salvatore, F. P.,
    Donelli, C. C., and Zangrandi, A. (2023). Big data analysis for decision-making
    processes: challenges and opportunities for the management of health-care organizations.
    Manag. Res. Rev. 46, 369–389. doi: 10.1108/MRR-09-2021-0648 Crossref Full Text
    | Google Scholar Fazel, E., Shayan, A., and Mahmoudi Maymand, M. (2023). Designing
    a model for the usability of fog computing on the internet of things. J. Amb.
    Intell. Human. Comput. 14, 5193–5209. doi: 10.1007/s12652-021-03501-5 Crossref
    Full Text | Google Scholar Gen, M., and Cheng, R. (2000). Genetic Algorithms and
    Engineering Optimization (Vol. 7). London: John Wiley and Sons. Google Scholar
    Gen, M., Cheng, R., and Lin, L. (2008). “Tasks scheduling models,” in Network
    Models and Optimization: Multiobjective Genetic Algorithm Approach (London: Springer),
    551–606. Google Scholar Gen, M., and Lin, L. (2023a). “Genetic algorithms and
    their applications,” in Springer handbook of engineering statistics (London: Springer),
    635–674. doi: 10.1007/978-1-4471-7503-2_33 Crossref Full Text | Google Scholar
    Gen, M., and Lin, L. (2023b). “Nature-inspired and evolutionary techniques for
    automation,” in Springer Handbook of Automation (Cham: Springer International
    Publishing), 483–508. doi: 10.1007/978-3-030-96729-1_21 Crossref Full Text | Google
    Scholar Gen, M., and Yoo, M. (2008). “Real time tasks scheduling using hybrid
    genetic algorithm,” in Computational Intelligence in Multimedia Processing: Recent
    Advances (Berlin, Heidelberg: Springer), 319–350. doi: 10.1007/978-3-540-76827-2_13
    Crossref Full Text | Google Scholar Gupta, H., Vahid Dastjerdi, A., Ghosh, S.
    K., and Buyya, R. (2017). iFogSim: a toolkit for modeling and simulation of resource
    management techniques in the Internet of Things, Edge and Fog computing environments.
    Software 47, 1275–1296. doi: 10.1002/spe.2509 Crossref Full Text | Google Scholar
    He, J., and Bai, W. (2023). “Computation offloading and task scheduling based
    on improved integer particle swarm optimization in fog computing,” in 2023 3rd
    International Conference on Neural Networks, Information and Communication Engineering
    (NNICE) (IEEE), 633–638. doi: 10.1109/NNICE58320.2023.10105675 Crossref Full Text
    | Google Scholar Hornik, J., Rachamim, M., and Graguer, S. (2023). Fog computing:
    a platform for big-data marketing analytics. Front. Artif. Intell. 6. doi: 10.3389/frai.2023.1242574
    PubMed Abstract | Crossref Full Text | Google Scholar Hoseiny, F., Azizi, S.,
    Shojafar, M., Ahmadiazar, F., and Tafazolli, R. (2021). “PGA: a priority-aware
    genetic algorithm for task scheduling in heterogeneous fog-cloud computing,” in
    IEEE INFOCOM 2021-IEEE Conference on Computer Communications Workshops (INFOCOM
    WKSHPS) (IEEE), 1–6. doi: 10.1109/INFOCOMWKSHPS51825.2021.9484436 Crossref Full
    Text | Google Scholar Hosseinzadeh, M., Azhir, E., Lansky, J., Mildeova, S., Ahmed,
    O. H., Malik, M. H., et al. (2023). Task scheduling mechanisms for fog computing:
    a systematic survey. IEEE Access 11, 50994–51017. doi: 10.1109/ACCESS.2023.3277826
    Crossref Full Text | Google Scholar Hussein, W. N., Hussain, H. N., Hussain, H.
    N., and Mallah, A. Q. (2023). A deployment model for IoT devices based on fog
    computing for data management and analysis. Wirel. Pers. Commun. 16, 1–13. doi:
    10.1007/s11277-023-10168-y Crossref Full Text | Google Scholar Jabour, I. M.,
    and Al-Libawy, H. (2021). “An optimized approach for efficient-power and low-latency
    fog environment based on the PSO algorithm,” in 2021 2nd Information Technology
    To Enhance e-learning and Other Application (IT-ELA) (IEEE), 52–57. doi: 10.1109/IT-ELA52201.2021.9773443
    Crossref Full Text | Google Scholar Jakwa, A. G., Gital, A. Y., Boukari, S., and
    Zambuk, F. U. (2023). Performance evaluation of hybrid meta-heuristics-based task
    scheduling algorithm for energy efficiency in fog computing. Int. J. Cloud Applic.
    Comput. 13, 1–16. doi: 10.4018/IJCAC.324758 Crossref Full Text | Google Scholar
    Khang, A., Gupta, S. K., Rani, S., and Karras, D. A. (2023). Smart Cities: IoT
    Technologies, Big Data Solutions, Cloud Platforms, and Cybersecurity Techniques.
    New York: CRC Press. doi: 10.1201/9781003376064 Crossref Full Text | Google Scholar
    Khiat, A., Haddadi, M., and Bahnes, N. (2024). Genetic-based algorithm for task
    scheduling in fog–cloud environment. J. Netw. Syst. Manage. 32:3. doi: 10.1007/s10922-023-09774-9
    Crossref Full Text | Google Scholar Kim, Y., Jang, S., and Kim, K. B. (2023).
    Impact of urban microclimate on walking volume by street type and heat-vulnerable
    age groups: Seoul''s IoT sensor big data. Urban Clim. 51:101658. doi: 10.1016/j.uclim.2023.101658
    Crossref Full Text | Google Scholar Kumar, M., Kishor, A., Abawajy, J., Agarwal,
    P., Singh, A., and Zomaya, A. Y. (2021). ARPS: An autonomic resource provisioning
    and scheduling framework for cloud platforms. IEEE Trans. Sustain. Comput. 7,
    386–399. doi: 10.1109/TSUSC.2021.3110245 Crossref Full Text | Google Scholar Kumar,
    M., Kishor, A., Samariya, J. K., and Zomaya, A. Y. (2023b). An autonomic workload
    prediction and resource allocation framework for fog enabled industrial IoT. IEEE
    Inter. Things J. 10, 9513–9522. doi: 10.1109/JIOT.2023.3235107 Crossref Full Text
    | Google Scholar Kumar, M., Sharma, S. C., Goel, A., and Singh, S. P. (2019).
    A comprehensive survey for scheduling techniques in cloud computing. J. Netw.
    Comput. Applic. 143, 1–33. doi: 10.1016/j.jnca.2019.06.006 Crossref Full Text
    | Google Scholar Kumar, M., Walia, G. K., Shingare, H., Singh, S., and Gill, S.
    S. (2023a). “Ai-based sustainable and intelligent offloading framework for iiot
    in collaborative cloud-fog environments,” in IEEE Transactions on Consumer Electronics.
    doi: 10.1109/TCE.2023.3320673 Crossref Full Text | Google Scholar Kumar, M. S.,
    and Karri, G. R. (2023). Eeoa: cost and energy efficient task scheduling in a
    cloud-fog framework. Sensors 23:2445. doi: 10.3390/s23052445 PubMed Abstract |
    Crossref Full Text | Google Scholar Mohamed, A. A., Abualigah, L., Alburaikan,
    A., and Khalifa, H. A. E. W. (2023). AOEHO: a new hybrid data replication method
    in fog computing for IoT application. Sensors 23:2189. doi: 10.3390/s23042189
    PubMed Abstract | Crossref Full Text | Google Scholar Mohammadzadeh, A., Akbari
    Zarkesh, M., Haji Shahmohamd, P., Akhavan, J., and Chhabra, A. (2023). Energy-aware
    workflow scheduling in fog computing using a hybrid chaotic algorithm. J. Supercomput.
    79, 18569–18604. doi: 10.1007/s11227-023-05330-z Crossref Full Text | Google Scholar
    Mohanty, H. (2015). “Big data: an introduction,” in Big Data. Studies in Big Data,
    eds. H. Mohanty, P. Bhuyan, D. Chenthati (New Delhi: Springer). doi: 10.1007/978-81-322-2494-5
    Crossref Full Text | Google Scholar Mortaheb, R., and Jankowski, P. (2023). Smart
    city re-imagined: city planning and GeoAI in the age of big data. J. Urban Manage.
    12, 4–15. doi: 10.1016/j.jum.2022.08.001 Crossref Full Text | Google Scholar Olawoyin,
    A. M., Leung, C. K., Hryhoruk, C. C., and Cuzzocrea, A. (2023). “Big data management
    for machine learning from big data,” in International Conference on Advanced Information
    Networking and Applications (Cham: Springer International Publishing), 393–405.
    doi: 10.1007/978-3-031-29056-5_35 Crossref Full Text | Google Scholar Qi, Q.,
    Xu, Z., and Rani, P. (2023). Big data analytics challenges to implementing the
    intelligent Industrial Internet of Things (IIoT) systems in sustainable manufacturing
    operations. Technol. Forecast. Soc. Change 190:122401. doi: 10.1016/j.techfore.2023.122401
    Crossref Full Text | Google Scholar Raj, N. (2023). “Analysis of fog computing:
    an integrated internet of things (IoT) fog cloud infrastructure for big data analytics
    and cyber security,” in 2023 International Conference on Artificial Intelligence
    and Smart Communication (AISC) (IEEE), 1215–1219. doi: 10.1109/AISC56616.2023.10085681
    Crossref Full Text | Google Scholar Reddy, K. H. K., Luhach, A. K., Pradhan, B.,
    Dash, J. K., and Roy, D. S. (2020). A genetic algorithm for energy efficient fog
    layer resource management in context-aware smart cities. Sustain. Cities Soc.
    63:102428. doi: 10.1016/j.scs.2020.102428 Crossref Full Text | Google Scholar
    Rosati, R., Romeo, L., Cecchini, G., Tonetto, F., Viti, P., Mancini, A., et al.
    (2023). From knowledge-based to big data analytic model: a novel IoT and machine
    learning based decision support system for predictive maintenance in Industry
    4.0. J. Intell. Manufact. 34, 107–121. doi: 10.1007/s10845-022-01960-x Crossref
    Full Text | Google Scholar Saad, M., Qureshi, R. I., and Rehman, A. U. (2023).
    “Task scheduling in fog computing: parameters, simulators and open challenges,”
    in 2023 Global Conference on Wireless and Optical Technologies (GCWOT) (IEEE),
    1–6. doi: 10.1109/GCWOT57803.2023.10064652 Crossref Full Text | Google Scholar
    Saif, F. A., Latip, R., Hanapi, Z. M., and Shafinah, K. (2023). Multi-objective
    grey wolf optimizer algorithm for task scheduling in cloud-fog computing. IEEE
    Access 11, 20635–20646. doi: 10.1109/ACCESS.2023.3241240 Crossref Full Text |
    Google Scholar Shami, T. M., Mirjalili, S., Al-Eryani, Y., Daoudi, K., Izadi,
    S., and Abualigah, L. (2023). Velocity pausing particle swarm optimization: A
    novel variant for global optimization. Neural Comput. Applic. 35, 9193–9223. doi:
    10.1007/s00521-022-08179-0 Crossref Full Text | Google Scholar Sheikh, M. S.,
    Enam, R. N., and Qureshi, R. I. (2023). Machine learning-driven task scheduling
    with dynamic K-means based clustering algorithm using fuzzy logic in FOG environment.
    Front. Comput. Sci. 5:1293209. doi: 10.3389/fcomp.2023.1293209 Crossref Full Text
    | Google Scholar Sheng, X., Lin, X., Deng, L., Wu, Z., Chen, J., Liu, B., et al.
    (2023). RETRACTED: Big data Markov chain model based dynamic evolution of regional
    gap—Take China''s highway resources as an example. Int. J. Electr. Eng. Educ.
    60, 1146–1161. doi: 10.1177/0020720919884237 Crossref Full Text | Google Scholar
    Tran-Dang, H., and Kim, D. S. (2023). Dynamic collaborative task offloading for
    delay minimization in the heterogeneous fog computing systems. J. Commun. Netw.
    25, 244–252. doi: 10.23919/JCN.2023.000008 Crossref Full Text | Google Scholar
    Vispute, S. D., and Vashisht, P. (2023). Energy-efficient task scheduling in fog
    computing based on particle swarm optimization. SN Comput. Sci. 4:391. doi: 10.1007/s42979-022-01639-3
    Crossref Full Text | Google Scholar Walia, G. K., Kumar, M., and Gill, S. S. (2023).
    “AI-empowered fog/edge resource management for IoT applications: A comprehensive
    review, research challenges and future perspectives,” in IEEE Communications Surveys
    and Tutorials. doi: 10.1109/COMST.2023.3338015 Crossref Full Text | Google Scholar
    Wang, C., and Yin, L. (2023). Defining urban big data in urban planning: literature
    review. J. Urban Plann. Dev. 149:04022044. doi: 10.1061/(ASCE)UP.1943-5444.0000896
    Crossref Full Text | Google Scholar Xu, M., Mei, Y., Zhu, S., Zhang, B., Xiang,
    T., Zhang, F., et al. (2023). Genetic programming for dynamic workflow scheduling
    in fog computing. IEEE Trans. Serv. Comput. 16, 2657–2671. doi: 10.1109/TSC.2023.3249160
    Crossref Full Text | Google Scholar Yoo, M., and Gen, M. (2007). Scheduling algorithm
    for real-time tasks using multiobjective hybrid genetic algorithm in heterogeneous
    multiprocessors system. Comput. Oper. Res. 34, 3084–3098. doi: 10.1016/j.cor.2005.11.016
    Crossref Full Text | Google Scholar Zhang, W., Li, C., Gen, M., Yang, W., Zhang,
    Z., and Zhang, G. (2022). Multiobjective particle swarm optimization with direction
    search and differential evolution for distributed flow-shop scheduling problem.
    Math. Biosci. Eng 19, 8833–8865. doi: 10.3934/mbe.2022410 PubMed Abstract | Crossref
    Full Text | Google Scholar Keywords: fog computing, task scheduling, genetic algorithm,
    particle swarm optimization, hybrid algorithm, hybrid GA-PSO, fog computing (FC),
    cloud computing Citation: Saad M, Enam RN and Qureshi R (2024) Optimizing multi-objective
    task scheduling in fog computing with GA-PSO algorithm for big data application.
    Front. Big Data 7:1358486. doi: 10.3389/fdata.2024.1358486 Received: 21 December
    2023; Accepted: 06 February 2024; Published: 21 February 2024. Edited by: Carson
    Leung, University of Manitoba, Canada Reviewed by: Mitsuo Gen, Fuzzy Logic Systems
    Institute, Japan Mohit Kumar, Dr B. R. Ambedkar National Institute of Technology
    Jalandhar, India Copyright © 2024 Saad, Enam and Qureshi. This is an open-access
    article distributed under the terms of the Creative Commons Attribution License
    (CC BY). The use, distribution or reproduction in other forums is permitted, provided
    the original author(s) and the copyright owner(s) are credited and that the original
    publication in this journal is cited, in accordance with accepted academic practice.
    No use, distribution or reproduction is permitted which does not comply with these
    terms. *Correspondence: Muhammad Saad, msaad@ssuet.edu.pk Disclaimer: All claims
    expressed in this article are solely those of the authors and do not necessarily
    represent those of their affiliated organizations, or those of the publisher,
    the editors and the reviewers. Any product that may be evaluated in this article
    or claim that may be made by its manufacturer is not guaranteed or endorsed by
    the publisher. Download Footer Guidelines Author guidelines Editor guidelines
    Policies and publication ethics Fee policy Explore Articles Research Topics Journals
    Outreach Frontiers Forum Frontiers Policy Labs Frontiers for Young Minds Connect
    Help center Emails and alerts Contact us Submit Career opportunities Follow us
    © 2024 Frontiers Media S.A. All rights reserved Privacy policy | Terms and conditions
    We use cookies Our website uses cookies that are necessary for its operation and
    other cookies to track its performance or to improve and personalize our services.
    To manage or reject non-essential cookies, please click \"Cookies Settings\".
    For more information on how we use cookies, please see ourCookie Policy Cookies
    Settings Accept Cookies"'
  inline_citation: '>'
  journal: Frontiers in Big Data
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Optimizing multi-objective task scheduling in fog computing with GA-PSO algorithm
    for big data application
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Karnehm D.
  - Samanta A.
  - Neve A.
  - Williamson S.
  citation_count: '0'
  description: 'It is needless to mention that the proven capability of cloud computing
    and digital twin-based monitoring and control systems will play a major role in
    the implementation of digital twin-based lithium-ion battery management systems
    (BMS) for e-transportation. In this context, the combination of the Internet of
    Things (IoT) and fog computing emerges as an enabling technology that facilitates
    bidirectional data communication and processing. This paper proposes a five-layer
    IoT framework for the implementation of cloud-based battery monitoring and BMS.
    Additionally, it critically analyzes the existing IoT frameworks applied to BMS
    and justifies the suitability of the five-layer architecture. Furthermore, the
    advantages of fog computing, such as low latency, fault tolerance, limitless systems,
    multiple applications, and shared responsibility, are discussed. The paper also
    explores four previously proposed computing-enabled architectures: centralized,
    hybrid, distributed, and hybrid-distributed computing. Moreover, the potential
    of utilizing hybrid computing architectures, incorporating fog computing as a
    supplement to cloud computing, is highlighted to address fault, privacy, or latency
    issues in cloud-based BMS.'
  doi: 10.1109/SGRE59715.2024.10428786
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2024 4th International Confer... Five-layer
    IoT and Fog Computing Framework Towards Digital Twinning of Battery Management
    Systems for e-Transportation Publisher: IEEE Cite This PDF Dominic Karnehm; Akash
    Samanta; Antje Neve; Sheldon Williamson All Authors 10 Full Text Views Abstract
    Document Sections I. Introduction II. Internet of Things in BMS III. Fog Computing
    for BMS IV. Experimental Validation V. Potentials and Challenges Show Full Outline
    Authors Figures References Keywords Metrics Abstract: It is needless to mention
    that the proven capability of cloud computing and digital twin-based monitoring
    and control systems will play a major role in the implementation of digital twin-based
    lithium-ion battery management systems (BMS) for e-transportation. In this context,
    the combination of the Internet of Things (IoT) and fog computing emerges as an
    enabling technology that facilitates bidirectional data communication and processing.
    This paper proposes a five-layer IoT framework for the implementation of cloud-based
    battery monitoring and BMS. Additionally, it critically analyzes the existing
    IoT frameworks applied to BMS and justifies the suitability of the five-layer
    architecture. Furthermore, the advantages of fog computing, such as low latency,
    fault tolerance, limitless systems, multiple applications, and shared responsibility,
    are discussed. The paper also explores four previously proposed computing-enabled
    architectures: centralized, hybrid, distributed, and hybrid-distributed computing.
    Moreover, the potential of utilizing hybrid computing architectures, incorporating
    fog computing as a supplement to cloud computing, is highlighted to address fault,
    privacy, or latency issues in cloud-based BMS. Published in: 2024 4th International
    Conference on Smart Grid and Renewable Energy (SGRE) Date of Conference: 08-10
    January 2024 Date Added to IEEE Xplore: 15 February 2024 ISBN Information: DOI:
    10.1109/SGRE59715.2024.10428786 Publisher: IEEE Conference Location: Doha, Qatar
    SECTION I. Introduction Effective battery management system (BMS) plays a vital
    role in enhancing battery performance, ensuring safety, and prolonging cell life
    [1]. In addition to the battery pack control, the estimation of battery states
    is an important part of modern BMS [2]. These states are, such as state of charge
    (SOC), state of health (SOH), and the core temperature. Over the years, data-driven
    methods and digital twins have been proposed to estimate battery states [3], [4].
    NASA defined a digital twin as a simulation of the physical object based on historical
    data and current sensor measurements, to estimate and forecast the state of the
    physical object [5]. In addition to the digital twin, the digital model and the
    digital shadow must also be defined for clear definition and differentiation [6].
    A digital model is defined as a static representation of the physical object.
    A digital shadow includes a one-way data flow from the physical object to the
    digital representation of the object to update the state of the shadow. Such a
    digital shadow can be used to monitor the physical object. A digital twin contains
    a fully interconnected automated data flow with the physical object. This enables
    automated control of the physical object based on the knowledge created by the
    digital twin. Assume that a BMS using a digital twin for state estimation and
    control requires bidirectional data communication between the physical object
    and the digital representation. Furthermore, larger data storage and higher processing
    power compared to the classical approach of BMS. Especially to enable highly accurate
    machine learning methods as digital representations to estimate the state of the
    battery cell and the battery pack. To use such proposed methods, the problem of
    necessarily increasing computational resources and access through historical data,
    adopting the models, can be faced from two sites. On the one hand, the installed
    computing power and storage on the device can be increased. On the other hand,
    the scalability and flexibility of cloud computing technologies can be utilized.
    This paper is concerned with the second method of using the possibilities of cloud
    computing and its technologies. For this purpose, Internet of things (IoT) and
    fog computing are some of the primary enabling technologies for cloud computing.
    A. Internet of Things Internet of things (IoT) connects physical devices to the
    internet. Connected devices share data measured by sensors and data through operation
    tasks [8]. These data are sent to cloud or on-premise nodes to analyze and operate
    the data. Depending on the specific application, different connectivity, networking,
    communication technologies, and protocols are used [9]. There are a wide variety
    of networking technologies, such as 5G, LTE, LoRa, ZigBee, Wi-Fi, Bluetooth 5,
    and other cellular networks. The different cellular networks can be utilized in
    different areas of application depending on the required bandwidth, data range,
    and power usage [7]. A comparison of these technologies is shown in Table I. In
    the case of public e-transportation, primary LTE and 5G as the next generation
    of broadband cellular networks must focus on research and development. This is
    because existing infrastructures can be used and the available bandwidth. Other
    technologies, such as Wi-Fi, LoRa, and ZigBee, may be suitable for localized applications,
    such as buses in amusement parks and cargo vehicles at factory sites. In addition
    to communication technologies, another important decision to make during the conception
    of a IoT application is the choice of the application layer protocol. Multiple
    application layer protocols exist, such as HTTP REST, MQTT, AMQP, CoAP, XMPP,
    DDS, and WebSocket. Table II shows a comparison between the protocols listed.
    The table distinguishes between the two transmission patterns, publish/subscriber,
    and request/responce. The publisher/subscriber pattern is a messaging pattern
    based on the principle of loose coupling, where publishers generate messages and
    send them to a message broker, which distributes them to interested subscribers.
    The process of the request/response pattern is that a client sends a request to
    a server and waits for the corresponding response. The server processes the request
    and sends back the requested information as a response. The MQTT and HTTP REST
    protocols are the main protocols used today in the field of IoT. Not only the
    analysis conducted by Bayilmis et al. [10] shows this by the use in research studies
    nowadays. Furthermore, the fact that cloud service providers, such as Amazon Web
    Services (AWS), Google Clouds, and Microsoft Azure, supply mainly these protocols
    highlights the importance of these two protocols [11–13]. B. Fog Computing Fog
    Computing is a computational paradigm that acts as a bridge between the device
    and the cloud [14], [15]. The fog is highly distributed and highly integrated
    with the cloud to bring the possibilities of cloud services close to the device
    and reduce the communication latency between the device and the cloud services.
    In Fig. 1 the schema of cloud computing and fog computing is shown. In Fig. 1a
    the devices are connected directly to the cloud. This increases the complexity
    of such systems. Furthermore, Fig. 1b shows the layer structure including the
    fog layer. The distributed fog nodes facilitate local buffering of data and execution
    of time-critical calculations. Habibi [16] clarifies that there is no consensus
    definition of fog computing, and some authors use it as a synonym of edge computing
    and others as an extension of cloud computing. In this paper, the concept of fog
    computing is defined as the delivery of services and storage on the network edge,
    with the objective of reducing latency and improving Quality of Service (QoS)
    [17]. Fig. 1. Cloud computing and Fog computing models Show All SECTION II. Internet
    of Things in BMS Sivaraman et al. [18] applied the three-layer IoT framework proposed
    by the IEEE Internet Initiative [19] through the application of BMS. The concept
    behind proposing such a general framework is to generalize the structure, define
    the required levels and connections between different components, and set the
    data flow and dependencies between them. Fig. 2a shows the framework proposed
    by Sivaraman et al. [18]. The first layer, the perception layer includes the physical
    components, thus the sensors and the battery pack. The network layer connects
    the BMS with cloud services. It is also used to process and transfer. Specific
    applications, including storage, are provided by the application layer. As the
    arrows in the figure indicate, a two-way data flow is not scheduled in this framework.
    Only data exchange from the device to the cloud is foreseen. Furthermore, the
    cloud service provider AWS introduced at the end of 2021 a service called AWS
    IoT FleetWise [20]. The architectural layers of the platform are shown as proposed
    in Fig. 2b [21]. The approach of this service is to provide a highly scalable
    platform for collecting, transforming, and transferring vehicle data to the cloud
    in close to real-time. These data can be used to analyze the state of the fleet
    in a highly automated manner. By gathering information for crucial use cases from
    customer fleets, Continental expects to save up to 50% on field operation test
    (FoT) costs, as well as lower the cost of routine software upgrades by up to 25%
    and provide software updates to customers faster [21]. TABLE I Comparison of Wireless
    Communication Technologies and their Parameters [7] TABLE II Comparison of Different
    Application Layer Protocols [10] These proposed frameworks cannot provide input
    to control the battery pack. Toward the cloud-based BMS and a real digital twin
    of the battery pack as defined previously, it is necessary to enable a bidirectional
    data flow between the physical device and the digital representation. Therefore,
    in Fig. 3 a IoT framework based on the five-layer framework by Guth et al. [22]
    applied through BMS is proposed, which enables a fully automated bidirectional
    data flow. Also, this framework includes a gateway. This is responsible for the
    communication technologies and protocols between the local device and the cloud
    and also converts the data into the necessary format, such as between binary Controller
    Area Network (CAN) and JavaScript Object Notation (JSON). CAN is a standard that
    is extensively utilized in the automotive sector to enable communication between
    microcontrollers and devices. On the other hand, JSON is a format that uses text
    to represent structured data and is widely used for data transfer in web applications.
    It offers a lightweight solution for data storage and transmission and is frequently
    employed to transport data between servers and clients over the Internet. SECTION
    III. Fog Computing for BMS Beside the IoT framework, also cloud computing in combination
    with fog computing will also have a huge impact in cloud-based BMS developments.
    To decrease the delay between the physical BMS, and its digital twin, fog computing
    will be an essential component. Yang et al. [23] proposes a framework for cloud-based
    BMS that includes four subsystems: end, edge, cloud, and knowledge. To enable
    time critical functionalities, the local edge device is used. Such an edge device
    reduces the communication between the device and the cloud. The cloud component
    can be used to run not as time critical functions such as SOH estimation, adaptive
    control, fallout prediction, and early warning of safety. The study by Yang et
    al. expected, a research gap can be identified in the field of edge and fog computing
    for cloud-based BMS. The results in the field of industrial internet of things
    (IIoT), and smart cities can be partly adopted for the field of e-transportation.
    Due to the assumed infrastructure in both fields, issues like rural areas, scaling
    for mobile devices around the globe, and network connection losses are only minor
    factors. Nevertheless, concepts and frameworks how to use fog computing in these
    fields can be analyzed and adopted for e-transportation and cloud-based BMS. Fig.
    2. Existing IoT frameworks for BMS and vehicles. The applied three-layer IoT framework
    (a) and the Aws IoT Fleetwise Connected vehicle platform (b) [18], [21] Show All
    Fig. 3. Five layer IoT framework proposed Guth [22] appplied for BMS Show All
    Basir et al. [24] highlight fog computing as a major component to enable more
    effective, efficient and manageable communication in the case of a large number
    of IoT devices. To allow low latency, high security, location awareness, and real-time
    connectivity for IIoT. With an increase in the number of connected vehicles to
    enable intelligent vehicles, the communication bandwidth to central data centers
    can become a bottleneck. This will increase the round-trip time in communication
    between vehicles and the cloud infrastructure [25]. Therefore, edge computing
    extends the computing and caching capabilities to network edges. Maheswaran et
    al. [26] describe five main challenges to overcome to achieve the goal of fog
    computing for autonomous driving. The expected data rate and computing power for
    autonomous driving is even higher compared to the requirements of online BMS,
    but the challenges are similar. The authors highlighted the following points:
    Low Latency: To enable the main control components in the fog, the latency between
    the fog component and the device is a primary object. Fault Tolerance: The challenge
    is to find mechanisms to ensure the accessibility of the vehicle service, even
    if a fog component fails. This is necessary to prevent the failure of local vehicle
    groups. Limitless System: The architecture of the fog must be designed in such
    a way that the architecture scales with the territorial size. So, the request
    time must be independent by the area covered and thus with the quantity of nodes.
    Multiple Applications: The combination of different applications, such as autonomous
    driving, optimal routing of individual vehicles, or battery management to increase
    vehicle range. Shared Responsibility: Actions to control the vehicle have local
    inputs directly from the device and the fog. Fig. 4. Centralized (a), hybrid (b),
    distributed (c), and hybrid-distributed (d) computing-enabled architectures [27]
    Show All To enable cloud-based BMS many smart city approaches can be applied,
    especially vehicle-to-everything (V2X). V2X is an emerging technology that improves
    the mobility and efficiency of urban traffic operations and driver safety. V2X
    includes vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), vehicle-to-network
    (V2N) and vehicle-to-pedestrian (V2P) communications [28]. In [27] the authors
    describe four different methods of data storage and computing architectures for
    connected vehicles proposed and discussed in the literature. Fig. 4 illustrates
    these four different architectures: centralized (4a), hybrid (4b), distributed
    (4c), and hybrid distributed (4d). They differ in the location of computing and
    storing: Centralized: The computing tasks are executed in distributed computing
    units, such as fog nodes and cloud computing nodes. The connected vehicle only
    generates computing tasks, uploads them and process the result. Local execution
    of computing tasks is not scheduled. Hybrid: The vehicle generates computing tasks
    and computes them locally or uploads them to distributed computing units and processes
    the result. Distributed: A cluster of connected vehicles shares computational
    and storage resources. Here, a vehicle generates a computing task, which is executed
    by one or more of the connected vehicles in the cluster. Hybrid-Distributed: A
    cluster of connected vehicles is also connected to distributed computing units,
    such as fog nodes or cloud computing nodes. Thus, a generated computing task can
    be executed locally, by other connected vehicles, or by unit of distributed computing
    units. In the case of centralized computing, as shown in Fig. 4a, the on-board
    BMS would be completely replaced by the cloud-based BMS, and all control would
    take place in fog, or cloud nodes. In the case of network loss, no control would
    be possible anymore. This would greatly increase the probability of errors due
    to faulty data packets, temporary or permanent network loss. For scenarios, like
    in fabric properties, where network coverage can be ensured and temporary network
    losses can be excluded or rapid recovery is ensured, this could be a chosen architecture.
    A distributed network scenario, as shown in Fig. 4c, would be the case if the
    cars shared computational power to calculate computationally expensive tasks.
    One scenario for this would be a highly accurate SOC, SOH, and core temperature
    estimation. Therefore, a cluster of vehicles could share data and computational
    power as a computing cluster. To enable such an architecture, multiple issues
    must be resolved, such as interoperability between different resellers, fault
    tolerance, privacy, and security. In addition, the control of computing and networking
    increases in this architecture. The same issues would arise with the hybrid-distributed
    architecture, as shown in Fig. 4d. In this architecture, the resources are shared
    between the vehicles in a cluster, also with fog nodes, and cloud computing resources.
    To share data, like in the concept of vehicle-to-vehicle (V2V), for automotive
    driving, such a concept could be helpful, especially for safety issues like accident
    warning or the early indication of a traffic jam, or swarm-based fleet charge
    control. Hybrid computing, as seen in Fig. 4d, enables computing on the vehicle
    and on fog notes. This gives the possibility to use adaptive resources of the
    fog to use computing and storage resources for not as time-dependent, or for computational
    intensive calculations. This architecture gives huge potential by using additional
    services in the fog and cloud, but the car is still not totally dependent on the
    network connection. Therefore, a connection failure will not result in an inoperable
    state of the vehicle. Fig. 5. Experimental test-bed for the real-time battery
    monitoring and state estimation in cloud Show All Fig. 6. Real-time visualiation
    of measured and estimated battery states in cloud Show All SECTION IV. Experimental
    Validation To validate the effectiveness of the proposed five-layer IoT framework
    for the BMS application, an experimental setup is developed for real-time battery
    monitoring and control. The components of the experimental setup conjugated to
    the layers of the proposed framework in Fig. 3 can be seen in Fig. 5. The experimental
    setup consists of a single cell with temperature, current, and voltage sensors,
    a BMS to control the discharge, and a Raspberry Pi with a CAN shield. Cumulatively,
    the experimental setup represents the physical device in this study. Apache Kafka,
    an open-source event-stream platform, is used as IoT integration middleware. The
    application level is capable of providing multiple services for a cloud-based
    BMS which includes SOC estimation, long-term storage for future analytics, and
    real-time monitoring and visualization of the system parameters. Real-time visualization
    of the measured and estimated battery parameters SOC can be seen in Fig. 6. SECTION
    V. Potentials and Challenges In the field of IoT and fog computing for BMS mainly
    for e-transportations applications multiple questions are already been towards
    implementing cloud-based BMS. Nevertheless, many challenges still need to be addressed.
    There is a lack of research on how to deal with loss of connectivity issues and
    the ways to distribute and manage fog computing nodes, especially in rural areas.
    The global urban region has a coverage of 97% with 4G connectivity. In rural areas,
    it covers only 76% of the population [29]. To the best of our knowledge, the details
    of the coverage of the road network have not been published. To enable smart cloud-based
    BMS, it is an open question how to enable fog computing in rural areas and how
    to handle areas without network connectivity. Fallback methods must be proposed
    to ensure vehicle usability in the case of network loss. In addition, control
    mechanisms must be developed at higher latencies due to the longer distances between
    the fog nodes and the electrical vehicles in rural areas. If it comes to real-world
    implementations, it remains to be seen by whom these fog nodes will be operated,
    by automakers like Tesla, BMW, and BYD. For example, additional hardware can be
    implemented with more than 45,000 superchargers worldwide operated by Tesla [30].
    Furthermore, company consortiums of different electric vehicle companies can also
    collaboratively install common systems to provide cloud computing service providers
    in collaboration with cloud service providers like AWS, Microsoft Azure, Google
    Cloud, and so on. In [27] different kinds of computing-enabled architectures were
    discussed. Among them the distributed and hybrid distributed architectures, as
    shown in Fig. 4c and 4d, are highlighted in this study. The concept of sharing
    computational resources over a cluster of vehicles is an interesting topic of
    research especially the ways to orchestrate computational tasks while ensuring
    privacy and security. In addition, handling the leaf of a single vehicle from
    a cluster by identifying the driving pattern would also be an interesting topic.
    For the implementation in commercial fleets, questions such as interoperability
    between car manufacturers, privacy, and benefits of such an architecture for customers,
    and manufacturers by reducing the total ownership cost by providing financial
    incentives can be explored. In the case of bus fleets or vehicles on a company
    campus, such an architecture could be useful to utilize existing capacities more
    efficiently. In a more applied manner, the hybrid architecture seen in Fig. 4b
    araises high opportunities. The potential to utilize additional computational
    and storage resources in addition to the hardware installed in the vehicle allows
    flexibility in the development of methods and services used for electrical vehicles.
    Furthermore, cloud computing is seen as an enabler for digital twinning-based
    BMS to improve on-board SOC estimation by adopting model parameters, or cloud-based
    SOH and core temperature estimation. The benefit of the scalable resources of
    the cloud can also be used on a limited level in the fog, but with the benefit
    of low latencies compared to cloud computing due to the closer physical location
    of the computing resources from the end user. In addition, the fog layer can be
    used as a data aggregator and the filter layer can be used to reduce the amount
    of data needed to transfer to the cloud. SECTION VI. Conclusion A detailed discussion
    on the three-layer, four-layer, and five-layer IoT framework, and the AWS IoT
    Fleetwise platform architectural layers towards developing digital twin enabled
    BMS is presented in this paper. The three-layer framework is only suitable for
    monitoring whereas the four-layer IoT framework is suitable for both monitoring
    and controlling functionality of a BMS. The five-layer framework includes Sensor
    and actor, device, gateway, IoT integration middleware, and application found
    to be the best suitable for the digital twinning of BMS for e-mobility applications.
    Furthermore, fog computing has been shown as an enabler for time-dependent cloud-based
    BMS functionalities. The scalability and nevertheless local alignment of fog computing
    in combination with cloud computing infrastructure for computationally intensive
    but less time-dependent tasks. A significant research and development potential
    is also noticed in the field of digital twinning and cloud-based BMS and the application
    of IoT and fog computing in addition to cloud computing. Authors Figures References
    Keywords Metrics More Like This Integration of Cloud Computing with Internet of
    Things for Network Management and Performance Monitoring 2020 18th International
    Conference on ICT and Knowledge Engineering (ICT&KE) Published: 2020 A Universal
    Complex Event Processing Mechanism Based on Edge Computing for Internet of Things
    Real-Time Monitoring IEEE Access Published: 2019 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 4th International Conference on Smart Grid and Renewable Energy, SGRE 2024
    - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Five-layer IoT and Fog Computing Framework Towards Digital Twinning of Battery
    Management Systems for e-Transportation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Amzil A.
  - Abid M.
  - Hanini M.
  - Zaaloul A.
  - El Kafhali S.
  citation_count: '0'
  description: In recent years, healthcare monitoring systems (HMS) have increasingly
    integrated the Internet of Medical Things (IoMT) with cloud computing, leading
    to challenges related to data latency and efficient processing. This paper addresses
    these issues by introducing a Machine Learning-based Medical Data Segmentation
    (ML-MDS) approach that employs a k-fold random forest technique for efficient
    health data classification and latency reduction in a fog-cloud environment. Our
    method significantly improves latency issues, enhancing the Quality of Service
    (QoS) in healthcare systems and demonstrating its adaptability in heterogeneous
    network scenarios. We specifically employ the Random Forest algorithm to mitigate
    the common problem of overfitting in machine learning models, ensuring broader
    applicability across various healthcare contexts. Additionally, by optimizing
    data processing in fog computing layers, we achieve a substantial reduction in
    overall latency between healthcare sensors and cloud servers. This improvement
    is evidenced through a comparative performance analysis with existing models.
    The proposed framework not only ensures secure and scalable management of IoMT
    health data but also incorporates a stochastic approach to mathematically formulate
    performance indicators for the HMS queuing model. This model effectively predicts
    system response times and assesses the computing resources required under varying
    workload conditions. Our simulation results show a classification accuracy of
    92%, a 56% reduction in latency compared to existing models, and an overall enhancement
    in e-healthcare service quality.
  doi: 10.1007/s10586-024-04285-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Cluster Computing Article Stochastic
    analysis of fog computing and machine learning for scalable low-latency healthcare
    monitoring Published: 21 February 2024 (2024) Cite this article Download PDF Access
    provided by University of Nebraska-Lincoln Cluster Computing Aims and scope Submit
    manuscript Abdellah Amzil, Mohamed Abid, Mohamed Hanini, Abdellah Zaaloul & Said
    El Kafhali  82 Accesses Explore all metrics Abstract In recent years, healthcare
    monitoring systems (HMS) have increasingly integrated the Internet of Medical
    Things (IoMT) with cloud computing, leading to challenges related to data latency
    and efficient processing. This paper addresses these issues by introducing a Machine
    Learning-based Medical Data Segmentation (ML-MDS) approach that employs a k-fold
    random forest technique for efficient health data classification and latency reduction
    in a fog-cloud environment. Our method significantly improves latency issues,
    enhancing the Quality of Service (QoS) in healthcare systems and demonstrating
    its adaptability in heterogeneous network scenarios. We specifically employ the
    Random Forest algorithm to mitigate the common problem of overfitting in machine
    learning models, ensuring broader applicability across various healthcare contexts.
    Additionally, by optimizing data processing in fog computing layers, we achieve
    a substantial reduction in overall latency between healthcare sensors and cloud
    servers. This improvement is evidenced through a comparative performance analysis
    with existing models. The proposed framework not only ensures secure and scalable
    management of IoMT health data but also incorporates a stochastic approach to
    mathematically formulate performance indicators for the HMS queuing model. This
    model effectively predicts system response times and assesses the computing resources
    required under varying workload conditions. Our simulation results show a classification
    accuracy of 92%, a 56% reduction in latency compared to existing models, and an
    overall enhancement in e-healthcare service quality. Similar content being viewed
    by others Fog based smart healthcare: a machine learning paradigms for IoT sector
    Article 26 August 2022 Importance of Fog Computing in Healthcare 4.0 Chapter ©
    2021 Fog-based healthcare systems: A systematic review Article 04 September 2021
    1 Introduction The use of IoMT devices in the healthcare industry has been increasing
    rapidly in recent years, with the potential to improve patient outcomes and reduce
    costs [1]. However, time-sensitive healthcare IoMT-based applications require
    real-time data processing, which can be challenging to achieve due to the limitations
    of cloud computing [2, 3]. The cloud’s centralized architecture, coupled with
    network latency, can lead to delays in data processing, making it difficult to
    provide timely and accurate responses. The fog/edge computing paradigm is a solution
    that aims to minimize latency issues in cloud-IoMT architectures by bringing computing
    resources closer to the data source [4, 5]. This is particularly important for
    healthcare monitoring, where timely access to real-time data can make a significant
    difference in the patient’s outcome [6]. By leveraging fog/edge computing, healthcare
    professionals can quickly intervene and make informed decisions that can potentially
    save lives. In addition to latency issues, the ever-increasing number of connected
    IoMT devices has made scalability a crucial factor in providing proper healthcare
    services [7]. To keep up with fluctuating processing needs and the computing resources
    needed to maintain the quality of service (QoS) guaranteed by SLAs, an efficient
    auto-scalability mechanism is essential. The goal of this approach is to achieve
    optimal resource allocation by utilizing minimal computing resources such as fog
    computing and private and public VM nodes while meeting the required SLA response
    time of 1.2 ms for any incoming workload initiated by the MIoT or body sensor
    devices. The customer and health care provider agree to the pricing, QoS level,
    and punishment for SLA breaches in the SLA agreement. SLA breaches may cost the
    healthcare provider income and reputation; hence, QoS must be maintained. To accomplish
    this, an efficient elasticity mechanism that accounts for health data burden and
    meets SLA criteria is needed. Regular monitoring of high-risk patients is an essential
    aspect of healthcare, but it can be challenging and impractical for humans to
    maintain due to the large volume of data that needs to be analyzed [8]. To address
    this issue, an automated system that can evaluate patient health data to detect
    any potential high risks is required. To achieve this, a machine learning algorithm
    such as random forest is utilized to analyze and detect patterns in healthcare
    data. Random forest is a popular algorithm that is used for classification, regression,
    and feature selection in large datasets. It is beneficial in healthcare because
    it can avoid the overfitting problem, which occurs when a model is too complex
    and has difficulty generalizing to new data. Once the high-risk patient is identified,
    the predicted data is sent to the end-user within a short time, ensuring timely
    intervention and appropriate healthcare. This approach helps prevent negligence
    towards high-risk patients and improves the overall quality of healthcare services.
    The queuing model has historically been a cornerstone for studies of this nature
    due to its ability to elucidate a myriad of QoS parameters. It offers insights
    into pivotal metrics such as system response time, CPU utilization, mean throughput,
    and several others, thereby becoming a vital tool for system analysis. In this
    context, fog computing emerges as a crucial mechanism to enhance the QoS, specifically
    for tasks deemed high-priority. It bridges the gap between massive cloud infrastructures
    and end devices, serving as an intermediate layer that can process data more responsively.
    Yet a notable challenge arises from the inherent computational constraints found
    in many of these devices. Oftentimes, these limitations mean that only certain
    tasks can be completed within an acceptable delay threshold. Recognizing this,
    we’ve integrated a priority queueing strategy into our system. This approach ensures
    that high-priority tasks receive the attention they demand, maximizing their throughput.
    By doing so, we intentionally prioritize these urgent tasks over others with lower
    urgency, ensuring optimal system performance and responsiveness. In this paper,
    we present a novel patient monitoring system for e-health that utilizes various
    computing resources within a platform consisting of private and public clouds
    as well as fog nodes. The private cloud, located on or near the hospital premises,
    is a smaller-scale data center than the public cloud. The use of fog nodes helps
    reduce the SLA response time by processing requests and data received from body
    sensor devices locally near the IoMT devices rather than at a remote public cloud.
    To estimate key performance metrics and determine the optimal data processing
    units required for nodes in the cloud (whether public or private) and nodes in
    the fog, the paper proposes analytical models and mathematical formulas. Unlike
    simulation, these formulas provide an immediate solution to identify the compute
    node count needed to achieve a specific performance measure and enable dynamic
    scalability [9, 10]. The central aim of this study is to present and evaluate
    effective resource allocation frameworks and tactics for IoMT, fog, and cloud
    environments while taking into account critical design and performance factors
    such as scalability, load distribution, traffic requirements and workloads, and
    the computational processing capabilities of fog and cloud nodes. The following
    is a list of the key contributions that the research study has made: A novel IoT-fog-cloud
    architecture is investigated for the purpose of monitoring patients’ health using
    IoMT devices. A random forest algorithm is utilized to address the issue of overfitting.
    Key performance characteristics are deduced mathematically, and an analytical
    queuing model is provided to assess the reliability of a remote patient surveillance
    network. Numerical examples demonstrate how our proposed model can be used to
    study the performance of healthcare systems and determine the necessary computing
    resources under different IoMT workload conditions. The rest of the paper is structured
    as follows: Sect. 2 discusses related research. In section 3, we present a system
    model that combines the Internet of Things, the fog, and the cloud. An analytical
    model of the proposed system is presented in Sect. 4. In Sect. 5, we provide our
    analysis based on these simulated outcomes. The conclusion and the paper’s potential
    future directions are presented in Sect. 6. 2 Related work Sensor data is used
    to monitor physiological vital signs and sometimes psychological or emotional
    status via wearable devices. One billion wearables will be sold shortly. Smartwatches,
    armbands, chest straps, shoes, helmets, glasses, lenses, rings, patches, textiles,
    and hearing aids are wearable devices [11]. Despite this increase, further research
    is needed to improve these devices’ accuracy, use diverse bodily signals for new
    applications, and efficiently deal with the complexity of the human body. 2.1
    Learning from the signals of wearable devices Remote patient monitoring (RPM)
    is a fast-emerging topic in healthcare that uses flexible materials for wearable
    sensors to help doctors in a variety of general hospital medical and surgical
    wards [12]. Telehealth apps, wearable gadgets, and contact-based sensors are used
    in healthcare to accomplish this. RPM is used to measure vital signs and other
    physiological parameters like motion recognition, which can help clinicians diagnose
    and treat psychological and movement disorders [13]. Table 1.a lists studies that
    investigated the benefits and drawbacks of telehealth. Telehealth reduces travel
    time, clinic visits, and time away from work. Table 1.b summarizes the IoT devices
    employed by the research community for patient monitoring, as well as their benefits
    and drawbacks. Table 1 Review of the literature on monitoring in the cloud, fog,
    and edge, monitoring in telehealth, and monitoring in IoMT devices Full size table
    Fog and cloud computing in remote health monitoring may better manage the adverse
    effects of deploying many medical sensors in IoMT systems. The cloud/fog/edge
    monitoring architectures provide real-time monitoring with a decentralized approach
    for individualized treatment; however, as indicated in Table 1.c, the technologies
    have drawbacks. Several studies categorize and describe the active features of
    the resource-/service-based fog/edge computing approach in healthcare systems
    [14]. 2.2 The application of machine learning techniques in the context of wearable
    devices Machine learning allows devices with sensors to make decisions and react
    without explicit programming. Learning from prior events allows the device to
    do this [25]. Based on training data, machine learning is classified as supervised,
    unsupervised, semi-supervised, or reinforced. Data instances with or without labels
    indicate learning from past events. The dependent variable in labeled data might
    be categorized or numerical. Machine learning includes categorical target output
    variable categorization, numerical label regression, and unlabeled data clustering.
    Most machine learning studies on wearable devices focus on classification tasks,
    with a lesser number on clustering [26] and a few on regression [27]. Machine
    learning (ML) applications on wearables can face several challenges that can affect
    their performance and accuracy. Some of these challenges include: Limited computing
    power: Wearable devices often have limited computing power and memory, which can
    make it difficult to run complex ML algorithms. This can result in longer processing
    times and less accurate results. For machine learning applications on wearables,
    power consumption is greatly affected by the need to send physiological data measured
    by the device’s sensors to the cloud for computation. Different approaches have
    been proposed to reduce power consumption, including special embedded hardware
    for running machine learning algorithms [28], data reduction and compression [29],
    scheduling of data transfer [30], computational offloading [31], and self-powered
    wearable devices [32]. Performing embedded machine learning on the device (i.e.,
    following the tinyML approach) can significantly increase the battery lifetime,
    as shown by the authors in [33], who reported an increase of over 70%. Noise in
    sensor data: Wearable devices rely on sensors to collect data about the user’s
    physiological and environmental conditions. However, this data can be noisy due
    to factors such as motion artifacts, environmental interference, and sensor drift.
    This can lead to inaccurate results and make it challenging to train ML models
    effectively. The study in [34]proposes a method to reduce motion artifacts in
    wearable photoplethysmography (PPG) signals by using information from an attached
    gyroscope and applying spectral filtering. By combining these two approaches,
    the authors show that they can significantly improve the accuracy of heart rate
    measurements in the presence of motion artifacts. Data variability: Wearable devices
    are designed to be used in different contexts and by different users, which can
    lead to variability in the data collected. This can make it challenging to train
    ML models that are accurate across different users and contexts. The study in
    [35] investigated the sources of inaccuracy in different wearable optical heart
    rate sensors, using heart rate and PPG data from consumer and research-grade wearables.
    The study found no significant difference in accuracy across skin tones but significant
    differences between devices and activity types, with an average absolute error
    of 30% more than during rest. Privacy and security concerns: Wearables capture
    biometric and location data. This may compromise data privacy and security and
    make research and development difficult. Wearable and IoMT devices carry user
    data that may be stolen and transferred to machine learning cloud services. Bluetooth
    security guidelines propose Medical Device Security Mode 1 Level 4 for low-energy
    safe connectivity and encryption. Secure wearable device design, administrative
    policy settings, and ML-based solutions may prevent assaults [36]. To overcome
    these challenges, researchers and developers must design ML algorithms that are
    optimized for wearable devices, taking into account the limitations of these devices.
    This may involve using lightweight ML algorithms, optimizing data collection and
    preprocessing methods, and developing strategies for dealing with noise and data
    variability. Additionally, it is essential to address privacy and security concerns
    and ensure that data is collected and used ethically and responsibly. Table 2
    summarizes the available literature review and comparative analysis. Our proposed
    Machine Learning-based Medical Data Segmentation (ML-MDS) technique differs from
    previous work. While El Kafhali et al [37] focus on queuing models and IoT components,
    Kishor et al [38] specialize in multimedia separation using fog computing, and
    Kumar et al [39] focus on image classification using fog-based deep learning.
    ML-MDS, on the other hand, blends machine learning, cloud security, IoT-Fog-Cloud
    agility, and efficient high-volume data processing, all of which contribute to
    overall system efficiency and effective management of time-sensitive data. Table
    2 Comparison of proposed techniques in HMS Full size table 3 System architecture
    3.1 Network explanation Emergency Medical Services (EMS) hardware and software
    may vary. A variety of patient bio-signals are recorded by analog sensors on an
    IoMT device or BSN node. Using Bluetooth Low Power, 6LoWPAN, and Wi-Fi, BSN nodes
    digitize analog signals and send them to gateways. After gateway processing, data
    is stored and analyzed on the cloud. Medical personnel can quickly grasp data
    visualization. Modeling uncertainty from complex human body input is challenging,
    as seen in Fig. 1. Its intricacy may impair model accuracy. Researchers overcame
    this challenge using multimodal health monitoring data. Wearables detect yelling,
    shouting, coughing, and laughing. The method has downsides. Despite their difficulty,
    EEG signals may enhance dehydration, mood, and mental illness detection. IoT devices
    and body sensors, including ECG, EEG, EMG, EOG, BLE, temperature, electrocardiogram,
    and ambient humidity sensors, are linked using the patient’s state. An access
    point transmits GW data from these devices. IoT devices link to backend processing
    through the GW. GW requests health data from a buffer. High-risk health data is
    sensitive or dangerous if released or mishandled. This may include sensitive health
    information, including medical diagnoses or genetics. Private-public cloud separation
    eliminates data leaks and cross-contamination. Managing sensitive data in the
    healthcare, finance, and government sectors may benefit from this method (see
    figure 2).In the healthcare domain, the secure transmission and storage of sensitive
    patient data are paramount. To achieve this, a two-channel approach is often adopted.
    Low-risk data can be efficiently transferred via a public cloud, leveraging its
    scalability and cost-effectiveness. Meanwhile, high-risk health data is directed
    to a private cloud infrastructure equipped with fog computing nodes at the network’s
    edge. This private cloud setup ensures stringent security measures and offers
    greater control over data, addressing the unique privacy and compliance demands
    of healthcare while enabling real-time processing and analysis. Fig. 1 Maximizing
    the benefits of human body signals in machine learning-based HMS Full size image
    Fig. 2 Development of a classification system for heart sounds Full size image
    3.2 Materials and methods A health monitoring system consists of three key components:
    BSN nodes collect patient biosignals via various sensors and relay them using
    wireless protocols (e.g., Sigfox, Wi-Fi, or Bluetooth Low Power) to a gateway.
    This gateway forwards the data to a remote cloud server. Healthcare providers
    access and diagnose the data from this server, which includes textual and graphical
    representations. Our system, as shown in Fig. 3, utilizes BSN for data capture,
    a gateway (GW), a private cloud, fog computing nodes, a cloud gateway (CG), and
    a public cloud. Fig. 3 Fog-cloud patient monitoring system deployment Full size
    image 3.3 Datasets Clinical and physiological data is stored on PhysioNet. The
    NIH-funded project [40] is managed by MIT-CD’s Laboratory for Computational Physiology.
    The PhysioNet database provides access to ECG, blood pressure, respiration, and
    other physiological data [41]. Scientists, medics, and others may utilize these
    data sets. Strengths include PhysioNet’s open research and software policy. Data,
    problem, and application determine which machine learning algorithm classifies
    wearable sensor data. Data sets and tools for pioneering sleep apnea, arrhythmia,
    stress, and human activity research are publicly available. Decision trees, random
    forests, support vector machines, k-nearest neighbors, artificial neural networks,
    and deep learning models like convolutional and recurrent neural networks are
    popular among wearable sensor data classifiers. Sensor data quality may affect
    algorithm choice. Some algorithms process time-series data better than spatial
    or frequency-domain data. Finally, choice algorithms and feature extraction approaches
    focused on accuracy and precision. Random forests categorize large, multi-feature
    datasets. By connecting decision trees, ensemble learning enhances classification
    accuracy and stability. Random forests manage categorical data even with missing
    values and noise. A straightforward method requiring modest hyperparameter modifications
    [42]. It handles several features and noise for wearable device data classification.
    Check out Fig. 4 for the pipeline, which comprises data collection, cleaning,
    model training, testing, and deployment. In our dataset [43], sensor data from
    humans is pre-processed and features extracted. Fig. 4 The chain of processing
    sensor data in an ML system Full size image The flow chart in Fig. 5 illustrates
    the ML-MDS scheme for machine learning-based medical data segmentation. High-risk
    data is processed near the sensor devices through fog computing, which minimizes
    the distance and number of hops between the sensor and the cloud server. This
    approach reduces transmission time and total latency time, resulting in more efficient
    data transfer. Fig. 5 Flow chart for ML-MDS scheme Full size image 3.4 Queuing
    model for the proposed system To formulate and study the problem of the proposed
    patient monitoring system, we consider a network of queues as shown in Fig. 6.
    Fig. 6 A queuing network model for performance quantification of an Internet of
    Healthcare Things (IoMT) infrastructure for medical monitoring Full size image
    The presented diagram in Fig. 6 depicts a queuing model that is based on theoretical
    principles and applies to the specified architecture. The problem of the proposed
    patient monitoring system is formulated and studied by considering a network of
    queues. Table 3 enumerates all the constituents of the model. Table 3 Model parts
    and their descriptions Full size table This study examines a \\(\\lambda _i\\)
    health data Poisson process. Each health data service’s inter-arrival durations
    are independent and exponentially distributed random variables with a mean of
    \\(1/\\lambda _i\\), where i is an index into X medical record requests. The total
    system arrival rate is \\(\\lambda =\\sum _{i=1}^X \\lambda _i\\), where X is
    the total number of request types. With probabilities \\(P_C\\) and \\(P_{\\text
    {Cloud}}\\), the gateway either sends the health data to a private or public cloud
    data center. In an M/M/1 arrangement, GW and CG are queued with unlimited buffers.
    This prevents health data loss between IoT devices, processing models, and the
    FC layer and PC data center. The \\(\\textrm{CG}\\) distributes incoming health
    traffic across VMs to load balance the private cloud. The processing time at \\(\\textrm{GW}\\)
    and \\(\\textrm{CG}\\) is defined by exponential random variables that are independent
    and identically distributed. These random variables have mean rates of \\({1 \\mathord{\\left/
    {\\vphantom {1 {\\mu _{{GW}} }}} \\right. \\kern-\\nulldelimiterspace} {\\mu _{{GW}}
    }}\\) and \\({1 \\mathord{\\left/ {\\vphantom {1 {\\mu _{{{\\text{CG}}}} }}} \\right.
    \\kern-\\nulldelimiterspace} {\\mu _{{{\\text{CG}}}} }}\\), respectively. Each
    FC node receives a \\(\\textrm{M} / M / 1 / B\\) queue with a uniform service
    time of \\(\\mu _{{{\\text{FC}}}}\\). To account for the cloud’s practically limitless
    computational capability, a \\(\\textrm{M} / \\textrm{M} / \\textrm{C}\\) queue
    is allocated with an unbounded waiting buffer. Virtual machines (VMs) in the public
    cloud data center deliver exponentially distributed services at \\(\\mu\\). Each
    virtual machine instance may provide some health information. In a private cloud
    datacenter, the \\(\\textrm{M} / \\textrm{M} / \\textrm{c} / \\textrm{K}\\) queueing
    model is used to characterize individual nodes. The service time for each private
    node is a \\(1 / \\mu _1\\) exponential random variable. Given the variety of
    incoming health data workloads, fog and cloud nodes may be dynamically generated
    and eliminated depending on the number of services needed to scale up and down.
    Table 4 provides a comprehensive description of all the essential input parameters
    utilized in this paper for the purpose of enhancing clarity. Table 4 Defining
    the primary input variables of the system Full size table 4 Mathematical modeling
    4.1 Gateway processing and service To effectively manage the incoming workload
    of IoT data (denoted as \\(\\lambda\\)) and the extensive capacity available in
    the public cloud, both the Gateway (GW) and Cloud Gateway (CG) are represented
    as queues. Specifically, they are modeled using an M/M/1 queue structure with
    an infinite waiting buffer. We can then express the performance at GW. With \\(\\rho
    = \\left( {{\\lambda \\mathord{\\left/ {\\vphantom {\\lambda {\\mu _{{{\\text{GW}}}}
    }}} \\right. \\kern-\\nulldelimiterspace} {\\mu _{{{\\text{GW}}}} }}} \\right)
    < 1\\), the system is stable, which means that over time the queue will not grow
    indefinitely. This modeling approach allows us to analyze and approximate their
    performance characteristics [44]. Mean Response Time in GW The mean response time,
    denoted as \\(W_{{{\\text{GW}}}}\\), represents the average time an item spends
    in the GW from the moment it arrives until it leaves. This time includes both
    the waiting time and the service time. For an \\(\\textrm{M} / \\textrm{M} / 1\\)
    queue, \\(W_{{{\\text{GW}}}}\\) is given by: $$W_{{{\\text{GW}}}} = \\frac{1}{{\\mu
    _{{{\\text{GW}}}} - \\lambda }}$$ (1) 4.2 Cloud gateway processing and service
    Let’s focus on the scenario where health data, after being processed by the Fog
    Computing (FC) nodes, either gets forwarded to a private cloud for additional
    services or leaves the system altogether. The probabilities for these two events
    are p and \\(1-p\\), respectively. Mean Response Time \\(W_{CG}\\) The mean response
    time \\(W_{{{\\text{CG}}}}\\) represents the average time an item spends in the
    system from the moment it arrives until it leaves. This time includes both the
    waiting time \\(W_{q,\\text {CG}}\\) and the service time \\(\\frac{1}{\\mu _{\\text
    {CG}}}\\).\\({\\text{ With }}\\tau = {{\\left( {P_{{{\\text{ C }}}} \\lambda p}
    \\right)} \\mathord{\\left/ {\\vphantom {{\\left( {P_{{{\\text{ C }}}} \\lambda
    p} \\right)} {\\mu _{{{\\text{CG}}}} }}} \\right. \\kern-\\nulldelimiterspace}
    {\\mu _{{{\\text{CG}}}} }}\\). Thus, \\(W_{{{\\text{CG}}}}\\) can be expressed
    as: $$W_{{{\\text{CG}}}} = W_{{q,{\\text{CG}}}} + \\frac{1}{{\\mu _{{{\\text{CG}}}}
    }} = \\frac{\\tau }{{\\mu _{{{\\text{CG}}}} - P_{{\\text{C}}} \\lambda p}} + \\frac{1}{{\\mu
    _{{{\\text{CG}}}} }} = \\frac{{\\frac{1}{{\\mu _{{{\\text{CG}}}} }}}}{{1 - \\tau
    }}.$$ (2) 4.3 Modeling of fog computing nodes In the given scenario, each Fog
    Computing (FC) node is modeled as an \\(M/M/1/B\\) queue, where \\(B\\) represents
    the maximum number of “high-risk” items that can be in the system (either waiting
    or being processed). The maximum queue length is \\(B-1\\). Given \\(p_j = \\frac{P_{\\text
    {C}}}{N}\\) and \\(\\lambda _{h} = \\frac{\\lambda P_{\\text {C}}}{N}\\), each
    FC node effectively has its own arrival rate \\(\\lambda _{h}\\). Balance Equations
    For each FC node modeled as an \\(M/M/1/B\\) queue, the balance equations can
    be formulated as: $$\\begin{aligned} \\lambda _{h} \\pi _{k-1}^j&= \\mu _{\\text
    {FC}} \\pi _k^j \\quad \\text {for } k=1,2,\\ldots ,B \\end{aligned}$$ (3) $$\\begin{aligned}
    \\sum _{k=0}^{B} \\pi _k^j&= 1 \\quad \\text {for each } j \\quad \\quad \\quad
    \\text {Normalization Condition} \\end{aligned}$$ (4) Steady-State Probability
    of Having k items in the j-th Fog Node: The steady-state probability \\(\\pi _k^j\\)
    for \\(k\\) items in the \\(j\\)-th fog node can be dependent on whether \\(\\alpha
    = \\frac{\\lambda _{h}}{\\mu _{\\text {FC}}}\\) is equal to 1 or not. $$\\begin{aligned}
    \\pi _k^j = {\\left\\{ \\begin{array}{ll} \\frac{1}{B+1} &{} \\text {if } \\alpha
    = 1, \\\\ \\alpha ^k \\frac{1 - \\alpha }{1 - \\alpha ^{B+1}} &{} \\text {if }
    \\alpha \\ne 1. \\end{array}\\right. } \\end{aligned}$$ (5) Probability of Finding
    k items Already in the Queue: $$\\begin{aligned} P_k^j = {\\left\\{ \\begin{array}{ll}
    \\frac{(1-\\alpha ) \\alpha ^k}{1-\\alpha ^B}, &{} \\alpha \\ne 1 \\\\ \\frac{1}{B},
    &{} \\alpha =1 \\end{array}\\right. } \\end{aligned}$$ (6) Overflow Probability
    in the (j)-th Fog Node The overflow probability, denoted as \\(P_{\\text {overflow}}^j\\),
    refers to the probability that an arriving health data request at the \\(j\\)-th
    fog node will find the system in a highly utilized state, with \\(A \\le k < B\\)
    items already in the queue. Here, \\(A\\) is a minimum threshold of load, and
    \\(B\\) is the maximum capacity of the system, is given by $$\\begin{aligned}
    P_{\\text {overflow}}^j = \\left( 1 + \\alpha + \\alpha ^2 + \\ldots + \\alpha
    ^{B-1} \\right) ^{-1} \\frac{\\alpha ^A - \\alpha ^B}{1 - \\alpha }=\\frac{\\alpha
    ^A - \\alpha ^B}{\\alpha ^{B-1} - 1} \\end{aligned}$$ (7) Effective Arrival Rate
    of Health Data to j -th Fog Node Service $$\\begin{aligned} \\lambda _{\\text
    {eff}}=\\lambda _{h}\\left( 1 - \\pi _B^j \\right) =\\lambda _h \\frac{\\alpha
    -\\alpha ^{B+1}}{1-\\alpha ^{B+1}} \\end{aligned}$$ (8) Rate of Loss (Probability
    of Rejected Requests) The rate of loss \\(P_{\\text {loss}}^j\\) for the \\(j\\)-th
    fog node is given by: $$\\begin{aligned} P_{\\text {loss}}^j = \\lambda _{h} \\pi
    _B^j= \\lambda _{h}\\frac{1-\\alpha }{1-\\alpha ^{B+1}} \\alpha ^B, \\quad \\alpha
    \\ne 1 \\end{aligned}$$ (9) Throughput Service of j-th Fog Computing Node $$\\begin{aligned}
    \\text {TH}_{\\text {FC}}^j=\\lambda _{h}\\left( 1-P_{\\text {loss}}^j\\right)
    =\\lambda _{h} \\frac{1-\\alpha ^B}{1-\\alpha ^{B+1}}, \\quad \\alpha \\ne 1 \\end{aligned}$$
    (10) CPU Utilization of Fog Computing Server $$\\begin{aligned} U_{\\text {FC}}=\\frac{\\text
    {TH}_{\\text {FC}}^j}{\\mu _{\\text {FC}}}=\\alpha \\frac{1-\\alpha ^B}{1-\\alpha
    ^{B+1}}, \\quad \\alpha \\ne 1 \\end{aligned}$$ (11) Mean Number of items (In
    Waiting and Execution) in j-th Fog Node The mean number of items \\(\\bar{E}_{\\text
    {FC}}^j\\) is calculated as: For \\(\\alpha \\ne 1\\) $$\\begin{aligned} \\bar{E}_{\\text
    {FC}}^j = \\sum _{k=0}^{B} k \\pi _k^j = \\frac{1 - \\alpha }{1 - \\alpha ^{B+1}}
    \\sum _{k=0}^{B} k \\alpha ^k = \\frac{1 - \\alpha }{1 - \\alpha ^{B+1}} \\times
    \\alpha \\frac{1 - (B+1) \\alpha ^B + B \\alpha ^{B+1}}{(1 - \\alpha )^2} \\end{aligned}$$
    (12) For \\(\\alpha = 1\\) $$\\begin{aligned} \\bar{E}_{\\text {FC}}^j = \\sum
    _{k=0}^{B} k \\pi _k^j = \\sum _{k=0}^{B} k \\left( \\frac{1}{B+1} \\right) =
    \\frac{1}{B+1} \\sum _{k=0}^{B} k=\\frac{B}{2} \\end{aligned}$$ (13) Then: $$\\begin{aligned}
    \\bar{E}_{\\text {FC}}^j=\\sum _{k=0}^B k \\pi _k^j={\\left\\{ \\begin{array}{ll}
    \\frac{\\alpha }{1-\\alpha } \\frac{1-(B+1) \\alpha ^B+B \\alpha ^{B+1}}{1-\\alpha
    ^{B+1}}, &{} \\alpha \\ne 1 \\\\ \\frac{B}{2}, &{} \\alpha =1 \\end{array}\\right.
    } \\end{aligned}$$ (14) Average Busy Server (Number of items in Service) $$\\begin{aligned}
    \\text {Busy}_{\\text {FC}j} = {\\left\\{ \\begin{array}{ll} 1 - \\frac{1 - \\alpha
    }{1 - \\alpha ^{B+1}} &{} \\text {if } \\alpha \\ne 1 \\\\ 1 - \\frac{1}{B+1}
    &{} \\text {if } \\alpha = 1 \\end{array}\\right. } \\end{aligned}$$ (15) Mean
    Number of items Waiting in the Fog Computing Node $$\\begin{aligned} \\bar{M}_{\\text
    {FC}j}^{\\text {queue}} = \\bar{E}_{\\text {FC}}^j - \\text {Busy}_{\\text {FC}j}
    \\end{aligned}$$ (16) The mean number of items waiting \\(\\bar{M}_{\\text {FC}j}^{\\text
    {queue}}\\) provides a measure of the average number of items that are in the
    queue but not yet in service in the \\(j\\)-th fog node. Mean Waiting Time of
    Items at the j-th Fog Computing Node $$\\begin{aligned} \\bar{W}_{\\text {FC}}^j=\\frac{\\bar{M}_{\\text
    {FC}}^j}{\\lambda _{h}} \\end{aligned}$$ (17) Mean Response Time of Items at the
    j-th Fog Computing Node By applying Little’s Law [45], the mean response time
    \\(W_{\\text {FC}}^j\\) for items at the \\(j\\)-th fog node can be calculated
    as: $$\\begin{aligned} W_{\\text {FC}}^j= \\frac{\\bar{E}_{\\text {FC}}^j}{\\text
    {TH}_{\\text {FC}}^j} =\\frac{\\bar{E}_{\\text {FC}}^j}{\\text {TH}_{\\text {FC}}^j}=
    \\frac{\\alpha \\left( B \\alpha ^{B + 1} - \\alpha ^B (B + 1) + 1 \\right) }{\\lambda
    _{h} (\\alpha - 1) (\\alpha ^B - 1)}, \\quad \\alpha \\ne 1 \\end{aligned}$$ (18)
    Total Response Time in Fog Computing Layer System $$\\begin{aligned} \\text {T}_{\\text
    {fog}} = \\frac{1}{N} \\sum _{i=1}^{N} \\text {W}_{\\text {FC}}^j \\end{aligned}$$
    (19) 4.4 Public cloud modeling In our cloud data center model for items, we use
    a queueing approach with an unlimited buffer and a constant service rate \\(\\mu\\).
    The center comprises \\(M\\) nodes, each capable of handling up to \\(C\\) requests
    for “low-risk” simultaneously. Requests arrive according to a Poisson process
    with rate \\(\\lambda _{\\ell } = \\frac{{{\\text{P}}_{{{\\text{Cloud}}}} }}{
    M }\\times \\lambda \\times {\\text{p}}\\). To ensure stability, we calculate
    the system utilization \\(\\rho _j\\) for each node as \\(\\rho _j = \\frac{\\lambda
    _\\ell }{C \\mu }\\), and require that \\(\\rho _j < 1\\). Balance Equations For
    \\(n < C\\), the balance equation is: $$\\begin{aligned} \\lambda _\\ell P(n)
    = \\mu (n+1) P(n+1) \\end{aligned}$$ (20) For \\(n \\ge C\\), the balance equation
    is: $$\\begin{aligned} C \\mu P(n) = \\lambda _\\ell P(n-1) \\end{aligned}$$ (21)
    Stability Probability of Having n Items By solving these balance equations, we
    can express \\(P(n)\\) in terms of \\(P(0)\\). $$\\begin{aligned} P(n) = {\\left\\{
    \\begin{array}{ll} \\frac{{\\lambda _\\ell ^n}}{\\mu ^n} \\frac{P(0)}{n!} &{}
    \\text {for } 0 \\le n < C, \\\\ \\frac{{\\lambda _\\ell ^C}}{C! \\mu ^C} \\left(
    \\frac{\\lambda _\\ell }{C \\mu } \\right) ^{n-C} P(0) &{} \\text {for } n \\ge
    C. \\end{array}\\right. } \\end{aligned}$$ The sum of all probabilities must be
    1 for normalization: $$\\begin{aligned} \\sum _{n=0}^{\\infty } P(n) = 1 \\end{aligned}$$
    (22) Substituting the expressions for \\(P(n), \\quad n \\ge C\\): yields: $$\\begin{aligned}
    P(0) \\left( \\sum _{k=0}^{C-1} \\frac{{\\rho _j^k}}{k!} + \\frac{{\\rho _j^C}}{C!
    \\left( 1 - \\rho _j\\right) } \\right) = 1 \\end{aligned}$$ (23) Solving for
    \\(P(0)\\) gives: $$\\begin{aligned} P(0) = \\left( \\sum _{k=0}^{C-1} \\frac{{\\rho
    _j^k}}{k!} + \\frac{{\\rho _j^C}}{C! \\left( 1 - \\rho _j\\right) } \\right) ^{-1}
    \\end{aligned}$$ (24) Probability That an Arriving Request Has to Wait \\(P_{\\text
    {wait}}\\) To assess server availability at each public node, apply the Erlang
    C formula to find the probability that an incoming health data request must wait
    in the queue. $$\\begin{aligned} P_{\\text {wait}} = P(0) \\frac{{\\rho _j^C}}{C!
    \\left( 1 - \\rho _j\\right) } = \\frac{{\\left( \\lambda _\\ell / \\mu \\right)
    ^C \\mu }}{(C-1) !\\left( C \\mu -\\lambda _\\ell \\right) } P(0) \\end{aligned}$$
    (25) Mean Queue Length (\\(L_q\\)) and Mean System Size (L) $$\\begin{aligned}
    L_q = P_{\\text {wait}} \\times \\frac{\\lambda _\\ell }{\\mu \\times (1 - \\rho
    _j)} ,\\quad L = C \\times \\rho _j + L_q \\end{aligned}$$ (26) Average Waiting
    Time in Queue (\\(W_q\\)) and The Response Time in System (\\(T_i\\)) $$\\begin{aligned}
    W_q = \\frac{{L_q}}{\\lambda _\\ell } = \\frac{{P_{\\text {wait}}}}{\\mu (1-\\rho
    _j)},\\quad T_i = W_q + \\frac{1}{\\mu } = W \\quad , i=1, \\dots ,M \\end{aligned}$$
    (27) Overall Response Time The overall response time can be calculated as the
    average of the response times \\(T_i\\) for all M public nodes: $$\\begin{aligned}
    T_{\\text {pu}} = \\frac{1}{M} \\sum _{i=1}^{M} T_i \\end{aligned}$$ (28) 4.5
    Private cloud modeling The M/M/c/K queue is employed to model nodes in a private
    cloud, each accommodating up to \\(c\\) VMs and \\(K\\) queued requests. Given
    \\(R\\) total nodes, each node experiences a Poisson arrival rate of \\(\\lambda
    _{1} = \\frac{{P_{C} \\lambda }}{R}\\). The service rate for each VM is denoted
    by \\(\\mu _1\\). Steady-State Probability \\(\\pi _n\\) The steady-state probability
    \\(\\pi _n\\) of having \\(n\\) items in the \\(i^{\\text {th}}\\) node in the
    private cloud datacentre can be calculated using the following formula for an
    M/M/c/K queue: $$\\begin{aligned} \\pi _i^n = {\\left\\{ \\begin{array}{ll} \\frac{1}{G}
    \\left( \\frac{(\\lambda _1 / \\mu _1)^n}{n!} \\right) &{} \\text {for } n = 0,1,2,\\ldots
    ,c, \\\\ \\frac{1}{G} \\left( \\frac{(\\lambda _1 / \\mu _1)^n}{c! \\times c^{(n-c)}}
    \\right) &{} \\text {for } n = c+1,c+2,\\ldots ,K. \\end{array}\\right. } \\end{aligned}$$
    (29) Where \\(G\\) is the normalization constant, given by: $$\\begin{aligned}
    G = \\left( \\sum _{n=0}^{c} \\frac{(\\lambda _1 / \\mu _1)^n}{n!} \\right) +
    \\left( \\frac{(\\lambda _1 / \\mu _1)^{c+1}}{c!} \\sum _{n=c+1}^{K} \\frac{(\\lambda
    _1 / \\mu _1)^{n-c}}{c^{n-c}} \\right) \\end{aligned}$$ (30) The quality of service
    (QoS) can be evaluated through various metrics. One of these metrics is the rate
    of loss. The rate of loss \\(L_i\\) in the \\(i^{\\text{ th } }\\) private node
    can be calculated as the arrival rate times the probability of the system being
    full, which is the steady-state probability \\(\\pi _{K, i}\\) of having K items
    in the \\(i^{\\text{ th } }\\) node. $$\\begin{aligned} L_{\\text {loss}} = \\lambda
    _1 \\times \\pi _{K,i} \\end{aligned}$$ (31) Mean number of items and the mean
    waiting time $$\\begin{aligned} \\begin{aligned} L_i = \\sum _{n=0}^{K} n \\pi
    _{n,i} ,~~W_i = \\frac{L_i}{\\lambda _1 (1 - \\pi _{K,i})} \\end{aligned} \\end{aligned}$$
    (32) Average wait time and average response time in the ith private node $$\\begin{aligned}
    \\begin{aligned} W_i = \\frac{L_i}{\\lambda _1 (1 - \\pi _{K,i})},~~R_i = W_i
    + \\frac{1}{\\mu _1} \\end{aligned} \\end{aligned}$$ (33) Where \\(R_i\\) is defined
    as: $$\\begin{aligned} R_i= \\frac{\\frac{1}{G} \\left( \\sum _{n=0}^{c} n \\frac{(\\lambda
    _1 / \\mu _1)^n}{n!} + \\sum _{n=c+1}^{K} n \\frac{(\\lambda _1 / \\mu _1)^n}{c!
    \\times c^{(n-c)}} \\right) }{\\lambda _1 \\left( 1 - \\frac{1}{G} \\left( \\frac{(\\lambda
    _1 / \\mu _1)^K}{c! \\times c^{(K-c)}} \\right) \\right) } + \\frac{1}{\\mu _1}
    \\end{aligned}$$ (34) Overall average response time \\(T_p\\) in the system $$\\begin{aligned}
    T_{pr} = \\frac{1}{R} \\sum _{i=1}^{R} R_i \\end{aligned}$$ (35) 4.6 Analysis
    of the system’s response time globally The gateway may send IoMT traffic to a
    public cloud data center or to a fog, then to a private cloud data center. Health
    data transmission by the GW is assumed to occur with a probability of \\(P_C\\)
    for public cloud data centers and \\(P_{\\text {Cloud}}\\) for private cloud data
    centers. The MRT for processing aggregate IoMT traffic in a private cloud data
    center may be calculated using (1) and (28). $$T_{{{\\text{Public}}}} = \\bar{T}_{{{\\text{GW}}}}
    + T_{{{\\text{pu}}}}$$ (36) The average response time under the assumption that
    all IoT traffic is processed in a public cloud data center is calculated as follows:
    Using Eqs. (1), (2), (19), and (35). $$T_{{{\\text{ Private }}}} = \\bar{T}_{{{\\text{GW}}}}
    + T_{{{\\text{ fog }}}} + \\bar{T}_{{{\\text{CG}}}} + T_{{pr}}$$ (37) Therefore,
    the total reaction time of the system is calculated as follows: $$\\begin{aligned}
    T_{\\text{ System } }=P_C \\times T_{\\text{ Private } }+P_{\\text{ Cloud } }
    \\times T_{\\text{ Public } } \\end{aligned}$$ (38) This comprehensive formula
    for \\(T_{\\text {System}}\\) takes into account all the components and their
    respective proportions, providing a detailed view of the overall system’s mean
    response time. 5 Simulation and numerical results 5.1 Health data classification
    using machine learning 5.1.1 Data acquisition and pre-processing This study used
    a high-quality biomedical dataset from PhysioNet [46]. Figure (7) displays our
    dataset. The Cleveland Heart Disease (CHD) dataset consists of 303 instances and
    encompasses 14 heart disease-related attributes, including age, gender, and biological
    measurements such as EEG and ECG. The dataset’s target variable indicates the
    presence or absence of heart disease. MIT Laboratory for Computational Physiology
    and several academic works retain the dataset. Before data analysis and machine
    learning, pre-processing occurred. Employing excellent practices from Wang [47]
    and Kocheturov et al. [48]. Clean data, normalize feature scales, segment epochs,
    extract features for analysis, impute missing values, and balance model bias.
    Fig. 7 Data Acquisition Full size image 5.1.2 Classification methods and performance
    SVM is a supervised machine learning method developed by Vapnik for classification
    and regression problems [49]. It was utilized to binary classify heart disease
    in this investigation. The best hyperplane that divides classes is found via SVM
    to minimize structural risk and overfitting. Based on Bayesian networks, the Naïve
    Bayes classifier represents variables and dependencies using a directed acyclic
    graph. It is effective for categorizing datasets under specific assumptions [50].
    Instead, Logistic Regression classifies real-valued input vectors discriminatively.
    Bernoulli trials compute the probability of dichotomous events, which may be used
    for multiclass categorization [51]. Classification Trees structure data hierarchically,
    with each node containing particular outcomes and probabilities [52]. Calculating
    entropy, or information gain, determines the tree root. Finally, the Random Forest
    algorithm mixes many decision trees for classification, regression, and other
    applications [42]. Average, or \"voting,\" on many decision tree outputs improves
    its performance and resilience. Because of their ensemble structure, Random Forests
    can handle numerical and categorical data without overfitting. Machine learning
    algorithms in our case study categorize people into “high-risk” and “low-risk”
    heart disease groups. The “low-risk” group is healthy, whereas the “high-risk”
    group has heart disease. We want to build a prediction model that can accurately
    identify these risk groups using age, gender, and PhysioNet biological information.
    Healthcare providers may use this to identify patients who require quick and focused
    therapy. Performance measurement This research classified heart disease samples
    using five supervised machine-learning approaches. Tenfold cross-validation and
    eight quality criteria assessed categorization performance. The presence or absence
    of cardiac disease classified the samples as positive or negative. Table 5 displays
    the numerical results obtained from the classification of the health data sets
    for the clinical assessment of heart health. These results represent the performance
    metrics of different classification algorithms used to predict the presence of
    “high risk” or the absence of “low risk” of heart disease based on the available
    clinical data. Table 5 Comparison of Model Accuracies Full size table In terms
    of training data, the best-performing model is the Random Forest method, which
    achieves an accuracy of 100%. Noting that the test accuracy is lower than the
    training accuracy (about 86.84 percent), it’s possible that overfitting has occurred
    during training. Even still, when compared to other models, the Random Forest
    has one of the best test accuracy ratings. Although it may be susceptible to overfitting,
    it outperforms competing algorithms in terms of generalization to novel, unknown
    data. The high test accuracy of Random Forest suggests that it may be the best
    model for this purpose, assuming that overfitting is avoided. Figure 8 (ROC Curve)
    reveals that Random Forest had the best training accuracy, indicating its strong
    fit to the training data, but it also showed signs of overfitting. In contrast,
    SVM, Naive Bayes, and Logistic Regression demonstrated good generalization with
    balanced ROC curves, making them solid choices for this classification task. However,
    the Decision Tree model struggled with overfitting and performed poorly on unseen
    data. Fig. 8 Receiver Operating Characteristic (ROC) Curve Full size image Random
    Forest, the most accurate algorithm evaluated, classified 40% of people as “high
    risk,” while 60% were classified as “low risk.” Following our investigations,
    these percentages are functional as well as descriptive. Denoted as \\(P_c\\)
    for “high risk” at \\(40\\%\\) and \\(1-P_c\\) for “low risk” at \\(60\\%\\),
    we will use these class distribution percentages as prior probabilities for subsequent
    assessments. Prior probabilities may improve our model’s prediction ability and
    provide more sophisticated heart disease risk classification insights. 5.2 Environment
    and settings for simulation To illustrate the analytical results presented herein,
    some numerical results are illustrated in tables and figures. The calculations
    were made with double accuracy and performed in a 64-bit Windows 10 professional
    OS possessing Intel®Core(TM) i5-7300U @ 2.60GHz–2.71GHz and 16 GB of RAM.manufacturer
    HP using MATLAB software, version R2019a. We reported the numeric results to only
    the nearest four digits, but the results were very accurate. Health data request
    rates ranged from 1000 to 10,000 requests/s. Workloads had two potential paths:
    GW to private cloud (0.4 probability) and GW to fog layers (0.6 probability).
    Health data was sent to the cloud gateway for further processing with a 0.6 probability.
    The simulation included a small private cloud with 10 nodes, each housing 3 VMs,
    maxing at 300 health data. A public cloud had 20 nodes, with 4 VMs per node. The
    fog layer featured 20 FC nodes, maxing at 150 health data. GW request processing
    times averaged 0.0001s, FC nodes at 0.01s, and cloud gateway server at 0.0001s.
    The VM public node service time was 0.00015s. Simulation parameters varied by
    scenario, as outlined in Table 6. Table 6 Summary of parameters and values used
    in the simulation of the proposed healthcare system Full size table 5.3 Experimental
    evaluation Our proposed model utilizes minimal computing resources (fog computing
    nodes, private VM nodes, and public VM nodes) to achieve efficient and dynamic
    scalability, meeting an imposed SLA response time of \\(1.2 \\mathrm {~ms}\\).
    Analytical and simulation results were then compared. The statistics confirm the
    analytics and simulation results. The analytical model was verified. The graph
    Fig. 9 depicts the correlation between the number of requests per millisecond
    and the escalation of a system’s response time. The system operates within the
    acceptable threshold of 1.2 milliseconds until the request rate surpasses a range
    of 6 to 7 requests per millisecond. Once this point is reached, the system’s response
    time increases significantly, surpassing the threshold set by the desired SLA
    latency. This suggests the presence of performance problems when the system is
    under heavier loads. Fig. 10 shows the correlation between the total response
    time of a system and the number of body sensors it is processing. The system’s
    response time remains stable, below the desired SLA latency threshold of 1.2 ms
    for a sensor count up to 120. However, handling a higher number of sensors can
    lead to a sharp increase in response time. The system response time grows as the
    number of body sensors on a patient increases, possibly exceeding SLA latency
    constraints. According to our findings, consistent computing resources result
    in longer responses with increased sensor-generated health workloads. To ensure
    SLA compliance and save costs, computer resources must be adjusted depending on
    shifting sensor workloads. Our research provides formulas for determining the
    ideal quantity of computer resources required for this balance. Fig. 9 Trade-offs
    of increasing fog nodes in reducing system response time Full size image Fig.
    10 Influence of private virtual machine count on system responsiveness Full size
    image Fig. 11 shows how the overall response time of our IoMT-fog-cloud system
    fluctuates with the number of requests per millisecond for various FC node configurations.
    As the load grows, systems with more FC nodes (20 and 30) keep response times
    well within the SLA limit of 1.2 ms, but systems with fewer nodes (5 and 10) rapidly
    surpass this limit. This suggests that bigger FC node configurations may manage
    higher loads more effectively while still meeting SLA requirements, emphasizing
    the significance of optimal resource allocation to fulfill system performance
    objectives. The graph Fig. 12 depicts the performance of our IoMT-fog-cloud system
    with varied private VM counts under various loads. It demonstrates that systems
    with more VMs, especially those with 20 or 30 VMs, maintain response times below
    the SLA level of 1.2 ms over a broader range of request rates. However, once requests
    approach 10 requests/ms, both setups approach the SLA limit, showing that scalability
    advantages are limited. Meanwhile, systems with 5 and 10 virtual machines reach
    the SLA limit at lower request rates, indicating that they are less appropriate
    for high-load situations. This knowledge is critical for optimum VM allocation
    in private cloud computing to sustain service levels. Fig. 11 Trade-offs of increasing
    fog nodes in reducing system response time Full size image Fig. 12 Influence of
    private virtual machine count on system responsiveness Full size image Fig. 13
    demonstrates that as demand grows, fog computing servers with more nodes (20 and
    30) have a lower CPU utilization rate than those with fewer nodes (5 and 10).
    With increased request rates, servers with fewer nodes approach 0.8 CPU usage,
    while those with more nodes maintain utilization rates far below 0.3. This suggests
    that a larger number of FC nodes may distribute workloads more effectively, potentially
    leading to improved performance and cost savings. Adding additional FC nodes is
    beneficial not just for managing higher workloads but also for maintaining a stable
    level of CPU usage. This scalability is critical for managing resource use and
    ensuring the dependability of fog computing services in the face of fluctuating
    demand. Thus, the graph stresses the significance of strategic resource scaling
    in fog computing settings for achieving operational efficiency as well as service
    stability. Fig. 13 Analyzing CPU load dynamics in relation to node count and request
    rates in fog computing Full size image Fig. 14 shows that the probability of overflow
    in fog computing systems grows with request rate, although the increase is much
    smaller in systems with a greater number of FC nodes. Systems with 30 FC nodes,
    for example, have a significantly more stable profile against growing loads than
    systems with just 5 FC nodes, which exhibit a quick rise in overflow probability.
    This conclusion implies that increasing the number of FC nodes may effectively
    lower the risk of data loss and system overload, emphasizing the significance
    of adequate resource allocation to guarantee system reliability as demand escalates.
    Fig. 14 Trade-offs in fog computing node design: visualizing overflow probability
    across different system configurations Full size image Fig. 15 depicts the loss
    rate as a function of the request rate, expressed as requests per millisecond,
    across various fog computing (FC) node configurations. The configurations shown
    are one, four, five, and seven FC nodes. The illustration shows that increasing
    the number of FC nodes in a fog computing system minimizes data packet loss, particularly
    at higher request rates. With increasing requests, the loss rate of a single-node
    system increases significantly, but systems with more nodes, notably 7 FC nodes,
    demonstrate substantially lower loss rates, suggesting improved data processing
    capabilities. This discovery has implications for building fog computing architectures
    where data loss reduction is critical, implying that a greater node count may
    contribute to enhanced reliability. Fig. 15 The effect of node count on data loss
    rate in fog computing environments Full size image The graph Fig. 16 shows that
    the mean system length, which can be interpreted as the average number of queued
    jobs in fog computing systems, grows in proportion to the request rate. The rise
    is more noticeable in systems with fewer FC nodes, with the 5-node configuration
    exhibiting fast queue length expansion. The 30-node topology, on the other hand,
    maintains a substantially shorter mean system length throughout the same range
    of request rates, suggesting a more resilient handling of growing loads. This
    implies that a greater number of FC nodes may reduce work buildup, which is critical
    for sustaining efficient operations under heavy demand. Fig. 16 Impact of arrival
    rate on mean system length across varied server counts in network systems Full
    size image The graph Fig. 17 depicts how the effective rate of health data arrivals
    in fog computing systems improves with the request rate. A network with 5 FC nodes
    has a sharp improvement in efficiency, but networks with more nodes have a more
    gradual increase. According to the statistics, adding additional nodes does not
    increase data processing speeds correspondingly, indicating a possible upper limit
    to the scalability and efficiency of FC nodes. This knowledge is critical for
    developing FC systems because it shows that there is an ideal node count beyond
    which adding additional nodes does not provide substantial increases in data processing
    speeds. Fig. 17 Effective data arrival rates across different fog computing node
    configurations in network systems Full size image The graph Fig. 18 emphasizes
    the important link between the service rates of fog computing nodes and the total
    response time of the system, particularly when the load, as measured by request
    rate (\\(\\lambda\\)), increases. Higher service rates (\\(\\mu _{FC} = 3000\\))
    are proven to maintain reduced response times, closely coinciding with the SLA
    latency threshold even under heavy traffic, indicating a strong processing capability.
    Systems with lower service rates (\\(\\mu _{FC} = 1000\\)) show a significant
    rise in response time, suggesting the possibility of SLA breaches with increasing
    loads. This information is critical for system optimization, underscoring the
    necessity to enhance the service rates of FC nodes to meet SLAs and maintain efficient
    system operation in the face of rising demand. Fig. 18 Impact of service rate
    on reducing system response time under variable loads Full size image The graph
    Fig. 19 shows that greater node service rates (\\(\\mu _{1}\\) values) retain
    decreased response times under rising workloads, with systems operating best at
    \\(\\mu _{1} = 3000\\). As demand grows, lower service rate systems rapidly exceed
    the SLA latency barrier, emphasizing the significance of service rate optimization
    to fulfill SLA criteria and enhance system performance. Fig. 19 Balancing service
    rate and load to achieve SLA response time objectives Full size image According
    to our findings, the link between workload changes and reaction times in a healthcare
    monitoring system using fixed cloud or fog computing resources is direct and substantial.
    Longer reaction times occur from increased effort, whereas shorter ones result
    from decreased workload. This emphasizes the need of healthcare service providers
    being able to dynamically scale computer resources in response to changing workload
    demands while maintaining continual alignment with Service Level Agreement (SLA)
    conditions. Because of the nature of healthcare systems, health requirements alter
    frequently. It is critical to dynamically assign the bare minimum of computer
    resources required to fulfill SLA requirements, changing these resources as needed.
    The key difficulty is to keep computing resources to a minimum at all times to
    meet SLA conditions, avoiding both excess and deficiency. Over-allocation results
    in over-provisioning and inflated expenses, while under-allocation increases the
    chance of SLA violation owing to under-provisioning. Consequently, the implementation
    of auto-scalability and elasticity is key to avoiding both extremes of resource
    provisioning. This strategy is essential for healthcare service providers to precisely
    gauge and meet the computing resource needs for handling varying healthcare workloads
    while adhering to SLA commitments. Our proposed model aims to guide healthcare
    providers in maintaining a balance in computing resources, responsive to workload
    changes, and consistent with SLA obligations. Table 7 breaks down the complex
    relationship between workload, system response, and resource management in a healthcare
    monitoring context. It outlines the key findings from our research or observations
    and translates these into actionable insights for healthcare monitoring systems,
    emphasizing the importance of dynamic, responsive, and efficient resource management.
    Table 7 Summary of key findings and implications for healthcare monitoring systems
    Full size table 6 Conclusion and future works This study has advanced the field
    of Healthcare Management Systems (HMS) by demonstrating the integral roles of
    fog computing, machine learning, and queuing models in addressing critical challenges
    such as latency, data classification, and resource allocation efficiency. Our
    analytical model, integrating Internet of Medical Things (IoMT) devices, fog computing,
    and cloud computing (both private and public), provides a nuanced understanding
    of resource optimization strategies that meet the quality of service (QoS) and
    service level agreement (SLA) response time requirements in e-health systems.
    Central to our findings is the use of queuing networks to represent cloud environments.
    This approach has enabled the development of predictive formulas for system response
    times and resource requirements, showcasing the potential for significant cost
    reductions while adhering to key performance metrics. Notably, our model underscores
    the feasibility of maintaining high-performance standards within cost-effective
    frameworks. Looking ahead, the model’s real-world applicability and scalability
    remain paramount for future exploration. Rigorous case studies, tailored to various
    healthcare scenarios and parameters, are planned to validate and refine our approach.
    These studies aim to extend the model’s utility across diverse healthcare settings,
    providing a roadmap for its practical implementation. Moreover, future research
    should delve into the integration of emerging communication technologies like
    5 G, which promise to further revolutionize healthcare monitoring systems. The
    intersection of these technologies with our proposed model holds the potential
    for creating smarter, more efficient, and patient-centric healthcare services,
    thereby marking a significant leap in healthcare management and delivery. Data
    availability The data supporting the findings of this study are available in the
    PhysioBank database, accessible freely via the PhysioNet platform (http://www.physionet.org).
    PhysioNet provides not only electronic access to PhysioBank data but also to PhysioToolkit
    software. References Zhao, J., et al.: Wearable optical sensing in the medical
    internet of things (MIoT) for pervasive medicine: opportunities and challenges.
    Acs Photonics 9(8), 2579-2599.3 (2022) Article   MathSciNet   CAS   Google Scholar   Saidi,
    K., Bardou, D.: Task scheduling and VM placement to resource allocation in cloud
    computing: challenges and opportunities. Clust. Comput. 26, 3069–3087 (2023).
    https://doi.org/10.1007/s10586-023-04098-4 Article   Google Scholar   Zhang, Qi.,
    et al.: Cloud computing: state-of-the-art and research challenges. J. Internet
    Services Appl. 1(1), 7–18 (2010). https://doi.org/10.1007/s13174-010-0007-6.3
    Article   CAS   Google Scholar   Chen, Y., Chen, S., Li, K.C., et al.: DRJOA:
    intelligent resource management optimization through deep reinforcement learning
    approach in edge computing. Clust. Comput. 26, 2897–2911 (2023). https://doi.org/10.1007/s10586-022-03768-z
    Article   Google Scholar   El Kafhali, S., Chahir, C., Hanini, M., & Salah, K.
    (2019, October). Architecture to manage Internet of Things Data using Blockchain
    and Fog Computing. In Proceedings of the 4th International Conference on Big Data
    and Internet of Things (pp. 1-8) Zhang, H., et al.: Graphene-enabled wearable
    sensors for healthcare monitoring. Biosensors Bioelectronics 197, 113777 (2022)
    Article   CAS   PubMed   Google Scholar   Sreedevi, A.G., et al.: Application
    of cognitive computing in healthcare, cybersecurity, big data, and IoT: a literature
    review. Inf. Process. Manage. 59(2), 102888 (2022) Article   Google Scholar   Pierre,
    N., et al.: Early hepatocellular carcinoma detection using magnetic resonance
    imaging is cost-effective in high-risk patients with cirrhosis. JHEP Rep. 4(1),
    100390 (2022) Article   Google Scholar   El Kafhali, S., et al.: Dynamic scalability
    model for containerized cloud services. Arabian J. Sci. Eng. Springer 45(12),
    10693–10708 (2020) Article   Google Scholar   El Kafhali, S., Salah, K., & Alla,
    S. B. (2018). Performance evaluation of IoT-fog-cloud deployment for healthcare
    services. In 2018 4th International Conference on Cloud Computing Technologies
    and Applications (Cloudtech) (pp. 1-6). IEEE Chan, M. D., et al.: Smart wearable
    systems: current status and future challenges. Artif. Intell. Med. 56(3), 137–156
    (2012) Article   PubMed   Google Scholar   Liu, H., et al.: Recent progress in
    the fabrication of flexible materials for wearable sensors. Biomater. Sci. 10(3),
    614632 (2022). https://doi.org/10.1039/d1bm01136g Article   MathSciNet   CAS   Google
    Scholar   Shaik, T., et al.: Personalized activity monitoring using stacked federated
    learning. Knowledge-Based Syst. 257(12), 109929 (2022). https://doi.org/10.1016/j.knosys.2022.109929
    Article   Google Scholar   Abdellah, A., et al.: Minimization of Task Offloading
    Latency for COVID-19 IoT Devices, The International Conference on Intelligent
    Systems and Smart Technologies. Springer, Settat (2023) Google Scholar   Bousefsaf,
    F., et al.: 3d convolutional neural networks for remote pulse rate measurement
    and mapping from facial video. Appl. Sci. 9(20), 4364 (2019). https://doi.org/10.3390/app9204364
    Article   Google Scholar   Cho, Y., et al. (2017). DeepBreath: Deep learning of
    breathing patterns for automatic stress recognition using low-cost thermal imaging
    in unconstrained settings. In the Seventh International Conference on Affective
    Computing and Intelligent Interaction (ACII). IEEE Khalid, W. B., et al. (2022).
    Contactless vitals measurement robot. In 8th International Conference on Automation,
    Robotics and Applications (ICARA). IEEE Laurie, J., et al.: An evaluation of a
    video magnification-based system for respiratory rate monitoring in an acute mental
    health setting. Int. J. Med. Inf. 148, 104378 (2021) Article   CAS   Google Scholar   El-Rashidy,
    N., et al.: Mobile health in remote patient monitoring for chronic diseases: principles,
    trends, and challenges. Diagnostics 11(4), 607 (2021). https://doi.org/10.3390/diagnostics11040607
    Article   PubMed   PubMed Central   Google Scholar   Sharma, P., et al. (2018).
    Sleep scoring with a UHF RFID tag by near-field coherent sensing. In 2018 IEEE/MTT-s
    International Microwave Symposium IMS. IEEE. https://doi.org/10.1109/MWSYM.2018.8439216
    Hui, X.,et al. (2018). Accurate extraction of heartbeat intervals with near-field
    coherent sensing. In 2018 IEEE International Conference on Communications (ICC).
    IEEE. https://doi.org/10.1109/icc.2018.8423000 Uddin, M.Z.: A wearable sensor-based
    activity prediction system to facilitate edge computing in smart healthcare system.
    J. Parallel Distrib. Comput. 123, 46–53 (2019) Article   Google Scholar   Vimal,
    S., et al.: Iot based smart health monitoring with cnn using edge computing. J.
    Internet Technol. 22(1), 173–185 (2021) MathSciNet   Google Scholar   Siam, A.I.,
    et al.: Secure health monitoring communication systems based on IoT and cloud
    computing for medical emergency applications. Comput. Intell. Neurosci. 2021,
    1–23 (2021). https://doi.org/10.1155/2021/8016525 Article   Google Scholar   Taheri-abed,
    S., Eftekhari Moghadam, A.M., Rezvani, M.H.: Machine learning-based computation
    offloading in edge and fog: a systematic review. Clust. Comput. 26, 3113–3144
    (2023). https://doi.org/10.1007/s10586-023-04100-z Article   Google Scholar   Park,
    S., et al.: Clustering insomnia patterns by data from wearable devices: algorithm
    development and validation study. JMIR mHealth uHealth 7(12), e14473 (2019) Article   PubMed   PubMed
    Central   Google Scholar   Sabry, F., et al.: Towards on-device dehydration monitoring
    using machine learning from wearable device’s data. Sensors 22(5), 1887 (2022)
    Article   ADS   PubMed   PubMed Central   Google Scholar   Weddell, A.S., et al.:
    Toward edge machine learning with RRAM-based hybrid digital/analog computing.
    IEEE Trans. Circuits Syst. II: Exp. Briefs 68(9), 3502–3506 (2021) Google Scholar   Xiao,
    F., et al.: A data reduction scheme for physiological signal processing on wearable
    devices. IEEE Sens. J. 20(10), 5589–5598 (2020) Google Scholar   Chen, F., et
    al.: Dynamic scheduling of wearable sensor data with minimum delay and energy
    consumption. IEEE Trans. Mobile Comput. 19(2), 359–371 (2020) Google Scholar   Cai,
    M., et al.: Joint computation offloading and resource allocation for wearable
    devices in cloud computing. IEEE Internet Things J. 8(4), 2504–2514 (2021) Google
    Scholar   Yuan, J., Zhu, R.: A fully self-powered wearable monitoring system with
    systematically optimized flexible thermoelectric generator. Appl. Energy 271,
    115250 (2020). https://doi.org/10.1016/j.apenergy.2020.115250 Article   Google
    Scholar   X. Fafoutis, et al. (2018) Extending the Battery Lifetime of Wearable
    Sensors with Embedded Machine Learning, In: Proceedings of the IEEE 4th World
    Forum on Internet of Gings (WF-IoT), pp. 269–274, Singapore Zhang, Y., et al.:
    Motion artifact reduction in wearable photoplethysmography using gyroscope signals
    and spectral filtering. J. Biomed. Opt. 26(4), 047003 (2021) Google Scholar   Bent,
    B., et al.: Investigating sources of inaccuracy in wearable optical heart rate
    sensors. Npj Digital Med. 3(18), 2020 (2020) Google Scholar   Singh, J., et al.:
    Security and privacy in edge computing: a review. IEEE Access 7, 18256–18277 (2019)
    Google Scholar   El Kafhali, S., Salah, K.: Performance modeling and analysis
    of IoT-enabled healthcare monitoring systems. IET Networks 8(1), 48–58 (2019)
    Article   Google Scholar   Kishor, A., et al.: A novel fog computing approach
    for minimization of latency in healthcare using machine learning. Int. J. Interact.
    Multimed 6(7), 7 (2021) Google Scholar   Kumar, B.S., et al.: A novel architecture
    based on deep learning for scene image recognition. Int. J. Psychosoc. Rehabil.
    23(1), 400–04 (2019) Google Scholar   Badawi, O., et al.: Making big data useful
    for health care: a summary of the inaugural MIT critical data conference. JMIR
    Med. Inf. 2, e3447 (2014) Google Scholar   Endo, H., Uchino, S., Hashimoto, S.,
    et al.: Development and validation of the predictive risk of death model for adult
    patients admitted to intensive care units in Japan: an approach to improve the
    accuracy of healthcare quality measures. J. Intensive Care 9, 18 (2021) Article   PubMed   PubMed
    Central   Google Scholar   Abdulhafedh, A.: Comparison between common statistical
    modeling techniques used in research, including discriminant analysis vs logistic
    regression, ridge regression vs LASSO, and decision tree vs random forest. Open
    Access Library J. 9(2), 1–19 (2022) Google Scholar   La, Q.D., et al.: Enabling
    intelligence in fog computing to achieve energy and latency reduction. Digit.
    Commun. Networks 5(1), 3–9 (2019) Article   Google Scholar   El Kafhali, S., Hanini,
    M.: Stochastic modeling and analysis of feedback control on the QoS VoIP traffic
    in a single cell IEEE 802.16e networks. IAENG Int. J. Comput. Sci. 44(1), 19–28
    (2017) Google Scholar   Little, J.D.C.: A proof for the queuing formula: L = \\(\\lambda\\)W.
    Operations Res. 9(3), 383–87 (1961) Article   MathSciNet   Google Scholar   Goldberger,
    A.L., et al.: PhysioBank, PhysioToolkit, and PhysioNet: components of a new research
    resource for complex physiologic signals. Circulation 101(23), e215–e220 (2000)
    Article   CAS   PubMed   Google Scholar   Soni, M., et al.: A review on privacy-preserving
    data preprocessing. J. Cybersecur. Inf. Manag. 4(2), 16–6 (2021) MathSciNet   Google
    Scholar   Kocheturov, A., et al.: Massive datasets and machine learning for computational
    biomedicine: trends and challenges. Ann. Operations Res. 276, 5–34 (2019) Article   MathSciNet   Google
    Scholar   Vapnik, V.N., Vapnik, V.: Statistical Learning Theory, vol. 2. Wiley,
    New York (1998) Google Scholar   Shafer, G., Pearl, J.: Readings in Uncertain
    Reasoning. Morgan Kaufmann Publishers Inc., Burlington (1990) Google Scholar   Hosmer,
    D.W., Jr., Lemeshow, S.: Applied Logistic Regression. Wiley, New York (2004) Google
    Scholar   Buntine, W.: Learning Classification Trees. Artificial Intelligence
    Frontiers in Statistics, pp. 182–201. Chapman and Hall/CRC, Boca Raton (2020)
    Google Scholar   Download references Funding There is no funding for this research
    paper. Author information Authors and Affiliations Computer, Networks, Mobility
    and Modeling Laboratory (IR2M), Faculty of Sciences and Techniques, Hassan First
    University of Settat, 26000, Settat, Morocco Abdellah Amzil, Mohamed Abid, Mohamed
    Hanini & Said El Kafhali Engineering, Mathematics and Informatics Laboratory (IMI),
    Faculty of Sciences, Ibn Zohr University of Agadir, Agadir, Morocco Abdellah Zaaloul
    Corresponding author Correspondence to Abdellah Amzil. Ethics declarations Competing
    interests The authors declare that they have no known competing financial interests
    or personal relationships that could have appeared to influence the work reported
    in this paper. Additional information Publisher''s Note Springer Nature remains
    neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society
    or other partner) holds exclusive rights to this article under a publishing agreement
    with the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Amzil, A., Abid, M., Hanini, M. et al. Stochastic analysis of fog
    computing and machine learning for scalable low-latency healthcare monitoring.
    Cluster Comput (2024). https://doi.org/10.1007/s10586-024-04285-x Download citation
    Received 29 September 2023 Revised 06 December 2023 Accepted 04 January 2024 Published
    21 February 2024 DOI https://doi.org/10.1007/s10586-024-04285-x Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Fog computing Machine learning Healthcare monitoring Queuing model Performance
    analysis Use our pre-submission checklist Avoid common mistakes on your manuscript.
    Sections Figures References Abstract Introduction Related work System architecture
    Mathematical modeling Simulation and numerical results Conclusion and future works
    Data availability References Funding Author information Ethics declarations Additional
    information Rights and permissions About this article Advertisement Discover content
    Journals A-Z Books A-Z Publish with us Publish your research Open access publishing
    Products and services Our products Librarians Societies Partners and advertisers
    Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy
    choices/Manage cookies Your US state privacy rights Accessibility statement Terms
    and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic
    Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024
    Springer Nature"'
  inline_citation: '>'
  journal: Cluster Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Stochastic analysis of fog computing and machine learning for scalable low-latency
    healthcare monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kumar N.
  citation_count: '0'
  description: Soft computing uses techniques such as fuzzy logic, neural networks,
    and evolutionary algorithms to solve complex problems. By combining these techniques
    with Cloud, Fog, and IoT-based computing systems, system designers can create
    more efficient, reliable, and secure systems. For example, cloud computing can
    be used to store data and provide computing power for applications. Fog computing
    can be used to provide low latency and high throughput for applications that require
    real-time data. IoT-based computing systems can be used to collect data from sensors
    and devices and send it to the cloud for further processing. By combining these
    technologies, system designers can create systems that are more efficient, reliable,
    and secure. In addition, soft computing techniques can be used to optimize the
    performance of these systems. For example, fuzzy logic can be used to improve
    the accuracy of data analysis; neural networks can be used to detect anomalies
    in data; and evolutionary algorithms can be used to optimize system parameters.
    By combining these techniques with Cloud, Fog, and IoT-based computing systems,
    system designers can create systems that are more efficient, reliable, and secure.
    The integration of Soft Computing with Cloud, Fog, and IoT-based computing system
    for system has become increasingly important in recent years. Soft Computing is
    an umbrella term for various techniques such as fuzzy logic, artificial neural
    networks, and evolutionary computing, which are used to solve complex problems.
    Cloud Computing uses a network of remote servers hosted on the Internet to store,
    manage, and process data, rather than a local server or a personal computer. Fog
    Computing is a distributed computing system that extends Cloud Computing capabilities
    to the edge of the network, allowing for data processing to be done closer to
    the source of the data. Internet of Things (IoT) based Computing systems are networks
    of physical devices, vehicles, buildings, and other items embedded with electronics,
    software, sensors, and network connectivity, which enable these objects to collect
    and exchange data.
  doi: 10.1201/9781032716718-12
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeComputer ScienceSoftware Engineering & Systems DevelopmentReal-Time
    SystemsSoft Computing Principles and Integration for Real-Time Service-Oriented
    ComputingApplications of Soft Computing Integration with Cloud, Fog, and IoT-Based
    Computing System for Systems Chapter Applications of Soft Computing Integration
    with Cloud, Fog, and IoT-Based Computing System for Systems ByNarendra Kumar Book
    Soft Computing Principles and Integration for Real-Time Service-Oriented Computing
    Edition 1st Edition First Published 2024 Imprint Auerbach Publications Pages 18
    eBook ISBN 9781032716718 Share ABSTRACT Soft computing uses techniques such as
    fuzzy logic, neural networks, and evolutionary algorithms to solve complex problems.
    By combining these techniques with Cloud, Fog, and IoT-based computing systems,
    system designers can create more efficient, reliable, and secure systems. For
    example, cloud computing can be used to store data and provide computing power
    for applications. Fog computing can be used to provide low latency and high throughput
    for applications that require real-time data. IoT-based computing systems can
    be used to collect data from sensors and devices and send it to the cloud for
    further processing. By combining these technologies, system designers can create
    systems that are more efficient, reliable, and secure. In addition, soft computing
    techniques can be used to optimize the performance of these systems. For example,
    fuzzy logic can be used to improve the accuracy of data analysis; neural networks
    can be used to detect anomalies in data; and evolutionary algorithms can be used
    to optimize system parameters. By combining these techniques with Cloud, Fog,
    and IoT-based computing systems, system designers can create systems that are
    more efficient, reliable, and secure. The integration of Soft Computing with Cloud,
    Fog, and IoT-based computing system for system has become increasingly important
    in recent years. Soft Computing is an umbrella term for various techniques such
    as fuzzy logic, artificial neural networks, and evolutionary computing, which
    are used to solve complex problems. Cloud Computing uses a network of remote servers
    hosted on the Internet to store, manage, and process data, rather than a local
    server or a personal computer. Fog Computing is a distributed computing system
    that extends Cloud Computing capabilities to the edge of the network, allowing
    for data processing to be done closer to the source of the data. Internet of Things
    (IoT) based Computing systems are networks of physical devices, vehicles, buildings,
    and other items embedded with electronics, software, sensors, and network connectivity,
    which enable these objects to collect and exchange data. Previous Chapter Next
    Chapter Your institution has not purchased this content. Please get in touch with
    your librarian to recommend this.  To purchase a print version of this book for
    personal use or request an inspection copy  GO TO ROUTLEDGE.COM  Policies Privacy
    Policy Terms & Conditions Cookie Policy Journals Taylor & Francis Online Corporate
    Taylor & Francis Group Help & Contact Students/Researchers Librarians/Institutions
    Connect with us Registered in England & Wales No. 3099067 5 Howick Place | London
    | SW1P 1WG © 2024 Informa UK Limited"'
  inline_citation: '>'
  journal: Soft Computing Principles and Integration for Real-Time Service-Oriented
    Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Applications of Soft Computing Integration with Cloud, Fog, and IoT-Based
    Computing System for Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Swapna B.
  - Divya V.
  citation_count: '0'
  description: The huge volume of data produced by IoT procedures needs the processing
    power and space for storage provided by cloud, edge, and fog computing systems.
    Each of these ways of computing has benefits as well as drawbacks. Cloud computing
    improves the storage of information and computational capability while increasing
    connection delay. Edge computing and fog computing offer similar advantages with
    decreased latency, but they have restricted storage, capacity, and coverage. Initially,
    optimization has been employed to overcome the issue of traffic dumping. Conversely,
    conventional optimization cannot keep up with the tight latency requirements of
    decision-making in complex systems ranging from milliseconds to sub-seconds. As
    a result, ML algorithms, particularly reinforcement learning, are gaining popularity
    since they can swiftly handle offloading issues in dynamic situations involving
    certain unidentified data. We conduct an analysis of the literature to examine
    the different techniques utilized to tackle this latency-aware intelligent task
    offloading issue schemes for cloud, edge, and fog computing. The lessons acquired
    consequently, from these surveys are then presented in this report. Lastly, we
    identify some additional avenues for study and problems that must be overcome
    in order to attain the lowest latency in the task offloading system.
  doi: 10.15622/ia.21.1.10
  full_citation: '>'
  full_text: '>

    "VISIT DOI.ORG DOI NOT FOUND 10.15622/ia.21.1.10 This DOI cannot be found in the
    DOI System. Possible reasons are: The DOI is incorrect in your source. Search
    for the item by name, title, or other metadata using a search engine. The DOI
    was copied incorrectly. Check to see that the string includes all the characters
    before and after the slash and no sentence punctuation marks. The DOI has not
    been activated yet. Please try again later, and report the problem if the error
    continues. WHAT CAN I DO NEXT? If you believe this DOI is valid, you may report
    this error to the responsible DOI Registration Agency using the form here. You
    can try to search again from DOI.ORG homepage REPORT AN ERROR DOI: URL of Web
    Page Listing the DOI: Your Email Address: Additional Information About the Error:
    More information on DOI resolution: DOI Resolution Factsheet The DOI Handbook
    Privacy Policy Copyright © 2023 DOI Foundation. The content of this site is licensed
    under a Creative Commons Attribution 4.0 International License. DOI®, DOI.ORG®,
    and shortDOI® are trademarks of the DOI Foundation."'
  inline_citation: '>'
  journal: Informatics and Automation
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: LATENCY AWARE INTELLIGENT TASK OFFLOADING SCHEME FOR EDGE-FOG-CLOUD COMPUTING
    – A REVIEW
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kanani P.
  - Vasoya A.
  - Shah K.
  - Kothari N.
  - Patil N.
  - Pandya G.
  - Padole M.
  citation_count: '0'
  description: 'The healthcare industry relies on efficient and fast decision making.
    This paper aims to expand the Fog computing and Distributed computing domains
    to optimize quality of service (QoS) in order to facilitate IoT based healthcare
    applications with low latency requirements and developing a smart fog gateway
    equipped with an optimized fog algorithm. The purpose of this study is to optimize
    real-time healthcare data processing using Fog computing, ensuring dependable,
    rapid decision-making while minimizing delays caused by data transmission and
    computation. This is also known as Health-as-a-service (HaaS). We conduct an electrocardiography
    (ECG) analysis utilizing three computing paradigms: Cloud computing, Fog computing,
    and a heterogeneous distributed Fog computing setup employing the dynamic OptiFog
    algorithm. This algorithm effectively manages computational resources within the
    distributed Fog environment, utilizing Raspberry Pi clusters to enhance performance
    during worst-case scenarios. The response time is measured using Short Message
    Service (SMS). The OptiFog node exhibited a response better than the Fog node
    and the cloud node. The OptiFog algorithm not only takes into account different
    computing parameters like number of cores, memory usage, CPU utilization and response
    time of the computing node but also assigns dynamic priorities to these parameters
    to get the best possible processing available. Based on the workload of the task/node,
    it dynamically decides the job size to save the network bandwidth and to reduce
    the network overhead. In conclusion, the proposed work demonstrates that optimizing
    Fog computing with the dynamic OptiFog algorithm is an effective approach to meet
    low-latency requirements in IoT-based healthcare applications making it a valuable
    addition to the Health-as-a-Service (Haas) framework for real-time healthcare
    data processing.'
  doi: 10.22266/ijies2024.0229.36
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: '>'
  journal: International Journal of Intelligent Engineering and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Optimization of Health-as-a-Service Using OptiFog Algorithm
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tomar A.
  - Tripathi S.
  citation_count: '0'
  description: The increasing number of vehicles has resulted in a tremendous growth
    of data in vehicular communications. Cloud-based models are inefficient for handling
    this large data due to high latency and bandwidth requirements. To address this,
    fog computing-based vehicular network models have been proposed for low latency
    and immediate responses. However, securing data transfer between fog servers and
    vehicles on public channels is essential to prevent malicious attacks. Existing
    authenticated key agreement schemes provide security but often incur high computational
    costs and vulnerability to attacks. Thus, this paper proposes a blockchain-based
    authenticated key agreement scheme for the fog computing-enabled vehicular network.
    The proposed scheme integrates blockchain into fog-based Vehicular Ad-hoc Network
    (VANET), where fog servers and the cloud servers serve as blockchain nodes for
    seamless re-authentication of a moving vehicle. For secure communication, the
    proposed scheme uses the Chebyshev polynomial to achieve low computational cost
    and establishes a common session key between cloud server, fog server, and vehicle.
    The formal security proof of the proposed scheme is carried out using Real-Or-Random
    (ROR) model. Finally, the Hyperledger Fabric and cryptographic libraries have
    been used for the experimental analysis to demonstrate the proposed scheme&#x0027;s
    communication and computational efficiency.
  doi: 10.1109/TMC.2024.3357599
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Mobile C...
    >Early Access A Chebyshev Polynomial-Based Authentication Scheme Using Blockchain
    Technology for Fog-Based Vehicular Network Publisher: IEEE Cite This PDF Ashish
    Tomar; Sachin Tripathi All Authors 70 Full Text Views Abstract Authors Keywords
    Metrics Abstract: The increasing number of vehicles has resulted in a tremendous
    growth of data in vehicular communications. Cloud-based models are inefficient
    for handling this large data due to high latency and bandwidth requirements. To
    address this, fog computing-based vehicular network models have been proposed
    for low latency and immediate responses. However, securing data transfer between
    fog servers and vehicles on public channels is essential to prevent malicious
    attacks. Existing authenticated key agreement schemes provide security but often
    incur high computational costs and vulnerability to attacks. Thus, this paper
    proposes a blockchain-based authenticated key agreement scheme for the fog computing-enabled
    vehicular network. The proposed scheme integrates blockchain into fog-based Vehicular
    Ad-hoc Network (VANET), where fog servers and the cloud servers serve as blockchain
    nodes for seamless re-authentication of a moving vehicle. For secure communication,
    the proposed scheme uses the Chebyshev polynomial to achieve low computational
    cost and establishes a common session key between cloud server, fog server, and
    vehicle. The formal security proof of the proposed scheme is carried out using
    Real-Or-Random (ROR) model. Finally, the Hyperledger Fabric and cryptographic
    libraries have been used for the experimental analysis to demonstrate the proposed
    scheme''s communication and computational efficiency. Published in: IEEE Transactions
    on Mobile Computing ( Early Access ) Page(s): 1 - 16 Date of Publication: 23 January
    2024 ISSN Information: DOI: 10.1109/TMC.2024.3357599 Publisher: IEEE Authors Keywords
    Metrics More Like This Design of Blockchain and ECC-Based Robust and Efficient
    Batch Authentication Protocol for Vehicular Ad-Hoc Networks IEEE Transactions
    on Intelligent Transportation Systems Published: 2024 BCPPA: A Blockchain-Based
    Conditional Privacy-Preserving Authentication Protocol for Vehicular Ad Hoc Networks
    IEEE Transactions on Intelligent Transportation Systems Published: 2021 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Mobile Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Chebyshev Polynomial-Based Authentication Scheme Using Blockchain Technology
    for Fog-Based Vehicular Network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Panigrahi S.K.
  - Goswami V.
  - Mund G.B.
  - Barik R.K.
  citation_count: '0'
  description: Modern Information and Communication Technology, Internet of Spatial
    Things(IoT), cloud, fog, and mist computing enable an expansion of real-time geospatial
    applications in crime analysis. Due to their sensitivity to latency and QoS, these
    applications must process at the network’s edge, not on the central cloud servers.
    Mist nodes have the ability to cache low-volume geographical data that is regularly
    requested and then process that data using lightweight applications. Display the
    results of the geospatial data processing on the client’s devices or systems in
    accordance with their requirements.Computing in the mist and fog have been the
    focus of a significant amount of study recently, particularly in geospatial application
    contexts such as crime analysis and visualization. Real-time geospatial crime
    data visualization can be more efficient and productive through the mist computing
    framework. By keeping this in mind, the present research paper proposes the IoST-Mist–Fog–Cloud
    framework for the visualization of crime data. With the help of this proposed
    framework, it visualizes the geospatial crime data through the thin client and
    mobile client environment. In addition to this, it provides a one-of-a-kind analytical
    model that investigates a state-dependent service queuing strategy using the IoST–Mist–Fog–Cloud
    framework and the influence of state-dependent service time on the system’s overall
    performance. It explains some of the system’s characteristics, and numerical evaluations
    and simulations validate the system’s functionality. According to the evaluation’s
    findings, it can attain an adequate degree of precision and successfully offload
    tasks when it uses the framework presented.
  doi: 10.1007/s42979-023-02400-0
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home SN Computer Science Article Performance
    Evaluation of IoST–Mist–Fog–Cloud Framework for Geospatial Crime Data Visualization:
    A State Dependent Queueing Approach Original Research Published: 11 December 2023
    Volume 5, article number 85, (2024) Cite this article SN Computer Science Aims
    and scope Submit manuscript Sunil K. Panigrahi, Veena Goswami, G. B. Mund & Rabindra
    K. Barik   46 Accesses Explore all metrics Abstract Modern Information and Communication
    Technology, Internet of Spatial Things(IoT), cloud, fog, and mist computing enable
    an expansion of real-time geospatial applications in crime analysis. Due to their
    sensitivity to latency and QoS, these applications must process at the network’s
    edge, not on the central cloud servers. Mist nodes have the ability to cache low-volume
    geographical data that is regularly requested and then process that data using
    lightweight applications. Display the results of the geospatial data processing
    on the client’s devices or systems in accordance with their requirements.Computing
    in the mist and fog have been the focus of a significant amount of study recently,
    particularly in geospatial application contexts such as crime analysis and visualization.
    Real-time geospatial crime data visualization can be more efficient and productive
    through the mist computing framework. By keeping this in mind, the present research
    paper proposes the IoST-Mist–Fog–Cloud framework for the visualization of crime
    data. With the help of this proposed framework, it visualizes the geospatial crime
    data through the thin client and mobile client environment. In addition to this,
    it provides a one-of-a-kind analytical model that investigates a state-dependent
    service queuing strategy using the IoST–Mist–Fog–Cloud framework and the influence
    of state-dependent service time on the system’s overall performance. It explains
    some of the system’s characteristics, and numerical evaluations and simulations
    validate the system’s functionality. According to the evaluation’s findings, it
    can attain an adequate degree of precision and successfully offload tasks when
    it uses the framework presented. This is a preview of subscription content, log
    in via an institution to check access.  References Barik RK, Dubey H, Samaddar
    AB, Gupta RD, Ray PK. Foggis: Fog computing for geospatial big data analytics.
    In: 2016 IEEE Uttar Pradesh Section International Conference on Electrical, Computer
    and Electronics Engineering (UPCON), IEEE; 2016. pp. 613–618. Mas L, Vilaplana
    J, Mateo J, Solsona F. A queuing theory model for fog computing. J Supercomput.
    2022;78(8):11138–55. Article   Google Scholar   Barik RK, Dubey H, Mankodiya K,
    Sasane SA, Misra C. Geofog4health: a fog-based sdi framework for geospatial health
    big data analysis. J Ambient Intellig Hum Comput. 2019;10(2):551–67. Article   Google
    Scholar   Goswami V, Panda G. Multimedia content delivery services in the cloud
    with partial sleep and abandonment. J. Supercomput 2022;1–24 Shahid H, Shah MA,
    Almogren A, Khattak HA, Din IU, Kumar N, Maple C. Machine learning-based mist
    computing enabled internet of battlefield things. ACM Trans Internet Technol (TOIT).
    2021;21(4):1–26. Article   Google Scholar   Barik RK, Misra C, Lenka RK, Dubey
    H, Mankodiya K. Hybrid mist-cloud systems for large scale geospatial big data
    analytics and processing: opportunities and challenges. Arabian J Geosci. 2019;12(2):1–15.
    Article   Google Scholar   Bekker R. Validating state-dependent queues in health
    care. Queueing Syst. 2022;100(3):505–7. Article   MathSciNet   Google Scholar   Lumb
    VR, Rani I. Analytically simple solution to discrete-time queue with catastrophes,
    balking and state-dependent service. Int J Syst Assur Eng Manag. 2022;13(2):783–817.
    Article   Google Scholar   Gupta V, Zhang J. Approximations and optimal control
    for state-dependent limited processor sharing queues. Stochastic Syst. 2022;12(2):205–25.
    Article   MathSciNet   MATH   Google Scholar   Nanda S, Goswami V, Brahma AN,
    Patra SS, Barik RK. Towards efficient and dynamic allocations of mist nodes for
    iost devices. In: 2022 IEEE International Conference on Electronics, Computing
    and Communication Technologies (CONECCT), IEEE; 2022. pp. 1–5. Das J, Ghosh SK,
    Buyya R. Geospatial edge-fog computing: a systematic review, taxonomy, and future
    directions. Mobile Edge Comput 2021; 47–69 Prathap BR. Geospatial crime analysis
    and forecasting with machine learning techniques. In: Artificial Intelligence
    and Machine Learning for EDGE Computing, Elsevier; 2022. pp. 87–102. Singh H,
    Kumar R, Singh A, Litoria P. Cloud gis for crime mapping. Int J Res Comput Sci.
    2012;2(3):57–60. Article   Google Scholar   El Kafhali S, Salah K. Efficient and
    dynamic scaling of fog nodes for iot devices. J Supercomput. 2017;73(12):5261–84.
    Article   Google Scholar   Barik RK, Dubey AC, Tripathi A, Pratik T, Sasane S,
    Lenka RK, Dubey H, Mankodiya K, Kumar V. Mist data: leveraging mist computing
    for secure and scalable architecture for smart and connected health. Procedia
    Comput Sci. 2018;125:647–53. Article   Google Scholar   Ketu S, Mishra PK. Cloud,
    fog and mist computing in iot: an indication of emerging opportunities. IETE Techn.
    Rev. 2021; 1–12 Galambos P. Cloud, fog, and mist computing: advanced robot applications.
    IEEE Syst Man Cybernet Magaz. 2020;6(1):41–5. Article   MathSciNet   Google Scholar   Santos
    RB. Crime analysis with crime mapping. Sage Publications 2016. Panigrahi SK, Jena
    JR, Goswami V, Patra SS, Samaddar SG, Barik RK. Performance evaluation of state
    dependent queueing based geospatial mist-assisted cloud system for crime data
    visualisation. In: 2022 3rd International Conference on Computing, Analytics and
    Networks (ICAN), IEEE; 2022. pp. 1–6. Rodrigues L, Rodrigues JJ, Serra AdB, Silva
    FA. A queueing-based model performance evaluation for internet of people supported
    by fog computing. Future Internet. 2022;14(1):23. Article   Google Scholar   El
    Kafhali S, Salah K, Alla SB. Performance evaluation of iot-fog-cloud deployment
    for healthcare services. In: 2018 4th International Conference on Cloud Computing
    Technologies and Applications (Cloudtech), IEEE, 2018. pp. 1–6. Baughman CJ. An
    introduction to GIS. In: The Crime Analyst’s Companion, Springer; 2022. pp. 105–124.
    Jubit N, Masron T. Gis for crime mapping: a case study of property crime in Kuching,
    Sarawak. J Asian Geography. 2022;1(1):25–33. Google Scholar   Ristea A, Leitner
    M. Urban crime mapping and analysis using GIS. ISPRS Int J Geo-Inform. 2020;9(9):511.
    Article   Google Scholar   Liu L. Progresses and challenges of crime geography
    and crime analysis. In: New Thinking in GIScience, Springer; 2022. pp. 349–353.
    Cheah JY, Smith JM. Generalized \\({M/G/c/c}\\) state dependent queueing models
    and pedestrian traffic flows. Queueing Syst. 1994;15:365–86. Article   MATH   Google
    Scholar   Jain R, Smith JM. Modeling vehicular traffic flow using \\({M/G/c/c}\\)
    state dependent queueing models. Transport Sci. 1997;31(4):324–36. Article   MATH   Google
    Scholar   Cruz FR, Smith JM. Approximate analysis of M/G/c/c state-dependent queueing
    networks. Comput Oper Res. 2007;34(8):2332–44. Article   MathSciNet   MATH   Google
    Scholar   Abouee-Mehrizi H, Baron O. State-dependent M/G/1 queueing systems. Queueing
    Syst. 2016;82(1–2):121–48. Article   MathSciNet   MATH   Google Scholar   Hejazi
    T-H. State-dependent resource reallocation plan for health care systems: a simulation
    optimization approach. Comput Indust Eng. 2021;159: 107502. Article   Google Scholar   Nithya
    M, Joshi GP, Sugapriya C, Selvakumar S, Anbazhagan N, Yang E, Doo IC. Analysis
    of stochastic state-dependent arrivals in a queueing-inventory system with multiple
    server vacation and retrial facility. Mathematics. 2022;10(17):3041. Article   Google
    Scholar   Yuhaski SJ, Smith JM. Modeling circulation systems in buildings using
    state dependent queueing models. Queueing Syst. 1989;4:319–38. Article   MathSciNet   MATH   Google
    Scholar   Smith JM. State-dependent queueing models in emergency evacuation networks.
    Transport Res Part B: Methodol. 1991;25(6):373–89. Article   Google Scholar   Banerjee
    S, Kanoria Y, Qian P. State dependent control of closed queueing networks. ACM
    SIGMETRICS Perform Evaluat Rev. 2018;46(1):2–4. Article   Google Scholar   Jain
    M, Sanga SS. State dependent queueing models under admission control F-policy:
    a survey. J Ambient Intellig Hum Comput. 2020;11:3873–91. Article   Google Scholar   Legros
    B. Dimensioning a queue with state-dependent arrival rates. Comput Oper Res. 2021;128:
    105179. Article   MathSciNet   MATH   Google Scholar   Khazaei H, Misic J, Misic
    VB. Performance analysis of cloud computing centers using M/G/m/m+ r queuing systems.
    IEEE Trans Parallel Distrib Syst. 2011;23(5):936–43. Article   Google Scholar   Goswami
    V, Patra SS, Mund GB. Performance analysis of cloud with queue-dependent virtual
    machines. In: 2012 1st International Conference on Recent Advances in Information
    Technology (RAIT), IEEE; 2012. pp. 357–362. Varma PS, Satyanarayana A, Sundari
    MR. Performance analysis of cloud computing using queuing models. In: 2012 International
    Conference on Cloud Computing Technologies, Applications and Management (ICCCTAM),
    IEEE; 2012. pp. 12–15. Mary NAB, Saravanan K. Performance factors of cloud computing
    data centers using [(M/G/1):([\\(\\infty\\)]/GDmodel)] queuing systems. Int J
    Grid Comput Appl. 2013;4(1):1. Google Scholar   Vakilinia S, Ali MM, Qiu D. Modeling
    of the resource allocation in cloud computing centers. Comput Networks. 2015;91:453–70.
    Article   Google Scholar   Atmaca T, Begin T, Brandwajn A, Castel-Taleb H. Performance
    evaluation of cloud computing centers with general arrivals and service. IEEE
    Trans Parallel Distrib Syst. 2015;27(8):2341–8. Article   Google Scholar   Mirtchev
    ST, Goleva RI, Atamian DK, Mirtchev MJ, Ganchev I, Stainov R. A generalized erlang-c
    model for the enhanced living environment as a service (eleaas). Cybernet Inform
    Technol. 2016;16(3):104–21. Article   Google Scholar   Goswami V, Mund GB. Computational
    analysis of multi-server discrete-time queueing system with balking, reneging
    and synchronous vacations. RAIRO-Oper Res. 2017;51(2):343–58. Article   MathSciNet   MATH   Google
    Scholar   Narman HS, Hossain MS, Atiquzzaman M, Shen H. Scheduling internet of
    things applications in cloud computing. Annals Telecommun. 2017;72:79–93. Article   Google
    Scholar   Beraldi R, Alnuweiri H. Sequential randomization load balancing for
    fog computing. In: 2018 26th International Conference on Software, Telecommunications
    and Computer Networks (SoftCOM), IEEE; 2018. pp. 1–6. Beraldi R, Alnuweiri H,
    Mtibaa A. A power-of-two choices based algorithm for fog computing. IEEE Trans
    Cloud Comput. 2018;8(3):698–709. Article   Google Scholar   Fan Q, Ansari N. Towards
    workload balancing in fog computing empowered iot. IEEE Trans Network Sci Eng.
    2018;7(1):253–62. Article   MathSciNet   Google Scholar   Al-Khafajiy M, Baker
    T, Al-Libawy H, Maamar Z, Aloqaily M, Jararweh Y. Improving fog computing performance
    via fog-2-fog collaboration. Future Gener Comput Syst. 2019;100:266–80. Article   Google
    Scholar   Beraldi R, Alnuweiri H. Exploiting power-of-choices for load balancing
    in fog computing. In: 2019 IEEE International Conference on Fog Computing (ICFC),
    IEEE; 2019. pp. 80–86. Chan S. Least loaded sharing in fog computing cluster.
    In: Proc. 15th Int. Conf. Netw. Services, 2019. pp. 27–31. Jain M, Sanga SS. Admission
    control for finite capacity queueing model with general retrial times and state-dependent
    rates. J Indust Manag Optim. 2020;16(6):2625–49. Article   MathSciNet   MATH   Google
    Scholar   Phung-Duc T. Batch arrival multiserver queue with state-dependent setup
    for energy-saving data center. Appl. Probab. Stochastic Processes 2020. 421–440.
    Casale G. Integrated performance evaluation of extended queueing network models
    with line. In: 2020 Winter Simulation Conference (WSC), IEEE; 2020. pp. 2377–2388.
    Beraldi R, Canali C, Lancellotti R, Mattia GP. Distributed load balancing for
    heterogeneous fog computing infrastructures in smart cities. Pervas Mobile Comput.
    2020;67: 101221. Article   Google Scholar   Stankevich E, Tananko I, Pagano M.
    Analysis of open queueing networks with batch services. In: International Conference
    on Information Technologies and Mathematical Modelling, Springer; 2021. pp. 40–51.
    Feitosa L, Santos L, Gonçalves G, Nguyen TA, Lee J-W, Silva FA. Internet of robotic
    things: A comparison of message routing strategies for cloud-fog computing layers
    using M/M/c/K queuing networks. In: 2021 IEEE International Conference on Systems,
    Man, and Cybernetics (SMC), IEEE; 2021. pp. 2049–2054. Mahavir Varma S, Theja
    Maguluri S. A heavy traffic theory of two-sided queues. ACM SIGMETRICS Perform
    Eval Rev. 2022;49(3):43–4. Article   MATH   Google Scholar   Goswami V, Sharma
    B, Patra SS, Chowdhury S, Barik RK, Dhaou IB. Iot-fog computing sustainable system
    for smart cities: A queueing-based approach. In: 2023 1st International Conference
    on Advanced Innovations in Smart Cities (ICAISC), IEEE; 2023. pp. 1–6. Sivasamy
    R, Paranjothi N. Modelling of a cloud platform via M/M1+ M2/1 queues of a jackson
    network. Int J Cloud Comput. 2023;12(1):63–71. Article   Google Scholar   Bergquist
    J, Elmachtoub AN. Static pricing guarantees for queueing systems. arXiv preprint
    arXiv:2305.09168 (2023) Tran-Dang H, Kim D-S. Dynamic collaborative task offloading
    for delay minimization in the heterogeneous fog computing systems. J Commun Netw.
    2023;25(2):244–52. Article   Google Scholar   Shortle JF, Thompson JM, Gross D,
    Harris CM. Fundamentals of Queueing Theory. John Wiley & Sons, 2018; 399. Bayoumi
    S, AlDakhil S, AlNakhilan E, Al Taleb E, AlShabib H. A review of crime analysis
    and visualization. case study: Maryland state, usa. In: 2018 21st Saudi Computer
    Society National Computer Conference (NCC), IEEE; 2018. pp. 1–6. Yang C, Goodchild
    M, Qunying H, Doug N, Raskin R, Robert X, Bambacus M, Fay D. Spatial cloud computing:
    how can the geospatial sciences use and help shape cloud computing? Int J Digital
    Earth. 2021;4(4):305–29. Article   Google Scholar   Das J, Mukherjee A, Ghosh
    S, Buyya R. Spatio-Fog: a green and timeliness-oriented fog computing model for
    geospatial query resolution. Simul Modell Pract Theory. 2020;100(4):1–23. Google
    Scholar   Panigrahi S, Goswami V, Apat H, Barik R, Vidyarthi A, Gupta P, Alharbi
    M. An interconnected IoT-inspired network architecture for data visualization
    in remote sensing domain. Alexandria Eng J. 2023;81(1):17–28. Article   Google
    Scholar   Panigrahi S, Goswami V, Apat H, Mund G, Das P, Barik R. PQ-Mist: priority
    queueing-assisted mist-cloud-fog system for geospatial web services. mathematics.
    2023;11(16):1–21. Article   Google Scholar   Download references Author information
    Authors and Affiliations School of Computer Engineering, Kalinga Institute of
    Industrial Technology, Bhubaneswar, 751024, Odisha, India Sunil K. Panigrahi &
    G. B. Mund School of Computer Applications, Kalinga Institute of Industrial Technology,
    Bhubaneswar, 751024, Odisha, India Veena Goswami & Rabindra K. Barik Corresponding
    author Correspondence to Rabindra K. Barik. Ethics declarations Conflict of Interest
    On behalf of all authors, the corresponding author states that there is no conflict
    of interest. All figures and contents taken from the literature are properly cited.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    This article is part of the topical collection “Diverse Applications in Computing,
    Analytics and Networks” guest edited by Archana Mantri and Sagar Juneja. Rights
    and permissions Springer Nature or its licensor (e.g. a society or other partner)
    holds exclusive rights to this article under a publishing agreement with the author(s)
    or other rightsholder(s); author self-archiving of the accepted manuscript version
    of this article is solely governed by the terms of such publishing agreement and
    applicable law. Reprints and permissions About this article Cite this article
    Panigrahi, S.K., Goswami, V., Mund, G.B. et al. Performance Evaluation of IoST–Mist–Fog–Cloud
    Framework for Geospatial Crime Data Visualization: A State Dependent Queueing
    Approach. SN COMPUT. SCI. 5, 85 (2024). https://doi.org/10.1007/s42979-023-02400-0
    Download citation Received 16 July 2023 Accepted 08 October 2023 Published 11
    December 2023 DOI https://doi.org/10.1007/s42979-023-02400-0 Keywords Cloud computing
    Fog computing Mist computing IoST Queueing model State-dependent Performance analysis
    Geospatial crime data Visualization Associated Content Part of a collection: Diverse
    Applications in Computing, Analytics and Networks Access this article Log in via
    an institution Buy article PDF USD 39.95 Price excludes VAT (USA) Tax calculation
    will be finalised during checkout. Instant access to the full article PDF. Rent
    this article via DeepDyve Institutional subscriptions Sections Figures References
    Abstract References Author information Ethics declarations Additional information
    Rights and permissions About this article Advertisement Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: SN Computer Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Performance Evaluation of IoST–Mist–Fog–Cloud Framework for Geospatial Crime
    Data Visualization: A State Dependent Queueing Approach'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu J.
  - Wang H.
  - Bao J.
  - Sun R.
  - Du X.
  - Guizani M.
  citation_count: '0'
  description: The increasing demand for intelligent management in modern power systems
    has emphasized the importance of smart grids, which facilitate real-time analysis
    and management through data aggregation. Fog computing provides efficient data
    processing and low-latency transmission for data aggregation. However, fog-assisted
    smart grids still face significant challenges, including privacy leakage, calculation
    limitations, and system stability issues. In response to these obstacles, we propose
    a robust and privacy-enhanced multidimensional data aggregation (RPMDA) scheme.
    Specifically, the Chinese Remainder Theorem is used to improve the efficiency
    of processing multidimensional data, combined with an innovative double-masking
    method to cope with secure data aggregation. For the purpose of reliable authentication,
    a conditional anonymous certificateless signature algorithm is designed in RPMDA,
    where the pseudonym generation mechanism ensures the conditional anonymity of
    smart meters. Besides, our scheme incorporates robustness, ensuring that the aggregated
    results remain unaffected even if smart meters malfunction. Compared to the existing
    solutions, RPMDA shows superior performance while meeting security requirements.
  doi: 10.1109/JIOT.2024.3352558
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Early Access RPMDA: Robust and Privacy-Enhanced Multidimensional Data Aggregation
    Scheme for Fog-Assisted Smart Grids Publisher: IEEE Cite This PDF Jingwei Liu;
    Haoze Wang; Jiajia Bao; Rong Sun; Xiaojiang Du; Mohsen Guizani All Authors 64
    Full Text Views Abstract Authors Keywords Metrics Abstract: The increasing demand
    for intelligent management in modern power systems has emphasized the importance
    of smart grids, which facilitate real-time analysis and management through data
    aggregation. Fog computing provides efficient data processing and low-latency
    transmission for data aggregation. However, fog-assisted smart grids still face
    significant challenges, including privacy leakage, calculation limitations, and
    system stability issues. In response to these obstacles, we propose a robust and
    privacy-enhanced multidimensional data aggregation (RPMDA) scheme. Specifically,
    the Chinese Remainder Theorem is used to improve the efficiency of processing
    multidimensional data, combined with an innovative double-masking method to cope
    with secure data aggregation. For the purpose of reliable authentication, a conditional
    anonymous certificateless signature algorithm is designed in RPMDA, where the
    pseudonym generation mechanism ensures the conditional anonymity of smart meters.
    Besides, our scheme incorporates robustness, ensuring that the aggregated results
    remain unaffected even if smart meters malfunction. Compared to the existing solutions,
    RPMDA shows superior performance while meeting security requirements. Published
    in: IEEE Internet of Things Journal ( Early Access ) Page(s): 1 - 1 Date of Publication:
    10 January 2024 ISSN Information: DOI: 10.1109/JIOT.2024.3352558 Publisher: IEEE
    Funding Agency: Authors Keywords Metrics More Like This Optimizing the Number
    of Collectors in Machine-to-Machine Advanced Metering Infrastructure Architecture
    for Internet of Things-Based Smart Grid 2016 IEEE Green Technologies Conference
    (GreenTech) Published: 2016 A Machine Learning Decision-Support System Improves
    the Internet of Things’ Smart Meter Operations IEEE Internet of Things Journal
    Published: 2017 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'RPMDA: Robust and Privacy-Enhanced Multidimensional Data Aggregation Scheme
    for Fog-Assisted Smart Grids'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bandyopadhyay A.
  - Mishra V.
  - Swain S.
  - Chatterjee K.
  - Dey S.
  - Mallik S.
  - Al-Rasheed A.
  - Abbas M.
  - Soufiene B.O.
  citation_count: '0'
  description: For an extended period, a technological architecture known as cloud
    IoT links IoT devices to servers located in cloud data centers. Real-time data
    analytic are made possible by this, enabling better, data-driven decision making,
    optimization, and risk reduction. Since cloud systems are often located at a considerable
    distance from IoT devices, the rise of time-sensitive IoT applications has driven
    the requirement to extend cloud architecture for timely delivery of critical services.
    Balancing the allocation of IoT services to appropriate edge nodes while guaranteeing
    low latency and efficient resource utilization remains a challenging task. Since
    edge nodes have lower resource capabilities than the cloud. The primary drawback
    of current methods in this situation is that they only tackle the scheduling issue
    from one side. Task scheduling plays a pivotal role in various domains, including
    cloud computing, operating systems, and parallel processing, enabling effective
    management of computational resources. In this research, we provide a multiple-factor
    autonomous IoT-Edge scheduling method based on game theory to solve this issue.
    Our strategy involves two distinct scenarios. In the first scenario, we introduced
    an algorithm containing choices for the IoT and edge nodes, allowing them to evaluate
    each other using factors such as delay and resource usage. The second scenario
    involves both a centralized and a distributed scheduling approach, leveraging
    the matching concept and considering each other. In addition, we also introduced
    a preference-based stable mechanism (PBSM) algorithm for resource allocation.
    In terms of the execution time for IoT services and the effectiveness of resource
    consolidation for edge nodes, the technique we use achieves better results compared
    with the two commonly used Min-Min and Max-Min scheduling algorithms.
  doi: 10.1109/ACCESS.2024.3350556
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 12 EdgeMatch:
    A Smart Approach for Scheduling IoT-Edge Tasks With Multiple Criteria Using Game
    Theory Publisher: IEEE Cite This PDF Anjan Bandyopadhyay; Vagisha Mishra; Sujata
    Swain; Kalyan Chatterjee; Sweta Dey; Saurav Mallik; Amal Al-Rasheed; Mohamed Abbas
    All Authors 220 Full Text Views Open Access Comment(s) Under a Creative Commons
    License Abstract Document Sections I. Introduction II. Literature Review III.
    System Model IV. Formulation V. Efficient IoT-to-Edge Scheduling Strategy: A Multi-to-Single
    Matching Game Show Full Outline Authors Figures References Keywords Metrics Abstract:
    For an extended period, a technological architecture known as cloud IoT links
    IoT devices to servers located in cloud data centers. Real-time data analytic
    are made possible by this, enabling better, data-driven decision making, optimization,
    and risk reduction. Since cloud systems are often located at a considerable distance
    from IoT devices, the rise of time-sensitive IoT applications has driven the requirement
    to extend cloud architecture for timely delivery of critical services. Balancing
    the allocation of IoT services to appropriate edge nodes while guaranteeing low
    latency and efficient resource utilization remains a challenging task. Since edge
    nodes have lower resource capabilities than the cloud. The primary drawback of
    current methods in this situation is that they only tackle the scheduling issue
    from one side. Task scheduling plays a pivotal role in various domains, including
    cloud computing, operating systems, and parallel processing, enabling effective
    management of computational resources. In this research, we provide a multiple-factor
    autonomous IoT-Edge scheduling method based on game theory to solve this issue.
    Our strategy involves two distinct scenarios. In the first scenario, we introduced
    an algorithm containing choices for the IoT and edge nodes, allowing them to evaluate
    each other using factors such as delay and resource usage. The second scenario
    involves both a centralized and a distributed scheduling approach, leveraging
    the matching concept and considering each other. In addition, we also introduced
    a preference-based stable mechanism (PBSM) algorithm for resource allocation.
    In terms of the execution time for IoT services and the effectiveness of resource
    consolidation for edge nodes, the technique we use achieves better results compared
    with the two commonly used Min-Min and Max-Min scheduling algorithms. In this
    research, we provide a multiple-factor autonomous IoT-Edge scheduling method based
    on game theory to solve this issue. Our strategy involves two distinct scenario...View
    more Published in: IEEE Access ( Volume: 12) Page(s): 7609 - 7623 Date of Publication:
    05 January 2024 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2024.3350556 Publisher:
    IEEE Funding Agency: SECTION I. Introduction Internet of Things (IoT) is becoming
    a popular thing in our daily lives. Everyone used the practical implementation
    of the IoT devices to get more advantages [4]. IoT devices create a link between
    wearable devices and smart meters in our smart cities. As a result, there has
    been a globally unparalleled rise in the deployment and use of IoT devices [5].
    For instance, Cisco predicts that by 2030, there will be 30 billion IoT devices
    [6]. Due to its extensive and high-capacity processing and storage capabilities,
    cloud computing [7], [8], [9] has remained the favored choice for IoT producers
    and suppliers to fulfill their storage and computational requirements [10], [11],
    [12]. However, the geographical distance between cloud datacenters [13], [14]
    and IoT devices, often located in remote areas, can lead to increased latency
    and delays in high-performance systems. Consider a scenario where a wearable device
    worn by a patient transmits brain data for rapid analysis. In this case, any delay
    in transmitting the information processing call to the designated server could
    escalate the potential health risks for the patient [15]. This challenge becomes
    particularly pronounced in a cloud-based environment, where analysis is required
    to be performed in regional cloud data centers. In light of the fact that applications
    for intelligent transportation systems and other delay-critical services sometimes
    need answers in milliseconds, this poses a severe difficulty. Fog computing [16],
    [17] is an emerging method that addresses latency problems by enabling data analysis
    at the edge [18], [19] in close proximity to IoT device sources [20], [21]. It
    is important to note that IoT devices have limited computational network, and
    storage capabilities [22], [23]. For example, these devices can consume substantial
    network bandwidth when exchanging data. This might result in bottleneck difficulties
    and slow down the entire network. By giving IoT devices access to quick data analyses
    and decision-making tools at the network’s edge, fog nodes help to mitigate this
    issue [24], [25]. The resource constraints that fog nodes commonly experience
    are the main cause of the system’s multiple major problems, which the fog computing
    system possesses despite its advantages. To increase both the number of tasks
    that can be done and the Quality of Service (QoS) [26] that is provided for every
    job, these resources must be properly employed and handled. When fog computing
    is deployed, data is examined within an IoT gateway. Data is examined on the sensor
    or device itself via edge computing. In short, data is not transported anywhere
    when edge computing is used. This lowers expenses and enables real-time data analysis,
    improving performance [27], [28]. The most important aspect of edge computing
    is the effective distribution of scarce resources, and the effectiveness of this
    process directly impacts how well the edge computing paradigm as a whole performs(see
    Figure 1 depicting the architecture of edge computing). Due to its enormous success,
    edge computing is becoming more and more popular, and the research community is
    paying close attention to it. The crucial factor, namely the latency between the
    two sides, must be taken into account in order to properly profit from edge computing
    technology. In order to ensure quick and effective data and message transfer,
    IoT devices must connect to the edge nodes that are nearest to them. In this study,
    we tackle the challenge of efficiently associating IoT devices with appropriate
    edge nodes while considering the interplay of these two conflicting objectives.
    Here, the issue is to guarantee both optimal resource utilization on edge nodes
    and efficient task execution of IoT-generated activities. Task scheduling optimizes
    resource utilization, enhances system efficiency, and minimizes idle time by organizing
    and prioritizing tasks. It aids in meeting deadlines, improving throughput, and
    balancing workloads in diverse computing environments. FIGURE 1. Architecture
    of edge computing. Show All A. Research Focus Several scheduling techniques have
    recently been presented for edge computing environments. These techniques’ main
    goals are to improve and accelerate IoT task execution times at the edge and to
    optimize resource allocation. However, a key challenge with these approaches is
    that they often focus on optimizing certain factors that are applicable only to
    one side of the equation. This leads to an imbalanced situation where the requirements
    of one party are disregarded during the scheduling process. In recent times, game
    theory [29], [30] has emerged as a tool to generate wise scheduling choices that
    consider the interests of both sides. These strategies tend to have a business-centric
    orientation, incorporating business-related considerations into the construction
    of utility applications. The Quality of Service (QoS) [28] given to IoT services
    is maximized by the approach we use, in contrast, while simultaneously effectively
    regulating resource usage on the edge nodes. B. Innovative Perspective In this
    research, we have proposed an intelligent autonomous scheduling technique in an
    IoT-edge context. This strategy employs game theoretic matching to make smart
    scheduling choices for edge and IoT gadgets. The method we employ differs from
    others in that it takes into account both the preferences and constraints of both
    types of devices when scheduling. Our approach aims to reduce the latency and
    duration of IoT service execution while also increasing the efficiency with which
    available resources on edge nodes are used. Here’s a rephrased version of our
    summary We introduce an independent and smart scheduling solution for IoT services
    within edge computing environments using matching algorithm. This work stands
    out as a novel performance-centric method that takes into account the preferences
    and limitations of both edge and IoT aspects when formulating scheduling choices,
    as indicated in [7], [31], and [32]. By formulating unique optimization challenges
    for individual stakeholders and suggesting specific constraints for each underlying
    problem, these difficulties are subsequently consolidated into a unified optimization
    problem, amenable to effective resolution using matching game theory. Developing
    preference metrics for both IoT devices and edge nodes, aiding them in evaluating
    and prioritizing each other using distinct criteria Furthermore, introducing methods
    for generating preferences will aid edge nodes and IoT devices in creating practical
    preference rankings aligned with the established preference metrics. The development
    of centralized and distributed scheduling algorithms By presenting two scheduling
    strategies, we want to make our solution more complete. Each kind has distinct
    benefits that apply in certain settings and applications. The concept of a centralized
    approach finds applicability in business scenarios where a central commanding
    organization becomes necessary to act as the focal point for all communication.
    The advantage of this tactic lies in the presence of a centralized governing body
    that assumes responsibility and addresses issues promptly upon emergence. The
    network is more secure [33] in such an environment because there is a centralized
    authority managing network scheduling. As an illustration, consider the network
    of weather stations consisting of microcontrollers that collect data on various
    environmental conditions across an expansive and remote region. Conversely, the
    decentralized approach could prove advantageous for rapidly creating small to
    medium-sized networks where there’s no need for a large-scale and business-oriented
    setup. The above approach is also appropriate for gatherings with strict security
    considerations that lack confidence in a centralized entity to carry out the scheduling
    process. The preference-Based Stable Matching (PBSM) algorithm that we developed
    considers a scenario where there are n IoT devices expressing their clear preferences
    for various edge nodes, and at the same time, n available edge nodes express their
    distinct preferences for the targeted IoT devices. These preferences are flexible
    and can be defined as needed during the implementation process. Each IoT device
    receives its own edge node. The suggested approach then distributes each edge
    node to an IoT device, with the condition that each IoT device receives a single
    unique edge node. In the second section, an examination of existing literature
    concerning task scheduling and associated algorithms in IoT environments is conducted.
    Moving on to the third section, difficulties with IoT and edge optimization aspects
    are formulated. The fourth section introduces the fundamentals and definitions
    of matching game theory, detailing preference functions and suggesting methods
    for creating preference lists. Transitioning to the fifth section, the proposed
    centralized, distributed matching, and PBSM algorithms are elucidated. In Section
    VI, the setup for the experiment and simulation is given in detail, and the explanation
    of the empirical results follows. Finally, in the concluding seventh section,
    a summary of the findings is presented. SECTION II. Literature Review In reference
    to the work outlined in [32], a study was presented addressing the tradeoff between
    makespan and cloud costs within the context of scheduling large-scale applications
    on a cloud platform. The study introduces an innovative scheduling algorithm referred
    to as the cost-makespan-aware scheduling heuristic. This algorithm aims to strike
    a balance between optimizing application execution performance and managing the
    requisite expenses associated with utilizing cloud resources. Additionally, the
    research proposes an effective strategy for task reassignment. This strategy leverages
    the critical path of a directed acyclic graph that models the application, enhancing
    the output schedules produced by the Cost-makespan-aware scheduling algorithm.
    These refinements were intended to meet user-defined deadline constraints and
    ensure the system’s quality of service. In paper [33], an approach to scheduling
    tasks in the context of IoT is introduced, leveraging data mining techniques by
    the authors. Their method involves the utilization of a prior task classification
    using an upgraded version of the prior rules-based algorithm; subsequently, they
    employ a unique strategy called TSFC (Task Scheduling in Fog Computing) for task
    scheduling. This strategy primarily relies on time-based factors, disregarding
    the availability of resources at both the network (such as bandwidth) and fog
    node levels (comprising CPU, RAM, and power). Consequently, this approach poses
    a potential risk to the overall quality of service for these tasks. Alternatively,
    in the article [34], a resource allocation model using a decoupled methodology
    is introduced as Zenith, proposed by the authors as a fresh technique. This new
    strategy is built on an auction-based resource sharing mechanism, made possible
    by a contract that guarantees resource optimization for both fog nodes and service
    providers as well as the preservation of integrity. Nonetheless, their other abstracted
    layers, such as microdata centers (MDCs), must be put between the different endpoints
    in this design.This exhibits significant resource utilization and might not be
    appropriate for time-sensitive tasks. A task scheduling approach centered around
    prioritization tiers is proposed by Choudhari in their publication [7]. In the
    paper [41], the energy-aware scheduling of dependent tasks in heterogeneous multiprocessor
    systems is addressed by the author. Tasks and processors have been modeled, formulating
    an optimization problem to minimize task schedule length. An optimization problem
    is formulated to minimize task schedule length involving a task prioritization
    method and a weight-based energy distribution strategy, leading to the creation
    of a list-based energy-aware scheduling algorithm. Efficient task execution while
    meeting dependencies and energy constraints is ensured by this approach. Additionally,
    in the paper [42], an energy-efficient scheduling algorithm is introduced, leveraging
    an enhanced per-assignment strategy. This novel approach optimizes processor allocation,
    frequencies, and task start times while ensuring compliance with data dependency
    and energy constraints. In [43], a global and local attention-based reinforcement
    learning approach for UAVs’ cooperative behavior control is devised by researchers.
    Motion, coordination models, and constraints are analyzed-focusing on collision
    avoidance, motion updates, and task execution for multiple UAVs. This is abstracted
    as a multi-constraint decision-making problem, and a multi-agent reinforcement
    learning algorithm is crafted. Inspired by human learning, the design incorporates
    a global-and-local attention mechanism, enabling cooperative behavior control
    and effective coordination among UAVs. In article [35] examines centralized user
    clustering to divide IoT users into various groups in accordance with users’ priorities.
    The cluster holding the utmost priority is tasked with offloading computations
    to the edge server, whereas the cluster with the lowest priority performs computations
    locally. The importance of the new edge computing paradigm and its contribution
    to the development of the IoT drive research in this area, as mentioned in [36].
    This study emphasizes the significance of edge computing in the IoT environment.
    This study examines, identifies, and documents current, ground-breaking advancements
    in edge computing from an IoT standpoint. It creates a taxonomy to categorize
    and organize the literature on edge computing and outlines the essential conditions
    for the successful implementation of edge computing in the IoT. In paper [37],
    to efficiently handle resource allocation for EN. In order to maximize the use
    of edge computing resources within the restrictions of the available budget, it
    provides resource bundles and services. The suggested paradigm ensures equity
    while sharing resources (wireless channels, communication) between different edge
    service providers and customers (or groups of users). In the research [38], the
    computation offloading mechanism was described as a stochastic game and analyzed
    how numerous selfish users allocate resources in IoT edge computing networks.
    In order to resolve the game using the suggested IL-based MA-Q method, a multi-agent
    reinforcement learning framework is created. Simulations show that the suggested
    IL-based MA-Q method is capable of solving the specified issue and is more energy-efficient
    without incurring additional costs for channel estimation at the centralized gateway.
    For the IoT edge computing system, according to the paper [39], a resource allocation
    policy was suggested to increase the effectiveness of resource consumption. The
    proposed guideline aims to minimize the combined value of the extended total of
    the mean task fulfillment duration and the mean count of resource appeals over
    time. To address this challenge, a strategy grounded in deep reinforcement learning
    is employed. An enhanced deep Q-network learning technique was put forward, where
    many replay memories are used to retain the experiences independently with little
    effect from one another. A brand-new framework, as stated in [40] built around
    markets was put out to effectively distribute the resources of heterogeneous,
    capacity-limited edge nodes to several competing services at the network edge.
    The suggested framework creates a market equilibrium solution by appropriately
    pricing the geographically dispersed services, maximizing the use of edge computing
    resources, and also allocating the best resource bundles to the services given
    the available budgets. Mentioned were two distributed methods that quickly reached
    market equilibrium. SECTION III. System Model In this section, we present an optimization
    problem formulation and an explanation of the related restrictions for the IoT-to-Edge
    scheduling problem in the subsequent section. A. Edge Nodes Optimization Problem
    The operating cost and traffic cost functions are very important to edge nodes.
    Both of these functions are thoroughly addressed in this section in terms of the
    numerous limitations that must be taken into account. TABLE 1 List of Definition
    1) Operational Cost CPU and memory have an impact on the edge nodes’ operating
    expenses. The CPU cost (measured in MIPS) accounts for both the active CPU consumption
    cost associated with an individual edge node’s IoT operations and also the idle
    CPU consumption cost. The cost of idle memory while the node is not in use is
    included in the memory cost (i.e., RAM), which defines how much memory is utilized
    by the edge node to support IoT operations. The operational cost O c (e) of e∈E
    is defined in technical terms as follows: O c (e)=RA M e +CP U e (1) View Source
    2) Traffic Cost The devices in an IoT ecosystem must transmit the data collected
    at specific bandwidth rates to the edge nodes. Various availability values for
    the active physical links may exist at various times, depending on the underlying
    demand. To send and receive important information, edge nodes also occasionally
    need to connect with one another. In order to determine the traffic cost of the
    edge nodes, the bandwidth cost associated with each link as well as the latency
    between the present hops are taken into account. Technically, the traffic cost
    T c (e) associated with an edge node e∈E , which corresponds to an IoT device
    a∈A , can be expressed as follows: T c (e)= ∑ a,e∈A,E W ae ⋅ B ae + ∑ e, e ′ ∈E|e≠
    e ′ W e e ′ ⋅ N ea (2) View Source Therefore, each edge node e∈E must reduce the
    subsequent objective function: T(e)= O c (e)+ T c (e) (3) View Source Some restrictions
    should be taken into account in order to do this: Condition 1:No edge node shall
    have more resources allocated to it overall compared to the available resource
    capacity. ∑ a∈A s r a ⋅ B ae ≤ s r e , ∀ e∈E ,  ∀ r∈R ,  ∀ a∈A (4) View Source
    Condition 2:Any edge node should only receive a certain amount of traffic at a
    time in order to avoid overloading it. T c (e)< ϕ e , ∀ e∈E (5) View Source B.
    IoT Devices Optimization Problem The least amount of lag possible is what IoT
    devices are mostly focused on [33]. The calculation of traffic expenses for each
    IoT device involves considering the network’s bandwidth expenses and latency between
    consecutive hops. The traffic cost T c (e) of an IoT device a∈A engaging with
    a edge node e∈E can be described formally by saying: T c (e)= ∑ a,e∈A,E W ae +
    N ea (6) View Source The following restrictions must be taken into account in
    order to reduce this cost: Condition 1:Only one edge node should be allotted to
    each Internet of Things task at a time, such as,. ∑ e∈E B ae =1, ∀ a∈A (7) View
    Source Condition 2:An IoT device’s overall traffic expenses must be lower than
    its traffic capacity. T c (a)< ϕ a , ∀ a∈A (8) View Source C. Generalized Optimization
    Difficulty We therefore establish the general optimization difficulty that we
    intend to minimize in this study on the basis of the optimization problems for
    edge nodes and IoT devices that were described in the sections above: B ae M(O,T)=
    O c (e)+ T c (e)+ T c (a) min B ae M(O,T) s.t ∑ a∈A s r a ⋅ B ae ≤ s r e , ∀ e∈E  ∀
    r∈R  ∀a∈A T c (e)< ϕ e , ∀ e∈E ∑ e∈E B ae =1, ∀ a∈A =0,1, ∀ a∈A  ∀ e∈E (9) (10)
    View Source SECTION IV. Formulation In this section, we go over the main concepts
    of matching game theory, discuss the preference functions of edge nodes and IoT
    devices, and then demonstrate how to create preference lists for both entities
    in real-world scenarios. A. Matching Game Concepts Concept 1:The matching relation(
    ω ) is the result of the IoT-to-Edge scheduling, where ω is a function such that
    A∪E→ 2 A∪E satisfying the following requirements: ω(e)⊆A such that |ω(e) | r ≤
    s ′r e . ω(a)⊆E such that |ω(a) | r ≤ s r e or |ω | a =0, ∀ a∈A ,e∈E and r∈R ,
    Where |ω | (a) =0 demonstrates that a is not allotted. a∈ω(e) if ⟺ω(a)=e, ∀ a∈A
    , ∀ e∈E Concept 2:A matching ω is considered blocked by an IoT-Edge pair (a,e)
    when there exists a pair if (a,e) where a∈ω(e) and e∈ω(a) . Additionally, a must
    be preferred over ω(e) and e must be preferred over ω(a) . Concept 3:If a particular
    edge node e is using all of its resource capacity, it is said to be saturated.
    If a node still has some resources available, it will take any IoT job a as long
    as s r e ← s r a , ∀ r∈R . Concept 4:When (1) Every IoT device a ’s is paired
    with an edge node and (2) there are not any blocking pairings, a matching relation
    ω is considered stable. B. An Optimal Function for IoT Devices Selection IoT devices
    have a natural inclination to assign tasks to edge nodes that achieve two objectives:
    firstly, they minimize the expense of task-related data transfer, and secondly,
    they capitalize on the largest pool of available resources. The overarching aim
    is to expedite job completion timelines. A transitive, comprehensive, and stringent
    preference relation L(a) exists between each IoT device a∈A and the set E of edge
    nodes. When there is a preference relation e ≻ a e ′ , a favors edge node e over
    edge node e ′ . Additionally, if an IoT device a is undecided about whether to
    join a edge node e or remain alone, a edge node e is said to be unacceptable to
    the device a . The provided explanation establishes the preference function of
    an IoT device in the following manner [23]. e 1 ≻ a e ′ 2 ⟺ L a ( e 1 )> L a (
    e 2 ) (11) View Source where (12), as shown at the bottom of the next page. L
    a (e)= ⎧ ⎩ ⎨ +∞,If e provides the lowest traffic cost and the most resources to
    do the jobs,  0,If e proposals the lowest traffic cost or has the most resources
    available to complete the jobs;  −∞,Otherwise (12) View Source C. Edge Node Preference
    Function In accordance with the consolidation policy of the edge nodes, the preferred
    list of edge nodes is created. An edge node particularly desires to increase the
    efficiency of its resource use by hosting the greatest number of IoT devices and
    making the best use of its resources. This suggests that edge nodes prefer IoT
    tasks to non-tasks, and vice versa. A well-defined preference relation L e (A)
    exists for each edge node e∈E in relation to the set of IoT devices, establishing
    a transitive, comprehensive, and rigorous comparison. In this context, the notation
    a1 ≻ e a2 signifies that edge node e prioritizes receiving tasks from IoT device
    a1 over IoT device a2 . Additionally, if a edge node e chooses to remain unmatched
    over being matched to an IoT device a , the IoT device is considered to be unacceptable
    to a(i.e., 0 ≻ e a then). Building upon this idea, the preference mechanism of
    an edge node can be formally described as stated in reference [29]. D. Generating
    a List of Preferred IoT Devices We outline how the preference list for each IoT
    device may be truly built using the criteria given in Algorithm 1 (is used by
    every IoT device) The method takes a set of edges that can perform IoT activities
    as input and generates a preference list L a for each IoT device a that contains
    the edge nodes sorted by a according to how much they are preferred by that device.
    The procedure starts by exploring unvisited edge nodes (Line 4). Subsequently,
    it evaluates the compatibility between available resources ( AvRes ) and task
    requirements ( AvOpr ) (Line 5). The algorithm assesses latency with the underlying
    IoT device, recording results (Lines 6–8) for edges having sufficient resources.
    In cases of inadequate resources, the edge is removed, and the next unvisited
    one is considered (Lines 6–8). For assisting the IoT device in deciding its preferred
    order among the retained edges, the algorithm employs Eq. (12). Algorithm 1 Generating
    a List of Preferred IoT Devices a ’s 1: Input: Create a collection labeled E comprising
    operational edge nodes, with the initial node denoted as e 0 . 2: Output: Preference
    list L(a) of IoT device a 3: while there are still some unexplored edges in E
    do 4: Mark e 0 as visited after choosing it 5: if AvRes( e 0 )←AvOpr(a) , i.e.,
    Resources are available for e 0 to complete the tasks then 6: Verify and note
    e 0 ’s delay 7: end if 8: Go to the following unexplored edge in E 9: end while
    10: Ranking the edge nodes using Eq. (12) and storing the results in L(a) . E.
    Generating a List of Preferred Edge Node By attempting to host as many IoT devices
    as they can, edge nodes primarily aim to increase their resource consumption.
    This demonstrates that edge nodes favor more IoT device assignments over fewer
    or inadequate ones. In technique 2, we suggest a heuristic technique to assist
    edge nodes in creating their preference lists. The goal is to add the subsequent
    IoT device to the preference function at each stage that leaves the least amount
    of space after being attached to the underlying edge node. The algorithm takes
    input about a collection of IoT devices to serve and generates a preference list
    for each edge node. This list ranks IoT devices based on how much the edge node
    prefers them, giving priority to devices that use fewer resources when assigned
    to the node. To find the preference ordering among the collection of potential
    edge nodes (Line 3), the program first applies Eq. This preference formulation
    prioritizes IoT devices that utilize minimal resources when allocated to edge
    nodes. The concept of sequential task execution is not invoked in this context.
    Consequently, We refrain from indicating that the edge node prioritizes any specific
    IoT task over others based solely on resource consumption. Instead, we discuss
    a strategy for task consolidation. Subsequently, the algorithm assesses whether
    the resource demand of each IoT device in the preference sequence, denoted as
    AvOpr , is less than or equal to the available resources on the executing edge
    node, represented as AvRes (Line 6). If an IoT task’s resource requirements exceed
    the edge node’s available resources, that respective IoT device is excluded from
    the list of favored devices (Line 7); The IoT device is retained in any other
    case. Follow this procedure until all IoT devices on the preference list have
    their resource requirements met. SECTION V. Efficient IoT-to-Edge Scheduling Strategy:
    A Multi-to-Single Matching Game The pair of methods we propose are decentralized
    and centralized. It is important to note that the procedure for pairing IoT devices
    with edge nodes follows the approach outlined in reference [36]. The simultaneous
    deployment of Virtual Network Functions (VNFs) within substrate networks is the
    main emphasis of this study. In conducting our experiments, each scenario was
    executed 50 times to ensure consistency and reliability. We utilized a standardized
    hardware setup with processor Intel Core i7 running at a speed of 3.8 GHz and
    32 GB of RAM specifications and employed 32-bit Windows operating system for all
    simulations. A. Centralized Matching Algorithm The group of edge nodes that are
    currently up and running and the group of IoT devices looking to organize their
    duties are used as inputs for the centralized scheduling approach. Each IoT job
    is embedded to a edge node as the output. Centralized matching algorithm is given
    in Algorithm 3. The process commences by evaluating unutilized edge nodes at saturation
    point (Line 4) and unassigned IoT devices (Line 5). If applicable, the edge node
    with the highest priority for the IoT device is chosen along with the first unassigned
    IoT device (Line 6). Line 7 Subsequently, the algorithm checks if the chosen edge
    node possesses adequate resources for the IoT device’s requirements. If affirmative,
    the IoT device is scheduled to the fog node, decreasing its available resources
    accordingly (Line 9). However, if the edge node’s resources are insufficient (Line
    12), the IoT device is declined. To optimize further, the algorithm discards IoT
    devices with lower priority than the refused one (Line 13), diminishing complexity.
    To update preference lists considering changing edge node resources, emerging
    nodes, and departing nodes (Line 18), we repeat algorithms 1 and 2 before cyclically
    reiterating the entire process at fixed intervals (Line 19). a 1 ≻ e e ′ 2 ⟺ L
    e ( a 1 )> L e ( a 2 ) (13) View Source where (14), as shown at the bottom of
    the next page. L e (a)={ +∞,if the completion of  a ′ s task consumes the least
    amount of space on the edge node −∞,Otherwise (14) View Source Algorithm 3 Algorithm
    for Centralized Matching 1: Input: For IoT devices, set A , and for edge nodes,
    set E 2: Output: IoT devices embedded on edge nodes 3: repeat 4: while There are
    non-standard ∃e∈E do 5: while Unmatched devices ∃a∈A do 6: a← as the head of L(e)
    7: if s r e > s r a then 8: Match a to e 9: s r e = s r e − s p a 10: Remove a
    from L(e) 11: else 12: Decline a 13: Decline every a ′ in order to have a ≻ e
    a ′ 14: Proceed to the following edge node e 15: end if 16: end while 17: end
    while 18: simulate algorithms 1 and 2 19: until ∈ elapses B. Algorithms for Distributed
    Matching As the IoT devices and edge nodes interact straight to perform the matching,
    the distributed execution of our approach does not require a central entity. Every
    IoT device executes Algorithm 4, which produces a matching scheme between the
    edge and IoT devices after receiving as an input the underlying IoT device’s preference
    list. To accomplish this, the procedure initially goes through the IoT device’s
    set of preferences (Step 4), followed by the selection of the preferred edge node,
    which is the top-ranked one (Step 5). Subsequently, the IoT device transmits an
    proposal message to the chosen edge node (Step 6) and patiently anticipates a
    reply (Step 7). Algorithm 4 Distributed Matching Algorithm in IoT 1: Input: The
    number of choice L(a) made by IoT device a 2: Output: Identification of coordinating
    edge and relevant IoT devices 3: repeat 4: while L(a) is not free do 5: e← first
    item in L(a) 6: Dispatch proposal message proposal[ e ] to e 7: Await response
    R[proposal[e]] 8: if R[proposal[e]]=‘‘declin e ′′ then 9: Move e to the last of
    L(a) 10: else if R[proposal[e]]=‘‘accep t ′′ then 11: Conclude the process 12:
    end if 13: end while 14: execute algorithms 1 15: until ∈ elapses The lowest-ranked
    edge on the IoT preference list is selected first. If this edge declines hosting
    the IoT tasks, the next highest-preference edge is chosen (Line 8). In case of
    a negative response, the process repeats until a willing edge is found, facilitating
    the matching between the IoT device and an edge (Line 11). This cycle repeats
    periodically (Line 15). Prior to this, Algorithm 1 is executed to adjust preference
    lists due to evolving edge resources, new additions, and departures, impacting
    the preference functions of both sides (Line 14). Each individual edge node executes
    Algorithm 5, generating a matching arrangement between edge and IoT devices. The
    algorithm takes a queue of IoT devices that have submitted proposal messages to
    the respective edge node running Algorithm 4 as input. Algorithm 5 Algorithms
    for Distributed Matching - Edge Side 1: Input: IoT devices are submitting proposal
    messages to e in Q(a) queue. 2: Output: IoT device mapping to a specific node
    e 3: repeat 4: while Q(a) is not empty do 5: if s r e > s r a  anda∈Q(a) then
    6: Deliver i a response with accept , i.e., R[proposal[f]]=‘‘accep t ′′ 7: Change
    the e resource so that s r e = s r e − s r a 8: else 9: Deliver a a response with
    decline , i.e., R[proposal[f]]=‘‘declin e ′′ 10: Decline all a ′ in a way that
    a ≻ e a ′ 11: Take a out of Q(a) 12: Take a ′ out of Q(a) 13: end if 14: end while
    15: execute algorithms 2 16: until ∈ elapses In order to determine if the input
    queue is empty or not, the algorithm first cycles through it (Line 4). A nonempty
    queue indicates that the underlying edge node has not yet checked any proposal
    messages from some IoT devices. If the queue has entries, the algorithm selects
    the most recent proposal message to evaluate whether the edge node possesses adequate
    resources for executing tasks produced by the Internet of Things device that sent
    the message. It also determines if the edge node is on the node’s preferred list
    (Line 6). If these conditions are fulfilled, the edge node responds to the IoT
    device with an acceptance message, confirming its readiness to perform the tasks.
    Subsequently, the edge node adjusts its available resource quantity by subtracting
    the necessary amount required for executing the IoT activities (Line 7). The IoT
    device receives a decline message informing it that the edge cannot currently
    fulfill its tasks (Line 9), and the edge declines all other IoT devices whose
    position in the preference list is lower compared to that of the declined IoT
    device (Line 10). Conversely, this situation arises when the resources on the
    edge node are inadequate for the tasks of the IoT, or if the IoT device is not
    prioritized highly on the edge node’s preference list. The underlying IoT devices
    are taken out of the queue in lines 11–12. The entire process is carried out on
    a regular basis after a predetermined amount of time (Line 16), but right before
    that, Algorithm 2 is again executed to obtain new updated preference lists according
    to the dynamic nature of the edge node resources, the emergence of new edge nodes
    and IoT devices, and the departure of current ones, all of which have an impact
    on the preference functions of both parties (Line 15). Algorithm 2 Preference
    List Creation for Edge Node e 1: Input: Enumerate a collection of Internet of
    Things (IoT) A devices aiming to organize their functions on edge nodes. 2: Output:
    Preference list L(e) of edge node e 3: In accordance with the IoT device’s preference
    degree, sort the IoT devices using Eq. (14) and store the results in L(e) . 4:
    while L(e) is not empty do 5: Get a 0 the head of L(e) 6: if AvRes(e)<AvOpr( a
    0 ) then 7: remove a 0 from L(e) 8: else 9: a 0 = a 0 ⋅ next 10: end if 11: end
    while The parameters for the PBSM comprise: the collection of n operational edge
    nodes denoted as E , the set of n active IoT devices labeled as A , the distinct
    order of preferences for the n edge nodes represented as ≻ e =( ≻ e1 , ≻ e2 ,…,
    ≻ en ) , and the unique preference order for the n IoT devices denoted as ≻ a
    =( ≻ a1 , ≻ a2 ,…, ≻ an ) . The PBSM’s outcome consists of assigned pairs of IoT
    devices and Edge nodes. Initially in Algorithm 6, the setup ensures no engagement
    between IoT devices and Edge nodes (line 2–7). The A ′ structure in line 2 manages
    unassigned IoT devices. Moving to the allocation phase, the while loop in line
    4 monitors Edge node assignments to IoT devices. The structure of the PBSM guarantees
    termination of the while loop when the output, Δ encompasses all IoT device-Edge
    node pairs. In line 10, the function labeled rand_pick() is employed to randomly
    select an IoT device from the set A ′ . The subsequent function, denoted as extract()
    and located in line 11, serves the purpose of retrieving the most favored edge
    node based on the preference profile a ∗ for each IoT device. This information
    is then stored within the data structure referred to as e ∗ . The condition presented
    in line 12 is evaluated, and if it is met, the IoT device denoted as a ∗ is inserted
    at the index of e ∗ using line 13. Once an IoT device represented by a ∗ is incorporated
    into Δ , it is subsequently removed from the set A ∗ . However, if the assessment
    in line 12 is unsuccessful, it indicates that a specific device e ∗ is contending
    with multiple proposals. To address the competitive landscape among IoT devices,
    a decision is taken in step 17 to single out an IoT device from the available
    options. Resolving conflicts involves utilizing the precise preference sequence
    of the prime edge nodes ( e ∗ ) and retaining the most favored IoT device among
    the proposals in Δ[ e ∗ ] . The 19th step involves eliminating the assigned IoT
    device from set A ′ while incorporating declined IoT devices. Ultimately, line
    24 of the PBSM function yields the conclusive pairings of IoT devices and Edge
    nodes within the system. Algorithm 6 Preference Based Stable Matching Algorithm
    Input: A= a 1 , a 2 ,…, a n ; E= e 1 , e 2 ,…, e n ; ≻ a =( ≻ a1 , ≻ a2 ,…, ≻
    an ) ; ≻ e =( ≻ e1 , ≻ e2 ,…, ≻ en ) . Output: Δ←ϕ 1: begin /* Initialization
    Phase* / 2: A ′ =A 3: for each i∈A do 4: Δ[i]←ϕ ▹ Initially, no IoT device is
    assigned a edge node 5: end for 6: for each i∈E do 7: Δ[i] ~ ←ϕ ▹ Initially, no
    edge node is assigned a IoT device 8: end for /* Allocation phase */ 9: while
    A ′ ≠ϕ do 10: a ∗ ←rand_pick( A ′ ) 11: e ∗ ←extract( ≻ ∗ a ) 12: if Δ ~ [ e ∗
    ]==ϕ then 13: Δ ~ [ e ∗ ]← a ∗ 14: A ′ ← A ′  a ∗ 15: else 16: a ∗ ≻ Δ ~ [ e ∗
    ] 17: if a ∗ ≻ e ∗ a ∗ then 18: Δ[ e ∗ ]← a ∗ 19: A ′ ≻ A ′  a ∗ 20: A ′ ≻ A ′
    ∪ a ∗ 21: end if 22: end if 23: end while 24: return Δ 25: end SECTION VI. Findings
    From Observations and Analysis In this part, we first explain the setting in which
    our simulations were run, followed by a discussion of the outcomes. A. Experimental
    Setup We take into account Remote Patient Monitoring using IoT-based implantable
    or wearable sensors for the simulations. The experiment replicates an IoT sensor
    that continuously sends patient physiological characteristics, including temperature,
    blood pressure, and pacing, to the edges for analysis. We create our own scheduling
    application to run the simulations, and each edge node is given a CPU with a capacity
    between [3,000,9,000] MIPS, a RAM with a capacity between [6,000,25,000] MB, and
    a link bandwidth with a capacity between [9,000,25,000] Mbps. But compared to
    edge nodes, IoT devices use far fewer resources. In particular, the IoT devices’
    CPU capacity is selected from a range of [400, 800] MIPS, their RAM capacity is
    selected from a range of [500, 1000] MB, and their link bandwidth capacity ranges
    from [600, 1000] Mbps. Every set of edge nodes and IoT devices has a delay that
    varies between 200 and 6000 milliseconds. Since MaxMin [35] and Min-Min [35],
    [40] two widely used scheduling algorithms, are utilized often in the scheduling
    literature in many relevant fields, including fog, edge, and cloud computing,
    we compare our solution with theirs. By scheduling the quickest-finishing jobs
    first, the Min-Min approach favors them. When there are more large-sized activities
    than smaller-sized tasks, Min-Min’s performance begins to suffer, resulting in
    inefficient resource use and prolonged makespan. On the other hand, the basic
    notion of Max-Min is to prioritize the jobs with the longest execution times in
    order to reduce the overall makespan. Our approach differs from current alternatives
    by encompassing a broader spectrum of metrics. These metrics include factors like
    latency, CPU, RAM, and bandwidth, considered from both IoT devices and edge nodes
    perspectives. This sets our solution apart from the MinMin and Max-Min algorithms,
    that alone focus on the edge nodes aspect. B. Results From the Experiment Our
    simulations primarily explore three key aspects: the average amount of time required
    to complete an Internet of Things job, how efficiently edge nodes use their resources,
    and how long the solution takes to execute altogether. We rigorously assess each
    indicator under a number of realistic circumstances in order to thoroughly examine
    the scalability of our system over a wide variety of real-world scenarios. 1)
    Average Makespan We commence by studying the time taken by different solutions
    for task completion. It is worth noting that this is the duration between the
    initiation and completion of a task. In the initial trials, the number of fog
    nodes remains constant, while we vary the quantity of IoT devices Figures 2, 3,
    4. This investigation aims to reveal how the makespan is influenced by the number
    of IoT devices generating tasks. Our experiments encompass IoT device quantities
    ranging from 200 to 600, while the number of fog nodes is consistent Figures 2,
    3, 4. FIGURE 2. 30 Edge nodes. Show All FIGURE 3. 60 Edge nodes. Show All FIGURE
    4. 110 Edge nodes. Show All The initial finding from figures 2, 3, 4 is that,
    in contrast to the Max-Min and Min-Min techniques, our two suggested algorithms
    produce low makespan. Furthermore, in each of the instances, our distributed and
    PBSM approaches performed better than the centralized algorithm. The key benefit
    of the distributed version is that it allows edge nodes and IoT devices to simultaneously
    and independently make judgments based on their preference lists by decentralizing
    the matching process. The Min-Min and Max-Min approaches exhibit subpar performance
    in contrast to our solution. This is because they solely consider task completion
    time, disregarding vital factors such as resource usage and latency. These factors
    profoundly influence makespan. Another key insight from Figures 2, 3, 4 is that
    makespan rises with the growing count of IoT devices. This is due to the fact
    that having more IoT devices will result in a higher number of tasks being assigned
    to each individual edge node when compared to having a set number of edge nodes.
    This inevitably lengthens the time jobs must wait before beginning, increasing
    the entire timeline. In Figures 5, 6, 7, we measure the makespan of the IoT jobs
    while also controlling the number of edge nodes that serve them and the number
    of IoT devices that are responsible for their creation. IoT devices in mentioned
    diagram is maintained at 110 Figure 5, 210 Figure 6 and 310 Figure 7. The results
    indicate that expanding the edge node count for a fixed IoT job set leads to a
    decrease in makespan. This is attributed to the higher availability of edge nodes,
    reducing waiting times and task completion duration. The outcomes depicted in
    Figures 5, 6, 7 highlight that our distributed, PBSM, and centralized algorithms
    outperform the Min-Min and Min-Max techniques in terms of makespan. Moreover,
    the distributed approach slightly surpasses the centralized method when dealing
    with varying IoT task volumes. Overall, our approach demonstrates superior makespan
    reduction and scalability compared to Min-Min and Min-Max methods, especially
    with the growth of IoT devices and edge nodes. FIGURE 5. 110 IoT devices. Show
    All FIGURE 6. 210 IoT devices. Show All FIGURE 7. 310 IoT devices. Show All 2)
    Resource Utilization We evaluate resource consumption on edge nodes in various
    studies where these nodes are assisting IoT tasks. The utilization of CPU, RAM,
    and link bandwidth is quantified. Resource consumption is crucial since it helps
    us determine how much money each of the compared techniques will cost edge providers.
    In order to investigate the scalability of the various investigated methodologies,
    we run four distinct experiments for this set of tests, modifying the quantity
    of IoT devices and edge nodes in different configurations. We examine the CPU,
    RAM, and bandwidth usage in Figures 8, 9, 10 for a system with 10 edge nodes and
    100 IoT devices. Further tests are also conducted in this section. FIGURE 8. CPU
    utilization Show All FIGURE 9. RAM utilization. Show All FIGURE 10. Bandwidth
    utilization. Show All From the provided data, we can infer that as the number
    of edge nodes increases while keeping the IoT jobs constant, there is a noticeable
    reduction in resource consumption per edge node. This aligns with expectations,
    as a higher quantity of edge nodes distributing the same workload leads to a lighter
    load on each node. Additionally, the study indicates that both of our algorithms
    consistently achieve near maximum resource utilization across different combinations
    of IoT and edge device quantities. Nevertheless, the resource utilization of the
    Min-Min and Max-Min algorithms exhibits variability, as specific edge nodes experience
    periods of high utilization while others remain partially or entirely idle. In
    contrast, our proposed solution’s preference function considers the resource consolidation
    approach of individual edge nodes. This enables these nodes to optimize resource
    usage effectively, minimizing the necessity for deploying extra edge nodes to
    cater to IoT devices. As a result, this approach proposals the benefit of lightening
    edge providers’ financial responsibilities. 3) Execution Time The execution time
    required by the various solutions under consideration is the third measure we
    take into account in our investigation. To achieve this, we design four distinct
    trials using various combinations of edge node and IoT device numbers. We adjust
    the number of edge nodes in Figure 11 from 10 to 30 and a certain amount of IoT
    devices at 200. In Figure 12, we deploy 400 IoT devices at a fixed number while
    varying the number of edge nodes from 20 to 60. In Figure 13, we increase the
    number of IoT devices from 400 to 800 while fixing the number of edge nodes at
    100. In Figure 14, we modify the number of IoT devices The execution time required
    by the various solutions under consideration is the third measure we take into
    account in our investigation. The initial inference from the data indicates that
    an escalation in the count of IoT devices across several examined methodologies
    corresponds to a proportional increase in execution time. This pattern emerges
    to provide broader task allocation capabilities to edge nodes via diverse algorithms.
    Furthermore, with a surge in the quantity of edge nodes, there is also a corresponding
    elevation in the run time of the diverse algorithms. This phenomenon is attributable
    to the expanded inputs that IoT devices require to take certain inputs into account
    when formulating their preference lists in the presence of additional edge nodes.
    These findings also highlight that our decentralized and centralized algorithms
    exhibit shorter execution times compared to the Min-Min and Max-Min methodologies.
    In comparison to the centralized variant, our distributed solution demonstrates
    improved execution speed. The basis for this is that in the distributed version
    of our technology, edge nodes and IoT devices can connect with one another directly
    without the need for a middleman. This helps to reduce the total communication
    overhead between the various nodes. FIGURE 11. 200 IoT devices alongside diverse
    edge nodes. Show All FIGURE 12. 400 IoT devices alongside diverse edge nodes.
    Show All FIGURE 13. 110 edge nodes alongside diverse IoT devices. Show All FIGURE
    14. 210 edge nodes alongside diverse IoT devices. Show All 4) Contrast Against
    the Ideal Outcome We contrast our solution’s performance with that of the best
    option. The appendix contains the comparison’s findings. SECTION VII. Conclusion
    In this research, we employed game theory to tackle the issue of scheduling time-sensitive
    IoT services in edge computing environments. Our strategy comprises of two main
    parts: IoT devices and edge nodes may be evaluated by each other by: (1) including
    preference functions, and (2) using both centralized and distributed intelligent
    matching algorithms. as well as important indicators like latency and resource
    utilization, to facilitate the allocation of IoT services to suitable edge nodes,
    considering the preferences of all parties involved. The key benefit of our approach
    over the state-of-the-art is that we generate scheduling decisions while taking
    the preferences and limits of both IoT devices and edge nodes into account. We
    evaluate how well our method performs in comparison to two widely used scheduling
    methods, Min-Min and Max-Min, used in cloud and edge computing contexts. The results
    show that our method works better than Min-Min and Max-Min by achieving up to
    a 30% to 40% increase in efficiency for resource utilization on edge nodes. Additionally,
    it reduces the execution time of IoT services by a factor of 2 to 9. Our solution
    also boasts faster execution times and enhanced scalability. In our future work,
    we will focus on an auction-based approach for better utilization of resources
    in a monetary environment. Authors Figures References Keywords Metrics More Like
    This A Fog Computing Framework for Quality of Service Optimisation in the Internet
    of Things (IoT) Ecosystem 2020 2nd International Multidisciplinary Information
    Technology and Engineering Conference (IMITEC) Published: 2020 Multi-user dynamic
    scheduling-based resource management for Internet of Things applications 2018
    International Conference on Internet of Things, Embedded Systems and Communications
    (IINTEC) Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'EdgeMatch: A Smart Approach for Scheduling IoT-Edge Tasks With Multiple
    Criteria Using Game Theory'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mallick S.R.
  - Lenka R.K.
  - Tripathy P.K.
  - Rao D.C.
  - Sharma S.
  - Ray N.K.
  citation_count: '0'
  description: The gathering, processing, transmission, sharing, and storage of healthcare
    data was the core idea behind Healthcare 4.0. Currently, most of the existing
    solutions for offering smart healthcare services rely on cloud-based platforms.
    The main issues with current healthcare systems include storage overhead, processing
    speed, scalability, single points of failure, bandwidth requirements, and device
    connectivity. The recent challenges in the healthcare system motivated us to develop
    a system by integrating the Internet of Medical Things (IoMT), Blockchain, fog
    computing, and InterPlanetary File System (IPFS) to achieve decentralization,
    scalability, security, immutability, and data privacy. In this paper, we propose
    a novel IoMT and Blockchain framework with fog node computing to reduce latency
    and speed up processing, as well as to reduce the network congestion, bandwidth
    requirements, and the main Blockchain network overload. Untrusted devices are
    connected to the system using a proxy monitor, which records the device’s activity.
    In addition, the interplanetary file system (IPFS) is integrated with the Blockchain
    network to store patient data and files in order to ensure the system’s decentralization,
    scalability, security, and privacy. Moreover, we evaluate the proposed framework
    in terms of security, scalability, latency, storage, efficiency, and performance.
    The experimental analysis shows that the proposed framework makes access, searches,
    uploads, and downloads faster and more secure.
  doi: 10.1007/s42979-023-02511-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home SN Computer Science Article A Lightweight,
    Secure, and Scalable Blockchain-Fog-IoMT Healthcare Framework with IPFS Data Storage
    for Healthcare 4.0 Original Research Published: 12 January 2024 Volume 5, article
    number 198, (2024) Cite this article SN Computer Science Aims and scope Submit
    manuscript Soubhagya Ranjan Mallick, Rakesh Kumar Lenka, Pradyumna Kumar Tripathy,
    D. Chandrasekhar Rao , Suraj Sharma & Niranjan Kumar Ray  145 Accesses Explore
    all metrics Abstract The gathering, processing, transmission, sharing, and storage
    of healthcare data was the core idea behind Healthcare 4.0. Currently, most of
    the existing solutions for offering smart healthcare services rely on cloud-based
    platforms. The main issues with current healthcare systems include storage overhead,
    processing speed, scalability, single points of failure, bandwidth requirements,
    and device connectivity. The recent challenges in the healthcare system motivated
    us to develop a system by integrating the Internet of Medical Things (IoMT), Blockchain,
    fog computing, and InterPlanetary File System (IPFS) to achieve decentralization,
    scalability, security, immutability, and data privacy. In this paper, we propose
    a novel IoMT and Blockchain framework with fog node computing to reduce latency
    and speed up processing, as well as to reduce the network congestion, bandwidth
    requirements, and the main Blockchain network overload. Untrusted devices are
    connected to the system using a proxy monitor, which records the device’s activity.
    In addition, the interplanetary file system (IPFS) is integrated with the Blockchain
    network to store patient data and files in order to ensure the system’s decentralization,
    scalability, security, and privacy. Moreover, we evaluate the proposed framework
    in terms of security, scalability, latency, storage, efficiency, and performance.
    The experimental analysis shows that the proposed framework makes access, searches,
    uploads, and downloads faster and more secure. This is a preview of subscription
    content, log in via an institution to check access.  Data availability In order
    to give stakeholders precise information on the accessibility and availability
    of healthcare data stored on the blockchain network, a strong Data Availability
    mechanism is planned to be implemented in. References Gupta R, Tanwar S, Tyagi
    S, Kumar N, Obaidat MS, Sadoun B. Habits: Blockchain-based telesurgery framework
    for healthcare 4.0. In: 2019 International Conference on Computer, Information
    and Telecommunication Systems (CITS). IEEE; 2019. p. 1–5. Google Scholar   Tanwar
    S, Parekh K, Evans R. Blockchain-based electronic healthcare record system for
    healthcare 4.0 applications. J Inf Secur Appl. 2020;50: 102407. Google Scholar   Kumar
    A, Krishnamurthi R, Nayyar A, Sharma K, Grover V, Hossain E. A novel smart healthcare
    design, simulation, and implementation using healthcare 4.0 processes. IEEE Access.
    2020;8:118433–71. Article   Google Scholar   Mallick SR, Sharma S, Tripathy PK,
    Ray NK. Adoption of blockchain-fog-iomt framework in healthcare 4.0 digital revolution.
    In: 2022 OITS International Conference on Information Technology (OCIT). IEEE;
    2022. p. 603–8. Chapter   Google Scholar   Jaleel A, Mahmood T, Hassan MA, Bano
    G, Khurshid SK. Towards medical data interoperability through collaboration of
    healthcare devices. IEEE Access. 2020;8:132302–19. Article   Google Scholar   Sharma
    P, Namasudra S, Crespo RG, Parra-Fuente J, Trivedi MC. Ehdhe: Enhancing security
    of healthcare documents in iot-enabled digital healthcare ecosystems using blockchain.
    Inf Sci. 2023;629:703–18. Article   Google Scholar   Mallick SR, Emri Sharma S.
    A scalable and secure blockchain-based iomt framework for healthcare data transaction.
    In: 2021 19th OITS International Conference on Information Technology (OCIT).
    IEEE; 2021. p. 261–6. Chapter   Google Scholar   Khettry AR, Patil KR, Basavaraju
    AC. A detailed review on blockchain and its applications. SN Comput Sci. 2021;2(1):30.
    Article   Google Scholar   Hashim F, Shuaib K, Zaki N. Sharding for scalable blockchain
    networks. SN Comput Sci. 2022;4(1):2. Article   Google Scholar   Khan M, Naz T.
    Smart contracts based on blockchain for decentralized learning management system.
    SN Comput Sci. 2021;2(4):260. Article   Google Scholar   Alrebdi N, Alabdulatif
    A, Iwendi C, Lian Z. Svbe: Searchable and verifiable blockchain-based electronic
    medical records system. Sci Rep. 2022;12(1):266. Article   Google Scholar   Nguyen
    DC, Pathirana PN, Ding M, Seneviratne A. Blockchain for secure ehrs sharing of
    mobile cloud based e-health systems. IEEE Access. 2019;7:66792–806. Article   Google
    Scholar   Khaydaraliev M, Rhie M-H, Kim K-H. Blockchain-enabled access control
    with fog nodes for independent iots. In: 2022 International Conference on Information
    Networking (ICOIN). IEEE; 2022. p. 78–83. Chapter   Google Scholar   Quy VK, Hau
    NV, Anh DV, Ngoc LA. Smart healthcare iot applications based on fog computing:
    architecture, applications and challenges. Complex Intell Syst. 2022;8(5):3805–15.
    Article   Google Scholar   Monga S, Singh D. Mrbschain a novel scalable medical
    records binance smart chain framework enabling a paradigm shift in medical records
    management. Sci Rep. 2022;12(1):17660. Article   Google Scholar   Biswas K, Muthukkumarasamy
    V, Bai G, Chowdhury MJM. A reliable vaccine tracking and monitoring system for
    health clinics using blockchain. Sci Rep. 2023;13(1):570. Article   Google Scholar   Kumari
    A, Tanwar S, Tyagi S, Kumar N. Fog computing for healthcare 4.0 environment: opportunities
    and challenges. Comput Electr Eng. 2018;72:1–13. Article   Google Scholar   Mahajan
    HB, Rashid AS, Junnarkar AA, Uke N, Deshpande SD, Futane PR, Alkhayyat A, Alhayani
    B. Integration of healthcare 4.0 and blockchain into secure cloud-based electronic
    health records systems. Appl Nanosci. 2023;13(3):2329–42. Article   Google Scholar   Shynu
    P, Menon VG, Kumar RL, Kadry S, Nam Y. Blockchain-based secure healthcare application
    for diabetic-cardio disease prediction in fog computing. IEEE Access. 2021;9:45706–20.
    Article   Google Scholar   Mallick SR, Goswami V, Lenka RK, Sharma S, Barik RK.
    Performance evaluation of queueing assisted iomt-fog-blockchain framework for
    healthcare organizations. In: 2022 IEEE 9th Uttar Pradesh Section International
    Conference on Electrical, Electronics and Computer Engineering (UPCON). IEEE;
    2022. p. 1–6. Google Scholar   Ray PP, Dash D, Salah K, Kumar N. Blockchain for
    iot-based healthcare: background, consensus, platforms, and use cases. IEEE Syst
    J. 2020;15(1):85–94. Article   Google Scholar   Debe M, Salah K, Rehman MHU, Svetinovic
    D. Iot public fog nodes reputation system: A decentralized solution using ethereum
    blockchain. IEEE Access. 2019;7:178082–93. Article   Google Scholar   Nguyen DC,
    Pathirana PN, Ding M, Seneviratne A. Bedgehealth: A decentralized architecture
    for edge-based iomt networks using blockchain. IEEE Internet Things J. 2021;8(14):11743–57.
    Article   Google Scholar   Shukla S, Thakur S, Hussain S, Breslin JG, Jameel SM.
    Identification and authentication in healthcare internet-of-things using integrated
    fog computing based blockchain model. Internet Things. 2021;15: 100422. Article   Google
    Scholar   Alzoubi YI, Al-Ahmad A, Kahtan H. Blockchain technology as a fog computing
    security and privacy solution: an overview. Comput Commun. 2022;182:129–52. Article   Google
    Scholar   Zhuang Y, Sheets LR, Chen Y-W, Shae Z-Y, Tsai JJ, Shyu C-R. A patient-centric
    health information exchange framework using blockchain technology. IEEE J Biomed
    Health Inform. 2020;24(8):2169–76. Article   Google Scholar   Azbeg K, Ouchetto
    O, Andaloussi SJ. Blockmedcare: A healthcare system based on iot, blockchain and
    ipfs for data management security. Egypt Inform J. 2022;23(2):329–43. Article   Google
    Scholar   Shen B, Guo J, Yang Y. Medchain: Efficient healthcare data sharing via
    blockchain. Appl Sci. 2019;9(6):1207. Article   Google Scholar   Baniata H, Kertesz
    A. A survey on blockchain-fog integration approaches. IEEE Access. 2020;8:102657–68.
    Article   Google Scholar   Uddin MA, Stranieri A, Gondal I, Balasubramanian V.
    A survey on the adoption of blockchain in iot: challenges and solutions. Blockchain
    Res Appl. 2021;2(2): 100006. Article   Google Scholar   Sun Y, Lo FP-W, Lo B.
    Security and privacy for the internet of medical things enabled healthcare systems:
    a survey. IEEE Access. 2019;7:183339–55. Article   Google Scholar   Bachtobji
    S, Kouicem DE, Mabrouk MB. Towards blockchain based architecture for building
    information modelling (bim). In: 2022 5th Conference on Cloud and Internet of
    Things (CIoT). IEEE; 2022. p. 53–9. Chapter   Google Scholar   Yang L, Li M, Zhang
    H, Ji H, Xiao M, Li X. Distributed resource management for blockchain in fog-enabled
    iot networks. IEEE Internet Things J. 2020;8(4):2330–41. Article   Google Scholar   Núñez-Gómez
    C, Caminero B, Carrión C. Hidra: A distributed blockchain-based architecture for
    fog/edge computing environments. IEEE Access. 2021;9:75231–51. Article   Google
    Scholar   Lei K, Du M, Huang J, Jin T. Groupchain: Towards a scalable public blockchain
    in fog computing of iot services computing. IEEE Trans Serv Comput. 2020;13(2):252–62.
    Article   Google Scholar   Download references Acknowledgements The authors gratefully
    credit the IoT and Cloud Computing Lab of IIIT Bhubaneswar for providing computational
    resources. Author information Authors and Affiliations Computer Science and Engineering,
    International Institute of Information Technology, Bhubaneswar, Odisha, 751003,
    India Soubhagya Ranjan Mallick & Rakesh Kumar Lenka Computer Science and Engineering,
    Silicon Institute of Technology, Bhubaneswar, Odisha, 751024, India Pradyumna
    Kumar Tripathy Information Technology, Veer Surendra Sai University of Technology,
    Burla, Odisha, 768018, India D. Chandrasekhar Rao Computer Science and Engineering,
    Guru Ghasidas Vishwavidyalaya, Bilaspur, Chhattisgarh, 495009, India Suraj Sharma
    School of Computer Science and Engineering, Kalinga Institute of Industrial Technology,
    Bhubaneswar, Odisha, 751024, India Niranjan Kumar Ray Corresponding authors Correspondence
    to Rakesh Kumar Lenka, Pradyumna Kumar Tripathy or D. Chandrasekhar Rao. Ethics
    declarations Conflict of interest The authors declare that they have no competing
    interests. Additional information Publisher''s Note Springer Nature remains neutral
    with regard to jurisdictional claims in published maps and institutional affiliations.
    This article is part of the topical collection “Innovation in Smart Things: A
    Systems, Security, and AI Perspective” guest edited by Niranjan K Ray, Prasanth
    Yanambaka and Rakesh Balabantaray. Rights and permissions Springer Nature or its
    licensor (e.g. a society or other partner) holds exclusive rights to this article
    under a publishing agreement with the author(s) or other rightsholder(s); author
    self-archiving of the accepted manuscript version of this article is solely governed
    by the terms of such publishing agreement and applicable law. Reprints and permissions
    About this article Cite this article Mallick, S.R., Lenka, R.K., Tripathy, P.K.
    et al. A Lightweight, Secure, and Scalable Blockchain-Fog-IoMT Healthcare Framework
    with IPFS Data Storage for Healthcare 4.0. SN COMPUT. SCI. 5, 198 (2024). https://doi.org/10.1007/s42979-023-02511-8
    Download citation Received 19 April 2023 Accepted 20 November 2023 Published 12
    January 2024 DOI https://doi.org/10.1007/s42979-023-02511-8 Keywords Blockchain
    IoMT Fog computing Security Efficiency Scalability IPFS Associated Content Part
    of a collection: Innovation in Smart Things: A Systems, Security, and AI Perspective
    Access this article Log in via an institution Buy article PDF USD 39.95 Price
    excludes VAT (USA) Tax calculation will be finalised during checkout. Instant
    access to the full article PDF. Rent this article via DeepDyve Institutional subscriptions
    Sections Figures References Abstract Data availability References Acknowledgements
    Author information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: SN Computer Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Lightweight, Secure, and Scalable Blockchain-Fog-IoMT Healthcare Framework
    with IPFS Data Storage for Healthcare 4.0
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Medhi K.
  - Hussain M.I.
  citation_count: '0'
  description: The Internet of Things can provide promising solutions for quick, reliable,
    and smart healthcare services to patients and health workers. Such smart healthcare
    devices generate a large chunk of data continuously at a very rapid rate. These
    healthcare data are required to be processed and action needs to be taken to save
    lives in real time. However, the traditional IoT-based healthcare architectures
    use the remote cloud as the processing and analysing unit which increases the
    response time and decreases the reliability of the service. To address this problem,
    researchers have provided several solutions such as Edge, Fog, and Dew computing
    to improve the reliability and service time. Dew computing enables the IoT architectures
    to provide important services even in offline conditions with ultra-low latency.
    In this paper, a detailed analysis of Dew computing, its use-cases in healthcare,
    its advantages and limitations, and different service architectures are presented
    in detail. A comparison with other IoT computing architectures is also discussed
    with different parameters. The survey also presents the state-of-the-art in this
    domain indicating various research issues in dew computing while used in healthcare
    applications. Finally, different future research directions are also pointed out.
  doi: 10.1007/978-981-99-4590-0_11
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Dew Computing pp 225–239Cite as
    Home Dew Computing Chapter Role of Dew Computing in Smart Healthcare Applications
    Kishore Medhi & Md. Iftekhar Hussain  Chapter First Online: 03 September 2023
    133 Accesses Part of the book series: Internet of Things ((ITTCC)) Abstract The
    Internet of Things can provide promising solutions for quick, reliable, and smart
    healthcare services to patients and health workers. Such smart healthcare devices
    generate a large chunk of data continuously at a very rapid rate. These healthcare
    data are required to be processed and action needs to be taken to save lives in
    real time. However, the traditional IoT-based healthcare architectures use the
    remote cloud as the processing and analysing unit which increases the response
    time and decreases the reliability of the service. To address this problem, researchers
    have provided several solutions such as Edge, Fog, and Dew computing to improve
    the reliability and service time. Dew computing enables the IoT architectures
    to provide important services even in offline conditions with ultra-low latency.
    In this paper, a detailed analysis of Dew computing, its use-cases in healthcare,
    its advantages and limitations, and different service architectures are presented
    in detail. A comparison with other IoT computing architectures is also discussed
    with different parameters. The survey also presents the state-of-the-art in this
    domain indicating various research issues in dew computing while used in healthcare
    applications. Finally, different future research directions are also pointed out.
    Keywords Dew computing Healthcare IoT Internet of medical things Access provided
    by University of Nebraska-Lincoln. Download chapter PDF 1 Introduction The advancement
    of the computer and Internet technologies has begun a new era where an individual
    and devices can exchange information over the Internet [1]. This information exchange
    trend has led to the development of a new technology known as the Internet of
    Things (IoT) [1, 2]. Using IoT technology, any object on the earth can be identified,
    monitored, and controlled via the Internet [3]. In 1999, Kevin Ashton introduced
    the term IoT, and he proved that by using IoT, any physical objects could be connected
    easily with intelligent devices, giving us a remarkable opportunity to create
    a smart environment [4]. In IoT, the things represents the sensors, actuators,
    tags, camera, smart devices, etc. With the help of these things, people can easily
    interact with their surrounding physical environment. IoT can be applied in different
    domains such as industry, healthcare, agriculture, and environmental monitoring.
    In recent years, the successful use of IoT in healthcare has given us tremendous
    capacity to improve healthcare facilities and human lifestyles. The market for
    the Internet of Medical Things (IoMT) and wearable healthcare technology has been
    growing exponentially and transforming the healthcare industry from treatment
    to prevention. A range of devices, from smartphones to smart wristbands, are used
    to capture different healthcare data and monitor the body’s condition. All these
    healthcare devices use various biomedical sensors such as ECG, EEG, EMG, temperature,
    and pressure. Data from such sensors are collected, stored, analysed, and key
    decisions are taken from them. The primary purpose of the healthcare device is
    to analyse the different body parameters of a person and provide correct assistance
    and guidance. Moreover, using these healthcare devices, we can make some early
    predictions and perform continuous monitoring of patients, reducing the mortality
    rate and improving lifestyle. IoT-based healthcare technologies are being researched
    extensively. Numerous computing techniques, including cloud, Fog, and edge computing,
    are used while creating healthcare applications [5]. In general, cloud architectures
    are typically located at distant Internet locations. Different types of wired
    and wireless network technologies make it possible to connect to the cloud. Smart
    healthcare solutions demand quick decisions in order to enhance the user’s condition.
    Because of the overall complexity and variety of components, the network is inappropriate
    for crucial IoT applications. Transmission of information between the end-user
    devices and the cloud usually takes a long time. Later, different computing architectures,
    such as fog and edge computing strategies, are suggested to ease these problems
    and aid in low-latency decision-making. By processing the frequently requested
    data near the user’s current network’s edge, such a computing solution intends
    to reduce the user’s reliance on cloud or fog resources. Consequently, the overall
    network security is improved. However, Edge computing is unable to offer the user
    point-of-call service, extreme scalability, or network independence [6]. To address
    these situations, time-sensitive data are processed near the source compared to
    the fog or edge layer, and the necessary action is decided and communicated to
    the consumers. However, because of the user’s mobility, the network structure
    and Internet connection speed extensively affect the overall performance of the
    healthcare systems [7]. Even in the case of an unstable or disconnected network,
    a healthcare application needs real-time responses. Massive attempts are being
    made to overcome these limitations, but they have not been very successful. The
    current state-of-the-art technologies are insufficient for the IoT-based intelligent
    e-healthcare system, which requires an extremely intelligent and ultra-low latency
    decision-making system [8]. The main issues with the current healthcare infrastructure
    are: 1. devices with limited resources. 2. a need for lightweight analytical modules.
    3. a requirement for a smart network and an Internet connection. The current cloud-based
    or other computing-based solutions are inadequate since they have a high delay
    requirement [9]. In regards to this, the industry has recently seen the introduction
    of the new Dew computing architecture, which offers flexibility in terms of ultra-low
    latency judgments, mobility, user control, and patient-specific requirements.
    The underlying idea of cloud computing is carried through by Dew computing to
    the end devices (e.g. Mobile phones, gateway nodes, raspberry pi, and personal
    computers). It addresses the crucial problems related to cloud computing technologies,
    namely reliance on the internet. Dew computing frameworks can be set up on a tiny,
    light Raspberry Pi device unlike the Cloud and Fog computing frameworks. Dew computing
    is substantially different from cloud and fog computing in crucial IoT applications
    and more feasible due to its qualities like ultra-low latency, zero Internet need,
    mobility in offline environments, and quick response time. Cloud computing on
    a smaller scale can be called “dew computing”, and it is situated close to the
    data source. Similar to a cloud, it is capable of carrying out all of the aforementioned
    functions (data preparation, analysis, storage, and transmission), but it is limited
    in terms of computational power. Table 1 Comparison of some existing e-healthcare
    technologies Full size table A detailed analysis of different computing architectures
    is given in Table 1. Dew computing can operate even when there is no active Internet
    connection. The patient bodies are connected to end devices in smart healthcare,
    which collect numerous health metrics to track various healthcare-related issues.
    Healthcare solutions based on dew can prepare potential future services and provide
    necessary user-specific decisions as needed. Individual fog modules are taken
    into consideration for an offline solution by extremely localised computing platforms
    like Dew computing [10]. This platform offers support for location-aware, user-specific,
    anytime, anywhere access. However, this dew platform needs to be properly designed
    with smart, lightweight analytical modules, and judgments in order to provide
    full smart healthcare IoT solutions. In Dew devices, machine learning-based solutions
    show remarkable performance in detecting body abnormalities, but they face certain
    challenges in some instances, like i. limited amount of training data ii. complex
    computational model for lightweight devices iii. dynamic nature of healthcare
    data. The rest of the paper is organised as follows. The background studies of
    computing architectures, such as the disadvantages of cloud computing and the
    advantages of dew computing, are explained in Sects. 2 and 3, respectively. Section
    3.2 describes the general architectures of the dew device. Literature reviews
    of various existing dew-based healthcare architectures are described in Sect.
    4. The explanation of the conducted survey is explained in Sect. 4.9. At last,
    the paper is concluded in Sect. 5. 2 Performance Issues in Centralised Cloud Computing
    In order to offer distant services through the Internet, cloud computing works
    in concert with hardware and software. To offer its clients services, cloud computing
    mainly relies on remote equipment. Due to the increase in sensors and data production,
    cloud computing is struggling to digest the data swiftly and struggles to respond
    to users more quickly [11]. Some of the common problems are discussed below. 1.
    Large volume of data: With the increase in the number of IoT devices, the generation
    of sensing data is also increasing, and it increases the requirement for data
    processing day by day. 2. High Latency: Critical IoT applications like healthcare,
    smart traffic, etc. require quick responses to provide emergency services to clients.
    However, cloud computing can’t provide quick services because of its long distance
    between the Cloud and the IoT device [12]. 3. Downtime: Due to huge volume of
    data processing requirement the services of cloud become unavailable and increases
    the network failure [12]. 4. Security: Cloud server can be accessed globally by
    different users which increases the risk for the private data shared from different
    sources. 5. Limited Flexibility: Cloud computing servers are totally managed by
    the third party service provider which leads to limited control to users and reduces
    the data security. 6. Bandwidth Cost: To provide quick services by the cloud computing,
    heavy communication channels are required which increases the cost of the system.
    3 Dew Computing as a Solution Dew computing was originally developed to reduce
    the traffic load in the cloud server, which is one of the major limitations of
    cloud computing [13]. In dew computing, data can be processed near the source
    of generation, which reduces the response time and can provide required services
    during offline conditions also. It will work like a mini Cloud, managing tasks
    and giving responses locally within a region. A diagrammatic representation of
    Dew computing architecture is shown in Fig. 1. Fig. 1 Architecture of dew device
    [13] Full size image 3.1 Advantages of Dew Computing 1. Low Latency: As the Dew
    Computing devices are deployed near the sensors, the data transfer time becomes
    negligible, which finally reduces the response time. 2. High Bandwidth: In dew
    computing, all the devices are set up locally, which increases the fast connection
    and availability of devices and reduces the downtime. 3. High Security: In dew
    computing, the data are processed locally, and the dew server can be accessed
    by local users only; as a result, the security of the data becomes increases.
    4. Better User Experience: Dew computing provides quick and correct responses
    to user applications, which increases user satisfaction and reliability. Fig.
    2 A dew-based healthcare architecture developed by Medhi et al. [13] Full size
    image 3.2 Architecture of Dew device To order to set up a fresh dew computing-based
    system for the healthcare industry, it is essential to perform all execution locally
    instead of in the cloud, with all the required services. Dew computing’s primary
    goal is to deliver a user-centric, adaptable, personalised, and immediate reaction
    to a particular task. With the help of dew architecture, we can quickly minimise
    reliance on the Internet. We can set up a Dew architecture on a resource constraint
    devices such as raspberry pi and mobile smartphone devices. Portable smartphones
    have a huge potential to contribute to Dew architecture because they can operate
    for long period without energy. With a smartphone-based mobile device, it is easier
    to give the services when the user is travelling from one location to another.
    A user’s tailored health assistant can be provided via the dew computing-based
    architecture without relying on the current computing model. As shown in Fig.
    1, every lightweight dew device uses a specialised module of the dew framework
    to function, such as cloud synchronisation, lightweight analysis, GUI, a dew-specific
    database, and dew server. 1. Manager: The dew manager starts all the modules of
    the dew device by calling the functionalities. 2. Communication Interface: As
    shown in Fig. 2, each dew device can communicate with the body sensors (BS) and
    other computing devices using any of the networking technology like WiFi, Bluetooth,
    Zigbee, etc. Medhi et. al. [13] use the MQTT message transferring protocol for
    exchanging the message between BS and dew device, where each BS working as a publisher
    and sends message to the dew client through the MQTT broker. The architecture
    of the MQTT message transferring is shown in Fig. 3. 3. Device Handler: It discovered
    all the new nodes and started communication between them. Using the message-passing
    method, the device handler keeps the connection open between the BS and its server.
    4. Dew Server: In the dew server, all the receiving data are preprocessed, filtered,
    and the noises are eliminated. Following the pre-processing, a quick analysis
    is carried out to look for any irregularities. 5. Dew Database: All the results
    produced by the server are stored in a little database called a Dew database.
    It mainly helps to store the result during offline conditions. Different dew devices
    use the MySQL database as the dew database. 6. GUI: It manages user communication
    and presents the findings of all analyses. It also has access to all of the dew
    software modules’ settings. 7. Cloud Gateway Interface: It maintains all the links
    with the edge device or the cloud server to provide and get control instructions
    for the connected IoT devices. Fig. 3 A dew-based healthcare architecture developed
    by Manocha et al. [14] Full size image 4 State-of-the-Art Dew Architectures for
    Healthcare Different researchers have used IoT in various domains such as healthcare,
    intelligent traffic, and emergency management. Among all the applications, healthcare
    needs various time-critical services, and this can be fulfilled by dew devices
    because of the extreme edge location. A detailed analysis of various dew computing-based
    healthcare applications is discussed below. 4.1 Dew-Based Offline Computing Architecture
    for Healthcare IoT [13] A dew health architecture was created by Medhi et al.
    [13] utilising a mobile smartphone. They used the Pydroid mobile programme to
    instal the Python environment on an Android smartphone in order to evaluate the
    data. The mobile device interacted with the MQTT publisher as a MQTT client through
    the MQTT broker. The ECG dataset obtained from the Physionet repository was used
    in this experiment. The ECG dataset remained in the MQTT publisher throughout
    the experiment and, like the sensor device it continuously provides data to the
    MQTT client at regular intervals. The MQTT client executed the trained CNN module
    as soon as a signal was received, reported the outcomes, and saved the information
    to a nearby MySQL server. Each single device gathers real-time streaming health
    data from several users, does the necessary analysis, and then provides the user
    current health status, the architecture is shown in Fig. 1. Each dew device gathers
    and analyses BS data from the linked patients, and it is shown in Fig. 2. With
    the aid of the dew GUI, the analysis module provides the analysis information
    of the patient’s condition. When the dew server is offline, it connects to the
    local database management system to update the database with the most recent details
    on current medical problems. The dew synchronizer updates the data that is saved
    in the cloud for later use by synchronising the dew device with it at predefined
    intervals. 4.2 Dew Computing-Inspired Health-Meteorological Factor Analysis for
    Early Prediction of Bronchial Asthma [14] A cyber-physical system (CPS) based
    on dew-cloud assistance was created by Manocha et al. [14] to examine the relationship
    between people’s meteorological and health indicators. The main goal of the effort
    is to identify the health issues brought on by the erratic scale of meteorological
    elements in real time. The development of a dew-based smart gadget allows for
    the use of sensors to gather information from the interior environment that is
    omnipresent and has a significant direct or indirect impact on an individual’s
    health. Utilising the Weighted-Naive Bayes data classification technique, the
    data is examined to assess the likely irregular health events. Additionally, the
    Adaptive Neuro-Fuzzy Inference System (ANFIS) is used to calculate a unifying
    factor over the temporal scale and assess the association between meteorological
    and health parameters. The suggested model is put into practice at four different
    schools in Jalandhar to validate the monitoring performance. 4.3 MedGini: Gini
    Index-Based Sustainable Health Monitoring System Using Dew Computing [15] Intelligent
    health apps often include biosignal monitoring. A novel route for monitoring biosignals
    is offered via the Internet of Health Things (IoHT). The design of data synchronisation
    with the cloud server is inadequate in the existing dew-based healthcare systems.
    To address this, a model is created for a sustainable health monitoring system
    that effectively utilises cloud-dew architecture. Various bio sensor nodes are
    employed to dynamically monitor the biosignals. To enable seamless accessibility
    and the best scalability of the data, all acquired data is momentarily stored
    in the dew database and synchronised with the cloud. Gini index and Shannon entropy
    were utilised to provide the best possible data synchronisation in order to guarantee
    (Fig. 4). Fig. 4 Dew-based healthcare architecture developed by [15] Full size
    image 4.4 CONFRONT: Cloud-Fog-Dew-Based Monitoring Framework for COVID-19 Management
    [16] In recent times, the use of IoT in the healthcare industry has been growing
    because and the low-cost sensors. Using IoT, we can cover a large area with a
    limited number of sensors, which makes it an important tool to fight against situations
    like the COVID-19 pandemic. In this research, a cloud-based monitoring framework
    for COVID-19 administration was suggested. This cloud, fog, and dew-based healthcare
    paradigm may help with early diagnosis and monitoring of patients even during
    quarantine or after undergoing home-based therapies. The reduced bandwidth requirements
    of the fog architecture guarantee that the model is appropriate for real-time
    settings. Cloud servers are used because of their scalable computing and storage
    capacity to analyse big-scale COVID-19 statistics data for obtaining aggregate
    information on the illness’s spread. Faster application uptime is achieved by
    the dew architecture, which makes sure that the application is still accessible
    at a small scale, even when cloud connectivity is lost (Fig. 5). Fig. 5 Dew-based
    healthcare architecture developed by [16] Full size image 4.5 AI Cardiologist
    at the Edge: A Use Case of a Dew Computing Heart Monitoring Solution [17] Applications
    that deal with the IoT, pervasive computing, and ubiquitous computing are what
    postcloud architectures are known for. The foundation of edge computing solutions
    is the addition of extra processing layers between end-user devices and cloud
    data centres to take advantage of quicker processing and smaller data transport
    requirements. It is categorised as a dew computing solution since these smart
    devices’ edge requirements for autonomous performance even without Internet support.
    In order to bring computing closer to the user in wearable sensors, edge computing
    devices should execute trillions of operations per second while consuming incredibly
    little power must be built. However, this is already being done by different industries
    with the newest chips used by smartphone makers. Although edge computing has been
    around for a while, no specific killer application has yet to drive widespread
    adoption. 4.6 Dew-Cloud-Based Hierarchical Federated Learning for Intrusion Detection
    in IoMT [18] Due to the overwhelming impact of the coronavirus epidemic on healthcare
    facilities, doctors are now forced to treat and diagnose patients from remote
    places. In addition, COVID-19 has increased human awareness of their health, leading
    to a significant increase in the purchasing of IoT-enabled medical devices. The
    rapid expansion of the Internet of medical things (IoMT) sector attracted cybercriminals
    for different attacks. Nowadays, medical health information data are extremely
    important and sensitive on the dark web. Despite the fact that it has not been
    adequately safeguarded, allowing trespassers to misuse the patient’s medical information.
    Due to the resource-constrained network devices’ limited storage and processing
    power, the system administrator is not able to strengthen security measures. Even
    though many supervised and unsupervised ML algorithms have been created to detect
    abnormalities, the main goal is to investigate rapidly developing hostile attacks
    before they compromise the integrity of the healthcare system. Using a Dew-Cloud-based
    model, hierarchical federated learning is made possible in this research. The
    suggested Dew-Cloud paradigm offers enhanced IoMT essential application availability
    along with a higher level of data privacy. At distributed Dew servers, the hierarchical
    long-term memory (HLSTM) concept is implemented with cloud computing as the backend.
    In the suggested model, the data pre-processing feature aids in achieving high
    training accuracy (99.31%) with little training loss (0.034%). The results of
    the trial show that the suggested HFL-HLSTM model outperforms existing plans in
    terms of performance indicators including precision, recall, accuracy, and f-score.
    4.7 Dew Computing-Assisted Cognitive Intelligence-Inspired Smart Environment for
    Diarrhea Prediction [19] Diarrhea is the most prevalent infectious disease influencing
    people of all ages and poses a severe public health risk worldwide. Diarrhea is
    primarily brought on by poor food quality, contaminated water, indoor and outdoor
    weather conditions, and meteorological factors. In order to examine the relationship
    between a person’s health, food condition, and indoor meteorological conditions
    to anticipate the source of diarrhea with the degree of severity. An adaptive
    dew computing-assisted surveillance system is developed to gather the individual’s
    diet, indoor weather, and specific health metrics. All the information are gathered
    in the physical layer using different smart sensors. At the cyber layer, the collected
    events are categorised using the Probabilistic Weighted-Naive Bayes (PWNB) classification
    method for estimating anomalous health occurrences. A Multi-scale Gated Recurrent
    Unit (M-GRU) is also recommended in order to determine the severity of irregular
    health, food, and environmental occurrences by examining their correlation. Accordingly,
    the suggested M-GRU model has attained a very high precision accuracy value of
    (93.26%), while LSTM, RNN, and SVM have attained precision values of (89.13%),
    (90.43%), and (88.23%), respectively. Additionally, the PW-precision NB’s value
    is (97.15%), which is greater than both KNN (93.25%) and DT (96.91%). In both
    the dew computing and cloud computing, the suggested methods’ results display
    greater Precision values. Additionally, in terms of event classification, severity
    assessment, monitoring stability, and prediction effectiveness, a comparative
    study characterises the proposed solution’s prediction efficacy relative to a
    number of alternative decision-making systems (Fig. 6). Fig. 6 Dew-based healthcare
    architecture developed by [19] Full size image 4.8 Real-Time Event-Driven Sensor
    Data Analytics at the Edge-Internet of Things for Smart Personal Healthcare [20]
    For the IoMT-based smart e-Healthcare, real-time service has become an essential
    requirement to operate effectively. There have been a number of tries to improve
    this area of technology, but there is a critical need to enhance the incorporation
    of lightweight IoT-based frameworks. In this research, two separate experiments
    are presented that deal with real-time visualisation, charting, and analytics
    while utilising real time and readily available Javascript frameworks, such as
    Node.js, EON.js, Chart.js, Express Server, and Socket.io. The goal of this research
    is to examine real-time analytics behaviour based on the IoT in a scenario where
    e-health sensors are being installed efficiently. The study’s findings support
    the development and use of open-source, IoT-based Javascript frameworks for supplying
    real-time sensor data analytics in the e-Healthcare industry (Fig. 7). Table 2
    A detailed comparison of different dew computing-based healthcare applications
    Full size table Fig. 7 A dew-based healthcare architecture developed by [20] Full
    size image 4.9 Synthesis A detailed comparison of all the above-mentioned existing
    dew-based experiments is mentioned in Table 2. From this literature survey, it
    is clear that for applications like healthcare, quick response is one of the major
    concerns in fulfilling the user’s requirements. However, due to the remote location,
    the cloud server fails to fulfil this demand. To solve this, all the above-mentioned
    experiments used dew computing as an alternative solution to process data near
    the source and reduced the overall latency. Different researchers have used various
    devices as dew nodes. For example, Medhi et al. [13], Manocha et al. [14], Karmakar
    et al. [15] use smartphones, raspberry pi, and Arduino Uno respectively as dew
    devices. However, due to the lightweight nature of dew devices, the different
    authors used different lightweight analytical models. Such as lightweight CNN
    [13], where the number of arithmetic operations is reduced by using the K-mean.
    From all these experiments, it is clear that using the dew device; it is possible
    to fulfil all the requirements of healthcare applications by processing data and
    giving quick responses to the users. 5 Conclusions This chapter examined various
    IoT-based healthcare systems critically and covered how dew computing technology
    can be used to satisfy the needs. The dew solution overcomes major challenges
    like time-criticality, lightweight services, flexibility, service during offline
    conditions, etc., which are essential for smart healthcare applications. To complete
    this literature survey, we have used more than ten different latest dew-based
    healthcare research articles collected from various reputed journals. All these
    papers used different devices, such as Raspberry Pi, Arduino Uno, and Smartphone,
    to instal the dew-based computing architecture. All of the created dew models
    effectively carry out various duties at the very edge of the network using various
    lightweight analytical modules. From this critical review, it is clear that using
    the dew computing architecture, we can easily overcome the issues like delay,
    response time, power consumption, CPU usage, memory consumption, and accuracy.
    We plan to establish a real-time testbed for evaluating the performance of dew-based
    healthcare schemes. References Atzori, L., Iera, A., Morabito, G.: The internet
    of things: a survey. Comput. Netw. 54(15), 2787–2805 (2010) Article   MATH   Google
    Scholar   Rezaeipanah, A., Nazari, H., Ahmadi, G.: A hybrid approach for prolonging
    lifetime of wireless sensor networks using genetic algorithm and online clustering.
    J. Comput. Sci. Eng. 13(4), 163–174 (2019) Article   Google Scholar   Alaa, M.,
    Zaidan, A.A., Zaidan, B.B., Talal, M., Kiah, M.L.M.: A review of smart home applications
    based on internet of things. J. Netw. Comput. Appl. 97, 48–65 (2017) Google Scholar   Sobin,
    C.C.: A survey on architecture, protocols and challenges in IoT. Wirel. Pers.
    Commun. 112(3), 1383–1429 (2020) Article   Google Scholar   Ray, P.P.: An introduction
    to dew computing: definition, concept and implications. IEEE Access 6, 723–737
    (2017) Article   Google Scholar   Ray, P.P., Dash, D., De, D.: Edge computing
    for internet of things: a survey, e-healthcare case study and future direction.
    J. Netw. Comput. Appl. 140, 1–22 (2019) Article   Google Scholar   Ahmed, E.,
    Ahmed, A., Yaqoob, I., Shuja, J., Gani, A., Imran, M., Shoaib, M.: Bringing computation
    closer toward the user network: is edge computing the solution? IEEE Commun. Mag.
    55(11), 138–144 (2017) Article   Google Scholar   Xiong, Y., Sun, Y., Xing, L.,
    Huang, Y.: Extend cloud to edge with KubeEdge. In: ACM/IEEE Symposium on Edge
    Computing (SEC), pp. 373–377 (2018) Google Scholar   Ali, O., Ishak, M.K.: Bringing
    intelligence to iot edge: machine learning based smart city image classification
    using microsoft Azure IoT and custom vision. J. Phys.: Conf. Ser. 1529, 042–076
    (2020) Google Scholar   Ray, P.P., Dash, D., De, D.: Internet of things-based
    real-time model study on e-healthcare: device, message service and dew computing.
    Comput. Netw. 149, 226–239 (2019) Article   Google Scholar   Botta, A., De Donato,
    W., Persico, V., Pescapé, A.: Integration of cloud computing and internet of things:
    a survey. Fut. Gener. Comput. Syst. 56, 684–700 (2016) Article   Google Scholar   Saini,
    K., Raj, P.: Chapter eight-edge platforms, frameworks and applications. Adv. Comput.
    127, 237–258 (2022) Article   Google Scholar   Medhi, K., Ahmed, N., Hussain,
    M.I.: Dew-based offline computing architecture for healthcare IoT. ICT Express
    8(3), 371–378 (2022) Article   Google Scholar   Manocha, A., Bhatia, M., Kumar,
    G.: Dew computing-inspired health-meteorological factor analysis for early prediction
    of bronchial asthma. J. Netw. Comput. Appl. 179, 102995 (2021) Google Scholar   Karmakar,
    A., Banerjee, P.S., De, D.,Bandyopadhyay, S., Ghosh, P.: Medgini: Gini index based
    sustainable health monitoring system using dew computing. Med. Novel Technol.
    Dev. 100145 (2022) Google Scholar   Poonia, A., Ghosh, S., Ghosh, A., Nath, S.B.,
    Ghosh, S.K., Buyya, R.: Confront: Cloud-fog-dew based monitoring framework for
    covid-19 management. Internet Things 16, 100459 (2021) Google Scholar   Gusev,
    M.: AI cardiologist at the edge: a use case of a dew computing heart monitoring
    solution. In: Artificial Intelligence and Machine Learning for EDGE Computing,
    pp. 469–477 (2022) Google Scholar   Singh, P., Gaba, G.S., Kaur, A., Hedabou,
    M., Gurtov, A.: Dew-cloud-based hierarchical federated learning for intrusion
    detection in IOMT. IEEE J. Biomed. Health Inform. (2022) Google Scholar   Afaq,
    Y., Manocha, A.: Dew computing-assisted cognitive intelligence-inspired smart
    environment for diarrhea prediction. Computing 104(11), 2511–2540 (2022) Article   Google
    Scholar   Ray, P.P., Dash, D., De, D.: Real-time event-driven sensor data analytics
    at the edge-internet of things for smart personal healthcare. J. Supercomput.
    76(9), 6648–6668 (2020) Article   Google Scholar   Download references Author
    information Authors and Affiliations Department of Information Technology, North-Eastern
    Hill University, Shillong, 793 022, Meghalaya, India Kishore Medhi & Md. Iftekhar
    Hussain Corresponding author Correspondence to Kishore Medhi . Editor information
    Editors and Affiliations Department of Computer Science and Engineering, Maulana
    Abul Kalam Azad University of Technology, West Bengal, Kolkata, India Debashis
    De Department of Computer Science and Engineering, D Y Patil International University,
    Pune, India Samarjit Roy Rights and permissions Reprints and permissions Copyright
    information © 2024 The Author(s), under exclusive license to Springer Nature Singapore
    Pte Ltd. About this chapter Cite this chapter Medhi, K., Hussain, M.I. (2024).
    Role of Dew Computing in Smart Healthcare Applications. In: De, D., Roy, S. (eds)
    Dew Computing. Internet of Things. Springer, Singapore. https://doi.org/10.1007/978-981-99-4590-0_11
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-4590-0_11
    Published 03 September 2023 Publisher Name Springer, Singapore Print ISBN 978-981-99-4589-4
    Online ISBN 978-981-99-4590-0 eBook Packages Engineering Engineering (R0) Share
    this chapter Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Download book PDF Download book
    EPUB Sections Figures References Abstract Introduction Performance Issues in Centralised
    Cloud Computing Dew Computing as a Solution State-of-the-Art Dew Architectures
    for Healthcare Conclusions References Author information Editor information Rights
    and permissions Copyright information About this chapter Publish with us Discover
    content Journals A-Z Books A-Z Publish with us Publish your research Open access
    publishing Products and services Our products Librarians Societies Partners and
    advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress
    Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Internet of Things
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Role of Dew Computing in Smart Healthcare Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Seyedi Z.
  - Rahmati F.
  - Ali M.
  - Liu X.
  citation_count: '0'
  description: Vehicular fog computing (VFC) is a powerful technology that extends
    fog computing to regular vehicular networks, providing end-users and vehicles
    with ultra-low-latency services. It enables connected Vehicular Fog Nodes (VFNs)
    to process real-time data and promptly respond to user queries. However, touching
    unencrypted data by VFNs raises security challenges definitely, the top of which
    is confidentiality, and giving encrypted data to VFNs causes other problems such
    as encrypted data processing. Apart from these, how to inspect and encourage VFNs
    to provide a secure, honest, and user-satisfactory network is of vital importance
    to this area. To address these challenges, we design a novel fine-grained data
    management (FGDM) approach for VFC-assisted systems. Our FGDM provides control
    over both retrieval and access to outsourced data in fine-grained ways. Also,
    it offers highly efficient approaches for the accuracy verification of operations
    performed by VFNs. In designing the system, we consider a three-player game among
    system entities to capture their interactions. We formulate the management problems
    as a Nash Equilibrium (NE) problem and show the existence of an equilibrium. Our
    security analysis and empirical results demonstrate that the FGDM is secure in
    the standard model and acceptably efficient. Our scheme’s performance is rigorously
    assessed, offers improved speed, and is cost-effective compared to existing methods.
    It reduces computational and communication costs, achieving a reduction of nearly
    25% compared to comparable designs.
  doi: 10.1007/s12083-023-01601-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Peer-to-Peer Networking and Applications
    Article Verifiable and privacy-preserving fine-grained data management in vehicular
    fog computing: A game theory-based approach Published: 20 December 2023 Volume
    17, pages 410–431, (2024) Cite this article Download PDF Access provided by University
    of Nebraska-Lincoln Peer-to-Peer Networking and Applications Aims and scope Submit
    manuscript Zahra Seyedi, Farhad Rahmati, Mohammad Ali & Ximeng Liu  121 Accesses
    Explore all metrics Abstract Vehicular fog computing (VFC) is a powerful technology
    that extends fog computing to regular vehicular networks, providing end-users
    and vehicles with ultra-low-latency services. It enables connected Vehicular Fog
    Nodes (VFNs) to process real-time data and promptly respond to user queries. However,
    touching unencrypted data by VFNs raises security challenges definitely, the top
    of which is confidentiality, and giving encrypted data to VFNs causes other problems
    such as encrypted data processing. Apart from these, how to inspect and encourage
    VFNs to provide a secure, honest, and user-satisfactory network is of vital importance
    to this area. To address these challenges, we design a novel fine-grained data
    management (FGDM) approach for VFC-assisted systems. Our FGDM provides control
    over both retrieval and access to outsourced data in fine-grained ways. Also,
    it offers highly efficient approaches for the accuracy verification of operations
    performed by VFNs. In designing the system, we consider a three-player game among
    system entities to capture their interactions. We formulate the management problems
    as a Nash Equilibrium (NE) problem and show the existence of an equilibrium. Our
    security analysis and empirical results demonstrate that the FGDM is secure in
    the standard model and acceptably efficient. Our scheme’s performance is rigorously
    assessed, offers improved speed, and is cost-effective compared to existing methods.
    It reduces computational and communication costs, achieving a reduction of nearly
    25% compared to comparable designs. Similar content being viewed by others Vehicular
    Edge Computing and Networking: A Survey Article 25 July 2020 A dynamic and optimized
    routing approach for VANET communication in smart cities to secure intelligent
    transportation system via a chaotic multi-verse optimization algorithm Article
    15 March 2024 Sybil attack detection in ultra-dense VANETs using verifiable delay
    functions Article 11 March 2024 1 Introduction With the increasing growth of the
    Internet of Things (IoT) and smart cities, a new paradigm for vehicular networks
    called the Internet of Vehicles (IoV), has emerged. IoV provides vehicle-to-vehicle
    (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-pedestrian (V2P) communications.
    This technology enables drivers and passengers to access real-time information
    through their vehicles [1, 2]. The global number of connected vehicles is expected
    to double in the coming years, with 192 million currently connected vehicles,
    and a predicted reach of 367 million by 2027 (a 91% increase). 1 IoV opens up
    many opportunities and offers various services such as weather programs, transportation
    and travel information, and other programs that were previously possible through
    smartphones. Using IoV-based systems can help to improving traffic management
    and preventing road hazards (currently, according to the World Health Organization
    (WHO), 2 1.35 million people die in road accidents every year) [3, 4]. The implementation
    of IoV applications, however, is faced with several challenges involving data
    exchange, storage, and processing. Due to the significant latency limitations
    associated with these emerging applications, it is not feasible to rely on transmitting
    data from vehicles to a remote server for storage or computation. By moving servers
    closer to vehicles, fog computing has been proposed as a promising solution to
    overcome such latency limitations. However, since vehicles are on-the-move users,
    connecting to fixed fog servers may result in similar latency issues. Furthermore,
    traversing an area with a limited network infrastructure may disrupt fog computing
    support for vehicles. Consequently, some IoV applications may stop and decrease
    the performance of vehicle systems. As a solution, vehicular fog computing(VFC)
    that uses vehicles as mobile fog nodes to meet dynamic user needs can be an alternative
    to support fog computing for vehicular network-based applications [5, 6]. VFC
    enables resource-constrained vehicles to utilize the idle resources of parked
    or moving vehicles, known as vehicular fog nodes (VFNs), for data outsourcing
    and task offloading [6]. These VFNs act as intermediaries, offering computing
    and data caching services to nearby vehicles [5]. Generally, to outsource and
    retrieve data in a VFC system, according to Fig. 1, data owners (DOs) that intend
    to share data, such as traffic data, outsource them to VFNs. On the other hand,
    data users (DUs) send requests to nearby VFNs to retrieve the desired data [5,
    6]. However, despite the numerous advantages VFC brings to the IoV, the process
    of outsourcing unencrypted data to VFNs and the subsequent data retrieval present
    various challenges for the involved entities, some of which we will explain in
    the following: Challenge 1. Participation of VFNs: One of the fundamental requirements
    for establishing a VFC system is the active involvement of VFNs within the network.
    Each VFN is a combination of vehicles and humans functioning as a single unit.
    Encouraging their owners to participate in the system is crucial to use the vehicles’
    resources. Thus, how to motivate VFNs to participate in the system poses a significant
    problem. Additionally, ensuring reliable and honest services from VFNs requires
    careful inspection and appropriate incentives. Challenge 2. Secure data retrieval:
    To protect sensitive data stored in VFNs, Searchable encryption schemes are usually
    utilized to enable DUs to provide fine-grained access control over encrypted data
    and retrieve their data without revealing sensitive information or risking unauthorized
    access. However, VFNs may vary in some attributes such as accuracy of search results,
    computing power, and distance from the DU, so DUs have to set access policies
    to ensure only compliant VFNs perform search operations. To the best of our knowledge,
    currently, there is no specific searchable encryption technique that takes into
    account the attributes of VFNs, presenting a limitation in vehicular fog computing
    networks. Challenge 3. Selecting suitable VFNs: The selection of an appropriate
    VFN is crucial for adhering to a DU’s access control policy. However, not all
    VFNs in proximity may meet the policy requirements, leading to two approaches:
    either the DU must identify the suitable VFN or forward the request to all nearby
    VFNs. The former solution necessitates the DU to acquire and maintain knowledge
    of all VFNs’ attributes, potentially jeopardizing their privacy. The latter can
    be time-consuming and burdensome for the involved VFNs, as they may receive excessive
    requests that do not align with their attributes, requiring them to scrutinize
    each request. Challenge 4. Verifying Search Results: Once a VFN has performed
    a search operation on behalf of a DU, the DU needs to verify the accuracy of the
    results. While various schemes have been proposed to address verification challenges,
    encountering incorrect results introduces new problems. DUs may waste time receiving
    wrong results and become unsure about the accuracy of future results. In some
    cases, this may even lead DUs to leave the system altogether. Challenge 5. Attribute
    revocation: As search result verification influences the attributes of a VFN,
    the corresponding secret key for its previous attributes should no longer be valid.
    Therefore, it is crucial to update a VFN’s attributes whenever necessary. Fig.
    1 A VFC-based system Full size image To alleviate the described challenges, we
    design a fine-grained data management (FGDM) scheme and describe how to implement
    it in a VFC environment. However, we believe that using only cryptographic approaches
    is insufficient for creating motivations and system management. The unique features
    of VFC environments, including the independence of entities, dynamic and extensive
    limited knowledge of the environment, relationships among entities, and lack of
    control over entities’ behavior, necessitate new security methods. Game theoretic
    approaches provide a promising solution to consider the benefits and interests
    of entities involved in the system and ensure their loyalty to the system. Utilizing
    this approach, we design an incentive mechanism that captures strategic interactions
    among the entities in a three-player game. The players’ strategies are highly
    influenced by other players’ actions. The main contributions are summarized as
    follows: Strategic game: To tackle Challenges 1 and 4, we propose a game-based
    keyword search outsourcing process (KWSOP) approach involving three parties: the
    CA, a DU, and a VFN. We demonstrate that this game has a Nash Equilibrium (NE),
    which yields an optimal strategy for each player. In particular, we prove that
    the VFN role player should aim to perform the search operation accurately and
    has no incentive to deviate from this strategy. By analyzing the players’ utility
    functions, we establish that not only are VFNs likely to participate in the KWSOP,
    but they also have a strong motivation to deliver high-quality services. Verifiable
    attribute-based keyword search (VABKS): We have designed a new VABKS cryptosystem
    with two access control policies enforcement to address Challenges 2 and 4. Apart
    from enforcing the access control policy of the DOs to utilize outsourced encrypted
    data, this cryptosystem also permits DUs to regulate the outsourced search operations
    using their control policies. Moreover, this system enables them to verify if
    the VFN has conducted the search operation honesty. Attribute-based VFN selection
    mechanism: In order to address Challenge 3 and prevent the leakage of VFNs’ attributes,
    we develop an attribute-based VFN finding mechanism. Our scheme utilizes an algorithm
    that generates a list of VFNs according to the policies of the DU. This way, once
    they receive the list of VFNs, DUs will not know anything beyond that these VFNs
    are in accordance with their policies. Revocable key delegation mechanism: To
    address Challenge 5, we design a process for updating the attributes of VFNs.
    When the CA identifies that an attribute of a VFN has been changed, certain algorithms
    are initiated to update the VFN’s attributes and generate any necessary parameters.
    Efficient search result verification: Our FGDM offers an efficient method for
    verifying search results, thereby addressing Challenge 4. Drawing on the scheme
    proposed by Ali et al. in [7], our FGDM scheme enables a DU to validate search
    results received from a VFN without the need to download the entire data. Paper
    Organization: In Section 2 we review the related work. Section 3 provides some
    preliminary information. Then, we describe our problem formulation in Section
    4. Section 5 presents the FGDM scheme in the IoV environment and provides a detailed
    presentation of it. In Section 6, we define our KWSOP game and find its NE. In
    Section 5.2, we provide a detailed presentation of our proposed FGDM. Section
    7 describes security analysis and Section 8 provides scheme verification using
    Proverif. Section 9 presents performance analysis. Concluding remarks can be found
    in Section 10. Additionally, we include the correctness analysis in the Appendix
    to ensure easy comprehension and completeness of this paper. 2 Related work This
    section aims to review two primary areas of research. Firstly, we will discuss
    related work regarding VFC and assess the strengths of our approach in comparison
    to other schemes outlined in Table 1. Secondly, we will explore related work on
    attribute-based cryptographic systems and examine various existing schemes in
    this field, making comparisons with our FGDM scheme in Table 2. Table 1 Functionalities
    of VFC system comparison Full size table Table 2 Functionalities of ABKS schemes
    comparison Full size table 2.1 Vehicular fog computing VFC is a variant of fog
    computing that leverages vehicles as fog nodes to overcome latency limitations.
    Existing work in this field has explored various aspects of VFC, including its
    architecture, latency optimization, and utility maximization [22, 23]. Most studies
    of VFC have focused on its architecture [23], which can be divided into two general
    categories: infrastructure-based VFC and vehicle-based VFC. The former considers
    infrastructure near vehicles [24], and the latter considers vehicles with idle
    resources [9] as fog nodes. In contrast to an infrastructure-based VFC system,
    that necessitates extra equipment such as roadside units (RSUs), a vehicle-based
    system is simpler to implement [12]. For example, Zhu et al. [25] proposed an
    architecture utilizing commercial fleets moving along specific routes as fog nodes.
    However, while the architecture of VFC has received considerable attention [23],
    there is a scarcity of studies examining utility maximization for users and vehicles.
    Several papers, including [8, 10, 11, 22, 26], have investigated the entities’
    interests and the maximization of their utilities. To address time-sensitive and
    computationally intensive tasks, a resource allocation algorithm was introduced
    in [22]. In [10], the authors designed a task scheduling method using the ant
    colony optimization method. Shojafar et al. [11] presented an adaptive resource
    scheduling method to optimize system performance. The scenario described in [26]
    involved vehicles sharing their idle computing resources, with task scheduling
    managed by RSUs. Furthermore, edge and cloud server providers were jointly employed
    to offer computing services to nearby vehicles. In [8], users’ requests were initially
    served in RSUs, with requests exceeding RSU resource limits redirected to the
    cloud center. To overcome the computing power limitations of moving vehicles,
    [13] suggested utilizing the computing resources of parked vehicles in VFC. Resource
    pooling was introduced in [27], where vehicle computing resources were combined
    to provide joint computing services within a community. Additionally, [27] proposed
    a genetic algorithm-based strategy for utility maximization. Other similar studies
    on resource allocation are also presented in [14, 15]. However, all these work
    try to maximize the benefits of VFNs in some way only through resource allocation.
    Underestimating the participants’ benefits in this system, one of our main challenges
    in this paper, makes its development problematic in the real world. 2.2 Attribute-based
    cryptographic systems There are two main types of attribute-based approaches that
    provide fine-grained access and keyword search control over encrypted data: attribute-based
    encryption (ABE) and attribute-based keyword search (ABKS) methods. Attribute-based
    encryption A popular cryptographic tool for enforcing access control policies
    is ABE, which was first proposed by Sahai and Waters in [28]. In an ABE scheme,
    the users’ access rights be controlled by considering access policies in ciphertexts
    or users’ secret keys. Depending on how access policies are associated with ciphertexts
    and secret keys, these schemes can be divided into two categories: ciphertext-policy
    ABE (CP-ABE) and key-policy ABE (KP-ABE). In CP-ABE, the ciphertext is associated
    with the access control policy [29], and in KP-ABE, the decryption key is associated
    with the access control policy [30]. A wide range of ABE schemes has been proposed
    to enrich the functionality of this technique, for instance, anonymity [31], multi-authorization
    [32], user revocation [33], policy update [34], and decryption/encryption outsourcing
    [35]. One significant problem in ABE schemes is preserving users’ privacy. To
    alleviate this challenge, the authors in [31] propose a model that preserves users’
    anonymity. However, as the number of users in a system increases, scalability
    issues may arise in communication networks. To this end, in the proposed approach
    described in [32], the central authority can create new domain authorities as
    needed for additional computational resources. ‌Besides these functions, if a
    user loses an attribute or a data owner’s policies has changed, many security
    and privacy problems will arise. Therefore some ABE schemes with attribute revocation
    and policy update functions were proposed [33, 34]. Attribute-based keyword search
    In order to investigate the notion of ABE in keyword search schemes, Zheng et
    al. proposed the attribute-based keyword search scheme [19]. In this scheme, keywords
    are encrypted based on an access control policy, and authenticated DUs can search
    the outsourced encrypted data using generated tokens according to their attributes
    and the keywords they want to search for. This method has been enhanced with many
    features that provide different functionalities. In [36], an efficient user-revocable
    ABKS scheme is proposed. Miao et al. [37] designed an ABKS scheme in the shared
    multi-owner settings that supports hidden access policy. The ability to search
    for multiple keywords is a feature that is required for more practical encryption
    schemes, although none of the mentioned ABKS schemes mentioned provide this requirement.
    Several multi-keyword search approaches [12, 25, 26, 29] have been presented to
    address the mentioned shortcoming. However, these multi-keyword search schemes,
    only return the desired ciphertext if the requested keywords match all keywords
    associated with the data. To improve the data retrieval process, users need the
    ability to set a threshold for the quantity of matching keywords. The scheme presented
    in [21] enables data users to set a limit for the quantity of matching keywords
    in the generation of search token. Despite all these functionalities, these schemes
    are still imperfect, and they have ignored the untrustworthiness of the server.
    Servers are not trusted entities, and we cannot be sure they will perform the
    search operation accurately. Therefore, it is very important to provide a verification
    mechanism to check the validity of the search results. Various solutions have
    been proposed, such as those discussed in [36], however, some come with drawbacks
    such as Bloom filters causing a high false positive rate leading to communication
    overhead. Alternatively, Miao et al. [38] offer a more efficient approach that
    allows for relatively less complicated verification operations. However, to the
    best of our knowledge, it is impossible to verify the validity of search results
    without all retrieved data in aforementioned schemes. Therefore, verifiers typically
    need to download the retrieved data to perform the verification process, which
    leads to wasted time and unnecessary communication overhead due to downloading
    unusable or incorrect data. To address this issue, one of the main contributions
    of [7] is the ability to authenticate the results of the search operation without
    requiring full downloads of the retrieved files. Despite all these features, none
    of the existing research studies has seriously addressed the issue of system management.
    For example, in the topic of computation outsourcing, while our system is distributed,
    e.g. fog computing or edge computing, there are several options, fog servers,
    for outsourcing the computation. In this case, the existing schemes have not provided
    any solution or analysis to choose the best option for outsourcing, which is one
    of our main challenges in this paper. 3 Preliminaries 3.1 Bilinear map Suppose
    two cyclic groups \\(G_1\\) and \\(G_2\\) of a prime order p. A map \\(\\sigma
    : G_1 \\times G_1 \\rightarrow G_2\\) is be bilinear if it has these three features:
    Bilinearity: We have \\(\\sigma (\\alpha ^x, \\alpha ^y) = \\sigma (\\alpha ^x,
    \\alpha ^y) = \\sigma (\\alpha , \\alpha )^{xy}\\) for each \\(\\alpha \\in G_1\\)
    and \\(x, y \\in \\mathbb {Z}_p\\); Non-degeneracy: There exists an element \\(\\alpha
    \\in G_1\\) such that \\(\\sigma (\\alpha ^x, \\alpha ^y) \\ne 1\\); Computability:
    There exists an efficient algorithm that computes \\(\\sigma (\\alpha , \\beta
    )\\) for each \\(\\alpha , \\beta \\in G_1\\). 3.2 Access structure Let \\(\\mathcal
    {E} = \\left\\{ \\mathcal {E}_1,..., \\mathcal {E}_n \\right\\}\\) be a set of
    entities. An access structure on \\(\\mathcal {E}\\) is a non-empty subset \\(\\mathcal
    {A}\\) of \\(2^\\mathcal {E}\\). If for every \\(A \\in \\mathcal {A}\\) and \\(\\mathbb
    {E} \\subset \\mathcal {E}\\) such that \\(A \\subset \\mathbb {E}\\), we have
    \\(\\mathbb {E} \\in \\mathcal {A}\\), then we say that \\(\\mathcal {A}\\) is
    monotone. Every set \\(A \\in \\mathcal {A}\\) is known as an authorized set,
    while all the other sets in \\(2^P \\setminus \\mathbb {A}\\) are referred to
    as unauthorized sets. A set \\(\\mathbb {E} \\subset \\mathcal {E}\\) is said
    to satisfy an access structure \\(\\mathcal {A}\\) if \\(\\mathbb {E} \\in \\mathcal
    {A}\\). In ABE, attributes replace entities. We can represent any monotone access
    structure by an access tree, where each unique attribute is represented by a leaf
    node [19]. The Share and Combine algorithms are two practical algorithms that
    we utilized in our scheme to this end, and were introduced in [39]: \\({\\textbf
    {Share}}(p, r, \\mathcal {T} )\\): This algorithm is probabilistic polynomial-time
    (PPT) and requires three inputs: a prime number p, an access tree \\(\\mathcal
    {T}\\), a secret value \\(r \\in \\mathbb {Z}_p\\). It generates a distribution
    \\(\\left\\{ D_i \\right\\} _{i \\in L_{\\mathcal {T}}}\\) of r according to the
    access tree \\(\\mathcal {T}\\), where \\(L_{\\mathcal {T}}\\) represents the
    set of leaf nodes in \\(\\mathcal {T}\\). \\({\\textbf {Combine}}(\\left\\{ \\sigma
    (\\alpha _1, \\alpha _2)^{D_i} \\right\\} _{i \\in S}, \\mathcal {T})\\): Let
    \\(\\mathcal {T}\\) be an access tree, and Att is the set of attributes associated
    with \\(L_{\\mathcal {T}}\\). This algorithm, given a bilinear group parameters
    \\((p, G_1, G_2, \\sigma )\\), takes a set of values \\(\\left\\{ \\sigma (\\alpha
    _1, \\alpha _2)^{D_i} \\right\\} _{i \\in S}\\) and the access tree \\(\\mathcal
    {T}\\), where \\(\\alpha _1, \\alpha _2 \\in G_1\\), \\(S \\subseteq Att\\), and
    for a secret value  \\(r \\in \\mathbb {Z}_p\\), \\(\\left\\{ D_i \\right\\} _{i
    \\in L_{\\mathcal {T}}}\\) represents the output of  \\({\\textbf {Share}}(p,
    r, \\mathcal {T} )\\). If the set S satisfies \\(\\mathcal {T}\\), then the output
    of the algorithm is  \\(\\sigma (\\alpha _1, \\alpha _2)^r\\). If not, an error
    message \\(\\perp\\) is returned as output. For further information about the
    Share and Combine algorithms and the concept of access tree, please refer to the
    source [39]. 3.3 DBDH assumption Assume a PPT algorithm \\(\\mathcal {G}\\) which
    takes as input a security parameter n, and produces a tuple \\((p, G_1, G_2, \\sigma
    )\\) as output, where \\(p, G_1, G_2\\), and \\(\\sigma\\) are the same as Section
    3.1. The decisional bilinear Diffie-Hellman (DBDH) assumption is true for \\(\\mathcal
    {G}\\) if, for any tuple \\((n, p, G_1, G_2, \\sigma , \\alpha , \\alpha ^x, \\alpha
    ^y, \\alpha ^z, \\sigma (\\alpha , \\alpha )^w)\\), the advantage of any PPT distinguisher
    \\(\\mathcal {D}\\) in distinguishing the case where w equals xyz from the case
    where w is a uniform element of \\(\\mathbb {Z}_p\\) is a negligible function
    in n. Here n is a security parameter, \\((p, G_1, G_2, \\sigma )\\) is an output
    of \\(\\mathcal {G}(1^n)\\), \\(\\alpha \\in G_1\\) and \\(x,y,z \\in \\mathbb
    {Z}_p\\) are selected uniformly at random, and w is either a uniform element of
    \\(\\mathbb {Z}_p\\) or equals to xyz. In other words, for each PPT distinguisher
    \\(\\mathcal {D}\\), a negligible function negl exists such that: $$\\begin{aligned}&|Pr(\\mathcal
    {D}(n, p, G_1, G_2, \\sigma , \\alpha , \\alpha ^x, \\alpha ^y , \\alpha ^z ,
    \\sigma (\\alpha , \\alpha )^{xyz}) = 1)\\\\&-Pr(\\mathcal {D}(n, p, G_1, G_2,
    \\sigma , \\alpha , \\alpha ^x, \\alpha ^y , \\alpha ^z , \\sigma (\\alpha , \\alpha
    )^w) = 1)|\\\\&\\leqslant\\ negl(n), \\end{aligned}$$ where the probabilities
    are calculated based on the random selection of \\(x, y, z, w \\in \\mathbb {Z}_p\\),
    as well as the randomness utilized in \\(\\mathcal {D}\\) and \\(\\mathcal {G}\\).
    3.4 Commitment scheme A commitment scheme is composed of these three algorithms:
    \\({\\textbf {KeyGen}}(1^n)\\), \\({\\textbf {Commit}}(pk, m)\\), and \\({\\textbf
    {Open}}(c, d)\\). \\({\\textbf {KeyGen}}(1^n)\\) is a PPT algorithm that generates
    the necessary public parameters pk and defines the message space M. \\({\\textbf
    {Commit}}(pk, m)\\) is a PPT algorithm that uses the public parameters pk, a message
    m from the message space M to produce a value c that represents the commitment
    to m and d as the opening value. \\({\\textbf {Open}}(c, d)\\) is a deterministic
    polynomial-time algorithm that uses the public parameters pk, opening value d,
    and commitment value c to output a boolean value \\(b \\in \\{0, 1\\}\\) indicating
    whether c is a valid commitment to m. Security properties of a commitment scheme
    include (1) correctness, i.e. the generated commitment for every message is valid.
    (2) perfect or computational hiding which guarantees that an attacker cannot get
    information with any or negligible advantage about m from c. (3) perfect or computational
    binding where m is exclusively linked to c or discovering another message with
    the same commitment has a negligible probability of success. 3.5 Strategic game
    and nash equilibrium A strategic game is a tuple \\((P, \\lbrace S_i \\rbrace
    _{i \\in P}, \\lbrace f_i \\rbrace _{i \\in P})\\) in which: P is a finite set
    (the set of players); For each player \\(i \\in P\\), \\(S_i\\) is a nonempty
    set (the set of actions available to player i); For each player \\(i \\in P\\),
    \\(f_i\\) is a payoff function on \\(S = \\prod _{i \\in P} S_i\\). We use the
    notation \\(s_i \\in S_i\\) for a strategy of player i. A collection of players’
    strategies is given by \\(s = \\lbrace s_i \\rbrace _{i \\in P}\\) and is referred
    to as a strategy profile. A collection of strategies for all players but the i-th
    one is denoted by \\(s_{-i} = (s_1, s_2,..., s_{i-1}, s_{i+1},..., s_p) \\in S_{-i}\\).
    A (pure) Nash equilibrium is a strategy profile \\(S^{*}\\) from which no player
    can unilaterally deviate and improve its payoff. Formally, the strategy profile
    \\(S^{*} = (s^{*}_i,s^{*}_{-i})\\) is a Nash Equilibrium if \\(f_i(s^{*}_i,s^{*}_{-i})\\
    \\geqslant\\ f_i(s^{*}_i,s_{-i})\\) for every \\(s_i \\in S_i\\) and \\(i \\in
    P\\). 4 Problem formulation To aid our FGDM scheme comprehension and adherence,
    in this section, we present its system model, threat model, and security requirements,
    respectively. 4.1 System model This research primarily focuses on a VFC-based
    IoV scenario. The system comprises five distinct entities, as depicted in Fig.
    2: A Central Authority (CA), a Cloud Server (CS), a number of Data Owners (DOs),
    Data Users (DUs), and Vehicular Fog Nodes (VFNs). The entities have distinct roles
    and relationships, which are detailed as follows. Central Authority: The CA assumes
    the responsibility of initializing system parameters and generating its own master
    secret key. Additionally, it generates the necessary keys and updates the attributes
    associated with VFNs. Data Owners: Each DO defines its data access control policies
    and outsources the encrypted data under the defined access policy along with the
    assigned keywords to the VFNs present in their vicinity. Data Users: Each DU looks
    for data with specific keywords. It is responsible for generating searchable tokens,
    defining token access policies, encrypting tokens based on these policies, and
    verifying the search results. Vehicular Fog Nodes: VFNs are individuals equipped
    with devices possessing moderate computing power and are responsible for caching
    data or forwarding them to the CS as edge servers. VFNs retrieve the requested
    data from their database or the CS. Cloud Server: The CS is a remote server with
    extensive storage capacity. It stores and shares encrypted data with VFNs through
    a public channel. Fig. 2 System model Full size image 4.2 Threat model and security
    requirements The CA and DOs, in this work, are considered to be completely trustworthy.
    Once the CA initiates the system and generates the required parameters, it securely
    provides the corresponding keys to each entity. The CS and VFNs, either intentionally
    or accidentally, may fail to accurately perform the outsourced operations, so
    they are unreliable in this respect. Also, by collaborating with unauthorized
    DUs, they might attempt to gain access to unauthorized information regarding the
    outsourced data and related keywords. The DUs are malicious and can carry out
    any attacks. An authorized DU that can decrypt a ciphertext would not reveal any
    details about the content of the data and linked keywords in any way. Furthermore,
    unauthorized DUs may collaborate with each other and the CS to obtain unauthorized
    access to the outsourced data. In this ABE-based system, the information provided
    to VFNs, except for their secret keys, is either in the form of encrypted data
    (DOs’ data and keywords, and DUs’ tokens) or plain parameters, which are publicly
    available attributes like other ABE systems. Therefore, the data dealt with by
    VFNs is either encrypted or publicly available, and there is no need for special
    algorithms or operations for additional security processing by VFNs. The responsibility
    for securing confidential information in the system, such as DOs’ data and DUs’
    tokens, lies with the DOs and DUs. In the FGDM scheme, it is assumed that the
    communication channels used to transmit secret keys between the CA and other entities
    (DOs, DUs, and VFNs) are secure, ensuring that secret parameters are not eavesdropped
    or tampered with by other parties. The communication channels between other entities
    are considered tamper-resistant, although information transmitted through these
    channels may be eavesdropped upon by malicious parties, but not necessarily tampered
    with. We summarize the security requirements of our design in the following two
    cases: Indistinguishability: Only authorized DUs or VFNs whose attributes meet
    the required access policies are allowed access to the ciphertext or tokens. And
    the encrypted files must not reveal any information about their data or keywords.
    Unforgeability: In the verification phase, if a VFN fails to correctly execute
    the necessary operations required in the search algorithm, then the search results
    returned to DUs must not be validated. 5 Proposed FGDM in IoV environment This
    section utilizes our FGDM as the fundamental component in designing a secure VFC-based
    IoV system. The notations employed in this section are listed in Table 3. The
    proposed system consists of nine distinct phases, as depicted in Fig. 2. In Phases
    1 and 2 the CA initializes the system parameters and produces secret-keys for
    all entities, respectively. In Phase 3, a DO selects an access tree and a set
    of keywords, encrypts its data using the chosen access tree, and outsources the
    encrypted data and keywords to nearby VFNs. In phase 4, a DU defines a set of
    policies and receives a list of VFNs from the CA corresponding to each policy.
    In Phase 5, the DU creates a token matching a set of keywords, encrypts it under
    a selected policy, and sends a search request to VFNs associated with that policy.
    Upon receiving data queries in Phase 6, one of the VFNs accepts it and decrypts
    the requested token. It can decrypt the token only if its attributes satisfy the
    DU’s access tree. Then it searches the local storage, interacts with other VFNs,
    or requests the CS to find all ciphertexts that meet these criteria: (1) Their
    access trees align with the DU’s attributes; (2) They contain \\(\\mathcal {R}\\)
    or more matching keywords from the DU’s selected keywords set. Afterward, it sends
    the search result to the DU. Phase 7 allows the DU to verify the accuracy of search
    operations before downloading encrypted ciphertexts. If the search result is not
    verified, the DU notifies the CA about the specific VFN. Then, in phase 8, if
    its attribute set matches the access policy in the ciphertext, it retrieves the
    data associated with ciphertexts by performing very efficient operations. In Phase
    9, if a VFN’s attribute changes, its access right is updated. The CA modifies
    the associated parameters and generates an update-key accordingly. The VFN’s secret-keys
    are then updated based on the new parameters. Table 3 Notations Full size table
    5.1 Definition of proposed FGDM scheme In order to effectively implement the FGDM
    scheme in an IoV environment, we employ a set of 14 algorithms. The comprehensive
    design and construction of these algorithms are thoroughly explained in Section
    5.2. By referring to Section 5.2, readers can gain a detailed understanding of
    each algorithm’s functionality. 1. \\({\\textbf {Setup}}(1^{n}, \\mathbb {U},
    \\mathbb {U}'')\\): It takes the security parameter n and generates the global
    public parameters PK and the master secret-key MSK. 2. \\({\\textbf {DO.KeyGen}}(MSK,
    PK, ID_{do} )\\): On input MSK and a DO’s identifier \\(ID_{do}\\), it returns
    \\((PK_{do}, SK_{do})\\). 3. \\({\\textbf {VFN.KeyGen}}(MSK, PK, ID_{vfn}, a )\\):
    On input MSK, a VFN’s identifier \\(ID_{vfn}\\), and an attribute a, it outputs
    a secret-key \\(SK_{a, vfn}\\). 4. \\({\\textbf {DU.KeyGen}}(MSK, PK, ID_{du},
    Att_{du})\\): On input MSK, a DU’s identifier \\(ID_{du}\\) and attribute set
    \\(Att_{du}\\), the algorithm returns a secret-key \\(SK_{du}\\). 5. \\({\\textbf
    {DO.Enc}}(PK, PK_{do}, SK_{do}, ID_{do}, M, \\, \\lbrace \\omega _j \\rbrace _{j=1}^m,
    \\mathcal {T})\\): On input a DU’s public-key \\(PK_{do}\\), secret-key \\(SK_{do}\\),
    and identifier \\(ID_{do}\\), a message M, a keyword set \\(\\lbrace \\omega _j
    \\rbrace _{j=1}^m\\), and an access tree \\(\\mathcal {T}\\), it returns a searchable
    ciphertext \\(SCT_{\\mathcal {T}}\\). 6. \\({\\textbf {VFN.Slc}}( \\lbrace \\mathcal
    {T}_{p_i} \\rbrace _{p_i \\in P_{du}}, \\lbrace Att_{v_i} \\rbrace _{v_i \\in
    V_{du}} )\\): It takes a DU’s set of access trees, \\(\\lbrace \\mathcal {T}_{p_i}
    \\rbrace _{p_i \\in P_{du}}\\), and its set of near VFNs’ attributes, \\(\\lbrace
    Att_{v_i} \\rbrace _{v_{i} \\in V_{du}}\\), as inputs. It returns a set of VFNs
    corresponding to each policy, \\(\\lbrace L_i \\rbrace _{i \\in P_{du}}\\). 7.
    \\({\\textbf {TokenGen}}(PK, ID_{du}, SK_{du}, \\lbrace \\hat{\\omega }_j \\rbrace
    _{j=1}^l)\\): On input \\(ID_{du}\\), \\(SK_{du}\\), and a keyword set \\(\\lbrace
    \\hat{\\omega }_j \\rbrace _{j=1}^l\\), the algorithm outputs is a token \\(TK_{du}\\)
    associated with the keyword set. 8. \\({\\textbf {TokenEnc}}(PK, TK_{du}, \\mathcal
    {T}_{du} )\\): It takes keywords token \\(TK_{du}\\), and an access policy tree
    \\(\\mathcal {T}_{du}\\) as inputs. It returns a search token ciphertext \\(CT_{Token}\\)
    according to the DU’s access tree. 9. \\({\\textbf {TokenDec}}(PK, SK_{a, vfn},
    CT_{Token})\\): On input a VFN’s secret-key \\(SK_{a, vfn}\\) and the ciphertext
    of a token \\(CT_{Token}\\), this algorithm returns the token corresponding to
    this ciphertext. 10. \\({\\textbf {Search}}(PK, TK_{du}, ID_{du}, SCT_{\\mathcal
    {T}}, \\mathcal {R})\\): It takes a searchable keywords token \\(TK_{du}\\), a
    DU’s identifier \\(ID_{du}\\), a searchable ciphertext \\(SCT_{\\mathcal {T}}\\),
    and a threshold value \\(\\mathcal {R}\\) as inputs. The algorithm outputs a response
    Re. 11. \\({\\textbf {Verify}}(PK, Re, ID_{do}, ID_{vfn}, \\mathcal {K})\\): On
    input Re, \\(ID_{do}\\), and \\(ID_{vfn}\\), this algorithm verifies the search
    result Re’s validity. If it is valid, the algorithm returns 1. Otherwise, 0 is
    output which means the search result is not valid. 12. \\({\\textbf {DU.Dec}}(\\,
    PK, \\, \\mathcal {K}, \\mathcal{T}\\mathcal{K}, C_{do}, t_{do})\\): Given a ciphertext
    \\(C_{do}\\), parameters \\(\\mathcal {K}\\) and \\(\\mathcal{T}\\mathcal{K}\\)
    from search result Re, and the DU’s token \\(t_{do}\\), this algorithm first checks
    whether this ciphertext matches with the DU’s token or not. If not, it aborts
    and outputs \\(\\perp\\). Otherwise, it recovers the message corresponding to
    \\(C_{do}\\). 13. \\({\\textbf {VFN.AttUpdate}}(PK, a_0, MSK)\\): This algorithm
    takes an attribute \\(a_0\\), and the master secret-key MSK. It updates the system
    parameters associated with the attribute and produces an update-key \\(UK_{a_0}\\).
    14. \\({\\textbf {VFN.KeyUpdate}}(PK, a_0, \\, UK_{a_0}, \\, SK_{a_0,vfn}, ID_{vfn})\\):
    It takes an attribute \\(a_0\\), an update-key \\(UK_{a_0}\\), a VFN’s secret-key
    associated with \\(a_0\\), \\(SK_{a_0,vfn}\\), and its identifier, \\(ID_{vfn}\\).
    It outputs an updated secret-key \\(SK''_{a_0,vfn}\\). 5.2 FGDM construction Our
    primary objective in developing FGDM is to allow authorized VFNs to handle keyword
    search operations on behalf of DUs. We have also created an effective mechanism
    for verifying search results that can be implemented on devices with limited resources
    in an IoV system. We provide a detailed overview of the construction of our FGDM
    scheme’s algorithms in Figs. 3 and 4. Fig. 3 The concrete construction of our
    FGDM Full size image Fig. 4 The concrete construction of our FGDM Full size image
    5.3 System overview Within this section, we will provide a comprehensive explanation
    of how to implemente our FGDM scheme within the proposed IoV system. This implementation
    will be carried out in nine distinct phases, utilizing the algorithms outlined
    in Section 5.1. The following paragraphs will outline each phase in detail, highlighting
    the key steps involved in integrating the FGDM scheme into the IoV system. System
    Initialization. To initiate the system, the CA selects a security parameter n
    and two attribute sets \\(\\mathbb {U}\\) and \\(\\mathbb {U}''\\). The CA then
    employs the \\({\\textbf {Setup}}(1^n, \\mathbb {U}, \\mathbb {U}'')\\) algorithm
    to generate the master secret key MSK and public parameters PK. The PK is shared
    with other entities while keeping MSK private. Key Generation. The CA carries
    out this phase to create the required keys for each entity. Upon receiving the
    MSK, it executes \\({\\textbf {DO.KeyGen}}(MSK, PK, ID_{do})\\) to derive secret
    and public keys for each DO based on its identity. Similarly, it uses \\({\\textbf
    {DU.KeyGen}}(MSK, PK, ID_{du}, Att_{du})\\) to compute secret keys for each DU
    with an identifier \\(ID_{du}\\) and attribute set \\(Att_{du}\\). Likewise, it
    employs \\({\\textbf {VFN.KeyGen}}(MSK, PK, ID_{vfn}, a)\\) to generate a secret
    key for each VFN based on its identity and attributes. Encryption. Before outsourcing
    a message M, a DO evaluates a set of keywords \\(\\lbrace \\omega _j \\rbrace
    _{j=1}^m\\). Using its public key \\(PK_{do}\\), secret key \\(SK_{do}\\), and
    identifier \\(ID_{do}\\), it runs \\({\\textbf {DO.Enc}}( PK, PK_{do}, SK_{do},
    ID_{do}, M, \\lbrace \\omega _j \\rbrace _{j=1}^m, \\mathcal {T} )\\) to produce
    a searchable ciphertext \\(SCT_{do}\\). Then, it sets up an access tree \\(\\mathcal
    {T}\\) specifying which DUs have permission to access the message. Finally, it
    sends the encrypted data to neighboring VFNs. VFN Selection. In this phase, a
    DU submits a set of policies to the CA. Utilizing \\({\\textbf {VFN.Slc}}(\\lbrace
    P_i \\rbrace _{i=1}^{m}, \\lbrace vfn_j \\rbrace _{j \\in VFNs} )\\), the CA generates
    a list of VFNs accessible to the DU whose attributes meet the policy requirements.
    These lists are subsequently shared with the DU. Token Generation. Using their
    secret key \\(SK_{du}\\) and identifier \\(ID_{du}\\), a DU executes \\({\\textbf
    {TokenGen}}(PK, ID_{du}, SK_{du}, \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^l)\\)
    to generate a token corresponding to its selected keywords. To ensure that only
    authorized VFNs can perform search operations, the DU encrypts the resulting token
    according to its access control policies \\(\\mathcal {T}_{du}\\) using the \\({\\textbf
    {TokenEnc}}(PK, TK_{du}, \\mathcal {T}_{du})\\) algorithm. Search. For a VFN to
    perform a search operation, it needs access to the searchable tokens. It decrypts
    the encrypted tokens \\(CT_{Token}\\) using its secret key \\(SK_{a, vfn}\\) through
    the \\({\\textbf {TokenDec}}(PK, SK_{a, vfn}, CT_{Token})\\) algorithm. Then,
    the VFN employs \\({\\textbf {Search}}( \\, \\, \\, PK, \\, \\, \\, TK_{du}, ID_{du},
    SCT_{\\mathcal {T}}, \\mathcal {R})\\) with the searchable token \\(TK_{du}\\)
    to find the DU’s desired files and sends the response Re back to the DU. Verification.
    To validate the search result, the DU performs the verification phase. Before
    downloading the response Re from the VFN, the DU uses \\({\\textbf {Verify}}(PK,
    Re, ID_{do}, ID_{vfn}, \\mathcal {K})\\) to confirm the accuracy of the search
    operation. If the verification fails, the DU reports the VFN’s identity \\(ID_{vfn}\\)
    to the CA. Otherwise, it proceeds to download Re. Decryption. The DU runs \\({\\textbf
    {DU.Dec}}( \\, PK, \\, \\, \\mathcal {K}, \\mathcal{T}\\mathcal{K}, C_{do}, t_{do})\\)
    to retrieve the original file. Updating VFNs. Based on search result reports or
    other conditions, the VFNs’ attributes may change. Upon receiving the reports,
    the CA updates the VFNs’ attributes by running \\({\\textbf {VFN.AttUpdate}} \\,
    ( \\, \\, \\, PK, \\, \\, \\, MSK, a_0)\\). Then, it runs \\({\\textbf {VFN.KeyUpdate}}(PK,
    a_0, UK_{a_0}, SK_{a_0, vfn}, ID_{vfn})\\) to update their secret-keys. In the
    next section, we will adopt a game-theoretic approach to address the issue of
    how to incentivize VFNs to participate in the system and ensure that they do not
    deviate from their tasks. 6 KWSOP game To manage the data retrieval process, especially
    the keyword search outsourcing process (KWSOP), in the previous section, we presented
    our FGDM scheme and explained how to implement it in a VFC environment. However,
    due to the reasons we stated in the introduction, this scheme is not adequate
    to overcome the mentioned challenges. Therefore, we will investigate KWSOP as
    a strategic game in the following section. 6.1 Game overview Three entities with
    different goals participate in the KWSOP: a DU, a VFN, and the CA. Each of these
    three players has its goal of being in the VFC system. The DU’s primary goal is
    to outsource the keyword search operation to a VFN according to its policies and
    get the desired result. The VFN intends to earn the highest wage in exchange for
    executing the operation. Meanwhile, the CA strives to maintain its clientele and
    not lose VFNs’ participation in the network while paying the lowest wage possible.
    These players employ various strategies to achieve their goals. The DU can decide
    which VFNs to delegate the search operation to, according to its policies. Once
    the request is accepted by a VFN, that VFN can decide whether to cancel the request
    or not. And if it does not cancel the request, whether to carry out the task accurately
    or not. Besides, the CA will have different strategies based on the reports received
    from the DU. If the DU reports any violation linked to the VFN, the CA should
    decide whether to update the attributes of the VFN or not, or whether to pay the
    VFN in full or not. Therefore, we formulate KWSOP as a game: Definition 1 (KWSOP
    Game) The sequential KWSOP game is the following triplet G, in which the DU is
    the first, the VFN is the second, and the CA is the last player (Fig. 5): $$\\begin{aligned}
    G = ( P , \\; \\lbrace S_i \\rbrace _{i \\in \\lbrace du, \\, vfn, \\, CA \\rbrace
    }, \\; \\lbrace f_{i} \\rbrace _{i \\in \\lbrace du, \\, vfn, \\, CA \\rbrace
    } ), \\end{aligned}$$ where \\(P = \\lbrace vfn \\in \\lbrace VFNs \\rbrace ,
    du \\in \\lbrace DUs \\rbrace , \\, CA\\rbrace ,\\) \\(S_{vfn} = \\lbrace A: \\text
    { Accurate execution}, \\, nA: \\text { not Accurate execution}, \\, C: \\text
    { Cancel } \\rbrace\\), \\(S_{du} = \\lbrace P_i | \\forall i = 1,..., m, P_i
    \\text { is an access } \\text { policy } \\rbrace\\), \\(S_{CA} = \\lbrace WP:
    \\text {Wage Payment}, AU: \\text {Attributes} \\text {Update}, WP + AU: \\text
    {Wage Payment + Attributes } \\text {Update } \\rbrace\\). Fig. 5 Game tree Full
    size image For each VFN and each operation O, if the VFN accurately operates O,
    we denote its reward as \\(R_{O}\\) and its penalty in case of making a mistake
    as \\(P_{O}\\). Its penalty in case of canceling the request is also shown with
    \\(P_{C}\\). Then, the utility function of the VFN will be as follows: \\(f_{vfn}
    = {\\left\\{ \\begin{array}{ll} R_{O} &{} A\\\\ -P_{O} &{} nA\\\\ -P_{C} &{} C.
    \\end{array}\\right. }\\) For each user DU and each operation O, the payoff of
    the DU will be 0 or \\(U_{O}\\) if it requests from the VFN, depending on the
    result of VFN’s operation, and will be \\(U''_{O}\\) if it does not request. Then,
    the utility function of DU will be as follows: \\(f_{du}(P_i) = c d_i(U_{O})\\)
    where, \\(c \\in \\lbrace 0,1 \\rbrace\\). If the CA chooses the WP strategy,
    it will lose the loyalty of DUs since the presence of VFNs with incorrect attributes
    in the system will cause DUs dissatisfaction. Therefore, the only benefit of this
    strategy is to keep the VFN in the system, which we represent with \\(U_{vfn}\\).
    In the same way, by choosing the AU strategy, the only benefit will be to keep
    DUs, \\(U_{du}\\). Then, the utility function of the CA will be as follows: \\(f_{CA}
    = {\\left\\{ \\begin{array}{ll} U_{vfn} &{} WP\\\\ U_{du} &{} AU\\\\ U_{vfn} +
    U_{du} &{} WP + AU. \\end{array}\\right. }\\) We now introduce the concept of
    Nash equilibrium, Definition 2 A strategy profile \\(s^* = (s_{du}^{*}, s_{vfn}^{*},
    s_{CA}^{*})\\) is a Nash equilibrium of the KWSOP game if at the equilibrium \\(s^*\\),
    no player can further upgrade its utility by unilaterally changing its strategy,
    i.e., $$\\begin{aligned} f_n (s_{n}^{*}, s_{-n}^{*}) \\le f_n (s_{n}, s_{-n}^{*}),
    \\, \\; \\forall s_{n} \\in S_{n}, \\; n \\in P. \\end{aligned}$$ The Nash equilibrium
    has a self-stability property such that the users at the equilibrium point can
    achieve a mutually satisfactory solution, and no user has the incentive to deviate.
    Since the VFNs are owned by different individuals, and they may act in their own
    interests, this property is very important to KWSOP management. The existence
    of Nash equilibrium shows us there is a strategy profile that all three players
    tend to occur. Furthermore, according to the NE, we find out whether VFNs generally
    have the desire to participate in the system or not. Fig. 6 Game table Full size
    image 6.2 Game property We then study the existence of the Nash equilibrium of
    the KWSOP game. To proceed, we first introduce an important concept of the best
    response: Definition 3 Given the strategies \\(s_{-n}\\) of the other players,
    player n’s strategy \\(s^{*}_n \\in S_{n}\\) is a best response if $$\\begin{aligned}
    f_n (s_{n}^{*}, s_{-n}) \\le f_n (s_{n}, s_{-n}), \\, \\; \\forall s_{n} \\in
    S_{n}. \\end{aligned}$$ According to Definitions 2 and 3, we see that at the Nash
    equilibrium all the users play the best response strategies towards each other.
    Based on the concept of best response and the table of players’ utilities, Fig.
    6, we have the following observation for the KWSOP game. For the DU’s best response,
    without loss of generality, we can order its set of access policies so that \\(U(P_1)
    \\ge U(P_2) \\ge ... \\ge U(P_m)\\). Then, the DU’s best response will be \\(P_1\\),
    the VFN’s best response will be A, and the CA’s best response will be \\(WP +
    AU\\). Therefore, according to Fig. 7, the Nash equilibrium of the game is equal
    to: \\((P_1, A, WP+AU)\\). Fig. 7 NE diagram Full size image 7 Security analysis
    In this section, we provide evidence that our FGDM scheme satisfies the security
    requirements mentioned in Section 4.2. We first define indistinguishability security
    in detail, followed by a demonstration of how our scheme meets the standard model’s
    security definition. Finally, we establish that our FGDM achieves unforgeability
    as well. 7.1 Indistinguishability Let’s consider the experiment for adversarial
    indistinguishability with \\(\\mathcal {A}\\) representing a PPT adversary and
    \\(\\mathcal {C}\\) representing a PPT challenger: Scenario: The adversary \\(\\mathcal
    {A}\\) aims to be challenged on a specific access tree \\(\\mathcal {T}^{*}\\)
    and provides it to \\(\\mathcal {C}\\). Setup: Initially, the challenger \\(\\mathcal
    {C}\\) chooses two sets of attributes, \\(\\mathbb {U}\\) and \\(\\mathbb {U}''\\),
    along with a security parameter n. Then, it performss \\((MSK, PK) \\leftarrow
    {\\textbf {Setup}}(1^{n},\\mathbb {U}, \\mathbb {U}'')\\) and shares PK with \\(\\mathcal
    {A}\\) while keeping MSK confidential. Additionally, it considers identifiers
    \\(ID_1 ,..., \\, ID_{q(n)}\\) for DUs in a hypothetical system and shares these
    parameters with A. Phase 1: \\(\\mathcal {A}\\) is allowed to query following
    oracles polynomially many times. \\(\\mathcal {C}\\) maintains separate empty
    lists \\(L_u\\) for each \\(u \\in \\lbrace 1,..., q(n)\\rbrace\\) and an empty
    list \\(L_{KW}\\). \\(\\mathcal {O}_{{\\textbf {DU.KeyGen}}}(ID_u, Att)\\): If
    \\(\\mathcal {T}^{*}\\) is satisfied by \\(L_u \\cup Att\\), \\(\\mathcal {C}\\)
    aborts. Otherwise, it performs  \\(SK_{u} \\leftarrow {\\textbf {DU.KeyGen}}(
    MSK, PK, ID_{u}, Att)\\) and replaces \\(L_u \\cup Att\\) with \\(L_u\\). \\(\\mathcal
    {O}_{{\\textbf {TokenGen}}}(ID_{u}, \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^{l},
    Att)\\): The challenger \\(\\mathcal {C}\\) executes \\({\\textbf {DU.KeyGen}}(MSK,
    PK, ID_{u}, Att)\\) to create a secret-key \\(SK_u\\). Then, it performs \\((TK_u,
    \\mathcal {K}) \\leftarrow {\\textbf {TokenGen}}( PK, ID_{u}, SK_u, \\lbrace \\hat{\\omega
    }_j \\rbrace _{j=1}^{l} )\\), returns \\(TK_u\\) to \\(\\mathcal {A}\\), and replaces
    \\(L_{KW}\\) with \\(L_{KW} \\cup \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^{l}\\).
    Challenge: After Phase 1, \\(\\mathcal {A}\\) picks two pairs \\((M_0, \\lbrace
    \\omega _{j}^{(0)} \\rbrace _{j=1}^{m})\\) and \\((M_1, \\lbrace \\omega _{j}^{(1)}
    \\rbrace _{j=1}^{m})\\), where m is an arbitrary natural number, and \\(|M_0|
    = |M_1|\\). \\(\\mathcal {C}\\) verifies if \\(( \\lbrace \\omega _{j}^{(0)} \\rbrace
    _{j=1}^{m} \\cup \\lbrace \\omega _{j}^{(1)} \\rbrace _{j=1}^{m}) \\cap L_{KW}
    \\ne \\emptyset\\). If so, it aborts. If not, after randomly selecting a coin
    \\(b \\in \\lbrace 0,1 \\rbrace\\), it performs \\(SCT_{\\mathcal {T}^{*}} \\leftarrow
    {\\textbf {DO.KeyGen}}( PK, \\, \\, \\, PK_{do}, \\, \\, \\, SK_{do}, \\, \\,
    \\, ID_{do}, \\, \\, M_{b}, \\lbrace \\omega _{j}^{(b)} \\rbrace _{j=1}^m, \\mathcal
    {T}^{*})\\) and shares \\(SCT_{\\mathcal {T}^{*}}\\) and \\(PK_{do}\\) with \\(\\mathcal
    {A}\\). Phase 2: This phase is comparable to Phase 1, with the exception that
    adversary \\(\\mathcal {A}\\) is not allowed to make any queries on \\(\\mathcal
    {O}_{{\\textbf {TokenGen}}}(ID_{du}, \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^{l},
    Att)\\) if \\(\\lbrace \\hat{\\omega _j} \\rbrace _{j=1}^{l} \\cap (\\lbrace \\omega
    _{j}^{(0)} \\rbrace _{j=1}^{m} \\cup \\lbrace \\omega _{j}^{(1)} \\rbrace _{j=1}^{m})
    \\ne \\emptyset\\). Guess: \\(\\mathcal {A}\\) returns a guess \\(b'' \\in \\lbrace
    0, 1 \\rbrace\\) of b. The winner of the indistinguishability game is \\(\\mathcal
    {A}\\) if and only if \\(b'' =b\\). Let us consider a function \\(Adv_{\\mathcal
    {A}, \\Pi }(n) = | Pr(b = b'') - 1/2 |\\) named advantage function, where \\(\\Pi\\)
    is our FGDM scheme, n is a security parameter, and \\(\\mathcal {A}\\) is a PPT
    adversary in the described indistinguishability experiment. Definition 4 Our FGDM
    scheme achieves indistinguishable security when the advantage function \\(Adv_{\\mathcal
    {A}, \\Pi }(n)\\) is a negligible function in n for any PPT adversary \\(\\mathcal
    {A}\\). Theorem 1 Assuming that the DBDH assumption holds for the \\(\\mathcal
    {G}\\), our FGDM scheme achieves indistinguishable security in the standard model.
    Proof Assume that \\(\\mathcal {A}\\) is a PPT adversary in the indistinguishability
    experiment. Let \\(\\mathcal {D}\\) be a PPT distinguisher that receives a tuple
    \\((n, p,G_1, G_2, \\sigma , \\alpha , \\alpha ^{x}, \\alpha ^{y}, \\alpha ^{z},
    \\sigma (\\alpha , \\alpha )^{w})\\) and wants to determine the value of w, where
    n is a security parameter, \\((p, G_1, G_2, \\sigma )\\) is an output of \\(\\mathcal
    {G}(1^{\\lambda })\\), \\(\\alpha \\in G_1\\) is a random generator, \\(x, y,
    z \\in \\mathbb {Z}_{p}\\) are selected uniformly at random, and w is either equal
    to xyz or is selected uniformly at random from \\(\\mathbb {Z}_{p}\\). We create
    \\(\\mathcal {D}\\) such that it emulates the challenger \\(\\mathcal {C}\\) in
    the described adversarial indistinguishability experiment. Later on, \\(\\mathcal
    {D}\\) will verify if \\(\\mathcal {A}\\) has achieved success or not. If successful,
    \\(\\mathcal {D}\\) will assume that \\(w = xyz\\). If unsuccessful, it concludes
    that w is chosen uniformly at random from \\(\\mathbb {Z}_{p}\\). To elaborate,
    \\(\\mathcal {D}\\) executes \\(\\mathcal {A}\\) as a subroutine in the following
    manner: Target: The adversary \\(\\mathcal {A}\\) determines an access tree \\(\\mathcal
    {T}^{*}\\). Setup: First, the distinguisher \\(\\mathcal {D}\\) chooses universal
    attribute sets \\(\\mathbb {U}\\) and \\(\\mathbb {U}''\\), and for a natural
    polynomial q, it picks q(n) identifiers \\(ID_1 ,..., \\, ID_{q(n)}\\). Then,
    it selects two random values \\(s, t \\in \\mathbb {Z}_p\\) and \\(\\beta _{2}
    \\in G_1\\) and sets \\(\\alpha _0 = \\alpha\\), \\(\\alpha _1 = \\alpha ^{x}\\),
    \\(\\alpha _2 = \\alpha ^{t}\\), \\(\\beta _0 = \\alpha ^{y}\\), \\(\\beta _{1}
    = \\alpha ^{x} \\alpha ^{-s}\\), \\(\\Sigma _1 = \\sigma (\\beta _0, \\alpha _1
    )\\), and \\(\\Sigma _2 = \\sigma (\\beta _{0}^{-1}, \\beta _{1})\\). Now, it
    randomly picks \\(s_a \\in \\mathbb {Z}_p\\) and \\(s''_b \\in \\mathbb {Z}_p\\),
    for each \\(a \\in \\mathbb {U}\\) and \\(b \\in \\mathbb {U}''\\). Also for each
    \\(a \\in \\mathbb {U}\\) and \\(b \\in \\mathbb {U}''\\), it sets \\(pk_a = \\alpha
    ^{s_a}\\) and \\(pk''_b = \\alpha ^{s''_b}\\). Moreover, it selects an ID-based
    signature scheme \\(\\Pi _{sign} \\, \\, \\, \\, = \\, \\, \\, \\, ({\\textbf
    {Setup}}_{sign}, \\, \\, \\, \\, {\\textbf {KeyGen}}_{sign}, \\, \\, \\, \\, {\\textbf
    {Sign}}, {\\textbf {Vrfy}}_{sign})\\), a commitment scheme \\(\\Pi _{com} = ({\\textbf
    {Commit}}, {{\\textbf {Open}}})\\), a symmetric encryption scheme \\(\\Pi _{enc}
    = ({\\textbf {Enc}}, {\\textbf {Dec}})\\), a message authentication code (MAC)
    \\(\\Pi _{mac} = ({\\textbf {Mac}}, {\\textbf {Vrfy}}_{mac})\\), and a hash function
    \\(H: G_2 \\rightarrow \\lbrace 0,1 \\rbrace ^{m}\\), where m is a natural number.
    By running \\({\\textbf {Setup}}_{sign}(1^n)\\), it generates \\((PK_{sign}, \\,
    MSK_{sign})\\), where \\(PK_{sign}\\) and \\(MSK_{sign}\\) are public parameters
    and master secret-key associated with \\(\\Pi _{sign}\\). Then, \\(\\mathcal {D}\\)
    gives \\(\\lbrace ID_i \\rbrace _{i=1}^{q(n)}\\) and \\(PK \\, = \\, ( n, \\,
    p, \\, G_1, \\, G_2, \\sigma , \\, \\alpha _0, \\, \\alpha _1, \\alpha _2, \\,
    \\beta _0, \\, \\beta _1, \\, \\beta _2, \\, \\Sigma _1, \\Sigma _2, \\lbrace
    pk_a \\rbrace _{a \\in \\mathbb {U}}, \\, \\lbrace pk''_b \\rbrace _{b\\, \\,
    \\in \\, \\,\\mathbb {U}''}, H, PK_{sign}, \\Pi _{sign}, \\Pi _{com}, \\Pi _{enc},
    \\Pi _{mac} )\\) to \\(\\mathcal {A}\\). Note that, if we assume that \\(sk =
    y\\) and \\(\\beta _2 = \\Gamma \\beta _{1}^{y} = \\Gamma \\beta _{1}^{sk}\\),
    for an unknown value \\(\\Gamma \\in G_1\\), then it can be observed that PK has
    been appropriately chosen. Phase 1: \\(\\mathcal {A}\\) queries the following
    oracles, and for every \\(u \\in \\lbrace 1,..., q(n)\\rbrace\\), \\(\\mathcal
    {D}\\) creates a list \\(L_u\\) and responds the queries as follows: \\(\\mathcal
    {O}_{{\\textbf {DU.KeyGen}}}(ID_u, Att)\\): \\(\\mathcal {D}\\) first checks whether
    \\(L_u \\cup Att\\) satisfies the access tree \\(\\mathcal {T}^{*}\\) or not.
    If so, it aborts. If not, for every \\(a \\in Att\\), \\(\\mathcal {D}\\) computes
    \\(sk_{u, a} = \\alpha _{1}^{s} \\Gamma ID_{u} = \\beta _{3}\\Gamma ID_{u}\\)
    and gives \\(\\lbrace sk_{u, a} \\rbrace _{ a \\in Att }\\) to \\(\\mathcal {A}\\).
    Otherwise, to produce a secret-key \\(SK_{u}\\), it executes \\({\\textbf {DU.KeyGen}}(MSK,
    PK, ID_{u}, Att)\\). Also, \\(\\mathcal {D}\\) replaces \\(L_u \\cup Att\\) with
    \\(L_u\\). \\(\\mathcal {O}_{{\\textbf {TokenGen}}}(ID_{u}, \\lbrace \\hat{\\omega
    }_j \\rbrace _{j=1}^{l}, Att)\\): The distinguisher \\(\\mathcal {D}\\), at first,
    calls the oracle \\(\\mathcal {O}_{{\\textbf {DU.KeyGen}}}(ID_u, Att)\\) to produce
    a secret-key \\(SK_{u}\\). Afterward, it executes the \\({\\textbf {TokenGen}}(PK,
    ID_{u}, SK_u, \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^{l} )\\) algorithm and
    gives a token \\(TK_u\\) to \\(\\mathcal {A}\\). Also, it replaces \\(L_{KW}\\)
    with \\(L_{KW} \\cup \\lbrace \\hat{\\omega }_j \\rbrace _{j=1}^{l}\\). Challenge:
    For a natural number m, \\(\\mathcal {A}\\) returns two pairs \\((M_0, \\lbrace
    \\omega _{j}^{(0)} \\rbrace _{j=1}^{m})\\) and \\((M_1, \\lbrace \\omega _{j}^{(1)}
    \\rbrace _{j=1}^{m})\\) where m is an arbitrary natural number, and \\(|M_0| =
    |M_1|\\). \\(\\mathcal {D}\\) checks if \\(( \\lbrace \\omega _{j}^{(0)} \\rbrace
    _{j=1}^{m} \\cup \\lbrace \\omega _{j}^{(1)} \\rbrace _{j=1}^{m}) \\cap L_{KW}
    \\ne \\emptyset\\). If so, it aborts. If not, it selects a coin \\(b \\in \\lbrace
    0,1 \\rbrace\\) randomly, and encrypts \\((M_b, \\lbrace \\omega _{j}^{(b)} \\rbrace
    _{j=1}^{m})\\) as follows: At first, \\(\\mathcal {D}\\) chooses a random value
    \\(r'' \\in \\mathbb {Z}_p\\) and assumes that \\(r'' = z + sk_{do}\\) for an
    unknown value \\(sk_{do} \\in \\mathbb {Z}_p\\). Then, it calculates \\(pk_{do}^{(1)}
    = \\alpha _{2}^{-r''} \\alpha ^{tz} = \\alpha _{1}^{-z-sk_{do}}\\alpha _{2}^{z}
    = \\alpha _{2}^{-sk_{do}}\\), \\(pk_{do}^{(2)} = \\sigma (\\alpha , \\alpha )^{-w}
    \\sigma (\\alpha ^{y}, \\alpha ^{x-s})^{r''} \\sigma (\\alpha ^{y}, \\alpha ^{z})^{s}\\),
    \\(pk_{do}^{(3)} = \\alpha ^{-r''}\\alpha ^{z} = \\alpha ^{-sk_{do}-z} \\alpha
    ^{z} = \\alpha _{0}^{-sk_{do}}\\), and \\(pk_{do,a} = (pk_{a}\\alpha _{2}^{-1})^{-r''}
    (\\alpha ^{z})^{s_{a} - t} = (pk_{a}\\alpha _{2}^{-1})^{-sk_{do}}\\), for each
    \\(a \\in \\mathbb {U}\\). After, it computes \\(C_1 = \\alpha _{2}^{r''}pk_{do}^{(1)}
    = \\alpha _{2}^{r}\\) and \\(C_2 = \\Sigma _{2}^{r''}pk_{do}^{(2)} = \\Sigma _{2}^{r}\\).
    Then, by running \\({\\textbf {Share}}(p, r'', \\mathcal {T})\\), \\(\\mathcal
    {D}\\) provides a distribution \\(\\lbrace D_{i} \\rbrace _{i \\in L_{\\mathcal
    {T}}}\\) of \\(r''\\), and for each \\(i \\in L_{\\mathcal {T}}\\), it calculates
    \\(C_{v_i} = \\alpha _{0}^{d_i}pk_{do}^{(3)} = \\alpha _{0}^{D_{i}-sk_{do}}\\)
    and \\(C''_{v_i} = ( pk_{i}\\alpha _{2}^{-1})^{D_i} pk_{do,i} = ( pk_{i}\\alpha
    _{2}^{-1})^{D_{i}-sk_{do}}\\). Also, it runs \\({\\textbf {Sign}}(PK_{sign}, ID_{do},
    SK_{do}, (C''_{do} ||c_{do}))\\) and generates a signature \\(\\sigma _{do}\\).
    Afterward, \\(\\mathcal {D}\\) computes \\(k = H(\\sigma (\\alpha , \\alpha )^{-w})\\)
    and runs \\({\\textbf {Commit}}(k)\\), \\({\\textbf {Enc}}(k,d_{do})\\), \\({\\textbf
    {Enc}}(k,M)\\), and \\({\\textbf {Mac}}(k, C_{do})\\) to produce \\((c_{do}, d_{do})\\),
    \\(C''_{do}\\), \\(C_{do}\\), and \\(t_{do}\\). Then, for every \\(j = 1,...,
    m\\), it chooses a value \\(W_{j} \\in \\mathbb {Z}_{p}\\) uniformly at random,
    and considers that, for an unknown value \\(r_{j} \\in \\mathbb {Z}_p\\), \\(W_{j}
    = \\omega _j - z + r_j\\). It sets \\(W''_{j} = \\Sigma _{1}^{W_{j}} \\Sigma _{1}^{-\\omega
    _{j}} \\sigma (\\alpha , \\alpha )^{w}\\). Finally, the distinguisher \\(\\mathcal
    {D}\\) returns a searchable ciphertext \\(SCT^{*}_{\\mathcal {T}}\\) and \\(PK_{do}\\)
    to \\(\\mathcal {A}\\) where, \\(SCT_{\\mathcal {T}} = ( \\mathcal {T}, C_{do},
    \\, \\, C''_{do}, \\, \\, C_1, \\, \\, C_2, \\, \\, c_{do}, t_{do}, \\sigma _{do},
    \\lbrace W_{j} \\rbrace _{j =1}^{m}, \\lbrace W''_{j} \\rbrace _{j =1}^{m}, \\lbrace
    C_{v_i} \\rbrace _{i \\in L_{\\mathcal {T}}}, \\lbrace C''_{i} \\rbrace _{i \\in
    L_{\\mathcal {T}}})\\). Phase 2: \\(\\mathcal {A}\\) submits more queries to the
    oracles, and \\(\\mathcal {D}\\) answers them the same as in Phase 1. Guess: \\(\\mathcal
    {A}\\) returns a guess \\(b'' \\in \\lbrace 0, 1 \\rbrace\\) of b. When \\(\\mathcal
    {D}\\) receives \\(b''\\) from \\(\\mathcal {A}\\), it verifies whether \\(b =
    b''\\) or not. If so, \\(\\mathcal {D}\\) returns 1 meaning \\(w = xyz\\). If
    not, it outputs 0, which means w is a uniform element of \\(\\mathbb {Z}_{p}\\).
    We see that if \\(w = xyz\\), then \\(k = H(\\sigma (\\alpha , \\alpha )^{-xyz})
    = H(\\Sigma _{1}^{-z})\\), \\(W''_{j} \\, = \\, \\Sigma _{1}^{W_{j}} \\Sigma _{1}^{-\\omega
    _{j}} \\sigma (\\alpha , \\alpha )^{xyz} \\, = \\, \\Sigma _{1}^{\\omega _{j}
    -z + r_j} \\, \\Sigma _{1}^{-\\omega _{j}} \\, \\Sigma _{1}^{z}\\),   and \\(pk_{do}^{(2)}
    = \\, \\sigma (\\alpha , \\alpha )^{-xyz} \\sigma (\\alpha ^{y}, \\alpha ^{x-s})^{r''}
    \\sigma (\\alpha ^{y}, \\alpha ^{z})^{s} = \\sigma (\\alpha ^{y}, \\alpha ^{x-s})^{sk_{do}}
    = \\sigma (\\beta _{0}, \\alpha _{1}^{-1})^{-sk_{do}} = \\Sigma _{2}^{-sk_{do}}\\).
    Therefore, the given responses to the adversary \\(\\mathcal {D}\\) are valid
    when \\(w = xyz\\). Consequently, we have \\(Pr(\\mathcal {A}(n, p, G_1, G_2,
    \\sigma , \\alpha , \\alpha ^{x}, \\alpha ^{y}, \\alpha ^{z}, \\sigma (\\alpha
    , \\alpha )^{xyz}) = 1) \\ge Adv_{\\mathcal {A}, \\Pi }(n) + 1/2\\). On the other
    hand, when w is a random value, we see that \\(Pr(\\mathcal {A}(n, p, G_1, G_2,
    \\sigma , \\alpha , \\alpha ^{x}, \\alpha ^{y}, \\alpha ^{z}, \\sigma (\\alpha
    , \\alpha )^{w}) = 1) = 1/2\\). Therefore, \\(Pr(\\mathcal {A}(n, p, G_1, G_2,
    \\sigma , \\alpha , \\alpha ^{x}, \\alpha ^{y},\\, \\alpha ^{z},\\, \\sigma (\\alpha
    , \\alpha )^{xyz}) = 1) - Pr(\\mathcal {A}(n, p, G_1, G_2, \\sigma , \\alpha ,
    \\alpha ^{x}, \\alpha ^{y}, \\alpha ^{z},\\sigma (\\alpha , \\alpha )^{w}) = 1)
    \\ge Adv_{\\mathcal {A}, \\Pi }(n)\\). Moreover, according to the DBDH assumption,
    the left hand side of the above inequality, and thus \\(Adv_{\\mathcal {A}, \\Pi
    }(n)\\), is a negligible function in n. It proves the theorem. \\(\\square\\)  7.2
    Unforgeability In this section, we present the evidence that demonstrates the
    unforgeability of our search results verification approach. Specifically, we establish
    that for a given token \\(TK_{du}\\) and a threshold \\(\\mathcal {R}\\), a response
    Re issued by the CS successfully passes the verification test only if for a searchable
    ciphertext \\(SCT_{\\mathcal {T}} = ( \\mathcal {T}, C_{do}, C''_{do}, C_1, C_2,
    c_{do}, t_{do},\\, \\sigma _{do}, \\, \\lbrace W_{j} \\rbrace _{j =1}^{m},\\lbrace
    W''_{j} \\rbrace _{j =1}^{m}, \\lbrace C_{v_i} \\rbrace _{j \\, \\, \\in \\, L_{\\mathcal
    {T}}}, \\, \\, \\lbrace C''_{i} \\rbrace _{i \\, \\, \\in \\, \\, L_{\\mathcal
    {T}}}) \\, \\,\\) and \\(\\mathcal{T}\\mathcal{K} \\, \\, = \\, \\, {\\textbf
    {Search}}( \\, \\, PK, TK_{du}, ID_{du}, SCT_{\\mathcal {T}}, \\mathcal {R}) \\ne
    \\perp\\), Re is equal to \\((C''_{do}, c_{do}, \\sigma _{do}, \\mathcal{T}\\mathcal{K})\\).
    Theorem 2 Assuming the security of the digital signature and commitment schemes
    employed in our FGDM scheme, the verification process achieves unforgeability.
    Proof Let the VFN receives a search token \\(TK_{du}\\) and a threshold value
    \\(\\mathcal {R}\\) form a DU and outputs a tuple \\(Re = (C''_{do}, c_{do}, \\sigma
    _{do}, \\bar{\\mathcal{T}\\mathcal{K}})\\) such that \\(\\bar{\\mathcal{T}\\mathcal{K}}\\)
    has not been generated by running the algorithm \\({\\textbf {Search}}(PK, TK_{du},
    ID_{du}, SCT_{\\mathcal {T}}, \\mathcal {R})\\), for a searchable ciphertext \\(SCT_{\\mathcal
    {T}}\\). Let us assume, by contradiction, that we have \\({\\textbf {Verify}}(PK,
    Re, ID_{do}, ID_{vfn}, \\mathcal {K}) = 1\\), where \\(\\mathcal {K}\\) is the
    private-key associated with \\(TK_{du}\\). Since \\({\\textbf {Verify}}(PK, Re,
    ID_{do}, ID_{vfn}, \\mathcal {K}) = 1\\), the signature \\(\\sigma _{do}\\) is
    valid. Therefore, according to the assumption that says the utilized digital signature
    scheme is secure, one concludes that \\(C''_{do}\\), \\(c_{do}\\), and \\(\\sigma
    _{do}\\) are components of an outsourced searchable ciphertext \\(SCT_{\\mathcal
    {T}}\\). Suppose that \\(\\hat{k} = H(\\bar{\\mathcal{T}\\mathcal{K}}^{-1} \\Sigma
    _{1}^{\\mathcal {K}} )\\), and k is the privet-key associated with \\(SCT_{\\mathcal
    {T}}\\). Since the VFN does not perform the search algorithm correctly, we can
    assume that \\(k \\ne \\hat{k}\\), and thus \\(d_{do} = {\\textbf {Dec}}(k,C''_{do})
    \\ne {\\textbf {Dec}}(\\hat{k},C''_{do}) = \\hat{d}_{do}\\). On the other hand,
    as \\({\\textbf {Verify}} (PK, Re, ID_{do}, ID_{vfn}, \\mathcal {K}) = 1\\), we
    have \\({\\textbf {Open}}(c_{do}, \\hat{d}_{do}) = {\\textbf {Open}}(c_{do}, d_{do})
    = 1\\). Thus, if the DU cooperates with the VFN and has the \\(d_{do}\\), then
    it can compromise the binding security of the commitment scheme in the real-time
    [40]. This goes against the assumption that the commitment scheme is secure. \\(\\square\\)   8
    Formal verification using ProVerif This section presents the formal modeling and
    verification of the proposed FGDM scheme using ProVerif [48], a tool for analyzing
    security protocols. ProVerif utilizes the Dolev-Yao [49] adversarial model and
    allows for the verification of security properties. It supports equational theories
    defined by users and employs abstraction, unbounded sessions, and precise Horn
    clause abstraction. The formal language used for modeling security protocols is
    applied pi calculus. The syntax is paired with formal semantics to facilitate
    reasoning about protocols. ProVerif also includes support for various cryptographic
    primitives, which are modeled using equations and rewrite rules. The tool translates
    the scheme into an internal representation using Horn clauses, enabling analysis
    with an unbounded number of sessions. Given its capabilities in formally verifying
    security protocols, ProVerif is well-suited for the analysis conducted in this
    paper. The process of modeling a scheme in ProVerif consists of several components,
    namely declarations, process macros, and main processes accompanied by queries
    for verifying the security properties of the scheme. The ProVerif code can be
    enhanced by incorporating events that can happen multiple times. These events
    correspond to specific steps in the protocol and can be differentiated by parameters.
    For example, an event that occurs by a VFN entity of our scheme may include the
    VFN’s identity as a parameter. These events are represented by the red words in
    Fig. 8. The list of events taken into consideration includes: The “doSend\" event
    occurs just before a DO sends a message containing encrypted data on the public
    channel. The “vfnCach\" event occurs just before a VFN caches a data file in its
    database. The “vfnRead\" event occurs just after a VFN retrieves and successfully
    decrypts a message containing data. The “vfnSend\" event occurs just before a
    VFN sends a message containing data on the public channel. The “duRequest\" event
    occurs just before a DU sends a search request to a VFN. The “duRread\" event
    occurs just after a DU retrieves and successfully decrypts a message containing
    data. Fig. 8 FGDM flowchart Full size image 8.1 ProVerif results In this subsection,
    we initially present the considered queries and their connection to the security
    requirements followed by the assessment results. The verification process for
    security requirements relies on a set of queries outlined in Table 4. Queries
    \\(q_1\\) to \\(q_6\\) are utilized to verify the scheme’s functionality, ensuring
    that all events can occur as intended. Query \\(q_7\\) ensures that the attacker
    is unable to access the data within outsourced encrypted data, while query \\(q_8\\)
    ensures the confidentiality of encrypted tokens within a search request. Queries
    \\(q_9\\) to \\(q_{12}\\) focus on preserving the secrecy of the master secret
    key MSK and secret keys for DOs, DUs, and VFNs. Query \\(q_{13}\\) verifies that
    if an encrypted data file is outsourced by a DO, it is cached by an authorized
    VFN. Similarly, query \\(q_{14}\\) ensures that if a VFN caches a data file, it
    has been outsourced by an authorized DO. Query \\(q_{15}\\) ensures that if a
    VFN can access search tokens from a search request, it must originate from an
    authorized DU. Finally, queries \\(q_{16}\\) and \\(q_{17}\\) confirm that if
    a DU can access data from a search result, it must have been cached by an authorized
    VFN and sent by an authorized DO, respectively. Table 4 Verification results Full
    size table These queries collectively contribute to the verification of the security
    requirements, Indistinguishability and Unforgeability, of the FGDM scheme outlined
    in Section 4.2 through the following rationale: Queries \\(q_7\\) to \\(q_{12}\\)
    ensure the secrecy and confidentiality of the master secret key, secret keys,
    encrypted keywords, and encrypted data files. These queries guarantee that unauthorized
    individuals, including attackers, are unable to access sensitive information.
    Queries \\(q_{13}\\) and \\(q_{14}\\) establish the relationship between authorized
    DOs and VFNs concerning data caching. Queries \\(q_{15}\\), \\(q_{16}\\), and
    \\(q_{17}\\) validate the authenticity and authorization of data access by DUs
    and VFNs. They ensure that if a VFN (DU) can read search tokens (data file) from
    an encrypted file, it must originate from an authorized DU (DO), and the attributes
    of the VFN (DU) align with the access policies of the DU (DO). This validation
    guarantees that only authorized DUs can access and retrieve data. The combined
    outcomes of queries \\(q_7\\) to \\(q_{17}\\) demonstrate that the protocol implements
    robust access control, preserving the secrecy and confidentiality of sensitive
    information while allowing only authorized entities to access and interact with
    the data. This verification process enhances the overall Indistinguishability
    property of the protocol. Furthermore, the verification of Unforgeability is deduced
    from \\(q_{16}\\). It demonstrates that if a VFN fails to execute the necessary
    operations required in the search algorithm correctly, the search results returned
    to DUs will not be validated. 9 Performance analysis To evaluate the performance
    of our FGDM scheme in terms of execution time, communication overhead, and storage
    cost, we compare its efficiency with schemes [38, 41, 42]. We have found the results
    of these comparisons through real implementation of aforementioned schemes and
    calculating their asymptomatic complexity. As for the actual performance analysis,
    we have implemented the aforementioned schemes using the actual Traffic Violations
    dataset provided by Data.gov. This dataset, which is updated daily, contains about
    800MB of traffic violation information from all electronic traffic violations
    issued in the Montgomery County of Maryland. We performed the implementation using
    the python Pairing-Based Cryptography (pyPBC) [43] and hashlib [44] libraries
    on an Ubuntu 20.04 laptop equipped with an Intel Core i5 Processor 2.4 GHz and
    4 GB RAM In addition, we have used type A pairings for these implementations.
    Type A pairings are constructed on the curve \\(y^2 = x^3 + x\\) over the finite
    field \\(F_q\\) [45], for a prime number q such that \\(q = 3\\) mod 4. As we
    mentioned before, the \\(\\mathcal {G}\\) algorithm, described in Section 3.3,
    takes a security parameter n and outputs a tuple \\((p,G_1,G_2, \\sigma )\\).
    In Type A pairings, \\(G_1\\) and \\(G_2\\) are p-order subgroups of \\(F_q\\)
    and \\(F_{q^2}\\), respectively [45]. In this section, we consider that the length
    of p and q are 160-bit and 512-bit, respectively. Also, in this implementation,
    we use 256-bit SHA-3 algorithm as the hash function, the ID-based signature scheme
    presented in [46], AES (Advanced Encryption Standard) symmetric encryption, a
    commitment scheme \\(\\Pi _{com} = ({\\textbf {Commit}}, {\\textbf {Open}})\\),
    and a MAC scheme \\(\\Pi _{mac} = ({\\textbf {Mac}}, {\\textbf {Vrfy}}_{mac})\\),
    where \\(\\Pi _{com}\\) and \\(\\Pi _{mac}\\) are described below. Moreover, it
    is easy to check that \\(\\Pi _{com}\\) is a secure commitment scheme in the random
    oracle model, and Theorem 4.6 and Exercise 5.11 of [47], shows that \\(\\Pi _{mac}\\)
    is secure in the random oracle model. \\({\\textbf {Commit}}(M, H)\\): This algorithm
    takes the SHA-3 hash function H and a message M as inputs. Then, for a random
    element k of \\(\\mathbb {Z}_p\\), it computes a commitment value \\(c = H(k \\mid
    \\mid M)\\) and an opening value \\(d = (k,M)\\). \\({\\textbf {Open}}(M, H, c,
    d)\\): On input a message M, the SHA-3 hash function H, and commitment and opening
    values, c and \\(d = (k, M)\\), if \\(c = H(k \\mid \\mid M)\\), the algorithm
    outputs 1. Otherwise, it outputs 0. \\({\\textbf {Mac}}(M, H, k)\\): Given a message
    M, the SHA-3 hash function H, and a symmetric-key k, this algorithm returns a
    tag \\(t = H(k \\mid \\mid M)\\). \\({\\textbf {Vrfy}}_{mac}(M,t,k)\\): For a
    message M, a tag t, and a symmetric-key k, it outputs 1, if \\(t = H(k \\mid \\mid
    M)\\), and 0 otherwise. 9.1 Execution time In Table 5 we have shown our asymptotic
    analysis. This table presents different computational overhead incurred using
    the FGDM approach and the schemes proposed in [38, 41, 42]. In this table, we
    considered the time consumption of pairing operation, \\(T_p\\), exponential operation
    in \\(G_1\\), \\(T_{e_1}\\), exponential operation in \\(G_2\\), \\(T_{e_2}\\),
    random selection of an element from \\(\\mathbb {Z}_p\\), \\(T_{rand_{\\mathbb
    {Z}}}\\), a symmetric encryption and decryption, \\(T_enc\\) and \\(T_dec\\),
    signature verification, \\(T_{vrfy}\\), and commitment verification time, \\(T_{open}\\).
    Figure 9 presents our experimental results. In this implementation, we assumed
    that the number of data files to be encrypted ranged between 100 and 600, the
    number of search results to be decrypted ranged between 10 and 60 and the number
    of attributes ranged from 25 to 150. Parts (a), (b), and (c) of Fig. 9 compares
    the actual performance of key generation algorithms in our FGDM and the schemes
    presented in [38, 41, 42] (i.e. SV-KSDS, VFKSM, LFGS). As for the computational
    costs of key generation, the VFNs and DUs in FGDM system have much less computational
    burden than those in [38, 41, 42] schemes. This is because the theoretical costs
    of VFNKeyGen and DUKeyGen algorithms in aforementioned three schemes are \\((|Att|
    + 5) T_{e_1} + T_{e_2} + 2T_{rand_{\\mathbb {Z}}}\\), \\(4T_{e_1} + T_{e_2} +
    2T_{rand_{\\mathbb {Z}}}\\), \\((2|Att|+3) T_{e_1} + 2T_{e_2} + 2|Att|T_{rand_{\\mathbb
    {Z}}}\\), respectively. Part (d) of Fig. 9 compares the execution time of search
    token generation algorithm in mentioned schemes. As for the computational costs
    of token generation, DUs in our system have a much lower computational overhead
    than the [41] and [42] schemes. Indeed, from Table 5, the number of exponential
    operations in these schemes grows with 2|Att|, while in our FGDM it grows with
    |Att|, where |Att| is the number of DU’s attributes. However, as we see in this
    bar chart, in our FGDM, the token generation time overhead is relatively more
    than the [38] scheme. It appears that minimizing the token generation time overhead
    in FGDM is a feasible and compelling problem. Parts (e)–(j) of Fig. 9 illustrate
    the encryption time of our FGDM and other three schemes for various number of
    attributes. Again, we see that our FGDM is significantly more efficient than the
    [41] and [42] schemes, specifically when the number of attributes increases. Parts
    (h) and (i) compare our FGDM with other schemes in terms of search operation and
    decryption execution time, respectively. It can be observed that FGDM significantly
    decreases the amount of time required for search operations. Table 5 Comparison
    of computation overhead Full size table Fig. 9 Execution time Full size image
    9.2 Storage cost and communication overhead In comparison to the existing methods,
    our FGDM decreases communication overhead and storage expenses. This fact can
    be derived from the asymptotic analysis given in Table 6 and Fig. 10. Parts (a),
    (b), and (c) of Fig. 10 present our experimental results in measuring keys length.
    In Part (c), we see that VFN Key length in our FGDM is incredibly lower than the
    length of a VFN key in other three schemes, specifically [42]. From Table 6, we
    notice that size of VFN key in schemes increases with the number of attributes,
    that of FGDM is just influenced by a variable of length \\(l_{G_1}\\). Also, part
    (d) of the figure and Table 6 show the ciphertext length produced in the encryption
    process. We see that our FGDM effectively reduces the ciphertext length compare
    with [42] scheme. Table 6 Comparison of storage cost Full size table Fig. 10 Storage
    costs Full size image 10 Conclusion In this paper, we designed a novel fine-grained
    data management (FGDM) approach for VFC-assisted IoV systems. We have shown that
    our FGDM provides control over both retrieval and access to outsourced data in
    fine-grained ways. We also demonstrated that it offers highly efficient approaches
    for the accuracy verification of operations performed by VFNs. In designing this
    system, we considered a three-player game between system entities to capture their
    interactions. We formulate the management problems as a Nash equilibrium problem
    and show the existence of an equilibrium. Using the NE, and players’ utility functions,
    we proved that system entities, especially VFNs, not only have the tendency to
    take part in the system, but they also are loyal to the system and provide accurate
    services. We also demonstrated the deployment of our FGDM in a VFC-based IoV environment.
    Our FGDM was proven to achieve indistinguishability and unforgeability in the
    standard model. Moreover, an evaluation of its functionality and performance revealed
    that our FGDM is both highly effective and practical in actual applications. Furthermore,
    we demonstrated how incorporating a keyword threshold can enhance the adaptability
    of multi-keyword search protocols by enabling accurate query input from users.
    Data availability All data generated or analyzed during this study are included
    in this published article. Notes https://www.juniperresearch.com/researchstore/operators-providers/connected-vehicles-research-report
    https://www.who.int/publications/i/item/9789241565684. References Contreras-Castillo
    J, Zeadally S, Guerrero-Ibañez JA (2017) Internet of vehicles: architecture, protocols,
    and security. IEEE Internet Things J 5(5):3701–3709 Article   Google Scholar   Rathore
    MS, Poongodi M, Saurabh P, Lilhore UK, Bourouis S, Alhakami W, Osamor J, Hamdi
    M (2022) A novel trust-based security and privacy model for internet of vehicles
    using encryption and steganography. Comput Electr Eng 102:108205 Article   Google
    Scholar   Hbaieb A, Ayed S, Chaari L (2022) A survey of trust management in the
    internet of vehicles. Comput Netw 203:108558 Article   Google Scholar   Wang J,
    Zhu K, Hossain E (2021) Green internet of vehicles (IOV) in the 6G era: toward
    sustainable vehicular communications and networking. IEEE Trans Green Commun Netw
    6(1):391–423 Article   Google Scholar   Mao W, Akgul OU, Mehrabi A, Cho B, Xiao
    Y, Ylä-Jääski A (2022) Data-driven capacity planning for vehicular fog computing.
    IEEE Internet Things J 9(15):13179–13194 Article   Google Scholar   Hamdi AMA,
    Hussain FK, Hussain OK (2022) Task offloading in vehicular fog computing: state-of-the-art
    and open issues. Futur Gener Comput Syst Ali M, Sadeghi M-R, Liu X, Miao Y, Vasilakos
    AV (2022) Verifiable online/offline multi-keyword search for cloud-assisted industrial
    internet of things. J Inf Secur Appl 65:103101 Google Scholar   Tang C, Wei X,
    Zhu C, Wang Y, Jia W (2020) Mobile vehicles as fog nodes for latency optimization
    in smart cities. IEEE Trans Veh Technol 69(9):9364–9375 Article   Google Scholar   Ning
    Z, Huang J, Wang X (2019) Vehicular fog computing: Enabling real-time traffic
    management for smart cities. IEEE Wirel Commun 26(1):87–93 Article   Google Scholar   Feng
    J, Liu Z, Wu C, Ji Y (2017) Ave: Autonomous vehicular edge computing framework
    with aco-based scheduling. IEEE Trans Veh Technol 66(12):10660–10675 Article   Google
    Scholar   Shojafar M, Cordeschi N, Baccarelli E (2016) Energy-efficient adaptive
    resource management for real-time vehicular cloud services. IEEE Trans Cloud Comput
    7(1):196–209 Article   Google Scholar   Peng X, Ota K, Dong M (2020) Multiattribute-based
    double auction toward resource allocation in vehicular fog computing. IEEE Internet
    Things J 7(4):3094–3103 Article   Google Scholar   Lee S-S, Lee S (2020) Resource
    allocation for vehicular fog computing using reinforcement learning combined with
    heuristic information. IEEE Internet Things J 7(10):10450–10464 Article   Google
    Scholar   Wu Q, Liu H, Wang R, Fan P, Fan Q, Li Z (2019) Delay-sensitive task
    offloading in the 802.11 p- based vehicular fog computing systems. IEEE Internet
    Things J 7(1):773–785 Article   Google Scholar   Peng X, Ota K, Dong M (2020)
    Multiattribute-based double auction toward resource allocation in vehicular fog
    computing. IEEE Internet Things J 7(4):3094–3103 Article   Google Scholar   Chai
    Q, Gong G (2012) Verifiable symmetric searchable encryption for semi-honest-but-curious
    cloud servers. In: 2012 IEEE International Conference on Communications (ICC).
    IEEE, pp 917–922 Benabbas S, Gennaro R, Vahlis Y (2011) Verifiable delegation
    of computation over large datasets. In: Advances in Cryptology–CRYPTO 2011: 31st
    Annual Cryptology Conference, Santa Barbara, CA, USA, August 14–18, 2011. Proceedings
    31. Springer, pp 111–131 Fiore D, Gennaro R (2012) Publicly verifiable delegation
    of large polynomials and matrix computations, with applications. In: Proceedings
    of the 2012 ACM Conference on Computer and Communications Security. pp 501–512
    Zheng Q, Xu S, Ateniese G (2014) Vabks: verifiable attribute-based keyword search
    over outsourced encrypted data. In: IEEE INFOCOM 2014-IEEE Conference on Computer
    Communications. IEEE, pp 522–530 Lu Y, Li J (2021) Lightweight public key authenticated
    encryption with keyword search against adaptively-chosen-targets adversaries for
    mobile devices. IEEE Trans Mob Comput 21(12):4397–4409 Article   ADS   Google
    Scholar   Miao Y, Deng RH, Choo K-KR, Liu X, Ning J, Li H (2019) Optimized verifiable
    fine-grained keyword search in dynamic multi-owner settings. IEEE Trans Dependable
    Secure Comput 18(4):1804–1820 Google Scholar   Tang C, Zhu C, Wei X, Wu H, Li
    Q, Rodrigues JJ (2020) Intelligent resource allocation for utility optimization
    in rsu-empowered vehicular network. IEEE Access 8:94453–94462 Article   Google
    Scholar   Xiao Y, Zhu C (2017) Vehicular fog computing: vision and challenges.
    In: 2017 IEEE International Conference on Pervasive Computing and Communications
    Workshops (PerCom Workshops). IEEE, pp 6–9 Ni J, Zhang A, Lin X, Shen XS (2017)
    Security, privacy, and fairness in fog-based vehicular crowdsensing. IEEE Commun
    Mag 55(6):146–152 Article   Google Scholar   Zhu C, Pastor G, Xiao Y, Ylajaaski
    A (2018) Vehicular fog computing for video crowdsourcing: Applications, feasibility,
    and challenges. IEEE Commun Mag 56(10):58–63 Article   Google Scholar   Tang C,
    Zhu C, Wei X, Chen W, Rodrigues JJ (2020) RSU-empowered resource pooling for task
    scheduling in vehicular fog computing. In: 2020 International Wireless Communications
    and Mobile Computing (IWCMC). IEEE, pp 1758–1763 Tang C, Xia S, Li Q, Chen W,
    Fang W (2021) Resource pooling in vehicular fog computing. J Cloud Comput 10:1–14
    Article   Google Scholar   Sahai A, Waters B (2005) Fuzzy identity-based encryption.
    In: Advances in Cryptology–EUROCRYPT 2005: 24th Annual International Conference
    on the Theory and Applications of Cryptographic Techniques, Aarhus, Denmark, May
    22–26, 2005. Proceedings 24. Springer, pp 457–473 Bethencourt J, Sahai A, Waters
    B (2007) Ciphertext-policy attribute-based encryption. In: 2007 IEEE Symposium
    on Security and Privacy (SP’07). IEEE, pp 321–334 Goyal V, Pandey O, Sahai A,
    Waters B (2006) Attribute-based encryption for fine-grained access control of
    encrypted data. In: Proceedings of the 13th ACM Conference on Computer and Communications
    Security. pp 89–98 Nasiraee H, Ashouri-Talouki M (2020) Anonymous decentralized
    attribute-based access control for cloud-assisted iot. Futur Gener Comput Syst
    110:45–56 Article   Google Scholar   Ali M, Mohajeri J, Sadeghi M-R, Liu X (2020)
    A fully distributed hierarchical attribute-based encryption scheme. Theoret Comput
    Sci 815:25–46 Article   MathSciNet   Google Scholar   Yeh L-Y, Chiang P-Y, Tsai
    Y-L, Huang J-L (2015) Cloud-based fine-grained health information access control
    framework for lightweight Iot devices with dynamic auditing andattribute revocation.
    IEEE Trans Cloud Comput 6(2):532–544 Article   Google Scholar   Li J, Wang S,
    Li Y, Wang H, Wang H, Wang H, Chen J, You Z (2019) An efficient attribute-based
    encryption scheme with policy update and file update in cloud computing. IEEE
    Trans Industr Inf 15(12):6500–6509 Article   Google Scholar   Lin S, Zhang R,
    Ma H, Wang M (2015) Revisiting attribute-based encryption with verifiable outsourced
    decryption. IEEE Trans Inf Forensics Secur 10(10):2119–2130 Article   Google Scholar   Sun
    W, Yu S, Lou W, Hou YT, Li H (2014) Protecting your right: Verifiable attribute-based
    keyword search with fine-grained owner-enforced search authorization in the cloud.
    IEEE Trans Parallel Distrib Syst 27(4):1187–1198 Article   Google Scholar   Miao
    Y, Liu X, Choo K-KR, Deng RH, Li J, Li H, Ma J (2021) Privacy-preserving attribute-based
    keyword search in shared multi-owner setting. IEEE Trans Dependable Secure Comput
    18(3):1080–1094 Article   Google Scholar   Miao Y, Deng RH, Choo K-KR, Liu X,
    Ning J, Li H (2021) Optimized verifiable fine-grained keyword search in dynamic
    multi-owner settings. IEEE Trans Dependable Secure Comput 18(4):1804–1820 Google
    Scholar   Ali M, Mohajeri J, Sadeghi M-R, Liu X (2020) Attribute-based fine-grained
    access control for outscored private set intersection computation. Inf Sci 536:222–243
    Article   MathSciNet   Google Scholar   Damgaard JB, Nielsen I (2002) Perfect
    hiding and perfect binding universally composable commitment schemes with constant
    expansion factor. Springer Berlin Heidelberg Gu K, Zhang W, Li X, Jia W (2022)
    Self-verifiable attribute-based keyword search scheme for distributed data storage
    in fog computing with fast decryption. IEEE Trans Netw Serv Manage 19(1):271–288
    Article   Google Scholar   Miao Y, Ma J, Liu X, Weng J, Li H, Li H (2019) Lightweight
    fine-grained search over encrypted data in fog computing. IEEE Trans Serv Comput
    12(5):772–785 Article   Google Scholar   The python pairing based cryptography
    library. Online: https://github.com/debatem1/pypbc Secure hashes and message digests
    library. Online: https://docs.python.org/3/library-hashlib.html#module-hashlib
    Lynn B (2006) PBC library manual 0.5. 11 Shim K-A (2010) An id-based aggregate
    signature scheme with constant pairing computations. J Syst Softw 83(10):1873–1880
    Article   Google Scholar   Katz J, Lindell Y (2014) Introduction to modern cryptography.
    CRC press Blanchet B, Smyth B, Cheval V, Sylvestre M (2018) ProVerif 2.00: automatic
    cryptographic protocol verifier, user manual and tutorial, Version from Dolev
    D, Yao A (1983) On the security of public key protocols. IEEE Trans Inf Theory
    29(2):198–208 Article   MathSciNet   Google Scholar   Download references Funding
    None. Author information Authors and Affiliations Department of Mathematics and
    Computer Science, Amirkabir University of Technology, Tehran, Iran Zahra Seyedi,
    Farhad Rahmati & Mohammad Ali College of Computer and Data Science, Fuzhou University,
    Fuzhou, 350116, Fujian Province, China Ximeng Liu Key Laboratory of Information
    Security of Network Systems, Fuzhou, 350116, Fujian Province, China Ximeng Liu
    Contributions Seyedi and Ali conceived, designed the scheme, proved the security,
    analyzed the data, performed the experiments, and wrote the paper Liu. and Rahmati
    reviewed and edited the manuscript. Corresponding author Correspondence to Mohammad
    Ali. Ethics declarations Ethics approval Not applicable. Consent to publish Not
    applicable. Competing interests The authors declare no competing interests. Additional
    information Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. This article is part
    of the Topical Collection: 1- Track on Networking and Applications Guest Editor:
    Vojislav B. Misic Appendix. Correctness proof Appendix. Correctness proof Definition
    5 Given a security parameter n, two universal attribute sets \\(\\mathbb {U}\\)
    and \\(\\mathbb {U}\\), an attribute set \\(Att_{du}\\), an access tree \\(\\mathcal
    {T}\\), two keyword sets \\(\\lbrace \\omega _j \\rbrace _{j=1}^m\\) and \\(\\lbrace
    \\hat{\\omega }_j \\rbrace _{j=1}^m\\), a threshold value \\(\\mathcal {R}\\),
    and identifiers \\(ID_{do}\\) and \\(ID_{du}\\), our FGDM is said to be correct
    if for each file F we have \\({\\textbf {DU.Dec}}(PK, \\mathcal {K}, \\mathcal{T}\\mathcal{K},
    C_{do}, t_{do}) = F\\) if and only if \\(Att_{du}\\) satisfies \\(\\mathcal {T}\\)
    and \\(| \\lbrace \\omega _j \\rbrace _{j=1}^m \\cap \\lbrace \\hat{\\omega }_j
    \\rbrace _{j=1}^m | \\ge \\mathcal {R}\\). Theorem 3 The proposed FGDM scheme
    is correct. Proof Assume the presented parameters in Definition 5. First we have
    to show that Token Decryption is correct. $$\\begin{aligned}B_{v_{a},vfn} &= \\frac{\\sigma
    (C_{a}, SK_{a, vfn})}{\\sigma (C_{a}, \\Gamma \\beta _{4}).\\sigma (C''_{a}, ID_{vfn})}
    \\\\&= \\frac{\\sigma (\\alpha _{0}^{q_{v_a}}, \\Gamma \\beta _{3}ID_{vfn}^{s''_a})}{\\sigma
    (C_{a}, \\Gamma \\beta _{4}).\\sigma (C''_{a}, ID_{vfn})} \\\\&= \\frac{\\sigma
    (\\alpha _{0}^{q_{v_a}}, \\beta _{3})\\sigma (\\alpha _{0}^{q_{v_a}}, \\Gamma
    \\beta _{0}^{-t}\\beta _{0}^{t} ID_{vfn}^{s''_a})}{\\sigma (C_{a}, \\Gamma \\beta
    _{4}).\\sigma (C''_{a}, ID_{vfn})} \\\\&= \\frac{\\sigma (\\alpha _{0}^{q_{v_a}},
    \\beta _{3}) \\sigma (C_{a}, \\Gamma \\beta _{4}) \\sigma (\\alpha _{0}^{q_{v_a}},
    \\beta _{0}^{-t}) \\sigma (\\alpha _{0}^{q_{v_a}}, ID_{vfn}^{s''_a})}{\\sigma
    (C_{a}, \\Gamma \\beta _{4}).\\sigma (C''_{a}, ID_{vfn})} \\\\&= \\frac{\\sigma
    (\\alpha _{0}^{q_{v_a}}, \\beta _{3}) \\sigma (\\alpha _{0}^{q_{v_a}}, \\beta
    _{0}^{-t}) \\sigma (\\alpha _{0}^{q_{v_a}}, ID_{vfn}^{s''_a})}{\\sigma (C''_{a},
    ID_{vfn})} \\\\&= \\frac{\\sigma (\\alpha _{0}^{q_{v_a}}, \\beta _{3}) \\sigma
    (\\alpha _{0}^{q_{v_a}}, \\beta _{0}^{-t}) \\sigma ((\\alpha _{2} \\alpha _{2}^{-1}
    \\alpha _{0}^{s''_a})^{q_{v_a}} , ID_{vfn})}{\\sigma (C''_{a}, ID_{vfn})} \\\\&=
    \\frac{\\sigma (\\alpha _{0}^{q_{v_a}}, \\beta _{3}) \\sigma (\\alpha _{0}^{q_{v_a}},
    \\beta _{0}^{-t}) \\sigma (C''_{a}, ID_{vfn}) \\sigma (\\alpha _{2}^{q_{v_a}},
    ID_{vfn})}{\\sigma (C''_{a}, ID_{vfn})} \\\\&= \\sigma (\\beta _0 , \\alpha _1)^{q_{v_a}}.\\sigma
    (\\beta _{0}^{-1}, \\beta _1)^{q_{v_a}}.\\sigma (\\alpha _2 , ID_{vfn})^{q_{v_a}}
    \\\\&= \\Sigma _{1}^{q_{v_{a}}}. \\Sigma _{2}^{q_{v_{a}}} . \\sigma (\\alpha _{2},
    ID_{vfn})^{q_{v_{a}}}. \\end{aligned}$$ This proves Eq. 1. Moreover, $$\\begin{aligned}
    TK_{du} = \\frac{V}{\\frac{B_{R,fn}}{{V''.\\sigma (C, ID_{vfn})}}} = \\frac{TK_{du}\\Sigma
    _{1}^{k}}{\\frac{\\Sigma _{1}^{k}V''\\sigma (C, ID_{vfn})}{V''\\sigma (C, ID_{vfn})}}
    = TK_{du}. \\end{aligned}$$ This proves Eq. 3. Now, let F be an arbitrary file.
    In the following, we show that DU.Dec\\((PK, \\mathcal {K}, \\mathcal{T}\\mathcal{K},
    C_{do}, t_{do}) = F\\) if and only if \\(Att_{du}\\) satisfies \\(\\mathcal {T}\\)
    and \\(| \\lbrace \\omega _j \\rbrace _{j=1}^m \\cap \\lbrace \\hat{\\omega }_j
    \\rbrace _{j=1}^m | \\ge \\mathcal {R}\\). We first show that Eq. 4 holds: $$\\begin{aligned}L_{a}
    &= \\frac{\\sigma (C_{v_{a}}\\lambda _{1}, \\lambda _{du,a})}{\\sigma (C_{v_{a}}\\lambda
    _{1}, \\beta _{2}). \\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}). \\sigma
    (C''_{v_{a}}, ID_{du}).\\sigma (\\alpha _{0}, \\alpha _{2})^{\\lambda _5}} \\\\&=
    \\frac{\\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal {K}} , \\alpha _{1}^{s}\\Gamma
    ID_{du}^{s_a} \\alpha _{2}^{\\mathcal {K}''})}{\\sigma (C_{v_{a}}\\lambda _{1},
    \\beta _{2}). \\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}). \\sigma (C''_{v_{a}},
    ID_{du}).\\sigma (\\alpha _{0}, \\alpha _{2})^{\\lambda _5}}\\\\&= \\frac{\\sigma
    (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal {K}} , \\alpha _{1}^{s}\\Gamma \\beta _{1}^{s}\\beta
    _{1}^{-s} ID_{du}^{s_a} \\alpha _{2}^{\\mathcal {K}''})}{\\sigma (C_{v_{a}}\\lambda
    _{1}, \\beta _{2}). \\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}). \\sigma
    (C''_{v_{a}}, ID_{du}).\\sigma (\\alpha _{0}, \\alpha _{2})^{\\lambda _5}} \\\\&=
    \\frac{\\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal {K}} , \\alpha _{1}^{s}\\beta
    _{2} ID_{du}^{s_a} \\alpha _{2}^{\\mathcal {K}''})}{\\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal
    {K}}, \\beta _{2}). \\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}). \\sigma
    (C''_{v_{a}}, ID_{du}).\\sigma (\\alpha _{0}, \\alpha _{2})^{\\lambda _5}} \\\\&=
    \\frac{\\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal {K}} , \\alpha _{1}^{s}\\beta
    _{1}^{-s} ID_{du}^{s_a} ) \\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal {K}},
    \\alpha _{2}^{\\mathcal {K}''})}{\\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}).
    \\sigma (C''_{v_{a}}, ID_{du}).\\sigma (\\alpha _{0}, \\alpha _{2})^{\\mathcal
    {K}\\mathcal {K}''}} \\\\&= \\frac{\\sigma (\\alpha _{0}^{D_{a}-sk_{do}+\\mathcal
    {K}} , \\alpha _{1}^{s}\\beta _{1}^{-s} ID_{du}^{s_a} ) \\sigma (\\alpha _{0}^{D_{a}-sk_{do}},
    \\alpha _{2}^{\\mathcal {K}''})}{\\sigma (pk_{a}\\alpha _{2}^{-1}, \\lambda _{3}).
    \\sigma (C''_{v_{a}}, ID_{du})} \\\\&= \\frac{(\\sigma (\\alpha _{0} , \\alpha
    _{1}^{s}\\beta _{1}^{-s}) \\sigma (pk_{a}\\alpha _{2}^{-1}\\alpha _{2}, ID_{du})
    )^{D_{a}-sk_{do}+\\mathcal {K}} \\sigma (\\alpha _{0}^{D_{a}-sk_{do}}, \\alpha
    _{2}^{\\mathcal {K}''})}{\\sigma (pk_{a}\\alpha _{2}^{-1}, ID_{du}^{\\mathcal
    {K}}). \\sigma ((pk_{a}\\alpha _{2}^{-1})^{D_{a}-sk_{do}, ID_{du})}} \\\\&= (\\sigma
    (\\beta _{0} , \\alpha _{1}) \\sigma (\\beta _{0}^{-1} , \\beta _{1}) \\sigma
    (\\alpha _{2}, ID_{du}) )^{D_{a}-sk_{do}+\\mathcal {K}} \\sigma (\\alpha _{0}^{D_{a}-sk_{do}},
    \\alpha _{2}^{\\mathcal {K}''}) \\\\&= (\\Sigma _1 . \\Sigma _2 . \\sigma (\\alpha
    _2 , ID_{du}))^{D_{a} - sk_{do} + \\mathcal {K}} . \\sigma (\\alpha _{0}^{\\mathcal
    {K''}}, \\alpha _{2})^{D_{a}-sk_{do}}. \\end{aligned}$$ Now, according to the
    definition of the \\({\\textbf {Combine}}\\) algorithm [39], the output of \\({\\textbf
    {Combine}}(\\lbrace L_{a} \\rbrace _{a \\in S}, \\mathcal {T})\\) is equal to
    $$\\begin{aligned} L&= (\\Sigma _1 . \\Sigma _2 . \\sigma (\\alpha _2 , ID_{du}))^{r''
    -sk_{do}+ \\mathcal {K}} . \\sigma (\\lambda _4, \\alpha _{2})^{r''-sk_{do}} \\\\&=
    (\\Sigma _1 . \\Sigma _2 . \\sigma (\\alpha _2 , ID_{du}))^{r + \\mathcal {K}}
    . \\sigma (\\alpha _{0}^{\\mathcal {K''}}, \\alpha _{2})^{r}. \\end{aligned}$$
    if and only if \\(Att_{du}\\) satisfies \\(\\mathcal {T}\\). Moreover, $$\\begin{aligned}
    \\mathcal{T}\\mathcal{K}&= LC_{2}^{-1}.\\sigma (\\beta _0 , \\lambda _2 ).\\sigma
    (C_1 , ID_{du}\\lambda _4 ) ^{-1} . \\sigma (\\alpha _2 , \\lambda _3 )^{-1} \\\\&=
    \\frac{\\Sigma _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{r+\\mathcal {K}} . \\sigma
    (\\alpha _2 , ID_{du})^{r+\\mathcal {K}} . \\sigma (\\alpha _{0}^{\\mathcal {K}''}
    , \\alpha _2)^{r} . C_{2}^{-1} \\sigma (\\beta _0 , \\lambda _{2})}{ \\sigma (C_1
    , ID_{du}\\lambda _{4}). \\sigma (\\alpha _2 , \\lambda _3) } \\\\&= \\frac{\\Sigma
    _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{r+\\mathcal {K}} . \\sigma (\\alpha _2 ,
    ID_{du})^{r+\\mathcal {K}} . \\sigma (\\alpha _{0}^{\\mathcal {K}''} , \\alpha
    _2)^{r} . C_{2}^{-1} \\sigma (\\beta _0 , \\lambda _{2})}{ \\sigma (\\alpha _{2}^{r}
    , ID_{du}\\alpha _{0}^{\\mathcal {K}''}). \\sigma (\\alpha _2 , ID_{du}^{\\mathcal
    {K}}) } \\\\&= \\frac{\\Sigma _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{r+\\mathcal
    {K}} . \\sigma (\\alpha _2 , ID_{du})^{r+\\mathcal {K}} . \\sigma (\\alpha _{0}^{\\mathcal
    {K}''} , \\alpha _2)^{r} . C_{2}^{-1} \\sigma (\\beta _0 , \\lambda _{2})}{ \\sigma
    (\\alpha _{2}^{r} , ID_{du}). \\sigma (\\alpha _{2}^{r} , \\alpha _{0}^{\\mathcal
    {K}''}). \\sigma (\\alpha _2 , ID_{du}^{\\mathcal {K}}) } \\\\&= \\frac{\\Sigma
    _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{r+\\mathcal {K}} . \\sigma (\\alpha _2 ,
    ID_{du})^{r+\\mathcal {K}} . \\sigma (\\alpha _{0}^{\\mathcal {K}''} , \\alpha
    _2)^{r} . C_{2}^{-1} \\sigma (\\beta _0 , \\lambda _{2})}{ \\sigma (\\alpha _{2}
    , ID_{du})^{r + \\mathcal {K}}. \\sigma (\\alpha _{2}^{r} , \\alpha _{0}^{\\mathcal
    {K}''}) }\\\\&= \\Sigma _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{r+\\mathcal {K}}
    C_{2}^{-1} \\sigma (\\beta _0 , \\lambda _{2}) = \\Sigma _{1}^{r+\\mathcal {K}}
    \\Sigma _{2}^{r+\\mathcal {K}} \\Sigma _{2}^{-r} \\sigma (\\beta _0 , \\beta _{1}^{\\mathcal
    {K}}) \\\\&= \\Sigma _{1}^{r+\\mathcal {K}} \\Sigma _{2}^{\\mathcal {K}} . \\sigma
    (\\beta _{0}^{-1} , \\beta _{1})^{-\\mathcal {K}} = \\Sigma _{1}^{r+ \\mathcal
    {K}}. \\end{aligned}$$ Now, we see that \\(\\mathcal{T}\\mathcal{K}.\\Sigma _{1}^{W_j}.W''_{j}
    = \\Sigma _{1}^{r+\\mathcal {K}}. \\Sigma _{1}^{\\omega _{j}-r+r_{j}}.\\Sigma
    _{1}^{-r_j} = \\Sigma _{1}^{\\mathcal {K}+\\omega _{j}}\\) On the other hand,
    for each \\(\\omega , \\hat{\\omega } \\in \\mathcal {Z}_{p}\\), we know that
    \\(\\omega = \\hat{\\omega }\\) if and only if \\(\\Sigma _{1}^{\\mathcal {K}+
    \\omega } = \\Sigma _{1}^{\\mathcal {K}+ \\hat{\\omega }}\\). Then, \\(|\\lbrace
    \\omega _{j} \\rbrace _{j=1}^{m} \\cap \\lbrace \\hat{\\omega }_{j} \\rbrace _{j=1}^{l}|
    = | \\lbrace H(\\mathcal{T}\\mathcal{K}.\\Sigma _{1}^{W_j}.W''_{j})\\rbrace _{j=1}^{m}
    \\cap \\lbrace \\hat{W}_{j} \\rbrace _{j=1}^{l}|\\). Therefore, \\(|\\lbrace \\omega
    _{j} \\rbrace _{j=1}^{m} \\cap \\lbrace \\hat{\\omega }_{j} \\rbrace _{j=1}^{l}|
    \\ge \\mathcal {R}\\) if and only if \\(| \\lbrace H(\\mathcal{T}\\mathcal{K}.\\Sigma
    _{1}^{W_j}.W''_{j})\\rbrace _{j=1}^{m} \\cap \\lbrace \\hat{W}_{j} \\rbrace _{j=1}^{l}|
    \\ge \\mathcal {R}\\). So, we proved that \\({\\textbf {Search}}(PK, TK_{du},
    ID_{du}, SCT_{\\mathcal {T}}, \\mathcal {R})\\) produces a valid response \\(Re
    = (C''_{do}, c_{do}, \\sigma _{do}, \\mathcal{T}\\mathcal{K})\\) if and only if
    \\(Att_{du}\\) satisfies \\(\\mathcal {T}\\), and \\(|\\lbrace \\omega _{j} \\rbrace
    _{j=1}^{m} \\cap \\lbrace \\hat{\\omega }_{j} \\rbrace _{j=1}^{l}| \\ge \\mathcal
    {R}\\). It proves the theorem as in this case \\(F =\\) DU.Dec\\((PK, \\mathcal
    {K}, \\mathcal{T}\\mathcal{K}, C_{do}, t_{do})\\). Rights and permissions Springer
    Nature or its licensor (e.g. a society or other partner) holds exclusive rights
    to this article under a publishing agreement with the author(s) or other rightsholder(s);
    author self-archiving of the accepted manuscript version of this article is solely
    governed by the terms of such publishing agreement and applicable law. Reprints
    and permissions About this article Cite this article Seyedi, Z., Rahmati, F.,
    Ali, M. et al. Verifiable and privacy-preserving fine-grained data management
    in vehicular fog computing: A game theory-based approach. Peer-to-Peer Netw. Appl.
    17, 410–431 (2024). https://doi.org/10.1007/s12083-023-01601-x Download citation
    Received 13 July 2023 Accepted 18 November 2023 Published 20 December 2023 Issue
    Date January 2024 DOI https://doi.org/10.1007/s12083-023-01601-x Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Vehicular fog computing Encrypted data processing Data retrieval Data
    management Nash equilibrium Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Associated Content Part of a collection: 1- Track on Networking
    and Applications Sections Figures References Abstract Introduction Related work
    Preliminaries Problem formulation Proposed FGDM in IoV environment KWSOP game
    Security analysis Formal verification using ProVerif Performance analysis Conclusion
    Data availability Notes References Funding Author information Ethics declarations
    Additional information Appendix. Correctness proof Rights and permissions About
    this article Advertisement Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Peer-to-Peer Networking and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Verifiable and privacy-preserving fine-grained data management in vehicular
    fog computing: A game theory-based approach'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
