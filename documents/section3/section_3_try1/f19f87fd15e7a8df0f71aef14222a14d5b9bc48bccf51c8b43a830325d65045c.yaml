- analysis: '>'
  authors:
  - Majumdar P.
  - Bhattacharya D.
  - Mitra S.
  - Bhushan B.
  citation_count: '8'
  description: Enabling technologies of Agriculture 4.0 such as IoT-driven Precision
    Agriculture (PA), Unmanned Aerial Vehicles (UAVs), and big data analytics are
    collaborating for the transformation of global agribusiness. In PA applications,
    IoT devices sense, collect, and transmit data to the cloud or edge for processing
    and processed data are stored in data centers. This exchange of a very large amount
    of information amongst billions of interconnected devices demands massive energy
    in PA applications. The growth of IoT devices is exponentially increasing directly
    or indirectly generating Green House Gases and causing energy deficiency in power-hungry
    IoT components like sensors. Thus, adopting green solutions is inevitable to promote
    energy-conserving, environment-friendly, and cost-effective IoT component designs.
    Inspired by achieving a green environment for IoT, we first give an overview of
    different data processing and energy-conserving strategies using machine learning,
    cloud computing, and edge computing. We then discuss and evaluate different Green
    IoT (GIoT) solutions that can be implemented for GIoT-based PA leveraging UAVs,
    Low Power Wide Area Networks (LPWANs), and 5G networks along with several implementation
    concerns. In addition, the sustainable progress towards Agriculture of 5.0 by
    integrating GIoT components is discussed to make the IoT greener using 5G networks
    and beyond. Based on the current survey, we have conceptualized a GIoT framework
    for designing energy-conserving, cost-effective, and environment-friendly PA applications
    while enabling ubiquitous connectivity. Finally, this paper systematically summarizes
    different security threats in GIoT layers and analyzes the measures that can be
    adopted to mitigate those threats in detail.
  doi: 10.1007/s11277-023-10521-1
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Wireless Personal Communications
    Article Application of Green IoT in Agriculture 4.0 and Beyond: Requirements,
    Challenges and Research Trends in the Era of 5G, LPWANs and Internet of UAV Things
    Published: 08 June 2023 Volume 131, pages 1767–1816, (2023) Cite this article
    Download PDF Access provided by University of Nebraska-Lincoln Wireless Personal
    Communications Aims and scope Submit manuscript Parijata Majumdar, Diptendu Bhattacharya,
    Sanjoy Mitra & Bharat Bhushan  567 Accesses 9 Citations Explore all metrics Abstract
    Enabling technologies of Agriculture 4.0 such as IoT-driven Precision Agriculture
    (PA), Unmanned Aerial Vehicles (UAVs), and big data analytics are collaborating
    for the transformation of global agribusiness. In PA applications, IoT devices
    sense, collect, and transmit data to the cloud or edge for processing and processed
    data are stored in data centers. This exchange of a very large amount of information
    amongst billions of interconnected devices demands massive energy in PA applications.
    The growth of IoT devices is exponentially increasing directly or indirectly generating
    Green House Gases and causing energy deficiency in power-hungry IoT components
    like sensors. Thus, adopting green solutions is inevitable to promote energy-conserving,
    environment-friendly, and cost-effective IoT component designs. Inspired by achieving
    a green environment for IoT, we first give an overview of different data processing
    and energy-conserving strategies using machine learning, cloud computing, and
    edge computing. We then discuss and evaluate different Green IoT (GIoT) solutions
    that can be implemented for GIoT-based PA leveraging UAVs, Low Power Wide Area
    Networks (LPWANs), and 5G networks along with several implementation concerns.
    In addition, the sustainable progress towards Agriculture of 5.0 by integrating
    GIoT components is discussed to make the IoT greener using 5G networks and beyond.
    Based on the current survey, we have conceptualized a GIoT framework for designing
    energy-conserving, cost-effective, and environment-friendly PA applications while
    enabling ubiquitous connectivity. Finally, this paper systematically summarizes
    different security threats in GIoT layers and analyzes the measures that can be
    adopted to mitigate those threats in detail. Similar content being viewed by others
    Smart Farming: Applications of IoT in Agriculture Chapter © 2022 Smart Agriculture:
    A Survey on Challenges and Opportunities with Recent Advancements Chapter © 2021
    IoT Enabled Technologies in Smart Farming and Challenges for Adoption Chapter
    © 2022 1 Introduction Agriculture 4.0 focuses on Precision Agriculture (PA) to
    replace traditional farming with smart farming through the introduction of modern,
    scalable, and automated technological solutions. The technological enablers of
    Agriculture 4.0 includes the Internet of Things (IoT), Big Data, Artificial Intelligence
    (AI), Cloud Computing, Remote Sensing, Unmanned Aerial Vehicles (UAVs), and the
    fifth generation (5G) network to deliver automated, predictive and reliable smart
    farming decisions [1]. The 5G network supports advanced communication technologies
    which help in the widespread of IoT-based services in future generations [2, 3].
    As agricultural practices involve the acquisition of large volumes of real-time
    weather data to be transmitted, processed, and stored for farmers to make time-sensitive
    decisions like irrigation scheduling, a lot of energy is consumed from IoT components
    such as sensors used for data acquisition, data transmission through communication
    protocols and data storage in cloud or edge, etc. Most of the IoT components emit
    greenhouses gases (GHGs) either directly or indirectly that are hazardous to the
    environment due to extreme device manufacturing, large shipments of those devices,
    overloading the data centers to serve millions of services, and heavy data traffic
    generated from billions of devices [4]. Promoting biodegradable IoT component
    designs with operational cost management and reduced power consumption can remarkably
    improve the efficiency of agricultural activities with little or no impact on
    the environment. PA applications relying on sensor-generated data for executing
    diverse applications necessitate the use of big data analytics coupled with Artificial
    Intelligence to perform intelligent analysis of these enormous data volumes associated
    with weather, soil, and crop characteristics. Wireless sensors employed for real-time
    weather data sensing facilitate and comprehend the intricate correlation of climate
    change and crop growth in real-time, decreasing the problem of environment parameter
    fluctuation and non-linearity in data acquisition when the agriculture field is
    located in a remote location. Among the widespread applications of PA, smart irrigation
    is gaining increasing attention because crop yield and farmland productivity are
    highly reliant on irrigation scheduling decisions that depend on environmental
    parameters like temperature, rainfall, etc. Traditional weather stations though
    help to determine the impact of climate change on crop yield but issues like the
    agriculture field being located in remote areas can cause weather data to fluctuate
    and data may become inconsistent. Unlike traditional weather stations, IoT-oriented
    agriculture practices are advantageous because real-time data can be acquired
    at any place and at any time through low-cost interconnected sensors transmitted
    to the cloud for storage through suitable communication protocols [5]. Despite
    the numerous advantages IoT offers in PA, the growing interconnection of IoT devices
    in PA is extremely power-consuming and generates GHGs that may lead to loss of
    soil fertility in agricultural land and also generates non-biodegradable solid
    and hazardous wastes from components like active Radio Frequency Identification
    (RFID) tags that could not be recycled easily posing threat to our environment
    [6]. An increasing number of IoT connections are energy intensive as huge data
    are transmitted and processed. Billions of batteries supporting energy-intensive
    IoT operations are also disposed of and replaced throughout the life cycle of
    these IoT devices. Combined global e-waste increased dramatically in 2016 which
    was 44.7 million metric tonnes an average of 6.1 kg/inh (kilograms per inhabitant)
    as compared to 5.8 kg/inh in 2014. However, only 20% (8.9 Mt) of total global
    e-waste is recycled globally. A report presented by Guardian Environment Network
    estimates that in the upcoming 10 years, 3.5% of the global carbon emissions will
    be coming from IoT devices [7]. Thus, the adoption of GreenIoT (GIoT) is inevitable
    to address the GHG emission problem from IoT components which in turn helps in
    better waste management through recycling due to biodegradable waste generation
    from components like passive RFID tags [6]. Design goals of GIoT include operating
    cost reduction and minimizing energy constraint issues of IoT components using
    renewable sources of energy and energy harvesting (EH) techniques and reducing
    the effects of GHGs emissions on the environment. To achieve this design goal,
    a plethora of green designs of hardware and software components are implemented
    that include Green Wireless Sensor Networks (GWSN), Green Radio frequency identification
    (GRFIDs) tags, Green Cloud Computing (GCC), and Green Datacenters (GDC), etc.
    to achieve maximum energy efficiency in all the stages namely, sensing, processing,
    transmitting and storing data [8]. Recently, UAVs emerged as a promising technique
    for intelligent monitoring of the agriculture field with considerable transmission
    power reduction and minimized environmental pollution unlike conventional IoT-based
    agriculture field monitoring techniques [6, 9]. As a result, this article provides
    an overall assessment of approaches for greening IoT leveraging UAVs to allow
    energy-saving, maintaining QoS, low-cost service delivery to vast regions, and
    mitigating the carbon footprint. Low-Power Wide Area Networks (LPWANs) are an
    alternative low-power communication protocol to traditional communication protocols
    that connects low-bandwidth, battery-powered IoT devices across large distances
    [10]. LPWANs support far-off communication using narrow-range communication channels
    for noise reduction [10, 11]. Earlier works based on energy-saving LPWANs are
    therefore also studied in detail in this paper. Algorithms based on Artificial
    Intelligence (AI) use methods to examine large volumes of data generated from
    IoT devices to provide value-added public services [12]. Intelligent processing
    can be done using Machine Learning (ML) algorithms to make time-sensitive, reliable,
    and accurate decisions for different PA tasks like irrigation scheduling to ensure
    crop yield with optimum utilization of water resources [13]. So, ML-based solutions
    for different PA tasks along with different energy management strategies are elaborated.
    The need for adopting 5G networks to meet low cost, low latency, and energy efficiency
    requirements of PA applications is also discussed for the sustainable transition
    from Agriculture 4.0 to Agriculture 5.0. IoT networks need to connect and establish
    communication links across numerous ubiquitous things, which generate a significant
    volume of data that is both vast and heterogeneous. Data processing by IoT devices
    themselves is constrained by their limited computational power and battery, which
    act as the device’s energy sources. Thus, it is necessary to use the cloud computing
    paradigm to process the data that IoT network devices create [14]. Processing
    the sensor-generated data transmitted using a large number of IoT communication
    protocols in the cloud or edge is also very energy intensive. Therefore, augmenting
    energy-saving solutions in the cloud or edge-based processing to process this
    enormous volume of data is also required to address the issue of energy consumption
    of the massive number of interconnected IoT components. Different energy-conserving
    strategies in machine learning, cloud computing, and edge computing-based data
    processing, therefore, need to be discussed in detail. Furthermore, the pros and
    cons of different energy-saving approaches adopted in GIoT and layer-specific
    security threats to GIoT systems are also discussed based on which avenues for
    future research in PA are presented. Based on the current survey, the main goal
    of our paper is to conceptualize a GIoT framework leveraging UAVs, LPWANs, and
    5G networks that meet the requirements of designing energy-aware, cost-effective,
    and environment-friendly PA applications for realizing the true potential of GIoT
    in PA while enabling ubiquitous connectivity. 1.1 Organization of the Paper The
    rest of the article is organized as follows. Section 2 describes the motivation
    and significance of the review. Section 3 describes IoT enablers and associated
    challenges. Section 4 describes green solutions for IoT Enablers and their implementation
    concerns. Data Processing techniques and different energy management strategies
    are described in Sect. 5. Section 6 describes different GIoT strategies leveraging
    Unmanned Aerial Vehicles (UAVs), Low Power Wide Area Networks (LPWANs), and 5G
    Networks. Section 7 explains the proposed GIoT Framework for Energy-Aware PA applications.
    Section 8 describes the different security attacks in different layers of GIoT
    and solutions to mitigate those threats. Section 9 concludes. Table 1 Energy saving
    methods for GIoT based smart PA design components Full size table 2 Motivation
    In PA applications, an enormous number of interconnected IoT devices communicate
    with one another to exchange huge volumes of weather data that directly or indirectly
    generate a lot of GHGs. These GHGs may result in loss of soil fertility in agricultural
    land hindering crop growth and yield. Thus, a significant reduction of environmental
    impacts on agricultural land must be ensured in terms of GHG emission reduction
    [4]. Adopting practices that minimize power usage and emissions without compromising
    system performance necessitates the use of GIoT. Conserving energy and reducing
    \\(\\text{CO}_{2}\\) emissions are the main obstacles in the implementation of
    a green environment. Numerous studies are working on creating green communication
    that minimizes harmful emissions (GHG), maximizes bandwidth usage, and consumes
    as little power as possible while maintaining the optimal performance of emerging
    IoT applications (such as PA). Adoption of green designs for IoT components can
    considerably reduce the environmental impacts of GHGs emissions on crop growth
    and yield by cutting down carbon emissions effectively. Furthermore, problems
    like the increasing number of power-hungry components in IoT interconnections,
    non-biodegradable dumping of non-recyclable waste generated from components like
    active RFID tags, energy-intensive computations of a huge volume of data in cloud
    or edge, operational costs incurred in an increasing number of interconnections
    of IoT components, overloaded data centers with enormous data traffic, etc. needs
    to be tackled successfully for leveraging the true potential of IoT in automation
    of PA tasks. All these factors have roused researchers for finding green solutions
    for IoT-based PA applications for designing low-cost and energy-efficient IoT
    components with minimal GHG emissions. To identify the significance of the paradigm
    shift of IoT to GIoT, we analyze different emerging trends like 5G networks and
    UAVs, etc. leveraging which energy-saving approaches can be designed to meet GIoT
    design goals. The suitability of LPWANs is also ascertained as a low power-consuming
    alternative to traditional communication protocols while providing wider coverage
    of agricultural land. Thus, this survey is envisioned to provide an organized
    assessment of existing GIoT-based PA solutions including 5G networks, UAVs, and
    LPWANs to provide farmers with a cost-effective, low-power consuming, environment-friendly,
    robust, and autonomous alternative to IoT-based PA solutions to replace traditional
    farming with smart green farming for sustainable progress towards Agriculture
    5.0 while ensuring ubiquitous connectivity in PA. Table 1 illustrates a few energy-saving
    methods implemented using GIoT-based smart PA design components. Fig. 1 Frequency
    of occurence of keyword Agriculture 4.0 in reputed journals Full size image Fig.
    2 Frequency of occurence of keyword Agriculture 5.0 in reputed journals Full size
    image Fig. 3 Frequency of occurence of keyword Green Cloud Computing(GCC) in reputed
    journals Full size image Fig. 4 Frequency of occurence of keyword Green Data Centers
    (GDC) in reputed journals Full size image 2.1 Keyword Analysis The keywords that
    have a significant correlation to green designs of IoT components in the PA field
    are analyzed that appeared in the title of the articles in reputed journal databases.
    The articles that appeared in Springer nature journals, Science Direct journals,
    and IEEE Xplore filtering the recently published papers in the last 4 years viz.,
    2019, 2020, 2021 to 2022 are illustrated. The links of reputed journal databases
    from where the significant keywords are searched are https://link.springer.com/,
    https://www.sciencedirect.com/, https://ieeexplore.ieee.org/Xplore/home.jsp. The
    keyword 5G appeared 2271, 2807, 4153, and 1571 times in Springer. The keyword
    5G appeared 1580, 1858, 2272, and 811 times in IEEE. The keyword 5G appeared 1019,
    1355, 1737, and 826 times in Science Direct. The keyword LPWANs appeared 23, 45,
    56, and 36 times in Springer. The keyword LPWANs appeared 32, 48, 58, and 31 times
    in IEEE. The keyword LPWANs appeared 498, 684, 731, and 352 times in Science Direct.
    The keyword Energy Efficiency and IoT appeared 769, 1261, 2239, and 947 times
    in Springer. The keyword Energy Efficiency and IoT appeared 217, 269,376, and
    155 times in IEEE. The keyword Energy Efficiency and IoT appeared 1243, 1633,
    2014, and 1007 times in Science Direct. The keyword UAV appeared 779, 1010, 1534,
    and 618 times in Springer. The keyword UAV appeared 635, 946, 1241, and 715 times
    in IEEE. The keyword UAV appeared 966, 1256, 1471, and 559 times in Science Direct.
    The keyword Agriculture 4.0 appeared 394, 480, 754, and 185 times in Springer.
    The keyword Agriculture 4.0 appeared 15, 7, 18, and 6 times in IEEE. The keyword
    Agriculture 4.0 appeared 498, 684, 731, and 355 times in Science Direct. Fig.
    5 Frequency of occurence of keyword Green Communication and Networking (GCN) in
    reputed journals Full size image Fig. 6 Frequency of occurence of keyword IoT
    and Green House Gases (GHG) in reputed journals Full size image Fig. 7 Frequency
    of occurence of keyword Green wireless Sensor Network(GWSN) in reputed journals
    Full size image Fig. 8 Frequency of occurence of keyword Green IoT in reputed
    journals Full size image The keyword Agriculture 5.0 appeared 119, 130, 412, and
    147 times in Springer. The keyword Agriculture 5.0 appeared 5, 4, 7, and 0 times
    in IEEE. The keyword Agriculture 5.0 appeared 466, 507, 643, and 324 times in
    Science Direct. The keyword Energy Efficiency and IoT appeared 29, 24, 32, and
    11 times in Springer. The keyword Energy Efficiency and IoT appeared 80, 51, 4,
    and 0 times in IEEE. The keyword Energy Efficiency and IoT appeared 57, 80, 52,
    and 24 times in Science Direct. The keyword IoT and GHGs appeared 36, 61, 128,
    and 65 times in Springer. The keyword IoT and GHGs appeared 27, 45, 53, and 39
    times in IEEE. The keyword IoT and GHGs appeared 117, 167, 250, and 164 in Science
    Direct. The keywords GDC appeared 3740, 4386, 5578, and 2472 times in Springer.
    The keyword GDC appeared 83, 91, 147, and 49 times in IEEE. The keyword GDC appeared
    in 7769, 9441, 12,209, and 6146 in Science Direct.The keyword GWSN appeared 435,
    518, 653, and 297 times in Springer. The keyword GWSN appeared 649, 750, 808,
    and 403 times in IEEE. The keyword GWSN appeared 427, 532, 606, and 300 in Science
    Direct.The keyword GCN appeared in 1990, 2310, 2961, and 1386 times in Springer.
    The keyword GCN appeared 2148, 2506, 3140, and 1494 times in IEEE. The keyword
    GCN appeared in 1476, 1720, 2236, and 1081 in Science Direct. The keyword GM2M
    appeared 3026, 3583, 4889, and 2387 times in Springer. The keyword GM2M appeared
    6243, 7610, 9799, and 4929 times in IEEE. The keyword GM2M appeared in 1476, 1720,
    2236, and 1081 in Science Direct.The keyword GRFID appeared 134, 126, 195, and
    83 times in Springer. The keyword GRFID appeared 111, 147, 171, and 81 times in
    IEEE. The keyword GRFID appeared in 207, 268, 314, and 148 in Science Direct.The
    keyword GCC appeared 719, 963, 1292, and 600 times in Springer. The keyword GCC
    appeared 873, 1053, 1224, and 605 times in IEEE. The keyword GCC appeared in 1653,
    1577, 1961, and 970 in Science Direct. Figures 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
    11, 12, 13 and 14 shows appearances of various keywords in reputed journal database.
    Fig. 9 Frequency of occurence of keyword Unmanned Aerial Vehicles (UAV) in reputed
    journals Full size image Fig. 10 Frequency of occurence of keyword Low Power Wide
    Area Networks (LPWANs) in reputed journals Full size image Fig. 11 Frequency of
    occurence of keyword Green Machine to Machine (GM2M) in reputed journals Full
    size image Fig. 12 Frequency of occurence of keyword Energy efficiency and IoT
    in reputed journals Full size image Fig. 13 Frequency of occurence of keyword
    5G in reputed journals Full size image Fig. 14 Frequency of occurence of keyword
    GRFID in reputed journals Full size image Fig. 15 Trends of Data Sent / Received,
    rate of energy consumption, data processing load and predicted number of connections
    Full size image 2.2 Ecological Impacts of IoT and Energy Saving Growing data traffic
    and boundless internet users are making the data centers exhausted. Figure 15
    portrays the approximate growth in data transmitted or received over the internet,
    energy consumption due to sensing, processing, and storing, and overburden on
    data centers due to increasing data traffic and predicted increasing number of
    connections from 2000 to 2021. Data traffic over the internet from 2010 to 2015
    ranges around 100 exabyte in 30 days (Eb/M) and can extend to 519 Eb/M with a
    growth rate of \\(31.58\\%\\) annually [20]. IoT components with exaggerated data
    traffic, data processing, and storage needs generate massive GHGs in the environment
    directly or indirectly. Worldwide internet traffic has risen 3 times from 2015
    to 2019 and can further raise in the current year, according to CISCO. Energy
    consumed from IoT components releases harmful \\(\\text{CO}_{2}\\) flinching from
    35.6 to 43.2 billion metric tons in 2040 [21]. Carbon footprint increased by 5
    times in the year 2020 due to the increasing use of servers in the IP-stimulated
    society [22]. Ample amount of data processing in the IoT network results in inflated
    power consumption and harms health and the environment due to the emission of
    GHGs as per the CISCO 2020 report. Reduction of GHGs emissions, renewable energy
    resources like solar energy usage, and recycling generated waste is the new criterion
    for the organizations. All IoT devices run on batteries, therefore conserving
    energy during data transmission is vital to increase the network’s operational
    time. Energy-hungry IoT devices can be categorized into three components [23]:
    (i) Power consumed by processor components. (ii) Power consumed during data collection
    and switching devices (ON/OFF) etc. (iii) Power consumed by communication protocols
    to enable communication. Sum total of power consumption can be expressed as [24]:
    $$\\begin{aligned} CP = TP + RP + SP + AP+ FP --GP \\end{aligned}$$ (1) where
    CP is consumed power, TP is Transmission Power consumption, RP is Data receiving
    Power consumption, SP is energy spent during data acquisition, AP is Power consumed
    during switching ON/OFF a device, FP is Fixed Power consumed from IoT devices
    and GP is Gain of Power from energy harvesting methods. The power requirement
    can be measured using an external measuring tool to get an overall idea of power
    consumption ignoring power consumption from individual components. Network simulators
    can be used to evaluate the power requirement used in modeling-based methods [25,
    26]. Figure 16 shows different applications of GIoT in PA to minimize cost and
    lessen power consumption while increasing automation and seamless connectivity
    of GIoT-based PA applications. Fig. 16 Applications of GIoT in PA Full size image
    Unlike traditional IoT, GIoT is an environment-friendly alternative that minimizes
    the adverse environmental impacts of computer hardware development and deconstruction
    while ensuring long-term environmental protection from hazardous GHG emissions.
    All the devices, communication networks, sensor systems, clouds, and the web collaborates
    to improve energy efficiency. Better allocation of resources while ensuring high
    performance, recyclable component designs, low latency and cost-effective data
    communication with extended device lifetime, optimal power saving through the
    usage of renewable energy resources and energy harvesting are all the advantages
    of GIoT over traditional IoT. 2.3 Significance of the Review The major significance
    of this review work is enumerated as follows: (i) The ecological impacts of IoT
    components and requirements of energy saving are explained. (ii) Green solutions
    in the form of GIoT enablers for minimizing power consumption of different power-hungry
    IoT components are evaluated in detail. (iii) Implementation concerns of GIoT,
    merits, and demerits of different energy-saving approaches adopted in GIoT are
    discussed in detail for realizing the true potential of GIoT in the implementation
    of PA applications while identifying major roadblocks. (iv) Data processing techniques
    and different energy management strategies are reviewed in detail to understand
    the need for the development of a GIoT framework suitable for energy-aware PA
    applications. (v) A GIoT framework based on GIoT enablers is conceptualized for
    designing cost-effective, energy-aware, and environment-friendly PA applications
    leveraging UAVs, LPWANs, and 5G networks. (vi) Layer-specific security issues
    of GIoT systems and the solutions to mitigate the vulnerability of security attacks
    are described for ensuring the security of GIoT systems. A comparative study of
    the existing work by different authors has been summarized in Table 2 by considering
    the following 12 criteria. 1: Ecological Impacts of IoT and energy saving; 2:
    IoT enablers and associated challenges; 3: Green solutions for IoT Enablers; 4:
    GIoT implementation concerns; 5: ML and Energy Management Strategies; 6: Cloud
    Computing and Energy Management Strategies; 7: Edge Computing and Energy Management
    Strategies; 8: GIoT solutions leveraging UAVs; 9: GIoT solutions leveraging LPWANs;
    10: GIoT solutions leveraging 5G networks; 11: Energy Management in 5G IoT; 12:
    GIoT implementation in Beyond 5G networks; 13: GIoT Framework for Energy-Aware
    PA applications; 14: Security Issues in GIoT and solutions. Here, “Yes” indicates
    that the topic has been covered in detail, “P” indicates that the topic has been
    partially covered, and “No” indicates that the topic has not been covered. From
    Table 2, it can be seen that the current review has addressed all the aspects
    of GIoT-based PA implementation concerns that are not addressed or partially addressed
    in the previous works. Table 2 Related Survey of GIoT based PA Full size table
    3 IoT Enablers and Associated Challenges The enabling technologies for IoT include
    RFID tags, cloud computing, internet, sensing networks, Machine to Machine (M2M),
    data centers and Communication Networks [6]. RFID tags minimize energy usage and
    radio costs during transmission. It uses tags for electronically marking goods
    in response to reader requests. RFID tags can be active and passive [6]. The active
    tags are fitted with a battery that boosts signal transmission. Passive tags use
    the induction principle that does not require batteries which makes the environment
    free from harmful toxic emissions from batteries. Hence, using passive tags is
    a biodegradable alternative as it generates renewable waste. In RFID tags, issues
    like overhearing and tag collisions must also be addressed very carefully [6,
    44]. Green Sensors offers a variety of low-power storage sensor nodes. It also
    aimed to improve energy efficiency, lengthen network life, eliminate relay nodes,
    and make the system cost-effective by efficiently employing resources. Persistent
    fulfillment of energy requirements for energy-constrained battery-operated IoT
    equipment is attainable by applying energy harvesting approaches using renewable
    energy sources. The power-hungry components of Datacenters (DC) are shown in Fig.
    17. Fig. 17 Power hungry components of datacenters [45] Full size image 4 Green
    Solutions for IoT Technology Enablers IoT components produce a significant amount
    of carbon footprint due to the use of scarce raw materials and their energy requirement
    in manufacturing, operating, and recycling processes. To tackle these issues,
    the Green IoT (G-IoT) paradigm comprising of designing green components has emerged
    as a research area of interest to mitigate such carbon footprint generation from
    IoT components. Different green solutions for IoT Technology Enablers are summarized
    here: 4.1 Green Wireless Sensor Networks To save energy from wireless sensors
    used for agriculture monitoring, specific sensing or peer-powering sensors should
    be used. Such techniques of efficient communication route forwarding between sensors
    and cloud systems recorded at the base station should be followed by sleep scheduling
    and compressed sensing. An effective load-balancing algorithm can also be used
    at the gateway nodes. Wireless sensors can also use resource allocator factor’s
    computation as a means of green wireless sensor networks (GWSN) [41]. Usually,
    sleep mode is activated for sensors to save energy, and greater power savings
    can only be attained by activating the WSNs only when data transmission is required
    [6]. 4.2 Green M2M Communication To save energy in M2M communication (GM2M), the
    transmission power can be intelligently adjusted, effective communication protocols
    can be designed, activity scheduling, collaborative energy-saving techniques,
    and energy harvesting (EH) must be used [41]. Energy Harvesting techniques may
    be used to capture energy directly from the environment like solar energy and
    wind energy etc. [46, 47]. 4.3 Green Radio Frequency Identification Tags RFID
    tags can be made greener by lowering the size and applying non-recyclable material
    during production [6]. Passive RFID tags that do not need batteries can be a promising
    green solution [44]. Techniques such as data aggregation, network coding, adaptive
    sampling, compression, routing with energy as a routing metric, and multipath
    routing are all green solutions to maximize energy saving in a GIoT environment
    [48, 41]. 4.4 Green Cloud Computing Green Cloud Computing (GCC) helps in both
    energy and cost reduction in the cloud by efficient allocation of cost-effective
    resources. Energy management strategies of green cloud such as Virtual Machine
    (VM) can be deployed to save energy by consolidation, relocation, placement, and
    allocation [49]. Energy-aware techniques such as renewable energy sources must
    be used to store data that can be easily replenished in nature which are cost-effective
    solutions [45]. Virtual machine (VM) migration and adaptive sampling are the communication
    techniques that can be used in green cloud [41, 44, 50]. 4.5 Green Data Centers
    Dynamic power allocation like dynamic voltage and frequency scaling, vary-on/vary-off
    are all useful for making Green Data Centers (GDC). Greening the IoT entails lowering
    energy while increasing the efficiency of automated connectivity among devices
    [6]. Duty cycling, compressive sensing, and cooperative relaying are all solutions
    that can be utilized to extend the battery life of data storage units [44]. The
    data reduction solution is concerned with lowering communication delays by aggregating
    data to reduce average data transfer rates and improve QoS [15]. Routing approaches
    aim to improve a network’s ability to handle data traffic by carefully selecting
    cluster heads throughout the data transfer process [15, 41]. 4.6 Green Internet
    and Communication Technology For energy saving in internet and communication technology
    (GICT) only the necessary data needs to be transmitted, the wireless data path’s
    overall length must be reduced and renewable green energy sources instead of non-renewable
    energy sources must be encouraged [41]. Figure 18 portrays a distinctive manifestation
    of green designs suggested for a paradigm shift from IoT to GIoT to ensure QoS
    in terms of saving energy, and cost, lowering latency in communication, prolonging
    the lifetime of IoT devices, and reducing GHG emissions to support green transmission
    and communication. Fig. 18 Paradigm shift from IoT to GIoT to adopt green designs
    for PA Full size image Several energy-saving techniques are adopted to implement
    GIoT. Wake-up radio is a scheme to reduce the idle listening period of sensors,
    which incurs extra power consumption for IoT-based data transmission components.
    The radio frequency (RF) energy harvesting mechanism keeps the wake-up radio in
    a deep sleep state which becomes active only after receiving an external RF signal
    to wake up the radio. This reduces the hardware requirement and signal processing
    to execute idle listening resulting in higher energy efficiency [51]. Duty cycling
    is another technique to reduce energy consumption which keeps the node in a sleep
    state most of the time and wakes up only at specific moments to increase the lifespan
    of nodes up to 5–10 years. The duty cycling technique is a trade-off between energy
    consumption and delays in data delivery to the target node. To overcome this delay
    problem, an additional wake-up radio channel can be used [52]. The energy consumption
    due to the sensor-generated data transmission is more than local data processing
    within the sensor node. The data aggregation mechanism conserves energy by eliminating
    redundant data transmission in dense sensors. Data aggregation is a process by
    which multiple similar packets are converted into a single packet. Several techniques
    like probability-based routing, mobility-based routing, cluster head communication,
    etc. can be used to improve latency, transmission power requirement, mobility,
    and network lifetime of sensors and other power-hungry data transmission components
    [53]. Since the number of sensors deployed to monitor an agriculture field is
    increasing, the need for new energy-saving mechanisms to reduce power consumption,
    cost, transmission delay, and network traffic is increasing. Compressed sensing
    is such an energy-saving mechanism to reduce power consumption, cost, transmission
    delay, and network traffic which shows that sparse signals and information in
    sensors can be exactly regenerated from very few random linear measurements. Figure
    19 describes the pros and cons of different energy-saving techniques adopted for
    the implementation of GIoT-based PA. Fig. 19 Pros and cons of different energy
    saving techniques adopted for GIoT based PA Full size image 4.7 Implementation
    Concerns of GIoT Based PA There are six primary implementation concerns to be
    resolved when creating a GIoT-based PA solution viz., hardware, data analytics,
    maintenance, mobility, infrastructure, data security, and privacy [35]. These
    implementation concerns can be explained as follows: (i) Hardware concerns: Different
    types of sensors can be used in IoT-based PA applications. The selection of sensors
    and meters for IoT devices to be used in PA is a hardware concern. (ii) Data analytics
    concerns: It mainly concerns the application of predictive algorithms and machine
    learning or deep learning techniques in IoT-based sensor-generated data to obtain
    a nutritive solution for smart PA. (iii) Maintenance concerns: The primary concern
    is to utilize different regular sensors that monitors all IoT interconnected devices
    as these devices can be easily damaged in the agriculture field. (iv) Mobility
    concerns: The primary concern is the type of wireless communication link like
    4G, 5G, 6LowPan and LoRa used to establish the connection between different sensors
    that are distributed over a large agricultural area. (v) Infrastructure concerns:
    The primary concern is the installation and development of IoT networking architecture
    using technologies like fog computing, cloud computing, network virtualization,
    etc. (vi) Data security and privacy concerns: Adaption of GIoT-based agriculture
    may make the system more susceptible to security attack by an intruder raising
    new security concerns which necessitate more secure communications to be established
    in the smart agriculture filed. 5 Data Processing Techniques and Energy Management
    Because IoT devices lack the computational power to analyze the acquired data,
    ML can be used for extracting meaningful information insights to generate intelligent
    predictive decisions based on acquired sensor data. Cloud computing appears as
    a promising paradigm to deliver high-performance computing and large storage capacity
    to satisfy the pervasive requirement for data processing in IoT networks. Cloud
    computing allows battery-powered IoT devices to offload various functions (such
    as data cleansing, and redundancy elimination) to the cloud. Edge computing allows
    the workload to be offloaded from the cloud to a location closer to the data source
    at the edge of the network to be efficiently handled by the users. Machine Learning,
    Cloud, and Edge computing are summarized in this section responsible for energy-intensive
    data processing along with different energy management techniques. 5.1 Machine
    Learning and Energy Management Following are some pertinent research works in
    the field of PA using ML algorithms such as fertilizer prediction [54], weather
    monitoring [55,56,57], soil moisture monitoring [58, 59], soil moisture monitoring
    with blockchain-based security [60]. To adjust irrigation schedules in response
    to changing weather conditions, ML process a vast amount of weather data [55,
    57]. Fuzzy logic is also used for mathematical modeling of unpredictably varying
    weather parameters. Long periods of data surveillance and monitoring are required
    to assess the accuracy of fuzzy logic. For precision irrigation, constant updating
    of irrigation schedules is accomplished by comparing ideal weather patterns with
    changing weather patterns. Hard coding is avoided with ML, and continual updating
    is achievable by learning from past input data [13]. There has been a substantial
    amount of research on the use of ML to attain energy savings. Artificial intelligence
    (AI) is identified as a possible driver in the optimization of energy resources
    and ML is accelerating the growth of IoT interconnected devices [61]. Localized
    AI in edge computing will filter data so that only relevant data is transmitted
    to the cloud. As a result, there are significant bandwidth and data transmission
    cost savings [62]. ML models help CPUs of edge devices run more quickly and efficiently
    at higher speeds and lower power levels. Video games, smart speakers, drones,
    security cameras, wearable health monitors, etc. are some examples of edge AI-enabled
    technology [7]. A cloud-based data center with real-time power management and
    resource allocation is proposed in [63]. The power requirements of cooling systems
    and data centers are estimated. A community and cloud-based data center powered
    by green energy results in saving 15.09 % on energy costs. On a 4G and 5G network,
    the proposed cloud-based approach creates a communication link with power users.
    Energy-efficient unicast (EEU) and reliable unicast (RU) efficient routing algorithms
    between communicating nodes, as well as an energy model of each node energy consumption,
    were suggested in [64]. The amount of energy used at nodes depends on the software
    operations involved in message transmission, reception, and the number of transmission
    attempts made. The Ad-hoc On-Demand Distance Vector protocol is used to find the
    shortest path between the communication nodes, and the RU protocol is used to
    calculate the likelihood that messages will be delivered successfully over that
    path. The EEU protocol identifies a path between the source and the destination
    that uses the least amount of energy at the nodes along the way. The EEU protocol
    consumes the least amount of energy possible, whereas the RU protocol increases
    the likelihood of delivery. The topic-based data transfer protocol using the mobile
    publish-subscribe Fog computing model is proposed in [65]. An event-driven, context-aware
    distribution system is utilized to decrease the amount of data that is transmitted
    between IoT networks and servers. It can be seen that the protocol’s data traffic
    has been considerably lowered, and its delivery ratio has been considerably decreased.
    ML may be utilized in energy-saving IoT applications like smart energy management
    [66] and smart water management [67], to assist in minimizing energy use. Other
    research consolidates on economizing energy by transferring data more efficiently
    and lessening the number of transmitted packets [68]. In [69], the focus is on
    utilizing ML to achieve energy-efficient mobility. ML techniques are revealed
    to be an intriguing option that can be used in different aspects of energy conservation
    in IoT networks if the data attributes are known. To deal with energy supply variability
    and intermittency, a deep reinforcement learning-based energy scheduling was presented
    in [70]. With the incorporation of ML algorithms to automate complex tasks and
    increase the overall system level of intelligence, UAV-based network performance
    can be significantly improved. Some of the existing works where ML algorithms
    are employed in conjunction with UAVs are summarized here. By reducing UAV’s power
    consumption, authors of [71] determine the best position to deploy airborne base
    stations to offload terrestrial base stations. UAVs are placed momentarily by
    anticipating wireless network congestion based on ML rather than being needed
    to continuously shift their placements. Through a structured radio map in [72]
    the optimal allocation for UAVs operating as airborne base stations are determined.
    The authors suggest a joint clustering and regression problem using a maximum
    likelihood strategy which is created based on the K-segment ray-tracing model
    because of the diverse terrain and difficulties in utilizing such radio maps.
    To recreate the radio map, ML is employed to predict the channel. In [73] the
    best allocation for airborne base stations utilizing UAVs is determined. Traffic
    is downloaded using weighted expectation maximization utilizing ML algorithms.
    Contract theory is also used to verify that the downlink demand is fulfilled through
    the proper selection of UAVs for each hotspot. To enable motion prediction for
    a collection of heterogeneous flying UAVs, an unsupervised method has been developed
    in [74]. The algorithm is intended to categorize the network nodes according to
    their mobility attributes in addition to forecasting the future locations of the
    UAVs. According to [75], by predicting the UAV’s location based on its previous
    locations, the communication efficiency between a UAV and a base station can be
    increased. The problem is that a UAV can experience wind perturbation when offloading
    a terrestrial base station, which might cause some offset and hence a reduction
    in capacity. The authors suggest a Recurrent Neural Network-assisted architecture
    to address this problem, in which the upcoming elevation and horizontal angles
    of the UAV concerning the base station are forecasted based on past angles. So,
    it is possible to forecast the precise location of a UAV which is moving at high
    speed. ML has been successfully integrated with UAV for energy savings [42]. 5.2
    Cloud Computing and Energy Management Although it may be advantageous to use this
    technology, it is widely known that cloud servers, which host cloud applications,
    consume a lot of energy. As a result, it is vital to limit energy exhaustion in
    cloud-based data processing by effective utilization of its resources. The authors
    of [76] presented a method for dividing an application into off-loadable and non-off-loadable
    components to reduce energy usage and execution time. This approach necessitates
    a high level of network dependability to assure the application’s functionality
    and availability. The authors developed an ant system for dynamic virtual machine
    placement in [77], which minimizes the number of active physical machines while
    also lowering the data center’s overall energy usage. Virtual machines can be
    transferred from one physical machine to another using dynamic virtual machine
    placement. As a result, security techniques should be used during relocation to
    secure sensitive data from attacks by intruders. A virtual machine placement method
    in [78] is proposed to reduce the number of active physical machines and their
    energy usage while also balancing the load amongst them. This method, however,
    uses a static virtual machine placement, which implies that once a virtual machine
    is created, it cannot be moved to another real computer. Energy-aware routing
    algorithms help to improve the network lifetime of sensor nodes as well as QoS
    even in the uneven distribution of traffic load. Co-operative relay solves the
    problem of high spectrum needs, reducing response time and allowing devices to
    consume less energy during transmission. Fig. 20 Energy management in cloud, fog
    and edge computing Full size image 5.3 Edge Computing and Energy Management Edge
    computing solutions have emerged as an appealing solution for energy management
    in IoT because it bridges the gap between low-power devices’ limited processing
    capabilities and their computational demands. Edge computing enables the workload
    to be offloaded from the cloud to a location closer to the data source to be efficiently
    handled by the users. It allows users to download data from cloud services and
    upload data to IoT devices. As a result, it can lengthen the battery life of battery-powered
    IoT devices, reducing network traffic, enhancing bandwidth, improving privacy,
    and saving significant amounts of energy and communication time. In [79], focus
    is on compressing data acquired from the edge devices before transmission. The
    proposed approach is based on an error-bounded lossy compressor that is optimized
    for high-data production scenarios where the retrieved data generates the same
    classification accuracy as the compressed data. In [80], a reinforcement-learning-based
    offloading technique is suggested for IoT devices that harvest energy. According
    to their current battery level, the past radio transmission rate of each edge
    device, and the projected amount of collected energy, IoT devices choose an edge
    device and their offloading rate. A deep reinforcement-based offloading method
    is also included in this system to help speed up learning. Fog computing is a
    layer between the cloud and the edge. Where edge computing might send large streams
    of data directly to the cloud, fog computing can receive the data from the edge
    layer before it reaches the cloud and then selectively send relevant data. A fog
    environment places intelligence at the local area network (LAN) and transmits
    data from endpoints to the gateway, where it is then transmitted to sources for
    processing and return transmission. Figure 20 shows different energy management
    techniques in cloud, fog, and edge computing. Some other recent energy management
    techniques adopted using Machine learning, Cloud and Edge Computing are discussed
    in Table 3. Table 3 Other recent energy management techniques adopted using Machine
    learning, Cloud and Edge Computing Full size table 6 GIoT Strategies Leveraging
    Unmanned Aerial Vehicles (UAVs), Low Power Wide Area Networks (LPWANs) and 5G
    Networks The adoption of Unmanned Aerial Vehicles (UAVs) has revolutionized Agriculture
    4.0 by offering farmers major cost savings, reliable connectivity, enhanced efficiency,
    and increased profitability. In [90], UAV is regarded as a sink node in wireless
    communication to collect sensor data. In [91], low-power processing of data is
    performed using UAV. In [92], a Genetic Algorithm (GA) is applied to improve drone-assisted
    networks analyzing energy consumption and sensor density. Sensor nodes suffer
    from limited range, downtime battery constraints, communication range, storing,
    processing, and computing resources [93]. So the integration of UAVs and WSNs
    is required for energy-efficient data acquisition. Recent UAV-based energy-efficient
    approaches for greening IoT are described in Table 4. Figure 21 describes the
    reputed journal distribution of articles using UAVs for green PA. Fig. 21 Reputed
    journal distribution of articles using UAV for green PA Full size image Table
    4 Recent UAV based energy-efficient approaches to GIoT Full size table 6.1 GIoT
    Strategies Using UAVs and RFID for Greening of Things Diminishing tag size followed
    by energy-saving strategies are required to optimize the energy requirement of
    RFID [98]. The RFID and UAV can be combinedly employed as a powerful tools required
    to deliver service in large deployment use cases including PA applications. In
    [99] UAVs with indoor localization are done with RFID to achieve cost efficiency
    using radio-frequency shadowing. Hubbard et al. [100] has shown the enhancement
    of UAV’s battery lifetime. In [101], a feasibility analysis is done to recharge
    the multipurpose tags in UAVs for energy requirement optimization. A few recent
    works are listed in Table 5. Table 5 UAV techniques to green RFID Full size table
    6.1.1 UAV and WSN for Greening of Things In [104] \\(\\text{CO}_{2}\\) concentration
    is monitored during data collection by UAV and WSN combinedly. The authors of
    [105] utilize Particle Swarm Optimization (PSO) for analyzing data. UAV and WSN
    are combinedly and used for data gathering with low power [106]. UAVs are capable
    of managing duty cycling dynamically. Advantages include enhanced communication
    bandwidth, the energy required for data transmission, and an extended coverage
    area. The cooperation of WSN, UAVs, and RFID helps in energy saving, and local
    and cost-effective processing using clustering techniques and has the potential
    of making PA applications greener as shown in Fig. 22. Recent works in the above
    context are discussed in Table 6. Table 6 UAV techniques to green WSN Full size
    table Fig. 22 Energy saving of WSN and RFID using UAV Full size image 6.1.2 Review
    of UAV Applications and Key Innovations in PA Over the last few years, UAVs played
    a very important role in agriculture monitoring. It has widespread applications
    in PA including 3D crop modeling, weed management, crop yield enhancement, vegetation
    index extraction, field-level phenotyping, etc. Some of the contributions in PA
    utilizing green technology of UAV are summarized in Table 7. Table 7 Contributions
    in PA utilizing green technology of UAV Full size table Applications of UAVs in
    PA are seeing an emerging trend. Recently, UAVs in the field of PA are applied
    for soil Mapping, Geographical Information Systems, etc. Different types of UAVs
    based on technical specifications and payload are also discussed [117]. UAVs integrated
    with sensors for crop monitoring and UAV trajectories are also used for data acquisition.
    Consensus and symbolic aggregate approximation algorithms are used at the network
    level for data transmission [118]. UAV in PA is discussed in diverse fields of
    irrigation, fertilization, chemical applications, weed management, and disease
    detection [119] etc. 6.1.3 Energy Constraints in UAV and Solutions Battery constraints
    of UAVs having limited energy, the energy consumption of UAVs during data collection
    is a matter of concern. Placement of UAVs, limited flight time, path planning,
    and interference among communicating UAVs are some of the constraints in using
    UAVs [4]. Energy Harvesting (EH) is a potential direction to augment battery capacity.
    Some of the energy constraint issues and solutions of UAVs are pointed out in
    Table 8. Table 8 Energy constraints in UAV and solutions Full size table Fig.
    23 Paper distribution of LPWAN Full size image 6.2 LPWANs for GIoT Based PA Sensors
    have a limited amount of resources like memory, power, and computing [15]. LPWANs
    are designed to support GIoT applications with low power requirements while retaining
    service quality and providing long-range communications. Figure 23 shows the paper
    distribution of LPWAN technologies. Long-range communication is supported by LPWANs
    having fewer gateways and long battery life [10, 11]. Long-term monitoring is
    possible with LPWANs because it allows sensors to monitor data and then transmit
    those monitored data with minimal power consumption. LPWANs are well-suited for
    deployment in GIoT-based PA applications as these are cost-effective and energy-efficient.
    PA applications where on several occasions, it relies on uplink communication
    for long-range transmission with minimal energy usage, LoRa becomes a feasible
    choice in such a scenario. DASH7 is less preferred in PA applications where the
    communication range exceeds 2 km [10]. Sigfox and LoRa are better than NB (NarrowBand)-IoT,
    when battery life, power, and cost incurred, are considered. NB-IoT is less ideal
    in PA tasks as many farms lack Long-Term Evolution (LTE) cellular coverage which
    is expensive due to weak cellular signal [10]. NB-IoT has a poor level of interference
    immunity [11]. Table 9 shows the specification of different LPWAN techniques.
    Table 9 Specification of LPWANs Full size table It has been found that a LoRaWAN
    communication technology expands the opportunity to install a large and sophisticated
    wireless sensor network in a remote agricultural land of 10 km without being dependent
    on an LTE (4G/5G) or other backhaul networks, by increasing the communication
    range of an IoT network to over 10 km wirelessly [121]. In [122] NB-IoT is evaluated
    in the agricultural environment for field experiments. A Raspberry-based NB-IoT
    node is implemented in terrace, ground, underground and cellar conditions. In
    [123] low-cost sensor nodes are designed for Agro-intelligence IoT using LoRaWAN
    for smart farming applications. In [124] self-powered nodes are used for real-time
    soil health monitoring using LoRaWAN for effective data storage and analysis.
    In [125] a low-cost design is implemented for automated agricultural irrigation
    using both LoRa and Sigfox for the implementation of a cloud-based WSN. In [126]
    a low-cost WSN is implemented for real-time intruder detection and crop data acquisition.
    EnOcean sensors operating in energy harvesting mode and Sigfox-based data transmission
    are used for transmitting real-time agricultural data in the cloud. 6.3 Utility
    of 5G in GIoT and PA Applications To satisfy global goals of designing energy-efficient
    IoT devices, advancing towards 5G cellular systems is a solution for ubiquitous
    connectivity. 5G networks are a highly energy-saving alternative to existing communication
    networks. In [127], a frame structure is proposed for physical layer radio frame
    flexibly to specifically target IoT implementation for \\(5\\text{th}\\) generation
    (5G) networks choosing appropriate Guard Bands for Random Access Channels. It
    supports enormous connections along with bursty and small packet transmissions
    while meeting low-cost, low power, and low complexity operation requirements of
    GIoT. The proposed design is effective for handling transceiver distortions, cell
    implementation, inter-band, and inter-cell interference shown through simulation
    results. 5G can accommodate different transmission speeds, bandwidths, and service
    quality needs. It consists of massive IoT connectivity, improved mobile broadband,
    and vital communications. The 3GPP has improved the design of 5G networks in a
    variety of ways. Reduced transmission delays will result from faster response
    times and improved reliability. Low latency connectivity is essential for communicating
    devices in PA that performs critical tasks based on real-time sensor-generated
    data. For efficient PA applications, a private network can be used that consists
    of independent spectrum options including dedicated, shared, and unlicensed frequency
    bands to support diverse spectrum requirements of IoT applications that allow
    the saving of network resources by allowing network slices assigned for specific
    functions. Network slicing also assures higher security and reliability of data
    transmission. Low latency, high bandwidth connections support of 5G networks results
    in QoS improvement during real-time transmission of a massive volume of data [128,
    129]. Also, the cost per bit of data transmission is reduced to enhance the economic
    feasibility of the 5G network.5G can be used to provide home access to the internet
    using wireless technologies instead of depending on fixed lines as broadband is
    not available through fixed-line operators, especially in remote and rural areas.
    QoS benefits include data communication speeds with reduced cost per bit of data
    transmitted using broadband. 5G networks connect households in rural areas with
    add-on advantages of capacity bandwidth and higher speeds to support massive IoT
    interconnections. Energy harvesting and power management of different power-hungry
    IoT devices in the 5G era need special attention. In [29] UAV-based 5G design
    is implemented using ML algorithms that improve service delivery using multiple
    antennas. In [28] data aggregation in UAV networks is done to reduce the energy
    requirement of power-hungry IoT components. In [130] reliable link selection is
    done to transmit information with reduced co-channel interference, increasing
    Signal to Noise for energy efficiency. In [131] a clustering algorithm is proposed
    where a cluster is formed based on the signal strength received to minimize energy
    consumption. In [132] to overcome the underutilized spectrum and constrained battery
    capacity in GIoT devices, energy harvesting for judicious use of resources is
    utilized. In [18] energy management is done by reducing sensor data transmission
    with increased fault tolerance, reduced energy consumption, and improved device
    failure rate. In [133] adaptive network coding is used to boost transmission efficiency.
    In [38] Three-tier architecture is proposed to reduce energy requirements and
    signaling overhead. The 5G network’s IoT-based solution allows the automated operation
    of various unmanned agricultural devices during plowing, planting, and all the
    growth stages of the crop, resulting in environment-friendly and energy-efficient
    farming operations. Data analytics in PA applications consists of a set of methodologies
    that focuses on gaining actionable insights from a large volume of data [128].
    So, it has a high bandwidth requirement, minimal delay, and maximized throughput.
    Figure 24 shows the benefits of adopting 5G networks in PA for shifting towards
    Agriculture 5.0. In PA, the adoption of 5G networks provides energy efficiency
    and ultralow latency to maximize throughput and reduce downtime of communicating
    devices for gaining higher reliability in data transmission. The data transmission
    speed can be increased during Interactive Augmented and Virtual Reality applications
    with reduced delay from multiple data transmission sources using virtual consultation
    and predictive maintenance services. AI-driven robots can also be utilized to
    achieve data transmission with reduced delay. For real-time monitoring, Machine-type
    communications with long-term evolution and narrowband IoT are paving the way
    for 5G integration in the future. It is used to connect numerous sensors to a
    single cellular tower, allowing farmers to set up more IoT devices to perform
    multiple tasks proficiently. Wearable glasses and smartphones can provide useful
    information about crop, animal, and machinery statistics, weather updates, AI-powered
    disease identification for both plants and cattle, insect exposure, land assessment,
    and so on to implement Augmented Reality and Virtual Reality [134]. In [135] AI-powered
    robots are programmed for using Global Positioning System to plot a route and
    recognize fruits and vegetables that are ripe to harvest. Fig. 24 Adoption of
    5G networks for shifting towards Agriculture 5.0 Full size image 6.3.1 Energy
    Management in 5G IoT Several studies in the literature have emphasized the necessity
    of energy efficiency in 5G networks. In [136] a comprehensive survey is done on
    energy-efficient cellular networks and a spectrum-sharing solution is designed
    for IoT networks with long battery life. In [137] an overview of energy-efficient
    strategies for 5G networks is delivered including resource allocation, network
    planning, network implementation, and hardware solutions. An integrating and energy-efficient
    system model for 5G-IoT is proposed in [138] that utilizes a massive Multiple-Input
    and Multiple-Output (MIMO) scheme to substitute the single remote antenna and
    the Cellular Partition Zooming (CPZ) mechanism for reducing the distance between
    the communicating components and the number of routing devices. 6.4 Implementation
    of GIoT in Beyond 5G Networks Beyond 5G/6G Networks are already seeing an emerging
    trend to make IoT designs greener with low power consumption, and energy harvesting
    strategies. Phase shifting the reflector and using iterative beamforming to lower
    the access point transmission power is a new Intelligent reflector surface and
    Ambient Backscatter communication joint design approach stated in [139]. The NOMA
    strategy is used in conjunction with simultaneous wireless information and power
    transmission technique that allow relays to harvest energy from radio signals
    while also enhancing the sum rate of each IoT device as stated in [140]. For 6G-enabled
    IoT devices, in [36] a cluster-based green communication strategy is proposed.
    A hybrid whale-spotted hyena optimization algorithm is used to guarantee the energy-efficient
    operation of IoT networks which limits the use of a single device as a cluster
    head, decreases the complexity of cluster selection, and prolongs network life.
    Table 10 describes other recent GIoT strategies adopted leveraging UAV, LPWANs
    and 5G networks. Fig. 25 Proposed GIoT layered framework for energy-aware PA Full
    size image Table 10 Other recent GIoT strategies adopted leveraging UAV, LPWANs
    and 5G networks in GIoT Full size table 7 Proposed GIoT Framework for Energy-Aware
    PA Applications To reduce the environmental impacts of hazardous emissions from
    IoT components, a low-cost, low-energy-consuming framework of GIoT for energy-aware
    PA applications is proposed using ML for intelligent decision-making, as shown
    in Fig. 25. To address power consumption, operational cost management, and minimizing
    environmental degradation from millions of interconnected IoT devices, the suggested
    conceptual framework is divided into five layers which are described as follows:
    The green perception layer does self-organized sensing and load balancing where
    parameters are extracted from weather, soil, crops, etc. The sensor operates using
    dynamic sleep/wake-up, energy harvesting to extract power from renewable sources,
    radio optimization, and energy-efficient routing to minimize energy usage [41,
    44]. The green transport layer is used for data processing. Passive RFID Tags
    that are biodegradable must be used in the green transport layer to optimize tag
    estimation adjust dynamic power levels and prevent tag collision as well as overhearing
    problems [6, 44]. Dynamic power adjustment can be attained using green M2M communication
    and energy harvesting is required for dynamic resource allocation [44, 41, 45].
    The green processing layer is responsible for evaluating, processing, and allocating
    resources for data storage [44]. In the green processing layer, processing and
    analysis of data can be done using ML techniques for predictive decisions and
    energy saving can also be achieved using ML solutions. Incorporating ML at the
    network’s edge can be used to achieve declined communication latency, enhanced
    security, and energy efficiency [150]. Edge computing in PA helps to reduce bandwidth
    and optimize latency requirements by sending data to the network’s edge to reduce
    network overloading while retaining flexibility and QoS enhancements [151]. In
    addition, the edge computing nodes provide adequate computational capability and
    low data transmission time. After these data have been processed in the green
    processing layer, these data are communicated to devices at various levels as
    part of a full system model in the green network layer. GCC using virtual machine
    (VM) techniques such as VM consolidation, placement, allocation, and migration
    can be applied in PA to ensure energy efficiency [44, 47]. GDC uses dynamic power
    allocation and energy-aware routing strategies in PA [44]. GICT uses compressive
    sensing, data fusion, cooperative relaying, dynamic topology management as well
    as duty cycling as an energy-efficient metric to extend the battery life of IoT
    components [44, 152]. In the green application layer, data can be obtained after
    being processed in all the layers in the form of readymade applications in PA.
    Table 11 shows the layered architecture of GIoT components to furnish green alternatives
    for power-hungry IoT enablers. While wake-up radio is a non-demand ON/OFF scheduling
    scheme, duty cycling can be on-demand and asynchronous. As a result, wake-up radio
    systems can help to limit the amount of idle hearing and overhearing that duty-cycling
    can cause. But, wake-up radio systems have a restricted communication range and
    incur a high cost as well as there is a possibility for severe interference. Duty-cycling
    can also suffer from high sleep latency and data transmission latency, in addition
    to idle hearing and overhearing. Energy-efficient data collection systems are
    effective methods for lowering IoT energy consumption. Security and latency tribulations
    for data aggregation as well as the barely obtainable data sparsity in data compression
    are still issues that are required to be addressed. Here, MQTT and CoAP stand
    for Message Queuing Telemetry Transport and Constrained Application protocol.
    Table 11 Layered architecture of GIoT components Full size table 8 Security Issues
    in GIoT and Solutions Virtualization of workstations and data abstraction from
    a common physical server is used to build cloud-based IoT services. As a result,
    there is a risk of data theft when a VM communicates with its neighbors. SQL injection,
    malware injection, and man-in-the-middle attacks are all the vulnerable security
    threats to which cloud data are exposed [154]. Furthermore, Blockchain technology
    has recently emerged as a means of connecting billions of devices on IoT networks.
    Blockchain creates a secure mesh network that allows devices to communicate reliably,
    avoiding the risks associated with centralized servers. As the network grows in
    size and the number of devices increases, traditional computing techniques have
    a direct impact on overall energy consumption. In addition to other obstacles
    such as scalability, latency, energy consumption, and flexibility issues, traditional
    computing techniques poses privacy and trust concerns. The sensors used to sense
    environmental parameters in an open agriculture field are very vulnerable to security
    threats as the sensors cannot be monitored regularly. Also, the area where sensors
    are located is not monitored properly compared to the sensors deployed inside
    a metropolitan city and it is simple to add malicious nodes that can overhear
    the information exchanged or impose security threats like [35]. Vulnerabilities
    and weaknesses in applications, interfaces, network parts, and software designs
    pose several security concerns for which user access is limited by allowing authorized
    access only [155]. To combat security risks, effective encryption techniques,
    Intrusion Detection Systems (IDS) [156] and secure authentication schemes are
    required to be implemented. As a result, it might be stated that enhancing security
    strategies are essential such as trust management, communication security, communication
    privacy, and application security. Different security attacks specific to different
    layers of GIoT-based systems are shown in Fig. 26. Fig. 26 Security attacks on
    different layers of GIoT based systems Full size image 8.1 Attacks in Device Layer
    Different security attacks in Device Layer of GIoT based systems are as follows:
    (i) Booting Attacks: The built-in security features of a device are not used during
    the booting process, the intruder tries to invade the communicating device while
    it is rebooting [157]. (ii) Eavesdropping: The intruder gets access over a conversation
    happening between unprotected devices or networks and the information is stolen
    when sent or received for malicious activities. (iii) Sleep Deprivation Attacks:
    The intruder tries to consume all the available power of IoT devices through infinite
    looping or mischievously maximizes the power consumption to reduce IoT device
    lifetime resulting in denial of services due to power exhaustion of these IoT
    devices [158]. (iv) Malicious Code Injection: The intruder injects malicious codes
    into the memory of IoT devices to achieve control over the entire system that
    causes system malfunctioning [159]. 8.2 Attacks in Communication Layer Different
    security attacks in Communication Layer of GIoT based systems are as follows:
    (i) Man-in-the-Middle Attack: The intruder secretly listens to the conversation
    between two parties and gain access over the real-time data traffic [160]. (ii)
    Routing Attack: The intruder attempts to change the transmission path of the data
    transit. A sinkhole attack is a routing attack in which the attacker broadcasts
    a fake transmission path to the nodes to re-route their traffic through it to
    cause various DoS attacks [161]. (iii) Phishing Site Attack: The intruder sends
    deceitful communications to different users, which appears to be valid messages
    and somehow the user id and password are retrieved with minimum effort to attack
    the hacked IoT devices [162]. (iv) Dos Attack: The intruder floods the network
    with unnecessary traffic to make the device fail to respond to genuine user requests.
    Due to the dissimilar configurations of the IoT devices, DoS attack occurs [163].
    8.3 Attacks in Data Analytics Layer Different security attacks in Data Analytics
    Layer of GIoT based systems are as follows: (i) Signature Wrapping attack: Here
    the signature algorithm is altered by the intruder to retrieve the resources and
    alter its information [164]. (ii) Malware Injection and Flooding: Here the attacker
    transfuse malicious code or even a virtual machine onto the cloud to gain right
    to use user’s sensitive information to degrade its quality [165]. (iii) SQL Injection
    Attack: The intruder transfuse malicious codes into the system to retrieve all
    the information about the system to achieve control [166]. 8.4 Application Layer
    Different security attacks in Application Layer of GIoT based systems are as follows:
    (i) Secure onboarding: When a new sensor is used in the network it passes the
    encryption key through the gateways which are vulnerable to various security attacks.
    Then the intruder can crack through the system using the encryption keys [167].
    (ii) Malicious Code Attack: The intruder utilizes cross-site scripting to get
    into the system which results in corrupting and malfunctioning of the entire IoT
    system [168]. (iii) Reprogram Attacks: Here the intruder can change the device
    configurations if it is not protected sufficiently and can induce very harmful
    damages [169]. (iv) Data Thefts: When data is sent over the network there are
    greater likelihoods of data theft. A single fault in the system may cause the
    failure of the entire system [170]. Physical security design, authentication,
    and access control are the security measures to be adopted in the device layer.
    Identity management, secure routing protocols, and encryption can be used in the
    communication layer as security measures. Cloud security, secure identification,
    and anti-virus are the security measures to be adopted in the data analytics layer
    as security measures. Authentication encryption, security protocols, and privacy
    management can be used in the application layer as security measures. Intrusions
    are detected and implemented using several authentication and access control mechanisms
    like encryption mechanisms to protect systems against cyber attacks. Using data
    mining and machine learning approaches, it is possible to differentiate between
    normal and malicious attacks. The intrusion detection for GIoT-based PA as a software
    application will able to reveal different security threats posed to the GIoT systems
    [35]. The resource and energy-constrained IoT devices are not always able to meet
    the computational and power requirement needed in the processing of different
    encryption/decryption techniques and cryptographic solutions at the Device layer.
    Therefore, the design of compatible cryptographic protocols is a significant research
    opportunity for the implementation of GIoT-based PA applications. 9 Conclusion
    Various Agriculture 4.0 strategies such as IoT-driven PA, UAVs, and big data analytics
    are remarkably advantageous given the growing food needs of the rising global
    population and diminishing agricultural land. With the rising number of IoT interconnections
    in PA applications, energy requirement issues of power-hungry IoT components like
    sensors, RFID tags, etc. are also increasing. Furthermore, equipped with the power
    of automation in real-time data acquisition and seamless connectivity in PA tasks,
    IoT components directly or indirectly generate Green House Gases and causes energy
    depletion. So, adopting green designs of GIoT is inevitable to promote energy-conserving,
    environment-friendly, and cost-effective IoT component designs. This work comprehensively
    reviews the most recent utilities of GIoT in PA applications such as energy-efficient
    irrigation monitoring based on commonly observed environment parameters like non-linear
    changes in soil, weather, and crop data. To resolve the energy consumption problem
    of existing IoT technology enablers, the plethora of GIoT enablers like GRFID,
    GWSN, GCN, GDC, and GCC are reviewed in detail for meeting the GIoT design goals
    of energy-efficient, environment-friendly, and cost-effective component designs.
    The overview of different data processing and energy-conserving strategies using
    machine learning, cloud computing, and edge computing is extensively reviewed.
    Further, energy-saving approaches leveraging 5G networks, UAV technologies, and
    LPWANs are presented along with their pros and cons to realize the true potential
    of GIoT. The main goal of this detailed review work is to conceptualize a GIoT
    framework for designing an energy-efficient, cost-effective, and environment-friendly
    PA application utilizing GIoT components for realizing the paradigm shift from
    IoT to GIoT while enabling ubiquitous connectivity in PA. Besides, different security
    threats in each layer of GIoT networks and the measures that can be adopted to
    mitigate those threats are also reviewed. In the future, it is important to identify
    the security loopholes in designing GIoT devices to ensure security amongst heterogeneous
    energy-efficient communicating devices. Also, to leverage the full potential of
    UAV (for UAV flying to long distances and for long duration) in GIoT-based PA
    applications, it is crucial to design an efficient power distributed algorithm
    for real-time data processing, finding suitable techniques for efficient batteries,
    increased battery lifetime and energy harvesting. Besides every other aspect,
    this paper analyzes why GIoT-based techniques need to be adopted to ensure sustainable
    progress towards Agriculture 5.0 leveraging UAVs, LPWANs, and 5G network designs
    in PA with optimized energy-saving and minimized ecological impacts of IoT components
    while ensuring QoS and cost-effectiveness. Data Availibility Data sharing not
    applicable to this manuscript as no datasets were generated or analysed during
    the current study. Code Availability No codes are made available for sharing at
    present. References Zhai, Z., Martinez, J. F., Beltran, V., & Martinez, N. L.
    (2020). Decision support systems for agriculture 4.0: Survey and challenges. Computers
    and Electronics in Agriculture, 170, 105256. Google Scholar   Shafi, M., Molisch,
    A. F., Smith, P. J., Haustein, T., Zhu, P., De Silva, P., Tufvesson, F., Benjebbour,
    A., & Wunder, G. (2017). 5G: A tutorial overview of standards, trials, challenges,
    deployment, and practice. IEEE Journal on Selected Areas in Communications, 35(6),
    1201–1221. Google Scholar   Shafique, K., Khawaja, B. A., Sabir, F., Qazi, S.,
    & Mustaqim, M. (2020). Internet of things (IoT) for next-generation smart systems:
    A review of current challenges, future trends and prospects for emerging 5G-IoT
    scenarios. IEEE Access, 8, 23022–23040. Google Scholar   Popli, S., Jha, R. K.,
    & Jain, S. (2022). Green IoT: A short survey on technical evolution and techniques.
    Wireless Personal Communications, 123(1), 525–553. Google Scholar   Zhang, L.,
    Dabipi, I. K., & BrownJr, W. L. (2018). Internet of things applications for agriculture.
    Internet of Things A to Z: Technologies and Applications, 507–528. https://doi.org/10.1002/9781119456735.ch18
    Alsamhi, S. H., Ma, O., Ansari, M. S., & Almalki, F. A. (2019). Survey on collaborative
    smart drones and internet of things for improving smartness of smart cities. IEEE
    Access, 7, 128125–128152. Google Scholar   Albreem, M. A., Sheikh, A. M., Alsharif,
    M. H., Jusoh, M., & Yasin, M. N. (2021). Green internet of things (GIoT): Applications,
    practices, awareness, and challenges. IEEE Access, 9, 38833–38858. Google Scholar   Arshad,
    R., Zahoor, S., Shah, M. A., Wahid, A., & Yu, H. (2017). Green IoT: An investigation
    on energy saving practices for 2020 and beyond. IEEE Access, 5, 15667–15681. Google
    Scholar   Hernandez-Vega, J.-I., Varela, E. R., Romero, N. H., Hernandez-Santos,
    C., Cuevas, J. L. S., & Gorham, D. G. P. (2018). Internet of things (IoT) for
    monitoring air pollutants with an unmanned aerial vehicle (UAV) in a smart city.
    In Smart Technology (pp. 108–120). Springer Ayoub, W., Samhat, A. E., Nouvel,
    F., Mroue, M., & Prevotet, J. C. (2018). Internet of mobile things: Overview of
    LoRaWAN, DASH7, and NB-IoT in LPWANS standards and supported mobility. IEEE Communications
    Surveys and Tutorials, 21(2), 1561–1581. Google Scholar   Ismail, D., Rahman,
    M., & Saifullah, A. (2018). Low-power wide-area networks: Opportunities, challenges,
    and directions. In Proceedings of the workshop program of the 19th international
    conference on distributed computing and networking (pp. 1–6). Mohamed, E. (2020).
    The relation of artificial intelligence with internet of things: A survey. Journal
    of Cybersecurity and Information Management, 1(1), 24–30. Google Scholar   Mughees,
    A., Tahir, M., Sheikh, M. A., & Ahad, A. (2020). Towards energy efficient 5G networks
    using machine learning: Taxonomy, research challenges, and future research directions.
    IEEE Access, 8, 187498–187522. Google Scholar   Nan, Y., Li, W., Bao, W., Delicato,
    F. C., Pires, P. F., Dou, Y., & Zomaya, A. Y. (2017). Adaptive energy-aware computation
    offloading for cloud of things systems. IEEE Access, 5, 23947–23957. Google Scholar   Popli,
    S., Jha, R. K., & Jain, S. (2016). A survey on energy efficient narrowband internet
    of things (NBIoT): architecture, application and challenges. IEEE Access, 7, 16739–16776.
    Google Scholar   Adam, A. H., Tamilkodi, R., & Valli, M. K. (2019). Low-cost green
    power predictive farming using IoT and cloud computing. In Proceedings of international
    conference on vision towards emerging trends in communication and networking (ViTECoN)
    (pp. 1–5). IEEE. Dhall, R., & Agrawal, H. (2018). An improved energy efficient
    duty cycling algorithm for IoT based precision agriculture. Procedia Computer
    Science, 141, 135–142. Google Scholar   Said, O., Zafer-Al, M., & Tolba, A. (2020).
    Ems: An energy management scheme for green IoT environments. IEEE Access, 8, 44983–44998.
    Google Scholar   Mekala, M. S., & Viswanathan, P. (2020). (t, n): Sensor stipulation
    with THAM index for smart agriculture decision-making IoT system. Wireless Personal
    Communications, 111(3), 1909–1940. Google Scholar   Cao, X., Song, Z., Yang, B.,
    ElMossallamy, M. A., Qian, L., & Han, Z. (2019). A distributed ambient backscatter
    mac protocol for internet-of-things networks. IEEE Internet of Things Journal,
    7(2), 1488–1501. Google Scholar   Sharma, V., You, I., & Kumar, R. (2016). Energy
    efficient data dissemination in multi-UAV coordinated wireless sensor networks.
    Mobile Information Systems, 2016, 1–13. Choi, D. H., Kim, S. H., & Sung, D. K.
    (2014). Energy-efficient maneuvering and communication of a single UAV-based relay.
    IEEE Transactions on Aerospace and Electronic Systems, 50(3), 2320–2327. Google
    Scholar   Bejiga, M. B., Zeggada, A., Nouffidj, A., & Melgani, F. (2017). A convolutional
    neural network approach for assisting avalanche search and rescue operations with
    UAV imagery. Remote Sensing, 9(2), 100. Google Scholar   Tuyishimire, E., Bagula,
    A., Rekhis, S., & Boudriga, N. (2017). Cooperative data muling from ground sensors
    to base stations using UAVs. In IEEE symposium on computers and communications
    (ISCC) (pp. 35–41). Quaritsch, M., Kruggl, K., Wischounig-Strucl, D., Bhattacharya,
    S., Shah, M., & Rinner, B. (2010). Networked UAVs as aerial sensor network for
    disaster management applications. Elektrotechnik Informationstechnik, 127(3),
    56–63. Google Scholar   Ren, Y., Zhang, X., & Lu, G. (2020). The wireless solution
    to realize green IoT: Cellular networks with energy efficient and energy harvesting
    schemes. Energies, 13(22), 5875. Google Scholar   Brewster, C., Roussaki, I.,
    Kalatzis, N., Doolin, K., & Ellis, K. (2017). Iot in agriculture: Designing a
    Europe-wide large-scale pilot. IEEE Communications Magazine, 55(9), 26–33. Google
    Scholar   Wang, S., Garg, H., Lin, G., Kaddoum, J., & Alhamid, M. F. (2021). An
    intelligent UAV based data aggregation algorithm for 5G-enabled internet of things.
    Computer Networks, 185, 107628. Google Scholar   Kouhdaragh, V., Verde, F., Gelli,
    G., & Abouei, J. (2020). On the application of machine learning to the design
    of UAV-based 5G radio access networks. Electronics, 9(4), 689. Google Scholar   Ray,
    P. P. (2017). Internet of things for smart agriculture: Technologies, practices
    and future direction. AIS, 9(4), 395–420. Google Scholar   Tzounis, A., Katsoulas,
    N., Bartzanas, T., & Kittas, C. (2017). Internet of things in agriculture, recent
    advances and future challenges. Biosystems Engineering, 164, 31–48. Google Scholar   Elijah,
    O., Rahman, T. A., Orikumhi, I., Leow, C. Y., & Hindia, M. H. (2018). An overview
    of internet of things (IoT) and data analytics in agriculture: Benefits and challenges.
    IEEE Internet of Things Journal, 5(5), 3758–3773. Google Scholar   Khanna, A.,
    & Kaur, S. (2019). Evolution of internet of things (IoT) and its significant impact
    in the field of precision agriculture. Computers and Electronics in Agriculture,
    157, 218–231. Google Scholar   Ruan, J., Wang, Y., Chan, F. T., Hu, X., Zhao,
    M., Zhu, F., Shi, B., Shi, Y., & Lin, F. (2019). A life cycle framework of green
    IoT-based agriculture and its finance, operation, and management issues. IEEE
    Communications Magazine, 57(3), 90–96. Google Scholar   Ferrag, M. A., Shu, L.,
    Yang, X., Derhab, A., & Maglaras, L. (2020). Security and privacy for green IoT-based
    agriculture: Review, blockchain solutions, and challenges. IEEE Access, 8, 32031–32053.
    Google Scholar   Verma, S., Kaur, S., Khan, M. A., & Sehdev, P. S. (2020). Toward
    green communication in 6G-enabled massive internet of things. IEEE Internet of
    Things Journal, 8(7), 5408–5415. Google Scholar   Alsamhi, S. H., Ma, O., Ansari,
    M. S., & Meng, Q. (2018). Greening internet of things for smart everythings with
    a green-environment life: A survey and future prospects. arXiv. arXiv preprint
    arXiv:1805.00844 Lyu, X., Tian, H., Jiang, L., Vinel, A., Maharjan, S., Gjessing,
    S., & Zhang, Y. (2018). Selective offloading in mobile edge computing for the
    green internet of things. IEEE Network, 32(1), 54–60. Google Scholar   Gupta,
    V., Tripathi, S., & De, S. (2020). Green sensing and communication: A step towards
    sustainable IoT systems. Journal of the Indian Institute of Science, 100(2), 383–398.
    Google Scholar   Foubert, B., & Mitton, N. (2020). Long-range wireless radio technologies:
    A survey. Future Internet Journal, 12(1), 13. Google Scholar   Malik, A., & Kushwah,
    R. (2022). A survey on next generation IoT networks from green IoT perspective.
    International Journal of Wireless Information Networks, 29(1), 36–57. Google Scholar   Lahmeri,
    M. A., Kishk, M. A., Alouini, M. S., Kishk, M. A., & Alouini, M. S. (2021). Artificial
    intelligence for UAV-enabled wireless networks: A survey. IEEE Open Journal of
    the Communications Society, 2, 1015–1040. Google Scholar   Alsamhi, S. H., Afghah,
    F., Sahal, R., Hawbani, A., Al-qaness, M. A., Lee, B., & Guizani, M. (2021). Green
    internet of things using UAVs in B5G networks: A review of applications and strategies.
    AdHoc Networks, 117, 102505. Google Scholar   Zhu, C., Leung, V. C., Shu, L.,
    & Ngai, E. C. (2015). Green internet of things for smart world. IEEE Access, 3,
    2151–2162. Google Scholar   Dayarathna, M., Wen, Y., & Fan, R. (2016). Data center
    energy consumption modeling: A survey. IEEE Communications Surveys and Tutorials,
    18(1), 732–794. Google Scholar   Azevedo, J., & Santos, F. (2012). Energy harvesting
    from wind and water for autonomous wireless sensor nodes. IET Circuits, Devices
    and Systems, 6(6), 413–420. MathSciNet   Google Scholar   Shaikh, F. K., & Zeadally,
    S. (2016). Energy harvesting in wireless sensor networks: A comprehensive review.
    Renewable and Sustainable Energy Reviews, 55, 1041–1054. Google Scholar   Wang,
    J., Hu, C., & Liu, A. (2017). Comprehensive optimization of energy consumption
    and delay performance for green communication in internet of things. Mobile Information
    Systems. https://doi.org/10.1155/2017/3206160 Article   Google Scholar   Liu,
    X. F., Zhan, Z. H., & Zhang, J. (2017). An energy aware unified ant colony system
    for dynamic virtual machine placement in cloud computing. Energies, 10(5), 609.
    Google Scholar   Jayalath, J. M., Chathumali, E. J., Kothalawala, K. R., & Kuruwitaarachchi,
    N. (2019). Green cloud computing: a review on adoption of green-computing attributes
    and vendor specific implementations. In International research conference on smart
    computing and systems engineering (SCSE) (pp. 158–164). Bello, H., Xiaoping, Z.,
    Nordin, R., & Xin, J. (2019). Advances and opportunities in passive wake-up radios
    with wireless energy harvesting for the internet of things applications. Sensors,
    19(14), 3078. Google Scholar   Kozlowski, A., & Sosnowski, J. (2019). Energy efficiency
    trade-off between duty-cycling and wake-up radio techniques in IoT networks. Wireless
    Personal Communications, 107(4), 1951–1971. Google Scholar   Rawat, P., & Chauhan,
    S. (2021). Probability based cluster routing protocol for wireless sensor network.
    Journal of Ambient Intelligence and Humanized Computing, 12, 2065–2077. Google
    Scholar   Goldstein, A., Lior, F., Amit, M., Bohadana, S., Lutenberg, O., & Ravid,
    G. (2018). Applying machine learning on sensor data for irrigation recommendations:
    Revealing the agronomist’s tacit knowledge. Precision Agriculture Journal, 19(3),
    421–444. Google Scholar   Kumar, A., Surendra, A., Mohan, H., Valliappan, K. M.,
    & Kirthika, N. (2017). Internet of things based smart irrigation using regression
    algorithm. In Proceedings of international conference on intelligent computing,
    instrumentation and control technologies (ICICICT) (pp. 1652–1657). IEEE Mohapatra,
    A. G., Lenka, S. K., & Keswani, B. (2019). Neural network and fuzzy logic based
    smart DSS model for irrigation notification and control in precision agriculture.
    Proceedings of the National Academy of Sciences, India Section A: Physical Sciences,
    89(1), 67–76. Google Scholar   Keswani, B., Mohapatra, A., Keswani, P., Khanna,
    A., Gupta, D., & Rodrigues, J. (2020). Improving weather dependent zone specific
    irrigation control scheme in IoT and big data enabled self driven precision agriculture
    mechanism. Enterprise Information Systems Journal, 14(9–10), 1–22. Goap, A., Sharma,
    D., Shukla, A. K., & Rama-Krishna, C. (2018). An IoT based smart irrigation management
    system using machine learning and open source technologies. Computers and Electronics
    in Agriculture, 155, 41–49. Google Scholar   Vij, A., Singh, V., Jain, A., Bajaj,
    S., Bassi, A., & Sharma, A. (2020). Iot and machine learning approaches for automation
    of farm irrigation system. Procedia Computer Science, 167, 1250–1257. Google Scholar   Munir,
    M., Safdar, I., Sarwar, B., & Cheema, S. M. (2019). An intelligent and secure
    smart watering system using fuzzy logic and blockchain. Computers and Electrical
    Engineering Journal, 77, 109–119. Google Scholar   Cioffi, R., Travaglioni, M.,
    Piscitelli, G., Petrillo, A., & DeFelice, F. (2020). Artificial intelligence and
    machine learning applications in smart production: Progress, trends, and directions.
    Sustainability, 12(2), 492. Google Scholar   Remmert, H. (2020). Edge computing,
    artificial intelligence, machine learning and 5G. [https://www.digi.com/blog/post/edge-compute-artificial-intelligence-ml-5g]
    Mehmood, F., Hamza, M. A., Bukhsh, R., Javaid, N., Imran, M. I. U., Choudri, S.,
    & Ahmed, U. (2020). Green fog: Cost efficient real time power management service
    for green community. In Proceedings of the 14th international conference on complex,
    intelligent and software intensive systems (pp. 142–155). Cham: Springer. Sakai,
    R., Saito, T., Nakamura, S., Enokido, T., & Takizawa, M. (2020). Software-oriented
    routing protocol for energy-efficient wireless communications. In Proceedings
    of the 14th international conference on complex, intelligent and software intensive
    systems (pp. 1–11). Cham: Springer. Saito, T., Nakamura, S., Enokido, T., & Takizawa,
    M. (2020). A topic-based publish/subscribe system in a fog computing model for
    the IoT. InProceedings of the 14th international conference on complex, intelligent
    and software intensive systems (pp. 12–21). Cham: Springer. Sheikhi, A., Rayati,
    M., & Ranjbar, A. M. (2015). Energy hub optimal sizing in the smart grid; machine
    learning approach. In IEEE power and energy society innovative smart grid technologies
    conference (ISGT) (pp. 1–5). IEEE. Mounce, S. R., Pedraza, C., Jackson, T., Linford,
    P., & Boxall, J. B. (2015). Cloud based machine learning approaches for leakage
    assessment and management in smart water networks. Procedia Engineering, 119,
    43–52. Google Scholar   Lavassani, M., Forsstrom, S., Jennehag, U., & Zhang, T.
    (2018). Combining fog computing with sensor mote machine learning for industrial
    IoT. Sensors, 18(5), 1532. Google Scholar   Paris, L., & Anisi, M. H. (2019).
    An energy-efficient predictive model for object tracking sensor networks. In IEEE
    5th World Forum on Internet of Things (WF-IoT) (pp. 263–268). IEEE. Liu, Y., Yang,
    C., Jiang, L., Xie, S., & Zhang, Y. (2019). Intelligent edge computing for IoT-based
    energy management in smart cities. IEEE Network, 33(2), 111–117. Google Scholar   Zhang,
    Q., Mozaffari, M., Saad, W., Bennis, M., & Debbah, M. (2018) Machine learning
    for predictive on-demand deployment of UAVs for wireless communications. In IEEE
    global communications conference (GLOBECOM) (pp. 1–9). Chen, J., Yatnalli, U.,
    & Gesbert, D. (2017). Learning radio maps for UAV aided wireless networks: A segmented
    regression approach. In IEEE International Conference on Communications (ICC)
    (pp. 1–6). Zhang, Q., Saad, W., Bennis, M., Lu, X., Debbah, M., & Zuo, W. (2021).
    Predictive deployment of UAV base stations in wireless networks: Machine learning
    meets contract theory. IEEE Transactions on Wireless Communications, 20, 637–652.
    Google Scholar   Peng, H., Razi, A., Afghah, F., & Ashdown, J. (2018). A unified
    framework for joint mobility prediction and object profiling of drones in UAV
    networks. Journal of Communications and Networks, 20, 434–442. Google Scholar   Xiao,
    K., Zhao, J., He, Y., & Yu, S. (2019). Trajectory prediction of UAV in smart city
    using recurrent neural networks. In IEEE international conference on communications
    (ICC) (pp. 1–6). Kumari, R., & Kaushal, S. (2017). Energy efficient approach for
    applicationexecution in mobile cloud IoT environment. In Proceedings of the second
    international conference on internet of things, data and cloud computing (pp.
    1–8). Alharbi, F., Tian, Y. C., Tang, M., Zhang, W. Z., Peng, C., & Fei, M. (2019).
    An ant colony system for energy-efficient dynamic virtual machine placement in
    data centers. Expert Systems with Applications, 120, 228–238. Google Scholar   Gharehpasha,
    S., Masdari, M., & Jafarian, A. (2021). Virtual machine placement in cloud data
    centers using a hybrid multi-verse optimization algorithms. Artificial Intelligence
    Review, 54, 2221–2257. Azar, J., Makhoul, A., Barhamgi, M., & Couturier, R. (2019).
    An energy efficient IoT data compression approach for edge machine learning. Future
    Generation Computer Systems, 96, 168–175. Google Scholar   Min, M., Xiao, L.,
    Chen, Y., Cheng, P., Wu, D., & Zhuang, W. (2019). Learning based computation offloading
    for IoT devices with energy harvesting. IEEE Transactions on Vehicular Technology,
    68(2), 1930–1941. Google Scholar   Ye, Y., Azmat, F., Adenopo, I., Chen, Y., &
    Shi, R. (2021). RF energy modelling using machine learning for energy harvesting
    communications systems. International Journal of Communication Systems, 34, 4688.
    Google Scholar   Khan, Z. A., Hussain, T., & Baik, S. W. (2022). Boosting energy
    harvesting via deep learning-based renewable power generation prediction. Journal
    of King Saud University-Science, 34, 101815. Google Scholar   Chu, M., Liao, X.,
    Li, H., & Cui, S. (2019). Power control in energy harvesting multiple access system
    with reinforcement learning. IEEE Internet of Things Journal, 6, 9175–9186. Google
    Scholar   Zhang, Y., He, J., & Guo, S. (2018). Energy-efficient dynamic task offloading
    for energy harvesting mobile cloud computing. In 2018 IEEE international conference
    on networking, architecture and storage (NAS) (pp. 1–4). Singh, S., Sharma, P.
    K., Moon, S. Y., & Park, J. H. (2017). EH-GC: An efficient and secure architecture
    of energy harvesting green cloud infrastructure. Sustainability, 9, 673. Google
    Scholar   Kakati, S., Mazumdar, N., & Nag, A. (2022). Green cloud computing for
    IoT based smart applications. In Green mobile cloud computing (pp. 201–212). Cham:
    Springer International Publishing. Zhang, G., Zhang, W., Cao, Y., Li, D., & Wang,
    L. (2018). Energy-delay tradeoff for dynamic offloading in mobile-edge computing
    system with energy harvesting devices. IEEE Transactions on Industrial Informatics,
    14, 4642–4655. Google Scholar   Lu, M., Fu, G., Osman, N. B., & Konbr, U. (2021).
    Green energy harvesting strategies on edge-based urban computing in sustainable
    internet of things. Sustainable Cities and Society, 75, 103349. Google Scholar   Tang,
    Q., Xie, R., Yu, F. R., Huang, T., & Liu, Y. (2020). Decentralized computation
    offloading in IoT fog computing system with energy harvesting: A dec-POMDP approach.
    IEEE Internet of Things Journal, 7, 4898–4911. Google Scholar   Kim, Y., & Lee,
    T. J. (2017). Service area scheduling in a drone assisted network. In International
    conference on computational science and its applications (pp. 161–171). Springer.
    Carrio, A., Parez, C. S., Ramos, A. R., & Campoy, P. (2017). A review of deep
    learning methods and applications for unmanned aerial vehicles. Journal of Sensors,
    2, 1–13. Google Scholar   Yoo, S. J., Park, J. H., Kim, S. H., & Shrestha, A.
    (2016). Flying path optimization in UAV-assisted IoT sensor networks. ICT Express,
    2(3), 140–144. Google Scholar   Hawbani, A., Wang, X., Kuhlani, H., Ghannami,
    A., Farooq, M. U., & Al-Sharabi, Y. (2019). Extracting the overlapped sub-regions
    in wireless sensor networks. Wireless Networks, 25(8), 4705–4726. Google Scholar   Moradi,
    M., Bokani, A., & Hassan, J. (2020). Energy-efficient and QoS-aware UAV communication
    using reactive RF band allocation. In 30th International telecommunication networks
    and applications conference (ITNAC) (pp. 1–6). IEEE. Ahmed, S., Chowdhury, M.
    Z., & Jang, Y. M. (2021). Energy-efficient UAV-to-user scheduling to maximize
    throughput in wireless networks. IEEE Access, 8, 21215–21225. Google Scholar   Li,
    M., Cheng, N., Gao, J., Wang, Y., Zhao, L., & Shen, X. (2020). Energy-efficient
    UAV-assisted mobile edge computing: resource allocation and trajectory optimization.
    IEEE Transactions on Vehicular Technology, 69(3), 3424–3438. Google Scholar   Nguyen,
    A. N., Vo, V. N., So-In, C., & Ha, D. B. (2021). System performance analysis for
    an energy harvesting IoT system using a DF/AF UAV-enabled relay with downlink
    NOMA under Nakagami-m fading. Sensors, 21(1), 285. Google Scholar   Namboodiri,
    V., & Gao, L. (2009). Energy-aware tag anticollision protocols for RFID systems.
    IEEE Transactions on Mobile Computing, 9(1), 44–59. Google Scholar   Choi, J.
    S., Son, B. R., Kang, H. K., & Lee, D. H. (2012). Indoor localization of unmanned
    aerial vehicle based on passive UHF RFID systems. In 9th international conference
    on ubiquitous robots and ambient intelligence (URAI) (pp. 188–189). IEEE. Hubbard,
    B., Wang, H., Leasure, M., Ropp, T., Lofton, T., Hubbard, S., & Lin, S. (2015).
    Feasibility study of UAV use for RFID material tracking on construction sites.
    In 51st ASC annual international conference proceedings. Allegretti, M., & Bertoldo,
    S. (2015). Recharging RFID tags for environmental monitoring using UAVs: A feasibility
    analysis. Wireless Sensor Network, 7(2), 13. Google Scholar   Hubbard, B., Wang,
    H., & Leasure, M. (2016). Feasibility study of UAV use for RFID material tracking
    on construction sites. In Presented at the Proc. 51st ASC annual international
    conference proceedings College Station, TX, USA. Greco, G., Lucianaz, C., Bertoldo,
    S., & Allegretti, M. (2015). A solution for monitoring operations in harsh environment:
    A rfid reader for small UAV. In International conference on electromagnetics in
    advanced applications (ICEAA) (pp. 859–862). IEEE. Malaver, A., Motta, N., Corke,
    P., & Gonzalez, F. (2015). Development and integration of a solar powered unmanned
    aerial vehicle and a wireless sensor network to monitor greenhouse gases. Sensors,
    15(2), 4072–4096. Google Scholar   Ho, D. T., Grotli, E. I., Sujit, P., Johansen,
    T. A., & Sousa, J. B. (2015). Optimization of wireless sensor network and UAV
    data acquisition. Journal of Intelligent and Robotic Systems, 78(1), 159. Google
    Scholar   Moreno, C. A., Marin, R. B., Marco, A. M., & Nebra, R. C. (2017). Unmanned
    aerial vehicle based wireless sensor network for marine-coastal environment monitoring.
    Jornada de Jovenes Investigadores del, I3A, 5. Google Scholar   Zanjie, H., Hiroki,
    N., Nei, K., Fumie, O., Ryu, M., & Baohua, Z. (2014). Resource allocation for
    data gathering in UAV-aided wireless sensor networks. In Network infrastructure
    and digital content (ICNIDC), 4th IEEE international conference (pp. 11–16). Zhan,
    C., Zeng, Y., & Zhang, R. (2017). Energy-efficient data collection in UAV enabled
    wireless sensor network. IEEE Wireless Communications Letters, 7(3), 328–331.
    Google Scholar   Jawhar, I. H., Mohamed, N., Trabelsi, Z., & Al-Jaroodi, J. (2016).
    Architectures and strategies for efficient communication in wireless sensor networks
    using unmanned aerial vehicles. Unmanned Systems, 4(04), 289–305. Google Scholar   Horstrand,
    P., Guerra, R., Rodriguez, A., Diaz, M., Lopez, S., & Lopez, J. F. (2019). A UAV
    platform based on a hyperspectral sensor for image capturing and on-board processing.
    IEEE Access, 7, 66919–66938. Google Scholar   Bah, M. D., Dericquebourg, E., Hafiane,
    A., & Canals, R. (2018). Deep learning based classification system for identifying
    weeds using high-resolution UAV imagery (pp. 176–187). Cham: Springer. Google
    Scholar   Hassanein, M., & El-Sheimy, N. (2018). An efficient weed detection procedure
    using low-cost UAV imagery system for precision agriculture applications. In International
    archives of the photogrammetry: remote sensing & spatial information sciences.
    Spachos, P., & Gregori, S. (2019). Integration of wireless sensor networks and
    smart UAVs for precision viticulture. IEEE Internet Computing, 23(3), 8–16. Google
    Scholar   Carl, C., Landgraf, D., van der Maaten-Theunissen, M. T., Biber, M.
    P., & Pretzsch, H. (2017). Robinia pseudoacacia l. flowers analyzed by using an
    unmanned aerial vehicle (UAV). Remote Sensing, 9(11), 1091. Google Scholar   Faical,
    B. S., Costa, F. G., Pessin, G., Ueyama, J., Freitas, H., Colombo, A., Fini, P.
    H., Villas, L., Osorio, F. S., Vargas, P. A., & Braun, T. (2014). The use of unmanned
    aerial vehicles and wireless sensor networks for spraying pesticides. Journal
    of Systems Architecture, 60(4), 393–404. Google Scholar   Hassan, M. A., Yang,
    M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., Xiao, Y., & He, Z. (2019). A
    rapid monitoring of NDVI across the wheat growth cycle for grain yield prediction
    using a multi-spectral UAV platform. Plant Science, 282, 95–103. Google Scholar   Radoglou-Grammatikis,
    P., Sarigiannidis, P., Lagkas, T., & Moscholios, I. (2020). A compilation of UAV
    applications for precision agriculture. Computer Networks, 172, 107148. Google
    Scholar   Popescu, D., Stoican, F., Stamatescu, G., Ichim, L., & Dragana, C. (2020).
    A compilation of UAV applications for precision agriculture. Sensors, 20, 817.
    Google Scholar   Boursianis, A. D., Papadopoulou, M. S., Diamantoulakis, P., LiopaTsakalidi,
    A., Barouchas, P., Salahas, G., Karagiannidis, G., Wan, S., & Goudos, S. K. (2020).
    Internet of things (IoT) and agricultural unmanned aerial vehicles (UAVs) in smartfarming:
    A comprehensive review. Internet of Things, 18, 100187. Google Scholar   Mekki,
    K., Bajic, E., Chaxel, F., & Fernand, M. (2019). A comparative study of LPWAN
    technologies for large-scale IoT deployment. ICT Express, 5(1), 1–7. Google Scholar   Islam,
    N., Ray, B., & Pasandideh, F. (2020). IoT based smart farming: Are the LPWAN technologies
    suitable for remote communication?. In IEEE international conference on smart
    internet of things (SmartIoT) (pp. 270–276). Valecce, G., Petruzzi, P., Strazzella,
    S., & Grieco, L. A. (2020). NB-IoT for smart agriculture: Experiments from the
    field. In International conference on control, decision and information technologies
    (pp. 71–75). Valente, A., Silva, S., Duarte, D., Cabral Pinto, F., & Soares, S.
    (2020). Low-cost LoRaWAN node for agro-intelligence IoT. Electronics, 9(6), 987.
    Google Scholar   Ramson, S. R. (2021). A self-powered, real-time, LoRaWAN IoT-based
    soil health monitoring system. IEEE Internet of Things Journal, 8, 9278–9293.
    Google Scholar   Fernandez-Ahumada, L. M., Ramirez-Faz, J., Torres-Romero, M.,
    & Lopez-Luque, R. (2019). Proposal for the design of monitoring and operating
    irrigation networks based on IoT, cloud computing and free hardware technologies.
    Sensors, 19, 2318. Google Scholar   Dai, J., & Sugano, M. (2019). Low-cost sensor
    network for collecting real-time data for agriculture by combining energy harvesting
    and LPWA technology. In IEEE Global humanitarian technology conference. Ijaz,
    A., Zhang, L., Grau, M., Mohamed, A., Vural, S., Quddus, A. U., Imran, M. A.,
    Foh, C. H., & Tafazolli, R. (2016). Enabling massive IoT in 5G and beyond systems:
    PHY radio frame design considerations. IEEE Access, 24(4), 3322–39. Google Scholar   Duan,
    L., & Xu, L. D. (2021). Data analytics in industry 4.0: A survey. Information
    Systems Frontiers. https://doi.org/10.1007/s10796-021-10190-0 Article   Google
    Scholar   Li, S., Iqbal, M., & Saxena, N. (2022). Future industry internet of
    things with zero-trust security. Information Systems Frontiers. https://doi.org/10.1007/s10796-021-10199-5
    Article   Google Scholar   Deng, D., Xia, J., Fan, L., & Li, X. (2020). Link selection
    in buffer-aided cooperative networks for green IoT. IEEE Access, 8, 30763–30771.
    Google Scholar   Din, S., Ahmad, A., Paul, A., & Rho, S. (2018). MGR: Multi-parameter
    green reliable communication for internet of things in 5G network. Journal of
    Parallel and Distributed Computing, 118, 34–45. Google Scholar   Na, Z., Wang,
    X., Shi, J., Liu, C., Liu, Y., & Gao, Z. (2020). Joint resource allocation for
    cognitive OFDM-NOMA systems with energy harvesting in green IoT. Ad Hoc Networks,
    107, 102221. Google Scholar   Li, J., Liu, Y., Zhang, Z., Ren, J., & Zhao, N.
    (2017). Towards green IoT networking: Performance optimization of network coding
    based communication and reliable storage. IEEE Access, 5, 8780–8791. Google Scholar   Garzon,
    J., Acevedo, J., Pavon, J., & Baldiris, S. (2020). Promoting eco-agritourism using
    an augmented reality-based educational resource: a case study of aquaponics. Interactive
    Learning Environments, 30(7), 1–15. Skvortsov, E. A., Skvortsova, E. G., Sandu,
    I. S., & Iovlev, G. A. (2018). Transition of agriculture to digital, intellectual
    and robotics technologies. EoR, 14(3), 1014–1028. Google Scholar   Gandotra, P.,
    Jha, R. K., & Jain, S. (2017). Green communication in next generation cellular
    networks: A survey. IEEE Access, 5, 11727–11758. Google Scholar   Buzzi, S., Chih-Lin,
    I., Klein, T. E., Poor, H. V., Yang, C., & Zappone, A. (2016). A survey of energy-efficient
    techniques for 5G networks and challenges ahead. IEEE Journal on Selected Areas
    in Communications, 34(4), 697–709. Google Scholar   Zhang, D., Zhou, Z., Mumtaz,
    S., Rodriguez, J., & Sato, T. (2017). One integrated energy efficiency proposal
    for 5G IoT communications. IEEE Internet of Things Journal, 3(6), 1346–1354. Google
    Scholar   Liu, Q., Sun, S., Wang, H., & Zhang, S. (2021). 6G green IoT network:
    Joint design of intelligent reflective surface and ambient backscatter communication.
    Wireless Communications and Mobile Computing, 2021, 1–10. Google Scholar   Amjad,
    M., Chughtai, O., Naeem, M., & Ejaz, W. (2021). SWIPT-assisted energy efficiency
    optimization in 5G/B5G cooperative IoT network. Energies, 14(9), 2515. Google
    Scholar   Pan, C., Ren, H., Deng, Y., Elkashlan, M., & Nallanathan, A. (2019).
    Joint blocklength and location optimization for URLLC-enabled UAV relay systems.
    IEEE Communications Letters, 23, 498–501. Google Scholar   Anand, A., deVeciana,
    G., & Shakkottai, S. (2020). Joint scheduling of URLLC and eMBB traffic in 5G
    wireless networks. IEEE/ACM Transactions on Networking, 28, 477–490. Google Scholar   She,
    C., Liu, C., Quek, T. Q., Yang, C., & Li, Y. (2019). Ultra-reliable and low-latency
    communications in unmanned aerial vehicle communication systems. IEEE Transactions
    on Communications, 67(5), 3768–3781. Google Scholar   Riva, C., & Zaim, A. H.
    (2023). A comparative study on energy harvesting battery-free lorawan sensor networks.
    Electrica, 23(1), 40–47. Google Scholar   Gleonec, P. D., Ardouin, J., Gautier,
    M., & Berder, O. (2021). Energy allocation for lorawan nodes with multi-source
    energy harvesting. Sensors, 21, 2874. Google Scholar   Delgado, C., Sanz, J. M.,
    & Famaey, J. (2019). On the feasibility of battery-less lorawan communications
    using energy harvesting. In Proceedings of IEEE global communications conference
    (GLOBECOM) (vol. 23, pp. 1–6). Waikoloa. Xu, J., Solmaz, G., Rahmatizadeh, R.,
    Turgut, D., & Boloni, L. (2016). Internet of things applications: Animal monitoring
    with unmanned aerial vehicle. arXiv preprint arXiv:1610.05287 Wang, X., Garg,
    S., Lin, H., Kaddoum, G., Hu, J., & Alhamid, M. F. (2021). An intelligent UAV
    based data aggregation algorithm for 5G-enabled internet of things. Computer Networks,
    185, 107628. Google Scholar   Shi, L., Jiang, Z., & Xu, S. (2021). Throughput-aware
    path planning for UAVs in D2D 5G networks. AdHoc Networks, 116, 102427. Google
    Scholar   Dawit, M., & Frisk, F. (2019) Edge machine learning for energy efficiency
    of resource constrained IoT devices. In SPWID: The Fifth international conference
    on smart portable, wearable, implantable and disability oriented devices and systems.
    O’Grady, M. J., Langton, D., & O’Hare, G. M. (2019). Edge computing: A tractable
    model for smart agriculture? Artificial Intelligence in Agriculture Journal, 3,
    42–51. Google Scholar   Baldi, M., & Ofek, Y. (2009). Time for a greener internet.
    In IEEE international conference on communications workshops, ICC Workshops (pp.
    1–6). IEEE. Tahiliani, V., & Mavuri, D. (2018). Green IoT systems: An energy efficient
    perspective. In Eleventh international conference on contemporary computing (IC3).
    IEEE. Phalaagae, P., Zungeru, A. M., Sigweni, B., Chuma, J. M., & Semong, T. (2020).
    Security challenges in IoT sensor networks Green internet of things sensor networks
    (pp. 83–96). Cham: Springer. Google Scholar   Jabbar, W. A., Alsibai, M. H., Amran,
    N. S., & Mahayadin, S. K. (2018). Design and implementation of IoT-based automation
    system for smart home. In Proceedings of International Symposium on Networks,
    Computers and Communications (ISNCC) (pp. 1–6). Bing, K., Fu, L., Zhuo, Y., &
    Yanlei, L. (2011). Design of an internet of things-based smart home system. In
    Proceedings of 2nd international conference on intelligent control and information
    processing (vol. 2, pp. 921–924). Lv, Z. (2020). Security of internet of things
    edge devices. Mahalakshmi, G., & Nadu, T. (2018). Denial of sleep attack detection
    using mobile agent in wireless sensors. International Journal for Research Trends
    and Innovation, 3(5), 139–149. Google Scholar   Gautam, S., Malik, A., Singh,
    N., & Kumar, S. (2019). Recent advances and countermeasures against various attacks
    in IoT environment. In 2019 2nd international conference on signal processing
    and communication (ICSPC (pp. 315–319). Cekerevac, Z., Dvorak, Z., Prigoda, L.,
    & Cekerevac, P. (2017). Internet of things and the man-in-themiddle attacks–security
    and economic risks. MEST, 5(2), 15–25. Google Scholar   Singh, K. J., & Kapoor,
    D. S. (2017). Create your own internet of things: A survey of IoT platforms. IEEE
    Consumer Electronics Magazine, 6(2), 57–68. Google Scholar   Gupta, K. S., & Jayant,
    K. P. (2010). A review study on phishing attack techniques for protecting the
    attacks. Globus-An International Journal of Management and IT, 10(2), 22–25. Google
    Scholar   Kim, H., Kang, E., Broman, D., & Lee, E. A. (2018). Resilient authentication
    and authorization for the internet of things (IoT) using edge computing. ACM Transactions
    on Internet Things, 1, 1–27. Google Scholar   Quasim, M. T. (2021). Challenges
    and applications of internet of things (IoT) in Saudi Arabia. Easy Chair Preprint,
    1–25. [https://easychair.org/publications/preprint_open/r2W4] Ravi, N., & Shalinie,
    S. M. (2020). Learning-driven detection and mitigation of DDoS attack in IoT via
    SDN-cloud architecture. IEEE Internet of Things Journal, 7(4), 3559–3570. Google
    Scholar   Zolanvari, M., Teixeira, M. A., Gupta, L., Khan, K. M., & Jain, R. (2019).
    Machine learning-based network vulnerability analysis of industrial internet of
    things. IEEE Internet of Things Journal, 6(4), 6822–6834. Google Scholar   Gupta,
    H., & Van-Oorschot, P. C. (2019). Onboarding and software update architecture
    for IoT devices. In 17th International conference on privacy, security and trust
    (PST), 8949023. Mahmoud, C., & Aouag, S. (2019). Security for internet of things:
    A state of the art on existing protocols and open research issues. In Proceedings
    of the 9th international conference on information systems and technologies (pp.
    1–6). Hind, M., Noura, O., Amine, K. M., & Sanae, M. (2020). Internet of things:
    Classification of attacks using ctm method. In Proceeding series: In ACM international
    conference. Li, W., Logenthiran, T., Phan, V. T., & Woo, W. L. (2019). A novel
    smart energy theft system (SETS) for IoT-based smart home. IEEE Internet of Things
    Journal, 6(3), 5531–5539. Google Scholar   Download references Acknowledgements
    The authors are grateful to all the staffs and Faculty Members of Computer Science
    and Engineering department of Tripura Institute of Technology, Agartala and National
    Institute of Technology, Agartala, India for providing smooth access to the computing
    resources under their custody. Author information Authors and Affiliations Department
    of Computer Science and Engineering, National Institute of Technology, Agartala,
    Jirania, Barjala, 799046, India Parijata Majumdar & Diptendu Bhattacharya Department
    of Computer Science and Engineering, Techno College of Engineering, Agartala,
    Maheshkhola, 799004, India Parijata Majumdar Department of Computer Science and
    Engineering, Tripura Institute of Technology, Agartala, Narsingarh, 799009, India
    Sanjoy Mitra Department of Computer Science and Engineering, School of Engineering
    and Technology, Sharda University, Greater Noida, Uttar Pradesh, 201310, India
    Bharat Bhushan Contributions Conceptualization of Idea of the Article: PM and
    SM. Literature Search and Data Analysis: SM and PM. Resources: DB and SM. Writing—Original
    Draft Preparation: PM and SM. Writing—Review and Editing: SM and PM and BB. Critical
    Revision of the Work: SM and BB. Visualization: PM and BB. Supervision: DB and
    SM. Overall Administration: DB and SM Corresponding author Correspondence to Sanjoy
    Mitra. Ethics declarations Conflict of interest The authors declare that they
    have no conflict of interest. Human and Animals Rights This review research is
    not having any involvement of Human Participants and/or Animals. Informed Consent
    Not applicable as no any involvement of human participants or any other living
    being. Additional information Publisher''s Note Springer Nature remains neutral
    with regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Springer Nature or its licensor (e.g. a society or other
    partner) holds exclusive rights to this article under a publishing agreement with
    the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Majumdar, P., Bhattacharya, D., Mitra, S. et al. Application of Green
    IoT in Agriculture 4.0 and Beyond: Requirements, Challenges and Research Trends
    in the Era of 5G, LPWANs and Internet of UAV Things. Wireless Pers Commun 131,
    1767–1816 (2023). https://doi.org/10.1007/s11277-023-10521-1 Download citation
    Accepted 18 May 2023 Published 08 June 2023 Issue Date August 2023 DOI https://doi.org/10.1007/s11277-023-10521-1
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Agriculture 4.0 Energy saving GIoT Internet of UAV things
    LPWANs 5G networks Use our pre-submission checklist Avoid common mistakes on your
    manuscript. Sections Figures References Abstract Introduction Motivation IoT Enablers
    and Associated Challenges Green Solutions for IoT Technology Enablers Data Processing
    Techniques and Energy Management GIoT Strategies Leveraging Unmanned Aerial Vehicles
    (UAVs), Low Power Wide Area Networks (LPWANs) and 5G Networks Proposed GIoT Framework
    for Energy-Aware PA Applications Security Issues in GIoT and Solutions Conclusion
    Data Availibility Code Availability References Acknowledgements Author information
    Ethics declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Wireless Personal Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Application of Green IoT in Agriculture 4.0 and Beyond: Requirements, Challenges
    and Research Trends in the Era of 5G, LPWANs and Internet of UAV Things'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Li T.
  - Ma Y.
  - Endoh T.
  citation_count: '1'
  description: Next-generation industrial edge artificial intelligence (AI) applications
    will undoubtedly emerge on energy-efficient, highly-integrated platforms incorporating
    various sensors, processors, and functional modules. Embedding the data quantization
    module onto edge AI chips, connecting sensors, processors, and functional modules
    is critical to achieving adaptive transformation of diverse data representation
    formats. This paper proposes a novel adaptive and low-power quantization technique
    and systematically validates its effectiveness from algorithm to hardware module
    for industrial IoT applications, covering precise navigation for autonomous vehicles
    and accurate classification utilizing deep neural networks (DNNs). The proposed
    quantization method merges an adaptive conversion function from floating-point
    to fixed-point binaries and an adaptive radix-point determination function, ensuring
    adequate resolution and minimal error loss of the fixed-point inputs to the edge
    AI modules. The experimental results demonstrate that the quantization error in
    the proposed quantization technique contributes negligible errors to the navigation
    solutions of the strapdown inertial navigation system and the DNNs' top-1 and
    top-5 classification accuracy (on the order of 10-8 and 10-7). Moreover, a quantization-on-multiplier
    (QoM) hardware module is designed, synthesized, and routed in accordance with
    the proposed quantization technique. The simulation results indicate that the
    QoM's power consumption and area are 0.1 mW and 649.552 μ m2, accounting for 5%
    and 14.15% of its total energy consumption and area, respectively. With our proposed
    on-chip quantization technique, the time required to quantize the parameters of
    DNNs is up to 1142 times shorter than with the existing benchmark off-chip quantization
    approaches.
  doi: 10.1109/TII.2022.3223222
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Industri...
    >Volume: 19 Issue: 8 From Algorithm to Module: Adaptive and Energy-Efficient Quantization
    Method for Edge Artificial Intelligence in IoT Society Publisher: IEEE Cite This
    PDF Tao Li; Yitao Ma; Tetsuo Endoh All Authors 1 Cites in Paper 397 Full Text
    Views Abstract Document Sections I. Introduction II. Proposed Scheme III. Results
    and Discussion IV. Conclusion Authors Figures References Citations Keywords Metrics
    Footnotes Abstract: Next-generation industrial edge artificial intelligence (AI)
    applications will undoubtedly emerge on energy-efficient, highly-integrated platforms
    incorporating various sensors, processors, and functional modules. Embedding the
    data quantization module onto edge AI chips, connecting sensors, processors, and
    functional modules is critical to achieving adaptive transformation of diverse
    data representation formats. This paper proposes a novel adaptive and low-power
    quantization technique and systematically validates its effectiveness from algorithm
    to hardware module for industrial IoT applications, covering precise navigation
    for autonomous vehicles and accurate classification utilizing deep neural networks
    (DNNs). The proposed quantization method merges an adaptive conversion function
    from floating-point to fixed-point binaries and an adaptive radix-point determination
    function, ensuring adequate resolution and minimal error loss of the fixed-point
    inputs to the edge AI modules. The experimental results demonstrate that the quantization
    error in the proposed quantization technique contributes negligible errors to
    the navigation solutions of the strapdown inertial navigation system and the DNNs''
    top-1 and top-5 classification accuracy (on the order of 10 −8 and 10 −7 ). Moreover,
    a quantization-on-multiplier (QoM) hardware module is designed, synthesized, and
    routed in accordance with the proposed quantization technique. The simulation
    results indicate that the QoM''s power consumption and area are 0.1 mW and 649.552
    μ m 2 , accounting for 5% and 14.15% of its total energy consumption and area,
    respectively. With our proposed on-chip quantization technique, the time required
    to quantize the parameters of DNNs is up to 1142 times shorter than with the existing
    benchmark off-chip quantization approaches. Published in: IEEE Transactions on
    Industrial Informatics ( Volume: 19, Issue: 8, August 2023) Page(s): 8953 - 8964
    Date of Publication: 18 November 2022 ISSN Information: DOI: 10.1109/TII.2022.3223222
    Publisher: IEEE Funding Agency: SECTION I. Introduction In The contemporary civilized
    industrial Internet of Things (IoT) society, the innovative technology revolution
    powered by artificial intelligence (AI) will reshape every aspect of the human
    experience. The next-generation IoT society is distinguished by highly-intelligent,
    autonomous, unmanned, and AI-assisted features across a broad range of domains,
    encompassing transportation, healthcare, industry, public security, and agriculture
    [1], [2], [3], [4]. Therefore, it requires low-power, highly-integrated platforms
    with various sensors, processors, and functional modules to sustain their high
    performance and extend their operating hours. Nowadays, a wide range of data processing
    tasks from sensor network nodes are offloading from the cloud server to the local
    edge, which affords a great opportunity and raises large challenges for developing
    edge AI chips for industrial IoT applications. As shown in Fig. 1, the enormous
    volume of interconnections among IoT sensor nodes generates massive and complex
    unstructured data capturing with 64/32-bit floating-point central processing unit
    (CPU) of system-on-chip (SoC). The CPU allocates collected information to different
    processing modules (PMs) to accomplish high-speed and high-throughput data processing
    for various edge devices, such as smart watches, unmanned aerial drones, mobile
    phones, and industrial robots. The PMs within highly-integrated and compact SoC
    is typically stored (memory) or calculated (computation logic) with low-bit representations,
    for instance, 16-bit, 8-bit, 4-bit, or even 1-bit [5], [6], [7], [8]. A microprocessor
    or CPU is generally utilized to convert 32-bit floating-point decimal values to
    16-bit, 8-bit, and 4-bit integer representations (INT16, INT8, or INT4), which
    are then decoded to fixed-point binaries for the input requirement of PMs. However,
    the accuracy of deep neural networks (DNNs) declines with the decrease of the
    bit length, and the design of a compact SoC tends to be impracticable with long-bit
    representations. Young et al. [9] proposed row-ELT, a transform quantization methodology,
    for compressing the DNNs'' models utilizing a rate-distortion framework, which
    is applicable in both post-training and quantization-aware training scenarios.
    Incremental network quantization, proposed by Zhou et al. [10], confines the weights
    to powers of two or zero for the applications of edge devices using a 5-bit representation
    format. Zhang et al. [11] developed LQ-Nets, which compress the DNNs'' weights
    or feature maps with arbitrary-bit precision to facilitate deployment on devices
    with limited resources. Various post-training quantization approaches, such as
    FIBQ [12], DFP-16 [13], M4E3 [14], DeepShift-Q [15], BFP [16], and AngleEye [17],
    have been explored in previous studies. Regardless of the fact that these approaches
    may greatly reduce the model size of DNNs, they cause a significant loss of precision.
    Therefore, it is essential for the design of highly integrated SoC-based edge
    processors to explore the representation of low-bit parameters adaptively while
    maintaining the DNNs'' accuracy. This research offers an adaptive and energy-efficient
    quantization technique and verifies its effect on the accuracy of neural networks
    from the standpoints of algorithm and circuit design. Fig. 1. Research background
    of adaptive quantization method in the applications of edge AI. Show All Due to
    the CPU''s serial processing capability, it is challenging to simultaneously execute
    multiple tasks at a specified clock, such as data acquisition and pre/postprocessing.
    Most industrial IoT applications typically need online data preprocessing, such
    as input feature maps in real-time object detection, which require online data
    normalization, image scaling, zero-padding, etc. [18], [19]; decoding constellations
    (ephemeris, clock offsets, etc.), raw pseudorange, and carrier phase data for
    the global navigation satellite system (GNSS) [20]; and various error compensation
    and calibration for localization sensors in IoT positioning [21]. Therefore, one
    of the primary motivations for embedding the quantization module into the edge
    AI chip proposed in this research is to tackle the data communication issue between
    the intensive preprocessing outputs (floating-point format) from different sensor
    nodes and the function PM. DNNs'' weight or feature maps quantization, as a practical
    approach for parameters'' compression, is assumed to be the most potential technique
    in edge AI owing to its significant advantages. 1) The quantization technique
    efficiently reduces the bit length of weights or activations, making it possible
    to utilize fewer logic gates to fulfill the arithmetic operation. 2) Deploying
    the short-bit representation can lessen the memory requirement for the pretrained
    weights'' storage, further reducing the power consumption of edge AI processor
    due to less external memory access. The quantization-aware training and post-training
    quantization are two primary techniques for bit quantization. Quantization-aware
    training is defined by training the neural networks'' models with quantized weights
    or activation, which is generally utilized for both the training and inference
    scene. Quantized the weights or activations offline in the post-training quantization,
    mainly contributes to the inference stage. Many studies have been published about
    training with low-precision bits [22], [23], [24], [25]. In the literature of
    [26], the authors proposed an optimization algorithm based on quantization errors
    to resolve the bit length of activations in each layer. The experimental result
    demonstrates that it is up to 20% to 40% compression rate for neural networks
    without losing accuracy. Besides, Zhu et al. [27] deployed an adaptive quantization
    technique to the neural networks, which affirmed that the model size and computation
    cost are decreased by utilizing CIFAR10, and ImageNet2012 dataset. An adaptive
    quantization technique proposed by Kwon et al. [28] was applied to transfer the
    trained weights to the synaptic devices, attaining up to 98.09% accuracy rate.
    However, as shown in Table I, retraining high-performance, complicated neural
    networks may need enormous infrastructure resources, consume a tremendous amount
    of time and money, and harm the environment with excessive carbon emissions, dramatically
    prolonging edge AI products'' research and development cycle [29], [30]. Our study
    concentrates on a novel quantization algorithm and hardware module for determining
    a low-precision fixed-point representation of floating-point numbers for post-training
    quantization to minimize the heavy resource consumption associated with retraining
    neural networks. TABLE I Retraining Cost for Different AI Models Moreover, many
    existing quantization strategies, quantizing the weights or feature maps across
    different layers and channels, have been introduced in previous studies [31],
    [32]. It implies that the representation formats for edge AI devices with multiple
    parallel PMs may vary for different input ports. Each fixed-point representation
    format requires at least one clock cycle to fulfill the data transfer, which raises
    data control complexity and system delay. On the other hand, current SoC designs
    typically utilize universal CPU modules in the market, and their output interfaces
    are usually flexible with a bus-based architecture. The bit adjustment circuitry
    for different input bits connected to the algorithmic accelerator (e.g., AI hardware
    accelerator) leads to the necessity of duplicating circuit interfaces and even
    redesigning their internal circuit architecture for each edge processor. Various
    representation formats of input bits probably require the design of different
    edge processors (EP-1, EP-2, or EP-3, as shown in Fig. 1), which will render the
    computational circuit modules nonreusable. An edge AI processor with a universal
    quantization circuit interface can efficiently improve the AI module''s utilization
    and significantly reduce the system''s design cycle. Embedding the quantization
    circuitry on the multiplier module (QoM) has the potential to conserve the CPU''s
    resource utilization. It is also a key to achieving a highly-integrated and compact
    SoC-based edge AI processor. In brief, the motivation of this proposal is summarized
    as follows. It is fundamental for industrial IoT applications to be able to quantize
    sensor data or DNNs'' parameters without sacrificing their performance or accuracy.
    The majority of industrial IoT applications need a substantial portion of CPU
    or microprocessor resources and time for data pre/postprocessing; thus, it is
    desirable to separate data quantization from the CPU or microprocessor. Retraining
    huge, sophisticated DNNs will cost a significant amount of time and money, and
    damage the environment; therefore, post-training quantization is an excellent
    alternative to prevent resource waste and accelerate the development process of
    edge AI processors. Existing quantization solutions tend to quantize sensor data
    or DNN models using a wide range of representation formats; thus, developing quantization
    modules with general-purpose capability is an efficient technique for shortening
    the design cycle of edge AI SoC. Motivated by the application requirements mentioned
    above, an adaptive quantization algorithm and the hardware architecture are expected
    to facilitate the progress of edge AI processors with a general-purpose interface
    (32-bit) for industrial IoT society. In summary, the contribution of this article
    is presented as follows. An efficient adaptive quantization algorithm that converts
    32-bit floating-point to arbitrary fixed-point representations is proposed, and
    the mathematical expression is derived and theoretically proved. This algorithm
    delivers efficient data format transformation for edge AI computing systems while
    assuring no loss of accuracy. The circuit architecture that embeds a bit-adjustment
    block into the quantization module is proposed for the first time, and its impact
    on the DNNs'' classification accuracy and estimation errors of the inertial navigation
    system is verified by the algorithm and gate-level simulation. The circuit that
    dynamically determines the integer bit length of fixed-point numbers can effectively
    improve the error resolution and reduce the overall latency of the edge AI system.
    A post-layout design and analysis of the quantization-on-multiplier module are
    presented developing with 55 nm processing technology, which exhibits greater
    energy efficiency and low latency in comparison to the benchmark quantization
    approach. The rest of this article is organized as follows. Section II presents
    the adaptive quantization algorithm and hardware circuit, including the software-oriented
    Q -format determination (Section II-A) and hardware-oriented quantization block
    (Section II-B). Section III illustrates the experimental results and discussion.
    Finally, Section IV concludes this article. SECTION II. Proposed Scheme A. Software-Oriented
    Q-Format Determination 1) Representation of Floating-Point Numbers A binary representation
    of floating-point numbers consists of sign bits, exponent bits, and mantissa bits,
    as shown in Fig. 2 [33]. The exponent part primarily controls the dynamic range
    of floating-point numbers, whereas the mantissa is principally responsible for
    their precision. The lengths of sign bits, exponent bits, and mantissa bits for
    single floating-point (32-bit) and double floating-point (64-bit) are {1, 8, 23}
    and {1, 11, 52}, respectively. Once these three parameters are specified, a floating-point
    value ( X float ) may be derived by X float =(−1 ) S × 2 E x − b i ×(1+ M a )
    , where b i represents the bias, which is 127 and 1023 for the single and double
    floating-point format, respectively. E x and M a are the exponent and mantissa,
    respectively. S is the sign bit. There is a “1” bit hidden (named “implicit leading
    one”) between exponent and mantissa, making the floating-point in the format of
    normalization, which is the reason that mantissa and 1 are added ( M a +1 ) in
    the last term of the abovementioned equation. It is noted that the hidden bit
    of mantissa occupies no memory space. Fig. 2. 32-bit floating-point representation
    consisting of one sign bit, eight exponent bits, and 23 mantissa bits. Show All
    2) Representation of Fixed-Point Numbers Fixed-point format, commonly referred
    to as Q -format, can be used to describe either a signed or unsigned two''s complement
    value. As shown in Fig. 3, a signed fixed-point value typically consists of a
    sign, an integer, and fraction bits. Fig. 3. Representation of a 32-bit fixed-point
    number. Show All For a fixed-point number, the bit length ( L b ) is defined by
    L b = L FI + L FR +1 , where L FI and F FR are the lengths of integer and fraction
    bits, respectively. Although fractional parts mostly dominate the resolution of
    fixed-point numbers, their dynamic range is determined by the integer component.
    Specially, the dynamic range of a fixed-point number ( X fixed ) is − 2 L FI ≤
    X fixed ≤ 2 L FI −ξ , where ξ is the resolution of the fixed-point number, which
    is written by ξ= 2 − L FR . Table II summaries the range of representation for
    a 16-bit fixed-point value. A straightforward method of representing fixed-point
    numbers is Q -format, which is described by Q( L FI . L FR ) or Q( L FR ) . The
    symbol “ ⋅ ” indicates the radix point. The fixed-point number depicted in Fig.
    3 can be expressed by Q(15.16) or Q(16) . The integer and fraction parts are split
    by an implied radix point that does not use any memory. The fixed-point number
    is stored in memory as a signed integer in the format of two''s complement, and
    the location of the radix point is tuned by a scaling factor, which equals 2 L
    FR . As shown in Fig. 3, the dynamic range and resolution of a 32-bit fixed-point
    number with 15-bit integer and 16-bit fraction is [− 2 15 , ( 2 15 − 2 −16 )]
    and 2 −16 , allowing to represent a fixed-point number from −32768 to 32767.99998474121
    with a resolution of 0.0000152587890625. Although Q (15.0) can represent any integer
    between − 32 768 and 32 767, its resolution is just “1.” Therefore, it is vital
    to maintain a balance between the dynamic range and the required resolution when
    using the fixed-point representation. TABLE II Representation Range of 16-Bit
    Fixed-Point Numbers The fixed-point numbers with various Q -format expressions
    corresponding to the same binary sequence in memory are always distinct. The fixed-point
    numbers are stored in memory with signed integers, and the conversion between
    the signed integer and the fixed-point number is approximated by the scaling factor
    ( 2 L FR ). For instance, the decimal number of 0xA069 with Q(0.15) is 101000000110
    1001 Q(0.15) =−0.746783 , which can be evaluated as X float =(−1 ) S × b N−1 ×
    2 L FI + ∑ L FI i=1 b N−1−i × 2 L FI −i + ∑ L FR j=1 b N−1− L FI −j × 2 −j , where
    b N is the N binary. The decimal number for the same binaries with different L
    FI are completely different, e.g., 101000000110 1001 Q(15.0) =−24471 . Consequently,
    it is crucial to exploit an optimal Q -format representation for fixed-point numbers
    in edge AI computing. 3) Determination of L FI This article proposes an adaptive
    quantization approach that flexibly evaluates the integer and fraction bits''
    length of floating-point numbers, as stated by the Theorem 1. Theorem 1: The integer
    bits length ( L FI ) of floating-point number ( X float ) is defined by the following
    equation: L FI = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ceil[ log 2 ( X float 1− 2 1− L b )] ceil[ log
    2 (− X float )] 0 if  X float ≥1− 2 1− L b if  X float ≤−1 others (1) View Source
    where the symbol ceil[∙] represents the ceil function which returns the minimum
    integer larger than or equal to the input, as defined by ⌈x⌉=min{n∈Z | n≥x} .
    Proof: The dynamic range of fixed-point values, − 2 L FI ≤ X float ≤ 2 L FI −
    2 − L FR , can be divided into the following two parts: − 2 L FI ≤ X float ≤0
    and 0≤ X float ≤ 2 L FI − 2 − L FR . The following inequalities can be obtained
    by rearranging these two expressions, X float ≥− 2 L FI when X float ≤0 and X
    float ≤ 2 L FI (1− 2 1− L b ) when X float ≥0 . From the first expression, it
    can be derived that L FI ≥ log 2 (− X float ) , X float ≤0 . In addition, since
    the integer bit length ( L FI ) should be larger or equal to zero, log 2 (− X
    float )≥0 , which indicates that X float ≤−1 . From the later inequality, it can
    deduced the solution range of L FI when X float ≥0 , L FI ≥ log 2 ( X float 1−
    2 1− L b ) . Similarly, it needs to meet the condition that X float 1− 2 1− L
    b ≥1 (i.e., L FI ≥0 ), which deduces that X float ≥1− 2 1− L b . In brief, the
    range of L FI is constrained by two inequalities, L FI ≥ log 2 (− X float ) if
    X float ≤−1 and L FI ≥ log 2 ( X float 1− 2 1− L b ) if X float ≥1− 2 1− L b .
    The L FI solution of inequality containing larger than constrain can be discovered
    by choosing its minimum integer, which can be achieved by using ceil function.
    Moreover, since the numbers with range of [−1,1− 2 1− L b ] are represented by
    Q(0.15) (see Table II), L FI is zero in this range. ■ Fig. 4 depicts the detailed
    L FI determination procedure, which demonstrates that the integer bit length of
    fixed-point representation is zero if the data points located in the range of
    [−1,1− 2 1− L b ] . If the data points fall outside this range, the L FI can be
    calculated by (1). Additionally, Fig. 5 provides a comparison between the log
    2 ( X float ) and the L FI calculation method, demonstrating that ceil function
    achieves the quantization of log 2 ( X float ) in L FI evaluation. In this experiment,
    40 data points, [ ±0.1,±0.125,±0.25,±0.5,±1,±2,±3,⋯±16 ], are used to simulate
    the adaptive L FI determination function. As an illustration, −0.746783 is represented
    by Q(0.15) since it locates between −1 and 0.999969482421875. As illustrated in
    (1), L FI of −2.89037 can be evaluated by L FI ←ceil[ log 2 (2.89037))]=2 . Once
    the L FI is fixed, conversion of a floating-point number to a fixed-point number
    can be accomplished by multiplying the floating-point number by the scaling factor,
    which is evaluated by X fixed =INT( X float × 2 L FR ) where INT(∙) indicates
    the function of rounding to the nearest integer number. As an example, the fixed-point
    number of −2.89037 can be calculated by −2.89037 Q(2.13) ←INT(− 2.89037 float
    × 2 13 )≈−23678 . Hence, fixed-point representation format of −2.89037 can be
    represented by the binary of 101000111000 0010 2 . Fig. 4. Adaptive L FI determination
    principle and assessment method. Show All Fig. 5. Implementation method of adaptive
    L FI determination: delivering the log 2 ( X float ) to the ceil function yields
    L FI . Show All B. Hardware-Oriented Quantization Block The SoC''s design concept
    aims to integrate the CPU, graphics processing unit (GPU), memory, and interfaces,
    among other components, onto a single silicon chip. A typical representative commercial
    product of SoC is the ARM-compatible CPU-based Raspberry-Pi series microcomputer,
    which employs the vector floating-point computation fully compliant with the IEEE
    stand for floating-point arithmetic (IEEE-754) [34]. Generally, the length of
    the sign bit is one, while the lengths of exponent and mantissa bits are different
    for various floating-point representation formats, as shown in Table III. TABLE
    III Parameters of Different Floating-Point Representations Currently, the CPUs
    of personal computers or cloud servers are represented by 64-bit or higher floating-point
    formats, whereas ARM or embedded computing units utilize 32-bit or lower representations.
    The following equation can represent a floating-point number X float = (−1 ) S
           sign × ( 2 E x − b i )          combination × (1+ M a
    )          significant . (2) View Source The floating-point number in
    (2) includes a sign bit, exponent, and mantissa (also named significant). The
    representation format of floating-point numbers is based on scientific notation
    (i.e., 1.23× 10 4 ) except for adopting different bases and coefficients. Equation
    (2) can be decomposed into the following three parts: sign determination ((−1
    ) S ) , combination field ( 2 E x − b i ) , and significant field (1+ M a ) ,
    corresponding to the scientific notation. In particular, a 32-bit floating-point
    binary has 1-bit sign: S b = X b f [0] , 8-bit exponent: E b x = X b fl [1:8],
    and 23-bit mantissa: M b a = X b fl [9:31]. The superscript “ b ” denotes the
    format of binary representation. Bit concatenation between 1 ′ b0 and mantissa
    based on the significant assessment ( 1+ M a ) in (2) is the initial step in achieving
    the QoM. Although the hidden bit is unavailable between exponent and mantissa,
    a 1 ′ b1 is concatenated before the mantissa''s most significant bit (MSB). The
    mantissa extension ( M EX a ) can be induced by M EX a ⟵{ 1’b1, M a } , where
    {A,B} specifies the concatenation between A and B . Moreover, considering from
    a circuit design perspective, the combination in (2) is the shift operation for
    the significant. The output of shift operation ( S out ) can be achieved by S
    out =(1+ M a )≪ ( E x − b i ) when E x > b i and S out =(1+ M a )≫( E x − b i
    ) when E x < b i , where “ ≪ ” and “ ≫ ” indicates left and right shift operations,
    respectively. It illustrates that calculating the output of shift operation contains
    the following two steps: estimating the number of shifted-bit ( N s ) and performing
    shift operation. The number of shifted-bit is computed algorithmically by subtracting
    E x and b i . Meanwhile, a selection circuit is also required to regulate the
    shift direction. To simplify the circuit design for S out , the QoM makes full
    of the exponent''s MSB to realize the determination of N s and right/left shifter
    control since the exponent''s MSB ( E MSB x ) can be served as an indicator for
    shift direction. The circuit design of QoM includes five primary parts: combination
    calculation, shift operation, L FI evaluation, sign determination, and bit adjustment.
    Taking the 32-bit floating-point number as an example, the significant will be
    shifted left and right if E MSB x =1 and E MSB x =0 since the bias ( 8 ′ b011111111
    ) is the boundary point of the left and right shift. As shown in the shift operation
    of Fig. 6, in this study, the enable signals of right shifter ( R EN ) is controlled
    by E MSB x through a not gate, which is described as: E MSB x =0→ R EN =1 . Fig.
    6. Schematic of the quantization and bit-adjustment blocks in the quantization
    module. MUX: multiplexer. Show All Moreover, an exponent mask is proposed to evaluate
    the number of shifted-bit by the exclusive or operation between E MSB x and b
    i , which is defined by E m = E MSB x ⊕ b i . The symbol “ ⊕ ” represents bit-wise
    exclusive or operation. The function of E m is utilized to filter out the N s
    by the following expression: N s = = E m ⊕ E x + E MSB x ( E MSB x ⊕ b i )⊕ E
    x + E MSB x . (3) View Source Substituting the bias ( 0x7F ) and exponent''s MSB
    ( 1 ′ b0 or 1 ′ b1 ) into (3), yields the following specific equation: N s ={
    8 ′ b01111111⊕ E x 8 ′ b10000000⊕ E x +1 if  E MSB x =0 if  E MSB x =1 . (4) View
    Source The exclusive or result between E m and the minimum exponent of the left
    shift ( 0x80 ) is 8 ′ b0 , while the number of left shifts should be 1 ′ b1 .
    Thus, the N s is the sum of E m and E MSB x . The combination calculation part
    in Fig. 6 illustrates the detailed circuit design of N s s evaluation, which is
    a combinational circuit using two xor gates and an adder. The E MSB x is utilized
    to determine the enable signals of right shifter ( R EN ), whereas N s represents
    the number of shifts. Fig. 7 shows the QoM''s shift technique. The bit length
    following right shift operation ( E MSB x =0 ) remains unaltered ( N s ) since
    the low-bit removed by right shift will be supplemented in high-bit with “0.”
    On the other hand, the shift output ( S out ) of N s +1 bits “0” at low-bit will
    be increased by the left shift operation. Since the left shift will not change
    the binaries of S out , the left shifter is not employed in the circuit architecture
    design; instead, the binaries are directly intercepted from M EX a to achieve
    quantization. In brief, the bit length of S out ( L S out ) is determined by L
    S out = L M EX a when E MSB x =0 and L S out = N s + L M EX a +2 when E MSB x
    =1 , where L M EX a is the bit length of mantissa extension that is 24-bit (23-bit
    mantissa and 1 hidden bit) for 32-bit floating-point representation. The S out
    is an unsigned binary selecting by E MSB x with a multiplexer. In addition, since
    the M EX a does not include any sign information of the floating-point number,
    which is considered an unsigned binary, 1 ′ b0 is concatenated before the M EX
    a to avoid introducing sign errors when E MSB x = 1 ′ b0 is also concatenated
    behind the least significant bit of S out to make the bits length of shifted binary
    the same. As described in Fig. 7, the output of shift operation ( S shift out
    ) is written by follows: S shift out ={ { S out , 1 ′ b0} { 1 ′ b0, M EX a } if  E
    MSB x =0 if  E MSB x =1 . (5) View Source The sign of a fixed-point number is
    also dominated by the sign bit of floating-point representation ( S b ). Since
    original and 2s complement codes represent the positive and negative numbers in
    a digital circuit, respectively, a combinational circuit with an xor gate and
    an adder is proposed to fulfill the sign determination of fixed-point representation
    in this work. Specifically, the 32-bit fixed-point representation ( X 32 fi )
    can be deduced by the following expression: X 32 fi ={ S out ∼ S out +1 if  S
    b =0 if  S b =1 (6) View Source where the symbol “ ∼ ” represents the not operation.
    Substituting S b into the abovementioned equation and rearranging it, the sign
    determination circuit for 32-bit fixed-point representation can be designed under
    the following expression: S shift out − → − − S b =0 Positive S shift out − →
    − − S b =1 Negative ⎫ ⎭ ⎬ ⎪ ⎪ ⟶ X 32 fi = S b + S shift out ⊕ S b . (7) View Source
    Since shift operation also modifies the radix point''s location, the L FI of fixed-point
    is also related to the number of shifts ( N s ). The right shift operation denotes
    that the floating-point number is less than zero, which deduces that L FI is zero.
    Owing to the existence of hidden bit, the L FI must be one regardless of right
    or shift operation when N s equals zero. Moreover, it moves the mantissa left
    by N s +1 bits when E MSB x =1 , which also increases the L FI by N s +1 . It
    can be concluded that the L FI is determined by N s and shift direction ( E MSB
    x ) and the following expression illustrates the relationship between L FI and
    ( N s , E MSB x ): L FI = ⎧ ⎩ ⎨ 1 0 N s +1 if  N s =0, E MSB x =0,1 if  N s ≠0,
    E MSB x =0 if  N s ≠0, E MSB x =1 . (8) View Source Table IV presents the detailed
    truth table for the parameters of QoM''s quantization block shown in Fig. 6. The
    nor gate is utilized to realize the L FI evaluation when N s is zero, which ensures
    that the L FI =1 whether E MSB x is “0” or “1.” At the same time, the and gate
    is selected to meet the requirement that the L FI =1 following the right shift
    operation ( E MSB x ). The and operation between the output of or gate and E MSB
    x provides 1 ′ b0 for L FI evaluation ( L FI = N s  + “1”) when implementing the
    left operation ( N s ≠0 , E MSB x =1 ). Under the abovementioned three conditions,
    a combinational logic circuit is proposed to realize the L FI calculation in this
    article (refer to Fig. 6). TABLE IV Truth Table for the Parameters in Quantization
    Block Fig. 7. Bit-adjustment of QoM''s quantization block with shifter. Show All
    SECTION III. Results and Discussion This section provides an experimental assessment
    of the proposed quantization technique, including algorithm simulations, gate-level,
    and postlayout verification. Using the strapdown inertial navigation system (SINS)
    estimation errors and different DNNs'' benchmark models (MobileNet, VGG16, VGG19,
    ResNet50, DenseNet121, and Tiny YOLO3), the effectiveness of the proposed quantization
    algorithm and module is studied. A. Algorithm Evaluation of Proposed Quantization
    Method 1) Case Study 1. Position, Velocity, and Attitude Errors of SINS In today''s
    industrial society, autonomous or unmanned aerial vehicles mainly rely on inertial
    sensors (gyroscopes and accelerometers) to resolve their location or orientation.
    Conducting SINS mechanization on the semiconductor is a promising way to improve
    its overall latency, power consumption, and throughput. Considering that the estimate
    errors of SINS (position, velocity, and attitude) are sensitive to data noise,
    the quantization error of the SINS chip''s inputs will significantly impact its
    estimation error. Therefore, this section aims to validate the accuracy of the
    proposed quantization method by analyzing the estimation errors of SINS. In this
    simulation, the proposed quantization approach is utilized to convert the angular
    rate and specific force of 3-axis gyroscopes and 3-axis accelerometers to 16-bit
    fixed-point representation formats. Accelerometers have a bias instability and
    velocity random walk of 5 × 10 −5 m/s 2 and 0.0005 m/s/ hr − − √ , whereas the
    bias instability and angle random walk of gyroscopes are set to 1.6968 × 10 −5
    ∘ /hr and 7.2722 × 10 −5 ∘ / hr − − √ , respectively. The data generation period,
    SINS frequency, and Earth''s gravity are 80 s, 100 Hz, and 9.826357. Fig. 8 illustrates
    the position, velocity, and attitude errors of SINS using different representation
    formats in the north, east, and down coordinates. Comparing Q(8.7) ( L FI =8 )
    to the other fixed-point representation formats, this finding shows that the northern
    and eastern locations suffer from large errors. Since the position solution proportionally
    scales as the velocity increases, huge errors are observed in the northern and
    eastern velocities ( V North and V East ) when L FI =8 . Additionally, the Q(8.7)
    fixed-point representation format of gyroscopes and accelerometers induces a severe
    drift in SINS yaw, pitch, and roll estimation errors. The primary factor contributing
    to the SINS'' position, velocity, and attitude estimation errors is the quantization
    rounding error in the fixed-point representation format of Q(8.7) . There are
    two primary reasons why the estimation error with Q(4.11) representations is lower
    than Q(8.7) : 1) Q(4.11) is a representation format with a much higher resolution
    than Q(8.7) , owing to its additional four fractional bits. 2) Since the specific
    force measured by the accelerometer in the downward direction for a stationary
    or moving object in the northern or eastern direction is represented by Q(4.11)
    (Earth''s gravity: − 9.826357), deploying the format of Q(4.11) to represent the
    sensors'' inputs causes small quantization errors. In contrast to Q (4.11) and
    Q (8.7) representation formats selected arbitrarily, our proposed quantization
    technique can adaptively explore the data''s L FI , assuring adequate fractional
    bits to retain its resolution. Fig. 8. Estimation errors of position, velocity,
    and attitude with the proposed quantization method. Show All The SINS estimation
    errors accumulate over time if no external sensors, such as GNSS or magnetometers
    are employed for alignment. The maximum estimation errors at the 80-s epoch are
    selected to validate the effectiveness of the proposed quantification approach
    in this experiment. As illustrated in Fig. 9, the maximum eastern position errors,
    maximum eastern velocity errors, and maximum roll errors with Q(8.7) representation
    formats are 71.180791 m, 0.023360 m/s 2 , and 0.005484 ∘ , which are 39×, 6×,
    and 64× larger than the estimation errors assessed using our quantization methods
    (1.831210 m, 0.004247 m/s 2 , and 8.510073 × 10 −5 ∘ ), respectively. In brief,
    the experimental results indicate that the adaptive quantization algorithm proposed
    in this article appropriately restrains the SINS estimation errors introduced
    by quantization rounding errors. A future trend in developing SINS chips with
    low latency, low power consumption, and high throughput features will be incorporating
    the quantization module into the circuitry. Fig. 9. Analysis of maximum estimation
    errors of position, velocity, and attitude at 80 s. Show All 2) Case Study 2.
    Performance Analysis of DNNs As shown in Fig. 10, six different parameters'' L
    FI distribution of benchmark DNNs are offered. In this experiment, the L FI is
    calculated based on (1), and the maximum L FI for each DNN is marked in the results
    as well. More specifically, the maximum L FI for MobileNet, VGG16, VGG19, ResNet50,
    DenseNet121, and Tiny YOLO3 are 9, 4, 4, 15, 6, and 9, respectively. It demonstrates
    that most of the parameters for the benchmark DNNs can be represented by Q(0.15)
    format, accounting for 99.26116%, 99.99998%, 99.99998%, 99.898583%, 99.99277%,
    99.904857% for MobileNet, VGG16, VGG19, ResNet50, DenseNet121, and Tiny YOLO3,
    respectively. In light of the fact that the largest L FI is 15, as denoted in
    the distribution diagram, the 16-bit fixed-point representation format is employed
    in this study to illustrate the effectiveness of the quantization algorithm and
    module in avoiding errors induced by numerical representation overflow. Fig. 10.
    Parameters'' L FI distribution of different neural networks. N p is the number
    of parameters with corresponding L FI . The red dashed line indicates the maximum
    L FI . Show All The quantization performance is assessed using the average conversion
    error ( δ ¯ ¯ ), which is defined by δ ¯ ¯ = 1 N ∑ N i=1 ( X 16 float − X 32 float
    ) 2 − − − − − − − − − − − − − √ , where N is the number of parameters and 16,
    32 represent the number of bits. Fig. 11 illustrates the conversion error of 16-bit
    quantization using the proposed algorithm [refer to (1)]. The CPU algorithm-based
    average conversion errors for the parameters of MobileNet (16.2272 Mb), VGG16
    (527.7921 Mb), VGG19 (548.0470 Mb), DenseNet121 (30.7560 Mb), ResNet50 (97.7963
    Mb), and Tiny YOLO3 (33.7934 Mb), are 6.305 × 10 −9 , 7.488 × 10 −10 , 7.348 ×
    10 −10 , 3.0460 × 10 −9 , 3.909 × 10 −8 , 9.455 × 10 −11 , respectively. ResNet50
    achieves a maximum conversion error of 3.909 × 10 −8 in this simulation. In addition,
    the total and average quantization time of CPU-based (Intel i9-11950H@2.60 GHz,
    160 GB RAM) conversion is roughly 47 (11.07 μ s), 1680 (12.14 μ s), 1812 (12.62
    μ s), 85 (10.55 μ s), 325 (12.66 μ s), and 113 (12.79 μ s) seconds for MobileNet,
    VGG16, VGG19, ResNet50, DenseNet121, and Tiny YOLO3, respectively. Due to the
    serial nature of CPU operation, as shown in Fig. 11, the running time required
    increases with the number of parameters. Nonetheless, the quantization time of
    each parameter for these benchmark DNNs is determined to be between 10 and 12
    μ s. In brief, the quantization error of the proposed framework is on the order
    of 10 −8 and the average quantization time for each parameter is approximately
    11 μ s. Fig. 11. Experimental results of algorithm evaluation. (a) Conversion
    error with different types of neural networks. (b) Size of parameters for each
    neural network. (c) Execution time. (d) Execution time per parameter. Abbreviation:
    MN–MobileNet, VG16–VGG16, VG19–VGG19, DN–DenseNet121, RN50–ResNet50, TY3–Tiny
    YOLO3. Show All B. Module Verification According to the hardware architecture
    proposed in Fig. 6, in this section, a hardware quantization module is designed
    with 32-bit inputs ( X float ), 16-bit ( X fixed ), and 8-bit ( L FI ) outputs
    using the Verilog programming language and evaluated utilizing gate-level and
    postlayout simulation. 1) Postsynthesis Simulation The Fujitsu 55-nm technology
    library is utilized to synthesize the proposed hardware quantization module with
    Synopsys Design Compiler software, and the gate-level simulation is conducted
    with Synopsys VCS software. A gate-level function simulation is accomplished by
    practical decimal parameters (1.664306640625, 16.2880859375, 72.16796875, 9.3447265625)
    from MobileNet. The output contains 25 bits, and the simulation clock is 100 MHz.
    Since it is to validate the functional correctness of the designed circuit, all
    25-bit outputs are provided in this simulation stage. Nevertheless, the actual
    output of the quantization module is 16-bit, which extracts the first 16-bit from
    the 25-bit outputs. As shown in Table III, the bias of 32-bit floating-point representation
    format is 127 (8’h7F). Given that the circuit requires one clock to transfer the
    data to the output register, there is no output on the first clock. Fig. 12 provides
    some simulation examples, and it demonstrates that all outputs of L FI locates
    the correct representation range (compare to Table II). In addition, the experimental
    result illustrates that the fixed-point representations are roughly equivalent
    to the 32-bit floating-point inputs in the postsynthesis simulation. The 25-bit
    fixed-point representations can be converted to the 32-bit decimal numbers employing
    the 8-bit output L FI as follows: 25 ′ h0D5_0 779 L FI =1 25 ′ h082_4D 11 L FI
    =5 25 ′ h090_56F 5 L FI =7 25 ′ h095_8 308 L FI =4 =1.664245605 =16.28710938 =72.16796875
    =9.344238281 ≈1.664306641 ≈16.28808594 =72.16796875 ≈9.344726563. View Source
    In the postsynthesis simulation of the quantization module, it is capable of demonstrating
    that the quantization error is negligible or even equal to zero. In order to further
    validate the performance of the proposed module, each parameter from various neural
    networks is fed into the quantization module to fulfill the quantization from
    32-bit floating-point to 16-bit fixed-point representations. Fig. 12. Gate-level
    simulation example of the proposed quantization module. Show All Fig. 13(a) compares
    conversion errors for the 32-bit floating-point CPU-based algorithm [see (1)]
    and the 16-bit fixed-point device-under-test (DUT)-based module (refer to Fig.
    6). Similarly, we utilize Synopsys VCS software to conduct the simulation. The
    average conversion errors with DUT-based module are 5.25753265 × 10 −7 , 1.49283803
    × 10 −9 , 1.46485530 × 10 −9 , 8.26253938 × 10 −8 , 6.16297378 × 10 −9 , 1.13068143
    × 10 −7 for MobileNet, VGG16, VGG19, ResNet50, DenseNet121, and Tiny YOLO3, respectively.
    MobileNet delivers the highest DUT-based conversion error of 5.25753265 × 10 −7
    , mainly caused by the rounding error that happens while converting from 32-bit
    floating-point to 16-bit fixed-point integers. The difference in conversion errors
    between CPU-based and DUT-based simulation is 5.19448094 × 10 −7 , 7.44086534
    × 10 −10 , 7.30024145 × 10 −10 , 4.35357143 × 10 −8 , 3.11699810 × 10 −9 1.09801103
    × 10 −7 , as illustrated in Fig. 13, and the maximum δ ¯ ¯ for both CPU-based
    and DUT-based quantization are only on the order of 10 −8 and 10 −7 , respectively.
    It can be proved that the DUT-based simulation produces rounding errors on the
    order of 10 −7 . In conclusion, the proposed quantization algorithm and module
    exhibit excellent low conversion error features. As shown in Fig. 13(b), the difference
    of conversion errors between CPU-based and DUT-based simulation is 5.19448094
    × 10 −7 , 7.44086534 × 10 −10 , 7.30024145 × 10 −10 , 4.35357143 × 10 −8 , 3.11699810
    × 10 −9 1.09801103e × 10 −7 , and the maximum δ ¯ ¯ for both CPU-based and DUT-based
    quantization are only on the order of 10 −8 and 10 −7 , respectively. It can be
    demonstrated that the DUT-based simulation introduces errors on the order of 10
    −7 on account of the rounding errors. In summary, both the proposed quantization
    algorithm and module exhibit excellent low conversion error properties. Fig. 13.
    Conversion error of neural networks'' parameters with adaptive quantization module.
    (a) Comparison of conversion errors with CPU-based and DUT-based simulation. (b)
    Difference of conversion error between CPU and DUT-based simulation. Show All
    In addition, the ImageNet-2012 validation dataset, including 50 000 images, is
    deployed to assess the performance of the proposed quantization algorithm and
    module on the benchmark neural networks. Likewise, each parameter of neural networks
    is quantized with the proposed CPU-based algorithm and DUT-based module and converted
    to 32-bit floating-point representations to evaluate the neural networks'' top-1
    and top-5 accuracy. Table V provides a detailed accuracy comparison of neural
    networks between CPU-based algorithm, DUT-based module, and ground truth accuracy.
    It illustrates that the simulation results using both CPU-based quantization algorithm
    and DUT-based quantization module are almost identical to the state-of-the-art
    DNNs'' top-1 and top-5 accuracies. Fig. 14 further contrasts the DNNs'' accuracy
    using the CPU-based algorithm and DUT-based module, indicating that the top-1
    and top-5 accuracy are equivalent with the exception of ResNet50. Specifically,
    the maximum top-1 and top-5 differences between CPU and DUT are 0.01442 and 0.00706,
    respectively, from the ResNet50. The average accuracy for the corresponding DNNs
    is 0.725488, 0.722668, 0.7258, 0.907640, 0.906212, and 0.908, implying that the
    CPU-based quantization algorithm and DUT-based quantization module will not degrade
    the benchmark DNNs'' top-1 and top-5 accuracy. TABLE V Accuracy Comparison of
    Neural Networks Fig. 14. Accuracy comparison of neural networks. (a) Accuracy
    of neural networks with CPU and DUT simulation. (b) Accuracy difference between
    CPU and DUT simulation. (c) Average accuracy for different neural networks. Show
    All Fig. 15 depicts the accuracy drop comparison of DNNs between our proposed
    method and the benchmark quantization approaches. The experimental results, as
    illustrated in Fig. 15(a), reveal that the maximum top-1 and top-5 accuracy drop
    ratios of ResNet50 for our work ([ − 0.0399%, − 0.062%]) are 61 times and 21 times
    lower than row-ELT ([2.4%, 1.3%]), respectively. Nonetheless, Fig. 15(b) exemplifies
    that the accuracy drop of our proposed quantization technique ([ − 0.058, − 0.048])
    is 7.7 times and 7 times smaller than that of M4E3 and DPF16, respectively. In
    contrast, the accuracy drop rates of top-1 and top-5 of VGG16 are the largest
    for these approaches ([0.33%, 0.29%]). In brief, the adaptive quantization approach
    reported in this article has a negligible impact on the classification accuracy
    of the neural networks compared to the benchmark techniques. Fig. 15. Accuracy
    drop comparison of (a) ResNet50 and (b) VGG16 with benchmark quantization methods.
    Show All 2) Postlayout Analysis A QoM module is designed to precisely analyze
    the power consumption of the proposed quantization approach. Specifically, the
    QoM consists of two quantization modules and a multiplier, and the quantization
    module is used to quantize the multiplier''s inputs. Similarly, this experiment
    deploys the library exchange format file of Fujitsu 55-nm technology to layout
    and route the quantization module using Cadence Innovus software. Table VI specifies
    the place and layout specification of QoM module, which has five routing layers.
    TABLE VI Place and Route of QoM Module With Innovus The power rings are located
    on the fourth (vertical) and fifth (horizontal) layers and their widths are 3
    μ m and 1.8 μ m, respectively. The floorplan widths and heights for the multiplier,
    quantization-1, and quantization-2 are (83.16 μ m, 47.88 μ m), (40.86 μ m, 30.24
    μ m), and (41.4 μ m, 30.24 μ m), correspondingly. The target utilization for the
    multiplier, quantization-1, and quantization-2 are 82.6%, 52.6%, and 51.9%, respectively,
    and the effective utilization of the QoM module is 74.03%. This simulation sets
    the frequency, power supply, and temperature to 200 MHz, 1.2 V, and 25 ∘ C , respectively.
    Under the abovementioned parameter constraints, the whole module is synthesized
    with 6745 logic gates (1906 cells). Moreover, the area of QoM''s cells, core and
    chip in this design are 4589.298 μ m 2 , 6658.394 μ m 2 , and 10356.142 μ m 2
    , respectively. Specifically, the cells area of the quantization module and multiplier
    are 649.552 μ m 2 and 3290.1876 μ m 2 , respectively. The area of each quantization
    module accounts for 14.15% of the total QoM''s area. Furthermore, the QoM''s power
    consumption is 2.099 mW, which is composed of three modules: quantization-1 (0.107
    mW), and quantization-2 (0.1056 mW), and multiplier (1.887 mW). As shown in Fig.
    16(a), the power consumption of each quantization module and multiplier of QoM
    accounts for 5% and 90%, respectively. In addition, the QoM circuit is essentially
    composed of combinational logic and consumes 95.03% of the module''s power consumption
    [refer to Fig. 16(b)]. As illustrated in Fig. 16(c), the QoM''s power consumption
    contains internal power (0.7018 mW), switching power (1.392 mW), and leakage power
    (0.005165 mW), in which the switching power dominates QoM''s power consumption
    (account for 66.32%). The quantization module''s internal, switching, and leakage
    power, shown in Fig. 16(d), (e), and (f), are 69.82%, 92.43%, and 55.30% lower
    than those of the multipliers, respectively. Fig. 16. Percentage of QoM''s power
    consumption according to (a) module types, (b) circuit types, (c) power types,
    (d) internal power, (e) switching power, (f) leakage power. Abbreviations: QM
    → Quantization Module, Comb → Combinational Circuit, Seq → Sequential Circuit,
    Mul → Multiplier. Show All Embedding the quantization module into the multiplier
    eliminates the time delay caused by quantization calculations, i.e., each quantization
    calculation takes only five nanoseconds at an operating frequency of 200 MHZ.
    Quantization of neural networks in CPU or GPU raises the time consumption of the
    entire computing module, which in turn contributes to an increase in system latency.
    Employing Tensorflow [35] and PyTorch [36] benchmark quantization techniques,
    Fig. 17 illustrates a running time assessment with a 2.61 GHz CPU. The simulation
    results demonstrate that the maximum quantization time for each parameter in Tensorflow
    and PyTorch is 5715.99 ns and 71.65 ns, respectively, which is 1142 times and
    13 times slower than the proposed DUT module (5 ns). Although the amount of time
    required for quantization computing varies across quantization frameworks, nonembedded
    quantization computation needs time to support its operations. In contrast, the
    embedded quantization module proposed in this research can eliminate the time
    delay associated with quantization computation. Fig. 17. Running time evaluation
    of different neural networks with different post-training quantization methods.
    Show All SECTION IV. Conclusion This article proposes an adaptive L FI determination
    algorithm and an energy-efficient quantization module for IoT location and classification.
    Both the CPU-based quantization algorithm and the DUT-based quantization module
    are simulated. The experimental results demonstrate that both benchmark DNNs''
    classification accuracy and SINS'' estimation errors exhibit approximately the
    same accuracy with floating-point formats, whether utilizing the CPU-based algorithm
    or the DUT-based module simulations. Moreover, a quantization module-based QoM
    is designed for the first time to validate the performance of the proposed quantization
    method. The simulation result reveals that the proposed quantization module''s
    power consumption and cell area are only 0.1 mW and 649.552 μ m 2 , respectively.
    The running time of the on-chip quantization module is up to 1142 times faster
    than the current off-chip quantization strategies, proving its remarkable potential
    for designing energy-efficient and compact edge AI chips for industrial IoT applications.
    In the future, this article will focus on investigating an adequate and well-optimized
    circuit architecture for industrial IoT positioning and real-time object detection
    applications. Future case studies are expected to explore the compatibility of
    brain-inspired computing algorithms (such as spiking neural networks) with the
    adaptive quantization algorithms and modules proposed in this study, as well as
    to design an energy-efficient brain-inspired chip based on the quantization module
    and nonvolatile memory to promote brain-inspired computing in the application
    of industrial IoT. Authors Figures References Citations Keywords Metrics Footnotes
    More Like This Modeling of gap sensor for high-speed maglev train based on fuzzy
    neural network 2011 Eighth International Conference on Fuzzy Systems and Knowledge
    Discovery (FSKD) Published: 2011 Development of Input Training Neural Networks
    for Multiple Sensor Fault Isolation IEEE Sensors Journal Published: 2022 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Industrial Informatics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'From Algorithm to Module: Adaptive and Energy-Efficient Quantization Method
    for Edge Artificial Intelligence in IoT Society'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Song W.
  citation_count: '0'
  description: Visual crowdsensing (VCS) is becoming predominant in mobile crowdsensing,
    but there still exist various unique challenges, including large sizes of visual
    data, multidimensional requirements, and intensive processing demands. As a key
    research problem in VCS, data selection filters out redundant data and only retains
    most representative samples, which can effectively reduce the complexity and cost
    for VCS. In this paper, we study a phase-by-phase data selection approach, in
    which metadata are first used to pre-select collected photos and then only selected
    ones are sent to a backend server for further processing based on content features.
    As such, the initial selection can be completed on nearby edge servers in mobile
    edge computing (MEC), while more intensive content processing can be done in a
    remote cloud. We evaluate different initial data selection algorithms using traditional
    performance measures as well as adapted clustering indices as quality metrics.
    Moreover, we formulate an integer linear program (ILP) problem for the final data
    selection based on the scale-invariant feature transform (SIFT) feature. This
    content-based selection can complement the initial data selection based on contextual
    metadata. The simulation results show the differences of these selection algorithms
    and provide guidance on how to choose an appropriate one according to application
    needs.
  doi: 10.1109/OJVT.2022.3205422
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Open Journal of Vehicula...
    >Volume: 3 Hybrid Data Selection With Context and Content Features for Visual
    Crowdsensing Publisher: IEEE Cite This PDF Wei Song All Authors 239 Full Text
    Views Open Access Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Related Works III. System Modelling IV. Problem Analysis and
    Solution V. Simulation Results Show Full Outline Authors Figures References Keywords
    Metrics Abstract: Visual crowdsensing (VCS) is becoming predominant in mobile
    crowdsensing, but there still exist various unique challenges, including large
    sizes of visual data, multidimensional requirements, and intensive processing
    demands. As a key research problem in VCS, data selection filters out redundant
    data and only retains most representative samples, which can effectively reduce
    the complexity and cost for VCS. In this paper, we study a phase-by-phase data
    selection approach, in which metadata are first used to pre-select collected photos
    and then only selected ones are sent to a backend server for further processing
    based on content features. As such, the initial selection can be completed on
    nearby edge servers in mobile edge computing (MEC), while more intensive content
    processing can be done in a remote cloud. We evaluate different initial data selection
    algorithms using traditional performance measures as well as adapted clustering
    indices as quality metrics. Moreover, we formulate an integer linear program (ILP)
    problem for the final data selection based on the scale-invariant feature transform
    (SIFT) feature. This content-based selection can complement the initial data selection
    based on contextual metadata. The simulation results show the differences of these
    selection algorithms and provide guidance on how to choose an appropriate one
    according to application needs. Published in: IEEE Open Journal of Vehicular Technology
    ( Volume: 3) Page(s): 426 - 440 Date of Publication: 27 October 2022 Electronic
    ISSN: 2644-1330 DOI: 10.1109/OJVT.2022.3205422 Publisher: IEEE Funding Agency:
    CCBY - IEEE is not the copyright holder of this material. Please follow the instructions
    via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles
    and stipulations in the API documentation. SECTION I. Introduction Mobile crowdsensing
    (MCS) is a cost-effective approach for data collection [1]. It leverages smart
    devices'' built-in sensors and the inherent mobility of device holders to obtain
    comprehensive knowledge of interesting targets. In particular, visual crowdsensing
    (VCS) allows a large group of mobile users to share visual data (in the form of
    photos and videos) acquired by their devices [2]. User-contributed data can be
    further aggregated to generate insights with greater breadth and depth. With the
    benefits of low costs, high scalability, and high energy efficiency, VCS finds
    wide applications in virtual tours, smart cities, environment monitoring, emergency
    management, and disaster relief [2]. For example, smart vehicles or unmanned vehicles
    can be used to collect photos of certain regions or road segments in special events.
    Though visual data provide rich information, their large sizes and multidimensionality
    can cause overwhelming demands for processing and transmission [2]. It is also
    highlighted in [2] that if redundant or irrelevant data can be filtered out and
    only most representative samples are retained, it can effectively reduce the complexity
    and cost for VCS. This data selection problem is a key research problem in VCS.
    In the literature, there have been many existing studies in this area such as
    [3], [4], [5]. In these works, both context and content features have been considered
    in similarity and redundancy measurements for data selection. For example, the
    work in [3] focuses on context features also known as metadata. In [4], [6], context
    and content features are jointly considered. In fact, the metadata and the actual
    visual data are complementary. On one hand, a variety of metadata can be accessed
    easily from a sensor-rich smart device or directly provided by users for a captured
    photo, such as the location, shoot angle, view coverage, and timestamp. These
    lightweight metadata can be processed locally. On the other hand, visual content
    features provide richer information than contextual metadata but require more
    computational resources for processing. In addition, the transmission of content
    data to a remote server can be costly and time-consuming. To compare and relate
    visual objects more effectively, the raw pixel data from the visual contents are
    often processed to derive new features, such as color histograms and features
    from scale-invariant feature transform (SIFT) [7]. A content-based method needs
    to extract such features from the visual contents and further makes use of them
    for classification, detection, selection or other tasks. The feature extraction
    and comparison are often computationally intensive, especially, when a large number
    of visual objects are involved. Therefore, a content-based method may not be responsive
    enough to some time-critical sensing tasks. In the literature, some existing works
    consider both contextual metadata and visual content data together in data selection
    during the participants-to-server stage (e.g., [8]) or the server-to-requester
    stage (e.g., [9]). They often require the participants to upload the collected
    photos or their thumbnails to the server, so that the visual features are extracted
    and utilized in photo selection. In this paper, we adopt a phase-by-phase data
    selection approach, in which metadata are first used to pre-select crowdsourced
    photos. As the metadata sizes are very small and the pre-processing with only
    metadata is not computationally intensive, this can be completed on nearby edge
    servers in mobile edge computing (MEC). After that, only selected data samples
    will be sent to a backend server, which may be hosted in a remote cloud, for further
    processing based on content features. With this phase-by-phase approach, the close
    proximity of distributed edge servers can enable real-time data analysis for better
    timeliness than one-time centralized processing at a remote backend server. Specifically,
    the main contributions of this work are summarized as follows: We consider a phase-by-phase
    data selection framework, which first pre-screens photos with validity constraints,
    then selects promising photo candidates based on context metadata, and last finalizes
    selection leveraging features extracted from visual contents. For the initial
    data selection based on metadata, we extend the approaches in [3], [4]. Also,
    for comparison purpose, we consider a clustering-based benchmark approach which
    is often used in the literature to deal with redundant data. Unsupervised clustering
    can group data samples according to their features so that close samples are placed
    into the same cluster but separated from other distant samples. If each photo
    in the data selection problem is considered as a data sample in clustering, we
    can group all photos into clusters and select the most representative photo from
    each cluster. Each selected photo is expected to be similar to the rest of photos
    in the same cluster but dissimilar to other photos in different clusters. Thus,
    only these selected photos are retained and others can be screened out. We evaluate
    the performance of several data selection methods on various metrics. The simulation
    results demonstrate the performance of the methods in different aspects. Existing
    studies on data selection often use customized quality metrics, such as k -depth
    coverage [10], quality-aware coverage [11], and temporal-spatial coverage [12].
    In addition to such traditional metrics, we also exploit a variety of clustering
    indices [13] for quality validation and comparison, which is a side benefit of
    mapping the data selection problem to a clustering problem. Compared with these
    customized quality metrics, the clustering indices can better integrate the multidimensional
    attributes of visual data in both the context and content domains and evaluate
    the overall performance more comprehensively. Last, we formulate an integer linear
    program (ILP) problem for the final data selection based on visual features. A
    heuristic algorithm is developed to solve a large-scale instance of the ILP problem.
    The performance of the heuristic algorithm is validated by comparison with that
    of the optimal solution. The rest of the paper is organized as follows. In Section
    II, we explore the related works. In Section III, we present the system model
    and the data selection problem under study. Section IV further analyzes the problem
    and gives several candidate solutions. Section V evaluates these solutions and
    discusses observations from the simulation results. Last, Section VI concludes
    this paper. SECTION II. Related Works Compared to traditional MCS, VCS needs to
    tackle some unique challenges, such as large data amounts, multidimensional coverage
    requirements, and complex quality assessments. Consequently, there are some key
    research problems in VCS, such as diversity-oriented task allocation, efficient
    data transmission, and representative data selection [2]. In [14], the authors
    studied how to assign workers to tasks to maximize completion reliability and
    the spatial/temporal diversities of tasks. In particular, this work defines spatial
    and temporal diversities in terms of entropies and combines them with a weighted
    sum. The formulated problem is proved to be NP-hard and solved by using effective
    approximation approaches, including the greedy, sampling, and divide-and-conquer
    approaches. Similar temporal-spatial constraints are considered in [12] for multi-task
    allocation. It defines the temporal-spatial coverage to measure the target sensing
    quality and further evaluates the overall utility accordingly. A descent greedy
    approach is adopted to determine the task-worker pairs so that the overall utility
    is maximized while individual task quality is assured in the meantime. Data transmission
    is also essential to VCS due to the inherently large sizes of visual data. A variety
    of communication techniques have been considered to address the transmission challenges
    such as opportunistic transmission [15], MEC [16], and disruption-tolerant networks
    (DTNs) [6]. In [15], the authors proposed a cooperative and selective picture
    forwarding framework, called CooperSense. In this framework, a tree model called
    PicTree is used to structure the picture collection from a crowdsensing participant
    based on the metadata of pictures. When two participants encounter, they exchange
    their PicTrees at a low transmission cost and then select high-quality pictures
    by merging these PicTrees. Thus, they only need to selectively forward certain
    useful pictures to each other. In [16], the authors proposed a solution where
    a worker can upload crowdsourced data by leveraging the redundant resources of
    edge nodes in MEC. Due to the large sizes of visual data, the solution incorporates
    collaborations among multiple edge nodes to satisfy the transmission demand of
    one worker. Meanwhile, it requires that the data held by a worker be uploaded
    completely or not transmitted at all, since part of a captured photo may not provide
    useful knowledge. As this data transmission problem is proved to be NP-hard, an
    efficient method based on Lagrangian relaxation is used to obtain an approximate
    solution. Furthermore, data selection is another key research problem for VCS.
    In [3], the authors proposed a framework, called SmartPhoto, to assess crowdsourced
    photos based on only metadata and select a given number of photos to maximize
    the total utility. Here, the utility of a photo mainly depends on the number of
    its covered aspects, which can be conveniently calculated from the metadata. Accordingly,
    four different optimization problems are studied to trade-off between photo quality
    and resource constraints. In [4], the authors proposed an online data selection
    approach based on a pyramid tree (PTree). The proposed method PicPick can dynamically
    select an optimal set of pictures from workers based on multidimensional constraints.
    Contextual metadata and content-based visual features are used together to assess
    data redundancy in real time. As a result, the thumbnails of incoming pictures
    need to be sent to the backend server for data selection. In [5], the PTree approach
    is further used for data grouping at the macro-diversity level based on multidimensional
    semantic attributes such as location, shooting angle, and shot size. At the micro-diversity
    level, three data selection schemes are developed for different prioritization
    needs. In [8], the PTree model is also integrated into a generic framework, called
    CrowdPic, to solve the multiconstraint-driven data selection problem. A PTree-based
    data stream clustering method is used in CrowdPic to dynamically divide a data
    stream into microclusters and select pictures from each microcluster to form the
    maximum diversified subset (MDS). When a new picture arrives, it is placed into
    the PTree according to the matching and branching algorithms, while the MDS can
    get updated. All the works in [3], [4], [5], [8] focus on pre-data selection during
    the participants-to-server stage. In contrast, a novel server-to-requester photo
    selection problem is investigated in [9], [17]. The authors studied how the server
    selects a subset of high-quality photos from the raw set for the requester to
    better meet the requester''s expectations. In [17], the proposed approach leverages
    simple metadata information (i.e., GPS location) and SIFT features extracted from
    photos. A utility measure is designed to assess the quality of a photo set, which
    integrates an entropy-based spatial diversity factor and a content influence factor
    based on visual similarities. A greedy approximation algorithm is proposed to
    solve the NP-hard utility-based photo selection problem. The experiments with
    real-world datasets show high performance of the proposed approach in terms of
    photo coverage and view quality. In [9], the authors further extended the study
    on server-to-requester photo selection in [17]. They considered a more realistic
    photo coverage model by taking into account aspects of point of interests (PoIs).
    Moreover, speeded up robust features (SURF) [18], a speeded-up version of SIFT,
    are used to assess visual similarities in the calculation of the content influence
    factor. In addition to a greedy-based algorithm, termed BasicSelection (BPS),
    a PoI number-aware photo selection scheme (termed PAPS) is further proposed. PAPS
    first constructs a similarity graph over all photos and partitions it into clusters
    via spectral clustering, provided that the number of PoIs is known a priori. Then,
    BPS is run on each cluster to select photos from each cluster independently. PAPS
    is shown to outperform BPS in photo coverage but performs similarly as BPS in
    view quality. In this work, we focus on data selection during the participants-to-server
    stage as in [3], [4], [5], [8]. Similar to [4], [8], we consider multifacet contextual
    metadata as well as visual content-based features. This is different from the
    work in [3], which is only based on geographical and geometrical metadata obtained
    from built-in cameras of smartphones. In [4], [8], the metadata and visual features
    are processed together to construct different layers of a PTree for clustering.
    Thus, thumbnails of all pictures need to be uploaded and processed at the server.
    In contrast, we consider a phase-by-phase approach, which pre-screens photos based
    on metadata first and only requires upload of pre-selected photos for final selection
    based on visual contents. This work is also different from [9], [17], which focus
    on the server-to-requester photo selection problem. Because the works in [9],
    [17] intend to reduce the burden on participants in metadata collection to encourage
    participation, only simple location information is required. In contrast, our
    work, as well as [3], [4], [8], study how to exploit a variety of metadata in
    photo selection when they are available. Another difference between this work
    and [9], [17] lies in the use of features extracted from visual contents. In [9],
    [17], the similarity between two photos are defined as the ratio of the number
    of matched features over all features extracted from them. The sum of visual similarities
    between a subset of selected photos and its complement is used to evaluate the
    representativeness of the selected subset. Differently, we directly use the mean
    distance of good matches identified between two photos to measure their dissimilarity.
    Then, the sum of pairwise distances of a subset of photos can also assess the
    distinctness of the selected photos therein. SECTION III. System Modelling A.
    Data Selection for Visual Crowdsensing Consider a visual crowdsensing scenario
    shown in Fig. 1, where a crowdsensing task (denoted by G ) is published to collect
    photos for some interesting object, such as a landmark building, a street segment,
    or a celebration ceremony. The task and the sensing target need to be specified
    clearly, e.g., in terms of its temporal, spatial, and quality constraints. A group
    of mobile users are recruited to accomplish the task by taking and uploading photos
    (denoted by P ) with their smart devices. Due to the large number of photos and
    the potential redundancy therein, they will be processed to select the most representative
    ones, for example, to further reconstruct a virtual display. Figure 1. Visual
    crowdsensing scenario. Show All Foremost, the selected photos should meet the
    task constraints to be valid. Moreover, the data selection should balance between
    cost and quality. The number of selected photos is often limited by B to bound
    the processing and storage costs. Ideally, these selected photos should be as
    diverse as possible, so that the sensing target is covered comprehensively. Meanwhile,
    they should have high similarity or redundancy with those photos that are filtered
    out, so that minimal information is lost after the data selection. B. Task Model
    Referring some previous work [3], [4], we consider a task model that specifies
    a sensing task and target by the following tuple: G={[ g x , g y ],[ t s , t e
    ],[ θ min , θ max ], φ min , d max } . Here, [ l x , l y ] is the location of
    the sensing target, [ t s , t e ] is the valid period of performing the task,
    [ θ min , θ max ] is the valid view angles with respect to the target, φ min is
    the minimum coverage span within the above range required for a valid photo, and
    d max is the maximum acceptable distance between the shoot location and the target.
    As seen, a valid sensing photo for a task has to be taken within the valid period
    and distance, while it must cover sufficient views within the expected range.
    C. Data Features To select a limited number of qualified photos for a task, we
    consider two types of data features, namely, the context metadata recorded with
    a photo, and certain visual features extracted from the content data. Let F denote
    the number of context-based metadata features. Specifically, the metadata associated
    with photo P i ∈P are given by M i ={[ x i , y i ], t i ,[ α i , β i ], φ i ,
    d i } . Here, [ x i , y i ] and t i are where and when the photo is taken, respectively.
    Accordingly, the shoot distance to the target d i and the shoot angle φ i can
    be derived. Last, [ α i , β i ] represents the range of views covered by the photo.
    A valid photo for the sensing task has to meet the conditions that d i ≤ d max
    , t s ≤ t i ≤ t e , | β i − α i |≥ φ min , and [ α i , β i ]⊆[ θ min , θ max ]
    . In addition, we can extract visual features from the captured photos. One powerful
    technique is the feature detection algorithm, SIFT [7]. A good property of SIFT
    is that it is invariant to the size or orientation of a photo. SIFT has been shown
    to be robust in identifying objects even in presence of clutter and occlusion
    [7]. In [19], SIFT is also tested with kinds of image distortions, including scaling,
    rotation, fisheye, shearing and salt and pepper noise. It is found that SIFT performs
    the best in most scenarios and achieves high matching rates. Using SIFT, we can
    locate a collection of image features commonly known as “keypoints,” which are
    scale and rotation invariant. Further, a “descriptor” is generated from the samples
    in the neighbourhood of each keypoint, as a unique fingerprint for the keypoint.
    Here, we denote the visual content feature of photo P i by V i ={ K i , D i }
    , where K i and D i represent the sets of keypoints and descriptors generated
    from photo P i , respectively. D. Quality Metrics To bound the transmission and
    processing costs, we intend to select certain most representative photos from
    the photo set. The selected photos are expected to be dissimilar to each other
    but more similar to the photos that are filtered out. To quantify the selection
    effectiveness, there are various metrics proposed in the literature. For example,
    the coverage metric and its variants have been widely used, such as k -depth coverage
    [10] and quality-aware coverage [11]. In [4], the proposed approach aims to maximize
    the coverage by the selected photos. An eliminated photo is counted as covered
    by a selected photo if the distances between the two photos with respect to all
    features fall within the corresponding similarity thresholds. For instance, if
    the Euclidean distance between their shoot locations is no more than a threshold,
    say 5 meters, one photo is considered covered in terms of the shoot distance.
    If an eliminated photo is similar to a selected photo on all features, the eliminated
    photo is said to be redundant and covered. It is ideal that the set of selected
    photos can maximize the coverage and even provide full coverage. As seen, this
    coverage metric is simple and easy to compute. However, this 0/1 count of coverage
    cannot quantify redundancy elimination in a finer granularity. Moreover, this
    coverage metric cannot characterize the diversity among the selected photos. Differently,
    the work in [3] defines a utility measure, which is proportional to the range
    of aspects covered by a photo. For instance, a photo with a coverage interval
    [ 40 ∘ , 110 ∘ ] has a utility that is twice that of another photo with a coverage
    interval [ 50 ∘ , 85 ∘ ] . Clearly, the larger the overlap between two photos''
    coverage intervals, the higher the similarity and redundancy between them. Moreover,
    in the above case, their coverage intervals not only overlap, but also the coverage
    of the latter photo is a subset of that of the former photo. Then, the latter
    photo can be discarded assuming that it does not provide new information. As seen,
    this utility measure can quantify the similarity between two photos in a scale
    finer than a 0/1 count. However, the work in [3] mainly focuses on maximizing
    the covered aspects, while the similarity and redundancy among a set of photos
    depend on more features such as shoot time, shoot distance, and visual content.
    To select a number of diverse photos from a whole set, a natural idea is to cluster
    the photo set into groups and choose the one that is the closest to the centroid
    of each cluster, which is presumably the most representative photo of each group.
    Ideally, each cluster should be dense within the cluster but well separated in
    between. To evaluate the clustering performance, many metrics have been implemented
    in scikit-learn [20] when there is no ground truth, e.g., the silhouette coefficient,
    the Davies-Bouldin index, and the Calinski-Harabasz index (also known as the variance
    ratio criterion). We can extend these metrics to evaluate the data selection performance.
    The silhouette coefficient of each point x i in a dataset can be written as [21]
    S i = b i − a i max{ b i , a i } ,−1≤ S i ≤1. (1) View Source Here, b i is the
    nearest-cluster distance, i.e., the average distance between point i and all points
    in the nearest cluster. In other words, b i quantifies the cluster separation,
    since it is the smallest average distance of x i to all points in any other cluster,
    of which x i is not a member. The cluster cohesion a i gives the average distance
    between point x i and all other points in the same cluster. As seen, b i captures
    how dissimilar a point is to other clusters, and a i tells how similar it is to
    the other points in its own cluster. The mean silhouette coefficient of all points
    is the silhouette score of the clustering. The score is close to 1 when clusters
    are dense and well separated. The Davies-Bouldin index computes the average similarity
    of each cluster with its most similar one. The similarity of cluster j and cluster
    k can be evaluated by [21] R jk = s j + s k d jk (2) View Source where s k is
    the average distance between each point of cluster k and its centroid, also known
    as the cluster diameter, and d jk is the distance between the centroids of clusters
    j and k . As seen, the similarity measure is the ratio of the within-cluster distance
    to the between-clusters distance. Then, the Davies-Bouldin index of K clusters
    can be expressed as [21] DB= 1 K ∑ k=1 K max j≠k R jk . (3) View Source Clearly,
    the Davies-Bouldin score is close to 0 when the clusters are far apart. In the
    data selection problem, the selected photos are expected to be dissimilar to each
    other but similar to screened out photos. When we evaluate any given data selection
    solution, a selected photo is not necessarily the centroid of a cluster or its
    mean point. It may not even be the point that is the closest to the centroid.
    However, the Davies-Bouldin index is based on the distances to the centroid, which
    may not represent well the features of a selected photo. Hence, the Davies-Bouldin
    index cannot evaluate the performance of any data selection solution accurately.
    Therefore, we extend the above definition of the Davies-Bouldin index by generalizing
    the use of centroids in the calculation. Instead of taking the mean point of each
    cluster as the centroid, we can use any given point as the centroid of the cluster,
    which is mapped to the photo selected from each cluster. As such, the Davies-Bouldin
    index measures the similarity with respect to a real given photo instead of a
    virtual mathematic centroid of the cluster. Last, the Calinski-Harabasz index
    is the ratio of the sum of between-clusters dispersion and the sum of within-cluster
    dispersion of all clusters, while dispersion is measured by the sum of squared
    distances [21]. Specifically, the Calinski-Harabasz index of K clusters for a
    dataset of totally N points can be written as [22] CH= ∑ K k=1 n k N−1 ∥ μ k −μ
    ∥ 2 ∑ K k=1 1 N−K ∑ n k i=1 ∥ x i − μ k ∥ 2 . (4) View Source Here, for any cluster
    k , there are centroid μ k and n k points denoted by x i , where i=1,…, n k ,
    while μ is the global centroid. The separation of clusters in the numerator is
    based on the distances of the cluster centroids from the global centroid, while
    the internal cluster cohesion in the denominator is estimated by the distances
    from the data points within a cluster to its cluster centroid. Hence, a higher
    score indicates better clustering that is dense within each cluster but well separated
    between clusters. Due to the similar reason for adapting the Davies-Bouldin index,
    we also extend the Calinski-Harabasz index by using any given point to replace
    a centroid. Then, we can better map a data selection solution to a clustering
    result for quality evaluation. SECTION IV. Problem Analysis and Solution Given
    a sensing task G , a group of participants collect a set of photos P . To select
    no more than B photos from the set, we take a three-phase approach. First, all
    participants upload the metadata of their photos to the crowdsensing server. The
    cost is acceptable considering the small sizes of the metadata. The server then
    pre-screens the photos based on the metadata and identifies the valid ones that
    meet the temporal, spatial, and quality constraints given in Section III-B. The
    group of valid photos after pre-screening is denoted by P v . Assuming that there
    are more than B valid candidate photos, we need to proceed and refine the selection.
    Based on the context metadata, we select γB ( 1≤γ≤| P v |/B ) photos from the
    valid candidates in the second phase. These γB photos should be as diverse as
    possible, while the other eliminated photos are redundant with respect to the
    selected ones. In the last phase, the server requests the original files of these
    γB selected photos from the corresponding participants and selects B photos therein
    by further processing the visual contents. With the initial selection based on
    the lightweight metadata, this step now becomes more manageable. Moreover, the
    ratio γ can be adjusted according to the available resources for transmission
    and computation. When a larger cost is affordable, the server can set a larger
    value for γ and choose more candidates based on the metadata. A. Initial Selection
    Based on Context Metadata 1) SmartPhoto + for Initial Selection For the initial
    selection based on the photo metadata, we consider three candidate approaches.
    First, we can extend the scheme SmartPhoto proposed in [3]. For ease of comprehension,
    let us introduce the main steps of SmartPhoto in the following. Consider a single
    target G . Each photo P i covers a range of aspects of the target, represented
    by an interval [ α i , β i ] . With the coverage intervals of all photos in set
    P v , we can split the whole range of full coverage [0, 360 ∘ ) into a number
    of sub-intervals, by taking the boundaries of each interval as the dividing points.
    These sub-intervals form a universe set of elements, where the length of the sub-interval
    for an element is defined as its weight. Then, each photo covers a subset of elements
    in the universe set. The total utility of a photo selection result is the total
    weight of the elements covered by the selected photos. Thus, the data selection
    problem is formulated as an instance of the maximum coverage problem [23], which
    is to find a bounded number of subsets to maximize the total weight of the elements
    covered by the selected subsets. As the maximum coverage problem is known to be
    NP-hard, a greedy selection algorithm is used in [3] for the photo selection.
    The basic idea is to iteratively select an unselected subset, which has the highest
    incremental contribution to the total utility. That is, the elements in the new
    subset that are uncovered so far have the largest total weight. The selection
    process continues until the required number of subsets (photos) have been selected
    or every aspect of the target has been covered. As the original algorithm focuses
    on the aspect coverage, we need to incorporate more context features, including
    the shoot angle, the shoot time, and the distance to the sensing target. Algorithm
    1 shows the extended version, subsequently referred to as SmartPhoto + . As seen,
    there are two main procedures. In Lines 1--6, the whole set of elements are defined
    with respect to each feature in the context metadata, and each candidate photo
    is mapped to a subset of such elements. While the aspect coverage is specified
    by a range of view angles, the other features are single values. For each single-valued
    feature, we just divide its data scale to a number of uniformly distributed intervals,
    which represent the set of elements for this feature. If a photo''s feature falls
    into an interval, it means that the photo covers the element corresponding to
    this interval. Since these intervals are uniformly distributed, each element has
    the same weight 1. The aspect coverage is defined by two end points for the view
    angles, e.g., [ 30 ∘ , 70 ∘ ] . The end points of all photos divide the entire
    coverage interval [0, 360 ∘ ) to a set of sub-intervals. As considered in [3],
    each sub-interval is added to the universe set of elements, while its weight is
    proportional to the length of the sub-interval. Moreover, we normalize the weight
    to [0,1] to be comparable with the elements from other features. For each photo,
    a subset of elements is generated according to which coverage sub-intervals it
    falls into. After defining the elements of the universe set and their weights
    and the subset of elements covered by each photo, Lines 7--13 select a limited
    number of photos and label all photos into the clusters. First, the greedy algorithm
    in [3] is used to solve an instance of the maximum coverage problem, which gives
    γB representative photos to achieve a high total utility. Then, each selected
    photo is taken as a cluster centroid, while each remaining unselected photo is
    classified into the cluster with the nearest centroid. For ease of distance calculation,
    we normalize the context features before the classification. The algorithm SmartPhoto
    + is a potential solution to the initial selection of candidate photos only based
    on the context metadata. Compared to the original approach SmartPhoto, we further
    take into account multiple features. Accordingly, we need to properly weight and
    normalize the feature elements. In addition, we map the selection result to a
    straightforward clustering so that it can be evaluated with various clustering
    indices discussed in Section III-D. 2) PicPick + for Initial Selection Another
    interesting approach for the initial photo selection is PicPick proposed in [4].
    PicPick uses a pyramid tree (PTree) structure to group pictures with multidimensional
    features. PTree has (F+2) layers, where F is the number of features under consideration.
    The root node is at Layer-0, while Layer-1 to Layer- F is mapped to one of the
    F features. The bottom Layer- (F+1) consists of the leaf nodes, each of which
    corresponds to a picture instance. Each subset of leaf nodes on the bottom layer
    descending from the same node on Layer- F naturally forms a cluster of pictures,
    and one representative picture can be selected from each cluster. Here, we re-define
    the label of each node in the tree. The label of each node is given by L⟨ℓ⟩-N⟨n⟩
    , where ⟨ℓ⟩ is the layer of the node and ⟨n⟩ is simply the order of the node added
    to the layer. To generate a PTree, it begins with the root node. Then new nodes
    are added in according to a sequence of pictures. Each node at Layer-1 to Layer-
    F (excluding the top and bottom layers) is associated with an attribute with respect
    to the feature used on that layer (e.g., the shoot angle). This attribute is obtained
    from the features of the leaf nodes descending from the node. Specifically, it
    is the mean value of these leaf nodes for the feature of that layer, e.g., the
    mean shoot angle of leaf descendants on the bottom layer. Thus, each picture can
    be added into the tree by comparing its distances to the attributes of the nodes
    already in the tree. PicPick originally uses context features and visual content
    features together. Since we intend to streamline the data selection procedure
    phase by phase, we can use PicPick for the initial photo selection based on only
    context metadata. However, one limitation of PicPick is that it does not restrict
    the number of node clusters on the bottom layer. If one picture is selected per
    cluster, we cannot limit the number of selected pictures. Hence, we need to extend
    PicPick on the generation of the PTree. Algorithm 2 shows the algorithm PicPick
    + extended from [4]. Similar to SmartPhoto + , there are two main procedures.
    Lines 1--12 generate a PTree by processing the set of pictures one by one. For
    each picture, it traverses the current PTree from the root node, and compares
    the features of the new picture with the attributes of the existing non-leaf nodes
    on Layer-1 to Layer- F . If a non-leaf node is within a distance threshold to
    the new picture, it is called a match in [4]. Note that match here should be distinguished
    from match of SIFT features to be discussed in Section IV-B. Otherwise, a new
    non-leaf node is created on the current layer. It is worth noting that the distance
    thresholds are important to PicPick and PicPick+ and should be carefully selected
    according to the requirements of the crowdsensing task. Then, we move downward
    to the next layer through the matched node or the newly created node. Finally,
    the new picture is added to the bottom layer, i.e., Layer- (F+1) , as a leaf node.
    In the above procedure, there are two key aspects. First, we need to properly
    calculate the distance between a non-leaf node and a new picture based on the
    layer feature. For a single-valued feature, such as the shoot angle and shoot
    distance, we can simply use the Euclidean distance as in [4]. However, for the
    distance between two coverage intervals inspired by how coverage intervals are
    handled in [3], we first find their overlapping sub-interval if any, normalize
    the length of the sub-interval with the maximum range covered by the two intervals,
    and last take one minus the ratio. This distance can quantify the dissimilarity
    between the two intervals. Second, we need a match policy to match a new picture
    to a non-leaf node when traversing the PTree. In [4], the authors proposed two
    match policies, namely first-match and min-match. First-match just takes the first
    matched non-leaf node on Layer-1 to Layer- F if any exists, whereas min-match
    compares the distances of the new picture to all matched nodes on a layer and
    chooses the node with the shortest distance to the picture. As mentioned earlier,
    one limitation of PicPick is that it does not restrict the number of clusters
    on the bottom layer, which makes it not applicable to our problem. Hence, we adapt
    the algorithm in Lines 7--10. Here, we only add a new non-leaf node on a layer
    if the number of existing clusters has not reached the limit. Moreover, we apply
    the min-match policy and always choose the closest non-leaf node as a parent,
    even when this node is not within the distance range and a maximum number of clusters
    have been created. As such, we do our best to group pictures into the PTree while
    meeting the selection constraint. The second procedure in Lines 13--18 labels
    the clusters for all pictures according to the above generated PTree and selects
    a subset of pictures from the clusters. In the original algorithm PicPick, it
    simply picks the first picture added into each cluster for timely processing.
    Here, since we only consider metadata in the initial photo selection, it is affordable
    to choose a more representative picture for each cluster. Hence, in Line 18, referring
    to the mean-priority strategy in [5], we select the picture that is the closest
    to the centroid of each cluster in terms of the feature distances. 3) Clustering
    Algorithms for Initial Selection In addition to SmartPhoto + and PicPick + , for
    comparison purpose, we also consider a clustering-based benchmark algorithm which
    is often used in the literature to process redundant data. The idea is to consider
    the set of valid photos as data samples, use a clustering algorithm to group them
    into γB clusters, and then choose one representative photo from each cluster.
    Since a clustering tends to be dense within the cluster but well separated in
    between, the selected photos are therefore expected to be diverse but redundant
    to the eliminated ones. Algorithm 3 shows the approach for initial photo selection
    based on clustering and centroids. It first normalizes the features of all valid
    photos. Then, a clustering algorithm is run to separate the photos into γB clusters.
    In the literature, there have been a variety of clustering algorithms to choose
    from, such as k -means, mean shift, spectral clustering, density-based spatial
    clustering of applications with noise (DBSCAN), and agglomerative clustering.
    Last, it selects the photo that is the closest to the centroid of each cluster.
    Algorithm 3: ClusterFirst: Initial Photo Selection Based on a Standard Clustering
    Algorithm. Input: Sensing task G , metadata for valid photo set: { M i :∀ P i
    ∈ P v } , maximum photo number for initial selection γB Output: Initial selection
    x={ x i :∀ P i ∈ P v } , cluster labelling y={ y i :∀ P i ∈ P v } 1: Normalize
    features of all photos in P v ; 2: Choose a clustering algorithm, e.g., agglomerative
    clustering; 3: Run the clustering algorithm to separate photos in P v into γB
    clusters; 4: Assign cluster labels to photos in P v and record clustering in y
    ; 5: Compute the centroid of each cluster; 6: Select the photo that is the closest
    to the centroid and record selection result in x ; 7: Return x , y ; B. Final
    Selection Based on Visual Content After the valid photo set is initially examined
    based on context metadata, we end up with a much smaller photo subset, denoted
    by P s . Then, it becomes affordable to analyze their visual features and further
    select B most distinct photos therein. Here, we take the SIFT-based visual feature
    as one example, while this can be extended to other visual features as well. Note
    that the pre-selected photos may contain clutter, blur, occlusion or other distortions.
    Such noises may interfere with the visual feature extraction and matching, although
    SIFT has good robustness even in matching noisy images [7], [19]. If a noisy photo
    provides complementary information that is absent in other photos, it may be selected
    but not meet the requester''s expectation on image quality. Therefore, if the
    requester has a requirement on the minimum image quality, we can use an image
    quality assessment (IQA) approach [24], [25] to filter out photos of low image
    quality first. As such, in the subsequent photo selection, we can avoid the interferences
    from noisy photos and do not need to deal with conflicting decisions with respect
    to photo uniqueness and image quality, of course, at the additional cost of IQA
    screening. Algorithm 4 shows the final photo selection algorithm based on the
    SIFT feature. Here, we expect to select a subset of dissimilar photos that can
    provide complementary information. As seen, Algorithm 4 consists of two main procedures.
    In Lines 1--12, we extract the visual features of pre-selected photos and evaluate
    their pairwise distances. In Line 3, we derive the SIFT feature V i from each
    pre-selected photo P i , where V i ={ K i , D i } , respectively. Here, K i and
    D i represent the sets of keypoints and descriptors generated from photo P i .
    Given that there are ρ keypoints identified for photo P i , K i contains ρ keypoint
    objects, while D i is an ρ×128 array describing the 16×16 neighborhood of each
    keypoint. Then, in Line 6, we measure the distances between the descriptor features
    of each pair of photos to find possible match(es) if there is any. That is, each
    descriptor of one photo is compared with all descriptors of the other photo to
    find one or more matches with short distances. The distances are evaluated by
    Euclidean distances based on L2-norm. As the k -nearest neighbour (KNN) matching
    algorithm was shown to be generally robust, it is considered in Algorithm 4. If
    there are ρ descriptors from photo P i , the KNN algorithm outputs ρ×k matches
    from photo P i to photo P j . Among the k matches for each descriptor, Line 7
    further takes the ratio test [7] to decide whether to keep the top match. Only
    if the shortest distance of the top match is less than ϕ ( 0<ϕ<1 , e.g., ϕ=0.75
    ) of that of the second match is the top match considered as a good match. As
    such, only those matches with sufficiently small distances are retained, and there
    are at most ρ such good matches identified for two photos, where there is at most
    one good match for each descriptor. After finding the good matches between two
    pre-selected photos, the mean distance of the matches is recorded as a corresponding
    element in a distance matrix (Line 9). If no good match is found, a large constant
    is used in the distance matrix (Line 11), which indicates that the two corresponding
    photos are very distinct. As the so-generated distance matrix may not be symmetric,
    we further take the maximum value of each pair of elements W ij and W ji to reset
    their values (Line 12). Then, the distance matrix becomes symmetric and better
    quantify the dissimilarity of each pair of photos. In the second procedure in
    Lines 13--23, we use the above generated distance matrix W to select B most distinct
    photos from set P s . Naturally, we expect that the selected photos are as dissimilar
    as possible. Such a subset of diverse photos can cover the sensing target comprehensively.
    Translating this idea to a mathematical problem, we can formulate the photo selection
    as the following optimization problem: (P) s.t. max x . ∑ i ∑ j>i x i x j W ij
    ∑ i x i ≤B. (5a) (5b) View Source Here, x i is a binary decision variable, indicating
    whether photo P i ∈ P s is selected, and W ij is the distance between photos P
    i and P j estimated in the above procedure. As the distance matrix is symmetric,
    we only need to consider the upper triangular portion of the distance matrix above
    the diagonal. The only constraint is to limit the total number of selected photos
    by B . To solve this optimization problem, we can reformulate it as an ILP problem
    as follows: (D) s.t. max x . ∑ i ∑ j>i z ij W ij z ij ≤ x i ,∀i,jandj>i z ij ≤
    x j ,∀i,jandj>i x i + x j −1≤ z ij ,∀i,jandj>i ∑ i x i ≤B. (6a) (6b) (6c) (6d)
    (6e) View Source In (6), one additional binary variable z ij is introduced, which
    can be interpreted as an indicator on whether distance W ij between photos P i
    and P j is counted in the objective value. Obviously, z ij is related to x i and
    x j . As defined in constraints (6b)--(6d), z ij =1 only if both x i =1 and x
    j =1 . That is, to have z ij =1 and count distance W ij , both photos P i and
    P j should be selected. When the ILP problem has a small scale, it can be solved
    by modern solvers, such as Gurobi [26] and the GNU linear programming kit (GLPK)
    [27]. However, due to the high complexity, it can be slow to solve a large-scale
    instance. Hence, we also use a heuristic algorithm given in Lines 17--23 to solve
    it. Initially, it chooses the first photo that is the most distant to all others.
    Then, it iteratively selects a new photo with the largest mean distance to other
    already selected photos. The subsets of selected photos and eliminated photos
    are updated accordingly. This procedure continues until B photos are selected.
    SECTION V. Simulation Results In this section, we evaluate the data selection
    approaches introduced in Section IV. First, we introduce the datasets used in
    the performance evaluation, including those for the contextual metadata and the
    visual content data. Then, we present the results for the initial photo selection
    based on metadata. Last, we show the performance of the final photo selection.
    A. Datasets 1) Datasets for Context Metadata In the following experiments, we
    use both synthetic and real datasets to evaluate the data selection approaches.
    The key parameters are listed in Table 1. TABLE 1 Experiment Parameters As given
    in Section III-B, a sensing task target is specified by tuple G={[ g x , g y ],[
    t s , t e ],[ θ min , θ max ], φ min , d max } . We generate the target location
    [ g x , g y ] uniformly within a rectangular region. The valid start time t s
    is randomly picked within a maximum sensing period, and the task lasts for a random
    duration within a range, which gives the valid end time t e bounded by the maximum
    sensing period. The range of valid views [ θ min , θ max ] is simply set to [
    0 ∘ , 360 ∘ ] . The minimum valid coverage span φ min is taken uniformly in a
    given range. The maximum acceptable shoot distance d max is also randomly selected
    from a range. According to the task information, we can set the similarity thresholds,
    which are used in the PicPick-based approaches and for the coverage metric. Some
    example values are given in Table 1. For each photo P i ∈P , its context-based
    metadata are given by M i ={[ x i , y i ], t i ,[ α i , β i ], φ i , d i } . The
    shoot time t i is set uniformly within the whole sensing period. The sensing location
    [ x i , y i ] can be taken from a real dataset or generated as a clustered random
    point process within a region slightly larger than the sensing region (to be discussed
    in detail). Based on the sensing location, the shoot distance d i to the target
    and the view angle φ i can be derived. The range of views covered by the photo
    is given by [ α i , β i ] . We randomly generate a width for the view coverage
    and a ratio less than 1. The ratio defines how the view angle φ i splits the width
    of the coverage in [ α i , β i ] . That is, the width of the sub-interval [ α
    i , φ i ] over that of the entire interval [ α i , β i ] is given by the ratio.
    To get synthetic location data, we can use a cluster point process such as the
    Matérn process to model the locations. For a Matérn process, the cluster centres
    are generated according to a homogeneous Poisson point process (PPP), while each
    cluster contains a random number of children points that are uniformly distributed
    within a circular area of a given radius and a centre from the preceding PPP.
    To just generate a total number of location points, we also limit the maximum
    number of points in each cluster, as shown in Table 1. Since the sensing locations
    tend to cluster in the surrounding environment of the target, this Matérn-like
    process can capture the clustering effect while meeting our simulation needs.
    To get more realistic location information, we can also use some existing datasets
    such as the New York City (NYC) and Tokyo (TKO) check-in datasets [28]. They were
    crawled from Foursquare [29], which is a location data platform and mobile app.
    These datasets include check-in data at venues in NYC and TKO collected from April
    12, 2012 to February 16, 2013. Each check-in record is a tuple (user ID, venue
    ID, latitude, longitude, date and time). The latitude and longitude are the GPS
    coordinates obtained from the check-in user device. The datasets contain 227,428
    check-ins in NYC and 573,703 check-ins in TKO. As users may visit a venue repeatedly,
    it is time-consuming and unnecessary to consider all records in the datasets.
    Instead, we pre-process the data as proposed in [30] to get a required number
    of location records. Specifically, we sample 500 records from the TKO dataset
    in each run and they are sufficient to capture the data characteristics that our
    experiments need. Here, we first load the check-in records and filter them by
    keeping the newest check-in record for each mobile user but removing duplicate
    user IDs and venue IDs. Then, we limit the scales in space and time to select
    a required subset of samples that meet the distance and time constraints from
    the above filtered records. Finally, the GPS locations of the selected records
    are mapped to 2D Cartesian locations for ease of calculation. We also normalize
    the location coordinates and scale them into the given sensing region. 2) Datasets
    for Visual Content It is challenging to create a usable photo set with a variety
    of metadata under consideration. First of all, the photos must be toward the same
    target and also significantly vary in certain aspects such as shoot time, shoot
    angle, and coverage of views. Hence, the photo set needs to be sufficiently large
    so that there remain a good number of valid candidates after the dataset is pre-screened
    with the validity constraints. Unfortunately, we are not able to create or find
    such an ideal photo set. Nonetheless, since some key aspects used in photo selection
    are the angle, span and range of views, we find that the Columbia University Image
    Library (COIL-100) [31] can meet our needs in a degree. This library contains
    a database of color images of 100 objects, which were shot through 360 degrees
    at pose intervals of 5 degrees. We will use this image database to test the effectiveness
    of Algorithm 4 for final photo selection with visual features. It is worth mentioning
    that Algorithm 4 is not limited to the COIL-100 database or SIFT features. It
    would be interesting future work to test Algorithm 4 with other appropriate photo
    datasets and visual features. B. Results of Initial Selection With Metadata According
    to the settings in Section V-A, we evaluate the initial selection methods introduced
    in Section IV-A with different datasets. To investigate their performance extensively,
    we test each method for multiple runs. In each run, each method takes a new dataset
    of 500 samples as its input. Fig. 2 shows the total utility achieved by the photo
    selection with SmartPhoto [3] and the extended algorithm SmartPhoto + in Algorithm
    1. Clearly, SmartPhoto + significantly improves the total utility by selecting
    photos according to multiple features instead of just the coverage aspect. Figure
    2. Total utility. Show All The photo selection approach PicPick [4] aims to maximize
    the total coverage of the selected photos by using a PTree to group the photo
    set. As shown in Fig. 3, PicPick + in Algorithm 2 slightly enhances this goal.
    To generate Fig. 3, the original PicPick is slightly adapted to be comparable
    to PicPick+ and meet the limit on the number of selected pictures by stopping
    selection after the limit is reached. The average coverage is improved by 15.32%
    with the synthetic metadata, while it is improved by 10.34% with the dataset of
    real locations. These results demonstrate that our extension does well in keeping
    the clustering structure when pruning the branches of the PTree. Figure 3. Total
    coverage. Show All Figs. 4–6 compare the three photo selection approaches discussed
    in Section IV in terms of the three clustering metrics given in Section III-D.
    For the approach ClusterFirst, we use agglomerative clustering in Algorithm 3.
    Fig. 4 shows the silhouette coefficients of the three approaches. As seen, if
    we select photos using the agglomerative clustering algorithm in Algorithm 3,
    the resulting clustering can achieve the highest silhouette coefficients. It is
    known that the silhouette coefficient is larger when the clusters are denser and
    better separated. Similarly, it is seen in Fig. 5 that the Davies-Bouldin indices
    of ClusterFirst are the lowest among the three approaches. Different from the
    silhouette coefficient, a lower Davies-Bouldin index indicates a better clustering
    result. Figure 4. Silhouette coefficient. Show All Figure 5. Davies-Bouldin index.
    Show All Figure 6. Calinski-Harabasz index. Show All Fig. 6 shows the Calinski-Harabasz
    index of the three photo selection approaches. It is observed that ClusterFirst
    achieves the highest Calinski-Harabasz indices. Similar to the silhouette coefficient,
    a higher Calinski-Harabasz index indicates a better clustering result. With the
    synthetic metadata, the average Calinski-Harabasz index of ClusterFirst is 40.22%
    higher than that of SmartPhoto + and 1.655 times higher than that of PicPick +
    . With the dataset of real locations, SmartPhoto + performs closer to ClusterFirst
    in some cases. However, the average Calinski-Harabasz index of ClusterFirst is
    still 29.01% higher than that of SmartPhoto + . Also, both ClusterFirst and SmartPhoto
    + significantly outperform PicPick + in terms of the Calinski-Harabasz index.
    Figs. 7–8 compare the three photo selection approaches in terms of other performance
    metrics such as total utility and total coverage. Since SmartPhoto and its extension
    SmartPhoto + aim to maximize the total utility, it is not surprising to see in
    Fig. 7 that SmartPhoto + achieves the highest total utility. ClusterFirst with
    agglomerative clustering comes next, while the total utility of PicPick + is the
    lowest. This is because PicPick + selects photos to maximize the number of other
    photos covered by the selected ones. Figure 7. Total utility. Show All Figure
    8. Total coverage. Show All As shown in Fig. 8, the total coverage of PicPick
    + is the highest on average among the three approaches. We can see in Fig. 8(a)
    that, with the synthetic dataset, the total coverage of PicPick + is 61.66% higher
    than that of SmartPhoto + , but it performs very closely to ClusterFirst. Similar
    trends are observed in Fig. 8(b). PicPick + performs the best since it targets
    at maximizing the total coverage. ClusterFirst with agglomerative clustering achieves
    total coverage that is slightly lower that of PicPick + , but it performs better
    than SmartPhoto + . As the edge servers in MEC usually have limited computing
    resources, the computational cost of the data selection approaches is an important
    factor to ensure feasible implementation. In Fig. 9, we further evaluate the computing
    time (in milliseconds) of PicPick + , SmartPhoto + , and ClusterFirst. They are
    implemented in Python and run on a Mac mini station with a processor of 3.2 GHz
    6-core Intel Core i7 and memory of 32 GB. As seen, all three methods can run on
    the order of seconds. ClusterFirst is the fastest with computing time of several
    milliseconds. PicPick + is the slowest but the average computing time of all runs
    is still less than 2 seconds. One reason for the longer time is that PicPick +
    needs to constantly maintain a complex data structure, PTree, which organizes
    the picture set according to their features. The results in Fig. 9 demonstrate
    that these three methods are computationally efficient and it is viable to deploy
    them on the edge servers in MEC. Figure 9. Computing time. Show All C. Results
    of Final Selection With Visual Data As mentioned in Section IV, we take a three-phase
    approach to select a limited number of photos. After the pre-screening with the
    validity constraints and the initial photo selection based on the context metadata,
    we need to further process the visual content data of the pre-selected photos
    and choose a given number of the best ones among them. In Section IV-B, we give
    Algorithm 4 for the final photo selection based on SIFT. First, the SIFT features
    are extracted to evaluate the pairwise distances of the pre-selected photos. Then,
    we solve the optimization problem in (6) with an ILP solver or a heuristic approach.
    Using the COIL-100 image library, we compare the heuristic solution with the optimal
    solution obtained by the ILP solver CVXOPT [32] with the GLPK extension [27].
    Here, we randomly choose γB photos for an object and select B most distinct photos
    among them using the optimal solution and the heuristic solution. Similar to the
    above experiments, we compare these solutions in multiple runs. In each run, we
    randomly select a new subset of photos from the image library and apply the selection
    solutions over them. Fig. 10 compares the objective values achieved by the two
    solutions and their computing time. As seen in Fig. 10(a), the heuristic solution
    performs very closely to the optimal solution. The average approximation ratio
    of the objective value achieved by the optimal solution to that of the heuristic
    solution is 0.968. Nonetheless, Fig. 10(b) shows that the heuristic solution takes
    significantly less computing than the optimal solution. To further examine the
    differences between the optimal solution and the heuristic solution, Fig. 11 shows
    the photos selected by the two approaches for a case where the heuristic solution
    has an approximation ratio 0.9879. The selected photos are annotated with ***
    and framed by red borders. As seen, both approaches successfully choose the photos
    with fairly distinct shoot angles. These photos are the representative ones for
    the whole set. Specifically, the heuristic approach selects two photos in the
    first row with close views. In contrast, the optimal solution selects one photo
    in the last row with a side view, which is clearly a better choice. However, the
    heuristic approach performs closely to the optimal solution overall, and it is
    much faster. Especially, the ILP solver becomes quite slow when there are more
    than 30 candidates in the photo set. Figure 10. The optimal solution vs. the heuristic
    solution for final photo selection. Show All Figure 11. Final photo selection
    results based on SIFT features. Show All SECTION VI. Conclusion In this paper,
    we study the data selection problem in VCS. A phase-by-phase hybrid framework
    is considered, which first filters collected pictures based on the metadata and
    then selects the final pictures using the content features. Particularly, we extend
    SmartPhoto [3] and PicPick [4] to be applicable to the hybrid framework. We also
    consider a benchmark approach with a standard clustering algorithm for comprison.
    In addition, we evaluate different selection approaches using adapted clustering
    indices as well as traditional metrics such as total utility and coverage. Extensive
    simulations are conducted with both synthetic and real datasets. The results show
    that, the extended algorithm SmartPhoto + performs significantly better than the
    original algorithm SmartPhoto in terms of total utility. PicPick + slightly improves
    the total coverage in comparison with PicPick when the number of selected pictures
    is constrained. Among the three algorithms (SmartPhoto + , PicPick + , and ClusterFirst),
    SmartPhoto + achieves the highest total utility, while PicPick + performs the
    best in terms of total coverage but fairly close to ClusterFirst. Regarding the
    clustering indices, ClusterFirst shows the best results. In practice, we can choose
    an appropriate solution among these candidates according to specific application
    needs and target performance. Authors Figures References Keywords Metrics More
    Like This CoopEdge: Cost-effective Server Deployment for Cooperative Multi-Access
    Edge Computing 2022 19th Annual IEEE International Conference on Sensing, Communication,
    and Networking (SECON) Published: 2022 Cost-Effective Edge Server Network Design
    in Mobile Edge Computing Environment IEEE Transactions on Sustainable Computing
    Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Open Journal of Vehicular Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Hybrid Data Selection With Context and Content Features for Visual Crowdsensing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Karlstetter R.
  - Raoofy A.
  - Radev M.
  - Trinitis C.
  - Hermann J.
  - Schulz M.
  citation_count: '3'
  description: 'Real-time sensor monitoring is critical in many industrial applications
    and is, e.g., used to model and predict operating conditions to optimize operations
    as well as to prevent damage in machinery and systems. In many cases, this data
    is generated by a myriad of sensors and stored or transmitted for post-processing
    by data analysts. Handling this data near its origin - on the edge - imposes significant
    challenges for storage and compression: it is necessary to store it in a format
    that is suitable for large data analytics algorithms, which in most cases means
    columnar storage. Furthermore, to provide efficient storage and transmission of
    such sensor data, it must be compressed efficiently. However, existing solutions
    do not address these challenges sufficiently. In this work, we present a holistic
    approach for fast streaming of large scale sensor data directly into columnar
    storage and integrate it with a proven compression scheme. Our approach uses a
    pipelined scheme for streaming and transposing the data layout, combined with
    a byte-level transformation of data representation and compression, which we evaluate
    in comprehensive experiments. As a result, our approach enables transformation
    of large scale sensor data streams into an efficient, analytics-friendly format
    already at the sensor site, i.e., on the edge, at data ingestion time. By implementing
    our optimized approach in the open and widely used columnar storage format Apache
    Parquet, which we already partly upstreamed, we ensure its accessibility to the
    community.'
  doi: 10.1109/CCGrid51090.2021.00010
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2021 IEEE/ACM 21st Internatio... Living
    on the Edge: Efficient Handling of Large Scale Sensor Data Publisher: IEEE Cite
    This PDF Roman Karlstetter; Amir Raoofy; Martin Radev; Carsten Trinitis; Jakob
    Hermann; Martin Schulz All Authors 3 Cites in Papers 287 Full Text Views Abstract
    Document Sections I. Introduction II. Problem Statement III. Approach IV. Implementation
    in Apache Parquet V. Experimental Evaluation Show Full Outline Authors Figures
    References Citations Keywords Metrics Footnotes Abstract: Real-time sensor monitoring
    is critical in many industrial applications and is, e.g., used to model and predict
    operating conditions to optimize operations as well as to prevent damage in machinery
    and systems. In many cases, this data is generated by a myriad of sensors and
    stored or transmitted for post-processing by data analysts. Handling this data
    near its origin—on the edge—imposes significant challenges for storage and compression:
    it is necessary to store it in a format that is suitable for large data analytics
    algorithms, which in most cases means columnar storage. Furthermore, to provide
    efficient storage and transmission of such sensor data, it must be compressed
    efficiently. However, existing solutions do not address these challenges sufficiently.
    In this work, we present a holistic approach for fast streaming of large scale
    sensor data directly into columnar storage and integrate it with a proven compression
    scheme. Our approach uses a pipelined scheme for streaming and transposing the
    data layout, combined with a byte-level transformation of data representation
    and compression, which we evaluate in comprehensive experiments. As a result,
    our approach enables transformation of large scale sensor data streams into an
    efficient, analytics-friendly format already at the sensor site, i.e., on the
    edge, at data ingestion time. By implementing our optimized approach in the open
    and widely used columnar storage format Apache Parquet, which we already partly
    upstreamed, we ensure its accessibility to the community. Published in: 2021 IEEE/ACM
    21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
    Date of Conference: 10-13 May 2021 Date Added to IEEE Xplore: 02 August 2021 ISBN
    Information: DOI: 10.1109/CCGrid51090.2021.00010 Publisher: IEEE Conference Location:
    Melbourne, Australia SECTION I. Introduction Industrial installations use an ever-growing
    number of sensors to monitor machine health and operational state. The petroleum
    industry [1], water supply and distribution networks [2], power generation, e.g.,
    wind turbines [3] and gas-fired power plants [4], HPC centers [5], and many more:
    they all benefit from the data generated by an enormous number of sensors in their
    monitoring systems, which they analyze to optimize operations, detect potential
    problems, and prevent failures. In many cases, these sensors measure physical
    phenomena or other derived values and deliver continuous streams of sensor data.
    These streams often include data sampled at a very high data rate, which is especially
    necessary for monitoring and studying the underlying physical processes that generate
    high-frequency oscillations. Further, for each installation, monitoring systems
    measure and analyze the signals from many sensors simultaneously, leading to numerous
    amounts of parallel and fast data streams (see Figure 1 ②). These parallel streams
    are processed in real-time to adjust machine parameters, prevent machine failures
    or to detect potential problems (see Figure 1 ③). Additionally, many scenarios
    require storing this data for later use, either on site or—if network bandwidth
    allows it—in the cloud. For example, the collected data is used to design new
    and more expressive metrics to indicate the health of underlying machinery and
    to improve the monitoring systems’ real-time failure detection mechanisms. Moreover,
    in case of damage to the machinery, the collected data is used for root cause
    analysis, i.e., to understand failures and their reasons. Traditionally, such
    analyses do not happen on site, but are rather moved to an off-site data center,
    where data from many installations can be combined (e.g., see Figure 1 ⑥). Handling
    the large scale sensor data streams for such analysis scenarios imposes two main
    challenges: First, data streams are typically ingested and stored in a row-oriented
    (time-ordered) layout (Figure 1 ④), which is the straightforward approach for
    high-throughput data stream storage, as the streams are directly written to permanent
    storage without the need for any further layout processing after ingestion. However,
    for most analysis algorithms, data stored in a column-oriented (sensor-ordered)
    storage format leads to more efficient analyses [6]. This underlines the need
    for an automated layout processing step, i.e., conversion from row-oriented to
    column-oriented layout. Ideally, this is done before persisting the data to permanent
    storage, close to the data origin, i.e., on the edge device. However, the processing
    resources at this stage of data processing, i.e., at the edge, are scarce, rendering
    this a challenging task. Fig. 1: Deployment scenario for monitoring industrial
    assets, here at the example of a gas turbine for power generation ①. A limited
    time frame of data from several sensors ② is combined and analyzed on an embedded
    digital signal processor without any storage capabilities ③. This happens in real-time
    to protect the machinery. In order to store the data for further offline analysis,
    the row-oriented (time-ordered) data stream ④ is sent to an edge-system ⑤, still
    on site. This edge-system stores the sensor data and provides real-time analysis
    results. A subset of the data stored on site is sent to a central storage system
    ⑥, where data from other, similar installations is gathered as well. Our contributions
    address efficient handling of this sensor data stream by efficiently transforming
    it to a column-oriented (sensor-ordered) format as well as optimizing compression.
    Show All Second, yet perhaps even more important, these sensor data streams generate
    huge volumes of data. Optimizing compression directly impacts the needed storage
    space and network bandwidth for data transmission. It is thus essential to consider
    data compression from the beginning and integrate it directly into the ingestion
    system at the edge, near the machinery. While there are systems handling these
    individual aspects, there is no holistic approach that optimizes the entire data
    flow from sensor to columnar storage; and there is also no study to quantify the
    handling of fast sensor data in a practical real-world scenario (see Figure 1).
    In this paper, we present a novel end-to-end solution to address these challenges.
    Our approach extends the pipeline with two optimized, complementary components
    that enable both efficient layout processing and sensor data compression on the
    edge device. We demonstrate the feasibility of our approach by implementing a
    fully working system that extends the Apache Parquet [7] storage format. We upstreamed
    our implementation as part of Apache Parquet so that the research community can
    easily reproduce and apply our approach to their data sets and use it in their
    projects. Additionally, we comprehensively evaluate the handling of fast and large
    scale sensor streams with our system. For these evaluations, we rely on a real-world
    use case, exploit realistic settings, and take the unique advantage of using data
    acquired from a real-world industrial monitoring system to quantify the performance
    and practical resource requirements on different hardware platforms. Finally,
    using a case study, we show that our system is also applicable to low-power edge
    systems, which is an important usage scenario in industrial settings. In summary,
    we make the following key contributions: We design, implement and optimize an
    end-to-end system, from sensor to compressed columnar storage. We provide a comprehensive
    evaluation of the developed system on multiple platforms. Using a real-world data
    set, we show that our approach improves both compression ratio and sustained throughput
    for floating-point sensor data compared to prior art. We integrate our approach
    into an industrial system and demonstrate its functionality and efficiency in
    a real-world setting. The remainder of this paper is organized as follows: We
    first give an overview of the context and define the problem statement in Section
    II. In Section III, we describe our holistic approach of streaming sensor data
    to columnar storage and how to efficiently compress these sensor data streams,
    and in Section IV we detail our implementation. We provide an extensive evaluation
    of our system together with a discussion in Section V. In Section VI we demonstrate
    the applicability of our approach in a real-world use case, and we discuss related
    work Section VII. Finally, Section VIII concludes our paper. SECTION II. Problem
    Statement Streams of sensor data are ubiquitous, representing the backbone of
    virtually every modern intelligent monitoring system, as they are used in industrial
    (e.g., Industry 4.0 applications), home/consumer (e.g., smart homes and vehicles)
    and research (e.g., large scale experimental setups) environments alike. As one
    concrete example, which is representative of many practical scenarios in scale
    and resource requirements, we describe a combustion monitoring and protection
    system for heavy-duty gas turbines used for power generation (see Figure 1). The
    combustion process in current generations of gas turbines is monitored using up
    to 32 high-frequency pressure oscillation sensors, each of which generates a stream
    of single-precision floating-point values at a rate of 25.6 kHz (see Figure 2).
    Further, monitoring systems generate not only raw sensor data: they also integrate
    operational information, derive extra information such as frequency spectra of
    raw data as well as other results of application-specific real-time analyses,
    which are represented as additional data streams and can easily double the amount
    of produced data. In total, a single turbine alone generates more than 500 GBytes
    of uncompressed single-precision floating-point data per day. State-of-the-art
    systems handle this data by storing it in a row-oriented format (in most cases,
    binary), as this is the most natural and efficient way of streaming it to permanent
    storage. As data analysis and visualization often requires the processing of individual
    sensor streams, a columnar data storage format speeds up analysis workloads considerably
    [4], [6]. For example, pre-filtering or simple analysis of individual signals,
    could already be performed on the edge, avoiding unnecessary data transmission
    to cloud systems. In other words, a column-oriented layout makes data analytics
    more straightforward, since data is already in a suitable format for analysis.
    Further, column-oriented layout enables compression algorithms to work more efficiently,
    as alike data is implicitly grouped together. Consequently, we need to be able
    to convert the incoming data stream from the row-oriented layout—as the sensor
    processing system generates it—to a column-oriented format usable for analysis
    workloads directly already on the edge device near the origin of data. Fig. 2:
    Example of raw sensor data streams for three different sensors, sampled synchronously.
    This two-seconds excerpt shows more than 50 000 sensor readings in each plot.
    The right part of the plot shows a histogram of the value distribution. Show All
    However, transferring and storing this data—even after such transformations—puts
    a lot of pressure on network and storage systems; it is thus necessary to compress
    it with a suitable compression method, and do so as soon as possible in the data
    pipeline. We show an example of sensor values for the described use case in Figure
    2. It can be clearly seen that there is a lot of noise in the sensor signal, a
    combination of measurement noise and physically existing process noise of the
    monitored asset. On the other hand, there are limitations on the range and resolution
    of possible sensor values, creating opportunities for optimizing compression,
    specializing them specifically for such sensor systems. Additionally, during normal
    operation, there is only a low dynamic range in these values (as indicated by
    the histogram on the right of Figure 2). Consequently, in this paper we show that,
    by taking these data properties into consideration in the design of our compression
    stack, it is possible to substantially improve both compression ratio and throughput,
    while taking the limited processing capabilities on the edge into account. Overall,
    we therefore propose an end-to-end system, which 1) transforms the data layout
    from the row-oriented input stream into a column-oriented analytics format, 2)
    efficiently handles compression, specifically targeted at sensor data and 3) show
    that all this can be done on a low-power edge device. SECTION III. Approach We
    start by describing the core ideas of our approach in designing an end-to-end
    system: the two aspects of streaming data to columnar storage and compressing
    floating-point data more efficiently are discussed separately in the following
    two subsections. Although the design criteria for these two aspects in our end-to-end
    approach are conceptually orthogonal, the resulting effects are intertwined, and
    both need to be considered together to reach an optimal sensor handling system
    suitable for a large range of sensor streams, including the one discussed in Section
    II. We start with explaining the core idea of converting row-oriented data into
    column-oriented format. A. Streaming to Columns As shown in Figure 1, in a typical
    monitoring system for industrial assets, sensor readings arrive in parallel for
    one particular instant ti. The data for these sensor readings is sent to an edge
    computer, which handles permanent storage. State-of-the-art systems store the
    arriving data streams to permanent storage as is, i.e., they continuously append
    the row-oriented sensor values at instant ti to a file. This is typically followed
    by encoding, compression and manual conversion steps. As this layout is not helpful
    for most analytics operations, we transpose the data stream and structure it into
    a columnar layout. In the remainder of this paper, we will call this step layout
    transformation. Since we do this layout transformation on the fly and at the edge
    to avoid high transformation costs needed when applied during post-processing,
    we need to take the limited compute and memory resources on such edge devices
    into account. We employ a double-buffered and pipelined streaming scheme for the
    layout transformation and apply it before the encoding and compression step. As
    the length of the data stream can be assumed to be infinite (systems are operating
    24/7) and the memory resources of any computing device are limited, the streaming
    scheme relies on assembling the arriving streams into buffers with predefined
    capacities. As a consequence, only a limited number of consecutive rows from the
    row-oriented data stream are buffered in memory. We call this set of consecutive
    rows buffered together a row group (see dashed red rectangles in Figure 1). We
    assume there is enough physical RAM to buffer two of these row groups completely
    in memory. While buffering on different persistent storage technologies (either
    explicitly via memory mapped files or implicitly via system swap) is potentially
    possible as well, it creates a variety of additional challenges that are out of
    scope of this paper. We assume further that the number of sensors does not change
    dynamically— resulting in a rigid data schema—and that all sensors are sampled
    synchronously by a signal acquisition system1 (cf. Figure 1 ③). Such a system
    typically processes a window of fixed (small) length and sends the raw sensor
    data (see Figure 2) together with several analysis results. Based on these assumptions,
    the actual approach for buffering a row group is the following: since we know
    the set of columns (i.e., number and types of sensors data streams) and the row
    group size (rgs: the number of rows in a row group, can be configured based on
    the memory limitations on the device), we can calculate the buffersize for one
    row group as buffersize=rgs× ∑ c∈Columns sizeof(type(c)). View Source For the
    pipelined layout transformation, we allocate two of these buffers and use them
    in an alternating fashion for data buffering. These buffers have a transposed
    layout in which different columns are structured at different offsets. A row that
    arrives at instant ti is cut up and scattered to the right spots in the buffer.
    Once a buffer is completely filled (or, to ensure timely streaming, when a certain
    time interval has passed), it is terminated, prepared for serialization and written
    to a file or to the network. We execute this preparation and writing phase using
    a background thread to decouple buffering from data writing. This enables us to
    avoid disruption in data ingestion, as we are dealing with a continuous stream
    of values arriving from the sensors, and since preparing and writing data can
    be time consuming (see Section V). Having data in column-oriented format, we then
    also enable benefits like improved compressibility due to the implicitly achieved
    grouping, as subsequent values in the data storage come from the same sensor.
    It also enables further optimizations, which are discussed in Section V-F. After
    a predefined number of row groups have been streamed to a file, this file is terminated
    and new data goes into a new file. To keep up with the incoming data stream, all
    parts of the pipeline need to support the throughput of the arriving data stream,
    including the two software components layout transformation and preparing data
    for serialization. B. Two-Step Floating-Point Compression We base the compression
    component of our system on a proven compression approach, consisting of 1) a fast
    reversible reorganization step and 2) the usage of a proven general-purpose compressor.
    This base scheme is thereby similar to the Blosc library [8] or the shuffle filter
    in HDF5 [9]. The rationale behind our approach is to prepare the stream of floating-point
    numbers so that the general-purpose compressor is much more efficient on parts
    of the reorganized data stream. For this, we take a window of fixed length of
    the buffered stream of one sensor and reorganize the floating-point binary representations
    of this single sensor stream, using a specialized scheme we call byte stream split,
    into multiple, more \"similar\" streams (see Figure 3). These intermediate streams
    are then concatenated and compressed using a general-purpose compressor (e.g.,
    zstd). For reconstruction, we just decompress the compressed stream and combine
    the bytes from the resulting streams to assemble values and reconstruct the original
    floating-point stream. This reconstruction requires information about how many
    values from the stream are split up in one pass; we call this blocksize in later
    parts of this paper. Fig. 3: Example floating-point byte stream split. Here we
    are transforming a simple stream of two floating-point values 2.3010745 = 0x401344CE
    and 2.3111875 = 0x4013EA7F into four separate streams. Show All C. Combining Streaming
    and Compression Although the streaming and compression components’ design and
    optimization are conceptually orthogonal, combining them in a complete end-to-end
    system creates synergies, as converting data streams first to a columnar layout
    is beneficial for compression. Specifically, for compression, we are explicitly
    relying on the \"similarity\" of streams in column-oriented layout after the on-the-fly
    layout transformation step, as such similarity is less likely for streams in row-oriented
    layout. Consequently, our approach enables efficient compression of data streams
    by exploiting a layout transformation step, early on, at the data ingestion point.
    Therefore, the combination of streaming and compression steps makes deployment
    on an edge system practical. SECTION IV. Implementation in Apache Parquet To demonstrate
    the feasibility of our end-to-end approach, we implement a fully working system
    based on Apache Parquet, which is a widely used columnar storage format in the
    data analytics community. We start with a short introduction to Apache Parquet
    and argue why it is a suitable skeleton for our system. Next, we discuss implementation
    details of streaming sensor data to Apache Parquet and then present our efficient
    two-step compression implementation. Finally, we sketch how both ideas are combined
    into a fully working system. A. Apache Parquet Apache Parquet [7] is a column-oriented
    storage format integrated into popular computing and data analytics frameworks
    such as Apache Spark, Apache Arrow and Pandas, and is suitable for efficient representation
    of tabular data. The format splits all rows into smaller chunks called row groups;
    in fact, we borrowed this term from the Apache Parquet format specification in
    Section III-A. A serialized Apache Parquet file consists of so-called pages, which
    contain the actual data values (at least one page for each column in each row
    group) and meta data information. A page also serves as the smallest unit of compression.
    Our implementation is based on the C++ implementation of Apache Parquet, which
    is part of Apache Arrow [10]. Since its version 0.11.0, Apache Arrow offers two
    possible API-alternatives to write to a row group. In the first approach, the
    developer calls AppendRowGroup() on a ParquetFileWriter instance, providing data
    for one row group in a column-by-column order. The other approach, AppendBufferedRowGroup(),
    internally buffers the complete row group until it is terminated, so that data
    can be appended in an out-of-order fashion to all the columns. The Apache Parquet
    specification describes two possibilities of compressing data pages. First, encodings
    provide a sort of lightweight compression, which may already considerably reduce
    data size. Second, encoded pages may be compressed using one of several supported
    general-purpose compressors, like zstd or Brotli [11], [12]. These two complementary
    steps in the Apache Parquet specification match our two-step compression approach:
    we insert our data reorganization scheme as an additional encoding and combine
    it with a general-purpose compressor available in Parquet. B. Streaming to Apache
    Parquet When using AppendBufferedRowGroup() for buffering, the Arrow implementation
    relies on the use of one separate memory allocation per column and growing all
    these allocations as data is ingested. This approach imposes no memory consumption
    restriction early on, and the memory consumption grows with ingesting more data,
    resulting in a very flexible buffering scheme. However, it potentially requires
    repeated memory reallocation calls, to serve the memory demand on growing row
    groups. Consequently, operating system overheads in managing all these buffers
    grow when streaming to a large number of columns. Furthermore, for streaming one
    row to Apache Parquet, only a single value is added to every column, which amplifies
    any overhead the library calls entail. This version of the API is thus not well
    suited for the layout transformation step. To avoid these performance problems,
    we allocate two large chunks of memory, each with enough space for all rows and
    columns in the row group, and use a logical column-wise layout on top of each.
    Adding new values to this buffer as they arrive from the stream one by one now
    only incurs the cost of copying the data to the buffer with offsets that are computed
    based on the logical column-wise layout. Once all data for one row group is buffered,
    we swap buffers and continue with layout transformation on the other buffer. As
    described before, the buffer that has been fully filled is now prepared for serialization
    in a background thread. To do so, we call the Apache Arrow API AppendRowGroup()
    to create a new row group and put the data into this newly generated row group
    of the Apache Parquet file, with a single call to the library for each column.
    This invokes several data serialization steps. First, the data is grouped into
    data-pages, which subsequently are encoded and compressed one-by-one. Meta information
    describing the column is added last in another data page. It is important to note
    that the buffers for layout transformation are allocated only once, since allocating
    and deallocating them repeatedly induces an unnecessary overhead. C. Two-Step
    Compression For the implementation of our two-step compression scheme, we use
    the encoding and compression functionalities of Apache Parquet implemented in
    Apache Arrow. With that, we rely on the general-purpose compressors integrated
    into Apache Arrow which simplifies our implementation. We implement byte stream
    split as a new encoding in Apache Parquet. In contrast to other encodings in Apache
    Parquet, this encoding does not reduce the data size on its own, but prepares
    the input for compression. The implementation consists of an encoder and decoder
    for writing and reading2 respectively. Again, we use the existing infrastructure
    in Apache Parquet that stores the size of blocks (in our case the page size),
    which is necessary for decompression. To ensure that the additional byte stream
    split step does not slow down the compression stack, we evaluated three alternative
    implementations: a simple implementation using two nested loops and two manually
    vectorized implementations using SSE and AVX2 instruction sets. The simple implementation
    loops over the input, splits a single element and scatters the bytes to the corresponding
    streams. While this simple implementation serves as a baseline which can be used
    on any system, we determined that the compiler does not efficiently auto-vectorize
    all code paths. The SSE and AVX2 implementations use a combination of shuffle,
    unpack and permute instrinsics with different lane and stride sizes. As an example,
    we describe the single-precision version utilizing SSE -intrinsics (see Figure
    4), using an optimal sequence of dependent stages. This transformation sequence
    requires four stages for the encode-transformations and two stages for the decode-transformations.
    It processes 16 consecutive single-precision values at the same time, split up
    in four 128-bit SSE registers. After loading data into the registers, the encoder
    applies a series of interleavings using unpack intrinsics to distribute the bytes.
    The encoding transformation finishes after four stages where each SSE register
    contains the bytes of 16 values for each corresponding output stream. Then, the
    encoder stores the registers to the intermediate buffers. The decoder works analogously,
    but only requires two stages. Our performance comparison on the systems in Table
    I shows a few important characteristics. First, all (automatically or manually)
    vectorized code is bound by memory bandwidth. This implies that smaller block
    sizes benefit from the CPUs cache hierarchy and thus can yield higher throughput,
    similar to how memcpy behaves. Next, the achievable throughput for the handcrafted
    vectorized version is at least one order of magnitude higher than what compressors
    like zstd can achieve. Fig. 4: Single-precision floating-point byte stream split
    encoder-decoder transformation sequences using unpack intrinsics. This processes
    16 single-precision floating-point values simultaneously. Note that the encoding
    and decoding are independent operations. For simplicity and ease of explanation,
    they are visualized back-to-back here. The grayed out arrows represent the same
    operations in the respective stages, and the annotations are left out for clarity.
    Show All SECTION V. Experimental Evaluation In this section, we provide an empirical
    evaluation of our implementation. We start by proposing a number of evaluation
    questions that help to characterize the various aspects of our approach’s performance.
    Then we describe our experimental setup and go over the evaluation aspects one
    by one. We finish this section with a short discussion of our results and findings.
    A. Evaluation Questions As in the previous sections, we first analyze the streaming
    and compression aspects of our approach individually. Then, we look at the combination
    of the two aspects. Our experiments are organized around the following question
    sets (QSx): QS1: What is the impact of the number of sensors and row group size
    on layout transformation’s performance? QS2: How much can byte stream split improve
    compression ratio of general-purpose compressors? How much does it speed up compression
    and decompression? Which compression algorithms and settings work best? QS3: Since
    our approach is primarily targeting streaming systems at the edge, we raise and
    answer these questions: What is the sustained throughput achieved by our complete
    system for various realistic scenarios? What impact do the storage medium options
    have on the overall performance? B. Experimental Setup Since our approach is meant
    to apply to both high-end server systems and low-power edge computers, i.e., close
    to the machine or system producing the sensor data, we evaluate performance on
    two types of systems: One is equipped with a powerful server CPU, the other is
    a passively cooled low-power edge system, as it is typically found in industrial
    installations. The details of those two systems are in Table I. TABLE I: Configurations
    of the systems used for evaluations Both systems run Ubuntu 20.04 and use the
    performance CPU frequency scaling governor. For implementing our streaming approach,
    we use C++, compile with g++ compiler version 9.3.0 and use the options ‘-O3 -march=native’.
    As our implementation of byte stream split is upstream in the Apache Arrow and
    Parquet-MR libraries, we use the Conda packages arrow-cpp and pyarrow version
    0.17.1 and link against this version of the library for the streaming experiments.
    Unless specified otherwise, we use default parameter values in the libraries for
    our experiments. C. QS1: Layout Transformation Performance In the first experiment,
    we assess the impact of the number of sensors (columns) and row group size on
    the performance of layout transformation. For our test, we create a small driver
    program that runs the experiments. This driver program uses pre-recorded floating-point
    sensor values and feeds them into our layout transformation procedure. This procedure
    carries out the layout transformation and fills a pre-allocated floating-point
    buffer with the columnar layout: For each row, it iterates over all sensor values
    in the row-oriented input stream and appends one value to each column. To make
    the results easily reproducible, and to avoid any influence of the underlying
    storage system, we disable serialization to persistent storage for these experiments.
    With Apache Parquet for our implementation, it is essential to also consider the
    performance impacts of Apache Parquet serialization, which happens right after
    the layout transformation. For this reason, we conduct two experiments for every
    combination of the number of columns and row group size that we test: 1) we only
    conduct the layout transformation, and 2) we additionally serialize to Apache
    Parquet. Our evaluation in Figure 5 shows that the configurations with a larger
    number of columns have lower throughput, and those with a medium number of columns
    yield the best throughput. This behavior directly stems from the performance of
    the layout processing step and its efficiency in using in the cache hierarchy
    of the system, as transforming the layout of the arriving sensor values almost
    exclusively consists of memcpy operations. Another observation in the plots is
    that except for very small row group sizes, serializing data into Apache Parquet
    (using the background thread) does not bring additional overhead. This is also
    expected since we do not have compression or encoding enabled for this experiment
    yet, so preparing the Apache Parquet format is a matter of several memcpy operations.
    For small row group sizes, the overhead of these many small memcpy operations
    becomes noticeable, though. Fig. 5: Layout transformation performance for varying
    rows per row group and number of columns. Show All For certain row group sizes,
    a cache eviction effect considerably decreases the performance for layout transformation.
    Such performance degradations happen when the number of columns is greater than
    the number of cache sets, and the addresses of \"neighboring\" elements in each
    row fall into the same cache set in the transposed layout. In our experiments,
    we mainly use row group sizes that avoid this problem, so this effect can only
    be seen for number of columns above 200 and large row group sizes in Figure 5.
    Additionally, this problem can be avoided by padding the allocation of row groups,
    so that we do not hit the same cache lines. D. QS2: Compression Performance In
    this section, we investigate the overall performance of our two-step compression
    approach, which includes the study of compression metrics. We use our raw sensor
    data stream (see Figure 2) as benchmark data set. The different encoding/compression
    configurations are tested with the following approach: Using python as test driver,
    we load 1 GB of data (250M single precision values) into an Apache Arrow Table
    object, and call write_table() from the pyarrow.parquet package with the respective
    encoding and compression settings. This results in all data being put into a single
    row group. Data is then written to a file on a local SSD-drive. After that, we
    clear the Linux page cache and read the file that has been just written to measure
    read performance. The compression ratio is computed as the number of bytes in
    the Apache Arrow table divided by the size of the resulting Apache Parquet file.
    For throughput measurements, we average our results across 10 runs. The results
    of this compression performance evaluation are visualized in Figure 6. They show
    that our approach improves the best state-of-the-art variants in all three metrics,
    compression ratio, write and read throughput, and does so on both hardware platforms
    under test. Further, it is apparent that the performance of our approach does
    not depend on any particular general-purpose compressor, as it improves the compression
    ratio for all tested compression algorithms, regardless of compression level,
    and improves write and read throughput in almost all cases. On the other hand,
    our approach enables significant improvements for compression algorithms like
    Snappy and LZ4, which almost do not compress the data streams at all without preprocessing.
    These now provide significant gains in compression ratio, more than any other
    unmodified state-of-the-art algorithms we tested. Yet, they still provide highest
    compression and decompression speeds among all examined alternatives. For those
    compression algorithms that have a compression level setting (zstd, gzip, and
    Brotli in Figure 6), using higher compression levels does not yield significant
    compression ratio improvements when compared to additional computational effort
    that has to be invested. Adding the byte stream split reorganization step already
    improves the compression ratio more than any compression level increase could
    yield. This suggests that our method is especially well-suited for simple, high-throughput
    compression algorithms. Fig. 6: Comparison of compression ratio, write and read
    throughput for Atom and Xeon platforms across a set of encoding/compression combinations.
    The first measurement (None) is there for reference, providing a baseline of the
    I/O-system’s speed. We are using dictionary encoding—a fast and lightweight alternative
    encoding in Apache Parquet—as an additional baseline. Show All The plot also shows
    that both tested systems benefit from our two-step approach, for compression and
    decompression. E. QS3: Sustained Throughput Performance In the last experiment,
    we evaluate the overall performance for streaming sensor data to Apache Parquet
    combining all ideas presented in this paper. In this experiment, we take a buffer
    in memory of row-oriented floating-point values originating from multiple sensors
    as the input, using the data from Section II. We take values from this row-oriented
    buffer and feed them to our layout transformation step, as described before. In
    the next step, we perform byte stream split in combination with zstd on the columns
    with default compression level of 1. This is followed by persistent storage of
    the result into an Apache Parquet file. Since we are interested in sustained throughput,
    we set the Linux kernel parameter vm.dirty_bytes [13] to a small value of 100
    MB. This ensures that our driver process blocks when it is waiting for the data
    to be flushed to storage. To compensate for any OS-buffering effects that might
    happen regardless, we write files with a size of 20 GiB for these experiments.
    We run the experiments for a realistic configuration of 200 sensors and a row
    group size of 500 000. To illustrate the impact of the underlying storage medium
    on throughput, we write into various mediums: SSD, HDD, and a comparably powerful
    10 GBit NAS, as three typical mediums used for back-end storage. Like before,
    we also include results when not persisting at all as a \"best case\". As baseline,
    we examine what throughput the Apache Arrow BufferedRowGroupWriter solution can
    deliver. Additionally, we perform one large experiment for 20 000 sensors and
    a row group size of 20 000. We obtain various insights from the results in Figure
    7, and start by examining compression ratios. We achieve a compression ratio of
    1.34 with zstd, while compressing byte stream split encoded streams with zstd
    results in the much better compression ratio of 1.78. These ratios correspond
    to the compression of 20 GiB raw sensor streams into an Apache Parquet file of
    size 14.95 GiB and 11.24 GiB, respectively. In addition to this considerable reduction
    in disk storage utilization, the additional improvement in compression ratio has
    another performance implication: As a result of this higher compression ratio,
    less amount of data passes I/O, which can be a potential bottleneck in many systems.
    Fig. 7: Sustained end-to-end performance for a configuration of 200 columns and
    a row group size of 500 000. Large shows the results for a configuration of 20
    000 columns and a row group size of 20 000. Note that the y-axes have different
    scales for the two test systems, annotated by Atom and Xeon. Error-bars indicate
    min and max throughput of experiment runs. Show All We next look at the results
    on the edge system. Since the 10 GBit NAS is only connected via the 1 GBit network
    interface of the edge system, we hit the limit of this Ethernet interface in the
    uncompressed setting. We also observe that compression even reduces the overall
    throughput here, since the processor cannot compress the columns fast enough (see
    Section V-F for a potential solution to this problem). Even though the large configuration
    delivers a throughput that is considerably lower, it would still be fast enough
    for typical deployment scenarios. For the Xeon system, we note that writing uncompressed
    to local persistent storage is bound by write throughput of the HDD and SSD, respectively.
    We highlight here that compression improves the overall performance, and the higher
    compression ratio that can be achieved by the byte stream split approach has a
    positive influence on sustained performance for local storage. In case of the
    NAS, adding compression slightly reduces throughput, since the storage system
    is faster than zstd throughput in this case. We discuss a potential improvement
    to avoid this compression bottleneck in Section V-F. In all cases, our approach
    clearly outperforms the Apache Arrow baseline, which is bound by some implementation-specific
    overhead. F. Discussion a) Streaming performance and memory consumption: While
    Apache Arrow supports buffering data since version 0.11, our approach outperforms
    this implementation by one order of magnitude (see Figure 7). However, we trade
    that performance by having less flexibility in the final size of the row-group.
    That being said, the fixed row group sizes is not a huge limiting factor by itself
    for our use case: row group sizes can be pre-configured based on the system’s
    memory resources, which effectively compensate for the inflexibility. b) Row group
    size: The maximum possible row group size is tuned based on the amount of memory
    the system can use for writing. In addition to performance aspects when writing,
    the row group size also impacts read performance. Larger row group sizes are generally
    preferred for data analysis, since decompression performance is higher on larger
    buffers, while small row group sizes increase the share of meta data information.
    Another aspect that is affected by row group size is latency of writing data to
    and reading it from permanent storage. Larger row group sizes mean that data takes
    longer until it can be serialized to the storage system. If latency is a concern,
    however, the system needs to be augmented with an orthogonal component capable
    of reading the row group buffers. c) Compression performance: Our buffering approach
    enables performing the compute-intensive tasks on all the columns in parallel,
    on the available processing units in a system. More specifically, as our experiments
    show, compression is the dominating component in preparing serialization to disk
    on the edge system (see Section V-E). This opens up exploiting parallel processing
    for an originally sequential data stream. While we postpone this idea to future
    work, it gives additional potential for optimization. SECTION VI. End-to-End Case
    Study In this section, we show how our system can be used in a realistic end-to-end
    setting. We describe a typical setup for the example scenario we introduced in
    Section II and prove the feasibility of this scenario for the edge-system from
    Table I. For the setup we evaluate in this section, we simulate multiple row-oriented
    data streams (④ in Figure 1) on a dedicated machine. Each such data stream generates
    its own time stamps and gets converted into a separate columnar layout. We generate
    single-precision floating-point data samples at a rate of 25 600 Hz from a real
    recording (see Figure 2), which is important for reproducing compression behavior.
    One stream simulates 32 such sensors, representing an edge-case for state-of-the-art
    monitoring systems (typical setups in this domain often still have fewer sensors).
    Together, each simulated turbine thus produces a data stream with a raw data rate
    (not considering time stamps and other meta data) of: 25600 Samples sec and Sensor
    ×4 Bytes Sample ×32 Sensors≈3.28 MByte sec View Source A data generator sends
    these streams via TCP over a GBit Ethernet connection to the edge device (Edge-System
    in Table I). There, for each stream, we apply our complete stream handling pipeline,
    consisting of layout transformation, encoding and compression and store the resulting
    parquet files to an HDD3. We let this pipeline run long enough to ensure that
    for each stream, eight row-groups are filled completely. We keep increasing the
    number of simulated turbines connected to the one system handling this data, up
    to the point where it fails to handle all data streams. At this point, the generator
    is forced to pause data generation, resulting in a gap in the data stream (backpressure
    via TCP). Choosing a row group size of 800 000 rows, which results in a maximum
    required buffer size of roughly 6 GBytes4 for layout transformation, is a viable
    configuration for the Edge-System from Table I. For analysis, we evaluate the
    maximum gap5 between consecutive samples for any of the simultaneous streams,
    induced by a bottleneck in stream handling. Fig. 8: The maximum gap remains zero
    up to a certain number of simulated turbines where some part of the pipeline cannot
    handle the amount of data anymore. Show All Our results in Figure 8 show that
    our system can seamlessly handle up to 23 simulated turbines, equivalent to the
    overall throughput of ≈ 75 MByte/sec. Further analysis shows the limit here is
    disk throughput, which underlines that both byte stream split and compression
    are essential steps in the pipeline. Otherwise, the system’s capacity in handling
    data streams will be hit already for fewer simulated turbines, as indicated by
    the curve for the uncompressed stream in Figure 8. We further measured the resulting
    file size, which shows that the proposed compression approach shrinks the required
    storage space considerably compared to existing compression alternatives. SECTION
    VII. Related Work a) Time series storage systems: There are several existing solutions
    that handle storage of time series streams. InfluxDB provides a time series database
    which uses a custom storage backend [14]. To tackle their problems with some workloads,
    they work on a new database core called IOx, which also uses Apache Parquet for
    persisting data [15]. TimescaleDB, an extension on PostgresQL, utilizes so called
    Hypertables to manage one consistent view over multiple chunks of data [16]. While
    this increases performance of typical workloads for time series data, it still
    employs row-oriented storage. Building on top of HBase, OpenTSDB provides a solution
    targeting distributed setups [17]. All of these solutions require some kind of
    data serialization and parsing to and from text to ingest data, which is prohibitive
    for edge systems where processing resources are scarce. b) Distributed and event
    stream storage systems: A lot of research effort has been spent on distributed
    streaming storage and processing systems. While some of the use cases and problems
    discussed in such works [2], [18] are similar or related to our use case, distributed
    stream processing is not what we aim for with this paper. Furthermore, many publications
    regarding distributed stream processing consider event streams and the challenges
    that come with it [19]–[24]. This is fundamentally different from our usage scenario
    in both data type and data rate or regularity. An event is usually much more complex
    (it may arrive in the form of text or structured text) than a single sensor reading
    (one 4-byte floating-point value) and arrives with a much more irregular frequency.
    c) Compression: While there is a huge amount of work on data compression in industry
    and academia, we want to highlight shuffle filters in HDF5 [9] and the blosc library
    [8]. These two approaches are use a similar approach for improving general purpose
    compression. SECTION VIII. Conclusions We presented a holistic approach for handling
    large and fast sensor data in modern monitoring systems through efficient streaming
    into a columnar data layout combined with an effective data representation and
    compression scheme. Our compression implementation is based on Apache Parquet
    and has been upstreamed, making it possible for the broader community to reuse.
    Our in-depth investigations show that our approach is practicable on low-power
    edge devices, providing data in an analytics-ready format directly at the data
    generation site. By converting the data into a columnar format on the edge, followed
    by efficient data reorganization and compression, our approach improves both efficiency
    and effectiveness when compared to existing solutions, and it can be easily generalized
    to other and even lossy compression schemes. This opens up a set of new possibilities
    for analytic use-cases in large scale industrial settings, providing new opportunities
    such as effective predictive maintenance or dynamic operational optimizations
    based on the rich data streams available already today from such systems, but
    which often are left unused due to the missing processing options. Our approach
    allows future optimizations like parallelizing compression workloads, further
    enhancing sensor monitoring and acquisition systems. Thus, it brings significant
    improvements to systems for streaming of sensor data and has a direct influence
    on real-time monitoring systems across the industry. Authors Figures References
    Citations Keywords Metrics Footnotes More Like This Brief Industry Paper: Digital
    Twin for Dependable Multi-Core Real-Time Systems — Requirements and Open Challenges
    2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS)
    Published: 2021 An Empirical Survey-based Study into Industry Practice in Real-time
    Systems 2020 IEEE Real-Time Systems Symposium (RTSS) Published: 2020 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and
    Internet Computing, CCGrid 2021
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Living on the edge: Efficient handling of large scale sensor data'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Dokuchaev V.A.
  citation_count: '7'
  description: digital transformation is a rapid transformation of the global information
    space, which affects the market, society, persons, business and the state. The
    main drivers of digital transformation are innovative technologies such as Artificial
    Intelligence, Intelligent Apps and Analytics, Intelligent Things, Digital Twins,
    Cloud and Edge computing etc. However, the introduction of these technologies
    leads to ever increasing risks of unauthorized access to confidential information.
    This circumstance makes the problem of classifying the risks caused by digital
    transformation and the development of new approaches to managing these risks urgent.
  doi: 10.1109/EMCTECH49634.2020.9261544
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2020 International Conference... Digital
    Transformation: New Drivers and New Risks Publisher: IEEE Cite This PDF V. A.
    Dokuchaev All Authors 6 Cites in Papers 811 Full Text Views Abstract Document
    Sections I. Introduction II. Analysis of Intelligent Information Processing Systems
    III. Prerequisites and Stages of Digital Transformation IV. General Scheme for
    Managing Risks Arising in the Information System When Introducing Innovation Services
    V. Classification of Innovative Development Risks for Infocommunication Enterprises
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    Digital transformation is a rapid transformation of the global information space,
    which affects the market, society, persons, business and the state. The main drivers
    of digital transformation are innovative technologies such as Artificial Intelligence,
    Intelligent Apps and Analytics, Intelligent Things, Digital Twins, Cloud and Edge
    computing etc. However, the introduction of these technologies leads to ever increasing
    risks of unauthorized access to confidential information. This circumstance makes
    the problem of classifying the risks caused by digital transformation and the
    development of new approaches to managing these risks urgent. Published in: 2020
    International Conference on Engineering Management of Communication and Technology
    (EMCTECH) Date of Conference: 20-22 October 2020 Date Added to IEEE Xplore: 30
    November 2020 ISBN Information: DOI: 10.1109/EMCTECH49634.2020.9261544 Publisher:
    IEEE Conference Location: Vienna, Austria SECTION I. Introduction Currently, there
    is a rapid transformation of the global information space, which affects the market,
    society, persons, business and the state. This process is called “Digital Transformation”.
    Digital transformation is the use of modern technologies to dramatically increase
    the productivity and value of enterprises [1]. However, along with the obvious
    business benefits, digital transformation leads to an increase in potential risks
    [2], [3], such as: risk of job cuts; risk of information leakage when using UC&C;
    risk of personal data leakage about the intimate aspects of the subject''s life;
    risk of substitution of the digital image of the subject; risks of digital identification;
    risks of artificial intelligence when making decisions; risks of using blockchain
    when concluding smart contracts; risks of new criminal offenses, etc. According
    to the analytical agency IDC, the market size of technologies and services in
    the field of big data continues to grow by about 30% annually and in 2018 reached
    $ 41.5 billion. In the published results of the Digital Universe study conducted
    by EMC and IDC, a forecast multiple growth of the “digital universe” by 2020 relative
    to the level of 2013. It is expected that the volume of data in the cloud segment
    will double, which will make up 40% of the data in the digital universe, which
    is facilitated by the development of wireless technologies, smart devices, the
    Internet of things, augmented and virtual reality technologies. For an enterprise,
    this means that digital technologies not only significantly affect the efficiency
    of its work - they radically change its structure, business processes, organizational
    principles, management methods, and for telecom operators, they can also significantly
    increase the package of services provided. Among the trends of digital transformation
    [4], the following main ones can be distinguished. 1) Artificial Intelligence
    Organization The ability to leverage AI to improve decision making, create new
    business models and ecosystems, and reimagine the customer experience. According
    to analysts at research company Gartner, 59% of organizations are still collecting
    information to develop their AI strategies, while the rest have already made progress
    in the exploitation or decision making of AI. Today, AI is at the stage of a narrow
    AI consisting of high-level machine learning programs that are focused on a specific
    task (for example, understanding a language or controlling a vehicle in a controlled
    environment) with selected algorithms optimized for a specific task. Over time,
    using AI in the right way will lead to great digital business progress, both in
    solving day-to-day business processes and in dynamic learning. 2) Intelligent
    Apps and Analytics Over the next few years, every software, application and service
    will include a certain level of AI. AI will work unobtrusively against the background
    of many familiar application categories, while creating entirely new ones. AI
    has become the next major battleground in a wide range of software and services
    markets, including ERP aspects. Services that will independently conduct an analysis
    of intellectual processes and extended user experience cannot but be in demand.
    Smart apps also create a new intelligent middle ground between people and systems
    and can transform the way work and the structure of the workplace, as seen in
    virtual customer assistants and corporate consultants and assistants. Advanced
    analytics is a specific strategic area that uses machine learning to automate
    data preparation, information retrieval, and information sharing for a wide range
    of business users, engineers, and professionals. 3) Intelligent (Smart) Things
    Intelligent things use AI and machine learning to interact more intelligently
    with people and the environment. Some intelligent things won''t exist without
    AI, but others are existing things (like a camera) that the AI makes intelligent
    (i.e., Smart cameras). These things act semi-automatically or autonomously in
    an uncontrolled environment for a certain amount of time to complete a specific
    task. Examples include homing vacuum or autonomous agricultural vehicles. As technology
    advances, AI and machine learning will increasingly appear in objects ranging
    from intelligent medical equipment to autonomous cleaning robots for farms. As
    the number of intelligent things increases, there will be a transition from autonomous
    intelligent things to a swarm of shared intelligent things. In this model, several
    devices will work together, independently of or with a person. The concept is
    most widely used by the military, who are exploring the use of unmanned swarms
    to attack or defend military targets. 4) Digital Twins A digital twin is a digital
    representation of a reality or realworld system. In the context of the IoT, digital
    twins relate to objects in the real world and provide information about the state
    of their copies, respond to changes, improve operations and add value. In the
    short term, digital twins can be used to provide assistance with asset management,
    but ultimately have value in terms of operational efficiency and understanding
    of how products are used and how they can be improved. In the future, it will
    be possible to provide digital representations of all aspects of the real world,
    for its global modeling, and subsequent analysis by artificial intelligence. Hypothetically,
    this could lead to an innovative round of AI development. However, it is necessary
    to note the increased risks from the use of digital twins. Especially now (in
    the conditions of COVID-19), the problem of a sharp increase in the risk of unauthorized
    changes in the digital twin of the subject of personal data is becoming urgent
    [5], [6]. Note that this type of digital twin is already turning into an augmented
    reality object. 5) Shifting Cloud Computing to Edge Edge computing is a computing
    topology in which the collection and processing of information, as well as the
    delivery of content, are located closer to the sources of this information. Connectivity
    and latency issues, bandwidth constraints, and more functionality at the edge
    of the network are driving this model forward. If in the model of edge computing
    the content, its calculation and processing are transferred closer to the user,
    then in cloud computing the same services are provided over the Internet. 6) Conversational
    User Interfaces Conversational user platforms are changing the paradigm of business-customer
    interaction. These systems are able to answer a number of the most common questions,
    according to a predetermined algorithm. A possible future application of such
    platforms is the collection of client information on a large scale and its subsequent
    analysis. 7) Blockchain Technology Blockchain is a decentralized, distributed
    and publicly available digital ledger that is used to record transactions on many
    computers, so that the record cannot be changed retroactively without changing
    all subsequent blocks, allowing participants to validate and audit transactions.
    The blockchain database is managed autonomously using a peer-to-peer network and
    a distributed server with reliable time synchronization. The use of blockchain
    removes the endless emission characteristic from a digital asset. This means that
    each digital unit was transferred only once, solving the problem of double costs.
    Blockchain-based digital unit exchange can be done faster, safer and cheaper than
    traditional systems. 8) Event-Driven Model Services based on an event-driven model
    allow both monitoring and managing existing business processes of enterprises,
    and reporting the need to intervene in them under certain circumstances. With
    the advent of AI, IoT and other technologies, certain events can be detected faster
    and analyzed in more detail. 9) Continuous Adaptive Risk and Trust Assessment
    (CARTA) Continuous adaptive risk and trust assessment (CARTA) enables you to make
    real-time decisions about risk and trust using a variety of security techniques.
    The main principle of CARTA is the so-called micro-segmentation, that is, the
    division of the infocommunication system into segments, with protection on each
    of them, which allows monitoring and management of interconnection within the
    virtual network infrastructure. SECTION II. Analysis of Intelligent Information
    Processing Systems This section is devoted to the analysis of systems and methods
    of intelligent information processing, in the context of their implementation
    in the service packages of ICT/Telecommunication companies. A. General Intelligent
    information processing, data mining, data mining - all these formulations are
    variants of the translation of the English phrase “Data Mining”, which is understood
    as a set of methods for detecting previously unknown, significant correlations,
    patterns and trends in data by sifting large amounts of data, subsequently used
    for making decisions in the assigned tasks. Deep data analysis combines methods
    of database theory, statistics, algorithmization, machine learning, artificial
    intelligence, pattern recognition, visualization and a number of other sciences
    [7]. Data Mining differs from traditional methods of data analysis by searching
    for previously unformed requirements based on fuzzy formulations and implicit
    requirements, followed by a visual presentation of the calculation results. In
    other words, deep data analysis due to the processing of a large amount of information
    makes it possible to discover previously unknown patterns and visualize them graphically.
    The main tasks of Data Mining are classification, modeling and forecasting, and
    therefore, data mining is a convenient tool for processing big data, and building
    various kinds of services on this basis. Data Mining methods and algorithms have
    found wide application in the field of analysis of business processes of enterprises,
    called “business analytics”. Business analysis (Business Analytics, BA) is a set
    of tasks and methods used in the work, in the process of interacting with stakeholders
    in order to understand the structure, policies and actions of the organization,
    and recommend solutions that will allow the organization to achieve its goals.
    The main tasks of business analysis are: Reporting Reporting is a formalized representation
    of the past process, which allows you to analyze the parameterized organization
    of business processes and the company as a whole. Analysis Examining reports allows
    you to form a model that will allow you to make the right decisions. The solution
    to this problem allows you to detect previously unknown dependencies. Monitoring
    This task is reduced to assessing the consequences of making various decisions.
    At this stage, a number of proposals and tips for adjusting business processes
    have already been formed using the methods of business analysis. There is a specific
    term for this particular form of monitoring. “Operational Business Intelligence”,
    sometimes referred to as “Real-Time Business Intelligence”, is an approach to
    data analysis that enables data-driven decisions to be made in real time. Predictive
    Analysis At this stage, using the methods of business analysis, data are processed,
    on the basis of which the most probable development of events is formed, the realization
    of which already now allows us to make the most effective decisions. Services
    that solve business analysis problems based on Data Mining require high performance
    from the hardware, in this regard, they have gained great popularity as a cloud
    service. B. Cloud Models for Providing Information Analysis Methods As shown in
    Section I, the names of cloud services have become more and more over time [8],
    and therefore a general designation for all cloud services - XaaS - has appeared.
    This trend has also affected solutions related to the provision of analytics based
    on cloud computing. By combining the Data Mining methodology, the following cloud
    service models are provided: Business Analysis as a Service (BAaaS) This model
    of cloud services provides a wide range of data mining tools in terms of business
    processes of the enterprise. Combining in its structure a mathematical apparatus
    that is demanding on computing power, the provision of this service through the
    Internet, and the use of servers allocated for these tasks, it can be used on
    the overwhelming number of devices. File Analysis as a Service with Technology
    (FAaaST) This service provides data mapping to improve the efficiency of companies''
    infocommunication systems. FAaaST allows you to reduce the number of duplicated
    files, combining them into a kind of topology, which in turn ensures the efficient
    use of processing centers and data storage systems, allows you to make the most
    rational backup and data migration. Sentiment Analysis as a Service (SAaaS) This
    service considers social networks, media platforms, information services as “social
    sensors”, allowing you to analyze the collected subjective information from them
    over time, and find dependencies between them, as well as predict possible behavioral
    patterns. Analysis as a Service (AaaS) This model is basic in terms of providing
    access to data mining methods using the Internet. The services analyzed above
    are in some part a modification of AaaS. The above models of cloud services allow
    companies to analyze their digital resources using the computing power of the
    provider of these services. C. Insight as a Service Experts state that more than
    70% of large organizations are already buying external data and by end of 2020
    almost 100% of companies will be doing the same. The ability of companies to collect
    large amounts of data is not equated with the processing of this data, therefore,
    does not mean that the data is used effectively. Most organizations around the
    world do not have the necessary resources to analyze all generated and already
    accumulated data. Much of this data is not used and therefore becomes useless.
    According to a study by IBM Business Technology Trends, only 20% of companies
    have the necessary tools and equipment to analyze this entire data stream effectively.
    According to another study by the IBM Center for Applied Insights, various companies
    that use unstructured and structured data use a forward-looking analysis system
    for their processing and analysis, which allows them to increase the performance
    indicators of business processes. A service that provides tools for intelligent
    processing of various types of data provided in a given context, the use of which
    is possible through the Internet and is called Insights as a Service (IaaS). Insight
    (from the English insight - insight, understanding, sudden guess) is a term describing
    a complex intellectual phenomenon, the essence of which is an unexpected, partly
    intuitive breakthrough to understanding the problem posed and “suddenly” finding
    its solution. This type of service differs from SaaS and the cluster of AaaS services,
    as they offer analytics only tools for analysis, and insights as a service, in
    addition to various types of data provided, offers specific templates for modernizing
    business processes based on the analysis. There are various ways insights are
    provided, for example: Business benchmarking - that is, benchmarking based on
    business benchmarks; Optimization of business processes; Increased business productivity.
    There are three types of data that can be used when creating insights, namely:
    Company data, i.e. data aggregated from the SaaS database; Usage data. It is a
    type of web data that is collected while using a SaaS application; Syndicated
    data, that is, data from third parties that can be integrated into company data
    to create information-rich datasets. According to a study by KDNuggets, large
    telecommunications companies channel about 15% of their IT investments into Insights-as-a-Service
    and other cloud offerings. By 2021, investments will increase to 35%. SECTION
    III. Prerequisites and Stages of Digital Transformation There are a different
    ways for companies digital transformation [9]. Below we will consider the prerequisites
    for the digital transformation of companies in the ICT/Telecommunication industry.
    emergence of instant messengers once hit the revenue of mobile operators from
    SMS, and the development of VoIP poses a threat to voice communications. Operators
    compensate for losses through data transmission, but the following is gradually
    happening: data transmission is growing at a faster pace than revenue from it,
    while costs grow along with the increase in transmission volume. Data traffic
    in Western Europe, for example, increased from 2010 to 2016 by 10–15 times (in
    some countries - by 30–40 times). At the same time, revenue from data transmission
    increased by only 1.4-1.7 times, depending on the country and the method of accounting
    for package offers. There are certainly reserves for revenue Changing the Paradigm
    of Service Consumption The growth within the second stage of digitalization, that
    is, without a radical change in the business model. This is the further development
    of digital interaction with the client: a more convenient and intuitive personal
    account, an expansion of the range of services available through it, a convenient
    link between a personal account and physical channels. Possibility to Reduce the
    Costs of Telecom Operators The total cost reduction with wider use of digital
    technologies can be 10-20%, while the number of calls to the call center on service
    issues can be reduced by 40%, and the number of interactions with customers in
    stores is reduced by 30%. The Ability to Use Information Analysis Tools Using
    big data for customer base management: analyzing customer behavior, generating
    targeted offers based on the analysis of their behavior and managing churn. Due
    to such actions, the leading operators in customer base management manage to increase
    their revenue by more than 10%. Due to the fact that digital transformation completely
    changes the structure of companies, it is rational to divide the entire process
    into stages. The first stage is to expand the range of services offered without
    directly changing the business model. This stage is characterized by the targeted
    use of digital technologies, mainly for cost optimization. The second stage implies
    the expansion of the range of services not related to the telecom industry within
    the existing business model. The third stage is changing the existing business
    model. The transformation of a telecom operator into a “digital service operator”
    requires operators to have the following competencies: Competence to form the
    Optimal Set of Digital Products and Services Operators should navigate a wide
    range of unrelated markets (from entertainment to road safety) and effectively
    monitor any new service opportunities. Mobile Application Development Competencies
    After the formation of a package of services, operators need to determine which
    of the services and applications will be developed using their own infrastructure,
    and which - through partners, subsidiaries, etc. The heterogeneity of apps and
    services can make it harder for users to use mobile apps, leading to a loss of
    customer engagement. Competence for Forming Partnerships The formation of a “digital
    platform” model implies new revenues for the service operator in terms of providing
    advertising spaces, delegating the provision of services, etc. Accordingly, the
    importance of such competencies as partner selection, negotiation, determination
    of the optimal revenue distribution model and subsequent management of relations
    with a partner is increasing. According to the research company Bain, five conditions
    must be met for a successful digital transformation of a telecom operator and
    the implementation of Data Mining methods: Clearly define customer requirements
    at every stage of the customer journey (purchase, bill payment, solving a technical
    problem, etc.). Only by understanding the client''s expectations, the operator
    will be able to provide a “seamless” docking of digital and physical communication
    channels. Simplify your portfolio of products and services. Digital transformation
    of a complex portfolio will require a very large investment and run the risk of
    failure. A complex is a portfolio that is characterized by any of the following
    elements: a large number of archived tariffs that are no longer for sale, but
    are still serviced by the operator; a large number of additional packages used
    in addition to the basic tariff (for example, when a client gains the necessary
    traffic volume by purchasing additional packages on the Internet every day); various
    variations of offers in roaming; non-standard terms of tariffs (for example, additional
    charges for “special” numbers). Also, the digital transformation process is complicated
    by the presence of tariffs that differ from region to region, but in some cases,
    regional specificity is inevitable. Reorganize the work of IT processes, integrate
    the agile development methodology focused on the use of iterative development,
    dynamic formation of requirements and ensuring their implementation as a result
    of constant interaction within self-organizing working groups consisting of specialists
    of various profiles. Reconfigure the operating model from a regional or functional
    structure that has existed in the industry for a long time to a structuring model
    by customer segments (B2C, B2B, and others). At the same time, it is important
    to create cross-function teams that are responsible for specific proposals to
    individual segments. Simplify the organizational structure and ensure quick decision-making.
    It is necessary to develop digital competencies in all departments in the company,
    including sales and service departments. SECTION IV. General Scheme for Managing
    Risks Arising in the Information System When Introducing Innovation Services Let
    us consider the general scheme of the algorithm, which contains a list of organizational
    and managerial measures for organizing the innovative strategy of enterprise development
    (Fig. 1). There are following main blocks. Analysis block, consisting of the following
    tasks: Analysis of the impact of the external environment on the activities of
    the enterprise: Fig. 1. Example of an algorithm for development an ict/telecommunication
    enterprise strategy. Show All Changes in trends: goods, services, consumer behavior;
    Decisions of state authorities; Changes in competition policy; Changes in technology,
    etc. Analysis of the impact of the internal environment on the activities of the
    enterprise: Marketing management; Financial management; Personnel management;
    The choice of methods, methods and technologies) strategic planning; Research
    of the process of interaction with clients, etc. In this block, the formation
    of an information and analytical base begins, which is used at all stages of planning
    and implementation of the enterprise development strategy. In the block of goal-setting,
    a system of goals and objectives for the development of an enterprise is built
    in accordance with the SWOT analysis of the organization. Depending on the choice
    of approach to strategic management, the following tasks can be solved in this
    block: Correlation of the goals and objectives of the enterprise with specific
    quantitative criteria and parameters; Based on their forecasts of the development
    of individual elements of the external environment, the planning of the results
    of the enterprise''s activities is carried out. In the Action Planning Block,
    an algorithm is developed for using methods, mechanisms and tools to achieve the
    stated goals and objectives of the enterprise. In the block “Strategy implementation”,
    organizational, managerial and economic technologies are used for the optimal
    development of the enterprise. In the block for evaluating the effectiveness of
    the strategy, control and evaluation of measures for the development and implementation
    of an algorithm for monitoring the implementation of the organization''s development
    strategy is carried out. The effectiveness of the measures used to increase the
    success of the applied strategy is evaluated. The development of a strategy for
    the innovative development of infocommunication enterprises based on risk management
    should be carried out on the basis of the following principles Compliance with
    the principle of complexity is due to the need to take into account the totality
    of risks that can affect the application of the strategy of innovative development
    of an infocommunication company. Compliance with the principle of economic feasibility
    is due to the implementation of measures within the strategy of innovative development
    of infocommunication enterprises, based on the assessment of resources and potential
    effects from their use. Compliance with the principle of adaptability is due to
    the need for the enterprise to change the innovation strategy depending on the
    emerging threats to ensure the economic security of the company. Compliance with
    the principle of software and hardware feasibility is due to the need to assess
    the potential for the introduction of various kinds of managerial innovations
    in the processes of infocommunication companies, depending on their technological
    feasibility. Compliance with the principle of minimizing financial risks and threats
    is due to the need for optimal distribution of resources of an infocommunication
    enterprise to ensure innovative development. Compliance with the principle of
    optimal prioritization is due to the need to rank projects and activities in the
    context of their risk management. Compliance with the principle of systemic development
    in the context of the strategy of innovative development focused on risk management
    is due to ensuring, first of all, the systemic development of the enterprise,
    and, if possible, but not necessarily, the improvement of the specific areas of
    the enterprise. Compliance with the principle of focusing the strategy on the
    effectiveness of risk management is due to the need to allocate resources based
    on planning to obtain previously formulated economic effects. SECTION V. Classification
    of Innovative Development Risks for Infocommunication Enterprises To develop an
    algorithm, it is necessary to identify and classify the risks arising in the business
    processes of the enterprise. Based on the typical structure of business processes
    of an ICT/Telecommunication company, the following classification of risks is
    proposed (Table I). Table I. Risk classification of the ict/telecommunication
    industry It should also be remembered that when digital transformation of business
    processes is carried out, there is still a risk called “black swan”. An example
    of such a risk can be sanctions to restrict the use of any technologies, the supply
    of high-tech equipment to a particular country, the impossibility of technical
    support for one or another information system. SECTION VI. Conclusion The given
    main trends of innovative infocommunication services make it possible to classify
    the stages of digitalization of enterprises in the infocommunication industry
    and formulate the basic requirements for the risk management algorithm in the
    context of the enterprise innovative development strategy. Analysis of the main
    directions of development of innovative infocommunication services showed that
    various kinds of innovative services, mainly based on cloud technologies, which
    are to some extent correlated with the systems of intelligent information analysis,
    are gaining the greatest popularity. In the process of analyzing the stages of
    digital transformation, it was noted that this process leads to a change in the
    business model of enterprises, which is necessary for the provision of innovative
    services. It should be noted that in addition to the obvious benefits and drivers,
    digital transformation leads to an increase in potential risks. The main risks
    include the risk of incomplete information about business processes subject to
    digitalization; the risk of incomplete implementation of control functions; the
    risk of violation of regulations (including violations of legislation) for the
    implementation of business and technological processes and “black swan” risk.
    Authors Figures References Citations Keywords Metrics More Like This Financial
    Data Mining in Chinese Public Companies: Corporate Performance and Corporate Governance
    in Business Groups 2008 International Conference on Intelligent Computation Technology
    and Automation (ICICTA) Published: 2008 Data mining in Cloud Computing 2021 5th
    International Conference on Computing Methodologies and Communication (ICCMC)
    Published: 2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2020 International Conference on Engineering Management of Communication
    and Technology, EMCTECH 2020 - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Digital transformation: New drivers and new risks'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Khare S.
  - Totaro M.
  citation_count: '25'
  description: The Internet of Things is generating an enormous amount of data. Analyzing
    and managing that data requires programming and statistical approaches. Big Data
    technology operates on this massive data and pushes new products, applications,
    future research and developments to improve decision making. In this paper, we
    explore Big data in IoT driven technologies and the issue of the four V's in Big
    Data. This paper also highlights the importance of pre-processing, metadata, data
    storage formats, data management and how big data is closely associated with IoT
    technologies. Today, with the rapid growth of IoT, everything is connected. To
    stay ahead of demands, new technologies such as Cloud Computing and Edge Computing
    are transforming IoT organizations. This paper discusses in which layers edge
    computing operates in the IoT reference model to achieve low-latency and greater
    efficiency solutions. This paper also reviews the IoT reference model layers that
    are associated with cloud computing, the structure of cloud computing architecture,
    data acquisition and data cleaning. This paper also discusses on various cloud-based
    IoT platforms such as AWS, Google Cloud IoT, Microsoft Azure, and Cisco IoT Cloud.
    We examined the importance of Big Data visualization, gives insights on various
    visualization tools and techniques. Lastly, this paper also addresses various
    significant challenges of Big Data in IoT, security issues and future research
    directions.
  doi: 10.1109/ICCCNT45670.2019.8944495
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2019 10th International Confe... Big Data
    in IoT Publisher: IEEE Cite This PDF Shivanjali Khare; Michael Totaro All Authors
    23 Cites in Papers 1738 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Big Data IV. Intelligent Data Processing V. Data Storage
    Formats and Databases Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: The Internet of Things is generating an enormous amount of data.
    Analyzing and managing that data requires programming and statistical approaches.
    Big Data technology operates on this massive data and pushes new products, applications,
    future research and developments to improve decision making. In this paper, we
    explore Big data in IoT driven technologies and the issue of the four V''s in
    Big Data. This paper also highlights the importance of pre-processing, metadata,
    data storage formats, data management and how big data is closely associated with
    IoT technologies. Today, with the rapid growth of IoT, everything is connected.
    To stay ahead of demands, new technologies such as Cloud Computing and Edge Computing
    are transforming IoT organizations. This paper discusses in which layers edge
    computing operates in the IoT reference model to achieve low-latency and greater
    efficiency solutions. This paper also reviews the IoT reference model layers that
    are associated with cloud computing, the structure of cloud computing architecture,
    data acquisition and data cleaning. This paper also discusses on various cloud-based
    IoT platforms such as AWS, Google Cloud IoT, Microsoft Azure, and Cisco IoT Cloud.
    We examined the importance of Big Data visualization, gives insights on various
    visualization tools and techniques. Lastly, this paper also addresses various
    significant challenges of Big Data in IoT, security issues and future research
    directions. Published in: 2019 10th International Conference on Computing, Communication
    and Networking Technologies (ICCCNT) Date of Conference: 06-08 July 2019 Date
    Added to IEEE Xplore: 30 December 2019 ISBN Information: DOI: 10.1109/ICCCNT45670.2019.8944495
    Publisher: IEEE Conference Location: Kanpur, India SECTION I. Introduction Internet
    of Things (IoT) is generating massive quantities of data every second. Bernard
    Marr in [1] projects the increase in data creation from past years. The Internet
    daily generates a massive amount of data through various services such as web
    searches, social-media platforms such as Facebook, Instagram, and so on. IoT is
    accelerating these statistics by connecting physical devices (sensors) to the
    Internet, providing variety of services to its users, while collecting different
    kinds of data. IoT involves data management and data analysis techniques. Data
    analysis requires an exclusive approach. Many organizations accomplish the data
    generated from IoT devices and use these insights for smart decision-making. Kashmir
    Hill in [2] cites an example where a US-based store, Target, was able to detect
    the pregnancy of women with advertising and purchases they made through credit
    card and analysis of their routine purchases against historical data. IoT has
    many applications such as in healthcare, manufacturing, industrial IoT, smart
    homes, smart cities, and so on. IoT devices require the right form of sensors
    to be deployed in the right areas to capture the data. The collected data can
    vary, depending upon the service provided by the IoT device. IoT sensors have
    few restrictions such as environment sensitivity, distance limitations, etc. IoT
    sensors gather information from the environment, forwards it to the central node
    where data analysis take place, and then forwards the information to another node.
    Consider a smart home, which consists of multiple IoT devices such as thermostats,
    smart lighting systems, smart door locks, smart gardening, personal assistants,
    and so on. Across the entire house, there are bundles of nodes passing formation
    to the main server which stores or communicates this information the cloud. The
    user should be aware of restrictions by the sensors, which affect the data analysis,
    in order to avoid inaccurate or bad data. The use of IoT devices shows a continuous
    collection of data. Gathering this data leads to observations that are remarkable.
    Big Data deals with the data set, analyzes and extracts meaningful information
    from collected data. There exist various online sources that provide open access
    data collections [3] [4]. The objective of this paper is to highlight the association
    between Big Data in IoT and create a relationship that determines the processing
    and analysis of data collected by IoT devices. This paper discusses big data management
    techniques at various levels such as collection, processing, analysis, and so
    forth. This paper also provides a survey of the existing IoT related technologies
    such as cloud and edge computing. The paper also delivers many attributes that
    are not addressed in current survey papers, along with some new challenges and
    future research. SECTION II. Related Work A review of the IoT literature suggests
    that there is considerable eagerness in the field of IoT systems [5]–[8]. These
    studies, however, centered their research direction entirely on the architecture,
    applications and investments. Yunhao et. al in [9] reviewed the state-of-the-art
    of big data. They introduced general background, examined several applications
    and related technologies. Archenaa et. al in [10] focus on the seriousness of
    performing big data analysis on the data collected by the healthcare and government.
    In contrast, our work focuses on the techniques and mechanisms of data collected
    by the IoT devices and establishes a correlation between them. Figure 1 illustrates
    topics covered in this paper. Table I Structure of the paper SECTION III. Big
    Data Gathering such massive data integrates storing the data generated from multiple
    technology nodes. IoT networks operate depending upon the analysis of this data.
    The network generate different data types, noise and some redundant data. Since
    IoT sensors and devices degrade over time, Big Data organizations reduce the risk
    of errors and maintain accurate decision making. Wetzkar et al. in [11] show various
    examples in the area of Industrial IoT (IIoT) where they faced issues in identifying,
    analyzing failures and troubleshooting failures. There is the need of automatic
    data collection and automatic error corrections. Traditionally, big data involves
    four dimensions, also known as Four V''s. They are: Volume: amount of data Variety:
    different types of structured and unstructured data Velocity: processing speed
    of the data Veracity: truthness of the data Some research scholars list these
    issues of Big Data as 3Vs'' by removing Veracity, or by adding more issues such
    as Value, Validity, and so on. Ishwarappa and Anuradha in [12] considered 5Vs'',
    whereas Khan in [13] considered 10Vs'' as Big Data issues. A. Volume IoT devices
    stores massive data such as employee records, stock information, invoices, purchase
    history, card details, along with location details, and so on. Such additional
    information is called a meta-data that helps to contextualize the knowledge. The
    majority of large organizations invest in cutting edge databases, data management
    firms, distributed systems, and cloud storage for storing digital information.
    The quantity of data generated and collected from IoT devices is essential as
    all the data needs to be measured, stored or transmitted to other nodes. This
    has become a challenge as the amount of data has become very large and traditional
    database technology is no longer favorable. B. Variety Big data involves the gathering
    of target data from a wide range of sources simultaneously. IoT data involves
    data from different kinds of sensors, non-numerical items such as mp3, mp4, radio
    signals, and so on. Handling this variety of data is a challenge. The meta-data
    should be stored in correct context with the collected data and should allow to
    associate future data collections automatically. Another issue when considering
    the current state-of-art of IoT and change in techniques is the ability for storage
    software to adapt to these changes. For example, change of video quality or format
    in sensors. C. Velocity The data produced by sensors or other inputs in IoT devices
    occur at an extremely high rate. This high velocity of data production and collection
    becomes challenging because the data should be handled promptly for new data to
    come in. Moreover, the velocity of data production is not always constant. The
    velocity changes over time; for example, sales of a company increases during a
    certain offer period. Gandomi and Haider in [14] discuss the importance of time
    here. In such situations, there is a need for appropriate planning, processing
    power and storage to avoid data loss and system outage. Although such a commitment
    of computing power may be expensive, it should be planned ahead of time to increase
    the revenue of an organization. D. Veracity IoT sensors do not have margins of
    error in measurement. Wireless sensors can face communication error, hardware
    failure due to shift in the environment, animals or any other factors. As such,
    it is essential that data is properly stored, accurate and complete. The “truthness”
    of data forms the basis of many business decisions. It is necessary to differentiate
    between reliable and unreliable data. SECTION IV. Intelligent Data Processing
    One common solution to the problem encountered during data collection and use
    of big data in IoT is the intelligent use of software. Some general approaches
    of intelligent data processing are Pre-processing and Meta-data creation. A. Pre-Processing
    The data collected by IoT sensors is often sent to different locations and processed
    there. The large amount of data produced needs to be sent quickly to the processing
    location. Data can be lost entirely or in part if there is latency. Baker et al.
    in [15] discuss instances of medical emergency situations where such delays in
    communication can lead to possible detrimental effects on patients. Often, in
    many situations the data regarding particular event is required for further processing.
    Pre-processing helps to reduce the volume of data. It moves the processing function
    closer to the sensors and reduces the amount of data to be sent. Smart sensors
    in IoT uses built-in resources to perform pre-processing before sending it further.
    Antonini et al. in [16] presents a design framework for smart audio sensors. These
    smart sensors locally perform the computations on raw audio streams before transmitting
    those features wirelessly to IoT gateway. B. Meta-Data Creation After processing,
    data is stored to be used again. Metadata is used to put the stored data into
    context. When needed the stored data is queried for information. Given the variety
    and volume of data, it can take a considerable amount of resources to process
    that data again. Meta-data to speed up the process by adding additional data that
    describes or references the stored data. Park et al. in [17] proposed a conceptual
    metadata model for sensor data abstraction in IoT environments. This model helps
    to create a structured format for the low-level context and helps in higher abstraction
    procedures. Dawes et al. in [18] describe a deployable system to bridge the gap
    between data management. They propose a tiered metadata recording system using
    a non-semantic and a semantic wiki related to a single sensor. Stevens in [19]
    discuss the importance of meta-data in big data analytics. SECTION V. Data Storage
    Formats and Databases The relational database is used in traditional technical
    environments to store the data. They are used extensively and dominate most of
    the commercial data storage. The characteristics of IoT data make the traditional
    relational-based data management impractical. The use of a relational database
    can make the overall querying slow and might result in delayed responses. A. Structured
    Databases An IoT program can be made more flexible by involving few restrictions,
    but it often makes the system less efficient. It is necessary to consider trade-offs
    while developing an IoT system. Relationship among the data elements establishes
    the structure of a database, making it efficient for storage and querying. The
    structured database leads to a lack of flexibility with modern software methodologies.
    IoT devices have achieved technical advances and are able to communicate with
    almost any “thing.” It requires an expansion of a network to accommodate more
    devices and their software. This is known as horizontal scalability. With the
    relational database, it becomes difficult to break these multiple clusters of
    machines. Sarkar et al. in [20] proposed an architecture to tackle the issue of
    scalability. B. Unstructured Data Storage Modern data today has made relational
    data management less efficient. Unstructured (also referred to as document store)
    and Semi-structured databases are developed to meet the needs of different types
    of data collected by IoT devices. Kumar in [21] discuss various techniques for
    maintaining unstructured data in IoT. According to Alnsari et al. in [22], due
    to massive developments in information technology, there is a need for solutions
    that should enable unstructured data management and analysis. A new range of databases
    such as MongoDB and NoSQL are becoming more significant in IoT developments. They
    are unstructured database platforms that are proven effective in many IoT applications.
    NoSQL is also a non-relational database that can efficiently store key-value pairs,
    wide columns or search engines data, and so on. It makes them ideal for big data
    use and in particular IoT device development. Serdar in [23] discusses NoSQL in
    detail and outlines the advantages such as flexibility and overcoming horizontal
    scalability in detail. SECTION VI. Data Management Collecting and utilizing data
    can be useful but it also carries many risks and responsibilities. There are legal
    and ethical issues involved in collecting data without consent. This results in
    data breaches, which damages individuals'' privacy. Guan et. al in [24] discuss
    how hackers can access the IoT data by multiple sources and use it for illegal
    benefits. A. IoT Device Security Many IoT devices that are accessible via the
    network should have some sort of credentials by which to connect. Unfortunately,
    this is not the situation. Many IoT devices are shipped without authentication
    to connect with or have default credentials which are highly insecure. In many
    situations, those devices that come with complex authentication details do not
    include credential changing manual which makes them vulnerable to attack once
    the credentials become known. IoT devices have thus become ideal targets for hackers.
    With the growing number of IoT devices, there is an increased risk of attackers
    present in a botnet. IoT devices can be used for multiple functions such as distributed
    denial of service attacks. This results in reducing the performance of the device
    along with “blacklisting” the network for hosting malicious attacks. Cluley in
    [25] describes the vulnerability of IoT devices to Mirai Botnet and stresses the
    importance of changing one''s IoT device''s default password. Greene in [26] points
    to a huge DDoS attack on IoT devices such as cameras, lightbulbs, and thermostats
    by a botnet. The use of default or weak passwords in IoT devices makes them more
    susceptible to such attacks. SECTION VII. Data at the Edge A. IoT Reference Model
    According to Cisco''s IoT reference model in Figure 1 [27], the data is in motion
    in the lower layer of IoT. The dynamic data comes from the sensors and there exists
    a continuous communication of messages to actuators. Recent advancements in the
    IoT architecture has added more processing near the Edge of the IoT network. Edge
    pushes the intelligent processing capabilities closer to the network edge, which
    gives flexibility and makes the system much more responsive. There is a slight
    difference between Edge and Fog computing. Fog pushes the intelligence to the
    fog node, which resides in local area networks, close to the data. At this node,
    some of the information might transmit to the cloud. However, the edge node directly
    pushes the data to the “thing.” In some cases, the key data is transmitted to
    the cloud for further analysis. Fig. 1. Internet of things(iot) reference model
    [27] Show All B. Data Acquisition The sensors in Level 1 of the IoT Reference
    Model [27] are key sources of data in the IoT system. The sensors or “things”
    (such as computers) are connected to the Internet. IoT gateways provide an access
    route for devices without IP-address (such as lights, locks, gates, etc) to the
    Internet. A gateway provides a bridge between sensors, actuators and the Internet
    or Intranet with the use of different communication technologies. These communication
    technologies differ in terms of connectivity types, interfaces, or protocols.
    For example, IoT devices use more common technologies such as Bluetooth, LE, ZigBee,
    and Z-wave. Given the volume of the data collected by sensors, data filtration
    reduces the amount of data that is forwarded to the back-end for further processing
    or analysis. Also, edge computing helps to provide the IoT gateway security. The
    IoT architecture connects devices directly to the cloud for processing and analysis.
    In Figure 2, all data from the sensor is sent to the cloud which leads to an unnecessary
    traffic and security risk. Waiting for messages to and from increases latency,
    which might affect real-time responses. This may not be favorable in emergency
    situations. It requires the resources to store and process the data, which is
    expensive. From Figure 2, we can estimate the latency of each part of network
    as: Latency=T1+T2+T3+T4 View Source With a gateway, T2, T3, T4 are replaced by
    much faster interactions and data is transmitted to the cloud only when needed.
    This reduction in transmission of data requires considering the rate and type
    of data. Sensors in the IoT system collect huge amount and variety of data, which
    results in considering the combination of four V''s in making a necessary decision.
    Cisco in [28] describes how devices send the right data to cloud for big data
    analytics and storage. SECTION VIII. Data in the Cloud A. IoT Reference Model
    From the IoT reference model in Figure 1, the accumulated data in level 5 is abstracted
    for analysis. It involves processing with the queries on data sets. The data is
    first cleaned using various techniques such as normalization, standardisation,
    and other terminologies prior to the analysis and is then made available to level
    6. Here, software applications of IoT devices provide back-end support for users.
    It generates business intelligence reports, analytics for decision-making, system
    management and other uses to control the IoT system. Level 7 involves collaboration
    and processes beyond the IoT network and application. Fig. 2. Total time for an
    iot response Show All B. Data Cleaning Before the data collected by the sensor
    is ready for analysis, this raw data is required to be cleaned to make it technically
    correct and consistent. It should be done systematically and should be well documented
    for reproducibility and possible automation. Jonge et al. in [29] explain the
    steps involved in improving and refining data. The collected data comes with some
    identification. To reach technical correctness this raw data is encoded, decoded,
    converted, stripped, tagged and combined with meta-data. After this processing,
    data may still be inconsistent and unexpected. It requires domain knowledge of
    the IoT device to get past any compilation errors in the system. This processing
    is required before analysis in level 5. C. Why is Data Stored in Cloud? Cloud
    infrastructure services such as Infrastructure-as-a-Service (IaaS), Platform-as-a-Service
    (PaaS) and Software-as-a-Service (SaaS) allows organizations to avoid the need
    for in-house equipment, power, networking and IT support. Cloud, as a part of
    the Internet, can be accessed from anywhere, can shrink and grow according to
    the consumers demand. Clouds can be both public and private. Clouds such as Amazon
    Web Services (AWS) and Microsoft Azure, Google Cloud Platform are public clouds
    whereas private clouds sit within the security firewalls of an organization. IoT
    devices have relatively small storage and processing power. The big-data generated
    from the IoT devices is stored, aggregated, processed and analysed in cloud. Moving
    the data towards the cloud gives “infinite” processing and storage capabilities.
    Below the cloud there are data centers, with numerous severs or host computers.
    Each host computer has multiple instances of Virtual Machine (VM) running as an
    application on the actual hardware, looking as a separate machine. The specification
    of these instances are taken into account and thus, the organisation pays for
    the additional resources used. VM''s are an example of IaaS. However, web, blog-hosting,
    and IoT platforms are an example of SaaS which are more expensive than primary
    IaaS. D. Cloud Architecture Figure 3 represents the IBM reference architecture
    infrastructure [30]. Cloud services such as IaaS, PaaS and SaaS are on the top
    left, while physical infrastructure is in the lower section. Consumer tools and
    in-house IT are used by users to interact with the Cloud Services. The service
    creation tools allows sustaining cloud resources along with important nonfunctional
    aspects such as security, performance, resilience, consumability, compliance and
    overall governance. Cloud architecture can be virtualized across many data-centers.
    Fig. 3. IBM cloud reference architecture [30] Show All E. Cloud Based Iot Platforms
    IoT platforms include a dashboard to display and control devices. Additional features
    such as data collection, data management, testing, software updates and inventory
    management are also prominent. Amazon Web Services (AWS) [31] is an IoT platform
    that includes a wide range of tools and services to deploy, setup and manage IoT
    solutions. It consists of four main products. They are: AWS IoT Core - base to
    built an IoT application AWS IoT Device - allows easy addition and organization
    of devices AWS IoT Analytics - provides service for automated analytics of massive
    amount of varied IoT data, including different data types AWS IoT Device Defender
    - support security mechanism of IoT systems The AWS environment provides scalable
    and secure environment for IoT systems. Google Cloud IoT [32] builds and manages
    IoT systems of any size and complexity. This cloud service includes: Cloud IoT
    core - allows connecting various devices and collects their data Cloud Pub/Sub
    - provides real-time stream analytics and processes event data Cloud Machine Learning
    Engine (ML) - allows the building of ML models and use of data received from IoT
    devices Google Cloud IoT includes a number of service that might be useful for
    building a comprehensive connection of networks. Microsoft Azure IoT Suite [33]
    provides security mechanisms, easy integration, and scalability. The Suite can
    easily connect to many devices from different manufacturers, collects data analytics
    and use the IoT data for machine learning purposes. The suite also offers preconfigured
    and customisable solutions to match requirements of the project. Cisco IoT Cloud
    Connect [34] presents an end-to-end convenient platform for mobile cloud based
    IoT solutions. This service supports data and voice communication, customization
    of IoT applications and various monetization opportunities. The cloud consists
    of a complete package of monitoring functions, device management, advanced security
    measures, and scalability. With the growth of IoT devices, Cisco developed the
    kinetic platform supporting Edge and Fog computing. The kinetic platform manages
    IoT devices and gateways by giving support for data reduction, event processing,
    response, and data transfer to the cloud. SECTION IX. IoT and Big Data Visualization
    Big data generated by IoT devices (after collecting and analyzing) have to be
    represented in a visual way that allows humans to understand such analyses in
    an intuitive way. Visualization often allows gaining additional benefits or interpretations
    from a data set, providing more meaningful information. Along with this, presenting
    convincing graphics of the data helps to communicate those results to a wider
    range of audiences. Many algorithms and statistical methods are used on a large-scale
    and high-dimensional varied data which helps in the visualization of those data
    sets. The relationship between geometric objects within a data set is established
    using various parameters. Therefore, data visualization has become an important
    strategy for many business organizations to generate maximum revenue by improving
    decision making. There are several very powerful data visualization tools and
    techniques developed for IoT applications. A. Data Visualization Techniques Techniques
    such as simple plots, charts, maps, line or bar graphs, diagrams and matrices
    can be a very powerful way of highlighting any inconsistencies in the data set.
    This allows uncovering complex tables or numerical summaries and easy understanding
    of the results. Several techniques such as matrix methods in data mining, aggregations
    of attributes, dimensionality reduction techniques [35], [36] are highly used.
    Big data visualization cannot be approached using conventional techniques. Wang
    et. al in [37] propose a method called Discriminative Generalized Eigendecomposition
    (DGE), based on separation of multi-dimensional feature that could be useful in
    finding better discriminant vectors. This method deals with both Gaussian and
    non-Gaussian distribution. Zhong et. al in [38] proposed a RFID-Cuboid model which
    visualizes the realtime big data from cloud. This model can be used by end-users
    for their daily operations in a practical and feasible way. B. Data Visualization
    Tools It is important to decide on the appropriate tool to be used for visualization
    to utilize the full potential of the collected data. Before exploring different
    visualization platform, the organization should identify its end-goals, identify
    its purpose, keep in mind its target audience, and should concentrate how to make
    the context more appealing. Several very sophisticated tools such as Plotly [39]
    and Sisense [40] provide some level of data analytics along with data visualization.
    Plotly enables the user to build charts using R or Python programming languages.
    It builds custom web applications using Python, provides access to open sources
    libraries for R, Python and JavaScript. Sisense is a cloud-based platform that
    has easy to use drag-and-drop interface. It supports natural language queries
    and can handle multiple data sources. Tableau [41] is a leading data visualization
    tool that has easy interface and interactive visualizations. Many large organizations
    rely on Tableau to generate meanings from their collected data. It has features
    such as automatic update, quick sharing, smart dashboards, and so on. There are
    several other tools that can deal with massive and complex IoT data. Microsoft
    Azure and Power BI [42], outstanding tools that can deal with any amount and type
    of real-time data. It provides several analytical power such as large integration
    capabilities, learning curve, along with drag-and-drop interface. ELK stack Kibana
    [43] is another tools that provide certain advance analytics such as exploring
    correlation between different observations, machines learning features to identify
    relationships between data events and so on. Grafana [44] provides services to
    query, visualize, create alerts and notifications along with several other capabilities.
    SECTION X. Challenges and Future Research With the rate of data growth and expansion
    of IoT networks, it is important to have an accurate data of the environment.
    Organizations should acquire a specific skill set to deal with the analytical
    analysis of big data. The data collected by the organizations should be well structured
    and should be made compatible for use. To meet the demands of accurate data, it
    is necessary to connect a wide range of devices at any point and at any time.
    Therefore, there is need for investments in the field of sensors, data security
    and analytical capability to meet supply chain demands. The collection, processing,
    analysis and visualization of data set is a challenging task. Analysis of data
    based on specific data formats can limit the efficiency of the results. It is
    important to have full knowledge of the IoT domain in order to decide on the structure
    and format of the data collected by the sensors. Lack of this knowledge might
    result in dirty or garbage data, which can be costly. The issue of the 4 V''s
    also pose a challenge while dealing with big data in IoT. Nerkar et. al in [45]
    discuss data isolation in cloud computing as another challenge. Common resources
    shared in a cloud platform may cause the problem of inconsistency and latency
    in data content. Erway et. al in [46] describe about the challenge of efficiently
    proving the integrity of data stored at dishonest cloud servers. Patil et. al
    in [47] addresses security and privacy challenges as applied to the healthcare
    industry. As IoT devices collect and analyze data in a decentralized model, performing
    exhausting analysis operations while preserving privacy might be a challenge.
    Even though the current technologies have achieved great results, there exists
    a wide scope in security and privacy concerns for the data collected by IoT devices.
    The communication overheads between the IoT devices that lead to latency must
    be optimized to achieve efficient results. With the growth of huge data, there
    is exist, storage overhead on the servers. Consumers who use IoT devices for personal
    use might lack the technical knowledge required to understand or process the software
    requirements of the device. Some IoT devices and their software lack accurate
    information for users to make consenting decisions. It is necessary to make IoT
    software for personal use user-friendly and should always requests user''s consent
    before sharing or making any decision. SECTION XI. Conclusion IoT has transformed
    many domains such as healthcare, infrastructures, manufacturing, retail, personal
    use and so on. As the data collected by IoT devices became big it became necessary
    to analyze this Big Data. Big Data has recently become more prominent in the IT
    technology, where it helps in product optimization, improves decision making and
    saves energy. As a result, Big Data has contributed substantially to IoT technology.
    Considering the huge amount of complex data produced by IoT devices, the analysis
    and visualization of that data has helped organizations meet demands and gain
    real-time business insights. Along with this, edge computing and cloud computing
    play highly important roles in aggregating large amounts of data and managing
    big data from anywhere in the world. This paper does restrict itself to big data
    techniques in IoT but these techniques themselves are very viable for future research.
    In this paper, we discussed the issue of 4 V''s in Big Data and how they are related
    to IoT. We discussed various data structure and data management approaches that
    should be used while managing Big Data in IoT. We discussed which layers in IoT
    reference model functions with respect to the existing and developing technologies
    such as edge and cloud computing. We also presented various cloud based IoT platforms,
    their key features and how they support organizations to handle massive and complicated
    big data. We discussed how data visualization approach is useful to interpret
    the meaning of data, along with several visualization tools and techniques. Lastly,
    we presented several challenges and future research work. Authors Figures References
    Citations Keywords Metrics More Like This Integrating Internet-of-Things with
    the power of Cloud Computing and the intelligence of Big Data analytics — A three
    layered approach 2015 2nd Asia-Pacific World Congress on Computer Science and
    Engineering (APWC on CSE) Published: 2015 Exploration of Geological Informatization
    Based on The Internet of Things and Cloud Computing in The Era of Big Data 2022
    6th International Conference on Computing Methodologies and Communication (ICCMC)
    Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2019 10th International Conference on Computing, Communication and Networking
    Technologies, ICCCNT 2019
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Big Data in IoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bhattacharjee A.
  - Barve Y.
  - Khare S.
  - Bao S.
  - Gokhale A.
  - Damiano T.
  citation_count: '22'
  description: With the proliferation of machine learning (ML) libraries and frameworks,
    and the programming languages that they use, along with operations of data loading,
    transformation, preparation and mining, ML model development is becoming a daunting
    task. Furthermore, with a plethora of cloud-based ML model development platforms,
    heterogeneity in hardware, increased focus on exploiting edge computing resources
    for low-latency prediction serving and often a lack of a complete understanding
    of resources required to execute ML workflows efficiently, ML model deployment
    demands expertise for managing the lifecycle of ML workflows efficiently and with
    minimal cost. To address these challenges, we propose an end-to-end data analytics,
    a serverless platform called Stratum. Stratum can deploy, schedule and dynamically
    manage data ingestion tools, live streaming apps, batch analytics tools, ML-as-a-service
    (for inference jobs), and visualization tools across the cloud-fog-edge spectrum.
    This paper describes the Stratum architecture highlighting the problems it resolves.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Proceedings of the 2019 USENIX Conference on Operational Machine Learning,
    OpML 2019
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Stratum: A serverless framework for the lifecycle management of machine
    learning-based data analytics tasks'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
