- analysis: '>'
  authors:
  - De Araújo A.P.D.
  - Daniel D.H.J.
  - Guerra R.
  - Brandão D.N.
  - Vasconcellos E.C.
  - Negreiros A.P.F.
  - Clua E.W.G.
  - Gonçalves L.M.G.
  - Preux P.
  citation_count: '1'
  description: Unmanned vehicles keep growing attention as they facilitate innovative
    commercial and civil applications within the Internet of Things (IoT) realm. In
    this context, autonomous sailing boats are becoming important marine platforms
    for performing different tasks, such as surveillance, water, and environmental
    monitoring. Most of these tasks heavily depend on artificial intelligence (AI)
    technologies, such as visual navigation and path planning, and comprise the so-called
    AI of Things (AIoT). In this article, we propose 1) the OpenBoat, an automating
    system architecture for AIoT-enabled sailboats with application-agnostic autonomous
    environment monitoring capability and 2) the F-Boat, a fully functional prototype
    of OpenBoat built with commercial off-the-shelf (COTS) components on a real sailboat.
    F-Boat includes low-level control strategies for autonomous path following, communication
    infrastructure for remote operation and cooperation with other systems, edge computing
    with AI accelerator, modular support for application-specific monitoring systems,
    and navigation aspects. F-Boat is also designed and built for robustness situations
    to guarantee its operation under extreme events, such as high temperatures and
    bad weather, through extended periods of time. We show the results of field experiments
    running in Guanabara Bay, an important aquatic ecosystem in Brazil, that demonstrate
    the functionalities of the prototype and demonstrate the AIoT capability of the
    proposed architecture.
  doi: 10.1109/JIOT.2023.3324525
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 11 Issue: 3 General System Architecture and COTS Prototyping of an AIoT-Enabled
    Sailboat for Autonomous Aquatic Ecosystem Monitoring Publisher: IEEE Cite This
    PDF André Paulo Dantas de Araújo; Dickson H. J. Daniel; Raphael Guerra; Diego
    N. Brandão; Eduardo Charles Vasconcellos; Alvaro P. F. Negreiros All Authors 104
    Full Text Views Abstract Document Sections I. Introduction II. Related Work III.
    Structural Elements of Sailboat IV. IoT Reference Architecture V. OpenBoat Architecture
    Show Full Outline Authors Figures References Keywords Metrics Abstract: Unmanned
    vehicles keep growing attention as they facilitate innovative commercial and civil
    applications within the Internet of Things (IoT) realm. In this context, autonomous
    sailing boats are becoming important marine platforms for performing different
    tasks, such as surveillance, water, and environmental monitoring. Most of these
    tasks heavily depend on artificial intelligence (AI) technologies, such as visual
    navigation and path planning, and comprise the so-called AI of Things (AIoT).
    In this article, we propose 1) the OpenBoat, an automating system architecture
    for AIoT-enabled sailboats with application-agnostic autonomous environment monitoring
    capability and 2) the F-Boat, a fully functional prototype of OpenBoat built with
    commercial off-the-shelf (COTS) components on a real sailboat. F-Boat includes
    low-level control strategies for autonomous path following, communication infrastructure
    for remote operation and cooperation with other systems, edge computing with AI
    accelerator, modular support for application-specific monitoring systems, and
    navigation aspects. F-Boat is also designed and built for robustness situations
    to guarantee its operation under extreme events, such as high temperatures and
    bad weather, through extended periods of time. We show the results of field experiments
    running in Guanabara Bay, an important aquatic ecosystem in Brazil, that demonstrate
    the functionalities of the prototype and demonstrate the AIoT capability of the
    proposed architecture. Published in: IEEE Internet of Things Journal ( Volume:
    11, Issue: 3, 01 February 2024) Page(s): 3801 - 3811 Date of Publication: 13 October
    2023 ISSN Information: DOI: 10.1109/JIOT.2023.3324525 Publisher: IEEE Funding
    Agency: SECTION I. Introduction Aquatic ecosystems are critical components to
    the maintenance of Earth’s life. Nonetheless, various factors can contaminate
    them, including pollution and extreme events like storms and floods due to climate
    change. In this context, the use of autonomous surface vehicles (USV) of type
    sailboats powered by the Internet of Things (IoT) and artificial intelligence
    (AI) can be essential for monitoring and protecting these ecosystems, leveraging
    the AI of Things (AIoT) systems. AIoT enables real-time data collection, analysis,
    and decision making, improving monitoring accuracy and efficiency [1]. Robotic
    sailboats powered by these technologies can autonomously navigate the seas and
    oceans, collecting data and performing various tasks without human intervention.
    It can operate for extended periods, cover vast distances, and collect data that
    would be difficult or impossible to obtain using traditional research vessels
    [2]. Operating over extended periods in marine environments demands robust and
    resilient vessels, components, and software to deal with extreme weather events.
    The IoT capabilities of an autonomous sailboat enable it to collect and transmit
    data on various environmental parameters, such as water temperature, pH, turbidity,
    and salinity, among others. This USV can also have sensors to detect marine life,
    pollutants, and other relevant information. Such data can help scientific research,
    aquatic conservation efforts, and understanding extreme events, such as severe
    storms, heavy rainfalls, and tidal surges, that can cause flooding [3], [4]. In
    addition, AI algorithms embedded in the robotic sailboat can analyze data while
    being collected, supporting decision making based on that data. These algorithms
    can adjust the sailboat course based on changing weather conditions or avoid obstacles
    and hazards. Despite advances in the AIoT, developing autonomous sailboats remains
    a challenge, mainly due to the inherent difficulties of navigation control, infrastructure,
    and energy management, leading much research to be restricted to simulation environments
    [5], [6], [7], [8], [9], [10], [11]. Toward solving these issues, we propose 1)
    the OpenBoat, an autonomous system architecture for AIoT-enabled sailboats with
    application-agnostic autonomous environment monitoring capability and robustness
    to guarantee its operation under extreme events, such as high temperatures and
    bad weather and 2) the F-Boat, a fully functional prototype of OpenBoat built
    with commercial off-the-shelf (COTS) components on a real sailboat (Fig. 1). F-Boat
    includes low-level control strategies for autonomous path following, communication
    infrastructure for remote operation and cooperation with other systems, edge computing
    with AI accelerator, modular support for application-specific monitoring systems,
    and navigation aspects. F-Boat allows the evaluation of different testbed experiments,
    such as navigation and computer vision algorithms, communication technologies,
    and others. In addition, F-Boat is designed and built for applications that include
    continuous autonomously monitoring of aquatic ecosystems for long periods, under
    adverse temperature conditions and heavy rainfall. Such applications include surveillance
    (criminal behavior, such as arms and drug smuggling) and environmental protection
    (water quality and wildlife preservation), among others. Project documentation,
    source code, images and videos of the building process and field deployment are
    available at https://www.natalnet.br/nboat/ and https://github.com/medialab-fboat.
    Fig. 1. F-Boat—the Sailboat autonomous robot. Show All We show the results of
    field experiments that demonstrate the functionalities of the prototype and validate
    the AIoT capability of the proposed architecture. We chose Guanabara Bay, an important
    marine and coastal ecosystem in southeastern Brazil, for the experiments. That
    area is also prone to extreme events that can significantly impact its ecosystems
    and population, including severe storms, heavy rainfalls, and tidal surges with
    eventual flooding [3], [4]. This article follows with Section II that presents
    related works and Section III that presents the main components of a sailboat.
    Then, several aspects of the IoT architecture are introduced and analyzed in Section
    IV. Section V presents the OpenBoat architecture, while the F-Boat prototype is
    presented in Section VI. Section VIII presents the experiments and results obtained
    by monitoring Guanabara Bay. Finally, Section IX presents the final considerations
    and proposals for future works. SECTION II. Related Work This work deals with
    the architectural design and prototyping of AI-based autonomous sailboats powered
    with IoT systems in order to establish seamless connectivity with both onshore
    control centers and other vessels. There are several approaches related to the
    architectural design of autonomous sailboats that inspire this work [12], [13],
    [14]. However, these approaches include neither IoT support nor AI computing capability.
    Therefore, we also broadened the search scope to include the architectural design
    of any AI-enabled and IoT-enabled unmanned vehicles. In a previous work [14],
    we introduced a sailboat prototype built for experimental validation of the presented
    architecture. The solution allows for performing long-endurance autonomous missions,
    such as ocean monitoring, by introducing sailboat techniques and discussing the
    needed sensors, actuators, and control laws. This article also presents the mathematical
    modeling of the sailboat and control strategies developed using PID and fuzzy
    controllers to control the sail and the rudder. Furthermore, this article presents
    a study of the hardware architecture that enables the system overall performance
    to be increased. The sailboat movement can be planned through predetermined geographical
    way-points provided by a base station. Simulated and experimental results are
    presented to validate the control architecture, including tests performed on a
    lake. Underwater robotics can rely on such platform as a surface base vessel.
    However, this article does not discuss the cost or feasibility of implementing
    the proposed control architecture on a larger scale. It also does not compare
    the performance of the proposed control architecture to other existing control
    architectures for autonomous sailboats. Burke [15] presented an open-source cloud
    (4G) connected and controlled self-flying airplane based on a secure, encrypted
    flight stack with a weight of 300 g as an example of a new class of unmanned aerial
    vehicles (UAVs). The avionics portion weighs only 40 g, about 10 times lighter
    than the previous avionics package achieving the same mission. This new class
    of aerial vehicles is enabled due to multiple recent advances, including Linux
    single-board computers (SBCs), cloud computing, software abstraction, secure encryption,
    and ubiquitous wireless communications. Among the contributions of this article,
    it is presented an Internet connectivity for control from any point on earth.
    Freimuth and König [16] proposed a framework that uses four dimensional building
    information modeling (4D-BIM) to extract locations of objects that require inspection
    and compute viable flight missions around the known structures of the construction
    site. During flight, the UAV uses stereo cameras to detect and avoid obstacles
    unknown to the model. An additional software component compares the captured point
    cloud data with the model data, enabling automatic per-object completion checking
    or reconstruction. The prototype is developed in the robot operating system (ROS)
    and evaluated in software-in-the-loop (SITL) simulations to be executable on real
    UAVs. Kitts et al. [17] detailed the development of an innovative robotic boat
    that can perform bathymetric mapping of very shallow coastal, estuarine, and inland
    waters. The boat uses a small waterplane area to provide natural platform stability
    for a multibeam sonar payload, and a navigation system automatically guides the
    boat in a lawn-mowing pattern to map a region of interest. The boat is operational
    and used to generate science-quality maps for scientific and civil use; it is
    also used as a testbed for evaluating the platform for other types of scientific
    missions and for demonstrating advanced control techniques. It is important to
    note that the boat was developed as part of a low-cost student design program,
    which may affect its scalability and robustness. Mendonza-Chok et al. [18] introduced
    a new methodology that establishes a hybrid control architecture based on systems
    engineering concepts, initially defining requirements, risk management, design
    flexibility, logical decomposition, functional classification, verification-validation-integration,
    and the technological plan of the resulting prototype. Garcia and Valvanis [19]
    detailed the technical aspects of a small UAV helicopter specifically designed
    for testbed. The design aims to provide a general framework allowing researchers
    to supplement the system with new technologies and add innovation to the vehicle.
    Koubâa et al. [20] contributed to the Internet-of-Drones (IoD) concept, its requirements,
    security considerations, challenges, and importance. Among different applications,
    they propose a Dronemap Planner, a cloud-based management of drones connected
    through the Internet with cloud integration. The network’s reliability is crucial
    for the system’s successful operation. Hard real-time control, which requires
    zero faults and zero deadline misses, imposes the usage of a very reliable and
    high-quality-of-service network. OpenUAV [21] consists of an open-source, Web,
    and cloud-based simulation testbed specifically designed for UAVs. Their solution
    leverages Containers as a Service (CaaS) technology to enable simulations on the
    cloud. This article also includes two use cases demonstrating the use of OpenUAV
    for machine learning and multi-UAV swarms. Lai et al. [22] presented techniques
    for integrating ArduPilot with the Android mobile platform, which provides AI
    computing power and 4G/5G connectivity to DIY drones. This article also presents
    embedded control software that cooperates with the AI Wings cloud, a cloud server
    for commanding drone fleets securely, software/hardware design for AIoT drones,
    and VR simulation for training and testing AI models. The authors propose an authentication
    protocol based on elliptic-curve cryptography with pseudo-identities and time
    freshness for checking secure communication. Their work demonstrates the system’s
    effectiveness by building an experimental medical drone service that delivers
    an automated external defibrillator to people with a sudden cardiac attack in
    the shortest time possible. SECTION III. Structural Elements of Sailboat Sailboat
    sails are designed to work as an airfoil, similar to an airplane wing. In this
    way, a sailboat can move forward with the wind blowing from almost any direction.
    A synergy between the boom (sail) and the rudder is essential for maneuvering
    a sailboat [23]. Depending on the wind force, even maintaining a straight course
    sometimes demands a combined operation of the boom and the rudder. In such a way,
    to develop a controller for a sailboat, both the sail and the rudder should be
    considered for developing its propulsion and guidance systems. There are several
    types of sailboats with different shapes and functionalities. The baseline shape
    of F-Boat is illustrated in Fig. 2 with its main parts being. Hull: The body of
    the boat. Boom: A horizontal pole attached to the mast, which extends the mainsail’s
    foot and controls the attack angle (angle between the sail chord line and the
    apparent wind). Mast: A vertical pole that supports the sails. Mainsail: Responsible
    for taking the major part of the wind, being the biggest sail of the boat; the
    aperture of the sail (attack angle) can be controlled by releasing or pulling
    a cable attached to the boom. Bow: The front part of the boat. Stern: The back
    part of the boat. Rudder: A wing-shaped device that changes the boat’s orientation
    in the water. Rudder Stem: A horizontal pole or rod to control the rudder. Keel:
    An element fixed on the bottom of the hull to prevent sideways drift and provide
    stability. Fig. 2. Overview of main sailboat parts. Show All SECTION IV. IoT Reference
    Architecture The term IoT denotes a collection of systems and devices that link
    real-world sensors and actuators to the Internet. Such devices include Internet-connected
    cars, wearable devices like health and fitness monitoring, watches, human-implanted
    devices, smart meters, home automation systems, lighting controls, smartphones,
    and wireless sensor networks. There are many architecture proposals to implement
    the IoT [24], [25], [26], [27]. Despite their differences in abstraction level,
    these proposals share many similarities, mainly covering the same elements. In
    the remainder of this section, we will briefly overview the architecture proposed
    by the Industrial Internet Consortium [24]. This architecture consists of edge,
    platform, and enterprise tiers connected by proximity, access, and service networks
    (see Fig. 3). Fig. 3. IoT reference architecture [24]. Show All According to the
    International Electrotechnical Commission [26], the devices are typically microcontrollers
    with low-processing power, memory capabilities, and intermittent network connections.
    Combining wireless and wired technologies, such as RFID, Bluetooth, Cellular,
    ZigBee, Z-Wave, Thread, and Ethernet, can create these networks. As shown in Fig.
    4, the edge tier employs the proximity network to obtain data from sensors and
    actuators. Data are transmitted through the access network to the platform tier
    (i.e., cloud servers), which processes, stores, and relays these data to the enterprise
    tiers. Communication between the platform and enterprise tiers is accomplished
    through the service network. The enterprise tier offers end-user interfaces, control
    commands, and domain-specific applications. Fig. 4. OpenBoat reference automating
    architecture. Show All The architecture pattern known as gateway-mediated edge
    connectivity and management involves a gateway acting as a mediator between a
    LAN of edge nodes on one side and a WAN on the other. This gateway functions as
    an endpoint for the WAN network and may also act as a management entity for the
    edge devices on the LAN, keeping them separate from the WAN [24]. Moreover, edge
    gateways can move data storage and processing capabilities closer to the endpoints,
    which can help reduce latency and increase network failure resilience. SECTION
    V. OpenBoat Architecture OpenBoat is a modular, abstracted, and flexible architecture
    for an IoT-enabled autonomous sailboat. An OpenBoat-based sailboat must sail autonomously,
    collect and preprocess data onboard, and communicate. Due to many uncertainties
    and nonlinearities, marine environments are challenging for several computational
    systems, mainly regarding communication. In this work, we aim to use a robotic
    sailboat for monitoring applications in a marine region considered extremely important
    for the population, named Guanabara Bay, in Rio de Janeiro. In addition to the
    marine environment by itself, this region has strong marine currents due to the
    rise and fall of the tide and extreme wind situations that occur at certain times
    of the year. Also, in the deepest channels, waves are up to 2 m in height. Another
    possible application region, at the east of Natal, Northeastern Brazil, also involves
    substantial differences between low and high tides (up to 3 m) and other weather
    conditions characteristic of the sea environment, such as extreme winds up to
    35 knots. Thus, not only to ensure the sailboat’s navigability, depending on the
    weather in extreme conditions but also to ensure that the communication part works
    properly, the IoT architecture must be thought of from the beginning of the sailboat’s
    construction, and some functionalities require adequate equipment and material.
    Hence, since its initial conception, the proposed IoT architecture had to be designed
    to work with the adverse situations of the sea, which is considered an extremely
    dynamic environment. We had to design an architecture that meets all the desired
    robustness requirements. The robustness part of the architecture will be discussed
    below, in Section VII. Therefore, the architecture of OpenBoat considers a specialized
    monitoring system consisting of specific sensors and a microcontroller responsible
    for collecting, processing, and relaying data to upper layers. For example, this
    monitoring system can be a data collection and processing system for water quality
    (e.g., pH, turbidity, gas dilution, etc.). The OpenBoat architecture is divided
    into six layers based on the IoT reference architecture in Section IV. These layers
    are the client, cloud, access and service network, edge gateway, devices, and
    sensors and actuators (see Fig. 4). The following sections describe each of these
    layers in detail. A. Sensors and Actuators Layer The sensors and actuators layer
    is the lowest level in our architecture. It contains the components necessary
    for the boat to capture data from the physical environment and perform navigation
    actions (e.g., rudder and sail pose). Thus, upper layers can be provided with
    data that enables autonomous vessel navigation and specialized monitoring. OpenBoat
    requires the use of an anemometer to obtain wind speed, a windsock for wind direction
    (essential to assist in the sail angle), a GPS for boat geolocation (position),
    and an inertial measurement unit (IMU) that provides the sailboat angles in relation
    to a global origin (for direction determination). Among the actuators, we include
    a propulsion motor for controlling the rudder and another actuator for controlling
    the sail angle. The external sensors of the specialized monitoring system also
    comprise this layer. Finally, we require stereo and panoramic cameras, necessary
    for computer vision and real-time navigation view by a remote operator. B. Devices
    Layer The devices layer contains microcontrollers directly connected to the sensors
    and actuators. The Control System is responsible for lower level control, directly
    controlling the actuators and collecting navigation sensor data. There is currently
    a range of devices specialized in this function, such as the many versions of
    Pixhawk and the Navio2. These devices run navigation control firmware, such as
    PX4 or Ardupilot. The control system connects to an edge computing component in
    order to enable remote operation and AI-based higher level control, such as path
    planning, complex maneuvers, and computer vision assistance. The second component
    of the Devices layer is the microcontroller of the specialized monitoring system.
    It must be able to connect to the specialized monitoring sensor and extract the
    data captured by this component. Additionally, this microcontroller needs to be
    connected to the proximity network layer to relay specialized monitoring data
    to local and remote repositories. C. Proximity Network Layer In this layer, we
    have two important components: 1) the radio antenna and 2) the router. The first
    enables data exchange with the ground station via radio frequency when the ground
    station is near the boat. This way, a boat that follows the OpenBoat architecture
    has the ability to communicate with the ground station without the Internet. The
    router interconnects all the components of the device and edge layers, as well
    as the modem, for external network access. This way, we have all the boat components
    connected to a LAN with a gateway for Internet access. D. Edge Gateway Layer The
    edge gateway layer has an Embedded Edge Computer responsible for data processing,
    mainly machine learning and filtering algorithms, data storage, and data transmission
    to remote servers (e.g., the cloud) [25]. Bringing processing and storage capability
    to the border reduces latency and network traffic and offers robustness against
    access network failure. Given the heterogeneity of the devices that make up the
    OpenBoat, the use of a middleware is necessary so that there is a centralized
    awareness of context, the ability to interact with all devices, as well as to
    trigger events arising from the recognition of a specific condition, such as high
    latency in the Internet connection, which can influence the operation of the boat.
    For example, the middleware should be able to interrupt the sending of data to
    the cloud and perform local storage, make the local database available, perform
    switching related to unloading processing to the cloud, and host APIs, among other
    important functions. The software responsible for this resource management is
    the IoT Middleware. There are many commercial and open-source solutions tailored
    to this end, such as AWS IoT Greengrass [28], Azure IoT Edge [29], ThingsBoard
    [30], ThingWorx [31], ThingSpeak [32], and OpenRemote [33], among others. They
    all offer edge computing capability with cloud synchronization, but some are not
    interoperable with different vendors. IoT Middleware is also a very active research
    field [34], [35], [36]. The embedded edge computer can also run higher level control
    algorithms that interact with the actuators of the boat for navigation purposes.
    These algorithms include path planning and obstacle avoidance. The first is highly
    complex due to the roles of wind speed and direction in navigation actions. Obstacle
    avoidance benefits from computer vision. Running these algorithms in a powerful
    embedded computer allows for deploying advanced AI techniques. E. Access and Service
    Network Layer The access and service network layer offers Internet access to the
    boat through a regular modem connected to the WAN interface of the router in the
    proximity network layer. F. Cloud Layer The cloud layer refers to cloud resources,
    such as processing and storage services essential for autonomous vehicles. Currently,
    there are some very robust providers with services to meet the needs of the IoT
    (e.g., AWS, Azure, and Google). OpenBoat uses cloud services for heavy machine
    learning processing offloading, real-time telemetry logging, and remote driving,
    among others. G. Client Layer We propose the inclusion of a second RF antenna
    and a base station in the client layer as an option for connecting the base station
    and the sailboat. It can be used for sending telemetry directly via radio frequency
    without going through the Internet. Finally, in the client layer, there is a base
    station. Also known as a ground control station (GCS), it is a simple workstation
    with a computer, laptop, or desktop, capable of presenting graphically, numerically,
    or textually the actuators behavior and the values of the sensors, in real time.
    Examples of software used as GCS include Mission Planner, APM Planner, QGroundControl,
    LibrePilot, and MAVProxy. SECTION VI. F-Boat Architecture F-Boat is a robotic
    sailboat built with COTS components for cost-effectiveness, time efficiency, reliability,
    flexibility, and accessibility. Project documentation, source code, and images
    and videos of the building process and field deployment are available at https://www.natalnet.br/nboat/
    and https://github.com/medialab-fboat. The baseline design of F-Boat is shown
    in Fig. 5 and follows the seven-layer model of OpenBoat as shown in Fig. 4. These
    layers are client, cloud, access and service network, edge gateway, proximity
    network, devices, sensor, and actuator, and their implementations are described
    in the following. Fig. 5. F-Boat automation architecture. Show All A. Sensor and
    Actuator Layer F-Boat is equipped with a Nagano electrical winch with a pulling
    force of 3000 pounds to control the sail. This winch is responsible for releasing
    and collecting the cable that limits the range of motion of the boom, which moves
    freely to both sides within this range of motion. A Bochen potentiometer model
    WXD3-13 with a multiturn design and resistance of 4.7 kΩ feedbacks on the rotational
    state of the winch, enabling the calculation of the amount of released cable.
    A linear actuator Vinitrônica model V1 steers the rudder. A linear potentiometer
    model WH148-1 with a resistance of 10 kilo-Ohms enables the calculation of the
    rudder angle as a function of the position of the linear actuator. There is also
    an electrical propeller for conventional boats (PHA-DIGSW54 54Lbs). This propeller
    is for emergencies and complex maneuvers. The sensors for allowing autonomous
    navigation consist of a Davis 6410 anemometer (capable of measuring both wind
    speed and direction), a Holybro GPS module model NEO-M8N with an embedded compass,
    and an IMU with 9DoF (3-axis accelerometer, 3-axis magnetometer, and 3-axis gyroscope).
    Besides, a 360 degrees panoramic camera and a Zed 3-D stereo camera provide real-time
    video streaming for human remote operation and computer vision-based autonomous
    navigation. Finally, specific sensors and actuators of the payload monitoring
    system complete the list of equipment in this layer. In this work, we use water
    temperature and water pH sensors. B. Devices Layer The main component of this
    layer is the Pixhawk, a hardware platform designed for autopilot systems. The
    autopilot system is Ardupilot, a full-featured and reliable open-source firmware
    used by industry, research organizations, and amateurs. This layer also features
    an Arduino Mega that acts as a motor driver for the rudder actuator, the sail
    actuator, and the propeller. This Arduino makes both the linear actuator connected
    to the tiller extension and the electric winch work as PWM servo motors that adjust
    the angle of the rudder and the sail, respectively. This Arduino also runs an
    algorithm that calculates the target angle of the sail as a function of the apparent
    wind direction. We run this algorithm on Arduino for simplicity as changing Ardupilot’s
    default sail control is more complex. Finally, the microcontroller of the payload
    monitoring system completes this layer. In this work, we use an Arduino Uno to
    collect, process and retransmit data from the temperature and the pH sensors.
    C. Proximity Network Layer The proximity network layer consists of an RC receiver,
    a 433MHz RF antenna, and an Ethernet LAN (included in the modem device). The RC
    receiver allows remote operation with an RC controller; the 433-MHz RF antenna
    is an alternative connection between the base station and the sailboat without
    going through the Internet; finally, the Ethernet LAN interconnects local devices
    and a cellular gateway. D. Edge Layer F-Boat has an embedded SBC NVidia-Xavier
    that provides enough processing power to allow computer vision tasks in real time.
    This boosts the stages of perception (object detection, classification, tracking,
    among others), localization (providing a reference path to the robot), planning
    (to define a new route), and controlling (sending commands to actuators). Basic
    sensory data necessary for short-term navigation include wind direction, GPS position,
    compass, rudder angle, and sail angle. The literature is particularly vast on
    object detection and collision avoidance [37], [38]. A Raspberry Pi 3B model is
    in the edge gateway layer. This device is an SBC that manages the communication
    between the autopilot system (Pixhawk/Ardupilot) and other devices. The Raspberry
    Pi also runs the IoT middleware responsible for resource management and collaboration
    with the cloud system. E. Access and Service Network Layer Serving the Access
    and Service Network layer, F-Boat has a modem/router model Aquário MD-4000, which
    is capable of interconnecting the Raspberry Pi and Jetson Xavier in an Ethernet
    network and serves as a modem for Internet access using a 4G cellular network.
    This way, the boat’s components are interconnected in the same Ethernet LAN with
    access to the Internet. F. Cloud Layer To date, the cloud layers offer only scalable
    live video streaming via Facebook Watch and WebRTC. In the future, we plan to
    add other services to this layer, such as processing and storage. G. Client Layer
    The base station is a laptop running Mission Planner software that shows graphically,
    numerically, or textually the real-time status of the sensors and actuators. Mission
    Planner can also be used to send navigation commands to F-Boat, allowing for either
    manual control or autonomous navigation by sending directions, desired speeds,
    and waypoints for the boat to reach. These capabilities can lead to more accurate
    and safer navigation. SECTION VII. Architecture’s Resilience and Fault Tolerance
    Robustness requirements were considered in the IoT architecture itself, being
    thought of, and in a way imposed, from the hardware design part (physical construction).
    Thus, we distinguish two aspects: 1) the hardware construction characteristics
    that allow communication robustness and 2) the IoT robustness. To meet these requirements
    effectively, we tried to answer some questions. How the physical construction
    of the sailboat should be, in terms of communication, both between internal components
    and between the sailboat and the land station and other instances, so that the
    robustness of the IoT system is guaranteed? What happens to communication in case
    of bad weather conditions? What is the attitude to be taken by the sailboat in
    case of a loss of communication? Regarding the design and physical project of
    the sailboat, which is open source (https://www.natalnet.br/nboat/ and https://github.com/medialab-fboat),
    some relevant points were considered, which also bring robustness to the IoT part
    as listed below. The hull offers separate and watertight compartments with different
    functionalities, in which the (separate) behaviors of the proposed control architecture
    must be implemented [39]. The Sensors and Actuators layer, and the Devices layer
    are connected to each compartment, which guarantees that the control part of the
    architecture is distributed and one behavior does not depend on the other to work.
    The communication between these behaviors (over devices and sensors) can be done
    redundantly, both via the onboard WiFi and via a wired connection (Ethernet).
    The behaviors are effectively controlled on a single board computer (SBC), which
    also implements behaviors from the vision part (N-Vidia Xavier), over which we
    use reinforcement learning to maintain the sailboat’s seaworthiness [39]. Communication
    between the sailboat and the shore (base) station (with higher parts of the architecture)
    is handled in several ways, depending on the application. On larger distances,
    it is currently done using a 4G cellular network. Still, it could even use satellite
    communication, which is the primary form of communication on the high seas. Also,
    communication can be done by RC radio and WiFi (at very short distances). This
    guarantees operation in any location as long as the sailboat is operational. The
    physical characteristics of the sailboat construction [13], [40] is also considered
    to guarantee the navigability even in case of serious problems or bad weather
    conditions, which in turn also helps in the robustness of the IoT architecture.
    The bottom of the sailboat has 3–4 cm of foam (which can help maintain buoyancy,
    even if a lot of water comes into the boat). The electronic components are all
    placed inside watertight boxes in each compartment. The wired part (Ethernet)
    is always placed inside ducts, with total sealing between each compartment, to
    prevent water from entering. The sail was designed to be undersized, giving less
    speed but providing more stability. The sailboat is a monohull with a low center
    of gravity, unlike catamarans. This ensures buoyancy even in case of bad weather
    conditions. Furthermore, we have the possibility of using a motherboard developed
    by the team (see Fig. 6) with slave distribution boards to avoid jumpers, providing
    more robustness in the internal communication. Fig. 6. Mother and slave boards
    specific developed and available for increasing components connectivity. Show
    All With regards to energy robustness, the sailboat is currently self-sustainable
    (solar energy), having a battery bank (105A+105A in parallel), which provides
    much more power than it would consume in 1 day if fully charged, and if it is
    propelled only by the sail. The amount of energy provided by the system lasts
    more than 48 h. In the load tests we carried out, even at night or in case of
    heavy clouds, the system maintains itself, guaranteeing navigability (and consequently
    communication). Of course, disasters can occur, and no marine system is robust
    enough to prevent accidents [41]. For example, in the case of a collision with
    an other vessel or a wreckage, or in extreme storms or hurricanes, the communication
    part may be affected. But, it is worth noting that, regarding the situation of
    loss of communication, the navigation system based on Pixhawk offers a failsafe
    functionality, which can be activated. In this case, the system can trigger an
    alternative route back home (or to another location), which can be followed if
    the sailboat’s seaworthiness is functional. With regards to maintaining the robustness,
    communication between the upper levels can be achieved through simple measures
    already widely adopted in the various architectures operating on land that exist.
    We believe that the critical parts of our application is the internal communication
    between the robotic sailboat components and the communication between the autonomous
    vehicle and the ground station, which is why these aspects were the most discussed
    in this section. SECTION VIII. Experiments and Results Guanabara Bay provides
    social, economical, and environmental asset services for more than 10 million
    people. However, this area is also prone to extreme events that can increase the
    sediment load and nutrient input into the bay, affecting water quality and ecosystem
    health [4]. Thus, monitoring the bay should be essential. In this experiment,
    manual remote control was initially used to guide the vessel with a radio controller.
    Next, tests of the automatic control were performed. For this stage, Ardupilot’s
    guided navigation system controlled the sail and the rudder. In the last set of
    tests, we generated different navigation scenarios. The energy consumption of
    the rudder and sail was relatively low, as shown in Fig. 7, with the rudder consuming
    less than the sail: 1000 and 4000 mAh, respectively. For the sail, it would give
    4000/1.5=2.67 A. The F-Boat has two 105-A batteries attached to a solar panel
    that manages to produce all the energy needed for the complete autonomy of the
    vehicle. Fig. 8 shows the total amount of the vessel’s consumption. Battery voltage
    remains between 11 and 12 v, and current remains constant between 231.68 and 5562.85
    mA. Fig. 7. Graphical representation of the energy consumption of the rudder (green)
    and the sail (red). Show All Fig. 8. Graphical representation of energy consumption:
    voltage (red) and current (green). Show All Fig. 9 illustrates the speed on the
    boat, where the VMG (correct speed or speed at which the vehicle is advancing
    directly toward the destination) and the actual wind (2 m per second) are found.
    The VMG wind of the vessel is close to 1.5 m/s, while the real wind has values
    between 2 and 8 m/s. Fig. 9. Graphical representation of real wind speed (red)
    and apparent wind (green). Show All It is possible to see the path taken by the
    F-Boat, given the defined waypoints. The path navigated by the vessel, as shown
    in Fig. 10, is started manually at some waypoints and then passed via ACRO mode
    to the autopilot. Fig. 10. Graphical representation of current direction (green)
    and desired direction (red). Show All Fig. 11 illustrates the vessel’s heading,
    measured in degrees of the roll (roll) and pitch (yaw) angles, which are varying
    between 15 and −15 for roll and 0 to 400 for yaw. Fig. 11. Graphical representation
    of the roll direction in red and the yaw direction in green. Show All As previously
    mentioned, the vessel used the PI controller in the actual tests to fix errors
    in the input responses of the navigation system to the actuators. Fig. 12 shows
    in green and blue the control of the winch actuator for sail control using PI,
    and the control of the rudder actuator using PI in red. Fig. 12. Graphical representation
    of the vessel’s PI control for the rudder (red) and for the sail (blue and green).
    Show All SECTION IX. Conclusion To date, our proposed architecture for automating
    sailboats in the context of the IoT and AI technologies, the so-called AIoT, has
    been implemented, tested, and validated in practice. We call our proposed general
    sailboat automating architecture OpenBoat. This architecture consists of six layers
    that address traditional automation, AI-processing units, interconnectivity, and
    interoperability with other subsystems. These six layers are client, cloud, access
    and service network, edge gateway, devices, and sensors and actuators. We also
    introduced F-Boat, an AIoT-enabled autonomous sailboat prototype built with COTS
    components and following the OpenBoat reference architecture. Building prototypes
    with COTS components offers numerous advantages in cost-effectiveness, time efficiency,
    reliability, flexibility, and accessibility. By leveraging these readily available
    components, we can streamline the development process, reduce costs, and benefit
    from established quality standards. F-Boat is designed and built for robustness
    to guarantee its operation under extreme events, such as high temperatures and
    bad weather, for extended periods of time. Project documentation, source code,
    and images and videos of the building process and field deployment are available
    at https://www.natalnet.br/nboat/ and https://github.com/medialab-fboat. The results
    of field experiments demonstrate the functionalities of the prototype and the
    AIoT capability of the general architecture. We conducted these experiments at
    Guanabara Bay, an important marine and coastal ecosystem in southeastern Brazil.
    This area is prone to extreme events that can significantly impact its ecosystems
    and population. Extreme events in this area include severe storms, heavy rainfall,
    and tidal surges that can cause flooding. Our architecture favors tasks that require
    a higher level of persistence where energy self-sufficiency is desired, such as
    monitoring and patrolling large water spaces like rivers, lakes, and the ocean,
    mainly in coastal regions. These ecosystems provide a wide range of goods and
    services essential for human well-being, including food, climate regulation, coastal
    protection, and nutrient cycling for plants and animals, among others. However,
    these ecosystems are vulnerable to extreme events, which can significantly impact
    their biodiversity, productivity, and the services they provide. Such events in
    the marine and coastal ecosystems include storms, floods, heat waves, cold spells,
    droughts, and sea-level rise, causing physical damage, such as the destruction
    of habitats, loss of biodiversity, and changes in the structure and functioning
    of these ecosystems. Future work includes using F-Boat as a testbed to compare
    and evaluate techniques for autonomous sailing. Often, sailing is not just pointing
    the boat in the direction to be followed, and maneuvers and course changes are
    commonly necessary. Deploying AI technologies to make maneuver and course decisions
    is a promising research field with great potential to improve traditional control
    theory-based techniques. Authors Figures References Keywords Metrics More Like
    This Intelligent Monitoring System of Residential Environment Based on Cloud Computing
    and Internet of Things IEEE Access Published: 2021 A study of integration Internet
    of Things with health level 7 protocol for real-time healthcare monitoring by
    using cloud computing 2017 10th Biomedical Engineering International Conference
    (BMEiCON) Published: 2017 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: General System Architecture and COTS Prototyping of an AIoT-Enabled Sailboat
    for Autonomous Aquatic Ecosystem Monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhao J.
  - Liu X.
  - Tian M.
  citation_count: '0'
  description: Minimizing the offloading latency of agricultural drip irrigation and
    fertilization tasks has long been a pressing issue in agricultural drip irrigation
    and fertilization wireless sensor networks (AIFWSNs). The introduction of edge
    computing as a robust and practical aid to cloud computing in AIFWSNs can significantly
    improve the execution speed of agricultural drip irrigation and fertilization
    tasks and effectively reduce the task offloading latency. Therefore, this paper
    investigates the optimization method of drip irrigation and fertilization task
    offloading delay in AIFWSNs based on edge computing and proposes a new edge task
    offloading method for AIFWSNs, namely, Quantum Chaotic Genetic Optimization Algorithm
    (QCGA). This paper introduces a novel quantum operator in QCGA, comprising a quantum
    non-gate and a quantum rotation gate, to improve the algorithm’s global search
    capability. The quantum operator accomplishes the updating of quantum rotating
    gates without querying the quantum rotation angle table, which reduces the computational
    complexity of introducing quantum optimization into the task offloading problem
    of AIFWSNs. This paper proposes a new chaotic operator to make the initial solution
    more uniformly distributed in the search space by chaotic mapping. This paper’s
    simulation experiments compared QCGA and snake optimizer (SO), genetic algorithm
    (GA), particle swarm optimization (PSO), sequential offloading, and random offloading
    methods. Simulation results showed that, compared with SO, GA, PSO, sequential
    offloading, and random offloading methods, the average delay of QCGA was reduced
    by 9.96%, 26.78%, 29.31%, 44.67%, and 61.24%.
  doi: 10.1063/5.0185999
  full_citation: '>'
  full_text: '>

    "All Content AIP Publishing Portfolio AIP Advances                              Advanced
    Search | Citation Search Univ Nebraska Lincoln Lib Sign In HOME BROWSE COLLECTIONS
    PUBLISH WITH US ABOUT Volume 14, Issue 1 January 2024 I. INTRODUCTION II. RELATED
    WORK III. SYSTEM MODEL IV. QCGA-BASED METHOD FOR REDUCING THE OFFLOADING DELAY
    OF TASKS OF AIFWSNs A. Population Coding B. Population Selection, Crossover, And
    Mutation C. Quantum Operator D. Chaotic Initialization And Perturbation E. Algorithm
    Complexity Analysis V. RESULTS AND DISCUSSION VI. CONCLUSIONS ACKNOWLEDGMENTS
    AUTHOR DECLARATIONS Conflict Of Interest Author Contributions DATA AVAILABILITY
    REFERENCES RESEARCH ARTICLE| JANUARY 09 2024 An efficient task offloading method
    for drip irrigation and fertilization at edge nodes based on quantum chaotic genetic
    algorithm Jiawei Zhao ; Xiang Liu ; Min Tian Author & Article Information AIP
    Advances 14, 015011 (2024) https://doi.org/10.1063/5.0185999 Article history Split-Screen
    Views PDF Share Tools Minimizing the offloading latency of agricultural drip irrigation
    and fertilization tasks has long been a pressing issue in agricultural drip irrigation
    and fertilization wireless sensor networks (AIFWSNs). The introduction of edge
    computing as a robust and practical aid to cloud computing in AIFWSNs can significantly
    improve the execution speed of agricultural drip irrigation and fertilization
    tasks and effectively reduce the task offloading latency. Therefore, this paper
    investigates the optimization method of drip irrigation and fertilization task
    offloading delay in AIFWSNs based on edge computing and proposes a new edge task
    offloading method for AIFWSNs, namely, Quantum Chaotic Genetic Optimization Algorithm
    (QCGA). This paper introduces a novel quantum operator in QCGA, comprising a quantum
    non-gate and a quantum rotation gate, to improve the algorithm’s global search
    capability. The quantum operator accomplishes the updating of quantum rotating
    gates without querying the quantum rotation angle table, which reduces the computational
    complexity of introducing quantum optimization into the task offloading problem
    of AIFWSNs. This paper proposes a new chaotic operator to make the initial solution
    more uniformly distributed in the search space by chaotic mapping. This paper’s
    simulation experiments compared QCGA and snake optimizer (SO), genetic algorithm
    (GA), particle swarm optimization (PSO), sequential offloading, and random offloading
    methods. Simulation results showed that, compared with SO, GA, PSO, sequential
    offloading, and random offloading methods, the average delay of QCGA was reduced
    by 9.96%, 26.78%, 29.31%, 44.67%, and 61.24%. Topics Chaotic maps, Sensors, Hydrology,
    Information technology, Evolutionary computation, Algorithms and data structure,
    Cloud computing, Optimization algorithms, Mathematical optimization, Quantum probability
    I. INTRODUCTION With the rapid development of the agricultural Internet of Things
    (AIoT), agricultural drip irrigation and fertilization wireless sensor networks
    (AIFWSNs) are widely used to improve the efficiency of agricultural drip irrigation
    and fertilization control and production.1–3 In AIoT, AIFWSNs generate massive
    data that need to be processed urgently. Sensor nodes are unable to process a
    large amount of data quickly due to hardware constraints and usually upload these
    data to cloud servers for processing.4 However, unlike traditional wireless sensor
    networks (WSNs), AIFWSNs are used in agricultural production activities where
    latency requirements are much lower. If based solely on the traditional cloud
    computing model, a significant time delay will occur due to the distance of the
    cloud server from the location where the task is generated, thus reducing the
    drip irrigation fertilization efficiency and fertilization accuracy, resulting
    in resource waste and environmental pollution.5 Thus, proposing a method that
    can effectively reduce the offloading time delay of agricultural drip irrigation
    fertilization tasks holds immense research significance. Edge computing6 addresses
    the issues of extended latency, network instability, and limited bandwidth encountered
    in conventional cloud computing models by establishing a network of edge servers
    close to data sources. The main attributes of edge computing of AIFWSNs are agricultural
    drip irrigation, fertilization task, and edge server attributes. Specifically,
    the attributes of the agricultural drip irrigation and fertilization tasks include
    the execution difficulty in the task, the input file size of the task, and the
    output file size. How to perform edge task offloading is a critical issue in reducing
    the task offloading latency of AIFWSNs. However, current task offloading algorithms
    have some limitations. Most existing studies usually focus on dividing and migrating
    these tasks to other devices while ignoring the latency constraints and the selection
    of offloading nodes for different tasks.7 For example, Thinh et al.8 introduced
    a linear relaxation-based approach and a comprehensive search-based approach to
    determine the offloading decision, effectively offloading tasks to cloud servers.
    However, both methods incur significant latency. Wu et al.9 proposed a Lyapunov-based
    task offloading technology, which satisfies the latency constraint, but the approach
    does not take into account different difficulties of task execution and sensitivity
    to latency. To reduce the latency of task offloading for AIFWSNs, a new task offloading
    model and a new heuristic algorithm called QCGA are proposed in this paper. The
    proposed QCGA utilizes quantum optimization and chaotic initialization to enhance
    the algorithm’s performance and obtain the desired latency reduction solution.
    The results of simulations indicate that the QCGA-based task offloading model
    surpasses other approaches of AIFWSNs in terms of latency performance. The main
    contributions of this paper are as follows. A new offloading model for agricultural
    drip irrigation and fertilization tasks is proposed, which aims to reduce the
    latency of offloading tasks at the edge of AIFWSNs. The model transforms the task
    offloading problem into a problem of assigning a certain number of tasks to the
    appropriate edge servers. A novel quantum operator is proposed to improve task
    offloading methods’ global search capability. The quantum operator uses quantum
    rotation gates and quantum non-gates to update the matrix, and in the process
    of completing the quantum rotation gate update, the operator removes the query
    operation of the rotation angle table, which reduces the complexity of the algorithm
    to a certain extent. A new chaotic operator based on chaos theory is introduced
    to enhance the convergence speed of the task offloading method. The proposed strategy
    effectively avoids premature convergence and falling into local optima by utilizing
    the traversal and randomness characteristics of chaos, resulting in a significant
    improvement in the algorithm’s convergence speed and accuracy. The remainder of
    this paper is organized as follows: Sec. II presents the work related to edge
    task offloading for AIFWSNs. Section III shows the edge offloading model for AIFWSNs.
    In Sec. IV, the proposed QCGA approach for solving the edge offloading problem
    of agricultural drip irrigation and fertilization tasks for AIFWSNs is explained
    in detail. Subsequently, simulation experiments are conducted in Sec. V to verify
    the effectiveness of QCGA in reducing the delay, and the results are explained
    in detail. Finally, Sec. VI describes this paper’s conclusions and future work
    directions. II. RELATED WORK In most task offloading studies, researchers usually
    focus on how to segment and migrate these tasks to other devices. With the convergence
    and development of mobile Internet, Internet of Things, and artificial intelligence
    technologies, various types of mobile applications based on deep learning are
    emerging. However, due to the limitations of the computing power and storage capacity
    of mobile devices, they cannot effectively support the inference process of complex
    deep neural network models, which directly affects the user experience. Offloading
    the inference computation of the models to edge servers or the cloud for execution
    becomes an effective way to solve this contradiction. Tang et al.10 designed a
    Double Deep Q-Network (DDQN) algorithm for optimizing the dynamic sub-frame offloading
    of large-scale sequential subtasks in the context of Telematics, which can significantly
    reduce the total latency and performs better, especially in the context of high-speed
    mobile and congested environments. Xue et al.11 proposed a Double Dueling Prioritized
    deep Q-Network (DDPQN) algorithm for deep learning model partitioning and offloading
    strategy acquisition with multiple metric co-optimization by introducing a coupling
    coordination metric and a node balancing metric in a local-edge-cloud cooperative
    environment. Wu et al.12 proposed to construct a virtual queue to assist the actual
    queue, ensuring that the problem of deep learning task duration constraints is
    transformed into a virtual queue stability control problem by constructing a Lyapunov
    function and designing an adaptive online task offloading algorithm to achieve
    long-term energy minimization and queue stability. Sun et al. proposed an energy-aware
    user-centric mobility management scheme that only uses user energy consumption
    as a constraint for task offloading and does not consider delay and energy consumption
    in an integrated manner.13 Chen and Hao14 treated the task offloading issue as
    a mixed integer nonlinear programming problem. This study proposed to transform
    the task offloading problem into two subproblems, task assignment and resource
    allocation, and proposed a software-defined task offloading scheme based on these
    two subproblems. The method succeeded in offloading the tasks, but the scheme
    did not consider the time delay as a constraint. Yi et al.15 proposed a MOTM task
    offloading technology based on queuing model, which considered that the equilibrium
    of a noncooperative game among mobile users successfully offloaded the tasks to
    the cloud server, but the method did not consider the priority among tasks. Energy
    consumption, latency, and task priority are critical concerns for AIFWSNs.16–18
    While conventional wireless sensor networks are also affected by these factors,
    the distinctive aspect of AIFWSNs is that if the nodes’ tasks are not offloaded
    on time, it can lead to task blockage, longer response latencies, reduced drip
    irrigation efficiency, and compromised fertilization accuracy. These outcomes
    result in resource wastage and environmental contamination. In addition, the task
    offloading optimization issue can be abstracted as a multi-objective optimization
    issue because AIFWSNs need to consider multiple constraints simultaneously, which
    is an NP-hard problem.19 Cui et al.20 proposed to use the K nearest neighbor algorithm
    to decide whether tasks are offloaded to the cloud or edge execution and use reinforcement
    learning to allocate non-local computational resources, which considers the task
    inter priority, but the disadvantage of the slower speed of K nearest neighbor
    algorithm is not optimized, while the use of K nearest neighbor algorithm is not
    effective when the samples are not balanced; therefore, the overall latency of
    the method is relatively high and less effective in AIFWSNs. Chen et al.21 explored
    the problem of offloading multiple tasks in a multi-channel wireless interference
    environment for edge cloud computing. They presented a distributed algorithm based
    on game theory to address task offloading, but the approach did not significantly
    reduce task offloading latency. The second class of task offloading methods for
    AIFWSNs is based on heuristic algorithms that can give a set of ideal task offloading
    solutions quickly. Ning et al.22 proposed integrating mobile cloud computing and
    edge computing to formulate task offloading decisions and gave an iterative heuristic
    task offloading strategy. Shen et al.23 presented an offloading technique utilizing
    an enhanced bald eagle search optimization algorithm, efficiently reducing task
    offloading latency. However, the algorithm exhibits slow convergence due to a
    more intricate model, and the inclusion of multiple conventional optimization
    operators in the algorithm results in increased algorithmic complexity. As heuristic
    algorithms continue to advance rapidly, there is an increasing application of
    quantum optimization to augment the global search ability of algorithms.24 Flori
    et al.25 proposed a particle swarm algorithm for quantum optimization based on
    the context of agricultural drip irrigation fertilization, which used quantum
    superposition to adjust the parameters of the particle swarm optimization (PSO)
    to effectively improve the problem of PSO easily falling into local optimum. However,
    the algorithm only applies to some specific scenarios, has poor applicability
    on global convex functions, and converges very slowly. de Andoin and Echanobe26
    proposed a quantum optimization algorithm based on an ant colony optimization
    algorithm to find an approximate solution to the NP-hard problem in agricultural
    drip irrigation fertilization production. The algorithm uses non-error-correcting
    quanta to generate new possible solutions as a superposition of states, but the
    algorithm does not perform well in the face of complex update strategies. In heuristic
    algorithms, a well-diversified initial population helps improve the search efficiency
    of the algorithm. Therefore, various chaotic operators have been added to heuristic
    algorithms in recent years to increase the population diversity and thus speed
    up the convergence of the algorithm.27–29 Xia and Li30 proposed a chaotic particle
    optimization algorithm that uses logistic chaos to search for the elite individuals
    in the population. Although it improves the situation where the PSO falls into
    a local optimum, the convergence speed of the algorithm is not significantly improved
    when facing high-dimensional optimization problems. In 2022, Hashim and Hussien
    proposed a new snake optimizer algorithm for solving complex high-dimensional,
    multi-constraint optimization problems based on natural behaviors, such as mating
    and competition of snakes, which possesses a strong performance for finding optimal
    results. However, the population updating mechanism of SO only considers the optimal
    and the worst individuals. It ignores the contribution of other individuals, and
    the search process of SO is limited by the snake’s movement mode, which can only
    be carried out along the direction of the optimal and the worst individuals.31
    Gupta and Deep proposed an improved gray wolf optimization algorithm based on
    the chaotic local search to alleviate the stagnation of the wolf population in
    the local optimum solution by enhancing exploration and maintaining a relevant
    balance between exploration and exploitation. The overall complexity of the algorithm
    is high, and the computational resources consumed increase the computational cost.32  To
    address the limitations of existing task offloading schemes for AIFWSNs, which
    are either limited in global search capability or slow in convergence speed, this
    paper proposes a new task offloading model and a heuristic approach based on quantum
    and chaos strategies. The proposed approach aims to obtain near-optimal task offloading
    solutions by introducing novel quantum and chaotic operators to enhance the algorithm’s
    search efficiency. By doing so, this research aims to contribute to filling the
    research gap in this area. III. SYSTEM MODEL To achieve proper and reasonable
    offloading of agricultural drip irrigation and fertilization tasks, this paper
    abstracts the process by evaluating the processing difficulty in agricultural
    drip irrigation and fertilization tasks in AIFWSNs in terms of the number of millions
    of instructions. Considering that edge devices usually have limited computing
    resources, it is assumed in this paper that all edge agricultural drip irrigation
    and fertilization devices need to offload their tasks to the edge server for execution
    completely and in a full offload manner. In addition, this paper also assumes
    that all agricultural drip irrigation and fertilization tasks have the same priority,
    including tasks such as agricultural drip irrigation and fertilization flow control
    tasks, agricultural drip irrigation and fertilization storage tasks, and agricultural
    drip irrigation and fertilization device status monitoring, i.e., they are processed
    with equal urgency. Under the above assumptions, the edge offloading problem for
    agricultural drip irrigation fertilization tasks can be translated into the problem
    of how to allocate a certain number of tasks to the appropriate edge servers.
    The goal of the problem is to ensure that the total completion time of all agricultural
    drip irrigation and fertilization tasks over some time is minimized. To this end,
    this paper proposes an edge computing task offloading system model for AIFWSNs,
    as shown in Fig. 1. FIG. 1. VIEW LARGEDOWNLOAD SLIDE Offloading system model for
    edge computing tasks. In the edge offloading model of agricultural drip irrigation
    and fertilization task designed in this paper, the attributes of agricultural
    drip irrigation and fertilization task and edge server are mainly involved. Specifically,
    the attributes of the agricultural drip irrigation fertilization task include
    execution difficulty, input file size, and output file size, while the attributes
    of the edge server include bandwidth, the number of central processing unit (CPU)
    cores, and computational power. In the task offload scenario of AIFWSNs considered
    in this paper, the computational power of the edge server is described in terms
    of the number of million machine language instructions processed per second. The
    primary optimization goal of the model is to minimize the total completion time
    of the agricultural drip irrigation and fertilization task. To achieve this, the
    edge offloading delay for the task is divided into three components: task upload
    delay, task execution delay, and task result return delay. Task upload delay refers
    to the time the agricultural drip irrigation and fertilization edge device takes
    to upload the task to the edge server through either wired or wireless means.
    In other words, it is the duration starting from the initiation of offloading
    by the edge device to uploading the task to the edge server. The task upload delay
    D1 is calculated, as shown in the following equation: D 1 = I F L b w , (1) where
    IFL is the size of the task file to be sent to the edge server, including detailed
    task data information, and bw is the transmission bandwidth. When the agricultural
    drip irrigation and fertilization task is successfully uploaded from the edge
    device to the edge server, the latter will be responsible for parsing and executing
    the input agricultural drip irrigation and fertilization task file, and the time
    spent in this process is the task execution delay D2, which can be calculated
    by using the following equation: D 2 = M I M I P S × C N , (2) where MI represents
    the amount of computation required for the execution of agricultural drip irrigation
    and fertilization tasks, MIPS represents the number of million machine language
    instructions per second processed by a single CPU of the edge server, and CN is
    the number of CPUs on an edge server. After the edge server executes the assigned
    agricultural drip irrigation and fertilization task, the whole task offloading
    process is not yet finished because the agricultural drip irrigation and fertilization
    edge device is not yet able to obtain the result of task execution completion.
    Therefore, the last part of the delay, the task result return delay D3, is calculated
    as follows: D 3 = O F L b w , (3) where OFL is the size of the task file that
    the edge server sends back to the agricultural drip irrigation and fertilization
    edge device, which includes information about the results of the task execution.
    To effectively reflect the benefits obtained by the agricultural drip irrigation
    fertilization task being assigned to different edge servers and based on the definition
    of the time delays of the three different parts of the edge offloading process
    of the agricultural drip irrigation fertilization task mentioned earlier, this
    paper introduces a task assignment evaluation function to reflect the task assignment
    efficiency obtained by an assignment scheme, whose mathematical expression is
    shown as follows: F obj = α × D 1 + β × D 2 + λ × D 3 , (4) where α, β, and λ
    are the time delay coefficients and D1, D2, and D3 are the task upload time delay,
    task execution time delay, and task result return time delay, respectively. In
    addition, the three different delay coefficients must satisfy both the constraints
    shown in the following equations: α + β + λ = 1 , (5) α + λ ≤ β . (6) In the above
    two constraints, Eq. (5) ensures that the delay coefficients balance the weights
    of the different delay components in the final evaluation criteria. Moreover,
    the constraint of Eq. (6) is that the task execution latency is usually longer
    compared to the other two latency components, and therefore, its weight should
    be larger. IV. QCGA-BASED METHOD FOR REDUCING THE OFFLOADING DELAY OF TASKS OF
    AIFWSNs The task edge offloading model of AIFWSNs proposed in this paper takes
    into account the heterogeneity of different agricultural drip irrigation fertilization
    tasks and edge servers, which complicates the task allocation problem. The problem
    can be judged as an NP-hard problem due to the following reasons: As the number
    of tasks and servers increases, the combination of task–server mappings grows
    exponentially, and the computational complexity of searching for the complete
    solution space is exceptionally high. Different tasks have different computational,
    storage, and network requirements, and there are also differences in the performance
    of the servers, which increases the problem-solving difficulty in solving the
    problem. The execution time of the task, server load, and other objectives need
    to be optimized at the same time while solving the problem. The tasks and servers
    change dynamically in the natural environment, which brings many challenges to
    the solution algorithms. Meanwhile, similar task allocation problems have been
    proven to be NP-complete, so traditional deterministic algorithms are complex
    to solve within polynomials.33–36 Therefore, in this paper, an improved algorithm
    QCGA based on a genetic algorithm (GA) is proposed to solve this NP-hard multi-objective
    optimization problem. Specifically, the main steps of QCGA are as follows. A.
    Population coding In solving the edge offloading problem of agricultural drip
    irrigation and fertilization tasks, QCGA adopts a special coding format, as shown
    in Fig. 2. Suppose there are three edge servers E1, E2, and E3. There are six
    urgent agricultural drip irrigation and fertilization tasks t1, t2, t3, t4, t5,
    and t6. Each edge server must complete two tasks on average, assuming that the
    initial solution is t 1 , t 2 t 3 , t 4 t 5 , t 6 ⁠, indicating that edge server
    E1 needs to complete tasks t1 and t2. The order of completing tasks is t1 first
    and then t2. Other edge servers complete tasks in the same order. If the encoding
    is changed to t 1 , t 5 , t 3 t 4 t 2 , t 6 after one mutation, edge server E3
    turns task t5 to E1 to execute, and the order is t1 → t5 → t3. In this way, the
    operations of each crossover variant can be operated on a single gene chain, increasing
    the simplification of the process. FIG. 2. VIEW LARGEDOWNLOAD SLIDE Coding method
    of QCGA. B. Population selection, crossover, and mutation The initial population
    is subject to the roulette wheel method during the selection operation, which
    selects individuals to form a new population. Subsequently, the new population
    is subjected to crossover operations, increasing the gene diversity on the chromosomes.
    The specific steps for selecting the operation are as follows: obtaining the value
    of the fitness function corresponding to the initial population by the fitness
    function and using the roulette wheel method to perform a selection operation
    on the initial population by the value of the fitness function corresponding to
    the initial population. The specific steps of the crossover are as follows: The
    crossover operation requires the comparison of crossover probabilities with random
    numbers for individuals who meet the requirements, as shown in the following equation:
    d m , n ( k + 1 ) = v m , n ( k + 1 ) , r ≥ r , d m , n ( k ) , o t h e r w i
    s e , (7) where d m , n k + 1 is the mth individual of the population in the k
    + 1 generation, n is the nth gene in the mth individual, and r is the crossover
    probability. A certain probability is used to decide whether to perform the crossover
    operation. During the crossover operation, two individuals are selected from the
    current population, known as the parent gene strings. A crossover position is
    randomly chosen on the two selected individuals to perform the crossover operation,
    as depicted in Fig. 3. When the crossover gene of the red gene string crosses
    the crossover gene of the blue gene string during the crossover process, a conflict
    elimination operation is necessary when the gene after the crossover conflicts
    with its gene. To conduct the conflict elimination operation, the gene at the
    crossover point of the blue gene string is initially swapped with the gene at
    the conflicting position. The original gene at the crossover point of the blue
    gene string is then compared with the gene at the crossover point of the red gene
    string. If a conflict exists, the gene in the red gene string at the conflicting
    position is swapped with the gene at the conflicting position. Finally, the blue
    gene string is crossed with the gene in the red gene string that requires crossing.
    By introducing a small probability of mutation, individual genetic diversity is
    enriched. After the crossover operation is completed, the mutation operation is
    executed. The mutation operation in the QCGA is intended to enhance the algorithm’s
    global search capability. Using only selection and crossover operators to generate
    new task assignment sequences can lead to the algorithm getting stuck in locally
    optimal solutions when optimizing the edge offloading of agricultural drip irrigation
    and fertilization tasks. These two operators can only combine permutations based
    on existing sequences and cannot generate new gene sequences as mutation operators
    can. Therefore, by introducing a certain probability of mutation operation on
    chromosome individuals, QCGA can obtain new task assignment sequences, thus increasing
    the search space and avoiding the risk of falling into local optimal solutions.
    Figure 4 shows how the mutation operation is implemented. FIG. 3. VIEW LARGEDOWNLOAD
    SLIDE Schematic diagram of two-point crossover operation. FIG. 4. VIEW LARGEDOWNLOAD
    SLIDE Schematic diagram of mutation operation. C. Quantum operator In the QCGA,
    a novel simplified quantum operator has been suggested to guide the algorithm’s
    evolution toward the optimal offloading scheme. This operator can update the quantum
    rotation gate without the need to query the rotation angle table. It consists
    of the quantum non-gate and the quantum rotation gate operation. At the initialization
    stage of the QCGA, each individual includes a quantum probability amplitude matrix,
    which is updated based on the offloading sequence after the current individual
    update. The pseudo-code of the quantum revolving gate for this streamlined quantum
    operator is shown in Algorithm 1. ALGORITHM 1. Refined and simplified quantum
    revolving door pseudocode. Inputs: current population pop, number of agriculture
    tasks n, number of edge servers m, quantum probability amplitude increment Pquantum,
    and quantum probability amplitude matrix F.  Output: The updated quantum probability
    amplitude matrix F.  01 for each i ∈ [1, n] do  02 for each j ∈ [1, m] do  03
    if pop(i,j) = = j && F(i,j) < 1 then  04 F(i,j) = F(i,j) + Pquantum;  05 else
    if pop(i,j) ! = j && F(i,j) > 0 then  06 F(i,j) = F(i,j) − Pquantum;  07 end if  08
    end for  09 end for  10 return F;  The pseudo-code of the quantum non-gate is
    shown in Algorithm 2. ALGORITHM 2. Quantum non-gate pseudocode. Inputs: quantum
    non-gate execution probability Pnon, random number p in [0,1], number n of edge
    servers, number m of agriculture tasks, quantum probability amplitude matrix F.  Output:
    The updated quantum probability amplitude matrix F.  01 for each i ∈ [1, n] do  02
    for each j ∈ [1, m] do  03 if p < Pnon then  04 F(i,j) = 1 − F(i,j);  05 end if  06
    end for  07 end for  08 return F;  D. Chaotic initialization and perturbation
    In solving the edge offloading problem of agricultural drip irrigation and fertilization
    task, the optimal solution generated in the current iteration is easily lost in
    the previous steps of QCGA, thus affecting the convergence performance of the
    algorithm. Therefore, this paper proposes a new chaotic operator to significantly
    accelerate the convergence speed of QCGA and enhance the global search capability.
    The specific implementation of the chaotic operator consists of the following
    two steps: The following equation presents the mathematical expression of the
    logistic mapping model, which is utilized for chaotic sequence initialization
    and population generation: y i j + 1 = 4 × y i j ( 1 − y i j ) , (8) where y i
    j ∈ 0 , 1 is the chaotic variable, i = 1, 2, 3, …, n denotes the ordinal number
    of the chaotic variable, and j = 1, 2, 3, …, N denotes the population ordinal
    number, where N denotes the population size. Using the feature that the logistic
    chaos mapping model is sensitive to initial values, n chaotic variables are obtained
    by assigning n small differences to the initial values of Eq. (8), and then, the
    chaotic variables are mapped to the solution space variables x i j of QCGA by
    using the following equation: x i j = l i + y i j × ( u i − l i ) . (9) Chaotic
    perturbation strategy: the unoptimized GA has the disadvantages of quickly falling
    into local optimum and insufficient search accuracy in the late stage of the algorithm.
    To overcome the above disadvantages, this paper uses the randomness, regularity,
    and ergodicity of chaotic sequences to perturb the three individuals in the decision
    layer, which can make the GA escape the local optimum and thus improve the global
    search ability of the GA. The chaotic perturbation strategy is shown in the following
    equation: y k ′ = y * × ( 1 − φ ) + φ × y k , (10) where y k ′ denotes the chaotic
    variable formed after adding the perturbation, y* denotes the chaotic variable
    formed after mapping individuals in the decision layer to [0,1], yk is the chaotic
    variable after k iterations, and φ φ ∈ 0 , 1 is the strength of the perturbation,
    which is calculated as shown in the following equation, where N is the population
    size: φ = 1 − k − 1 k N . (11) The complete algorithm flowchart of QCGA is shown
    in Fig. 5. E. Algorithm complexity analysis Because SO is the algorithm with the
    smallest performance gap with QCGA among several compared algorithms, the effectiveness
    of QCGA is further demonstrated by comparing the complexity of the two algorithms.
    The primary source of complexity in the proposed QCGA is attributed to the GA,
    chaotic operator, and quantum operator. The GA involves a triple loop encompassing
    the number of iterations, population size, and the number of edge servers, leading
    to its maximum computational complexity. Since the triple loop of the GA is executed
    three times in QCGA, its complexity can be expressed as follows: Q C G A G A =
    3 × M a x × P × T × e , (12) where Max expresses the maximum number of iterations,
    P expresses the population size, T expresses the number of agricultural drip irrigation
    and fertilization tasks, and e expresses the number of edge servers. The complexity
    of the quantum operator consists of the maintenance and update of the quantum
    probability amplitude matrix for each individual. In each iteration, the quantum
    rotation gate and the quantum non-gate are executed once, so the computational
    complexity of the quantum operator can be denoted as follows: Q C G A quantum
    = 2 × M a x × P × T × e . (13) The complexity of the chaotic operator is concentrated
    in the initial population and perturbed population operations. The complexity
    of the chaotic operator can be denoted as follows: Q C G A chaotic = log 2 ( P
    + T ) . (14) The total computational complexity of the QCGA can be denoted as
    follows: Q C G A total = log 2 ( P + T ) + 5 × M a x G e n × P × T × e . (15)
    The computational complexity of SO mainly comes from iterative computation, initialization
    of the population as a whole, updating of female and male population positions,
    elite individual replacement, and the number of edge servers. The computational
    complexity of the male population is shown in Eq. (16), and the female population
    is shown in Eq. (17), S O m = M a x G e n × P m 2 × T × e , (16) S O f = M a x
    G e n × P f 2 × T × e , (17) where MaxGen represents the maximum number of iterations,
    T represents the number of agricultural drip irrigation fertilization tasks, e
    represents the number of edge servers, Pm and Pf denote the male and female population
    sizes, respectively, which are calculated as shown in Eq. (18), and P is the overall
    population size, P m = P f = P 2 . (18) Therefore, the overall computational complexity
    of SO is shown in the following equation: S O total = P 3 2 × M a x G e n × T
    × e . (19) To compare the computational complexity of QCGA and SO, this study
    uses the big O notation to simplify the formulas; the lower-order terms, constant
    terms, and coefficients are removed, and the highest-order terms are retained,
    which leads to the following equations: Q C G A = O ( log 2 ( P + T ) + M a x
    G e n × P × T × e ) , (20) S O = O ( P 3 × M a x G e n × T × e ) . (21) Because
    log2(P + T) is a function of logarithmic order, it grows much slower than both
    P and T so that it can be ignored, and in addition, MaxGen, T, and e are set the
    same for both algorithms, which further leads to the following equations: Q C
    G A = O ( P ) , (22) S O = O ( P 3 ) . (23) Through the algorithm complexity analysis,
    the QCGA proposed in this paper is also lower in complexity than the advanced
    algorithm SO proposed in recent years, and the results show that the QCGA can
    accomplish the same task in less time and the effectiveness of the QCGA is verified
    in large-scale complex optimization scenarios. V. RESULTS AND DISCUSSION To verify
    the effectiveness of QCGA in edge task offloading of AIFWSNs, this paper compares
    it with other state-of-the-art task offloading methods and traditional offloading
    methods, including SO, GA, PSO, random offloading, and sequential offloading.
    This paper used the CloudSim cloud computing simulation platform to conduct real-world
    edge offloading tests for agricultural drip irrigation and fertilization tasks.
    The specific test method is to use CloudSim to build an edge computing platform
    for agricultural drip irrigation fertilization task, which contains some agricultural
    drip irrigation fertilization task attributes, such as task execution difficulty,
    task execution completion output file size, task input file size, and some edge
    server attributes, such as server memory, computational power, number of CPUs,
    and computational power. The above task attributes and server attributes are references
    to the task attribute parameters and server parameters that exist in the actual
    scenario. Based on this, the edge offloading method in agricultural drip irrigation
    and fertilization application obtained using various task offloading algorithms
    is tested, and the total task migration time is counted. The specific technical
    route is as follows: first, using the powerful computing power of the central
    cloud server, the characteristics data of the current agricultural drip irrigation
    and fertilization tasks are collected, including the attributes of the agricultural
    drip irrigation and fertilization tasks and the attributes of the edge server,
    and based on these data, the central cloud server obtains the edge offloading
    scheme in the agricultural drip irrigation and fertilization application by various
    offloading algorithms and then sends the detailed information about the location
    where the agricultural drip irrigation and fertilization tasks are offloaded to
    the edge server. After that, control information about the detailed location of
    the offloaded tasks to the edge server is sent, and the actual offloading of agricultural
    drip irrigation and fertilization tasks is finally realized. In addition, all
    simulation tests were done on a Windows 11 device with 12th Gen Intel(R) Core(TM)
    i5-12400F 2.50 GHz. The experimental data involved in this paper are averaged
    from the results of 100 experiments. To compare the time delay of different task
    offloading schemes of AIFWSNs, uniform standard parameters are used to ensure
    the objectivity and fairness of the simulation experiments. In the simulation
    experiments, all algorithms’ population size is 40, and the number of iterations
    is 1000. The detailed parameter settings of the four heuristic algorithms are
    shown in Table I. TABLE I. Algorithm parameter setting. Name Parameter QCGA  mutation_rate
    = 0.05 quantum_Not_prob = 0.08  SO  c1 = 0.5, c2 = 0.05, c3 = 2  GA  mutation_rate
    = 0.05  PSO  C1 = 2 C2 = 2 vmin = −5 vmax = 5 wmin = 0.4 wmax = 0.9  For this
    task offloading scheme effectiveness test, the list of parameters used for the
    agricultural drip irrigation and fertilization task is shown in Table II. Here,
    MI is a Million Instructions, the number of agricultural drip irrigation and fertilization
    task length in millions of instructions. TABLE II. Agricultural drip irrigation
    and fertilization task parameters. Attribute name Attribute value Length of agricultural
    drip irrigation and fertilization tasks  30 000–50 000 MI  Input file size for
    agricultural drip irrigation and fertilization tasks  350 bytes  Output file size
    for agricultural drip irrigation and fertilization tasks  350 bytes  Number of
    agricultural drip irrigation fertilization tasks  50–300  Table III presents the
    relevant attributes of the agricultural drip irrigation and fertilization edge
    server used during the testing. The unit of measurement for server computing power,
    MIPS, is the number of millions of machine language instructions per second that
    it processes. TABLE III. Related properties of agricultural edge servers. Attribute
    name Attribute value Number of edge servers  10–30  Server RAM size  512–2048
    MB  Server bandwidth  2000–4000 bps  Server computing capacity  1000–2000 MIPS  Number
    of CPUs on the server  1–8  Server operating systems  Linux  Virtual machine monitor  Xen  To
    comprehensively evaluate the superiority of offloading schemes obtained by QCGA,
    this paper uses four offloading schemes based on heuristic algorithms, QCGA, SO,
    GA, and PSO, and two common offloading schemes for agricultural drip irrigation
    and fertilization tasks: sequential offloading and random offloading. Sequential
    offloading assigns agricultural drip irrigation fertilization tasks to edge servers
    in the order of task numbers, while random offloading randomly assigns agricultural
    drip irrigation fertilization tasks to one edge server for execution. In this
    paper, four test scenarios with different metrics are set up: (1) 50 agricultural
    drip irrigation and fertilization tasks and 10 edge servers; (2) 100 agricultural
    drip irrigation and fertilization tasks and 20 edge servers; (3) 200 agricultural
    drip irrigation and fertilization tasks and 25 edge servers; and (4) 300 agricultural
    drip irrigation and fertilization tasks and 30 edge servers. The primary evaluation
    metric is the aggregate duration required to accomplish all agricultural drip
    irrigation and fertilization tasks, whereby a smaller value indicates the superior
    performance of the offload solution. Figures 6(a)–6(d) show the variation of the
    delay benefit values for the four heuristic-based task offloading methods. The
    horizontal axis of Figs. 6(a)–6(d) represents the number of iterations, and the
    vertical axis represents the delay benefit value, with lower benefit values indicating
    a lower delay of the associated task offloading schemes. Based on the iteration
    changes, it is obvious that the QCGA-based task offloading method has significantly
    lower latency than the other three heuristic comparison algorithms, while the
    latency results of the other task offloading methods do not differ much. FIG.
    5. VIEW LARGEDOWNLOAD SLIDE Algorithm flowchart of QCGA. FIG. 6. VIEW LARGEDOWNLOAD
    SLIDE Delay benefit values of task offloading algorithms with different indicators:
    (a) 50 tasks, 10 servers; (b) 100 tasks, 20 servers; (c) 200 tasks, 25 servers;
    and (d) 300 tasks, 30 servers. In addition, the QCGA algorithm presented in this
    paper has a superior completion time for obtaining an offloading scheme for agricultural
    drip irrigation and fertilization tasks compared to the other three heuristic
    algorithms. This is due to the fact that QCGA employs novel quantum and elite
    operators to enhance the algorithm’s ability to search for the best task offloading
    scheme. By comparing the results of QCGA, SO, PSO, and GA, as well as analyzing
    the result curves of the four algorithms in different scenarios, the results show
    that not only QCGA is significantly faster than the other three comparison algorithms
    in terms of convergence speed but also the delay benefit value obtained by QCGA
    is markedly better than the comparison algorithms. On the contrary, all three
    comparison algorithms fail to utilize the population information fully and lack
    population diversity, thus falling into local optimal solutions and ultimately
    not finding the best task offloading solution. The analysis of the results further
    demonstrates the effectiveness of QCGA using new quantum and novel elite operators
    to enhance its performance. Figures 7(a)–7(d) show the total offload time of the
    agricultural drip irrigation and fertilization tasks vs the number of tasks and
    edge servers in scenarios 1–4. As the number of tasks and edge servers increases,
    the task offloading delay increases for all algorithms. The QCGA-based offloading
    scheme performs the best among all algorithms, with the shortest time required
    to complete the agricultural drip irrigation fertilization task, while the random
    offloading method-based offloading scheme performs the worst. FIG. 7. VIEW LARGEDOWNLOAD
    SLIDE Different indicators under the agriculture task offloading completion time:
    (a) 50 tasks, 10 servers; (b) 100 tasks, 20 servers; (c) 200 tasks, 25 servers;
    and (d) 300 tasks, 30 servers. Taking scenarios 1 and 4 as an example, in scenario
    1, when there are 50 agricultural drip irrigation fertilization tasks and 10 edge
    servers, the total task completion times of QCGA, SO, GA, and PSO are 121.52,
    130.24, 140.59, and 133.31 ms, respectively. At this time, the performance difference
    between these four heuristic optimization algorithm-based offloading schemes is
    not significant. In contrast, the total task completion times of the unplanned
    sequential and random offloading schemes are 163.55 and 209.02 ms, respectively,
    which are much higher than those of the first four heuristic task offloading algorithms;
    the results show that the sequential and random offloading schemes do not consider
    the processing capacity and status of the edge servers, which leads to load imbalance,
    with some servers overloaded, while others are idle. However, the heuristic optimization
    based algorithm can find a better task allocation scheme through iterative search,
    which ultimately reduces the overall task completion time. In scenario 4, when
    there are 300 agricultural drip irrigation fertilization tasks and 30 edge servers,
    the total task completion time of QCGA is the shortest, which is only 274.98 ms.
    In contrast, the total task completion time of QCGA is lower than that of SO,
    GA, PSO, sequential offloading, and random offloading by 8.38%, 22.48%, 18.01%,
    54.22%, and 63.22%, respectively. The results show that when the problem size
    becomes more significant, the advantages of the QCGA algorithm’s fast convergence
    speed and strong search ability become more pronounced, and it can find the optimal
    solution in the large-scale solution space quickly and efficiently. To further
    validate the effectiveness of the task offloading schemes obtained by the QCGA
    algorithm, two different experimental scenarios are designed in this paper to
    test the stability of QCGA in large-scale AIFWSNs. With a fixed number of 100
    edge servers, Fig. 8 shows the variation of total task completion time for different
    task offloading schemes as the number of agricultural drip irrigation fertilization
    tasks increases from 1000 to 4000. When the number of agricultural drip irrigation
    fertilization tasks is 1,000, the total task completion time of QCGA is 330.97
    ms, SO is 363.38 ms, GA is 392.17 ms, PSO is 373.19 ms, sequential offloading
    is 398.96 ms, and random offloading is 475.01 ms. When the number of agricultural
    drip irrigation fertilization tasks increases to 4,000, the total task completion
    time of the QCGA algorithm is 1021.76 ms, SO is 1169 ms, GA is 1396.38 ms, PSO
    is 1223.40 ms, sequential offloading is 1424.48 ms, and random offloading is 1621.49
    ms. From the latency trends of different offloading scenarios in Fig. 8, it can
    be seen that when the number of edge servers is fixed, the number of tasks keeps
    increasing, and the task offloading scheme obtained by QCGA is still the scheme
    with the shortest total task completion time. According to the trend of the curve,
    it can be seen that the larger the size of AIFWSNs, the more pronounced the advantage
    of QCGA. By analyzing the above results, it is found that the design of a new
    delay evaluation function in this paper is one of the reasons why the total task
    completion time of QCGA is significantly lower than that of several other schemes;
    at the same time, in the evaluation function, this paper entirely takes into account
    the impact of different delay components on the evaluation function and balances
    the weights of different delay components in the final evaluation criteria. FIG.
    8. VIEW LARGEDOWNLOAD SLIDE Task offload completion time for different numbers
    of agriculture tasks under 100 edge servers. Figure 9 demonstrates that with a
    fixed number of 1000 agricultural drip irrigation fertilization tasks, this study
    evaluates the latency variations of different task offloading schemes when the
    number of edge servers is 200, 300, 400, and 500. The results show that when the
    number of edge servers is 200, the total time for QCGA to complete task offloading
    is 177.70 ms. In contrast, when the number of edge servers is increased to 500,
    the total time for QCGA to complete task offloading is 88.76 ms, compared to the
    total time for task offloading completion of SO, GA, PSO, sequential offloading,
    and random offloading, which is 113.78, 157.07, 144.07, 164.88, and 190.74 ms,
    respectively. The results show that QCGA exhibits more significant advantages
    as the number of edge servers increases with a fixed number of tasks. In addition,
    the new chaos operator and quantum operator designed in this paper are also among
    the reasons why QCGA shows excellent advantages. The convergence speed of QCGA
    is accelerated by designing new chaotic operators. At the same time, the ability
    of QCGA to jump out of local optimum is also enhanced as the number of agricultural
    drip irrigation and fertilization tasks increases from 1000 to 4000 with increasing
    quantum probability. FIG. 9. VIEW LARGEDOWNLOAD SLIDE Task offload completion
    time for different numbers of edge servers under 1000 agriculture tasks. As the
    number of tasks and edge servers increases, the performance advantage of the QCGA
    task offloading scheme becomes more and more significant. Several experimental
    results show that the QCGA is more efficient in task offloading scheme optimization
    and can complete tasks faster. Compared with other algorithms, the QCGA can significantly
    reduce the task completion time, thus enhancing the efficiency and performance
    of the system. This is important for agricultural drip irrigation and fertilization
    tasks in edge computing application scenarios that require high task processing
    speed. VI. CONCLUSIONS Due to the high time delay requirement for agricultural
    drip irrigation and fertilization tasks, task offloading in edge computing of
    AIFWSNs is more focused on satisfying the time delay constraints. Therefore, in
    building the model, this paper introduces a new delay coefficient design evaluation
    function to divide the delay of agricultural drip irrigation and fertilization
    task offloading into the task upload delay, task execution delay, and task completion
    delay. In addition, this paper proposes a QCGA task offloading method. The quantum
    revolving gate in QCGA innovatively simplifies the update method of quantum probability
    amplitude and significantly enhances the global search capability of QCGA. In
    addition, a new chaos learning operator is designed in this paper to improve the
    convergence performance of QCGA. Simulation results show that the solutions of
    the QCGA-based edge task offloading algorithm for AIFWSNs are all better than
    several other offloading methods and dramatically reduce the task offloading delay.
    With the continuous development of edge computing technologies and the expansion
    of application scenarios, future edge computing deployments will become more complex.
    To better address these challenges, this team will continue to study the edge
    task offloading problem of AIFWSNs in-depth and explore methods and strategies
    for task offloading in more complex edge computing environments. This includes
    considering more edge devices and network topologies and addressing the challenges
    of more dynamic and unstable network environments and task load variations. In
    addition, to further improve the stability and reliability of the QCGA, this team
    will conduct more in-depth research. This will include further analyzing and optimizing
    the performance of the QCGA, exploring more efficient and reliable genetic operators
    and quantum revolving gates, and designing evaluation functions and chaotic learning
    operators more suitable for agricultural drip irrigation and fertilization application
    scenarios. ACKNOWLEDGMENTS This paper was funded by the National Natural Science
    Foundation of China, Grant No. 61962053; the National Key R&D Program of China,
    Grant No. 2022ZD0115803; the Corps Science and Technology Plan Projects, Grant
    No. 2022BC004; the Shihezi University High-level Talent Research Start-up Fund
    Project, Grant No. RCZK2018C39; the Shihezi University Young Innovative Talent
    Program Project, Grant No. CXPY202204. AUTHOR DECLARATIONS Conflict of Interest
    The authors have no conflicts to disclose. Author Contributions J.Z. and X.L.
    contributed equally to this work. J.Z. and X.L. conceived and designed the study.
    J.Z. and X.L. performed the experiments. J.Z. and X.L. wrote the paper. J.Z.,
    X.L., and M.T. reviewed and edited the manuscript. All authors read and approved
    the manuscript. Jiawei Zhao: Conceptualization (equal); Data curation (equal);
    Funding acquisition (equal); Investigation (equal); Project administration (equal);
    Software (equal); Validation (equal); Writing – original draft (equal); Writing
    – review & editing (equal). Xiang Liu: Conceptualization (equal); Data curation
    (equal); Formal analysis (equal); Methodology (equal); Software (equal); Validation
    (equal); Writing – original draft (equal); Writing – review & editing (equal).
    Min Tian: Funding acquisition (equal); Methodology (equal); Project administration
    (equal); Supervision (equal); Writing – review & editing (equal). DATA AVAILABILITY
    The data that support the findings of this study are available from the corresponding
    author upon reasonable request. REFERENCES 1.O. Friha, M. A. Ferrag, L. Shu, L.
    Maglaras, and X. Wang, “Internet of things for the future of smart agriculture:
    A comprehensive survey of emerging technologies,” IEEE/CAA J. Autom. Sin. 8(4),
    718–752 (2021). https://doi.org/10.1109/jas.2021.1003925 Google ScholarCrossref   2.C.
    Li, Y. Liu, J. Xiao, and J. Zhou, “MCEAACO-QSRP: A novel QoS-secure routing protocol
    for industrial Internet of Things,” IEEE Internet Things J. 9(19), 18760–18777
    (2022). https://doi.org/10.1109/jiot.2022.3162106 Google ScholarCrossref   3.S.
    R. Bader, M. Maleshkova, and S. Lohmann, “Structuring reference architectures
    for the industrial internet of things,” Future Internet 11(7), 151 (2019). https://doi.org/10.3390/fi11070151
    Google ScholarCrossref   4.J. A. Alzubi, O. A. Alzubi, A. Singh, and M. Ramachandran,
    “Cloud-IIoT-based electronic health record privacy-preserving by CNN and blockchain-enabled
    federated learning,” IEEE Trans. Ind. Inf. 19(1), 1080–1087 (2023). https://doi.org/10.1109/tii.2022.3189170
    Google ScholarCrossref   5.H. Wang, L. Shao, M. Li, B. Wang, and P. Wang, “Estimation
    of clock skew for time synchronization based on two-way message exchange mechanism
    in industrial wireless sensor networks,” IEEE Trans. Ind. Inf. 14(11), 4755–4765
    (2018). https://doi.org/10.1109/tii.2018.2799595 Google ScholarCrossref   6.S.
    Dash, S. Biswas, D. Banerjee, and A. U. Rahman, “Edge and fog computing in healthcare
    – a review,” Scalable Comput. 20(2), 191–206 (2019). https://doi.org/10.12694/scpe.v20i2.1504
    Google ScholarCrossref   7.R. Zhang, L. Wu, S. Cao, X. Hu, S. Xue, D. Wu, and
    Q. Li, “Task offloading with task classification and offloading nodes selection
    for MEC-enabled IoV,” ACM Trans. Internet Technol. 22(2), 1–24 (2021). https://doi.org/10.1145/3475871
    Google ScholarCrossref   8.T. Q. Thinh, J. Tang, Q. D. La, and T. Q. S. Quek,
    “Offloading in mobile edge computing: Task allocation and computational frequency
    scaling,” IEEE Trans. Commun. 65, 3571–3584 (2017). https://doi.org/10.1109/tcomm.2017.2699660
    Google ScholarCrossref   9.H. Wu, Y. Sun, and K. Wolter, “Energy-efficient decision
    making for mobile cloud offloading,” IEEE Trans. Cloud Comput. 8(2), 570–584 (2020).
    https://doi.org/10.1109/tcc.2018.2789446 Google ScholarCrossref   10.H. Tang,
    H. Wu, G. Qu, and R. Li, “Double deep Q-network based dynamic framing offloading
    in vehicular edge computing,” IEEE Trans. Network Sci. Eng. 10(3), 1297–1310 (2023).
    https://doi.org/10.1109/tnse.2022.3172794 Google ScholarCrossref   11.M. Xue,
    H. Wu, G. Peng, and K. Wolter, “DDPQN: An efficient DNN offloading strategy in
    local-edge-cloud collaborative environments,” IEEE Trans. Serv. Comput. 15(2),
    640–655 (2022). https://doi.org/10.1109/tsc.2021.3116597 Google ScholarCrossref   12.H.
    Wu, J. Chen, T. N. Nguyen, and H. Tang, “Lyapunov-guided delay-aware energy efficient
    offloading in IIoT-MEC systems,” IEEE Trans. Ind. Inf. 19(2), 2117–2128 (2023).
    https://doi.org/10.1109/tii.2022.3206787 Google ScholarCrossref   13.Y. Sun, S.
    Zhou, and J. Xu, “EMM: Energy-aware mobility management for mobile edge computing
    in ultra dense networks,” IEEE J. Sel. Areas Commun. 35(11), 2637–2646 (2017).
    https://doi.org/10.1109/jsac.2017.2760160 Google ScholarCrossref   14.M. Chen
    and Y. Hao, “Task offloading for mobile edge computing in software defined ultra-dense
    network,” IEEE J. Sel. Areas Commun. 36(3), 587–597 (2018). https://doi.org/10.1109/jsac.2018.2815360
    Google ScholarCrossref   15.C. Yi, J. Cai, and Z. Su, “A multi-user mobile computation
    offloading and transmission scheduling mechanism for delay-sensitive applications,”
    IEEE Trans. Mobile Comput. 19(1), 29–43 (2020). https://doi.org/10.1109/tmc.2019.2891736
    Google ScholarCrossref   16.M. Dong, K. Ota, A. Liu, and M. Guo, “Joint optimization
    of lifetime and transport delay under reliability constraint wireless sensor networks,”
    IEEE Trans. Parallel Distrib. Syst. 27(1), 225–236 (2016). https://doi.org/10.1109/tpds.2015.2388482
    Google ScholarCrossref   17.P. Neamatollahi, S. Abrishami, M. Naghibzadeh, M.
    H. Yaghmaee Moghaddam, and O. Younis, “Hierarchical clustering-task scheduling
    policy in cluster-based wireless sensor networks,” IEEE Trans. Ind. Inf. 14(5),
    1876–1886 (2018). https://doi.org/10.1109/tii.2017.2757606 Google ScholarCrossref   18.X.
    Ding, Y. Tian, and Y. Yu, “A real-time big data gathering algorithm based on indoor
    wireless sensor networks for risk analysis of industrial operations,” IEEE Trans.
    Ind. Inf. 12(3), 1232–1242 (2016). https://doi.org/10.1109/tii.2015.2436337 Google
    ScholarCrossref   19.Y. Liu, C. Li, J. Xiao, Z. Li, W. Chen, X. Qu, and J. Zhou,
    “QEGWO: Energy-efficient clustering approach for industrial wireless sensor networks
    using quantum-related bioinspired optimization,” IEEE Internet Things J. 9(23),
    23691–23704 (2022). https://doi.org/10.1109/jiot.2022.3189807 Google ScholarCrossref   20.Y.
    Cui, Y. Liang, and R. Wang, “Resource allocation algorithm with multi-platform
    intelligent offloading in D2D-enabled vehicular networks,” IEEE Access 7, 21246–21253
    (2019). https://doi.org/10.1109/access.2018.2882000 Google ScholarCrossref   21.X.
    Chen, L. Jiao, W. Li, and X. Fu, “Efficient multi-user computation offloading
    for mobile-edge cloud computing,” IEEE/ACM Trans. Networking 24(5), 2795–2808
    (2016). https://doi.org/10.1109/tnet.2015.2487344 Google ScholarCrossref   22.Z.
    Ning, K. Zhang, X. Wang, L. Guo, X. Hu, J. Huang, B. Hu, and R. Y. K. Kwok, “Intelligent
    edge computing in internet of vehicles: A joint computation offloading and caching
    solution,” IEEE Trans. Intell. Transp. Syst. 22(4), 2212–2225 (2021). https://doi.org/10.1109/tits.2020.2997832
    Google ScholarCrossref   23.X. Shen, Z. Chang, X. Xie, and S. Niu, “Task offloading
    strategy of vehicular networks based on improved bald eagle search optimization
    algorithm,” Appl. Sci. 12(18), 9308 (2022). https://doi.org/10.3390/app12189308
    Google ScholarCrossref   24.J. Xiao, Y. Liu, J. Zhou, and O. Kaiwartya, “Quantum
    clone elite genetic algorithm-based evaluation mechanism for maximizing network
    efficiency in soil moisture wireless sensor networks,” J. Sens. 2021, 5590472.
    https://doi.org/10.1155/2021/5590472 Crossref   25.A. Flori, H. Oulhadj, and P.
    Siarry, “QUAntum Particle Swarm Optimization: An auto-adaptive PSO for local and
    global optimization,” Comput. Optim. Appl. 82(2), 525–559 (2022). https://doi.org/10.1007/s10589-022-00362-2
    Google ScholarCrossref   26.M. G. de Andoin and J. Echanobe, “Implementable hybrid
    quantum ant colony optimization algorithm,” Quantum Mach. Intell. 4(2), 12 (2022).
    https://doi.org/10.1007/s42484-022-00065-1 Google ScholarCrossref   27.X. Liu,
    M. Tian, J. Zhou, and J. Liang, “An efficient coverage method for SEMWSNs based
    on adaptive chaotic Gaussian variant snake optimization algorithm,” Math. Biosci.
    Eng. 20(2), 3191–3215 (2022). https://doi.org/10.3934/mbe.2023150 Google ScholarCrossref
    PubMed  28.J. Zhou, G. Qi, C. Liu, and B. Gao, “A chaotic parallel artificial
    fish swarm algorithm for water quality monitoring sensor networks 3D coverage
    optimization,” J. Sens. 2021, 5529527. https://doi.org/10.1155/2021/5529527 Crossref   29.B.
    Liu, R. Yang, M. Xu, J. Zhou, and B. Gao, “A chaotic elite niche evolutionary
    algorithm for low-power clustering in environment monitoring wireless sensor networks,”
    J. Sens. 2021, 5558643. https://doi.org/10.1155/2021/5558643 Crossref   30.X.
    Xia and S. Li, “Research on improved chaotic particle optimization algorithm based
    on complex function,” Front. Phys. 8, 368 (2020). https://doi.org/10.3389/fphy.2020.00368
    Google ScholarCrossref   31.F. A. Hashim and A. G. Hussien, “Snake optimizer:
    A novel meta-heuristic optimization algorithm,” Knowl. Based Syst. 242, 108320
    (2022). https://doi.org/10.1016/j.knosys.2022.108320 Google ScholarCrossref   32.S.
    Gupta and K. Deep, “An opposition-based chaotic Grey Wolf Optimizer for global
    optimisation tasks,” J. Exp. Theor. Artif. Intell. 31(5), 751–779 (2018). https://doi.org/10.1080/0952813x.2018.1554712
    Google ScholarCrossref   33.K. N. Apinaya Prethi and M. Sangeetha, “A multi-objective
    optimization of resource management and minimum batch VM migration for prioritized
    task allocation in fog-edge-cloud computing,” J. Intell. Fuzzy Syst. 43(5), 5985–5995
    (2022). https://doi.org/10.3233/jifs-213520 Google ScholarCrossref   34.Y. Huang,
    H. Chen, G. Ma, K. Lin, Z. Ni, N. Yan, and Z. Wang, “OPAT: Optimized allocation
    of time-dependent tasks for mobile crowdsensing,” IEEE Trans. Ind. Inf. 18(4),
    2476–2485 (2022). https://doi.org/10.1109/tii.2021.3094527 Google ScholarCrossref   35.J.
    Zhou and X. Zhang, “Fairness-aware task offloading and resource allocation in
    cooperative mobile-edge computing,” IEEE Internet Things J. 9(5), 3812–3824 (2022).
    https://doi.org/10.1109/jiot.2021.3100253 Google ScholarCrossref   36.W. Hou,
    H. Wen, N. Zhang, J. Wu, W. Lei, and R. Zhao, “Incentive-driven task allocation
    for collaborative edge computing in industrial internet of things,” IEEE Internet
    Things J. 9(1), 706–718 (2022). https://doi.org/10.1109/jiot.2021.3085143 Google
    ScholarCrossref   © 2024 Author(s). All article content, except where otherwise
    noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).
    View Metrics Citing Articles Via Google Scholar Submit your article   Sign up
    for alerts Most Read Most Cited The second law of infodynamics and its implications
    for the simulated universe hypothesis Melvin M. Vopson Study of magnetic force
    between the two magnets for the torque and speed evaluations of rim-driven motor
    Siqing Liu, Franklin Li Duan, et al. Non-enzymatic electrochemical sensor based
    on ZnO nanoparticles/porous graphene for the detection of hypoxanthine in pork
    meat N. T. H. Le, N. X. Viet, et al. Online ISSN 2158-3226 Resources For Researchers
    For Librarians For Advertisers Our Publishing Partners  Explore Journals Physics
    Today Conference Proceedings Books Special Topics Publishers pubs.aip.org About
    User Guide Contact Us Register Help Privacy Policy Terms of Use Connect with AIP
    Publishing Facebook LinkedIn Twitter YouTube © Copyright 2024 AIP Publishing LLC"'
  inline_citation: '>'
  journal: AIP Advances
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An efficient task offloading method for drip irrigation and fertilization
    at edge nodes based on quantum chaotic genetic algorithm
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rathore N.
  - Rajavat A.
  citation_count: '0'
  description: In today’s digital age, the Internet of Things (IoT) is gaining popularity.
    IoT has made everything smart, for example, smart cities (smart buildings and
    smart homes), smart healthcare (personal monitoring, smart wearables), industrial
    automation (especially manufacturing), commercial (shopping systems, retail),
    and even that agriculture too. IoT is becoming increasingly important in today’s
    digital age, resulting in a rapid rise in the number of devices connected to it.
    Massive amounts of data will be produced by such widely distributed IoT devices
    at the network’s edge. Processing these massive amounts of data in the centralized
    cloud is expected to result in increased 400bandwidth utilization, latency, and
    network congestion. Edge computing has become a popular paradigm in recent years
    for reducing network congestion and serving real-time IoT applications by providing
    services close to enduser devices. Agriculture is the foundation of the economy
    of any nation in the world. By 2050, the world population will need a 70% increase
    in food production to feed an estimated global population of more than 9 billion
    people. Potatoes are consumed all over the world and its production plays an important
    role in agriculture. The two primary diseases that adversely affect the yield
    of potato crop production are early blight and late blight. Therefore, in this
    work, we used containerized microservices to deploy machine learning models to
    resource restricted edge nodes on agricultural land for real-time disease and
    irrigation water requirement prediction in potato crops. Containers are lightweight
    and easy to deploy, making them the ideal choice for running machine learning
    models on resource-constrained edge nodes. We examined AlexNet, MobileNet, and
    VGG16, three deep convolutional neural networks (CNN), to detect these diseases
    automatically. A dataset of 7128 images containing healthy and diseased leaves
    of potato plants was used to train all three CNN models on the cloud. Even if
    training is outsourced, trained models need a lot of RAM; hence, the first aim
    of this study is to find a lightweight CNN model that can easily fit into resource-constrained
    devices. To improve potato crop yield and reduce economic losses, we found and
    deployed lightweight CNN model at the edge node to identify diseases in real time
    using leaf pictures recorded by in-place camera devices. The advantage of the
    developed technique is that by classifying potato leaf pictures in real time on-premises,
    there is no need to transfer images to the cloud for probable disease identification,
    which increases network congestion. The lightweight CNN model achieved 99.87%
    accuracy for both train and test images, according to the results. Precision (P),
    Recall (R), and F1 score (F1) are also displayed, to visualize the model’s efficiency.
    Similarly for real-time prediction of irrigation water requirement in potato crop,
    we trained two machine learning models, Support Vector Machine (SVM) and Logistic
    Regression (LR) on 100,000 values of records. Each record has four input parameters
    (soil moisture, temperature, humidity, and how many days the crop was planted
    before). With four input parameters in each record (soil moisture, temperature,
    humidity, and how many days before the crop was planted), the model decides whether
    the water pump should be turned on or off. The SVM model and the LR model attained
    92 and 73% accuracy, respectively, in determining whether the water pump should
    be turned ON or OFF.
  doi: 10.1201/9781003435228-24
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeEnvironment & AgricultureAgriculture & Environmental SciencesAgriculturePrecision
    Agriculture for SustainabilitySmart Farming Based on IOT-Edge Computing: Applying
    Machine Learning Models For Disease And Irrigation Water Requirement Prediction
    In Potato Crop Using Containerized Microservices Chapter Smart Farming Based on
    IOT-Edge Computing: Applying Machine Learning Models For Disease And Irrigation
    Water Requirement Prediction In Potato Crop Using Containerized Microservices
    ByNitin Rathore, Anand Rajavat Book Precision Agriculture for Sustainability Edition
    1st Edition First Published 2024 Imprint Apple Academic Press Pages 26 eBook ISBN
    9781003435228 Share ABSTRACT In today’s digital age, the Internet of Things (IoT)
    is gaining popularity. IoT has made everything smart, for example, smart cities
    (smart buildings and smart homes), smart healthcare (personal monitoring, smart
    wearables), industrial automation (especially manufacturing), commercial (shopping
    systems, retail), and even that agriculture too. IoT is becoming increasingly
    important in today’s digital age, resulting in a rapid rise in the number of devices
    connected to it. Massive amounts of data will be produced by such widely distributed
    IoT devices at the network’s edge. Processing these massive amounts of data in
    the centralized cloud is expected to result in increased 400bandwidth utilization,
    latency, and network congestion. Edge computing has become a popular paradigm
    in recent years for reducing network congestion and serving real-time IoT applications
    by providing services close to enduser devices. Agriculture is the foundation
    of the economy of any nation in the world. By 2050, the world population will
    need a 70% increase in food production to feed an estimated global population
    of more than 9 billion people. Potatoes are consumed all over the world and its
    production plays an important role in agriculture. The two primary diseases that
    adversely affect the yield of potato crop production are early blight and late
    blight. Therefore, in this work, we used containerized microservices to deploy
    machine learning models to resource restricted edge nodes on agricultural land
    for real-time disease and irrigation water requirement prediction in potato crops.
    Containers are lightweight and easy to deploy, making them the ideal choice for
    running machine learning models on resource-constrained edge nodes. We examined
    AlexNet, MobileNet, and VGG16, three deep convolutional neural networks (CNN),
    to detect these diseases automatically. A dataset of 7128 images containing healthy
    and diseased leaves of potato plants was used to train all three CNN models on
    the cloud. Even if training is outsourced, trained models need a lot of RAM; hence,
    the first aim of this study is to find a lightweight CNN model that can easily
    fit into resource-constrained devices. To improve potato crop yield and reduce
    economic losses, we found and deployed lightweight CNN model at the edge node
    to identify diseases in real time using leaf pictures recorded by in-place camera
    devices. The advantage of the developed technique is that by classifying potato
    leaf pictures in real time on-premises, there is no need to transfer images to
    the cloud for probable disease identification, which increases network congestion.
    The lightweight CNN model achieved 99.87% accuracy for both train and test images,
    according to the results. Precision (P), Recall (R), and F1 score (F1) are also
    displayed, to visualize the model’s efficiency. Similarly for real-time prediction
    of irrigation water requirement in potato crop, we trained two machine learning
    models, Support Vector Machine (SVM) and Logistic Regression (LR) on 100,000 values
    of records. Each record has four input parameters (soil moisture, temperature,
    humidity, and how many days the crop was planted before). With four input parameters
    in each record (soil moisture, temperature, humidity, and how many days before
    the crop was planted), the model decides whether the water pump should be turned
    on or off. The SVM model and the LR model attained 92 and 73% accuracy, respectively,
    in determining whether the water pump should be turned ON or OFF. Previous Chapter
    Next Chapter Your institution has not purchased this content. Please get in touch
    with your librarian to recommend this.  To purchase a print version of this book
    for personal use or request an inspection copy  GO TO ROUTLEDGE.COM  Policies
    Privacy Policy Terms & Conditions Cookie Policy Journals Taylor & Francis Online
    Corporate Taylor & Francis Group Help & Contact Students/Researchers Librarians/Institutions
    Connect with us Registered in England & Wales No. 3099067 5 Howick Place | London
    | SW1P 1WG © 2024 Informa UK Limited"'
  inline_citation: '>'
  journal: 'Precision Agriculture for Sustainability: Use of Smart Sensors, Actuators,
    and Decision Support Systems'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'SMART FARMING BASED ON IOT-EDGE COMPUTING: APPLYING MACHINE LEARNING MODELS
    FOR DISEASE AND IRRIGATION WATER REQUIREMENT PREDICTION IN POTATO CROP USING CONTAINERIZED
    MICROSERVICES'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hu S.H.
  - Luo Q.Y.
  - Li G.H.
  - Shi W.
  - Ye B.L.
  citation_count: '0'
  description: 'Erratum: Shi-Hong Hu, Qu-Yuan Luo, Guang-Hui Li, Weisong Shi, Bao-Liu
    Ye. CA-DTS: A distributed and collaborative task scheduling algorithm for edge
    computing enabled intelligent road network. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY
    2023 38(5): 1113–1131. DOI: 10.1007/s11390-023-2839-0. In the first two affiliations
    of the paper, i.e., the affiliations of author Shi-Hong Hu, “Hohai” was incorrectly
    spelled as “Houhai” by the author. Specifically, the first two affiliations should
    be: 1Key Laboratory of Water Big Data Technology of Ministry of Water Resources,
    Hohai University, Nanjing 210098, China 2School of Computer and Information, Hohai
    University, Nanjing 210098, China.'
  doi: 10.1007/s11390-023-0009-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Computer Science and Technology
    Article Erratum to: CA-DTS: A Distributed and Collaborative Task Scheduling Algorithm
    for Edge Computing Enabled Intelligent Road Network Erratum Published: 30 November
    2023 Volume 38, page 1451, (2023) Cite this article Download PDF Journal of Computer
    Science and Technology Aims and scope Submit manuscript Shi-Hong Hu, Qu-Yuan Luo,
    Guang-Hui Li, Weisong Shi & Bao-Liu Ye  97 Accesses Explore all metrics The Original
    Article was published on 30 September 2023 Article PDF Author information Authors
    and Affiliations Key Laboratory of Water Big Data Technology of Ministry of Water
    Resources, Hohai University, Nanjing, 210098, China Shi-Hong Hu School of Computer
    and Information, Hohai University, Nanjing, 210098, China Shi-Hong Hu & Bao-Liu
    Ye School of Information Science and Technology, Southwest Jiaotong University,
    Chengdu, 611756, China Qu-Yuan Luo School of Artificial Intelligence and Computer
    Science, Jiangnan University, Wuxi, 214122, China Guang-Hui Li Department of Computer
    and Information Sciences, University of Delaware, Newark, DE, 19716, USA Weisong
    Shi National Key Laboratory for Novel Software Technology, Nanjing University,
    Nanjing, 210093, China Bao-Liu Ye Corresponding author Correspondence to Guang-Hui
    Li. Additional information The online version of the original article can be found
    at https://doi.org/10.1007/s11390-023-2839-0 Rights and permissions Reprints and
    permissions About this article Cite this article Hu, SH., Luo, QY., Li, GH. et
    al. Erratum to: CA-DTS: A Distributed and Collaborative Task Scheduling Algorithm
    for Edge Computing Enabled Intelligent Road Network. J. Comput. Sci. Technol.
    38, 1451 (2023). https://doi.org/10.1007/s11390-023-0009-z Download citation Published
    30 November 2023 Issue Date December 2023 DOI https://doi.org/10.1007/s11390-023-0009-z
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Use our pre-submission checklist Avoid common mistakes on your manuscript.
    Sections Article PDF Author information Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Computer Science and Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Erratum to: CA-DTS: A Distributed and Collaborative Task Scheduling Algorithm
    for Edge Computing Enabled Intelligent Road Network (Journal of Computer Science
    and Technology, (2023), 38, 5, (1113-1131), 10.1007/s11390-023-2839-0)'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chamara N.
  - Bai G.
  - Ge Y.
  citation_count: '1'
  description: 'Precision Agriculture (PA) promises to meet the future demands for
    food, feed, fiber, and fuel while keeping their production sustainable and environmentally
    friendly. PA relies heavily on sensing technologies to inform site-specific decision
    supports for planting, irrigation, fertilization, spraying, and harvesting. Traditional
    point-based sensors enjoy small data sizes but are limited in their capacity to
    measure plant and canopy parameters. On the other hand, imaging sensors can be
    powerful in measuring a wide range of these parameters, especially when coupled
    with Artificial Intelligence. The challenge, however, is the lack of computing,
    electric power, and connectivity infrastructure in agricultural fields, preventing
    the full utilization of imaging sensors. This paper reported AICropCAM, a field-deployable
    imaging framework that integrated edge image processing, Internet of Things (IoT),
    and LoRaWAN for low-power, long-range communication. The core component of AICropCAM
    is a stack of four Deep Convolutional Neural Networks (DCNN) models running sequentially:
    CropClassiNet for crop type classification, CanopySegNet for canopy cover quantification,
    PlantCountNet for plant and weed counting, and InsectNet for insect identification.
    These DCNN models were trained and tested with >43,000 field crop images collected
    offline. AICropCAM was embodied on a distributed wireless sensor network with
    its sensor node consisting of an RGB camera for image acquisition, a Raspberry
    Pi 4B single-board computer for edge image processing, and an Arduino MKR1310
    for LoRa communication and power management. Our testing showed that the time
    to run the DCNN models ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet,
    and power consumption ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet.
    The classification model CropClassiNet reported 94.5 % accuracy, and the segmentation
    model CanopySegNet reported 92.83 % accuracy. The two object detection models
    PlantCountNet and InsectNet reported mean average precision of 0.69 and 0.02 for
    the test images. Predictions from the DCNN models were transmitted to the ThingSpeak
    IoT platform for visualization and analytics. We concluded that AICropCAM successfully
    implemented image processing on the edge, drastically reduced the amount of data
    being transmitted, and could satisfy the real-time need for decision-making in
    PA. AICropCAM can be deployed on moving platforms such as center pivots or drones
    to increase its spatial coverage and resolution to support crop monitoring and
    field operations.'
  doi: 10.1016/j.compag.2023.108420
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusion and future perspectives Funding
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgements
    Data availability References Show full outline Cited by (1) Figures (12) Show
    6 more figures Tables (7) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show
    all tables Computers and Electronics in Agriculture Volume 215, December 2023,
    108420 AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge Author links open overlay
    panel Nipuna Chamara a, Geng Bai a, Yufeng Ge a b Show more Share Cite https://doi.org/10.1016/j.compag.2023.108420
    Get rights and content Under a Creative Commons license open access Highlights
    • We developed AICropCAM, an edge-computing enabled camera system for crop monitoring.
    • It integrated Raspberry Pi and Arduino for image processing and LoRa communication.
    • It ran a stack of four deep neural networks to characterize multiple plant/canopy
    parameters. • We quantified the power consumption and speed of the system for
    edge image-processing. • AICropCAM is a next-generation enabling technology for
    real-time, low-latency Ag applications. Abstract Precision Agriculture (PA) promises
    to meet the future demands for food, feed, fiber, and fuel while keeping their
    production sustainable and environmentally friendly. PA relies heavily on sensing
    technologies to inform site-specific decision supports for planting, irrigation,
    fertilization, spraying, and harvesting. Traditional point-based sensors enjoy
    small data sizes but are limited in their capacity to measure plant and canopy
    parameters. On the other hand, imaging sensors can be powerful in measuring a
    wide range of these parameters, especially when coupled with Artificial Intelligence.
    The challenge, however, is the lack of computing, electric power, and connectivity
    infrastructure in agricultural fields, preventing the full utilization of imaging
    sensors. This paper reported AICropCAM, a field-deployable imaging framework that
    integrated edge image processing, Internet of Things (IoT), and LoRaWAN for low-power,
    long-range communication. The core component of AICropCAM is a stack of four Deep
    Convolutional Neural Networks (DCNN) models running sequentially: CropClassiNet
    for crop type classification, CanopySegNet for canopy cover quantification, PlantCountNet
    for plant and weed counting, and InsectNet for insect identification. These DCNN
    models were trained and tested with >43,000 field crop images collected offline.
    AICropCAM was embodied on a distributed wireless sensor network with its sensor
    node consisting of an RGB camera for image acquisition, a Raspberry Pi 4B single-board
    computer for edge image processing, and an Arduino MKR1310 for LoRa communication
    and power management. Our testing showed that the time to run the DCNN models
    ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet, and power consumption
    ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet. The classification
    model CropClassiNet reported 94.5 % accuracy, and the segmentation model CanopySegNet
    reported 92.83 % accuracy. The two object detection models PlantCountNet and InsectNet
    reported mean average precision of 0.69 and 0.02 for the test images. Predictions
    from the DCNN models were transmitted to the ThingSpeak IoT platform for visualization
    and analytics. We concluded that AICropCAM successfully implemented image processing
    on the edge, drastically reduced the amount of data being transmitted, and could
    satisfy the real-time need for decision-making in PA. AICropCAM can be deployed
    on moving platforms such as center pivots or drones to increase its spatial coverage
    and resolution to support crop monitoring and field operations. Graphical abstract
    Download : Download high-res image (227KB) Download : Download full-size image
    Previous article in issue Next article in issue Keywords Artificial intelligenceComputer
    visionEdge computingInternet of thingsLoRaWANPrecision agriculture 1. Introduction
    The demands for food, feed, fiber, and fuel increase rapidly due to the fast expansion
    of the global population, income growth, technological advancement, and transport
    and logistics improvements (van Dijk et al., 2021). Precision agriculture (PA),
    which seeks to apply the right amount of inputs (fertilizers, irrigation water,
    pesticides, and other chemicals) in the right location at the right time, is essential
    to meet the requirements of future global food production, as well as environmental
    sustainability and climate resilience. PA is predicated on accurate sensor measurements,
    timely and sound decision-making, and automated actuators. The backbone of PA
    is the Internet of Things (IoT) technology that automates data collection, data
    analytics, data presentation, control, and efficient data communication (Chamara
    et al., 2022). Imaging sensors or digital cameras are essential for PA as they
    can capture more information than traditional scalar or vector sensors. Images
    can capture crop phenology for precise decision-making (Taylor and Browning, 2022,
    Tian et al., 2020). Cyclic events such as vegetative growth, flowering, leaf count
    and color change, maturation, and senescence are studied in crop phenology, which
    is essential to PA as it determines the management inputs required by crops. Moreover,
    images have rich information on the scene that allows for pest pressure evaluation.
    At present, a limited number of sensors are available for pest identification
    and pest pressure estimation. Among them, imaging sensors provide the most promising
    solution. Conventional (handcrafted feature extraction) and Artificial Intelligence
    (AI)-based image processing are the two branches of image processing. Traditional
    approaches extract image features defined by shape, texture, and color (Anubha
    et al., 2019, Yuan et al., 2019). The AI-based methods use Convolutional Neural
    Networks (CNN) to extract features from images (Luis et al., 2020). CNN models
    with multiple hidden layers for feature extraction and learning are considered
    Deep Convolutional Neural Networks (DCNN) (LeCun et al., 1998). Conventional imaging
    platforms in PA store images locally using onboard storage memories. Post processing
    refers to the processing of images stored at the central data storage in batches
    at a later time to extract useful information (Aasen et al., 2020). Imaging platforms
    that can access the internet through a stable connection with high bandwidth can
    automatically upload images to Cloud data storage. The vast majority of farmlands
    worldwide are in rural and remote areas with poor access to electric power and
    internet connectivity. This represents a big challenge for camera systems deployed
    in rural farmlands for high-speed image processing, data transmission, and low-latency
    decision-making (Richardson, 2019). Post-processing of crop images has been used
    for the estimation of leaf area index (Aasen et al., 2020), growth rate (Sakamoto
    et al., 2012), leaf chlorophyll and nitrogen content (Wang et al., 2014), fruit
    counts (Wang et al., 2014), and plant height (Sritarapipat et al., 2014). Further
    post-image processing allows for the assessment of biotic stress, such as pest
    density (Barbedo, 2014; Park et al., 2007) and weed pressure (Wang et al., 2019),
    as well as abiotic stress, such as nutrient deficiency (Ghorai et al., 2021).
    Richardson (2019) suggested that deep learning-based methods have the potential
    to facilitate the extraction of more sophisticated phenological data from both
    new and previously archived camera imagery compared to conventional image processing.
    Semantic segmentation-based canopy coverage (CC) estimation (Chamara et al., 2021;
    Liang et al., 2023), image classification-based crop identification (Anubha et
    al., 2019), disease identification (Sharma et al., 2020), growth stage prediction
    (Yasrab et al., 2021) and object detection-based plant feature identification
    (A. Wang et al., 2019) are examples of DCNN applications in agriculture. Conventional
    image processing requires less computational power and less energy, but they are
    limited in adaption to new scenarios, while deep learning requires high computational
    power and consumes more energy. DCNN models require large memory due to the large
    number of parameters these models hold. Therefore, it is not easy to implement
    these models practically in embedded systems that have less memory and computation
    power. These models also require a large amount of data to train to predict with
    high accuracy. Therefore, it is resource intensive. Edge image processing is the
    image processing done on image-capturing devices. The main advantage of edge image
    computing is that it lowers the high throughput data transmission requirement
    over a wireless IoT-enabled imaging network (Cao et al., 2020). Wang et al. (2022a)
    demonstrated the capability of identifying potted flowers with precision above
    89 % in real-time in a Jetson TX 2 computing module based on a DCNN algorithm.
    These authors suggested that a cloud-edge collaborative framework could achieve
    real-time and automatic learning for the DCNN model they have developed. Wang
    et al. (2022b) proposed a real-time weed detection model run on Jetson AGX Xavier
    for field robots. The authors proved it was possible to do real-time weed detection
    with a precision above 90 % yet required expensive hardware. Wang et al. (2022a)
    reviewed Raspberry Pi single-board computer-based real-time image processing applications.
    They concluded that Raspberry Pi (Datasheet Raspberry Pi Model B, 2019) is a cost-effective
    edge computing unit that could potentially be used as an edge image processing
    unit, and the capability of integrating it with IoT was also discussed. Zualkernan
    et al. (2022) demonstrated an edge image processing platform for the classification
    of animals and transmitting the identified animal and time of identification via
    LoRa for a camera trap. Past literature on IoT and image processing applications
    in agriculture has highlighted a research gap in edge image processing with IoT-enabled
    crop monitoring cameras. In-field crop cameras are expected to make real-time
    crop management decisions based on real-time image processing; however, poor internet
    connectivity in agricultural fields severely limits their capability. To address
    this gap, we have developed a novel imaging platform named AICropCAM that extracts
    plant and crop canopy level parameters through DCNN and uploads them to the Cloud
    via low-power, low-throughput communication protocols. We also demonstrated AICropCAM
    on an IoT-enable wireless sensor network in corn and soybean fields. A technology
    that addresses image processing at the lowest level (edge) and transmits only
    useful information can revolutionize real-time decision-making in PA. Therefore,
    the main objective of this paper is to demonstrate AICropCAM to perform edge image
    processing and low-throughput, low-power, and long-range data transmission through
    IoT technology. In this novel AICropCAM platform, multiple DCNN image processing
    algorithms run in series to extract plant-level and canopy-level features in an
    embedded system. Image classification, object detection with classification, and
    image segmentation are the three major applications of DCNN image processing,
    and all three are included in AICropCAM to demonstrate the capabilities of DCNN
    for image processing in PA. AICropCAM has trained models for canopy segmentation,
    crop classification, plant growth stage identification, plant counting, weed counting,
    and plant type identification. All the protocols that transmit data from AICropCAM
    to the Cloud were custom designed. AICropCAM sends the generated data to a cloud
    platform for logging, visualization, and analysis. Furthermore, this paper explains
    the DCNN model training process, model performance, and test results. We reported
    the model training comprehensively because it was essential for AICropCAM development.
    2. Materials and methods Essential activities in this research were data/image
    collection and preprocessing, hardware design for AICropCAM, software design for
    data transmission between the edge and the Cloud, deep learning model design,
    and model training and optimization (Fig. 1). AICropCAM was implemented in a corn
    and soybean field at the field phenotyping facility in Mead, Nebraska, USA (Bai
    et al., 2019). We demonstrated the training of the following DCNNs: CropClassiNet
    for classifying images based on image quality and crop type, CanopySegNet for
    segmenting crop canopy from the background, PlantCountNet for classifying and
    counting soybean and weed plants, and InsectNet for identifying insects and counting
    them. Download : Download high-res image (412KB) Download : Download full-size
    image Fig. 1. Steps of edge image processing program deployment on the embedded
    system (edge devices). 2.1. Image collection, annotation, preprocessing, and augmentation
    Image collection for DCNN model training occurred in four growing seasons using
    three different types of cameras: (i) commercially available Meidas SL122 trail
    cameras in 2019 (Meidas Trail Cameras, 2022), (ii) OV5642 imaging sensors with
    ArduCAM camera shields in 2020, and (iii) Raspberry Pi Camera Module V2 with Raspberry
    Pi Zero in 2021 and 2022 (Chamara, 2021). All the cameras were mounted on the
    bars horizontally extended and fixed on stationary poles erected vertically in
    the fields, as shown in Fig. 2A. The distance between the crop canopy and the
    cameras was maintained between 0.5 and 1.5 m throughout the growing seasons. Images
    used for training the InsectNet were also captured with smartphones as we could
    not collect enough images with insects from the three types of cameras mentioned
    above. Download : Download high-res image (338KB) Download : Download full-size
    image Fig. 2. Left: An Illustration of how AICropCAM was set up in the field for
    image collection. In addition to the camera, other components such as the solar
    panel and data logger were also shown. Right: A close-up view of AICropCAM and
    its hardware components. All three standard image annotation techniques in deep
    learning model training were utilized: (1) folder labeling for the image classification
    models, (2) pixel-level annotation for the semantic segmentation model, and (3)
    bounding boxes for object detection models. Images belonging to the same class
    were grouped into a single folder, and five distinct classes (or folders) were
    created: rejected, corn, soybean, grass, and night. Separating the crop canopy
    from the soil was done with pixel-level annotation and semantic segmentation.
    Bounding boxes, the smallest rectangle around an object, were drawn for corn plants,
    soybean plants, weed plants, and insects. Table 1 explains each type of annotation
    used in the model training. Table 1. Annotation criteria used to generate labels
    from the images to train and test the four deep convolutional neural network models
    in AICropCAM. Labeling Type Class Description Image classification (CropClassiNet)
    Rejected Images were labeled as rejected due to multiple reasons: blurred images
    caused by water droplets on the lens; the cameras turned away from the targeted
    crop; crops growing up to the camera blocking the view or capturing only a few
    leaves; people present in the images; lens covered with different stuff; and the
    camera was not installed in the field. Corn Images entirely covered by corn plants
    at different growth stages. Soybean Images entirely covered by soybean plants
    at different growth stages. Grass/Weed Images only comprise grass/weed plants
    at different growth stages. Night Images captured under low lighting conditions.
    Most of the cameras were not programmed to stop collecting images under low light.
    Crop canopy and background (CanopySegNet) Canopy Pixel labeling was done on the
    crop canopy. We used assisted freehand tool and the superpixel option in the MATLAB
    image labeler. Background Pixel labeling was done on the crop canopy. We used
    assisted freehand tool and the superpixel option in the MATLAB image labeler.
    Plant-type (PlantCountNet) Weed Weed present in the image was labeled using bounding
    boxes. It was challenging to locate the weed after the corn or soybean canopy
    was closed. Soybean Soybean plants present in the image were labeled using bounding
    boxes. Insects (InsectNet) Insects During the labeling process, without distinguishing
    insects based on their type, all the insects present in the images were labeled
    using bounding boxes. Image preprocessing is necessary for DCNN model training
    and real-time edge image processing. Differences in the input layer size in different
    DCNN models demand that images be resized before passing through the model for
    training or prediction purposes. High-resolution images improve accuracy but require
    more computational power. For specific applications, labeled datasets were only
    limitedly available. Therefore, image augmentation techniques were used to increase
    the number of image data sets, including scaling, flipping, cropping, rotation,
    color transformation, PCA color augmentation, and noise rejection (Paymode and
    Malode, 2022). Multiple augmentation techniques were used for each model, as detailed
    in Table 2. Additionally, Table 2 provides the numbers of images in training,
    validation, and testing for the four DCNN models. Table 2. DCNN model image allocation
    and image augmentation. Model Number of images Data Augmentation Techniques Total
    Training Validation Test CropClassiNet 43,611 30,528 9,810 3,273 Random rotation,
    random X  and Y reflection CanopySegNet 51 31 10 10 Transformation (random left/right
    reflection and random X/Y translation of ±10 pixels) PlantCountNet 110 88 11 11
    Transformation (same as CanopySegNet) InsectNet 542 326 108 108 Transformation
    (same as CanopySegNet) Our main objective was not to make the most accurate prediction
    for the DCNN models but to demonstrate the concept of implementing edge image
    processing and transmitting the results to the Cloud for decision-making. Therefore,
    we selected a limited number of images for CanopySegNet, PlantCountNet, and InsectNet,
    which were sufficient to train models with a reasonable degree of accuracy. 2.2.
    DCNN model architecture selection, training, evaluation, and deployment on the
    edge device The steps to select model architecture/model backbone weights and
    image input sizes to train the best model for CropClassiNet, CanopySegNet, PlantCountNet,
    and InsectNet are summarized in Fig. 3. Unlike many DCNN applications that prioritize
    higher accuracy, our application focused on finding the balance between accuracy
    and model deployability on the edge device. Download : Download high-res image
    (771KB) Download : Download full-size image Fig. 3. DCNN model selection process
    during the training and testing by attempting different model architectures, model
    backdone weights, and input image sizes. For example, in the development of CropSegNet
    (Segmentation), we selected DeepLabv3+ (Firdaus-Nawi et al., 2018) with weights
    initialized from pre-trained networks of ResNet18 (He et al., 2016), ResNet50,
    Xception, InceptionresnetV2, and MobileNetV2. The input image sizes tested were
    512 × 512 × 3 and 256 × 256 × 3, and training options were kept constant to find
    the best-performing networks, which should also be deployable to Raspberry Pi
    4B. This process identified DeepLabv3 + with ResNet50 as the most suitable model
    for CropSegNet, with an input image size of 512 × 512 × 3. Table 3 summarizes
    the hyperparameter values and training options for the final DCNN models deployed
    to the edge device. (1) (2) (3) (4) (5) (6) (7) Table 3. Hyperparameter values
    and training options for the best models (SGDM - stochastic gradient descent with
    momentum, RMSProp - Root mean square propagation). Training option and the function/Hyperparameters
    Values for CropClassiNet Values for CanopySegNet Values for InsectNet (320 × 320
    × 3) Values for PlantCountNet (320 × 320 × 3) Optimizer SGDM SGDM SGDM RMSProp
    Momentum 0.9 0.9 0.99 NA Initial learning rate 0.001 0.001 0.001 0.001 Learn rate
    schedule Piecewise Piecewise Piecewise Piecewise Learn rate drop period 10 10
    10 10 Learn rate drop factor 0.3 0.3 0.1 0.3 Minibatch size 16 4 16 32 L2Regularization
    NA 0.005 0.005 0.005 Validation frequency 3 3 3 10 Shuffle Every epoch Every epoch
    Every epoch Every epoch Validation patience 4 10 10 10 Max epochs 100 300 1000
    100 Execution environment Multi GPU Multi GPU GPU GPU The performance of the four
    DCNN models was evaluated using the indices calculated from Eq. (1), (2), (3),
    (4), (5), (6), (7). Accuracy, Precision, Recall, F1 score, and Jaccard index were
    used for the classification models CropClassiNet and CropSegNet, whereas IoU and
    mAP (Mean Average Precision) were used for PlantCountNet and InsectNet. Jaccard
    index gives the proportion of correctly predicted labels to the total number of
    labels. Model training was performed on an NVIDIA GeForce GTX 1650 Ti Mobile processor,
    a dedicated mid-range graphics card with 4 GB GDDR6 memory on a Dell XPS 15 9500
    Laptop. The laptop had an Intel Core i7-10750H 10th Gen processor,16 GB DDR4 RAM,
    and 1 TB SSD hard disk. 2.3. Hardware and software of AICropCAM The IoT data transmission
    and edge image processing hardware comprised the following major components: a
    Raspberry Pi 4B single-board computer, an Arduino MKR1310 development board, an
    Arduino MKR Relay Proto Shield, and a Dragino OLG02 outdoor dual channels LoRa
    Gateway (Fig. 4). The 12 V 8Ah battery powered the Raspberry Pi 4B, controlled
    through the relay shield managed by the Arduino MKR1310. A 3.7 V lithium polymer
    battery powered the Arduino MKR1310 board. There are two advantages of having
    a separate Arduino board. First, the Arduino board consumes less power than the
    Raspberry Pi 4B module. It can be switched on and off according to user requirements.
    Second, it allows uninterrupted communication between the edge node and the Cloud
    with low power. Download : Download high-res image (303KB) Download : Download
    full-size image Fig. 4. Hardware overview of AICropCAM and data flow. AICropCAM
    required programming on two hardware platforms. Arduino was programmed using C++
    in Arduino’s Integrated Development Environment. Raspberry Pi imaging and image
    processing program was developed in MATLAB and deployed onto the Raspberry Pi
    4B using the MATLAB Coder and MATLAB Compiler. A python program was designed to
    read the saved data in the Raspberry Pi 4B and serially communicate to the Arduino
    MKR1310. The primary functions of the MRK1310 program were to (1) turn on the
    Raspberry Pi 4B module based on the user-defined time intervals, (2) get the processed
    data, including the results of DCNN model predictions, through serial communication
    from the Raspberry Pi 4B, and (3) transmit the data to the ThingSpeak Cloud channel
    through the LoRa gateway. All the DCNN models were trained using the MATLAB deep
    learning toolbox. In the edge deployment, a MATLAB program runs multiple models
    logically depending on the prediction result of the previous model estimation,
    as shown in Fig. 5. MATLAB coder generated the C and C++ code derived from the
    program we developed to run on the Raspberry Pi. MATLAB Compiler generated the
    standalone application on the Raspberry Pi (The MathWorks, 2022). Download : Download
    high-res image (477KB) Download : Download full-size image Fig. 5. Overall sequential
    image processing and data generation flow chart. Table 4 lists the parameters
    generated by the models in AICropCAM. The abbreviations in Table 4 are fields
    holding data in the program to reduce the complexity of system development and
    maintain a common standard among different platforms. Fig. 6 shows the data generation
    from images. According to Fig. 6, the size of the images were around 2 MB before
    being fed into the image processing pipeline. The output message contains the
    crop type (CT), plant count (PC), weed count (WC), canopy coverage (CC), and pest
    count (PstC). The resulting message is typically less than 100 bytes. This represents
    a substantial reduction of memory size with the output being 0.00005 times the
    size of the original image. Consequently, this message can be transmitted in a
    single message via LoRa as the maximum LoRa packet size is around 256 bytes. Table
    4. List of parameters used to represent information in the images. Parameter Abbreviation
    Represent information Image location LOC Node ID manually entered/Global positioning
    system location coordinates Image orientation IO Accelerometer/Manually feed/Gravity
    switch Image quality/Crop type CT Image classification based on image quality
    and the crop type Plant count/Weed count PC/WC Multiclass object detection/classification
    Crop canopy coverage CC Semantic segmentation Pest count PstC Multiclass object
    detection/classification Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 6. Examples of message generation and data size reduction
    for LoRa transmission. 2.4. Data transmission, visualization, and storage The
    data generated after image processing were saved on the Raspberry Pi 4B SD card,
    allowing access to the data remotely or through manual retrieval during field
    visits. Two options for transmitting the collected data to the ThingSpeak IoT
    platform are available. Firstly, the data can be uploaded directly from the Raspberry
    Pi 4B if internet connectivity is available for growers with Wi-Fi access. Secondly,
    the Raspberry Pi 4B transmits the recently acquired data to the Arduino MKR1310.
    The Arduino MKR1310 decodes the data received from the Raspberry Pi 4B and forwards
    it to the ThingSpeak. The second method is for low-rate, long-range communication
    beyond the limit of Wi-Fi. A single message receivable to the ThingSpeak server
    includes data for eight fields. In our demonstration, a single message was enough
    to transmit the data generated. Fields 1 and 2 are reserved for geographic coordinates
    (namely, latitude and longitude) to represent the device''s location. The third
    field was for camera orientation. Image quality/crop type, plant count, weed count,
    insect count, and crop canopy coverage were allocated from fields four to eight.
    ThingSpeak supports eight channels per gateway. If additional data is generated
    in the future, we have to create new channels to accommodate them. However, only
    data in a single channel can be passed through a single message. The Arduino-LoRa
    library was used to prepare the LoRa messages forwarded to the gateway (Mistry,
    2016). The message generated from the Arduino MKR1310 includes the device identification
    number and the data with the field number. Once the gateway receives this message,
    it adds the target client ID (generated by ThingSpeak when defining a device),
    host address (mqtt://mqtt3.thingspeak.com), server port number, username and password,
    channel ID, and the data in each field according to the Message Queuing Telemetry
    Transport (MQTT) protocol. Username and password ensure that only authorized devices
    can transmit data to the ThingSpeak platform. ThingSpeak provides two ways to
    interact with its platform, REST (Representative State Transfer) and MQTT protocols.
    The advantages of using MQTT over REST protocol are that it supports ThingSpeak
    data publishing, including immediate and minimum power consumption and data transmission
    over limited bandwidth, which encouraged us to select the MQTT protocol in our
    demonstration. 3. Results and discussion 3.1. DCNN model performance CropClassiNet
    had a test accuracy of 91.26 %, a Jaccard Index of 0.77, and an F1-score of 0.91;
    the confusion matrix is given in Fig. 7. The highest precision is for the “grass”
    class (100 %), and the lowest is for “soybean” (92.0 %). The highest recall is
    for the “corn” class (99.9 %), and the lowest is for “grass” (67.1 %). The primary
    goal of CropClassiNet is to determine the quality of new images and direct them
    for subsequent processing (Fig. 5). This step has never been executed in an image-based
    crop monitoring platform before. Further, CropClassiNet can eliminate erroneous
    images when humans are present in the camera’s field of view or when the camera
    is misaligned due to external factors. AICropCAM can send maintenance requests
    through IoT analytics if rejected images are continuously generated. Download
    : Download high-res image (275KB) Download : Download full-size image Fig. 7.
    Confusion matrix for test images by CropClassiNet. CanopySegNet on the test images
    achieved a global accuracy of 0.93, a weighted IoU of 0.87, and a mean BF score
    of 0.73. Fig. 8 shows an example of an original soybean image and the corresponding
    segmentation result by CanopySegNet, which estimated CC to be 18.72 %. Season-long,
    time-series images can be fed into CanopySegNet to generate diurnal and seasonal
    curves of crop CC, as shown in Fig. 9. Download : Download high-res image (621KB)
    Download : Download full-size image Fig. 8. An image of soybean crop and the segmentation
    result by CropSegNet to calculate canopy coverage. Download : Download high-res
    image (367KB) Download : Download full-size image Fig. 9. Examples of diurnal
    and seasonal variations of canopy coverage as computed by CropSegNet. According
    to Fig. 9, canopy coverage percentage variation is low during the daytime and
    reaches zero at night. This verifies the need to eliminate low-light images before
    segmenting. As shown in Fig. 5, it is possible to eliminate the generation of
    false values when the camera captures images under low light conditions by halting
    the process of running CanopySegNet. There are three diurnal variation series
    on 6/8/2021, 6/26/2021, and 7/12/2021 in Fig. 9. The CC increased from 8 % to
    95 % between 6/8/2021 to 7/12/2021. The seasonal trend showed that the CC reached
    a maximum around 7/8/2021. These results suggest that the proposed stacked models
    can track the daily and seasonal CC variation and eliminate the effect of lighting
    conditions on false value generation. Table 5. Performance of PlantCountNet and
    InsectNet on the test image set (Root mean square error (RMSE)/Final validation
    loss (FVL)). Model Name Architecture Input size Validation RMSE/FVL Mean average
    precision Object class PlantCountNet YOLOv2 320 × 320 × 3 0.888 (RMSE) 0.66 Soybean
    0.86 Weed InsectNet YOLOv4 320 × 320 × 3 26.2 (FVL) 0.02 Insect The overall performance
    of the PlantCountNet and InsectNet is given in Table 5. Fig. 10(A) and 10(B) show
    the result obtained by PlantCountNet for a soybean image at an early vegetative
    stage (V3). Meanwhile Fig. 10(C) and 10(D) shows the result at a reproductive
    stage (R1). It can be seen that, at V3 stage, the model outputs matched the labels
    of soybean and weed plants well, indicating a level of high accuracy. Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 10. The
    result of PlantCountNet for soybean and weed counting: Manually annotated vs.
    model-predicted bounding boxes at V3 growth stage (A and B); manually annotated
    vs. model-predicted bounding boxes at R1 growth stage (C and D). The size of insects
    is very small compared to the size of images (Fig. 11), which is the main reason
    for the low mAP for InsectNet (Table 5). Increasing input image resolution beyond
    480 × 480 × 3 is impractical as it exceeds the memory limitation to load models
    into Raspberry Pi 4B. A potential solution could be to increase the resolution
    of the region of interest by splitting the original image while keeping the resolution
    the same. Also, we suggest using the approach recommended by Tetila et al., 2020a,
    Tetila et al., 2020b in the future on Raspberry Pi model 4B. As technology advances,
    we expect the memory capacities will increase for edge computing units. At the
    same time, the state-of-the-art object detection algorithms will improve the accuracy
    for small object detection. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 11. The result of InsectNet for insect counting
    in soybean. The top row shows a situation of high false positives and low false
    negatives: (A) and (B) are manually annotated and model-predicted insect labels,
    respectively. The bottom row shows a situation of low false positive and high
    false negative: (C) and (D) are manually annotated and model-predicted insect
    labels. 3.2. Power consumption for Raspberry Pi 4B Since edge cameras in farmlands
    have limited access to electric power, information on their power consumption
    is essential for designing IoT devices and systems. AICropCAM is designed to be
    energized by solar power. It runs on a rechargeable battery when there is no solar
    power. We monitored the maximum energy consumption of each task performed by AICropCAM,
    and the result is presented in Table 6. Four main strategies are available for
    the power management of IoT edge devices: Selecting power-efficient hardware,
    maintaining low power modes, dynamic power management, and cloud-based management.
    Raspberry Pi 4B is an affordable power-efficient single-board computer suitable
    for our application, but it does not naturally support low-power modes. Therefore,
    we introduced the Arduino MKR1310 LoRa module for the Raspberry Pi 4B dynamic
    power management. Furthermore, this Arduino module allows us to perform cloud-based
    central management independently. Table 6. Electrical power consumption of the
    Raspberry Pi 4B and the Arduino MKR1310 during edge image processing. Device Activity
    The maximum current range and the voltage recorded Raspberry Pi 4B Idle run 5.25
    V × (0.45 – 0.53) A Image classification 5.25 V × (0.97 – 1.04) A Image segmentation
    5.25 V × (0.98 – 1.11) A Weed and plant detection 5.25 V × (0.62 – 0.70) A Insect
    detection 5.25 V × (0.62 – 0.70) A Arduino MKR1310 Sleep mode <0.01A Serial communication
    <0.01A LoRa transmission <0.01A For our measurements, we used a Raspberry Pi 4B
    with 8 GB of RAM, connected to an HDMI monitor, a USB keyboard, and a USB mouse,
    and ran a MathWorks® Raspbian image (file used to boot the Raspberry Pi 4B). The
    Raspberry Pi 4B was operated at room temperature and connected to a wireless LAN
    access point and a laptop via an Ethernet cable. The electric current consumption
    for running each DCNN model was recorded during the test. CropClassiNet had the
    highest current consumption, while the PlantCountNet and InsectNet models had
    the lowest. As for LoRa transmission, we could not measure its current consumption
    because the lowest value our instrument could measure was 0.01A. Based on the
    manufacturer''s specifications, the Arduino MKR1310 consumes 104 uA at 5 V. The
    average time to run the DCNN models is essential to estimate the energy consumed
    for each prediction. These parameters listed in Table 7 provide essential guidelines
    for designing IoT sensor nodes with suitable batteries and power sources. We also
    noticed that typically the first prediction of a model took the longest time,
    but the rest take a considerably shorter time to predict. Table 7. Time duration
    needed for the selected DCNN models deployed in the Raspberry Pi 4B. Model/Task
    Input image size Time for predicting results (s) The maximum power demand for
    the activity (W) CropClassiNet/Image quality evaluation and crop classification
    224 × 224 × 3 6.44 5.46 CanopySegNet/Semantic segmentation to separate canopy
    from background 512 × 512 × 3 20.20 5.83 PlantCountNet/Weed and plant detection,
    classification, and counting 320 × 320 × 3 14.38 3.68 InsectNet/Insect detection
    320 × 320 × 3 0.20 3.68 Semantic segmentation was the most power-demanding activity,
    while insect detection was the least. Changing the order of the image processing
    models and adding new models or dropping existing models is possible during regular
    operation. It enables dynamic power management within the Raspberry Pi module.
    The main advantage of AICropCAM is that it implements a stack of four DCNN-based
    image processing models with multiple objectives. To the best of our knowledge,
    this is the first time such a system has been developed for a field crop monitoring
    camera. AICropCAM has applications such as setting up smart in-field or greenhouse
    IoT camera networks with edge computing capability, monitoring crops by attaching
    them to sprinkler irrigation systems (pivots and linear moves), or collecting
    crop information through ground or aerial mobile robots. The relatively short
    time to run each DCNN model makes the system suitable for real-time applications,
    including variable rate irrigation, fertilization, and spraying. For example,
    a pivot irrigated multi-cropping system with AICropCAM can automate irrigation
    or fertigation transition between different crops or crops at different growth
    stages by automatically providing the crop type or growth stage information to
    the irrigation controller. Additionally, existing herbicide or pesticide sprayers
    can get the feedback of the PlantCountNet and InsectNet in the AICropCAM for precision
    spraying. 4. Conclusion and future perspectives This paper outlines the essential
    components of constructing a functional edge image processing framework for real-time
    crop monitoring. From a software standpoint, CropClassiNet can categorize captured
    images according to image quality and detect the presence of specific crop types
    for further processing. CanopySegNet can further quantify the degree of canopy
    coverage; PlantCountNet can count the number of plants and weeds in the image;
    and finally, InsectNet can count the number of insects in the image. These four
    DCNN models, when implemented on edge devices, can extract an array of important
    crop and canopy parameters from field images and enable real-time, low-latency
    decision making and applications. Deep learning-based image processing on the
    edge has excellent potential in PA. Applications of AICropCAM are not limited
    to image classification, segmentation, plant counting, or weed counting. Potential
    future applications include insect classification and crop damage estimation,
    weed classification and pressure estimation, fruit identification and yield estimation,
    decision on replanting (Whigham et al., 2000), and disease identification and
    disease damage estimation in real time using actual field images collected by
    AICropCAM. AICropCAM shows excellent potential in enhancing crop management through
    crop monitoring. However, the current demonstration requires significant improvements
    on both hardware and software fronts. Customized circuitry and modular design
    are required to put AICropCAM in commercial farm applications. The full potential
    of the AICropCAM can be achieved by putting this camera on a moving platform like
    a center pivot with a GPS receiver to generate spatiotemporal data. Crop classification
    must include more crop types, and segmentation models need training data from
    other crop types. The DCNN models for weed and insect identification require the
    capability to identify different weed types, their growth stage, different insect
    types, and their growth stages to generate effective pest control decisions. Additionally,
    improving the models’ accuracy in image classification, segmentation, and object
    detection is crucial. It can be achieved by increasing the number of training
    image data sets. We also planned to expand the research for multiple edge architecture
    evaluation. Architectures such as a high-performance edge computer that accepts
    images from multiple edge devices through short-range, high-speed communication
    (e.g., Wi-Fi) and can run more accurate deep learning models with higher numbers
    of parameters, might be a better solution for the primary objectives addressed
    in this paper. We aim to expand the AICropCAM applications to other crops beyond
    corn and soybean. By making these improvements, AICropCAM will become a more effective
    tool for crop management, potentially revolutionizing how we grow and manage crops.
    Funding This work was supported by the United States Department of Agriculture
    – National Institute of Food and Agriculture grants [Award 2020-68013-32371 to
    YG and GB, Award 2021-67021-34417 to YG]. CRediT authorship contribution statement
    Nipuna Chamara: Methodology, Software, Visualization. Geng Bai: Conceptualization,
    Methodology, Resources. Yufeng Ge: Conceptualization, Resources, Supervision,
    Project administration, Funding acquisition. Declaration of Competing Interest
    The authors declare the following financial interests/personal relationships which
    may be considered as potential competing interests: Nipuna Chamara, Yufeng Ge,
    Geng Bai has patent pending to University of Nebraska-Lincoln. Acknowledgements
    Jianxin Sun assisted in developing the imaging device with Raspberry Pi Zero used
    for image acquisition. David Scoby helped the field management and AICropCAM installation.
    Junxiao Zhang supported the field installation of AICropCAM and smart-phone based
    acquisition of crop images with insects. Data availability Data will be made available
    on request. References Aasen et al., 2020 H. Aasen, N. Kirchgessner, A. Walter,
    F. Liebisch PhenoCams for field phenotyping: using very high temporal resolution
    digital repeated photography to investigate interactions of growth, phenology,
    and harvest traits Front. Plant Sci., 11 (June) (2020), pp. 1-16, 10.3389/fpls.2020.00593
    Google Scholar Anubha et al., 2019 P.S. Anubha, V. Sathiesh Kumar, S. Harini A
    study on plant recognition using conventional image processing and deep learning
    approaches J. Intell. Fuzzy Syst., 36 (3) (2019), pp. 1997-2004, 10.3233/JIFS-169911
    Google Scholar ArduCAM, 2016 ArduCAM ESP8266 UNO board User Guide (pp. 0–9). (2016).
    www.ArduCAM.com. Google Scholar Bai et al., 2019 G. Bai, Y. Ge, D. Scoby, B. Leavitt,
    V. Stoerger, N. Kirchgessner, S. Irmak, G. Graef, J. Schnable, T. Awada NU-Spidercam:
    A large-scale, cable-driven, integrated sensing and robotic system for advanced
    phenotyping, remote sensing, and agronomic research Comput. Electron. Agric.,
    160 (March) (2019), pp. 71-81, 10.1016/j.compag.2019.03.009 View PDFView articleView
    in ScopusGoogle Scholar Barbedo, 2014 J.G.A. Barbedo Using digital image processing
    for counting whiteflies on soybean leaves J. Asia Pac. Entomol., 17 (4) (2014),
    pp. 685-694, 10.1016/j.aspen.2014.06.014 View PDFView articleView in ScopusGoogle
    Scholar Cao et al., 2020 K. Cao, Y. Liu, G. Meng, Q. Sun An Overview on Edge Computing
    Research IEEE Access, 8 (2020), pp. 85714-85728, 10.1109/ACCESS.2020.2991734 View
    in ScopusGoogle Scholar Chamara et al., 2021 N. Chamara, K. Alkhadi, H. Jin, F.
    Bai, A. Samal, Y. Ge A deep convolutional neural network based image processing
    framework for monitoring the growth of soybean crops. 2021 ASABE Annual International
    Meeting, 2100259 (2021), 10.13031/aim.202100259 Google Scholar Chamara et al.,
    2022 N. Chamara, M.D. Islam, G.F. Bai, Y. Shi, Y. Ge Ag-IoT for crop and environment
    monitoring: Past, present, and future Agr. Syst., 203, 103497 (2022), 10.1016/j.agsy.2022.103497
    Google Scholar Chamara, 2021 N. Chamara Development of an Internet of Things (IoT)
    Enabled Novel Wireless Multi-Sensor Network for Infield Crop Monitoring. Master’s
    Thesis, Department of Biological Systems Engineering, University of Nebraska-Lincoln
    (2021) Google Scholar Datasheet Raspberry Pi Model, 2019 Datasheet Raspberry Pi
    Model B, 2019. https://datasheets.raspberrypi.org. Accessed 11 November 2023.
    Google Scholar Firdaus-Nawi et al., 2018 Firdaus-Nawi, M., Noraini, O., Sabri,
    M.Y., Siti-Zahrah, A., Zamri-Saad, M., Latifah, H., 2018. DeepLabv3+_Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation. In: Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 801–818. Google Scholar
    Ghorai et al., 2021 A.K. Ghorai, A.R. Barman, B. Chandra, K. Viswavidyalaya, S.
    Jash, B. Chandra, K. Viswavidyalaya, B. Chandra, K. Viswavidyalaya Image processing
    based detection of diseases and nutrient deficiencies in plants SATSA Mukhapatra,
    25 (1) (2021), pp. 1-24 Google Scholar He et al., 2016 He, K., Zhang, X., Ren,
    S., Sun, J., 2016. Deep residual learning for image recognition kaiming. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778.
    doi: 10.1002/chin.200650130. Google Scholar LeCun et al., 1998 LeCun, Y., Bottou,
    L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document
    recognition. Proc. IEEE 86(11), 2278–2323. doi: 10.1109/5.726791. Google Scholar
    Liang et al., 2023 Liang, W. Z., Oboamah, J., Qiao, X., Ge, Y., Harveson, B.,
    Rudnick, D. R., Wang, J., Yang, H., Gradiz, A., 2023. CanopyCAM – an edge-computing
    sensing unit for continuous measurement of canopy cover percentage of dry edible
    beans. Comput. Electron. Agric. 204 (January), 107498. https://doi.org/10.1016/j.compag.2022.107498.
    Google Scholar Luis et al., 2020 Luis, S., Filipe, N.S., Paulo, M.O., Pranjali,
    S., 2020. Deep Learning applications in agriculture: a short review. Deep Learning
    Applications in Agriculture: A Short Review, 1092 AISC(January), C1. doi: 10.1007/978-3-030-35990-4.
    Google Scholar Meidas Trail Cameras, 2022 Meidas Trail Cameras, 2022. https://www.meidase.com/product-category/trail-cameras/.
    Accessed 11 November 2023. Google Scholar Mistry, 2016 Mistry, S., 2016. Arduino
    LoRa. MIT License. https://github.com/sandeepmistry/arduino-LoRa. Accessed 11
    November 2023. Google Scholar Park et al., 2007 Y. Park, R.K. Krell, M. Carroll
    Theory, technology, and practice of site-specific insect pest management J. Asia
    Pac. Entomol., 10 (2) (2007), pp. 89-101 View PDFView articleView in ScopusGoogle
    Scholar Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for
    multi-crop leaf disease image classification using convolutional neural network
    VGG Artif. Intell. Agric., 6 (2022), pp. 23-33, 10.1016/j.aiia.2021.12.002 View
    PDFView articleView in ScopusGoogle Scholar Richardson, 2019 A.D. Richardson Tracking
    seasonal rhythms of plants in diverse ecosystems with digital camera imagery New
    Phytol., 222 (4) (2019), pp. 1742-1750, 10.1111/nph.15591 View in ScopusGoogle
    Scholar Sakamoto et al., 2012 T. Sakamoto, A.A. Gitelson, A.L. Nguy-Robertson,
    T.J. Arkebauer, B.D. Wardlow, A.E. Suyker, S.B. Verma, M. Shibayama An alternative
    method using digital cameras for continuous monitoring of crop status Agric. For.
    Meteorol., 154–155 (2012), p. 113, 10.1016/j.agrformet.2011.10.014 View PDFView
    articleView in ScopusGoogle Scholar Sharma et al., 2020 P. Sharma, Y.P.S. Berwal,
    W. Ghai Performance analysis of deep learning CNN models for disease detection
    in plants using image segmentation Inf. Process. Agric., 7 (4) (2020), pp. 566-574,
    10.1016/j.inpa.2019.11.001 View PDFView articleView in ScopusGoogle Scholar Sritarapipat
    et al., 2014 T. Sritarapipat, P. Rakwatin, T. Kasetkasem Automatic rice crop height
    measurement using a field server and digital image processing Sensors (Switzerland),
    14 (1) (2014), pp. 900-926, 10.3390/s140100900 View in ScopusGoogle Scholar Taylor
    and Browning, 2022 S.D. Taylor, D.M. Browning Classification of daily crop phenology
    in phenocams using deep learning and hidden markov models Remote Sens. (Basel),
    14 (2) (2022), pp. 1-22, 10.3390/rs14020286 Google Scholar Tetila et al., 2020a
    Tetila, E.C., Machado, B.B., Astolfi, G., Belete, N.A.S., Amorim, W.P., Roel,
    A.R., Pistori, H., 2020. Detection and classification of soybean pests using deep
    learning with UAV images. Computers and Electronics in Agriculture, 179(May).
    doi: 10.1016/j.compag.2020.105836. Google Scholar Tetila et al., 2020b E.C. Tetila,
    B.B. MacHado, G.V. Menezes, N.A. De Souza Belete, G. Astolfi, H. Pistori A deep-learning
    approach for automatic counting of soybean insect pests IEEE Geosci. Remote Sens.
    Lett., 17 (10) (2020), pp. 1837-1841, 10.1109/LGRS.2019.2954735 View in ScopusGoogle
    Scholar The MathWorks, 2022 The MathWorks, I., 2022. MATLAB Coder - MATLAB. MathWorks.
    https://www.mathworks.com/products/matlab-coder.html. Google Scholar Tian et al.,
    2020 H. Tian, T. Wang, Y. Liu, X. Qiao, Y. Li Computer vision technology in agricultural
    automation—a review Inf. Process. Agric., 7 (1) (2020), pp. 1-19, 10.1016/j.inpa.2019.09.006
    View PDFView articleView in ScopusGoogle Scholar van Dijk et al., 2021 M. van
    Dijk, T. Morley, M.L. Rau, Y. Saghai A meta-analysis of projected global food
    demand and population at risk of hunger for the period 2010–2050 Nat. Food, 2
    (7) (2021), pp. 494-501, 10.1038/s43016-021-00322-9 View in ScopusGoogle Scholar
    Wang et al., 2022b Q. Wang, M. Cheng, S. Huang, Z. Cai, J. Zhang, H. Yuan A deep
    learning approach incorporating YOLO v5 and attention mechanisms for field real-time
    detection of the invasive weed Solanum rostratum Dunal seedlings Comput. Electron.
    Agric., 199 (July) (2022), Article 107194, 10.1016/j.compag.2022.107194 View PDFView
    articleView in ScopusGoogle Scholar Wang et al., 2022a J. Wang, Z. Gao, Y. Zhang,
    J. Zhou, J. Wu, P. Li Real-time detection and location of potted flowers based
    on a ZED camera and a YOLO V4-tiny deep learning algorithm Horticulturae, 8 (1)
    (2022), 10.3390/horticulturae8010021 Google Scholar Wang et al., 2014 Y. Wang,
    D. Wang, P. Shi, K. Omasa Estimating rice chlorophyll content and leaf nitrogen
    concentration with a digital still color camera under natural light Plant Methods,
    10 (3) (2014), pp. 273-286, 10.1016/S0378-4290(99)00063-5 View in ScopusGoogle
    Scholar Wang et al., 2019 A. Wang, W. Zhang, X. Wei A review on weed detection
    using ground-based machine vision and image processing techniques Comput. Electron.
    Agric., 158 (January) (2019), pp. 226-240, 10.1016/j.compag.2019.02.005 View PDFView
    articleView in ScopusGoogle Scholar Whigham et al., 2000 K. Whigham, D. Farnham,
    J. Lundvall, D. Tranel Soybean replant decision, Department of Agronomy, Iowa
    State University (2000) Google Scholar Yasrab et al., 2021 R. Yasrab, J. Zhang,
    P. Smyth, M.P. Pound Predicting plant growth from time-series data using deep
    learning Remote Sens. (Basel), 13 (3) (2021), pp. 1-17, 10.3390/rs13030331 View
    in ScopusGoogle Scholar Yuan et al., 2019 W. Yuan, N.K. Wijewardane, S. Jenkins,
    G. Bai, Y. Ge, G.L. Graef Early prediction of soybean traits through color and
    texture features of canopy RGB imagery Sci. Rep., 9 (2019), p. 14089, 10.1038/s41598-019-50480-x
    View in ScopusGoogle Scholar Zualkernan et al., 2022 I. Zualkernan, S. Dhou, J.
    Judas, A.R. Sajun, B.R. Gomez, L.A. Hussain An IoT system using deep learning
    to classify camera trap images on the edge Computers, 11 (1) (2022), pp. 1-24,
    10.3390/computers11010013 Google Scholar Cited by (1) YOLO performance analysis
    for real-time detection of soybean pests 2024, Smart Agricultural Technology Show
    abstract © 2023 The Authors. Published by Elsevier B.V. Part of special issue
    Agricultural Cybernetics: A New Methodology of Analysis and Development for Modern
    Agricultural Production Systems Edited by Yanbo Huang, Manoj Karkee, Lie Tang,
    Dong Chen View special issue Recommended articles Joint control method based on
    speed and slip rate switching in plowing operation of wheeled electric tractor
    equipped with sliding battery pack Computers and Electronics in Agriculture, Volume
    215, 2023, Article 108426 Qi Wang, …, Yongjie Cui View PDF LSCA-net: A lightweight
    spectral convolution attention network for hyperspectral image processing Computers
    and Electronics in Agriculture, Volume 215, 2023, Article 108382 Ziru Yu, Wei
    Cui View PDF Automatic detection of crop lodging from multitemporal satellite
    data based on the isolation forest algorithm Computers and Electronics in Agriculture,
    Volume 215, 2023, Article 108415 Rui Guo, …, Tingting Liu View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 1 Captures Readers: 19 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Harsh P.
  - Hachinger S.
  - Derquennes M.
  - Edmonds A.
  - Karagoz P.
  - Golasowski M.
  - Hayek M.
  - Martinovič J.
  citation_count: '0'
  description: 'In this contribution, we sketch an application of Earth System Sciences
    and Cloud-/Big-Data-based IT, which shall soon leverage European supercomputing
    facilities: smart viticulture, as put into practice by Terraview. TerraviewOS
    is a smart vineyard “operating system”, allowing wine cultivators to optimise
    irrigation, harvesting dates and measures against plant diseases. The system relies
    on satellite and drone imagery as well as in-situ sensors where available. Clearly,
    processing behind the UI is heavily based on Cloud Computing, with some Edge Computing
    close to the user or High-Performance/GPU Computing components. The substantial
    need for computing power in TerraviewOS, in particular for training AI-based models
    to generate derived data products, makes the further development of some of its
    modules a prime use case for the EU-funded Extreme Data processing project “EXA4MIND”
    (Horizon Europe GA No. 101092944). Two of the strongest academic supercomputing
    centres in Europe take part in EXA4MIND. The collaboration to evolve the “Smart
    Moisture Mapper” subsystem of TerraviewOS in this context is briefly sketched.
    Connecting database systems, High-Performance-/Cloud-Computing systems and European
    Data Spaces with appropriate data access and transfer mechanisms, EXA4MIND shall
    demonstrate competitive advantage in scientific and enterprise data analysis needed
    in complex applications such as TerraviewOS.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Proceedings of Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Wine in the Cloud, or: Smart Vineyards with a Distributed “Extreme Data
    Database” and Supercomputing'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ma X.
  - Zhou A.
  - Zhang S.
  - Li Q.
  - Liu A.X.
  - Wang S.
  citation_count: '10'
  description: The cloud-assisted mobile edge computing system is a critical architecture
    to process computation-intensive and delay-sensitive mobile applications in close
    proximity to mobile users with high resource efficiency. Due to the heterogenous
    dynamics of task arrivals at edge nodes and the distributed nature of the system,
    the workloads of edge nodes are prone to be unbalanced, which can cause high task
    response time and resource cost. This paper solves the dynamic task scheduling
    problem in cloud-assisted mobile edge computing (including both peer task scheduling
    among edge nodes and cross-layer task scheduling from edge nodes to the cloud),
    aiming at minimizing average task response time within resource budget limit.
    To overcome the challenges of task arrival dynamics, edge node heterogeneity,
    and computation-communication delay tradeoff, we propose a Water-filling Based
    Dynamic Task Scheduling (WiDaS) algorithm. WiDaS dynamically tunes the usage of
    cloud resources based on the Lyapunov optimization method and efficiently schedules
    mobile tasks among edge nodes (and the cloud) by exploiting the idea of water
    filling. Extensive simulations are conducted to evaluate WiDaS under a trace-driven
    traffic pattern and two mathematic traffic patterns. The results demonstrate that
    WiDaS shows two-fold benefits of efficiency and effectiveness. In terms of efficiency,
    WiDaS can achieve the approximate results with the KKT-based algorithm while reducing
    the computation complexity from exponential order to polynomial order. In terms
    of effectiveness, WiDaS can reduce the average task response time by up to 64.4%
    and 47.2% over the Fair-ratio and the Edge-first algorithm.
  doi: 10.1109/TMC.2021.3115262
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Mobile C...
    >Volume: 22 Issue: 4 Dynamic Task Scheduling in Cloud-Assisted Mobile Edge Computing
    Publisher: IEEE Cite This PDF Xiao Ma; Ao Zhou; Shan Zhang; Qing Li; Alex X. Liu;
    Shangguang Wang All Authors 11 Cites in Papers 1800 Full Text Views Abstract Document
    Sections 1 Introduction 2 Related Work 3 System Model and Problem Formulation
    4 Problem Transformation and Performance Analysis 5 The WiDaS Algorithm Design
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    The cloud-assisted mobile edge computing system is a critical architecture to
    process computation-intensive and delay-sensitive mobile applications in close
    proximity to mobile users with high resource efficiency. Due to the heterogenous
    dynamics of task arrivals at edge nodes and the distributed nature of the system,
    the workloads of edge nodes are prone to be unbalanced, which can cause high task
    response time and resource cost. This paper solves the dynamic task scheduling
    problem in cloud-assisted mobile edge computing (including both peer task scheduling
    among edge nodes and cross-layer task scheduling from edge nodes to the cloud),
    aiming at minimizing average task response time within resource budget limit.
    To overcome the challenges of task arrival dynamics, edge node heterogeneity,
    and computation-communication delay tradeoff, we propose a W ater-f i lling Based
    D ynamic T a sk S cheduling (WiDaS) algorithm. WiDaS dynamically tunes the usage
    of cloud resources based on the Lyapunov optimization method and efficiently schedules
    mobile tasks among edge nodes (and the cloud) by exploiting the idea of water
    filling. Extensive simulations are conducted to evaluate WiDaS under a trace-driven
    traffic pattern and two mathematic traffic patterns. The results demonstrate that
    WiDaS shows two-fold benefits of efficiency and effectiveness. In terms of efficiency,
    WiDaS can achieve the approximate results with the KKT-based algorithm while reducing
    the computation complexity from exponential order to polynomial order. In terms
    of effectiveness, WiDaS can reduce the average task response time by up to 64.4%
    and 47.2% over the Fair-ratio and the Edge-first algorithm. Published in: IEEE
    Transactions on Mobile Computing ( Volume: 22, Issue: 4, 01 April 2023) Page(s):
    2116 - 2130 Date of Publication: 24 September 2021 ISSN Information: DOI: 10.1109/TMC.2021.3115262
    Publisher: IEEE Funding Agency: SECTION 1 Introduction WITH the proliferation
    of computation-intensive and delay-sensitive mobile applications, mobile devices
    are required to provide enhanced computation capabilities and long-lasting battery
    life. However, the capacities of mobile devices are inherently limited due to
    its physical size. Mobile cloud computing has been proposed to augment the capacities
    of mobile devices by offloading mobile tasks to the configurable and sharable
    resource pool of cloud computing [1], [2]. Nevertheless, offloading mobile tasks
    to the remote cloud suffers from wide area network delay and jitters. Moreover,
    the demands of mobile applications are increasing much faster than the growth
    rate of core network capacity. All these limitations incent the emergence of mobile
    edge computing (MEC). By deploying powerful servers (i.e., edge nodes) within
    the wireless access network, mobile devices are able to access sufficient computation
    resources via only one-hop wireless transmission, benefiting from reduced delay
    and energy consumption [3], [4], [5]. Additionally, bandwidth-intensive tasks
    from mobile devices can be aggregated at the edge nodes, mitigating the burden
    of core network. In mobile edge computing, while the computation capacity of each
    edge node is fixed and limited, mobile task arrivals fluctuate significantly with
    time [6]. To accommodate the dynamic mobile tasks with high resource efficiency,
    we exploit the potential of the cloud-assisted mobile edge computing system in
    resource agility [7]. In this system, cloud resources are used on demand to process
    the excessive tasks outsourced from edge nodes. Thus, overloaded tasks at each
    edge node can either be outsourced to the cloud through the core network or migrated
    to nearby lightloaded edge nodes via the local area network (LAN) or wired peer-to-peer
    (P2P) connections [8]. Due to the heterogenous dynamics of task arrivals at edge
    nodes and the distributed nature of the system, the workloads of edge nodes are
    prone to be unbalanced, resulting in high task response time and resource cost.
    According to above analysis, dynamically scheduling mobile tasks to proper edge
    nodes (or the cloud), which thereby balances the system workloads is crucial for
    improving the system performance at low resource cost. In this paper, we investigate
    the dynamic task scheduling issue based on the ETSI MEC architecture [9]. In this
    architecture, a MEC application instantiation is triggered once an instantiation
    request is raised from a mobile device or from the Operations Support System.
    The MEC Orchestration (MEO) selects the ideal MEC server for the MEC application
    instance, and then routes the corresponding mobile traffic to the selected MEC
    server. With the above functionality of MEO, we seek to design a dynamic task
    scheduling strategy, which adaptively guides the MEO to make proper decisions
    for MEC application instantiation and traffic scheduling. Dynamically scheduling
    mobile tasks in the cloud-assisted mobile edge computing is a two-fold problem
    which needs to (i) adaptively tune the usage of cloud resources to cater for time-varying
    mobile tasks, (ii) properly schedule mobile tasks among edge nodes and the cloud.
    Solving this problem optimally faces critical challenges. First, the computation
    capacities of edge nodes are fixed and limited, while mobile task arrivals vary
    with time significantly due to the mobility of mobile users and fluctuation of
    wireless environment. Dynamic task scheduling is demanded to process the mobile
    tasks with reduced delay and resource consumption. Second, edge nodes are heterogeneous
    in terms of computation capacities and mobile task arrivals. Proper task scheduling
    requires workload balancing among the heterogeneous edge nodes and the cloud,
    causing exponential computation complexity. Third, proper task scheduling calls
    for tradeoff between computation and communication delay. Migrating tasks among
    nearby edge nodes or outsourcing excessive tasks to the cloud is beneficial to
    reduce the computation delay. Nevertheless, additional transmission requests are
    induced on the LAN or the core network, causing extra communication delay. Scheduling
    mobile tasks among the edge nodes and the cloud should coordinate the computation
    and the introduced communication delay to optimize the average task response time.
    To address the above challenges, we formulate it as a dynamic optimization problem
    and design an efficient task scheduling algorithm based on the Lyapunov optimization
    method and the idea of water filling. To deal with the first challenge of task
    arrival dynamics, we use the Lyapunov optimization method [10] to dynamically
    tune the usage of cloud resources and cater for dynamic task arrivals with high
    resource agility. To address the second challenge of exponential complexity caused
    by edge heterogeneity, we propose the Water-filling Based Dynamic Task Scheduling
    (WiDaS) algorithm, which is proved to have polynomial complexity. To address the
    third challenge of computation-communication tradeoff, we solve dynamic task scheduling
    considering the non-negligible transmission delay on LAN. A queuing network is
    used to model the computing processes at edge nodes and the cloud and the transmitting
    processes on the LAN and the core network. The computation and communication delay
    can be properly traded off when the average task response time in the queuing
    network is minimized. The contributions of this paper are summarized as follows.
    This work investigates the dynamic task scheduling issue in cloud-assisted mobile
    edge computing, which is the first to include both peer task scheduling among
    edge nodes and cross-layer task scheduling from edge nodes to the cloud. We formulate
    it as a dynamic optimization problem and propose the WiDaS algorithm to deal with
    the challenges of task arrival dynamics, edge node heterogeneity and computation-communication
    delay tradeoff. We conduct extensive simulations to evaluate the WiDaS algorithm
    under one trace-driven traffic pattern and two mathematic traffic patterns. The
    simulation results demonstrate that WiDaS shows two-fold benefits of efficiency
    and effectiveness. The organization of this paper is as follows. We first review
    the related work in Section 2. In Section 3, we first introduce the overview of
    the system, and then present the analytical model and problem formulation. Section
    4 provides the problem transformation and performance analysis. Based on the above
    result, we present the WiDaS algorithm design in Section 5. Section 6 conducts
    extensive simulations to evaluate the proposed algorithm, and Section 7 concludes
    the paper. SECTION 2 Related Work To meet the QoS requirements of the delay-sensitive
    and resource-intensive mobile applications, researchers have devoted extensive
    efforts to exploiting the potential of mobile edge computing, such as edge-based
    augmented reality applications [11], edge caching [12], [13], edge-based vehicular
    network [14], etc. Due to the limited computation capacities of edge nodes and
    the transmission bandwidth of wireless access network, a lot of work has investigated
    task scheduling in mobile edge computing. Sundar et al. [15] have solved the task
    scheduling decision problem from the perspective of applications, with each application
    consisting of multiple dependent tasks which should be processed within a deadline.
    There are also extensive works on task scheduling from the perspective of edge
    operators to provision improved system performance with reduced resource consumption.
    The existing work on task scheduling in mobile edge computing can be divided into
    two categories: cross-layer task scheduling and peer task scheduling. Cross-Layer
    Task Scheduling: Researchers have investigated cross-layer (device-to-edge or
    edge-to-cloud) task scheduling to enhance local computation capabilities by solving
    the computation offloading problem. Distributed computation offloading schemes
    have been widely investigated in which computation offloading decisions are made
    in a distributed manner to optimize various objectives, including energy consumption
    [16], [17], [18], processing delay [19], [20], [21], bandwidth cost [22] and mobility
    management [23], [24]. Centralized computation scheduling mechanisms have also
    been proposed by researchers. Sardellitti et al. [25] have jointly optimized the
    radio and computational resources in multicell mobile edge computing by designing
    algorithms for both the single-user and multi-user scenarios. Yang et al. in [26]
    have investigated network-aware computation partitioning for multi-user edge computing.
    Li et al. in [27] have designed offloading strategy between edge servers and cloud
    for deep learning based IoT applications. Kwak et al. [28] have proposed an energy-aware
    dynamic resource and task allocation algorithm to optimize CPU and network energy
    within delay constraints. In [29], centralized workload scheduling among multiple
    mobile devices and one single edge node has been investigated. An efficient algorithm
    has been proposed to achieve centralized access control of mobile tasks from mobile
    devices to the edge node and proper workload scheduling between the edge node
    and the cloud. In [30], Xu et al. have explored the edge-cloud cooperation to
    jointly optimize service caching and task offloading. Cloud servers act as a powerful
    backup resource provider in the system, provisioning ample storage and computation
    capacity for excessive tasks from each edge node. Peer Task Scheduling: Researchers
    have also devoted extensive efforts to peer task scheduling to explore the potential
    of peer cooperation in improving resource efficiency. Chen et al. [8] have exploited
    cooperation among small base stations (SBSs) to efficiently handle spatially uneven
    workloads of different SBSs. An online peer offloading algorithm has been developed
    by leveraging the Lyapunov optimization method to optimize the long-term system
    performance without knowledge of future information. Cui et al. [31] have promoted
    software defined cooperative offloading in D2D network and scheduled tasks among
    mobile devices in a centralized manner. A greedy algorithm has been designed to
    efficiently address the problem with large scale, targeting at reducing the energy
    consumption of mobile devices and the traffic on the access links. There are only
    a few works studying both cross-layer task scheduling and peer task scheduling
    [32], [33], [34]. Champati et al. [32] have solved the task scheduling problem,
    which decides task offloading to the cloud and task scheduling among local processors,
    without assuming that task processing times are known a prior. However, the main
    focus of the work is static task scheduling, and in the dynamic task arrival scenario,
    the performance bound can only be ensured for special cases (e.g., when the cloud
    processing delay is negligible). In work [33], [34], dynamic scaling of cloud
    resources is not considered, which cannot fully improve resource scalability by
    exploring the resource elasticity of cloud. Our main focus is dynamic task scheduling
    in the cloud assisted mobile edge computing, including both cross-layer task scheduling
    and peer task scheduling. Cloud resource usage is dynamically tuned and task scheduling
    is thereby adjusted among edge nodes and from edge nodes to cloud. An efficient
    workload scheduling algorithm which achieves close-to-optimal performance (illustrated
    in Section 6.2) with reduced complexity is provided considering the non-negligible
    transmission delay to the cloud and computation delay in the cloud. SECTION 3
    System Model and Problem Formulation In this section, we first introduce the overview
    of task scheduling in mobile edge computing, then we present the analytical model
    of the dynamic task scheduling problem. Finally, we provide the problem formulation.
    3.1 Task Scheduling in Cloud-Assisted Mobile Edge Computing In mobile edge computing,
    neighboring edge nodes are connected by LAN or wired P2P connections. Fig. 1 illustrates
    the architecture of cloud-assisted mobile edge computing based on the ETSI MEC
    architecture in [9]. Operation Support System receives mobile requests and makes
    granting decisions. The granted requests are directed to MEO for request scheduling.
    To properly scheduling mobile tasks among edge nodes, the MEO periodically collects
    information from edge nodes, including mobile task arrival rates and available
    edge resources. With the above information, MEO forms a network-wide view and
    manages the system in a centralized manner. Fig. 1. Task scheduling in cloud-assisted
    mobile edge computing. Show All Notice that information exchange overhead is inevitable
    for almost all centralized schemes, consuming communication resources and incurring
    additional information exchange delay. Nevertheless, the transmitted data size
    during the information exchange between MEO and edge nodes is quite small compared
    to the mobile tasks. Thus, the resource consumption and information exchange delay
    can be considered negligible. Moreover, we seek to design a workload scheduling
    algorithm that can be adjusted according to the practical scenario. For example,
    when it is not allowed to schedule tasks among edge nodes across different cities,
    each city can be managed by a MEO (which can be operated on an edge node). The
    information exchange delay between the edge nodes and MEO can be as low as 10ms
    according to our first measurement study on a leading public edge platform in
    China [35] (even in the worst case, the information exchange delay will not exceed
    20ms), which is negligible compared to the task processing delay. In the cloud-assisted
    mobile edge computing system, mobile task arrivals at each edge nodes vary with
    time significantly while the resource capacities of edge nodes are fixed and remain
    unchanged. To optimize system performance with reduced system resource cost, we
    investigate dynamic task scheduling for the MEO in cloud-assisted mobile edge
    computing. To better characterize system dynamics, the task scheduling decision
    is operated in a slotted manner. In each time slot, MEO dynamically schedules
    mobile tasks as follows. 1) MEO updates the cloud information of the last time
    slot kept in the cloud profile table, which records cloud resource utilization,
    core network bandwidth, and cloud resource cost. 2) MEO updates history information
    of edge nodes kept in the edge profile tables at MEO, which record mobile task
    arrival rates and available edge resources. 3) Based on the history information
    of the edge profile tables, mobile task arrivals of the current time slot can
    be predicted. This prediction is for short-term instantaneous task arrival of
    the current time slot, and high prediction accuracy can be achieved with the well-studied
    prediction methods, such as FFT [36], Vector Autoregression [37], and LSTM [38].
    As task arrival prediction is not the main focus of this work, we do not devote
    efforts to task arrival prediction and provide no results of the prediction. In
    Section 6, we present different task arrival patterns as the prediction results
    of mobile task arrivals. 4) With the above information, MEO makes the task scheduling
    decision, including i) cloud usage decision, i.e., the optimal number of the tenanted
    cloud instances and the optimal service level of core network, ii) edge workload
    scheduling decision, i.e., the optimal workloads scheduled to edge nodes and the
    cloud. Through dynamically tuning the usage of cloud resources and thereby adjusting
    the task scheduling decisions with the above steps, MEO seeks to optimize the
    long-term system performance within the resource budget limit. 3.2 Analytical
    Model This section presents the analytical model of dynamic task scheduling in
    the cloud-assisted mobile edge computing system. We consider IN={1,2,...N} edge
    nodes that are connected by LAN. The MEC application instantiation and traffic
    scheduling of the edge nodes are managed by the MEO. Let Υ n denote the set of
    neighbouring edge nodes that have connection with edge node n . Each edge node
    n has fixed and limited computation capability δ n (in CPU cycle per second).
    Cloud resources are utilized on demand to meet the QoS requirements of dynamic
    tasks. In public clouds, users are allowed to scale cloud resource usage by services
    such as AWS Auto Scaling [39]. We define a decision round as a time slot. At the
    beginning of each time slot, the controller makes a centralized cloud resource
    scaling and workload scheduling decision. As the scaling actions can not happen
    too frequently, a time slot is much longer than the time interval between two
    successive task arrivals at each edge node. In this work, we consider a set of
    T={1,2,...T} time slots. Detailed notations are listed in Table 1. 3.2.1 Dynamic
    Task Arrival To characterize the dynamics of mobile task arrival at each edge
    node, we use the non-homogeneous Poisson process to model the mobile task arrival
    [8], [40]. With this model, the task arrival process at edge node n ( n∈IN ) is
    a Poisson process in each time slot. Let A n (t) denote the average arrival rate
    of mobile tasks at edge node n in time slot t and A n (t) varies in different
    time slots. As different mobile tasks have various computation (in CPU cycles)
    and transmission requests (in bytes), we assume that the required CPU cycles of
    mobile tasks follow exponential distribution [8], with ξ representing the expectation.
    When transmitting one unit of mobile tasks (in CPU cycles), c units of input data
    are required (in bytes). 3.2.2 Executing at Edge Nodes As edge nodes have heterogeneous
    computation capacities and uneven mobile task arrivals, migrating tasks among
    nearby edge nodes is beneficial to exploit the under-utilized edge nodes and balance
    edge workloads. According to mobile task arrivals and the available edge computation
    capacities, the edge nodes can be divided into three categories: overloaded edge
    nodes, neutral edge nodes, and light-loaded edge nodes: 1) For a overloaded edge
    nodes s , a fraction of tasks are migrated to nearby edge nodes or outsourced
    to the cloud. Denote by r sd (t) the fraction of tasks migrated from edge node
    s to edge node d in time t ( r sd (t)≥0 ). In particular, r ss (t) represents
    the fraction of tasks executed locally, and r s0 (t) represents the fraction of
    tasks outsourced to the cloud. Thus, r sd (t) satisfies ∑ d∈N∪{0} r sd (t)=1 and
    the tasks executed at edge node s follow a Poisson process with the parameter
    λ s (t) given as λ s (t)= r ss (t)⋅ A s (t) . 2) For a neutral edge node m , it
    neither receives tasks from nearby edge nodes ( r sm =0 ) nor migrates tasks to
    other edge nodes ( r md =0 ) or the cloud ( r m0 =0 ). Thus, the tasks executed
    at edge node m follow a Poisson process with the parameter of A m (t) (i.e., r
    mm (t)=1 , λ m (t)= A m (t) ). 3) For a light-loaded edge node d , the tasks include
    the tasks originally arriving at this edge node and those migrated from the overloaded
    edge nodes. Thus the tasks executed at edge node d follow a Poisson process (sum
    of several Poisson processes) and the parameter λ d (t) is given as λ d (t)= A
    d (t)+ ∑ s∈N∖{d} r sd (t) A s (t) . Computation Delay: By summarizing above analysis,
    the task arrival at each edge node can be modeled as a Poisson process in each
    time slot. In addition, as the required computation of different tasks follows
    exponential distribution with the parameter of ξ , the serving time at each edge
    node also follows exponential distribution with the expectation τ n = ξ δ n .
    (1) View Source Therefore, in each time slot t , the serving process at each edge
    node can be modeled as an M/M/1 queue and the computation delay is given as D
    n (t)= 1 μ n − λ n (t) , (2) View Source where μ n = 1 τ n . To ensure the stability
    of the queue, it should be satisfied that λ n (t)< μ n for any time t . Thus,
    λ n (t) is constrained as 0≤ λ n (t)< μ n . (3) View Source Moreover, as only
    the neighbouring edge nodes in Υ n can migrate tasks to edge node n , the tasks
    executed at n cannot exceed the overall tasks arriving at the neighbouring edge
    nodes. Therefore, by combining the queue stability condition in Eq. (3), λ n (t)
    should satisfy 0≤ λ n (t)<min ⎧ ⎩ ⎨ ∑ i∈ Υ n ∪{n} A n (t), μ n ⎫ ⎭ ⎬ . (4) View
    Source Migration Delay Among Edge Nodes. Migrating mobile tasks among edge nodes
    can cause additional transmission requests on LAN. Let d n (t) denote the migration
    delay of inbound tasks to edge node n . We consider that neighboring edge nodes
    are connected by high-bandwidth wired connections, and the migration delay d n
    (t) ( d n (t)≥0 ) is irrelevant to the transmitted data size. Denote by λ ⋅n (t)
    the expected rate of incoming tasks at edge node n from other edge nodes. Thus,
    λ ⋅n (t) can be given as λ ⋅n (t)= ∑ s∈N∖{n} r sn (t) λ s (t) , and it is satisfied
    that λ ⋅n (t)=max{ λ n (t)− A n (t),0}. (5) View Source 3.2.3 Outsourcing to the
    Cloud Public clouds provide users with elastic computation resources that are
    encapsuled as cloud instances and charged based on usage hourly or even by second
    [41]. To accommodate dynamic mobile tasks with high resource efficiency, the controller
    tunes the usage of cloud instances and outsources the excessive tasks to the cloud
    at peak hours. When outsourcing workloads to the cloud, input data of the tasks
    needs to be transmitted through the core network. Since wide area network delay
    and jitters can be induced during the transmission [42], a dedicated connection
    should be established between edge nodes and the cloud to guarantee the system
    performance. Amazon makes it possible by providing the service AWS Direct Connect
    [43], enabling users to select from different service levels IL={0,1,...,L} .
    The transmission rate and pricing rate of AWS Direct Connect are determined by
    the service level x(t) , denoted as R(x(t)) and P(x(t)) respectively. Note that
    x(t)=0 indicates that no data is transmitted through the core network, i.e., the
    transmission rate R(0)=0 , and the pricing rate P(0)=0 . Table 2 lists the transmission
    rates and pricing rates of different service levels. TABLE 1 Table of Notations
    TABLE 2 Pricing of AWS Direct Connect Denote by λ out (t) the average rate of
    outsourced tasks to the cloud ( λ out (t)≥0 ), which can be computed as λ out
    (t)= ∑ n=1 N A n (t)− ∑ n=1 N λ n (t). (6) View Source Let b core (t) be the transmission
    rate of the core network ( b core (t)=R(x(t)) ), then the transmission delay in
    the core network is D core (t)= 1 μ core (t)− λ out (t) , (7) View Source where
    μ core (t)= b core (t) cξ . To ensure the stability of data transmission in the
    core network, it should be ensured that 0≤ λ out (t)< μ core (t). (8) View Source
    Denote by λ cloud (t) ( λ cloud (t)≥0 ) the task arrival rate in the cloud, there
    is λ cloud (t)= λ out (t). (9) View Source As cloud resources are provisioned
    as encapsuled instances, let m(t) represent the number of tenanted cloud instances
    in time t . 0≤m(t)≤ m max , (10) View Source where m max is the maximum cloud
    instance number that can be used. In the cloud, the usage of cloud resources is
    tuned to accommodate dynamic tasks with high resources efficiency. With services
    such as AWS Auto Scaling, cloud users are allowed to scale cloud resource usage
    by specifying scaling rules. In this work, the logical controller scales the cloud
    resource usage (i.e., the number of tenanted cloud instances m(t) ) according
    to cloud resource utilization ρ(t) : m(t+1)= ⎧ ⎩ ⎨ ⎪ ⎪ m(t)+i, max{m(t)−j,0},
    m(t), ρ(t)≥ U up ρ(t)< U down otherwise.  (11) View Source Here, i , j are positive
    constants, and U up , U down represent the upper and lower bound of the cloud
    resource utilization. The cloud resource utilization ρ(t) is represented by the
    resource utilization of the queue in the cloud (the definition is given later).
    Eq. (11) indicates that the number of tenanted cloud instances increases by i
    when the cloud resource utilization of last time slot grows higher than the upper
    bound, and the number decreases by j (if positive) when the cloud resource utilization
    drops lower than the lower bound. With the above scaling rules, the computing
    rate of the cloud can be given as δ c (t)=m(t) δ ins , where δ ins is the computing
    rate of one cloud instance. The serving time of tasks in the cloud also follows
    exponential distribution with the expectation 1 μ c (t) . Here, μ c (t) is the
    expected serving rate of the cloud and can be given as μ c (t)= δ c (t) ξ . Similar
    as Eq. (2), the serving process at the cloud can be modeled as an M/M/1 queue,
    and the average computation delay is D cloud (t)= 1 μ c (t)− λ cloud (t) , (12)
    View Source where λ cloud (t) is constrained as 0≤ λ cloud (t)< μ c (t). (13)
    View Source We define the resource utilization of the queue in the cloud as the
    cloud resource utilization, which is computed as ρ(t)= λ cloud (t) μ c (t) . Note
    that the actuation delay, i.e., the start-up time of cloud instances, can be several
    minutes, which is unacceptable for delay-sensitive mobile applications [44]. To
    deal with this problem, AWS Auto Scaling service [39] provides the predictive
    scaling policy to detect cloud usage patterns periodically and perform scaling
    actions in advance of predicted changes. Therefore, the performance degradation
    caused by actuation delay can be avoided. 3.3 Problem Formulation 3.3.1 System
    Cost In the cloud-assisted mobile edge computing framework, the system cost includes
    the on-premise cost of edge nodes and the usage-based cost of cloud resources.
    Let C edge denote the cost of edge nodes, which remains unchanged through time
    since fixed computation capacities are provisioned at edge nodes. The outsourcing
    cost of cloud resources consists of the cost on cloud instances and the cost on
    the core network bandwidth (e.g., AWS Direct Connect). When tenanting m(t) cloud
    instances and selecting the service level x(t) , the outsourcing cost on cloud
    resources C cloud (t) is C cloud (t)=m(t) p ins +P(x(t)). (14) View Source Here,
    p ins is the price of one cloud instance. Hence, the system cost is C(t)= C edge
    + C cloud (t). (15) View Source 3.3.2 System Performance We take the average task
    response time as the metrics of system performance. Let A(t) denote the overall
    task arrival rates at all edge nodes in time t , i.e., A(t)= ∑ n=1 N A n (t) .
    The average task response time is a weighted sum of delay at each part of Fig.
    2 and can be computed as D(t)= + ∑ n=1 N ( λ n (t) A(t) D n (t)+ λ ⋅n (t) A(t)
    d n (t))                                  
       executing at edge nodes λ out A(t) D core (t)+ λ cloud A(t) D cloud (t)
                                       outsourcing
    to the cloud , (16) View Source As shown in the above equation, the average task
    response time consists of the weighted delay when executed at edge nodes and the
    weighted delay when outsourced to the cloud. For the tasks executed at edge nodes,
    the weighted delay includes the computation delay at edge nodes and the migration
    delay among edge nodes. Note that only the migrated tasks from other edge nodes
    can induce transmission delay at edge node n . Thus, the weight of the migration
    delay is λ ⋅n (t) A(t) . When outsourcing mobile tasks to the cloud, the delay
    is composed of the computation delay in the cloud and the transmission delay in
    the core network. Fig. 2. Queuing network model. Show All In the cloud-assisted
    mobile edge computing system, to optimize the average task response time within
    the budget limit, the dynamic task scheduling problem is formulated as P1: min
    s.t. C1: lim T→∞ 1 T ∑ t=0 T−1 D(t) lim T→∞ 1 T ∑ t=0 T−1 C(t)≤Θ Constranits (4)
    (6) (8) (9) (13). (17) View Source In problem P1, the objective is to optimize
    the long-term average task response time. Constraint C1 ensures the long-term
    average system cost does not exceed the resource budget limit Θ . We seek to determine
    the optimal cloud resource usage (i.e., the number of cloud instances, m(t) ,
    and the service level of the core network, x(t) ) and the optimal task scheduling
    decision s ^ (t) ( s ^ (t)=⟨ λ 1 (t),..., λ N (t), λ out (t), λ cloud (t)⟩ ) by
    solving problem P1. SECTION 4 Problem Transformation and Performance Analysis
    In this section, we first present problem transformation based on the Lyapunov
    optimization method. Then we analyze the performance of this transformation, providing
    theoretical basis for algorithm design in the next section. As the usage of cloud
    resources is tuned dynamically, the system cost in different time slots can exceed
    or remain under the budget limit. To characterize the relative states of system
    cost and budget limit, we introduce the virtual queue Q(t) given as Q(t)={ max{Q(t−1)+C(t−1)−Θ,0},   0,                             t≥1
    t=0. (18) View Source Eq. (18) indicates that Q(t) accumulates the excessive system
    cost over the budget limit. With Q(t) , we have the following conclusion: Constraint
    C1can be ensured by enforcing the stability of Q(t) . Proof. According to the
    definition of Q(t) , the deviation of Q(t) satisfies Q(t+1)−Q(t)≥C(t)−Θ. (19)
    View Source Sum up the deviation through time slots t∈T , divide T and take a
    limit over T , there is lim T→∞ Q(T)−Q(0) T ≥ lim T→∞ 1 T ∑ t=0 T−1 C(t)−Θ. (20)
    View Source By enforcing the stability of Q(t) , i.e., lim T→∞ Q(T) T =0, (21)
    View Source we have lim T→∞ 1 T ∑ t=0 T−1 C(t)−Θ<0. (22) View Source Thus the
    constraint C1 is ensured. According to the above conclusion, it follows that if
    we can design a dynamic task scheduling algorithm that minimizes the long-term
    average task response time subject to the stability of virtual queue Q(t) , the
    original problem P1 is equivalently solved. As the Lypunov optimization method
    is dedicated to optimizing objectives while maintaining queue stability, this
    conclusion leads to solving problem P1 with the Lyapunov optimization method.
    4.1 Problem Transformation To solve P1 with the Lyapunov optimization method,
    we first introduce the Lyapunov function and Lyapunov drift. Define the Lyapunov
    function as L(t)= 1 2 Q 2 (t), (23) View Source and the Lyaponov drift as Δ(Q(t))=IE{L(Q(t+1))−L(Q(t))|Q(t)}.
    (24) View Source It can be inferred from [10] that the smaller is Δ(Q(t)) (for
    each t∈T ), the more likely Q(t) is stabilized. In problem P1, apart from the
    virtual queue Q(t) we want to stabilize, there is an associated ”penalty” process
    D(t) , the long-term average of which we want to minimize. Hence, to solve P1,
    we exploit the conclusion of the Lyapunov optimization method and turn to minimize
    the drift-plus-penalty Δ(Q(t))+VIE{D(t)|Q(t)} in each t∈T . Here V ( V≥0 ) implies
    the cost-performance tradeoff, i.e., how much the algorithm design emphasizes
    the system performance compared with the system cost. From the definition of Δ(Q(t))
    , we can derive that the drift-plus-penalty is upper bounded as Δ(Q(t))+VIE{D(t)|Q(t)}
    ≤B+Q(t)IE{(C(t)−Θ)|Q(t)}+VIE{D(t)|Q(t)}, (25) View Source where B is a constant
    and satisfies B≥ 1 2 IE{(C(t)−Θ ) 2 |Q(t)}. (26) View Source for all time t .
    According to the Lyapunov drift-plus-penalty framework in [10], we just need to
    optimize the bound in the right side of (25) in each time slot t , then the problem
    P1 is transformed into P2:min s.t.  Q(t)(C(t)−Θ)+VD(t)  Constraints (4) (6) (9)
    (8) (13). (27) View Source Till now, we have transformed the original dynamic
    optimization problem P1 to a static optimization problem P2 in each time slot.
    In the following part, we will analyze the performance of this problem transformation.
    4.2 Performance Analysis We present theoretical analysis of the problem transformation,
    demonstrating that solving P2 optimally in each time slot can lead to near-optimal
    solution of problem P1. The results are concluded in Theorem 1. Theorem 1. Denote
    by s ^ ∗ (t) the optimal workload scheduling decision obtained by solving problem
    P2 in each time slot. The long-term average virtual queue backlog Q( s ∗ (t))
    and the service response time D( s ∗ (t)) satisfy lim T→∞ 1 T ∑ t=0 T−1 E{D( s
    ∗ (t))}≤ B V + d ∗ lim T→∞ 1 T ∑ t=0 T−1 E{C( s ∗ (t))−Θ}≤ 1 ϵ (B+V( d max − d
    ∗ )), (28) View Source where d ∗ is the optimal long-term average service response
    time to problem P1, d max is the upper bound of the service response time, and
    ϵ is the system cost surplus which can be achieved by a stationary workload scheduling
    strategy. Proof. From Theorem 4.5 in [10], for any χ>0 , there exists a stationary
    workload scheduling policy s ^ Π (t) for problem P2, which is independent of the
    virtual queue satisfying E{D( s Π (t))}≤ d ∗ +χ, E{C( s Π (t))}≤χ (29) View Source
    By applying Eq. (29) into the drift-plus-penalty inequation (25), there is Δ(Q(t))+VE{D(
    s ∗ (t))|Q(t)} ≤B+Q(t)E{(C( s ∗ (t))−Θ)|Q(t)}+VE{D( s ∗ (t))|Q(t)} ≤B+Q(t)E{(C(
    s Π (t))−Θ)|Q(t)}+VE{D( s Π (t))|Q(t)} ≤B+χQ(t)+V( d ∗ +χ) (30) View Source Let
    χ→0 , sum up both sizes of (30) over t∈{0,1,...,T−1} and divide by T , we obtain
    1 T E{L(Q(T))−L(Q(0))}+ V T ∑ t=0 T−1 E{D( s ∗ (t))}≤B+V d ∗ . (31) View Source
    Let T→∞ , (31) is rearranged as lim T→∞ 1 T ∑ t=0 T−1 E{D( s ∗ (t))}≤ B V + d
    ∗ . (32) View Source To prove (28), we assume that there are ϵ>0 and a workload
    scheduling policy s w (t) satisfying E{C( s W (t))−Θ}≤−ϵ. (33) View Source Apply
    (33) to (25), there is Δ(Q(t))+VE{D( s ∗ (t))|Q(t)} ≤B+Q(t)E{(C( s W (t))−Θ)|Q(t)}+VE{D(
    s W (t))|Q(t)} ≤B−ϵQ(t)+VE{D( s W (t))}. (34) View Source Sum up both sizes of
    (34) over t∈{0,1,2,...,T−1} and divide by T , we have 1 T E{L(Q(T))−L(Q(0))}+
    V T ∑ t=0 T−1 E{D( s ∗ (t))} ≤B− ϵ T ∑ t=0 T−1 Q(t)+VE{D( s W (t))} (35) View
    Source Let T→∞ , and (35) can be rearranged as lim T→∞ 1 T ∑ t=0 T−1 E{Q(t)} ≤
    1 ϵ (B+ lim T→∞ V T ∑ t=0 T−1 E{D( s W (t))}) − 1 ϵ lim T→∞ V T ∑ t=0 T−1 E{D(
    s ∗ (t))} ≤ 1 ϵ (B+V( d max − d ∗ )). (36) View Source According to the definition
    of Q(t) , there is lim T→∞ 1 T ∑ t=0 T−1 E{C( s ∗ (t))−Θ}≤ 1 (B+V( d max − d ∗
    )). (37) View Source By summarizing the above analysis, Theorem 1 can be proved.
    Insight: Theorem 1 indicates that solving problem P2 optimally can lead to the
    near-optimal solution of the original problem by randomly changing the weight
    constant V . In each time slot t , observe the current state of Q(t) and solve
    the static optimization problem P2, then the long-term average task response time
    D(t) will be either smaller than the target value d ∗ or differs from d ∗ by less
    than B V . However, the long-term average virtual queue backlog increases linearly
    with V , as illustrated in Eq. (28). As the virtual queue accumulates the excessive
    part of the system cost over the budget limit, Theorem 1 also demonstrates the
    [O(V),O( 1 V )] tradeoff between the average task response time and the system
    cost. SECTION 5 The WiDaS Algorithm Design Section 4 has transformed the original
    dynamic optimization problem P1 into a static optimization problem P2 in each
    time slot. In this section, we first analyze the computation complexity of problem
    P2 by exploiting the convex property. Then, we present the algorithm design of
    WiDaS. 5.1 Complexity Analysis When given the cloud computation capacity (i.e.,
    the number of cloud instances m(t) ) and the core network bandwidth (i.e., the
    service level of AWS Direct Connect x(t) ), problem P2 has the the following property.
    Theorem 2. When given m(t) and x(t) , problem P2 is a convex optimization problem
    over the task scheduling decision s ^ (t) . Proof. According to the definition
    of a convex optimization problem, if we want to prove the covexity of problem
    P2, we need to prove that the objective function Q(t)(C(t)−Θ)+VD(t) is convex
    over s ^ (t) . In each time slot t , the virtual queue Q(t) is updated according
    to (18) and C(t) is determined by m(t) and x(t) . Thus, it is needed to prove
    that D(t) is convex over s ^ (t) . According to (16), D(t)= 1 A(t) [ ∑ n=1 N λ
    n (t) μ n − λ n (t) + λ ⋅n (t) d n (t)+ λ out (t) μ core (t)− λ out (t) + λ cloud
    (t) μ c (t)− λ cloud (t) ]. (38) View Source Here, λ ⋅n (t)=max{ λ n (t)− A n
    (t),0} is convex over λ n (t) , and d n (t) is constant with respect to λ n (t)
    , we just need to prove that D ′ (t)=[ ∑ n=1 N λ n (t) μ n − λ n (t) + λ out (t)
    μ core (t)− λ out (t) + λ cloud (t) μ c (t)− λ cloud (t) ] is convex over λ n
    (t) . Denote by H( D ′ (t)) the Hessian Matrix of D(t) , H( D ′ (t))= [ h mn ]
    (N+2)(N+2) , (39) View Source and h mn (t) is given as h mn (t)= ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪
    ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ 0,                    2 μ n ( μ n − λ n (t)) 3 ,           2
    μ core (t) ( μ core (t)− λ out (t)) 3 ,     2 μ c (t) ( μ c (t)− λ cloud (t))
    3 ,       m≠n m=n,n∈{1,2,...,N} m=n=N+1 m=n=N+2. (40) View Source We can conclude
    that h nn (t)>0 for each n∈{1,2,...,N+2} , and h mn (t)=0 for any m≠n , thus H(
    D ′ (t)) is a positive semi-definite matrix. Hence, it can be proved that D ′
    (t) is convex over s ^ (t) and thus, D(t) is convex over s ^ (t) . By summarizing
    above analysis, the objective function of P2 is a convex function over s ^ (t)
    , the inequation constraints are convex over s ^ (t) , and the equation constraints
    are affine functions of s ^ (t) . Therefore, problem P2 is a convex optimization
    problem over s ^ (t) [45]. Based on Theorem 2, we have transformed the original
    dynamic optimization problem P1 to the convex optimization problem P2 with the
    Lyapunov optimization method. Note that we remove the time parameter in this section
    since problem P2 is a static convex optimization problem. In a convex optimization
    problem, the points satisfying the KKT conditions are the optimal solutions to
    this problem [45]. From convex optimization analysis, the KKT conditions of P2
    are given as follows, ∇y( s ^ )+ ∑ i=1 2N+4 σ i ∇ f i ( s ^ )+ ∑ j=1 2 η j ∇ g
    j ( s ^ )=0 σ i f i ( s ^ )=0    i=1,2,...,2N+4 σ i ≥0    i=1,2,...,2N+4 f i (
    s ^ )≤0    i=1,2,...,2N+4 g j ( s ^ )=0    j=1,2. (41) View Source Here, y( s
    ^ )=Q(C−Θ)+VD( s ^ ), (42) View Source η j and σ i are Lagrange multipliers, and
    f i ( s ^ ) ( i=1,2,...,2N+4 ) are standard form of inequation constraints in
    (27), which are defined as f i ( s ^ )= ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪
    ⎪ ⎪ λ i −( μ i −ε),                    − λ i−N ,                        λ out
    −( μ core −ε),                    − λ out ,                         λ cloud −(
    μ c −ε),                   − λ cloud ,                         i=1,...,N i=N+1,...,2N
    i=2N+1 i=2N+2 i=2N+3 i=2N+4. (43) View Source Here, ε is a positive constant.
    g j ( s ^ ) ( j=1,2 ) are the standard form of equation constraints in (41) given
    as g 1 ( s ^ )= ∑ n=1 N ( A n − λ n )− λ out g 2 ( s ^ )= λ cloud − λ out . (44)
    View Source Insight: The KKT conditions (41) indicate that the optimal solutions
    of the convex optimization problem are searched among the extreme points and boundary
    points. For each inequation constraint f i ( s ^ ) ( i=1,2,...,2N+4 ), there are
    two possible results: f i ( s ^ )=0, σ i ≥0 implies that the optimal solution
    is on the boundary; f i ( s ^ )<0, σ i =0 indicates that the optimal solution
    is at the extreme points. As there are ( 2N+4 ) inequation constraints in problem
    P2, directly searching for the points satisfying the KKT conditions can yield
    O( 2 2N+4 ) computation complexity. Therefore, solving P2 based on the KKT conditions
    has an exponential computation complexity over the number of edge nodes, N . When
    N is large, a task scheduling algorithm with reduced computation complexity is
    needed. 5.2 Efficient Task Scheduling Based on Water Filling With the increasing
    density of edge nodes in 5G era, N grows rapidly. In this section, we seek to
    design an efficient task scheduling algorithm with reduced computation complexity.
    The complexity analysis in Section 5.1 implies that the exponential computation
    complexity of P2 arises from the inequation constraints, each of which limits
    the searching range with the upper and lower bounds for each task scheduling variable
    in s ^ . By exploiting this property, we design an efficient task scheduling algorithm
    as follows. We first assume that each part of the system, including the edge nodes,
    the core network, and the cloud, has unlimited resource capacity by removing the
    inequation constraints in P2. Then we can compute the relative relationship of
    the task scheduling variables by solving the modified convex optimization problem
    P2 in O(1) computation complexity. Finally, for the task scheduling variables
    which have grown or dropped beyond the bounds, we take the value of the upper
    or lower bounds; for the other variables, we still search for the results subject
    to the above relative relationship. This process is similar to filling water to
    tubes with different upper bounds. We first assume that the tubes have no upper
    bounds and fill water to these tubes subject to a certain relative relationship.
    Once the water level of one tube achieves the upper bound, we will no longer fill
    water to this tube, and the water level remains at the upper bound. Therefore,
    our algorithm is called Water-filling Based Dynamic Task Scheduling (WiDaS) algorithm.
    The details of the algorithm are as follows: Step 1. Remove the inequation constraints
    of P2, and search for the extreme points of y( s ^ ) within the equation constraints
    by solving the modified P2. When removing the inequation constraints of P2, the
    KKT conditions are transformed as: ∇y( s ^ )+ ∑ j=1 2 η j ∇ g j ( s ^ )=0 g 1
    ( s ^ )=0. (45) View Source g 2 ( s ^ )=0. (46) View Source Note that y( s ^ )
    is not partially derivable over λ n when λ n = A n (caused by λ ⋅n =max{ λ n −
    A n } in (16)). This can be solve by dividing λ n into two cases, i.e., λ n ≥
    A n and λ n < A n . Thus combining (45), the workload scheduling variables in
    s ^ can be represented as the functions of η 1 : λ n ( η 1 )= ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪
    ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ μ n − V μ n A( η 1 − d n ) − − − − − − √ ,     μ n − V μ n η 1 A
    − − − √ ,        A,                 η 1 ≥ β n + d n 0≤ η 1 < β n otherwise λ out
    ( η 1 )= μ core − V μ core ( η 1 + η 2 ( η 1 ))A − − − − − − − − √ λ cloud ( η
    1 )= μ c − −V μ c η 2 ( η 1 )A − − − − − √ , (47) View Source where β n = V μ
    n A ( μ n − A n ) 2 , (48) View Source η 2 ( η 1 )= V μ core A [ μ core −(A− ∑
    n=1 N λ n )] 2 − η 1 . (49) View Source Step 2. Restrict these workload scheduling
    variables within the upper and lower bounds (i.e., inequation constraints of P2)
    as follows: λ n = ⎧ ⎩ ⎨ ⎪ ⎪ 0,                    μ n −ε,                λ n (
    η 1 ),                if  λ n ( η 1 )<0 if  λ n ( η 1 )> μ n −ε otherwise. (50)
    View Source λ out = ⎧ ⎩ ⎨ ⎪ ⎪ 0,                        μ core −ε,                 λ
    out ( η 1 ),                if  λ out ( η 1 )<0 if  λ out ( η 1 )> μ core −ε otherwise.
    (51) View Source λ cloud = ⎧ ⎩ ⎨ ⎪ ⎪ 0,                     μ c −ε,                λ
    cloud ( η 1 ),            if  λ cloud ( η 1 )<0 if  λ cloud ( η 1 )> μ c −ε otherwise.
    (52) View Source Substitute (50) into (49) and compute g 2 ( η 1 )= λ cloud −
    λ out according to (52) and (51), then we can represent g 2 as the function of
    η 1 within the inequation constraints of P2. Step 3. Search for η 1 that enforces
    g 2 ( s ^ ( η 1 ))=0 . We can prove that g 2 ( η 1 ) increases with η 1 . Thus,
    the η 1 satisfying g 2 ( η 1 )=0 can be obtain by the bisection method. Proof.
    Substitute (6) and (47) into (44), then g 2 ( η 1 ) can be given as g 2 ( η 1
    ) = λ cloud ( η 1 )− λ out ( η 1 ) =( μ c − −V μ c η 2 ( η 1 )A − − − − − − −
    √ )−(A− ∑ n=1 N λ n ) = μ c −A− −V μ c η 2 ( η 1 )A − − − − − − − √ + ∑ n=1 N
    λ n , (53) View Source where η 2 ( η 1 ) and λ n are presented in (49) and (50).
    It can be derived that λ n increases with η 1 , and η 2 ( η 1 ) decreases with
    η 1 . Therefore, g 2 ( η 1 ) increases with η 1 . With above steps, we have obtained
    an efficient algorithm of problem P2 (which is also proved to be effective in
    Section 6). Thus we can solve the original problem P1 as follows: 1) In each time
    slot t , update m(t) according to (11) and compute Q(t) according to (18). 2)
    Traverse x(t)∈IL and solve problem P2 with the above three steps. We summarize
    the details in Algorithm 1. The main computation of Algorithm 1 comes from Line
    11. For each η 1 , computing g 2 ( s ^ ( η 1 )) requires traversing λ n ( n∈IN
    ) to restrict within the equation constraints, causing the complexity of O(N)
    . For each x(t) , searching for the optimal η 1 with the bisection method yields
    O(log( η R 1 − η L 1 σ )) iterations. Therefore, the total computing complexity
    of Algorithm 1 is O(TLNlog( η R 1 − η L 1 σ )) , where η L 1 , η R 1 are the left
    and right bound of the bisection method, and σ is the searching precision. Algorithm
    1. WiDaS Algorithm 1: for each t∈{1,2,...,T} do 2: Update m(t) according to (11).
    3: Update Q(t) according to (18). 4: for each x(t)∈{0,1,...,L} do 5: μ c (t)=m(t)
    μ ins . 6: μ core (t)=R(x(t)) . 7: C(t)=m(t) p ins +P(x(t)) . 8: NUM=⌈ log 2 (
    η R 1 − η L 1 σ )⌉−1 . 9: η m 1 = η L 1 + η R 1 2 . 10: for num=0:NUM do 11: Compute
    g 2 ( s ^ ( η L 1 )) and g 2 ( s ^ ( η m 1 )) according to (51), (52). 12: if
    g 2 ( s ^ ( η m 1 ))=0 then 13: break. 14: else if g 2 ( s ^ ( η 1 1 ))⋅ g 2 (
    s ^ ( η m 1 ))<0 then 15: η R 1 = η m 1 . 16: else 17: η L 1 = η m 1 . 18: end
    if 19: η m 1 = η L 1 + η R 1 2 . 20: end for 21: η 1 = η m 1 . 22: Compute s ^
    =⟨ λ 1 ,..., λ N , λ out , λ cloud ⟩ according to (50), (51), (52). 23: Compute
    Q(t)(C(t)−Θ)+VD(t) . 24: end for 25: x ∗ (t)= argmin x(t)∈{0,1,...l} {Q(t)(C(t)−Θ)+VD(t)}
    . 26: ρ( x ∗ (t))= λ cloud ( x ∗ (t)) μ c (t) . 27: end for Remark: Although the
    water-filling method has been widely adopted in the wireless communication area,
    we are the first to introduce the water-filling method into workload scheduling
    (considering both task transmission and task processing) in mobile edge computing.
    Apart from the lower bound (i.e., 0, which is the same as the above cases in wireless
    communications), the workload scheduling results in mobile edge computing also
    have upper bounds (related to the edge resource capacities), which further aggravates
    the difficulty of problem analysis and addressing. Existing water-filling based
    solutions have demonstrated that the waterfilling-like optimization problems with
    a single water level can be solved by iterative algorithms and exact algorithms
    [46]. In this paper, we first derive the workload scheduling results into a waterfilling-like
    form with a single water level (in Eqs. (43) and (44)). As the iterative algorithms
    for waterfilling-like optimization problems with a single water level can get
    close to the exact value when the number of iterations goes to infinity [20],
    our proposed algorithm can get to the optimal solution (same as the KKT-based
    algorithm) when the searching precision σ go to zero. 5.3 Discussion on Decentralized
    Implementation Algorithm 1 is designed under the ETSI MEC architecture, in which
    all the edge nodes belong to the same administrator or operator (e.g., a mobile
    network operator or a cloud service provider), thus all of them can be managed
    in a centralized manner. However, there are also scenarios that the edge nodes
    consist of multiple entities pursuing their own interests. In these scenarios,
    not all edge nodes are willing to share their spare resources cooperatively and
    centralized control over the whole system is not feasible. Decentralized implementation
    of the proposed algorithm should be taken into consideration. Coalition games
    can be utilized to characterize the behaviours of edge nodes. Edge nodes are motivated
    to form small coalitions by proper incentive mechanisms. With these incentive
    mechanisms, all the edge nodes within one coalition can increase their utility
    by working cooperatively while edge nodes across different edge nodes prefer to
    be segregated. In this case, our proposed algorithm can be applied to scheduling
    mobile tasks among the edge nodes that are within one coalition. It is not hard
    to notice that proper incentive mechanisms are the key to the coalition games.
    Based on the analysis in [47], [48], merge-and-split operations can be used to
    form proper coalitions and the stability can be enforced. SECTION 6 Performance
    Evaluation In this section, we conduct extensive simulations to evaluate WiDaS
    under different task arrival patterns. 6.1 Simulation Setup and Metrics We provide
    a simulator to realize the functionality of MEO which manages the MEC application
    instantiation and traffic scheduling over the cloud-assisted mobile edge computing
    system . We consider a system with N=10 edge nodes, which have uniformly distributed
    computation capacities in [10,20] Giga CPU cycles/second. The required CPU cycles
    of mobile tasks follow exponential distribution with the expectation of ξ=1 Giga
    CPU cycles. The task arrival process at each edge node is a non-homogeneous Poisson
    process with the expected arrival rate A n (t) uniformly distributed in [5,15]
    tasks/second. Nearby edge nodes are connected by LAN or wired P2P connection (with
    migration delay of d n =10 ms) and each edge node can connect to the cloud though
    the core network. We use the Amazon on-demand instance, m4.large [49], to process
    excessive tasks from edge nodes, with the computation capacity of 7.8 Giga CPU
    cycles/second and the pricing rate of 0.1 dollar/hour per instance. When migrating
    mobile tasks to nearby edge nodes or outsourcing these tasks to the cloud, input
    data of these tasks should be transmitted on the LAN or in the core network. Take
    augmented reality tasks as an example. When migrating augmented reality tasks
    among edge nodes or to the cloud, slotted frames of videos, i.e., figures, should
    be transmitted. As processing one 420 kB figure requires around 1 Giga CPU cycles
    computation [50], we set the transmission ratio c=3.36 Mbits/Giga CPU cycles.
    6.1.1 Task Arrival Pattern We use three types of traffic patterns to simulate
    the time-varying property of mobile task arrivals at edge nodes, including one
    trace-driven traffic pattern and two mathematical traffic patterns. Trace-Driven
    Traffic Pattern. Mobile edge computing has not been widely deployed in practice,
    thus we can not trace the mobile task arrivals at edge nodes precisely. In this
    work, we use the task arrival traces of Google compute cells to simulate the mobile
    task arrivals at edge nodes. In Google clusters, a compute cell is composed of
    several well connected computing machines with high-bandwidth network, which is
    similar to an edge node. We employ the tracelogs of Google compute cells, i.e.,
    Google cluster usage traces [51], to imitate the real traces of mobile task arrivals
    at edge nodes. These traces record the timestamps of task events (e.g., submit,
    schedule, finish, etc.) in task event tables and the CPU usage of compute cells
    in resource usage tables. The task arrival rates in terms of computation requests
    are computed as A cell =( t finish − t schedule )⋅ U cpu ⋅ μ cpu ⋅ r task , (54)
    View Source where t finish and t schedule are the timestamps of task finishing
    and scheduling to machines in a compute cell. U cpu represents the average CPU
    usage and μ cpu is the serving rate of a CPU. r task is the number of tasks arriving
    at the cell within a time slot. We obtain 5 traces of task arrivals over a 10
    10 ms period by slightly modifying the traces of a Google compute cell (similar
    to [7]). The results are shown in Fig. 3. Fig. 3. Tracelogs of Google compute
    cells. Show All Random Traffic Pattern. Each edge node has mobile tasks with random
    arrival rates uniformly distributed in [5,15] tasks/second. Normal Traffic Pattern.
    Each edge node has mobile tasks with normally distributed arrival rates through
    T time slots with the expectation 10 tasks/second, and standard deviation 2 tasks/second.
    Note that we take the absolute value if the arrival rate at any time slot is negative.
    6.1.2 Benchmark Algorithms We compare WiDaS with three benchmark algorithms: KKT-based
    algorithm: The KKT-based algorithm computes the optimal solution by directly solving
    the convex optimization problem P2 with KKT conditions in each time slot. According
    to the analysis in Section 5.1, the computation complexity of the KKT-based algorithm
    is O(TL⋅ 4 N ) . Edge-first algorithm: Mobile tasks are processed at edge nodes
    with high priority if the edge nodes have sufficient computation capabilities
    (i.e., μ n > A n (t) ). Otherwise, the excessive tasks are outsourced to the cloud.
    Fair-ratio algorithm: The mobile tasks scheduled to an edge node are proportional
    to the task arrival at the edge node, with the ratio identical among all the edge
    nodes. 6.1.3 Metrics In the simulation results, we take the average response time
    in T time slots, i.e., 1 T ∑ t=0 T−1 D(t) , as the metrics of system performance,
    and the average system cost in T time slots, i.e., 1 T ∑ t=0 T−1 C(t) , as the
    metrics of system cost. 6.2 Efficiency and Effectiveness Evaluation We compare
    WiDaS with the benchmark algorithms by conducting simulations under the three
    traffic patterns. The results demonstrate that WiDaS shows two-fold benefits of
    high efficiency and effectiveness. We first illustrate the average results of
    different algorithms in Fig. 4. To further illustrate the details of time-varying
    response time and system cost, we present the results over 100 time slots in Fig.
    5. Fig. 4. Results of algorithms under different task arrival patterns. Show All
    Fig. 5. The response time and system cost in different time slots: random task
    arrival pattern, V=5 , θ=1 dollar/h. Show All 6.2.1 Efficiency The simulation
    results show that WiDaS can achieve the approximate results with the KKT-based
    algorithm while reducing the execution time significantly. As shown in Figs. 4
    and 5, in terms of both response time and system cost, WiDaS has the approximate
    results with the KKT-based algorithm. In the simulation under the trace-driven
    traffic pattern, WiDaS has slightly higher response time and system cost than
    the KKT-based algorithm. This is because under the trace-driven task arrival pattern,
    mobile task arrivals at edge nodes are much more fluctuated with time. Removing
    the heterogenous computation capacity constraints (i.e., the inequation constraints
    in P2) of edge nodes results in performance degrade in the WiDaS algorithm. According
    to the complexity analysis in Sections 5.1 and 5.2, the KKT-based algorithm has
    an exponential complexity while WiDaS has a polynomial complexity. We have also
    tested the execution time by repeating for 100 times, and the WiDaS algorithm
    can complete in 9.2ms in each time slot while the KKT-based algorithm requires
    902.5ms (Surface laptop 2018 version, 13.5-inch, MATLAB R2017b). Therefore, WiDaS
    has the benefit of high efficiency. 6.2.2 Effectiveness The simulation results
    in Fig. 4a demonstrate that WiDaS can reduce the average response time by 24.1%-64.4%
    over the Fair-ratio algorithm and 12.7%-47.2% over Edge-first algorithm with a
    reduced system cost. In terms of response time, compared with the Fair-ratio algorithm,
    WiDaS reduces the average response time by 24.1%, 37.7%, and 64.4% under the random,
    normal, and trace-driven task arrival pattern, respectively. In the WiDaS algorithm,
    the heterogeneity of edge computation capacities is taken into consideration and
    the loads of edge nodes can be well balanced. Thus, the average response time
    can be effectively reduced over the Fair-ratio algorithm. Compared with the Edge-first
    algorithm, WiDaS reduces the average response time by 47.2%, 12.7%, and 33.4%
    under the three task arrival patterns. In the Edge-first algorithm, the computation
    delay at each edge node is highly dependent on the task arrival rate in each time
    slot, thus the response time fluctuates greatly with time, as shown in Fig. 5b.
    The WiDaS algorithm can take advantage of cloud resource agility to accommodate
    the dynamic tasks at edge nodes and can fully utilize the system resources through
    task migration among unbalanced edge nodes. Therefore, WiDaS can effectively reduce
    the response time over the Edge-first algorithm. In terms of system cost, WiDaS
    can maintain the average system cost under the budget and can effectively reduce
    the average system cost over the Fair-ratio algorithm, as shown in Fig. 4b. The
    Fair-ratio algorithm does not consider the heterogeneity of edge computation capacities,
    and thus can not fully utilize the computation capacities of edge nodes, leading
    to high outsourcing cost. The Edge-fist algorithm processes mobile tasks at edge
    nodes with higher priority and only uses cloud resources to process the excessive
    tasks from edge nodes. Thus, the usage of cloud resources is highly dependent
    on the arrival rates of mobile tasks, and the system cost of the Edge-first algorithm
    varies greatly with time, as shown in Fig. 5b. When edge nodes can provide sufficient
    computation capacities for mobile tasks, the Edge-first algorithm has lower system
    cost, e.g., under the normal traffic pattern in Fig. 4b. Otherwise, the Edge-first
    algorithm has higher system cost than the WiDaS algorithm, such as under the trace-driven
    traffic pattern in Fig. 4b. 6.3 Performance-Cost Tradeoff We experimentally show
    the relationship between the average response time and the system cost, helping
    system administrators to properly trade off the system performance and cost. According
    to the conclusion of Theorem 1, the performance-cost tradeoff relies on the weight
    ratio V . In this section, we first illustrate the average response time and system
    cost with changing V in Fig. 6. Then we show the relationship between the response
    time and system cost by changing the budget limit in Fig. 7. Fig. 6. Performance-cost
    tradeoff with V , θ=2 dollar/h. Show All Fig. 7. The average results of performance-cost
    tradeoff with 95% confidence interval by changing θ , V=5 . Show All Our Simulation
    Results Show That the Average System Cost of WiDaS Increases With the Weight Ratio
    V While the Response Time Decreases With V . In the Edge-first and the Fair-ratio
    algorithm, the usage of cloud resources and task scheduling results mainly rely
    on mobile task arrivals and edge computation capabilities, thus the system cost
    and the response time do not change significantly with V . When V is small, the
    WiDaS algorithm decreases the usage of cloud resources to reduce the system cost.
    Most mobile tasks are processed at edge nodes, incurring high response time. As
    V increases, the response time becomes more dominant in the objective. To reduce
    the response time, WiDaS outsources more tasks to the cloud, resulting in high
    system cost. The Simulation Results Demonstrate That the Average Response Time
    of WiDaS Decreases With the Budget Limit θ While the Average System Cost Increases
    With θ . Fig. 7 shows the average results of performance-cost tradeoff with 95%
    confidence interval. Note that as the we obtain the results with the Monte Carlo
    method [52], i.e., simulations are repeated with the edge task arrivals and computation
    capacities uniformly distributed among certain value ranges in each simulation,
    the ranges of the results are directly influenced by the value ranges of the edge
    computation capacities and task arrivals. In these simulations, the value range
    of task arrivals and computation capacities is large, thus, the results with 95%
    confidence interval are also significantly scattered. It can be observed that
    when the budget limit is significantly high (larger than 1.5 dollars/hour), the
    average system cost no longer increases and the average response time can not
    be further reduced. This is because when the budget limit is high, sufficient
    cloud computation resources can be tenanted. The communication delay among edge
    nodes or from edge nodes to the cloud becomes the bottleneck to reduce the average
    response time. Tenanting more cloud instances can not further reduce the response
    time. SECTION 7 Conclusion In this paper, we have solved dynamic task scheduling
    in the cloud assisted mobile edge computing system to facilitate the functionality
    of the MEO in the ETSI MEC architecture. The problem has been formulated as a
    dynamic optimization problem with queuing analysis, aiming at optimizing the average
    task response time within the resource budget limit. We have proposed the WiDaS
    algorithm to solve this problem, which is proved to have polynomial computation
    complexity. Extensive simulations have been conducted to evaluate WiDaS under
    different task arrival patterns, and the results demonstrate that WiDaS shows
    two-fold benefits of high efficiency and effectiveness. For the future work, we
    will investigate the workload scheduling problem in the cloud-edge-device system,
    in which the capabilities of mobile devices are also taken into consideration.
    Authors Figures References Citations Keywords Metrics More Like This P2PCompute:
    A Peer-to-peer computing system 2007 International Symposium on Collaborative
    Technologies and Systems Published: 2007 Ad hoc Grid: An Adaptive and Self-Organizing
    Peer-to-Peer Computing Grid 2010 10th IEEE International Conference on Computer
    and Information Technology Published: 2010 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Mobile Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Dynamic Task Scheduling in Cloud-Assisted Mobile Edge Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Melancon C.
  - Kaur K.
  - Gascon-Samson J.
  - Saad M.
  citation_count: '1'
  description: The global COVID-19 pandemic has put a strain on the healthcare system,
    further compounded by the aging population and the staffing shortage. As a result,
    the demand for healthcare exceeds the available offer, and health professionals
    are forced to compensate the best they can. On the other hand, some tasks can
    easily be offloaded to robots. Some solutions already exist, but they are not
    necessarily scalable, and exhibit a very high price point. This paper outlines
    our vision of a new distributed robotics approach to mitigate these constraints.
    We propose a prototype of an autonomous robot to assist healthcare professionals
    (e.g., nurses) in their work - for instance, by gathering equipment and delivering
    water and food to give them more time for human-centric tasks. The solution incorporates
    the ideas of cloud-fog-edge robotics to enable AI-based control with digital twin
    notions to help with the training. Preliminary work is introduced on the robot
    platform and the framework to facilitate communication at every infrastructure
    layer.
  doi: 10.1109/PEDS57185.2023.10246745
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2023 IEEE 14th International ... Towards
    Smart Distributed Robotics Solution using Digital Twin Publisher: IEEE Cite This
    PDF Cédric Melançon; Kuljeet Kaur; Julien Gascon-Samson; Maarouf Saad All Authors
    106 Full Text Views Abstract Document Sections I. Introduction II. Related Work
    III. Objectives IV. Mobile manipulator platform V. Global architecture Show Full
    Outline Authors Figures References Keywords Metrics Abstract: The global COVID-19
    pandemic has put a strain on the healthcare system, further compounded by the
    aging population and the staffing shortage. As a result, the demand for healthcare
    exceeds the available offer, and health professionals are forced to compensate
    the best they can. On the other hand, some tasks can easily be offloaded to robots.
    Some solutions already exist, but they are not necessarily scalable, and exhibit
    a very high price point. This paper outlines our vision of a new distributed robotics
    approach to mitigate these constraints. We propose a prototype of an autonomous
    robot to assist healthcare professionals (e.g., nurses) in their work – for instance,
    by gathering equipment and delivering water and food to give them more time for
    human-centric tasks. The solution incorporates the ideas of cloud-fog-edge robotics
    to enable AI-based control with digital twin notions to help with the training.
    Preliminary work is introduced on the robot platform and the framework to facilitate
    communication at every infrastructure layer. Published in: 2023 IEEE 14th International
    Conference on Power Electronics and Drive Systems (PEDS) Date of Conference: 07-10
    August 2023 Date Added to IEEE Xplore: 12 September 2023 ISBN Information: ISSN
    Information: DOI: 10.1109/PEDS57185.2023.10246745 Publisher: IEEE Conference Location:
    Montreal, QC, Canada SECTION I. Introduction In the last few years, the healthcare
    sector has started to use robots to address specific needs, such as transporting
    samples. Given the staffing shortage, adding more robots could be beneficial.
    Assistive robots, which can perform tasks that imply human interaction, also have
    a role to play, which helps free more time for professionals to focus on human-based
    tasks. Through discussions with stakeholders, robots are simultaneously the solution
    and the problem, as a hospital is not a factory with multiple robotics specialists
    to maintain the robots; i.e., maintaining robots can require specialized knowledge
    that has yet to be available in healthcare. Another concern is the price of such
    solutions, as autonomous robots can require very high computing power, given that
    many algorithms are now performed through artificial intelligence (AI) models
    that require a large amount of memory and Graphics Processing Units (GPU) capabilities.
    Running all computational tasks directly on the robot can be costly, primarily
    due to the miniaturization of the electronic components that must fit within the
    robot’s body. Such a solution can be expensive just for the robotic platform itself.
    Introducing concepts such as Cloud Robotics [1], Fog Robotics [2], and Edge Robotics
    [3], enables the deployment of some of the computational tasks outside the robot,
    which reduces the need for resources, and eventually, the cost. Further, it allows
    sharing of resources when multiple robots are involved. Also, reducing the complexity
    of the robot reduces the probability of component failure, which increases the
    robot’s reliability. Edge and fog infrastructures are well-known for Information
    Technology (IT) services, as they rely on similar networking and server-based
    technologies that are used in the industry [4]. The cloud can be used in a managed
    way, which removes the maintenance aspect from the end-user. This work proposes
    a vision for an infrastructure inspired by an Edge/Fog/Cloud-driven robotic solution.
    Such a solution also comes with associated constraints such as latency, critical
    timing, and resiliency. Another challenge is that the robot must evolve indoors
    on multiple floors that cover a vast surface. Although wireless connectivity is
    a good option, connection drops are possible, which can be problematic for the
    robot since some processing is performed outside its local network. Therefore,
    the robot must support transient connection failures. SECTION II. Related Work
    There are already robots in hospitals that perform various tasks. In [5] and [6],
    the authors describe the use of robots in healthcare facilities and the challenges
    associated with such solutions, including the agility to navigate in crowded environments.
    Some robots are specialized for heavy object lifting like the Robear [7] or sample
    delivery like the SpeciMinder [8]. Other robots are responsible for more generic
    usage, like the robot Lio [9], and Moxi [10]. All the previous robots listed have
    a common ground: they are self-contained, meaning that the whole computational
    process is hosted on the robot. This is good for solitary robots that focus on
    their tasks, but it misses the global picture when fleets of robots are deployed.
    Also, the price of the robot platform is higher since it holds all the required
    resources to perform all of the tasks (CPU, GPU, memory, etc.), which can also
    be redundant. Further, under this architecture, a complex task execution requiring
    multiple robots would need all of the robots to synchronize between themselves.
    Some distributed robotic approaches have been proposed over the past years, which
    include Cloud, Fog, and Edge Robotics. The authors of [1] introduce the concept
    of Cloud Robotics and justify its usage in human-robot interaction. They also
    admit that when real-time control is needed, solutions based solely on Cloud Robotics
    need the addition of edge computing. Edge Robotics is discussed in [3], where
    it compensates for the Cloud gaps. The authors also outline some challenges related
    to using edge computing: such as safety and availability. When the robot is missing
    data to control (due to data corruption or loss of connectivity), the robot needs
    to ensure the environment’s safety. They propose a lightweight checker to monitor
    such problems and perform a fail-stop maneuver to address them. With collaborative
    mobile robots, this type of maneuver can be complex. To address this, the redundancy
    concept, borrowed from the aerospace industry, can be implemented [11]. A less
    precise and lesser complexity algorithm can then take over to ensure safety. Another
    concept, Fog Robotics [2], proposes a hybrid solution between Cloud and Edge Robotics.
    It relieves the edge nodes of some of the heavy computation while solving the
    latency issue using cloud computing. The authors also identify some challenges
    to Fog Robotics, such as distributed orchestration and computing elasticity. These
    are addressed by the proposed solution with a flexible constraint-based orchestration
    that distributes the process based on several criteria, such as criticality and
    the amount of available resources, among others. SECTION III. Objectives The objectives
    of our robotics solution, which are mainly based on the constraints of its use
    in a healthcare environment, are as follows: Low-cost and low maintenance: removing
    the complexity from the robot to offload it to robust computing nodes. Scalability
    for multiple robots: through a distributed system with microservices architecture
    Ability to perform various tasks with low payload: using AI to train the robot
    for task execution Ability to perform real-time processing: using an intelligent
    workload distribution to ensure timings Resilience: flexible workload distribution
    based on available resources and redundancy of basic control on the robot To our
    knowledge, the robots proposed in prior work only cover some of these objectives
    in one solution. In this paper, we propose a multi-layer architectural vision
    of a robot that meets the aforementioned objectives, and we evaluate the digital
    twin concept and the integration of AI models in the data pipeline. SECTION IV.
    Mobile manipulator platform The mobile platform used for the project is a Pioneer
    3-AT from Adept, a subsidiary of Omron [12]. It is a legacy robot, which is no
    longer supported, with limited hardware documentation. Since the mechanical part
    of the robot is powerful, it was decided to reuse it and modernize it to be more
    performant and to adapt it to the distributed framework proposed for this paper.
    Most electronic components were replaced with a modern solution to help achieve
    the project’s goal. The following subsection describes these changes, including
    adding some sensors. A Gen3 Lite manipulator from Kinova will be used for the
    manipulator portion; it is not yet integrated. With these two components, it will
    eventually be possible to develop a distributed robotics solution that can be
    used as an autonomous robot to assist healthcare professionals. A. Pioneer 3AT
    mobile platform Figure 1 describes the building blocks of the new Pioneer 3-AT
    robot. The batteries, motors, encoders, and sonars (along with its board) are
    reused from the original robot. The main controller is now an OpenCR from the
    company Robotis [13]. It includes a 32-bit ARM Cortex®-M7 with a Floating-Point
    Unit microcontroller at 216MHz and multiple digital and analog I/O to interface
    with other components. A 9DoF Inertial Measurement Unit (IMU) is also included
    on this board for the robot’s localization. The motor encoders are connected to
    a Dual LS7366R Quadrature Encoder Buffer from SuperDroid Robots [14], which are
    accessible through Serial Peripheral Interface (SPI). Sonar signals from the two
    (front and rear) boards are connected directly on the OpenCR. To have more flexibility
    for the motor control development, the Pololu Dual G2 High-Power Motor Driver
    24v14 Shield for Arduino [15] is mounted on an Adafruit Grand Central M4 Express
    [16] control board. The shield drives the two motors on the same side, and the
    configuration is doubled to control each side of the robot. The Motor Driver and
    the OpenCR communicate through the inter-integrated circuit (I2C) protocol. A
    differential-drive robot configuration like the P3-AT robot is limited by a non-holonomic
    constraint that prevents lateral movements [17]. This constraint is nonlinear,
    making such robots’ position stabilization more difficult [18]. The mobile robot
    is designed to operate in dynamic unknown environments, which complicates the
    modeling and prediction of the robot’s motion. Furthermore, the effect of non-linearities,
    uncertainties, and external disturbances on the mobile robot can benefit from
    a robust non-linear controller [19]. The Pololu driver enables this type of control
    through the current sensors for each motor. Finally, a new printed circuit board
    was designed to manage the batteries since the power board documentation was unavailable.
    The robot can hold up to three batteries in parallel to increase its autonomy.
    Current and voltage sensors are added to the board to monitor the energy used
    for the decision process to balance the workload over the entire solution. B.
    Improvements Figure 2 represents the actual configuration of the robot that includes
    edge capabilities. The first level adds a Jetson Orin AGX [20] board responsible
    for gathering the information from the OpenCR (through USB serial connectivity)
    and all external sensors. ROS2 is installed on the gateway to help integrate the
    sensors and robotics processes. The second level is a network switch that enables
    all ethernet devices on the gateway, namely the manipulator arm (to be added)
    and the Lidar. The third level is for the Ultra-Wideband (UWB) tag used for indoor
    localization. The last level is for the Light Detection and Ranging (Lidar) sensor
    (an Ouster OS1 [21]) and the RGB/Depth Camera (Intel Realsense d435i [22]). Fig.
    1. New Pioneer 3-AT components Show All To compensate for the fact that the robot
    is limited in resources, the edge gateway is used to forward sensor data over
    a WiFi 6E link, and process commands to perform. Some data processing can also
    be done on the Jetson to reduce the traffic over the network. SECTION V. Global
    architecture The architecture is divided into four layers (Figure 3): the device,
    Edge, Fog, and Cloud. This section discusses the details of each layer. A. Device
    Layer The device layer includes all of the robot’s internal electronics, such
    as the OpenCR, the Motor Driver boards, and the sensors. It is minimal in computing
    power and memory and lacks GPU capability. Many robotics solutions use the DYNAMIXEL
    Protocol 2.0 from Robotis [23] to connect the two layers via USB. The communication
    speed is breakneck compared with other serial protocols. The baud rate used is
    1Mbps. The device layer is responsible for the following tasks: Motor Control
    based on desired velocities (linear and angular) Provide the internal sensor data
    to the next layer B. Edge Layer The edge layer is divided into two parts. The
    first one is external to the robot, with the Jetson gateway connected to the external
    sensors. It has good computing capabilities with its 12-core ARM Cortex®-A78AE
    CPU with 32GB RAM and 2048-core NVIDIA GPU. A 500GB, NVMe SSD drive is also available.
    The original wireless controller was replaced to enable the 802.1ax protocol,
    which has more advantages like higher data transfer speeds and data rate and lower
    power consumption, which helps the battery life. This edge node will perform the
    following tasks: Run the ROS2 node for internal communication. Forwards the external
    sensor data to the next layer Perform basic sensor fusion and filtering Expose
    the rest of its resources available for the distribution of the solution’s services
    Fig. 2. New Pioneer 3-AT with sensors Show All A mesh network topology will be
    used for wireless communication to have a fluid transition from one mesh node
    to another. On each mesh node, a small gateway will be installed with a Jetson
    Xavier NX computer representing the edge layer’s second part. Each node will be
    distributed on each floor to ensure the wireless coverage is complete throughout
    the building. The objective is to be able to run some services based on the proximity
    of devices and available resources. These edge nodes will not have specific tasks
    assigned. They will wait for task assignments by the load balancer based on the
    available resources. C. Fog Layer The Fog layer is located in the GREPCI-Robotics
    lab room for the development phase. For this project, we have two servers with
    a total of 48 CPU cores, 384 GB of RAM, 8TB SSD, and 40TB HDD. Another compute
    node with GPU has 24 CPU cores, 256GB of RAM, 4TB of SSD, and 48GB of GPU memory.
    Because of the large resource pool, this layer will perform most of the processes.
    The following tasks will be the responsibility of the fog layer: Gather all the
    data from the previous layers in a database. Monitor the health of the whole solution
    Optimize the process distribution based on criteria (e.g., criticality, timing,
    node load, etc.) Perform resource-intensive tasks based on available resources
    Offload tasks that are not possible to run at this layer to the cloud Fig. 3.
    Distributed robotics high-level architecture. Show All D. Cloud Layer Finally,
    the Cloud layer can be hosted in any cloud provider (Azure, AWS, Google). The
    solution is built in a way agnostic of the hosting company. A Kubernetes cluster
    will include all the nodes from every layer to manage the process distribution
    easily. Because of the latency induced by the distance between the data center
    and the robot, no critical task will be run from this layer. The objective is
    mainly to have a way to offload processes in case of unusual demand on the solution
    and to gather data that can be used in the case of multi-site solutions to manage
    the solution. The cloud can also be leveraged to train AI models when the model
    complexity or the volume of data is too much for the internal infrastructure.
    SECTION VI. Components To achieve the objectives of the robot, multiple components
    are required. This section describes each of them and how they contribute to the
    global solution. A. AIoT Platform The AI for IoT (AIoT) platform is at the solution’s
    core and is currently under development. It is responsible for managing the data
    flow from the device through all layers. Each layer has a database to keep data
    for visualization, eventual training, or buffer if the connectivity to the next
    layer is lost. The database is a No-SQL type that can store any data, including
    image and JSON content. FairCom Edge database is used for the edge nodes, and
    FairCom DB [24] is used for the Fog and Cloud layers. FairCom Edge comes with
    a Message Queue Telemetry Transport (MQTT) broker that is very powerful, implements
    a security layer over the MQTT protocol, and provides data ordering guarantees,
    which is essential for the use case. The motivation for this technology is that
    it is small and very efficient for edge devices and easy to maintain, and scalable
    for Fog and Cloud. The onboarding of a new robot is done through the platform
    along with the update process for services that are required by it. The platform
    is also in charge of managing the configuration. The metadata emerging from this
    configuration will help with the planning of resources. Another essential task
    of the platform is monitoring the solution, which is done by periodically detecting
    faults and monitoring the resource usage (CPU, GPU, memory, disk, and energy),
    if applicable. The goal is to balance the work over all the layers based on resource
    availability, the criticality of the service, and the timing requirements. Resource
    optimization will be done by keeping track of the device’s battery to ensure enough
    power to complete the task. The system will offload, if needed, some computation
    from the robot to save energy and may also simplify the task (e.g., by switching
    from an AI solution to a classical one that may be less efficient but needs less
    energy) to be able to reach the charging station. B. Digital twin simulator AI
    models will underpin many aspects of the autonomous robot. Reinforcement Learning
    (RL) techniques, which learn reactions based on the environment, will be used
    to learn how to navigate and avoid obstacles. RL will also be used for grasping,
    i.e., for training the robot to place itself and hold various object types. RL
    needs a lot of training, but doing it on a real robot can be impractical as it
    can require considerable time. Simulation is an excellent way to accelerate the
    training process; however, ensuring that it maps well to reality is a challenge.
    Techniques like transfer learning can help when switching from simulation to the
    real agent, as described in [25]. For this project, we use a digital twin simulator.
    The dynamics simulation is done through Vortex from CM-Labs. Visualization is
    covered by Unity integrated with Vortex. In [26], the authors use Vortex to validate
    reinforcement learning training over a simulator. We plan to implement a new approach
    to enrich the simulation based on the observation from the real system. This will
    close the simulation loop, improving the training performance and reducing the
    need to retrain the model on the real system. Vortex has a plugin to integrate
    with ROS2 that will help to integrate the simulator with the whole infrastructure.
    The simulation will run in parallel with the real robot to learn how to improve
    the sensors to reduce the gap between the digital and real world. Fig. 4. Deep
    Learning Localization model. Show All C. Robotic Solution The robot is fully integrated
    with ROS2, a widely used robotics framework. Most sensors are already available
    as packages and are easy to integrate. Integration of the manipulator will also
    be done in ROS2. Unfortunately, only the ROS1 package is available, so a migration
    to ROS2 is required. A camera will also be added to the robot’s wrist to help
    with object manipulation. Since the control of the robot is done outside of the
    robot, it has to be resilient to a potential loss of connectivity. Given its limited
    resources and depending on what task it is currently doing, a lighter version
    of the control will take over to ensure the safety of people. If the robot manipulates
    an object, a traditional controller will take over to safely put it back on a
    flat surface if the connection is not recovered quickly. If the robot navigates
    to another location, more traditional methods will be used instead of relying
    on AI for navigation and perception at the cost of reduced precision and flexibility,
    but the normal process will take over as soon as the connectivity is restored.
    Given that ensuring the safety of people is paramount, the robot will attempt
    to safely avoid any potentially dangerous situation or stop the robot altogether
    if it is safer to do so. The Jetson gateway will be responsible for detecting
    and managing this transition. SECTION VII. Case Study For now, the conversion
    of the mobile platform is completed except for the battery monitoring circuit.
    The robot is all integrated with ROS2, and all the data for all sensors can be
    accessed. Regarding the platform, all the data flowing from the device through
    all system layers is operational and is ingested in a database at each node. We
    perform a validation of the integration of the AI models within the platform,
    following a new localization model inspired from [27] that uses a camera and an
    IMU. The model is represented in Fig. 4. An over-fitting issue is still affecting
    the model, but it seems promising. Since localization controls the robot, it has
    to run at least at a rate of 30Hz, which will eventually become possible by optimizing
    the data pipeline (at the time being, without optimization, the rate is around
    20Hz). We use the MIT Stata Center Dataset described by [28] to train the model.
    In Fig. 5, we can see that the training process gives good results – the error
    is in the 15-20cm range, as shown in Fig. 6 and Fig. 7. Fig. 5. Localization result
    based on the Camera and IMU. Show All Fig. 6. Localization error in X. Show All
    Fig. 7. Localization error in Y. Show All Conclusion The proposed vision of our
    architecture solves the main objectives to help deliver an autonomous robot to
    assist nurses in their work. With the target of reducing the solution’s price
    and maintenance complexity, the robot’s intelligence is partially moved towards
    Edge and Cloud computing layers through a dynamic AIoT platform that distributes
    the computational load based on various criteria. We modernize the Pioneer 3-AT
    mobile robot to adapt to the new framework by adding an Edge node. As a case study,
    we present an overview of a cloud-fog-edge AI-based localization model. We perform
    a summary evaluation of that model, and we find that it provides a high localization
    accuracy. The overarching goal, which we will further develop in future work,
    is a fully-fledged cloud-fog-edge robotics solution that will be able to execute
    simple tasks such as food and drink delivery, or bringing equipment to a location,
    giving nurses more time to focus on human interaction with the patients. Authors
    Figures References Keywords Metrics More Like This Integration of IoT, Edge Computing
    and Cloud Computing for Monitoring and Controlling Automated External Defibrillator
    Cabinets in Emergency Medical Service 2019 5th International Conference on Information
    Management (ICIM) Published: 2019 Simulation of Image-Guided Microwave Ablation
    Therapy Using a Digital Twin Computational Model IEEE Open Journal of Engineering
    in Medicine and Biology Published: 2024 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the International Conference on Power Electronics and Drive
    Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Towards Smart Distributed Robotics Solution using Digital Twin
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Fang Q.
  - Liu L.
  - Fang P.
  - Zheng M.
  citation_count: '0'
  description: To strengthen the safety protection of personnel in deep water dangerous
    areas such as reservoirs and rivers, Relying on the technical characteristics
    of 5G network technology, such as high speed, low delay, high bandwidth, and high
    flexibility, Build a video surveillance network architecture; The cloud-edge collaboration
    technology based on 5G network sinks the cloud intelligent analysis and computing
    power to the edge side, The edge end realizes the comprehensive perception and
    real-time analysis of video data, Accelerate the analysis of real-time data, Form
    a scientific and effective decision-making and intelligent analysis; The cloud
    is responsible for storing the high-value data uploaded on the edge side, And
    applied the data to the training of the deep-learning models, Finally, the trained
    model is distributed to the edge terminal; In this paper, by replacing the SiLU
    activation function of the YOLO V5s original model as the Mish activation function,
    Improve the detection and identification rate of remote personnel in the video
    surveillance area; Improving the weighting factor for non-maximum suppression
    is for the CIoU function, Further improve the positioning and identification of
    personnel in the image, Finally, realize the detection and identification of personnel
    in dangerous areas and remote alarm, Avoid safety accidents.
  doi: 10.1109/ICPICS58376.2023.10235701
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2023 IEEE 5th International C... Research
    on Dangerous Area Personnel Detection Algorithm Based on 5G Cloud Edge Collaboration
    Publisher: IEEE Cite This PDF Qinguo Fang; Limei Liu; Peng Fang; Mingzhao Zheng
    All Authors 34 Full Text Views Abstract Document Sections I. Introduction II.
    Based on 5g Cloud-Side Collaborative Video Surveillance Network Architecture Design
    III. The Improved Yolo v5s Model IV. Study Protocol and Analysis of the Results
    V. Conclusion Authors Figures References Keywords Metrics Abstract: To strengthen
    the safety protection of personnel in deep water dangerous areas such as reservoirs
    and rivers, Relying on the technical characteristics of 5G network technology,
    such as high speed, low delay, high bandwidth, and high flexibility, Build a video
    surveillance network architecture; The cloud-edge collaboration technology based
    on 5G network sinks the cloud intelligent analysis and computing power to the
    edge side, The edge end realizes the comprehensive perception and real-time analysis
    of video data, Accelerate the analysis of real-time data, Form a scientific and
    effective decision-making and intelligent analysis; The cloud is responsible for
    storing the high-value data uploaded on the edge side, And applied the data to
    the training of the deep-learning models, Finally, the trained model is distributed
    to the edge terminal; In this paper, by replacing the SiLU activation function
    of the YOLO V5s original model as the Mish activation function, Improve the detection
    and identification rate of remote personnel in the video surveillance area; Improving
    the weighting factor for non-maximum suppression is for the CIoU function, Further
    improve the positioning and identification of personnel in the image, Finally,
    realize the detection and identification of personnel in dangerous areas and remote
    alarm, Avoid safety accidents. Published in: 2023 IEEE 5th International Conference
    on Power, Intelligent Computing and Systems (ICPICS) Date of Conference: 14-16
    July 2023 Date Added to IEEE Xplore: 05 September 2023 ISBN Information: ISSN
    Information: DOI: 10.1109/ICPICS58376.2023.10235701 Publisher: IEEE Conference
    Location: Shenyang, China SECTION I. Introduction With the popularity of road
    trips, outdoor fishing and outdoor swimming, More and more tourists go to the
    beautiful scenery, lakes and mountains, The scenery is good but the dangerous
    waters of streams, reservoirs, There are great security risks, Traditional manual
    inspection and video surveillance cannot detect the personnel situation in dangerous
    areas in time, Moreover, the traditional video surveillance system has the problems
    of high video transmission delay, high bandwidth and high cost of monitoring line
    laying, With the commercialization and continuous deployment of 5G network technologies,
    5G network technology with large bandwidth, large connection and low latency and
    other characteristics of the technology began to highlight the [1], The introduction
    of 5G private network technology can effectively solve the transmission delay
    problem faced by the traditional video surveillance network; Use the 5G network
    to upload the surveillance video streaming data to the cloud service [2], Massive
    data requires increasing cloud storage and video analysis and processing capabilities,
    Lead to a cloud cost increase of [3], This paper is based on 5G network integration
    cloud edge collaboration technology, Sink the cloud computing power to the edge
    of the network, The edge side can realize real-time analysis of video data and
    upload high-value data to the cloud, Cloud services optimize the deep learning
    model based on high value data and distribute to the edge terminal [4]; About
    the choice of deep learning algorithms for target detection deployed at the edge
    end, By comparing the detection effect and efficiencies of the single-stage target
    detection algorithm (YOLO, SSD, RetinaNet) and the multi-stage target detection
    algorithm (Fast RCNN, Faster RCNN) [5], Although the multi-stage target detection
    algorithm has better identification accuracy and lower omission rate, However,
    the slower speed is not suitable for the edge-side fast response application scenario
    [6], However, the single-stage target detection algorithm YOLO V5s model can detect
    the established target category and target location in the field of the image,
    By improving the Mish activation function of the model and the Detect module bounding
    box regression mechanism [7], Can greatly improve the target detection speed and
    the recognition rate of distal targets, To facilitate the rapid analysis of the
    personnel in the danger area, The detection algorithm based on 5G network can
    quickly realize the summary of video and image data area personnel information
    and intelligent alarm push, It has a good application prospect in areas with high
    safety requirements such as illegal intrusion in dangerous areas. SECTION II.
    Based on 5g Cloud-Side Collaborative Video Surveillance Network Architecture Design
    As shown in Fig 1 of the design, the 5G network is mainly used for the transmission
    of video data and structured data between cloud service and edge devices, exploiting
    the advantages of 5G network to improve the transmission efficiency; the CPE device
    at the edge connects to the 5G network, intelligently distributes the request
    data and upload the target detection information to the cloud in the form of structured
    data; the cloud service safely stores massive data, training, optimization and
    release of deep learning model. Fig. 1. Cloud-edge collaborative video surveillance
    network architecture based on 5G network Show All In order to better verify the
    characteristics of large bandwidth and low delay of the video network architecture,
    the laboratory has built a 5G system based on software definition, supporting
    SA and NSA, architecture 5G networking mode, and open RF front-end transmission
    interface supporting customization, which can effectively simulate the 5G network
    in actual application scenarios, as shown in the Figure 2: Fig. 2. The architecture
    of the 5G system based on the software definition Show All SECTION III. The Improved
    Yolo v5s Model YOLO V5s As a model with relatively small network depth and feature
    map width in the YOLO V5 family, the model can have a relatively good target detection
    effect under the specific computing force of the edge computing equipment. The
    algorithm model structure is mainly composed of Backbone, Neck and Head, and the
    overall algorithm structure is shown in the Figure 3: Fig. 3. YOLO V 5s model
    structure Show All Backbone module is mainly composed of ConvBNSiLU and C3 modules,
    whose main function is to extract image feature information; Nk module (SPPF)
    is a spatial pyramid pooling, which can realize the transmission of target feature
    information of different sizes, reduce the SPP model and improve the model speed.
    The Head module performs multiscale object detection on the feature graph extracted
    from the backbone network and improves the accuracy of the network detection by
    NMS. A. Optimize the Activation Function YOLO V5s In the original model, the SiLU
    activation function is used for the nonlinear transformation of the neural network
    model, which has the characteristics of smooth, non-monotone, unbound and lower
    bound. Silver function increases the network stability by using the smoothness
    and reduces the risk of overfitting. Meanwhile, the change of input value and
    noise are relatively robust, which can reduce the influence of data outlier on
    the model to some extent. The SiLU activation function is calculated by the following
    formula: SiLU(x)= x ∗ sigmoid(x) sigmoid(x)=1/(1+exp(−x)) (1) (2) View Source
    Replace the activation function of the YOLO V5s original model with the Mish activation
    function, Mish and SiLU activation functions have the calculated amount of the
    same equivalent, But with the same computational amount, Mish activation function
    showed better activation, And it is concluded that Mish is 0.494% better than
    SiLU, Was 1.671% [8] higher than the ReLU, The contrast curves of the SiLU, ReLU
    and Mish activation functions are shown in Figure 4, The smoothness of the Mish
    activation function is somewhat better than the SiLU function. Fig. 4. Activation
    Function (SiLU, Mish, ReLU) Show All The mathematical expression of the Mish activation
    function is shown in formula (3), which contains the tanh function. The shape
    of the tanh function is similar to the sigmoid function, but the tanh function
    is symmetric at the origin of the coordinate system. f(x)= x ∗ tanh(ln(1+ e x
    )) tanh(x)= 1−exp(−2x) 1+exp(−2x) (3) (4) View Source B. Non-Maximum Suppression
    Improvement YOLO V5s In the original model, non-maximum suppression (NMS) is used
    to detect target edge detection and portrait detection of images. NMS is introduced
    to solve the problem of generating multiple candidate boxes for one target. For
    the screening of many target boxes, weighted NMS operation is used, so as to obtain
    the optimal target box. YOLO V5s In the original model, the weighted NMS is the
    weighted average of coordinates, and the weighted average object is the adjacent
    box of the current candidate box with the highest score and the IOUNMS threshold;
    the weighting factor is IoU, only considering the overlapping area of the two
    target boxes, the problem of positioning and score inconsistency [9], formula
    (5) is the weighted factor IoU, the loss function, is the pixel area of the two
    bounding boxes. A B IoU(A,B)= |A⋃B| |A⋂B| (5) View Source In this paper, the CIOU
    loss function is used to replace the ordinary IOU evaluation strategy. The CIoU
    loss function is gradually evolved from IoU, GIoU and DIoU. GIoU increases the
    attention of the non-overlapping area and can better response coincidence. DIoU
    can minimize the distance between the two target boxes, which increases the convergence
    rate compared with GIoU in calculating the area between the two target boxes.
    CIoU=IoU−( ρ 2 (b, b gt ) c 2 +αv) v= 4 π 2 (arctan w gt h gt −arctan w h ) 2
    α= v (1−IoU)+v (6) (7) (8) View Source β is the Euclidean distance between b and
    b gt , b parameter representing coordinate of the prediction center, b gt parameter
    representing the center of the real target boundary box, β 2 is the square of
    the distance between the two center points, C is the minimum diagonal length of
    the two rectangles, α and v is the aspect ratio, w and h is represents the height
    of the prediction box, w gt and h gt is the height of the real box respectively.
    Formula (6) is the CIoU function, which not only considers the distance between
    the center point of the two candidate boxes, but also increases the loss of the
    detection box scale and the loss of length and width, so that the prediction box
    will be more consistent with the real box. In this paper, the CIoU loss function
    replaces the IoU function in the YOLO V5s original model weighted NMS, which can
    effectively improve the detection rate of the portrait target in the remote region.
    SECTION IV. Study Protocol and Analysis of the Results A. Construction of Video
    Surveillance Network Architecture The laboratory has built a software-defined
    5G-based system, Adopt the multi-node method to simulate the 5G network environment;
    The CPE devices are connected to the cloud and the edge side based on the 5G network,
    Connecting the edge computing equipment and the equipment monitoring equipment
    simultaneously, Upload the high-value data acquired at the edge side; 16 Taivision
    or TV surveillance cameras used for video surveillance, Two edge computing devices
    and one cloud server to jointly build a 5G cloud edge collaboration video network
    architecture, Up / down performance test of CPE equipment for 5G network during
    construction, the results shown in Table I, To demonstrate the transmission capacity
    of current 5G networks, Can meet the 16 IPC network cameras, 3Mbps per stream,
    Transmission requirements for the total code stream of 48Mbps. Table I. 5G UP
    / DOWN TEST DATA OF CPE EQUIPMENT B. Comparative Model Analysis By filtering and
    extracting COCO data set, portrait labels and images result in COCO-person, data
    set, which can be used as an independent training data set of YOLO V5s related
    model; pull the YOLO V5s original model code from Github to find the named common.py
    in the modeles module. The python file, replace Conv, the default SiLU activation
    function in the class is Mish activation function; find utils, named general.py
    in the module. The python file, modify the torchvision.ops.nms in the function
    no_max_suppression, The function is the modified ciou_nms function, yielding the
    modified YOLO V5s target detection model. Training and testing of the YOLO V5s
    model and the improved YOLO V5s model separately based on the COCO-person dataset,
    The corresponding weight parameter model can be obtained, For a better statistical
    analysis of the performance and identification effect of these two algorithms,
    Based on precision (Precision), mean precision (mAP), and recall (Recall), As
    the evaluation standard for the identification effect of personnel target detection
    algorithm [10]; Training the two algorithm models 100 times based on the COCO-person
    dataset, The final evaluation index results are shown in Table II: Table II. Alignment
    of the Two Algorithm Models The results of the performance index after model training
    show that the accuracy of the improved YOLO V5s algorithm model improves by 0.897%,
    the recall rate improvement is close to 0.622%, and the average accuracy mean
    mAP_0.5 and mAP_0.5:0.95; based on the evaluation index file result. The csv and
    Matplotlib components draw the evaluation index curve of the algorithm model.
    It can be found from Figure 5 that the improved model has better convergence and
    stability compared with the original model. Fig. 5. Evaluation index curve of
    the algorithm model Show All And the position marker of the detected object through
    the weight parameter model, Figure 6 The left figure shows the detection results
    of the YOLO V5s original model, As can be seen from the test results, The original
    model can identify most of the personnel targets, However, there are some missing
    detection cases, The yellow arrow points to the missed target; Figure 6 right
    panel shows the detection results of the modified YOLO V5s model, Is able to identify
    target targets missed by the original model, Higher confidence scores in target
    detection, Its robustness is better than the YOLO V5s original algorithm model,
    Good performance overall, Accurate target positioning, High recognition rate is
    observed. Fig. 6. Comparison of personnel target detection effect Show All SECTION
    V. Conclusion Using the technical characteristics of 5G network technology, such
    as high speed, low delay, large bandwidth and high flexibility, Build a video
    surveillance network for personnel detection in deep water dangerous areas based
    on 5G network; Through the comprehensive perception and real-time analysis of
    the current video data of the cloud-edge collaboration technology, Speed up the
    analysis of live video data, Form a scientific and effective decision-making and
    intelligent analysis; By replacing the SiLU activation function of the original
    YOLO V5s model as the Mish activation function, Improve the detection and identification
    rate of remote personnel in the video surveillance area; Improving the weighting
    factor for non-maximum suppression is for the CIoU function, Further improve the
    positioning and identification of personnel in the image, Finally, the construction
    of the dangerous area personnel detection algorithm model of 5G cloud edge collaboration,
    Can be applied to personnel detection and identification in deep-water hazardous
    areas, Remote personnel alarm can also be realized for special business scenarios,
    Effectively avoid the occurrence of safety accidents. Authors Figures References
    Keywords Metrics More Like This Distributed Real-Time Object Detection Based on
    Edge-Cloud Collaboration for Smart Video Surveillance Applications IEEE Access
    Published: 2022 Object Detection for Video Surveillance Using Edge-Cloud Collaboration
    2023 1st International Conference on Circuits, Power and Intelligent Systems (CCPIS)
    Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 IEEE 5th International Conference on Power, Intelligent Computing
    and Systems, ICPICS 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Research on Dangerous Area Personnel Detection Algorithm Based on 5G Cloud
    Edge Collaboration
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sakthi U.
  - Thangaraj K.
  - Anuradha M.
  - Kirubakaran M.K.
  citation_count: '1'
  description: With the improvement of Internet of Things (IoT) and edge computing,
    the smart agricultural system is driven by data produced by the different sensors
    and smart computing devices in the agricultural land. A new methodological paradigm
    high-performance edge computing is incorporated with blockchain implemented precision
    agriculture system to improve the data processing operation related with resource
    management. Edge computing nodes collect and analyze the sensor data locally without
    transforming to the remote centralized cloud server, which rises data processing
    speed and reduce the network latency. The blockchain technology is incorporated
    with machine learning algorithm to maintain secured and protected distributed
    database for storing smart farm details like pH, soil moisture, temperature, crop
    management, humidity, and water irrigation level. The proposed system improves
    the productivity of food items and performance of the smart agricultural system
    by providing useful information to the farmers to make time-based decision about
    the land and increase the profit.
  doi: 10.1007/978-981-99-0769-4_35
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Smart
    Trends in Computing and Communications SMART 2023: Smart Trends in Computing and
    Communications pp 397–405Cite as Home Smart Trends in Computing and Communications
    Conference paper Blockchain-Enabled Precision Agricultural System Using IoT and
    Edge Computing U. Sakthi , K. Thangaraj , M. Anuradha & M. K. Kirubakaran   Conference
    paper First Online: 15 June 2023 237 Accesses 2 Citations Part of the book series:
    Lecture Notes in Networks and Systems ((LNNS,volume 645)) Abstract With the improvement
    of Internet of Things (IoT) and edge computing, the smart agricultural system
    is driven by data produced by the different sensors and smart computing devices
    in the agricultural land. A new methodological paradigm high-performance edge
    computing is incorporated with blockchain implemented precision agriculture system
    to improve the data processing operation related with resource management. Edge
    computing nodes collect and analyze the sensor data locally without transforming
    to the remote centralized cloud server, which rises data processing speed and
    reduce the network latency. The blockchain technology is incorporated with machine
    learning algorithm to maintain secured and protected distributed database for
    storing smart farm details like pH, soil moisture, temperature, crop management,
    humidity, and water irrigation level. The proposed system improves the productivity
    of food items and performance of the smart agricultural system by providing useful
    information to the farmers to make time-based decision about the land and increase
    the profit. Keywords Blockchain technology Internet of Things Edge computing Smart
    agriculture Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF 1 Introduction Unlike conventional distributed datastore, blockchain
    technology provides advanced cryptographic techniques and emerging tools to store
    and access smart agricultural data that are shared with several untrusted users.
    Blockchain technology solves a number of data integrity issues caused by following
    untrusted operations: 1. single point failure in a supply chain centralized database,
    2. lack of data integrity due to the regular use of smart agricultural management,
    3. hackers can access and modify sensitive data, and 4. third-party assistance
    for providing security. The blockchain technology is applied on smart agriculture
    system to achieve ecosystem of agricultural services. Blockchain-based precision
    agricultural system provides secured environment for storing, exchanging, and
    retrieving confidential data between the several farmers and other persons. The
    integration of smart accuracy agriculture, edge computing, IoT, and blockchain
    technology allows the agricultural environment system to reduce the wastage of
    natural resources and increase the profit. With the new development of edge computing
    and blockchain technology, a precision agricultural system produces more profit
    by increasing crop and food productivity and reducing usage of natural resources.
    In the agricultural land, several sensors and devices are used to capture the
    environmental data and land-related data and these data are transferred to the
    data server in the cloud environment. Edge computing plays an important role to
    reduce the workload of cloud data server and minimize the volume of data moved
    to the data server. Instead of transferring data to the cloud data server, edge
    computing nodes perform the operation locally which is closer to the IoT devices
    in agricultural environment. The edge computing node provides necessary information
    to the farmers, agriculture officers, agents, researchers, and producers to earn
    more profit. The farmers can receive required information at the correct time
    to take better decision. The sensor data are collected, processed, and analyzed
    locally closer to the data center without transforming data to the remote centralized
    cloud server so that network transport latency is reduced. In Fig. 1, we have
    explained the precision farming system constructed on blockchain and edge IoT-based
    computing technology. IoT devices such as cameras, environmental sensors collect
    large amount of data from the farm and transmitted to cloud storage system for
    data analysis. The process of transferring data from IoT application area to the
    cloud service will take more time and reduce the speed of the system. The edge
    computing service is introduced between the IoT application system and cloud system
    to reduce the network traffic and latency in cloud environment. The edge computing
    server increases the speed of the data analysis process and provides solution
    to the farmer and the other users immediately without delay. The edge computing
    paradigm allows sensor data to be stored and analyzed close to the data source
    and application devices using 5G network connection, which reduce the cost and
    increase the crop yield. Fig. 1 A precision agriculture system using blockchain
    technology Full size image The proposed work is explained in this paper which
    is organized as follows. Section 2 explains the related work in a precision smart
    efficient agricultural system using blockchain technology and an edge computing.
    The IoT, edge computing, and blockchain functionalities of the various layers
    are discussed in Sect. 3. The process flow of the projected system is explained.
    In Sect. 5, we have explained the experimental environment and parameters, results,
    and analyses of various algorithm. In Sect. 6, we have explained the result of
    the proposed efficient precision agricultural system and future enhancements.
    2 Related Works In 2050, the world population is expected to reach 9.6 billion.
    To provide the food products to the large population, agriculture department should
    integrate with new technology [2, 5, 13]. Precision agriculture is a sensor-controlled
    and software-managed environment for automatic collection of land environmental
    data, data analysis, monitoring, and tracking of operations with advanced technologies
    to meet the food products’ demands in future. Precision agriculture uses IoT and
    blockchain technology to avoid centralized management of data [17]. Traditional
    centralized smart agriculture system has lack of data security, data misinterpretation,
    and data redundancy [1, 3]. Blockchain-based smart agriculture system provides
    efficient services to the different stockholders like farmers, farm product wholesalers,
    supply chain agents, and financial sectors participated in every process from
    farm to our plate [4, 6, 7]. Blockchain is a chain of secured and verified block
    of information with timestamp. Blockchain and smart contract already proved information
    transparency and security-based record keeping [8, 9]. It provides trusted way
    of storing and processing of data in a distributed environment. Blockchain information
    are immutable which avoids alteration of data in the blockchain. Distributed consensus
    algorithm is implemented using tamperproof method to support users to validate
    every transaction in the blockchain network [10, 12]. It ensures no false information
    is added in the blockchain hashed data with the support of timestamp and cryptographic
    footprint data. IoT devices are responsible for monitoring and collecting land
    environment data with minimum power and low cost and lack of information security.
    This security and traceability problem can be solved by smart contract and blockchain
    mechanism [11, 14]. Figure 2 shows the different components of three-layered precision
    agricultural system. Edge computing is a distributed environment, which moves
    storage and computing power nearer to the IoT wireless sensor network to reduce
    the workload of the cloud server and the latency [15, 19]. Edge computing node
    provides immediate response to the farmers to take better and efficient decision
    to improve the crop yield and reduce the usage of resources. It reduces the cost
    and increases the network efficiency by providing better security [16, 18]. Analysis
    of sensor data performed in the edge computing node reduces the network latency
    and increases reliability, modularity, and scalability. Raspberry Pi acts as an
    edge computing node, which executes a machine learning algorithm to predict the
    required information about the crop yield and status of resources using gateways
    [18]. Fig. 2 Precision agricultural system Full size image 3 IoT, Edge Computing,
    and Blockchain Technology in Precision Agricultural System Smart contract controls
    do not allow the third-party intermediate users to fix the price for food products
    in the market. The blockchain technology controls the usage and modification of
    data in the edge server and cloud server node using contracts and consensus algorithm.
    Food supply fully depends on the agricultural process, which is composed of pre-harvesting
    and post-harvesting. Pre-harvesting includes finding best quality seed, cultivation
    process, market demand, finding soil quality, and need of fertilizer. Post-harvesting
    represents distribution of food products in the market to the producer, supplier,
    and consumer. Food supply management involves direct and indirect users to accomplish
    the transfer of food products to consumer or producer through different agents.
    Blockchain allows different levels of users to access the centralized secured
    data and execute different operations on hashed data. 3.1 Proposed Blockchain-Based
    Agricultural Model The proposed system allows the users to trace the food product
    data from land to consumer and ensures the security and adoptability. In Fig.
    3, we have explained the different use cases of blockchain in precision agriculture
    system. The blockchain technology helps to create trustworthy environment between
    the producer and consumers in food supply chain management. Every transaction
    is stored as block in chain with time stamp, which cannot be modified in future.
    Sensors collects crop data and environment data from the agricultural and stored
    in edge computing node for data pre-processing. Some portion of the data is processed
    in the IOT side edge process node to provide information to the different users
    and farmers and other users immediately without any time delay. Machine learning
    algorithm is executed in the edge layer processing node to predict the crop yield
    and demand in the market. Fig. 3 Use cases of blockchain in precision agriculture
    Full size image 3.2 Functionality of Layered Architecture Blockchain-based precision
    agricultural system consists of different kind of components at each layer to
    perform agricultural land sensor data analysis, which are collected from agricultural
    land. In Fig. 4, we have explained different components and layers of the proposed
    system. The different functionalities of various layers are explained below: Data
    collection layer: It is responsible for collecting land- and crop-related data
    from different sensors and drones deployed in the agricultural area. The collected
    data is transferred to the highest level edge computing server node using wireless
    sensor network gateway. Edge computing layer: The edge computing layer comprises
    storage server and processing node, which is responsible for processing the data
    nearer to the data source to reduce the latency and network traffic. It provides
    required information to the user with minimum response time. Bigdata analytics
    layer: Data mining association rule algorithm is executed on the large volume
    of data to generate the pattern to be stored in the knowledge repository with
    contract-based blockchain technology. It can be used by the farmer and other stakeholders
    hashed in the blockchain. Blockchain service layer: Transaction details and data
    from each layer are stored in the blockchain in a distributed environment to support
    the property traceability for all users and security. Application layer: The user
    can access information or knowledge pattern from the knowledge repository using
    laptop, computer, or mobile phones at any time and everywhere. The farmer, retailer,
    agri-officer, supplier, and retailer can access information using mobile devices
    connected via Internet. Fig. 4 Layered architecture using blockchain technology
    Full size image 4 Performance Analysis The proposed blockchain system maintains
    transaction transparency using decentralized ledger when compared with traditional
    approach. The performance of the proposed secured blockchain system is measured
    in terms of speed, average latency, average throughput, resource utilization,
    and user performance. The experiment is performed on 250 transactions in two different
    blockchain platforms, Hyperledger Fabric and Ethereum. Ethereum is a distributed
    computing platform with smart contracts, open source, and public. Hyperledger
    Fabric is an open-source permissioned private blockchain technology with distributed
    ledger. Figure 5 shows the comparison of average throughput for Ethereum and Hyperledger
    with varying number of transactions. The experiment was conducted with two platforms
    on five different sizes of datasets. For all datasets, Hyperledger has more throughput
    than Ethereum. The next performance matrix latency is evaluated with different
    number of transactions for Ethereum and Hyperledger. In Fig. 6, we observed that
    the throughput increases when the number of transaction increases. The Ethereum
    has more latency than Hyperledger. Fig. 5 Comparison of throughput Full size image
    Fig. 6 Comparison of latency Full size image 5 Conclusion and Future Work With
    the development of edge computing efficient technologies and blockchain algorithm
    rapidly transfer traditional smart agricultural systems to advanced precision
    agricultural systems. The effective food supply chain system is developed with
    use of consensus algorithm in two different blockchain platforms. In this paper,
    we proposed secured advanced smart precision agricultural system deployed in the
    field of research and food supply, which provides transparency and security to
    the information stored in the knowledge repository. The farm management process
    is optimized with the use of machine learning algorithm and security contacts
    by improvising the food plant and crop yield and minimizing the convention of
    natural resources. The proposed system delivers the needed information or knowledge
    to the farmers at the correct time and correct place. The novelty of the proposed
    advanced system is to maximize the profit for the farmers and minimize the investment
    cost based on blockchain-based food supply system. In future, the proposed method
    can be extended for designing food chain management system in order to increase
    the profit for farmers and crop demand and disease analysis. References Antonucci
    F, Figorilli S, Costa C, Pallottino F, Raso L, Menesatti P (2019) A review on
    blockchain applications in the agri‐food sector. J Sci Food Agric Google Scholar   George
    R, Harsh H, Ray P, Babu A (2019) Food quality traceability prototype for restaurants
    using blockchain and food quality data index. J Clean Prod 240(1):118021 Article   Google
    Scholar   Bechtsis D, Tsolakis N, Bizakis A, Vlachos D (2019) A blockchain framework
    for containerized food supply chains. Comput Aided Chem Eng 46(1):1369–1374 Article   Google
    Scholar   Salah K, Nizamuddin N, Jayaraman R, Omar M (2019) Blockchain-based soybean
    traceability in agricultural supply chain. IEEE Access 7(1):73295–73305 Article   Google
    Scholar   Patil AS, Tama BA, Park Y, Rhee KH (2017) A framework for blockchain
    based secure smart green house farming. In: Advances in computer science and ubiquitous
    computing, Singapore Google Scholar   Abelseth B (2018) Blockchain tracking and
    cannabis regulation: developing a permissioned blockchain network to track Canada''s
    cannabis supply chain. Dalhous J Interdiscip Manag 14 Google Scholar   Figorilli
    S, Antonucci F, Costa C, Pallottino F, Raso L, Castiglione M, Pinci E, Del Vecchio
    D, Colle G, Proto A, Sperandio G (2018) A blockchain implementation prototype
    for the electronic open source traceability of wood along the whole supply chain.
    Sensors 18(9):3133 Article   Google Scholar   Sakthi U, Dafni Rose J, Sam D, Kirubakaran
    MK (2021) Blockchain-based internet of vehicles for intelligent transportation
    system using fog computing. In: 4th international conference on computer networks,
    big data and IoT (ICCBI 2021). Lecture notes on data engineering and communications
    technologies, vol 117. Springer, Singapore Google Scholar   Sakthi U, Dafni Rose
    J (2021) Blockchain-enabled smart agricultural knowledge discovery system using
    edge computing. In: 12th international conference on identification, information
    and knowledge in the internet of things, IIKI 2021, Hangzhou, China, 18 December
    2021, vol 202, pp 73–82 Google Scholar   Sakthi U, Thangaraj K, Poongothai T,
    Kirubakaran MK (2022) Big data analytics and machine learning approach for smart
    agriculture system using edge computing. In: Zhang YD, Senjyu T, So-In C, Joshi
    A (eds) Smart trends in computing and communications. Lecture notes in networks
    and systems, vol 396. Springer, Singapore Google Scholar   Sakthi U, Rose JD (2020)
    Smart agricultural knowledge discovery system using IoT technology and fog computing.
    In: 2020 third international conference on smart systems and inventive technology
    (ICSSIT), Tirunelveli, India, August 2020, pp 48–53 Google Scholar   Li Z, Yang
    Z, Xie S (2019) Computing resource trading for edgecloud-assisted internet of
    things. IEEE Trans Industr Inf 15(6):3661–3669 Article   Google Scholar   Xu C,
    Liu H, Li P, Wang P (2018) A remote attestation security model based on privacy-preserving
    blockchain for v2x. IEEE Access 6:67 809–67 818 Google Scholar   Manubot, Tech.
    Rep. (2019) Tschorsch F, Scheuermann B, Bitcoin and beyond: A technical survey
    on decentralized digital currencies. IEEE Commun Surv Tutor 18(3):2084–2123, 2016
    Google Scholar   Abbas N, Zhang Y, Taherkordi A, Skeie T (2018) Mobile edge computing:
    a survey. IEEE Internet Things J 5(1):450–465 Article   Google Scholar   Archontakis
    F, Anastasiadis F (2019) Technology and innovation in Southern Europe’s agri-food
    sector: a Delphi study. Int J Technol Manag Sustain Dev 18(1):17–36 Article   Google
    Scholar   Avgeris M, Spatharakis D, Dechouniotis D, Kalatzis N, Roussaki I, Papavassiliou
    S (2019) Where there is fire there is SMOKE: a scalable edge computing framework
    for early fire detection. Sensors 19(3):639 Article   Google Scholar   Gaura EI,
    Brusey J, Allen M, Wilkins R, Goldsmith D, Rednic R (2013) Edge mining the internet
    of things. IEEE Sensors J 13(10):3816–3825 Article   Google Scholar   Shi W, Cao
    J, Zhang Q, Li Y, Xu L (2016) Edge computing: vision and challenges. IEEE Internet
    Things J 3(5):637–646 Article   Google Scholar   Download references Author information
    Authors and Affiliations Department of Computational Intelligence, School of Computing,
    SRM Institute of Science and Technology, Kattankulathur, Chengalpattu, Chennai,
    TN, 603203, India U. Sakthi Department of Information Technology, Sona College
    of Technology, Junction Main Road, Salem, TN, 636 005, India K. Thangaraj Department
    of Computer Science and Engineering, S.A. Engineering College, Chennai 77, Thiruverkadu,
    TN, India M. Anuradha Department of Information Technology, St. Joseph’s Institute
    of Technology, Chennai, TN, 119, India M. K. Kirubakaran Corresponding author
    Correspondence to U. Sakthi . Editor information Editors and Affiliations University
    of the Ryukyus, Nishihara, Japan Tomonobu Senjyu Khon Kaen University, Khon Kaen,
    Thailand Chakchai So–In Global Knowledge Research Foundation, Ahmedabad, India
    Amit Joshi Rights and permissions Reprints and permissions Copyright information
    © 2023 The Author(s), under exclusive license to Springer Nature Singapore Pte
    Ltd. About this paper Cite this paper Sakthi, U., Thangaraj, K., Anuradha, M.,
    Kirubakaran, M.K. (2023). Blockchain-Enabled Precision Agricultural System Using
    IoT and Edge Computing. In: Senjyu, T., So–In, C., Joshi, A. (eds) Smart Trends
    in Computing and Communications. SMART 2023. Lecture Notes in Networks and Systems,
    vol 645. Springer, Singapore. https://doi.org/10.1007/978-981-99-0769-4_35 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-0769-4_35 Published
    15 June 2023 Publisher Name Springer, Singapore Print ISBN 978-981-99-0768-7 Online
    ISBN 978-981-99-0769-4 eBook Packages Intelligent Technologies and Robotics Intelligent
    Technologies and Robotics (R0) Share this paper Anyone you share the following
    link with will be able to read this content: Get shareable link Provided by the
    Springer Nature SharedIt content-sharing initiative Publish with us Policies and
    ethics Download book PDF Download book EPUB Sections Figures References Abstract
    Introduction Related Works IoT, Edge Computing, and Blockchain Technology in Precision
    Agricultural System Performance Analysis Conclusion and Future Work References
    Author information Editor information Rights and permissions Copyright information
    About this paper Publish with us Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Blockchain-Enabled Precision Agricultural System Using IoT and Edge Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang D.
  - Wang W.
  - Gao H.
  - Zhang Z.
  - Han Z.
  citation_count: '2'
  description: 'In large-scale multi-access edge computing (MEC) networks, each device
    should make the computation offloading decision distributively. In this paper,
    we target on a delay-optimal computation offloading problem in large-scale MEC
    systems, where each task has two properties: data size and computation amount.
    Because the detailed state information of massive devices are huge in large-scale
    systems, we propose a distributed computation offloading algorithm using the mean
    field game (MFG). To design the distributed computation offloading algorithm,
    we first formulate the delay-optimal computation offloading problem as a Markov
    decision process (MDP) and derive the Hamilton-Jaccobi-Bellman (HJB) equation
    with the unknown task allocation proportion, where the combined influence from
    other devices and MEC servers should be estimated. Based on MFG, we obtain the
    Fokker-Planck-Kolmogorov (FPK) equation to describe the evolution of the system&#x2019;s
    collective behavior, with the influence from other devices and MEC servers formulated
    as the mean field. To solve the large-scale problem with the unknown allocation
    proportion, we propose a optimal computation offloading algorithm based on the
    generative adversarial networks (GAN) structure. For the generator, we generate
    the unknown task allocation proportion due to its non-calculability and insufficient
    dataset. For the discriminator, we train the value function, and propose a water-filling
    algorithm to prioritize the task offloading. Finally, the simulation results evaluate
    the performance of the proposed algorithm and show the performance gain compared
    to conventional algorithms.'
  doi: 10.1109/TWC.2023.3291198
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Wireless...
    >Volume: 23 Issue: 3 Delay-Optimal Computation Offloading in Large-Scale Multi-Access
    Edge Computing Using Mean Field Game Publisher: IEEE Cite This PDF Dezhi Wang;
    Wei Wang; Hao Gao; Zhaoyang Zhang; Zhu Han All Authors 238 Full Text Views Abstract
    Document Sections I. Introduction II. System Model III. Problem Formulation IV.
    Mean Field Game for Large-Scale MEC V. Distributed Computation Offloading With
    Mean Field Games Show Full Outline Authors Figures References Keywords Metrics
    Footnotes Abstract: In large-scale multi-access edge computing (MEC) networks,
    each device should make the computation offloading decision distributively. In
    this paper, we target on a delay-optimal computation offloading problem in large-scale
    MEC systems, where each task has two properties: data size and computation amount.
    Because the detailed state information of massive devices are huge in large-scale
    systems, we propose a distributed computation offloading algorithm using the mean
    field game (MFG). To design the distributed computation offloading algorithm,
    we first formulate the delay-optimal computation offloading problem as a Markov
    decision process (MDP) and derive the Hamilton-Jaccobi-Bellman (HJB) equation
    with the unknown task allocation proportion, where the combined influence from
    other devices and MEC servers should be estimated. Based on MFG, we obtain the
    Fokker-Planck-Kolmogorov (FPK) equation to describe the evolution of the system’s
    collective behavior, with the influence from other devices and MEC servers formulated
    as the mean field. To solve the large-scale problem with the unknown allocation
    proportion, we propose a optimal computation offloading algorithm based on the
    generative adversarial networks (GAN) structure. For the generator, we generate
    the unknown task allocation proportion due to its non-calculability and insufficient
    dataset. For the discriminator, we train the value function, and propose a water-filling
    algorithm to prioritize the task offloading. Finally, the simulation results evaluate
    the performance of the proposed algorithm and show the performance gain compared
    to conventional algorithms. Published in: IEEE Transactions on Wireless Communications
    ( Volume: 23, Issue: 3, March 2024) Page(s): 1684 - 1698 Date of Publication:
    10 July 2023 ISSN Information: DOI: 10.1109/TWC.2023.3291198 Publisher: IEEE Funding
    Agency: SECTION I. Introduction With the rapid growth of the Internet of Things
    (IoT), an increasing number of smart mobile devices, such as smartphones, laptops,
    and sensors, can access the remote services by Internet [1]. As more computation-intensive
    services emerge, the computing capability of the devices usually hardly meet new
    requirements [2]. Consequently, it is urgent to propose a new communication technology.
    Recently, multi-access edge computing (MEC) is gaining attraction as new architecture
    and key technology for address the issue [3]. Unlike the cloud computing, MEC
    can embed computing capabilities into the radio access network, allowing the devices
    to offload computation tasks to edge servers nearby, which efficiently saves the
    energy and reduces the delay [4]. A. Related Work 1) Different MEC Systems: Many
    studies focus on resource allocation for different types of MEC systems, including
    single MEC server and multi-MEC servers. The computation offloading with a single
    device and a single MEC server was considered in [5] and [6]. A low-complexity
    online algorithm was proposed in [5] to determine the offloading decision, the
    CPU-cycle frequency for mobile execution, and the transmission power for computation
    offloading. In [6], the delay-optimal computation offloading was modeled as an
    infinite horizon average cost Markov decision process (MDP), and a closed-form
    multi-level water-filling computation offloading solution was proposed to minimize
    the delay. Different from a single device, a multiple-input multiple-output (MIMO)
    multicell system where the computation offloading from multiple users to a common
    cloud server was studied in [7]. For the scenarios of multi-MEC servers, the computation
    offloading in ultra-dense networks was investigated to minimize the delay in [8],
    and the combined computation offloading and resource scheduling problem was studied
    in [9]. These two problems were modeled as mixed-integer non-linear programming
    (MINLP) problems and solved via decomposition. Similarly, in [10], the computation
    offloading and scheduling problem was solved by the logic-based benders decomposition.
    In [11], the problem of distributed computation offloading was addressed by introducing
    an online learning-assisted algorithm based on distributed bandit optimization
    to cope with time-varying cost and constraints. Considering the uncertain load
    dynamics at the edge nodes, a model-free deep reinforcement learning-based distributed
    algorithm was proposed in [12], where each device determines its offloading decision
    without knowing the task models and offloading decisions of other devices. 2)
    Delay Optimization for MEC: Delay performance is one of the important performance
    metrics in the MEC system, especially for real-time computation-intensive services.
    Many studies have focused on the offloading policies to reduce the latency in
    the MEC system [13], [14]. In [15], a device-to-device-enabled computation offloading
    was considered for MEC systems, and a partial offloading and resource allocation
    scheme was proposed to reduce the task execution latency by solving the MINLP
    problem. Ale et al. [16] considered various requirements of computational tasks
    in a dynamics MEC system with multiple edge servers, and proposed an end-to-end
    deep reinforcement learning approach to select the best edge server. A game-based
    multi-type computation offloading algorithm was proposed to balance the computing
    delays among MEC base stations in [17]. Zhang et al. [18] proposed a risk-aware
    cloud-edge computing framework for the delay-sensitive inspections of autonomous
    manufacturing and developed a branch-and-check approach to efficiently deploy
    the decomposable inspection tasks with the minimum operation cost and acceptable
    latency. Tian et al. [19] designed a joint radio and computational resource allocation
    policy for a multi-user MEC system to minimize the power consumption while satisfying
    the long-term delay constraints. B. Motivation and Contribution The above existing
    research has mainly focused on multi-user MEC systems rather than large-scale
    MEC systems. In contrast to prior work on large-scale MEC system in [20], [21],
    [22], and [23], we consider the MEC system with massive number of devices in this
    paper. Existing research may loss effectiveness when dealing with scenarios involving
    massive number of devices due to the complicated interaction among devices. In
    such a large-scale MEC system, it is not trivial to make the computation offloading
    decision in a distributed manner while ensuring the delay performance. Therefore,
    in this paper, we consider massive number of devices and the dual-property tasks
    in the MEC system and address the computation offloading problem to minimize the
    delay by adopting the MFG, which can significantly improve the algorithm effectiveness
    and reduce computational complexity. There are two challenges as follows: Challenge
    due to the dual property of computation tasks: In our problem, each task has two
    properties, i.e., data size and computation amount, which are mutually coupled.
    The offloaded data size and computation amount cannot be controlled separately
    and are determined when certain task is selected for offloading. The two properties
    affect the allocation for computation and communication resources. For computation
    offloading, it is challenging to design a policy considering the joint scheduling
    of various types of tasks to balance the computation and communication resources.
    Challenge due to the joint influence from other devices and MEC servers: In the
    large-scale MEC system, each device should consider the joint influence from other
    devices and MEC servers when making computation offloading decisions distributively.
    The traditional MFG algorithm is unable to tackle the problem in a straightforward
    way. Designing the computation offloading policy needs the long-term task allocation
    proportion of computation offloading, which is unknown and depends on the policy
    itself. Traditional numerical algorithms cannot be used to solve the high-dimensional
    MFG. To address the challenge mentioned above, we design a distributed computation
    offloading algorithm based on mean field game (MFG) theory, which has been widely
    adopted in wireless networks [24]. Additionally, we also obtain some preliminary
    results for large-scale IoT networks [25], [26]. In such a large-scale interactive
    system, it is difficult for each device to obtain information of other devices.
    Therefore, making computation offloading distributively while overcoming the interactions
    among massive devices significantly increase the computational complexity. MFG
    can transform the interaction among devices into the mean field instead of reacting
    to other devices separately [27], [28], which reduces the computational complexity
    dramatically. In comparison to the previous work on MFG in MEC system [29], the
    influence from MEC server is considered in this paper, which needs to calculate
    the unknown task allocation proportion. Since the dataset may be insufficient
    and the task allocation proportion may not be computable, we utilize the generative
    adversarial networks (GAN)-based algorithm to solve the large-scale MEC problem.
    The GAN-assisted MFG algorithm avoids the use of spatial grids and is geared towards
    high-dimensional problems that are beyond reach with existing grid-based methods
    [30]. The main contributions of this paper are summarized as follows: We construct
    a framework for distributed computation offloading with MFG. The Hamilton-Jaccobi-Bellman
    (HJB) equation is adopted to provide the optimality condition of computation offloading
    policy for each device, taking into account the influence from other devices and
    MEC servers, where the unknown task allocation proportion is needed to solve the
    HJB equation. To estimate the influence from other devices and MEC servers, we
    derive the Fokker-Planck-Kolmogorov (FPK) equation with the influence in the mean
    field form. The MFG consists of the coupled HJB and FPK equations. We design a
    GAN-based algorithm to address the non-calculability and the insufficient dataset
    of the unknown task allocation proportion by utilizing the max-min structure of
    the large-scale MEC problem. Specifically, the generator generates the task allocation
    proportion, and the discriminator trains the value function and calculates the
    loss between the generative data and the real data, which is back-propagated to
    train both networks in GAN. To train the loss function in the GAN well, we propose
    a water-filling algorithm to show the priority of local computing and computation
    offloading due to the two properties of tasks, i.e., data size and computation
    amount, based on the given mean field and the generative task allocation proportion.
    The algorithm provides quantitative results that the tasks with large computation
    amounts and small data sizes are scheduled with high priority for computation
    offloading. The rest of this paper is organized as follows. The system model is
    introduced in Section II. In Section III, we present the problem formulation,
    and the MFG problem is formulated in Section IV. In Section V, the distributed
    computation offloading solution is proposed. We evaluate the performance of the
    proposed algorithm via numerical results in Section VI, and finally, we conclude
    the paper in Section VII. SECTION II. System Model In this section, we establish
    the MEC system model with large-scale homogeneous devices and multiple MEC servers,
    where each device has the computation tasks with different properties. Firstly,
    we introduce the large-scale MEC system model in Subsection II-A, and then we
    present the corresponding queue dynamics of devices and MEC servers in Subsection
    II-B. A. Large-Scale MEC System Consider an MEC system with N homogeneous devices
    and M MEC servers, denoted as set N and M , respectively, where each MEC server
    is equipped with a BS to provide computation offloading services for the devices,
    as shown in Fig. 1. There are K different types of tasks, denoted as set K , which
    means the system contains K types of tasks, while not every device may have all
    K types of tasks. In such cases, In such cases, the corresponding task arrival
    rate can be regarded as zero. Each task has two properties, namely computation
    amount and data size, which are denoted by d k 1 and d k 2 , respectively. For
    a certain task, the computation amount and data size are determined, thus the
    relationship of the two properties is expressed as d k 1 = ξ k d k 2 , (1) View
    Source where ξ k is the related task factor1 of the two properties of task k .
    Without loss of generality, we have ξ 1 < ξ 2 <…< ξ K . Fig. 1. System model of
    large-scale MEC system. Show All Time is slotted and the duration of each slot
    is τ . Each device determines to execute its tasks in either of two ways, i.e.,
    local computing or computation offloading, at the beginning of each slot. The
    local computing rate of device i  υ i (t) for different kinds of tasks satisfies
    υ i (t)= ∑ k∈K υ k i (t), (2) View Source where υ k i (t) is the local computing
    rate for task k of device i at time slot t . The total local computing power of
    device i at time slot t  P i (t) is P i (t)= ζ i ( f CPU i (t) ) 3 , where ζ i
    is the effective capacitance coefficient of device i and f CPU i (t) is the CPU-cycle
    frequency of device i at time slot t , respectively. Define κ i as the scale factor
    between the computation task and the ability of CPU of device i , we have υ i
    (t)= κ i f CPU i (t) . Thus, the local computing power P i (t) can be calculated
    by P i (t)= υ 3 i (t) ζ i κ 3 i . (3) View Source Denote the offloading transmission
    rate for device i to MEC server j at time slot t as R ij (t) , which satisfies
    R ij (t)= ∑ k∈K R k ij (t), (4) View Source where R k ij (t) is the offloading
    transmission rate of task k from device i to MEC server j . Then, based on the
    Shannon formula, we determine the transmission power of device i to MEC server
    j  P ~ ij (t) as P ~ ij (t)=( 2 R ij (t) η −1) N 0 H ij (t) , (5) View Source
    where η is the bandwidth, N 0 is the thermal noise power and H ij (t) is the channel
    state information (CSI) between device i and MEC server j at time slot t . B.
    Dual-Property Task Queue Dynamics 1) Task Queue Dynamics for Devices: Define α
    k i (t) as the arrival rate of task k for device i at time slot t , which is i.i.d.
    according to a general distribution with mean α ¯ k i . Define Q k i (t) as the
    computation queue of task k for device i at time slot t . For queue dynamics,
    besides the task arrival, the task departure by both local computing and computation
    offloading is considered. Thus, the computation queue dynamics of task k for device
    i can be expressed as Q k i (t+1) = [ Q k i (t)− υ k i (t)τ− ∑ j∈M γ ij (t) ξ
    k R k ij (t)τ] + + α k i (t) d k 1 τ, (6) View Source where [x ] + = △ max{x,0}
    , and γ ij (t) is the indicator that whether the tasks are transmitted successfully
    from device i to MEC server j at time slot t . Based on [26], the expectation
    of γ ij (t) can be calculated by E[ γ ij (t)]= a ij (t) ∑ min{ N j a (t), L j
    } k=1 kPr[k| N j a (t), L j ] N j a (t) , (7) View Source where L j is the number
    of available preambles for MEC server j , a ij (t)= 1 { ∑ k∈K P ~ k ij (t)>0}
    is the indicator of whether to access or not, and N j a (t) is the number of devices
    that attempt to access to MEC server2 j at time slot t . Given N j a (t) and L
    j , and using combinatorial theory, the conditional probability of the successfully
    accessing k devices is expressed as [33] Pr[k| N j a (t), L j ] = ( N j a (t)
    k )( L j k )k! ∑ min{ L j −k, N j a (t)−k} l=0 (−1 ) l ( L j ) N j a (t) ⋅( L
    j −k l )( N j a (t)−k l )l!( L j −k−l ) N j a (t)−k−l . (8) View Source Define
    T k i (t) as the data queue of task k for device i at time slot t . Given the
    computation queue Q k i (t) , the corresponding data queue T k i (t) is T k i
    (t)= 1 ξ k Q k i (t). (9) View Source 2) Task Queue Dynamics for MEC Servers:
    Define μ j as the computation rate of MEC server j , which is i.i.d. according
    to a general distribution with mean μ ¯ j . Define Q ~ j (t) as the computation
    queue for MEC server j at time slot t . Considering the task arrival from computation
    offloading, the queue dynamics of the computation queue for MEC server j is Q
    ~ j (t+1) =[ Q ~ j (t)− μ j (t)τ ] + + ∑ k∈K ∑ i∈N γ ij (t) ξ k R k ij (t)τ. (10)
    View Source Remark 1 (Influence on Communication and Computation):In this paper,
    all devices share common channels, the influence from other devices is reflected
    in the indicator γ ij (t) in (6). When γ ij (t) is determined, the actual transmission
    rate R ij (t) is also determined, and thus we can calculate the transmission power
    according to (5). In addition, the influence from MEC servers is reflected in
    (10), which indicates the computation competition among devices due to the limited
    computation capacity of MEC servers. The main obstacle for deriving computation
    offloading decisions in a distributed manner is to obtain the influence from other
    devices and MEC servers. SECTION III. Problem Formulation In this section, we
    first model the computation offloading problem for large-scale MEC system as an
    MDP in Subsection III-A, and then derive the HJB equation to calculate the optimal
    computation offloading policy in Subsection III-B. A. MDP Model We model the computation
    offloading problem as an MDP, which has the key elements as follows: State: The
    state includes the CSI, the task computation queue of devices, and the task computation
    queue of MEC servers, i.e., S(t)=[ H i,j (t), Q i (t), Q ~ j (t),∀i∈N,j∈M] , where
    Q i (t)=[ Q 1 i (t), Q 2 i (t),⋯, Q K i (t)] . We address the computation queue
    here, because the data queue is corresponding to the computation queue due to
    two properties of tasks. Action: The action is the rate allocation of local computing
    and offloading transmission for the tasks in each device. Denote υ k i (t),∀i
    ∈N,k∈K and R k ij (t),∀i∈N,j∈M,k∈K as the local computing rate of task k for device
    i and the offloading transmission rate of task k from device i to MEC server j
    at time slot t , respectively. Considering the limited power for device i , we
    have the following constraints: ∑ k∈K υ k i (t)≤ υ max i (t), ∑ k∈K R k ij (t)≤
    R max ij (t), (11) View Source where υ max i (t)= κ i ( P max i (t) ) 1 3 ( ζ
    i ) 1 3 and R max ij (t)=η log ij (1+ P ~ max ij (t) H ij (t) N 0 ) , where P
    max i (t) and P ~ max ij (t) are the power constraints of the local computing
    for device i and the offloading transmission from device i to MEC server j at
    time slot t , respectively.3 Policy: Define the computation offloading policy
    Ω=[ Ω 1 , Ω 2 ,⋯, Ω N ] , where Ω i maps from the state of device i to rate allocation
    with the consideration of the influence from other devices and the task offloaded
    to the MEC servers, i.e., { υ k i (t), R k ij (t)}= Ω i ({ Q k i (t) } K k=1 ,{
    Q ~ j (t) } M j=1 , { υ k −i (t), R k −ij (t) } K k=1 ) , where υ k −i (t)=[ υ
    k 1 (t),⋯, υ k i−1 (t), υ k i+1 (t),⋯, υ k N (t)] and R k −ij (t)=[ R k 1j (t),⋯,
    R k (i−1)j (t), R k (i+1)j (t),⋯ , R k Nj (t)] . To make sure the queues are stable
    and have unique stationary distribution, we define the admissible computation
    offloading policy as follows: Definition 1:(Admissible Computation Offloading
    Policy): A policy Ω is admissible if the following conditions satisfy: Ω is a
    unchain policy, i.e., S(t) under Ω has a single recurrent class. The computation
    amount queue under Ω is stable, i.e., lim t→∞ E Ω [( Q k i (t) ) 2 ]<∞ , where
    E Ω is the expectation w.r.t. the probability measure induced by the computation
    offloading policy Ω . Transition probability: The transition probability is expressed
    as Pr[S(t+1)|S(t)]=Pr[{ H ij (t+1) } N,M i,j=1 ] ⋅Pr[{ Q i (t+1) } N i=1 ∣ ∣ {
    Q i (t) } N i=1 ,{ H ij (t) } N,M i,j=1 ,Ω] ⋅Pr[{ Q ~ j (t+1) } M j=1 ∣ ∣ { Q
    ~ j (t) } M j=1 ,{ Q i (t) } N i=1 , { H ij (t) } N,M i,j=1 ,Ω], (12) View Source
    where the CSI is independent, and the probability is Pr[{ Q i (t+1) } N i=1 ∣
    ∣ { Q i (t) } N i=1 ,{ H ij (t) } N,M i,j=1 ,Ω] = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ∏ N i=1 Pr[{ α
    k i (t) } K k=1 ],if Q k (t+1)satis −fies(6),∀i∈N,k∈K, 0,otherwise. (13) View
    Source Similarly, Pr[{ Q ~ j (t+1) } M j=1 ∣ ∣ { Q ~ j (t) } M j=1 ,{ Q i (t)
    } N i=1 , { H ij (t) } N,M i,j=1 ,Ω] = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ∏ M j=1 Pr[ μ j (t)],if Q
    ~ j (t+1)satisfies (10),∀j∈M, 0,otherwise. (14) View Source Cost function: We
    consider the average delay of completing the computation task as the cost function
    [6]. To derive the average delay, we first give the definition about the task
    allocation proportion. Definition 2 (Task Allocation Proportion):Define β k ij
    ∈[0,1],∀i∈N,j∈M,k∈K as the long-term allocation proportion of task k assigned
    from device i to MEC server j , and β k i0 is the long-term proportion of task
    k for the local computing, where β k i0 + β k i1 +⋯+ β k iM =1,∀i∈N,k∈K . Based
    on queue theory and Little’s law [6], the average queuing delay can be calculated
    by dividing the queue length by the long-term average effective arrival rate.
    The delay comprises two parts i.e., the queuing delay on the devices and the MEC
    server. With the task allocation proportion defined, the average delay of task
    k for device i is expressed as C k i (t) = β k i0 Q k i (t) α ¯ k i + ∑ j∈M β
    k ij ( ξ k T k i (t) α ¯ k i + Q ~ j (t) ∑ l∈N ∑ k∈K β k lj α ¯ k l ) = Q k i
    (t) α ¯ k i + ∑ j∈M β k ij Q ~ j (t) ∑ l∈N ∑ k∈K β k lj α ¯ k l . (15) View Source
    Thus, the average delay of device i is C i (t) = ∑ k∈K λ k i C k i (t) = ∑ k∈K
    λ k i ( Q k i (t) α ¯ k i + ∑ j∈M β k ij Q ~ j (t) ∑ l∈N ∑ k∈K β k lj α ¯ k l
    ), (16) View Source where λ k i = α ¯ k i ∑ i∈K α ¯ k i is the weight of task
    k for device i . Remark 2 (Unknown Task Allocation Proportion):The long-term task
    allocation proportion β k ij ,∀i∈N,j∈M,k∈K are unknown, which cannot be calculated
    directly due to its non-calculability and insufficient dataset. Furthermore, as
    the number of MEC servers grows, the task allocation proportion becomes high-dimensional,
    making the calculation difficult. B. Derivation for HJB Equation To obtain the
    delay-optimal computation offloading policy, we transform the discrete form into
    the continuous form to derive the HJB equation. Based on (6) and (10), the corresponding
    continuous forms are expressed as d q k i (t) d q ~ j (t) =E[− υ k i (t)− ∑ j∈M
    γ ij (t) ξ k R k ij (t)+ α ¯ k i c k ]dt, =E[− μ ¯ j (t)+ ∑ i∈N ∑ k∈K γ ij (t)
    ξ k R k ij (t)]dt, (17) (18) View Source where q k i (t) and q ~ j (t) are the
    continuous form of Q k i (t) and Q ~ j (t) and E is the expectation w.r.t. CSI.
    Note that the condition [x ] + can be relaxed under the admissible computation
    offloading policy. The cost function of device i in continuous form is c i (t)=
    ∑ k∈K λ k i ( q k i (t) α ¯ k i + ∑ j∈M β k ij q ~ j (t) ∑ l∈N ∑ k∈K β k lj α
    ¯ k l ), (19) View Source and the corresponding value function of device i can
    be expressed as J i (t)= min υ i (t), R ij (t), β ij (t) ∫ T 0 c i (t)dt,t∈[0,T],
    (20) View Source where υ i (t)=[ υ 1 i (t), υ 2 i (t),⋯, υ K i (t)] , R ij (t)=[
    R 1 ij (t), R 2 ij (t),⋯, R K ij (t)] and β ij (t)=[ β 1 ij (t), β 2 ij (t),⋯,
    β K ij (t)] . Based on the above value function, the HJB equation of device i
    is derived as [34] − ∂ J i (t) ∂t = min υ i (t), R ij (t), β ij (t) { H i ( q
    i (t), ∂ J i (t) ∂ q i (t) )}. (21) View Source The right side hand (r.h.s.) of
    (21) is called Hamiltonian, which is expressed as H i ( q i (t), ∂ J i (t) ∂ q
    i (t) )= c i (t)+ ∑ k∈K ∂ q k i (t) ∂t ∂ J i (t) ∂ q k i (t) . (22) View Source
    When the slot duration τ is sufficiently small, the continuous form approximates
    to the discrete form, and the related approximation error is o(τ) , the proof
    can be seen in [6]. Thus, the optimal computation offloading policy can be derived
    via the Hamiltonian. However, the Hamiltonian cannot be solved directly due to
    the coupling among devices and the interaction with MEC servers. We formulate
    the influence from other devices and MEC servers by utilizing the MFG in the next
    section. SECTION IV. Mean Field Game for Large-Scale MEC To solve the HJB equation
    with large-scale complex interactions among devices, we utilize MFG to deal with
    the joint influence from devices and MEC servers, where we transform the influence
    from other devices into the mean field and the influence from MEC servers is reflected
    in the cost function. Specifically, we first introduce the concept of MFG in Subsection
    IV-A. Next, the interference based on the MFG algorithm is derived in Subsection
    IV-B. Finally, the mean field game formulation is given in Subsection IV-C. A.
    Mean Field To describe the influence from other devices and MEC servers, the MFG
    is introduced to solve the large-scale interactive problem. Define q = △ [ q 1
    , q 2 ,⋯, q K ] as the states and q i (t) = △ [ q 1 i (t), q 2 i (t),⋯, q K i
    (t)] as the state of device i at time slot t , respectively, then the mean field
    m(q,t) is defined as the probability distribution w.r.t. state q for all the devices,
    i.e., m(q,t)= lim N→∞ 1 N ∑ i∈N 1 { q i (t)=q} . (23) View Source The mean field
    m(q,t) and q ~ (t)=[ q ~ 1 (t), q ~ 2 (t),⋯, q ~ M (t) ] T contain computation
    queues of all tasks for devices and total computation queues for MEC servers.
    At each slot, each device distributively makes a computation offloading decision
    including the rate allocation of local computing and offloading transmission according
    to its state, the tasks waiting in MEC servers, and the influence from other devices.
    In the following part, we will calculate the influence from other devices based
    on the mean field m(q,t) , and also give the influence from the MEC servers. B.
    Influence Estimation via Mean Field Based on (6), the communication conflict is
    reflected in γ ij (t) , which is affected by the actions of other devices, i.e.,
    N j a (t) . Given the mean field and the computation offloading policy, we can
    derive N j a (t) as follows: N j a (t)= ∫ Ω m(q,t) a j (q,t)dq, (24) View Source
    where a j (q,t) is the indicator of whether to access to MEC server j or not at
    time slot t under state q . On the other hand, the influence from MEC servers
    is reflected in the tasks offloaded from the devices, i.e., ∑ i∈N ∑ k∈K γ ij (t)
    ξ k R k ij (t)τ , which is reflected in the task queue of MEC servers based on
    (10), and affects the decisions of devices via the cost function according to
    (19). In addition, the task waiting in MEC servers affect the delay performance
    of all devices in the system according to the cost function. The difference between
    the influence from other devices and MEC servers lies in the fact that, in a large-scale
    system, each device cannot obtain the detail information of other devices. Therefore,
    we utilize the MFG to transform the influence from other devices into the mean
    field. On the other hand, the influence from the MEC servers is reflected in the
    unknown task allocation of the cost function. Remark 3 (MFG for Solving Large-Scale
    Problem):MFG provides an efficient way to reduce the computational complexity
    significantly in large-scale decision-making problems. It transform the interaction
    among massive device into mean field, which can accelerate the algorithm convergence
    and solve the large-scale problems that multi-agent decision-making method cannot
    be modeled [27]. C. Mean Field Game Formulation The mean field game can be represented
    by the partial differential equations (PDEs) system, including the FPK and HJB
    equations. Based on the mean field m(q,t) , the FPK equation [35] is derived as
    ∂ t m(q,t)+ ∑ k∈K ∂ q k ( ∂ t q k m(q,t))=0. (25) View Source The derivation process
    of FPK equation can be seen in Appendix A. The FPK equation implies the evolution
    of the mean field by describing the evolution of the system state. The HJB equation
    in (21) provides the optimal policy for given system states. These two equations
    form the MFG problem. However, the solution to the MFG problem requires the unknown
    task allocation proportion, which cannot be obtained directly due to its non-calculability
    and insufficient dataset. In the next section, we will give the solution based
    on the GAN structure to solve the MFG problem and derive the optimal computation
    offloading policy. SECTION V. Distributed Computation Offloading With Mean Field
    Games Due to the non-calculability and insufficient dataset of the task allocation
    proportion, we propose the computation offloading policy based on the GAN structure
    in this section. In order to deal with the tasks with dual properties, we propose
    a water-filing algorithm considering the priority of the tasks according to different
    related task factor ξ k ,∀k∈K . Specifically, the FPK equation is solved via the
    Lax-Friedrichs method in Subsection V-A. Then, we adopt the GAN method to generate
    the task allocation proportion and train the value function in Subsection V-B.
    Finally, we propose a water-filling algorithm to derive the optimal computation
    offloading policy in Subsection V-C. A. Lax-Friedriches Method to the FPK Equation
    In this subsection, the mean field in the FPK equation is updated by utilizing
    the Lax-Friedrichs method [35], which is a difference method. First, we discretize
    the time [0,T] and the state space of the computation queues of tasks [0, Q k
    max ],∀k∈K into X× Y 1 ×⋯× Y K , which includes X+1 points in time and Y k +1,∀k∈K
    points in the state space of devices. Define the step lengths for the time and
    the computation amount queue as τ= T X and δ k q = Q k max Y k ,∀k∈K , respectively.
    Based on (17), we apply the Lax-Friedrichs method to (25), which is expressed
    as M(x+1, y 1 ,…, y K ) = M 1+ + M 1− +⋯+ M K+ + M K− 2K − ∑ k∈K τ 2 δ k q ( M
    k+ (− υ k+ i − ∑ j∈M γ ij (x) ξ k R k+ ij + α ¯ k i c k ) − M k− (− υ k− i − ∑
    j∈M γ ij (x) ξ k R k− ij + α ¯ k i c k )), (26) View Source where x and y k ,∀k∈K
    are the time level and the computation amount queue level of task k , respectively,
    and M k+ = △ M(x, y 1 ,⋯, y k +1,⋯, y K ),∀k∈K, M k− = △ M(x, y 1 ,⋯, y k −1,⋯,
    y K ),∀k∈K, υ k+ i = △ υ i (x . B. Policy Neural Networks to the HJB Equation
    The HJB equation cannot be solved directly using the finite difference technique
    due to the Hamiltonian. As a result, we reformulate the large-scale MEC problem
    by transforming the HJB equation into an optimal control problem with FPK equation
    as a constraint [24], i.e., min υ i (t), R ij (t), β ij (t) ∫ T 0 c i (t)dt,t∈[0,T]
    s.t. ∂ t m(q,t)+ ∑ k∈K ∂ q k ( ∂ t q k m(q,t))=0. (27) View Source Then, we show
    the primal-dual structure of MFG. Define ϕ i (q,t) as the Lagrange multiplier.4
    Consider ∫m(q,t)dq=1 , we have sup ϕ i inf m, υ i (t), R ij (t), β ij (t) ∫ T
    0 ∫ c i (t)m(q,t)− ϕ i (s,t) ⋅( ∂ t m(q,t)+ ∑ k∈K ∂ q k (m(q,t) ∂ t q k ))dqdt.
    (28) View Source Then we transform the MFG problem into the max-min game in the
    following proposition: Proposition 1 (Max-Min Game for MFG):The MFG problem in
    (28) is equivalent to the following max-min game: sup ϕ i inf m, β ij ∫ T 0 ∫(
    ∂ t ϕ i (q,t)+H(q, ∂ q k ϕ i ))m(q,t)dqdt +∫ ϕ i (q,0)m(q,0)dq−∫ ϕ i (q,T)m(q,T)dq,
    (29) View Source where H(q, ∂ q k ϕ i )= inf υ i , R ij { c i (t)+ ∑ k∈K ∂ ϕ i
    (q,t) ∂ q k ∂ q k ∂t } , which is the same as the Hamiltonian in (22), and thus
    ϕ i is the value function actually.5 Proof:Please refer to Appendix B. In the
    above max-min game for MFG problem in (29), the Hamiltonian contains the unknown
    parameter, i.e., task allocation proportion β k ij ,j∈M,k∈K involved in the cost
    function c i (t) in (19). Thus, we first obtain the task allocation proportion
    β k ij ,j∈M,k∈K , however, it cannot be calculated. Due to the insufficient training
    data, the task allocation proportion cannot be sufficiently trained using traditional
    neural networks. According to the max-min structure of the problem [36], the discriminator
    and the generator can act as the roles for maximization and minimization respectively.
    Based on the above analysis, we design a GAN-based method to solve the max-min
    problem in (29). GAN is rooted in game theory and aims to achieve Nash equilibrium
    between the discriminator and generator networks. The formulated problem in Proposition
    1 is a max-min problem, which aligns with the GAN structure. For the design of
    the GAN-based method, we first convert the max-min game for MFG problem in (29)
    into a two-person game. From this perspective, Player 1 tries to generate the
    task allocation proportion β k ij ,j∈M,k∈K for the infimum in (29), while Player
    2 distributively provides the value function that yields the best response for
    the supremum in (29) against the task allocation proportion. This interpretation
    is consistent with the design of GAN and the expression in (29) is the cornerstone
    of the GAN method [30]. In our problem, we choose the task allocation proportion
    β k ij ,j∈M,k∈K as the generator since the generator outputs samples from the
    desired distribution. 1) GAN Structure for the Optimal Control: The neural networks
    N ω (q,t) (in discriminator) and N θ (z,t) (in generator) are initialized, where
    ω and θ are the model parameters of the two neural networks, respectively, which
    reflect the node weights of neural networks. The two model parameters are updated
    by gradient descent during the training process. Then we set ϕ ω (q,t) G θ (z,t)
    =(1−t) N ω (q,t)+tg(q), =(1−t)z+t N θ (z,t), (30) (31) View Source where z is
    the sample drawn from the initial distribution of task allocation proportion and
    g(q) is the value function at the terminal time. The inputs of the discriminator
    and the generator are q and z , respectively. For discriminator, it trains the
    value function ϕ w according to (30), while for generator, it trains G θ to generate
    the samples β k ij (t),j∈M,k∈K from the distribution of task allocation proportion
    according to (31). Based on (30) and (31), ϕ w and G θ satisfy the initial and
    terminal conditions automatically, respectively. During the training process,
    we alternate between training the generator to output the task allocation proportion
    and training the discriminator to output the value function. To identify the equilibrium
    of MFG, we alternate the task allocation proportion and the value function neural
    networks. Specifically, to train the discriminator, we first sample a batch {
    z b } B b=1 from the given initial task allocation proportion distribution, and
    { t b } B b=1 uniformly from [0,T] , where b is the ID of the batch. Based on
    the max-min problem in (29), the total loss of the discriminator ϕ w is expressed
    as los s ϕ = 1 B ∑ b=1 B ϕ ω ( q b ,0)+ 1 B ∑ b=1 B ∂ t ϕ ω ( q b , t b ) +H(
    q b , ∂ q k ϕ w ( q b , t b )). (32) View Source Then, we can optionally add a
    regularization term [36] as l HJB = 1 B ∑ b=1 B ∥ ∥ ∂ t ϕ ω ( q b , t b )+H( q
    b , ∂ q k ϕ w ( q b , t b )) ∥ ∥ (33) View Source to correct deviations from the
    HJB equations. Finally, we back-propagate the total loss to update the weights
    of the discriminator ϕ w . We additionally sample { z b } B b=1 and { t b } B
    b=1 in order to train the generator, and the loss of the generator is los s G
    = 1 B ∑ b=1 B ∂ t ϕ w ( G θ ( q b , t b ), t b ) +H( q b , ∂ q k ϕ w ( G θ ( z
    b , t b ), t b )). (34) View Source Then, we back-propagate the loss to update
    the weights of G θ . In summary, at each time slot, we train the neural networks
    in the GAN structure, where the generator generates the task allocation proportion
    and the discriminator obtains the value function. C. Optimal Water-Filling Computation
    Offloading Policy Based on the task allocation proportion β k ij ,j∈M,k∈K , the
    value function ϕ i (q,t) and the mean field m , we try to derive the optimal computation
    offloading policy. At each time slot t , the access decision a ij (t) should be
    determined before the rate allocation. Notice that when device i does not offload
    the task to MEC server j , i.e., a ij (t)=0 , the rate allocation for offloading
    transmission is not necessary to be taken into consideration. Thus, we focus on
    the case that a ij (t)=1 . Given the access decision a ij (t)=1 , the rates need
    to be allocated to compute the task. At first, we calculate the derivatives of
    the Hamiltonian w.r.t. the rates. Based on (22), the extension of Hamiltonian
    is H i ( q i (t), ∂ J i (t) ∂ q i (t) ) = c i (t)+ ∑ k∈K { ∂ J i (t) ∂ q k i (t)
    (− υ k i (t) − ∑ j∈M γ ij (t) ξ k R k ij (t)+ α k i (t) d k 1 )}. (35) View Source
    Based on (17), the computation amount queue of task k for device i at time slot
    t is derived as q k i (t) = q k i (0)+E[ ∫ t 0 (− υ k i (t)− ∑ j∈M γ ij (t) ξ
    k R k ij (t) + α ¯ k i (t) d k 1 )dt]. (36) View Source Similarly, according to
    (18), the computation amount queue for MEC server j at time slot t is q ~ j (t)
    = q ~ j (0)+E[ ∫ t 0 (− μ ¯ j (t) + ∑ k∈K ∑ i∈N γ ij (t) ξ k R k ij (t))dt]. (37)
    View Source Consequently, the derivatives of H i ( q i (t), ∂ J i (t) ∂ q i (t)
    ) w.r.t. υ k i (t) and R k ij (t) are expressed as g k i (t)= g ^ k ij (t)= −(
    1 ∑ k∈K α ¯ k i +f( q k i (t))), γ ij (t) ξ k ( β k ij λ k i ∑ l∈N ∑ k∈K β k lj
    α ¯ k l − 1 ∑ k∈K α ¯ k i −f( q k i (t))), (38) (39) View Source where g k i (t)=
    ∂ H i ( q i (t), ∂ J i (t) ∂ q i (t) ) ∂ υ k i (t) , g ^ k ij (t)= ∂ H i ( q i
    (t), ∂ J i (t) ∂ q i (t) ) ∂ R k ij (t) and f( q k i (t))= ∂ J i (t) ∂ q k i (t)
    . The above derivative results show that as q k i (t) increases, the value function
    J i (t) increases and its growth rate also increases, because the value function
    is the accumulation over time of the queue length according to (20). It also means
    that f ′ ( q k i (t))= ∂ J i (t ) 2 ∂ 2 q k i (t) is an increasing function of
    q k i (t) . Based on (38), the local computing rate is affected only by f( q k
    i (t)) since the other coefficient is constant. Based on the above analysis, we
    propose a water-filling rate allocation algorithm for local computing and offloading
    transmission. For the local computing rate allocation, we allocate a unit rate
    for local computing δ υ to the task with the maximum f( q k i (t)) to minimize
    the cost function, since it has the maximum growth rate. When the rate allocated
    to task k increases, the growth rate of f( q k i (t)) decreases, which may lead
    to f( q k i (t))<f( q k ′ i (t)),∃ k ′ ∈K . Repeat the per-task rate allocation
    until the computation and communication capabilities, i.e., υ max i (t) and R
    max ij (t) , are used up. Similarly, we can design the offloading transmission
    rate allocation algorithm. Obviously, the allocated rates satisfy g ^ k 1 ij (t)=
    g ^ k 2 ij (t)∀ k 1 , k 2 ∈K when a unit rate for computation offloading δ R is
    sufficiently small. Based on (39), the task with larger ξ k will reduce the delay
    more efficiently when allocated the same offloading transmission rates. We provide
    the following theorem to show the optimality of the proposed rate allocation algorithm.
    Theorem 1:The sufficient conditions for the optimal rate allocation algorithm
    are g k 1 i (t)= g k 2 i (t),∀ k 1 , k 2 ∈K, k 1 ≠ k 2 , g ^ k 1 ij (t)= g ^ k
    2 ij (t),∀ k 1 , k 2 ∈K, k 1 ≠ k 2 , ∑ k∈K υ k i (t)= υ max i (t), ∑ k∈K R k ij
    (t)= R max ij (t). (40) (41) (42) View Source Proof:Please refer to Appendix C.
    The equalities in Theorem 1 mean that when the rate allocation is optimal when
    it meets the equalities. The conclusion is reasonable because each unit of rate
    allocation will seek the task that can most quickly reduce delay. When all tasks
    have equal impact on the delay, the optimal solution is achieved. Although it
    may not be possible to achieve exact equality in practical application, the rate
    allocation should strive to approach the value on the right side of the equations
    as closely as possible. The detail of the water-filling algorithm is shown in
    the Algorithm 1. Given the optimal task allocation derived by the proposed water
    filling algorithm, we can determine the optimal access decision a ij (t) from
    device i to MEC server j , which is shown in the following theorem. Algorithm
    1 Water-Filling Computation Offloading Policy Initialize the unit rate δ υ for
    local computing and δ R for computation offloading while the conditions in (42)
    and (42) are not satisfied do Calculate g k i (t) and g ^ k ij (t)∀k∈K,i∈N , and
    j∈M using (38) and (39) Find k i =arg min k { g k i (t),∀k∈K} and k ^ ij =arg
    min k { g ^ k ij (t),∀k∈K} , ∀i∈N,j∈M if ∑ k∈K υ k i (t)+ δ υ > υ max i (t) then
    υ k i i (t)= υ max i (t)− ∑ k∈K,k≠ k i υ k i (t),∀i∈N else υ k i i (t)= υ k i
    i (t)+ δ υ ,∀i∈N end if if ∑ k∈K R k ij (t)+ δ R > R max ij (t) then R k ^ ij
    ij (t)= R max ij (t)− ∑ k∈K,k≠ k ^ ij R k ij (t),∀i∈N,j∈M else R k ^ ij ij (t)=
    R k ^ ij ij (t)+ δ R ,∀i∈N,j∈M end if end while Output: υ i , R ij Theorem 2 (Optimal
    Access Decision):For optimality, the access decision for device i to MEC server
    j is obtained as a ij (t)={ 1, 0,  if  F j (t)<0, otherwise,  (43) View Source
    where F j (t)= ∑ k∈K ( β k ij ∑ l∈N ∑ k∈K β k lj α ¯ k i ∑ k∈K ξ k R k ij − ∂
    J i (t) ∂ q k i (t) ξ k R k ij (t)− 1 ∑ k∈K α ¯ k i ξ k R k ij (t))ψ(m(q,t)) .
    Proof:Please refer to Appendix D. Note that the calculation of the access decision
    needs the value function, which is the Lagrange multiplier in (29) [36]. In summary,
    the proposed computation offloading algorithm structure of the MFG problem is
    illustrated in Fig. 2 and provided using pseudocode in Algorithm 2. In the proposed
    distributed algorithm, the computation offloading policy is optimized under the
    GAN structure. The system updates the mean field m according to (18) and each
    device obtains the distributed computation offloading policy via the GAN framework.
    Specifically, for generator, it generates the task allocation proportion β k ij
    (t),∀j∈M,k∈K . On the other hand, for discriminator, it trains the value function
    ϕ i . During the training process of the discriminator, the water filling algorithm
    is utilized to derive the distributed optimal computation offloading policy, which
    contributes to train the GAN. Given the well-trained GAN network, we finally derive
    the optimal computation offloading policy. Fig. 2. The GAN structure for solving
    the MFG. Show All Algorithm 2 GAN-Based Optimal Computation Offloading Policy
    Initialize neural networks parameters N ω and N θ , batch size B while not converged
    do train ϕ ω : Sample batch {( z b , t b ) } B b=1 where z b ∼ β 0 and t b ∼Unif(0,T)
    l 0 ← 1 B ∑ B b=1 ϕ w ( q b ,0) l t ← 1 B ∑ B b=1 ∂ t ( q b , t b )+ H i ( q b
    , ∂ q k ϕ w ( q b , t b ), z b ) l HJB ← 1 B ∥ ∥ ∑ B b=1 ∂ t ( q b , t b )+ H
    i ( q b , ∂ q k ϕ w ( q b , t b ), z b ) ∥ ∥ Back-propagate the loss l total =
    l 0 + l t + l HJB to ω and update ϕ w according to (30) train G θ : Sample batch
    { z b , t b } B b=1 , where z b ∼ β 0 and t b ∼Unif(0,T) l t = 1 B ∑ B b=1 ∂ t
    ϕ w ( G θ ( q b , t b ), t b )+ H i ( q b , ∂ q k ϕ w ( G θ ( z b , t b ), t b
    ), z b ) Back-propagate the loss l loss = l t to θ weights and update G θ according
    to (31) Calculate the mean field m according to (26) Calculate the computation
    offloading policy according to Algorithm 1 end while SECTION VI. Numerical Results
    In this section, the performance of the proposed algorithm is evaluated by simulation.
    To demonstrate the superiority, we first analyze the convergence property and
    the delay performance in Subsection VI-A. Then, we compare the proposed algorithm
    with conventional algorithms in Subsection VI-B. In the simulation, there are
    1000 devices located in a 100m ×100m area with 3 MEC servers [37]. The path loss
    model is P L ij =15.3+37.6lg d ij with the fading coefficient distributed as CN(0,1)
    [38], where d ij is the distance between device i and MEC server j . The power
    efficient factor of device is 0.29 [6]. The noise power spectrum is set as −174
    dBm/Hz [39], and the system bandwidth is 20 MHz [40]. For the GAN, residual neural
    networks (ResNets) are used for both networks, which have three hidden layers,
    with 100 hidden units per layer. Tanh activation function is used for training
    ϕ w , and ReLU activation function is used in G θ . The learning rates are 4×
    10 −4 for ϕ w and 1× 10 −4 for G θ . A. Performance Analysis 1) Convergence Performance:
    Fig. 3 shows the convergence of the GAN-like neural network by analyzing the logarithm
    of the HJB residual error with varying numbers of MEC servers. The number of servers
    reflects the dimension of task allocation proportion. The fewer the MEC servers
    are, the faster the proposed algorithm reaches convergence. The HJB residual loss
    diminishes slowly as the process of update and oscillates around a small value
    in the end. Like other machine-learning methods, the convergence performance depend
    on the parameter settings of the neural networks. Fig. 3. Convergence analysis
    for the GAN algorithm. Show All 2) Intrinsic Performance: Fig. 4 gives the value
    function for different types of tasks under different states. As the computation
    queue length increases, the value function increases and its growth rate also
    increases. The reason is that the value function is the accumulation over time
    of the queue length. For different types of tasks, the value function is small
    when the task factor ξ k is large, because the larger ξ k means the higher offloading
    transmission rates assigned to the tasks, which will lead to faster reduction
    of the computation queue. Figs. 5 and 6 show the rate allocation for local computing
    and offloading transmission for different types of tasks. In particular, more
    local computing rates are assigned to the tasks with smaller ξ k . On the contrary,
    more offloading transmission rates are allocated to the tasks with larger ξ k
    . For the tasks with larger ξ k , the data size is smaller compared with other
    tasks, which means it leads less communication load for offloading compared with
    the tasks with smaller ξ k . Thus, the devices tend to allocate the tasks with
    large ξ k to the MEC server for computation offloading. On the other hand, for
    the tasks with small ξ k , the devices tend to compute the tasks locally due to
    their larger data size compared with the tasks with larger ξ k . Fig. 4. Value
    function for different types of tasks. Show All Fig. 5. Optimal local computing
    rate allocation. Show All Fig. 6. Optimal offloading transmission rate allocation.
    Show All In addition, we exhibit the evolution of the mean field in Fig. 7 to
    highlight the performance of the proposed algorithm. When the number of slots
    is small, the distribution of higher data queues increases because the policies
    that are not learned well. With the continuous learning to optimize the policies,
    the probability of lower data queue increases, and the probability of higher data
    queue approaches zero, which validates the proposed algorithm reduces the delay
    efficiently. Fig. 7. Evolution of the mean field. Show All B. Performance Comparison
    In this subsection, the proposed algorithm is compared with conventional algorithms.
    For performance comparison, three baselines are adopted as follows: Baseline 1,
    Mean Field Game without considering MEC Servers (MFG without servers) [29]: The
    computation tasks are offloaded in ultra-dense networks using the traditional
    mean field games, where the influence from MEC servers is neglected by assuming
    that the MEC servers have sufficient power to process the tasks. Baseline 2, Constant
    Rate Offloading Strategy (CROS): For constant rate offloading strategy, each device
    offloads the computation tasks at a constant rate, which is independent of the
    state. Baseline 3, Bisection Searching Iteration Algorithm (BSS) [41]: The task
    delay is minimized among the users by optimizing their task partition rates and
    offloading transmit powers. The proposed bisection searching algorithm is utilized
    to find the globally optimal solution. In Fig. 8, we compare the performance of
    energy consumption versus different number of devices, where the related energy
    consumption parameters can be seen in [37]. As seen in Fig. 8, the energy consumption
    of all four algorithms increases with the increased number of devices. This is
    because as the number of devices increases, interference among them becomes more
    serious, which affects the transmission efficiency and results in greater energy
    consumption. Compared with BSS and CROS algorithms, the proposed algorithm and
    MFG without servers algorithm consumes less energy since they can efficiently
    handle the interference among devices. For the proposed algorithm and MFG without
    servers algorithm, their energy consumption performance is almost the same. The
    reason is that MFG without servers has no difference in terms of energy consumption
    compared to the proposed algorithm. Fig. 8. Energy consumption comparison for
    different number of devices. Show All Fig. 9 shows the delay performance comparison
    for different numbers of devices. As the number of devices increases, the delay
    is reduced significantly for the proposed algorithm and MFG without servers algorithm,
    since the influence from other devices is estimated for computation offloading.
    The delay for the BSS algorithm is also reduced due to the non-orthogonal frequency
    division multiple access (OFDMA) technique when the number of devices is not sufficiently
    large, but this technique fails for large-scale devices due to the limited common
    resources. Fig. 9. Delay comparison versus the number of devices. Show All We
    have also included the delay performance for different number of MEC servers in
    Fig. 10. As shown in the figure, the average delay decreases for all four algorithms
    as the increased number of MEC servers. This is because the computing resources
    increase with the number of servers, resulting in reduced delay. Furthermore,
    the proposed algorithm achieves lower delay compared to the three baseline algorithms
    due to the consideration of the influence from other devices and MEC servers.
    It can be seen from the three figures that the proposed algorithm achieves good
    delay performance under different conditions, which proves that the proposed algorithm
    has good scalability and robustness. Fig. 10. Delay comparison for different number
    of MEC servers. Show All Fig. 11 indicates the delay performance comparison with
    different numbers of tasks. It can be observed that the delay increases as the
    number of tasks grows. Compared with the other baselines, the performance of the
    proposed algorithm is superior in terms of reducing delay, because it allocates
    the rates based on the characteristic of different tasks, which achieve more appropriate
    offloading decision by efficiently utilizing the communication and computation
    resources. We also illustrate the performance of the algorithms under tasks with
    different degrees of deviation in Fig. 12. where the same average value but different
    variances of ξ k ,k∈K are considered, where the higher variance is 0.5 and the
    lower variance is 0.125. The same average value but different variances of ξ k
    ,k∈K are considered, where the variance of the task reflects the degree of task
    difference. In the simulation, the higher variance is 0.5 and the lower variance
    is 0.125. It can be seen that the proposed algorithm shows little difference in
    the case of different variances of ξ k ,k∈K because it allocates the rates based
    on the task characteristics. However, when the variance of task increases, the
    delay of the two baseline algorithms increases significantly. Figs. 11 and 12
    demonstrate that the proposed algorithm can adaptively schedule tasks based on
    their types and the differences, i.e., task factors, ξ k ,k∈K , and exhibits good
    robustness. Fig. 11. Delay comparison versus the number of tasks. Show All Fig.
    12. Delay comparison versus different degrees of deviation for tasks. Show All
    SECTION VII. Conclusion and Future Work In this paper, we consider distributed
    computation offloading in a large-scale MEC system and propose an optimal distributed
    computation offloading policy by adopting the improved MFG algorithm. To minimize
    the delay, we formulate the problem as an MDP and derive the HJB equation with
    the unknown task allocation proportion. To present the evolution of the state
    in the system, we convert the influence from other devices into the mean field
    term and derive the FPK equation with multiple tasks, while reflecting the influence
    from MEC servers in the cost function. To solve the MFG problem involving the
    coupled HJB and FPK equations, we propose a GAN-based algorithm to solve the MFG
    problem due to the non-calculability and insufficient dataset of the task allocation
    proportion. Specifically, for the generator, it learns the task allocation proportion,
    and for the discriminator, it updates the value function and back-propagates the
    loss to train the neural network. To calculate the loss function, we use the Lax-Friedrichs
    method to update the mean field and propose a water-filling algorithm to derive
    the optimal computation offloading policy. The simulation results illustrate that
    the tasks with large computation amounts and small data sizes are scheduled with
    high priority to be offloaded to MEC servers. The results also validate the efficiency
    and convergence of the proposed computation offloading policy. The proposed framework
    can be further extended to the scenarios considering device location and imperfect
    CSI [42]. For the cases that multiple devices share the same computation task
    [43] or the same computation resource [44], the joint optimization of resource
    allocation and computation offloading will be considered in the future work. Appendix
    A Derivation Process of FPK Equation Define ϕ(x) as a smooth and compactly supported
    function. The integral ∫m(q(t),t)φ(q(t))dq can be seen as the limit of the sum
    1 N ∑ i∈N φ( q i (t)) , where q i (t) is the state of device i at time slot t
    . We differentiate the two parts w.r.t. t and use the chain rule, then ∫ ∂ t m(q(t),t)φ(q(t))dq
    ≈ 1 N ∑ i∈N ∑ k∈K ∂φ( q i (t)) ∂ q k i (t) ∂ q k i (t) ∂t . (44) View Source As
    the continuum limit N→∞ , we have ∫ ∂ t m(q(t),t)φ(q(t))dq] =∫ ∑ k∈K ∂φ(q(t))
    ∂q(t) ∂q(t) ∂t m(s(t),t)ds. (45) View Source Integration by parts, we have ∫(
    ∂ t m(q(t),t)+ ∑ k∈K ∂ q k ( ∂ t q(t)) m(q(t),t))φ(q(t))dq=0, (46) View Source
    which satisfies for arbitrary φ(q(t)) , thus the FPK equation is derived as ∂
    t m(q(t),t)+ ∑ k∈K ∂ q k ( ∂ t q k (t)m(q(t),t))=0. (47) View Source Appendix
    B Proof of Proposition 1 To equivalently transform the MFG problem, we first rewrite
    the MFG problem (28) as sup ϕ i inf m, υ i , R ij , β ij ∫ T 0 ∫ c i (t)m(q,t)dqdt
    − ∫ T 0 ∫ ϕ i (q,t) ∂ t m(q,t)dqdt − ∑ k∈K ∫ T 0 ∫ ϕ i (q,t) ∂ q k (m(q,t) ∂ t
    q k )dqdt. (48) View Source Then, we integrate by parts w.r.t. t for ∫ T 0 ∫ ϕ
    i (q,t) ∂ t m(q,t) dqdt and integrate by parts w.r.t. q for ∫ T 0 ∫ ∂ q k (m(q,t)
    ∂ t q k ) dqdt . We can obtain sup ϕ i inf m, υ i , R ij , β ij ∫ T 0 ∫ c i (t)m(q,t)dqdt
    −∫ ϕ i (q,t) ∂ t m(q,t) ∣ ∣ ∣ T 0 dq+ ∫ T 0 ∫ ∂ t ϕ i (q,t) m(q,t)dqdt− ∫ T 0
    ∑ k∈K m(q,t) ∂ t q k ∣ ∣ ∣ q dt + ∫ T 0 ∫ ∑ k∈K ∂ q k ϕ i (q,t)m(q,t) ∂ t q k
    dqdt. (49) View Source Assume the zero flux condition for the continuity equation
    [36]. Hence, we have m(q,t) ∂ t q k ∣ ∣ ∣ q =0 . Inserting the results into (49),
    we obtain sup ϕ i inf m, υ i , R ij , β ij ∫ T 0 ∫ c i (t)m(q,t)dqdt +(∫ ϕ i (q,0)
    ∂ t m(q,0)dq−∫ ϕ i (q,T) ∂ t m(q,T)dq) + ∫ T 0 ∫ ∂ t ϕ i (q,t)m(q,t)dqdt + ∫ T
    0 ∫ ∑ k∈K ∂ q k ϕ i (s,t)m(q,t) ∂ t q k dqdt. (50) View Source Rearranging the
    above equation, we have sup ϕ i inf m, β ij ∫ T 0 (∫( ∂ t ϕ i (q,t)+H(q, ∂ q k
    ϕ i ))m(q,t)dq)dt +∫ ϕ i (q,0)m(q,0)dq−∫ ϕ i (q,T)m(q,T)dq, (51) View Source where
    H(q, ∂ q k ϕ i )= inf υ i , R ij { c i (t)+ ∑ k∈K ∂ ϕ i (q,t) ∂ q k ∂ q k ∂t }
    , which completes the proof. Appendix C Proof of Theorem 2 Since the proofs of
    the two equations are similar, we only prove one of the two equations. Define
    R ~ 1∗ ij , R ~ 2∗ ij ,⋯, R tK∗ ij as the optimal transmission rate allocation
    of device i . Without loss of generality, we assume R 1∗ ij , R 2∗ ij >0 and ∂
    H i (t) ∂ R 1∗ ij < ∂ H i (t) ∂ R 2∗ ij . There exists δ∈(0, R 2∗ ij ) , which
    satisfies ∂ H i (t) ∂( R 1∗ ij +x) < ∂ H i (t) ∂( R 2∗ ij −x) for 0<x≤δ . Therefore
    H i ( R 1∗ ij +δ, R 2∗ ij −δ) = H i ( R 1∗ ij , R 2∗ ij )+ ∑ 0 δ ( ∂ H i (t) ∂(
    R 1∗ ij +x) − ∂ H i (t) ∂( R 2∗ ij −x) ) δ x < H i ( P ~ 1∗ ij , P ~ 2∗ ij ).
    (52) View Source The above result shows that the new transmission rate allocation
    [ R 1∗ ij +δ, R 2∗ ij −δ,⋯, R K∗ ij ] is better than R 1∗ ij , R 2∗ ij ,⋯, R K∗
    ij , which contradicts the assumption and indicates ∂ H i (t) ∂( R 1∗ ij ) = ∂
    H i (t) ∂( R 2∗ ij ) . On the other hand, suppose that the optimal transmission
    rate allocation of device i is R 1∗ ij , R 2∗ ij ,⋯, R K∗ ij and ∑ k∈K R k∗ ij
    < R max ij . When we add increment δ 1 , δ 2 ,⋯, δ K to the optimal transmission
    rate, we have c i ( R 1∗ ij + δ 1 , R 2∗ ij + δ 2 ,⋯, R K∗ ij + δ K )< c i ( R
    1∗ ij , R 2∗ ij ,⋯, R K∗ ij ) , which contradicts the assumption and completes
    the proof. Appendix D Proof of Theorem 3 According to (21), the Hamiltonian is
    H i ( q i (t), ∂ J i (t) ∂ q i (t) )= c i (t)+ ∑ k∈K ∂ J i (t) ∂ q k i (t) (−
    v k i (t) − ∑ j∈M γ ij (t) ξ k R k ij (t)+ α ¯ k i c k ). (53) View Source Based
    on (7) and (24), γ ij (t) is changed into γ ij (t)= a ij (t)ψ(m(q,t)), (54) View
    Source where ψ(m(q,t))= ∑ min{ ∫ s m(q,t) a j (q,t)dq, L j } k=1 k N j a (t) ⋅Pr[k|
    ∫ s m(q,t) a j (q,t)dq, L j ]. (55) View Source The optimal action can be obtained
    by minimizing H i ( q i (t), ∂ J i (t) ∂q(t) ) . Based on (39), H i (q(t), ∂ J
    i (t) ∂q(t) ) is a linear function w.r.t. a ij (t) with the linear coefficient
    F j (t)= ∑ k∈K ( β k ij ∑ l∈N ∑ k∈K β k lj α ¯ k i ∑ k∈K ξ k R k ij − ∂ J i (t)
    ∂ q k i (t) ⋅ ξ k R k ij (t)− 1 ∑ k∈K α ¯ k i ξ k R k ij (t))ψ(m(q,t)). (56) View
    Source Thus, the access decision a ij (t) is derived as a ij (t)={ 1, 0,  if  F
    j (t)<0, otherwise, (57) View Source which completes the proof. Authors Figures
    References Keywords Metrics Footnotes More Like This EAE-GAN: Emotion-Aware Emoji
    Generative Adversarial Network for Computational Modeling Diverse and Fine-Grained
    Human Emotions IEEE Transactions on Computational Social Systems Published: 2023
    MGDGAN: Multiple Generator and Discriminator Generative Adversarial Networks for
    Solving Stochastic Partial Differential Equations IEEE Access Published: 2022
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Wireless Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Delay-Optimal Computation Offloading in Large-Scale Multi-Access Edge Computing
    using Mean Field Game
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Thondebhavi Shanthakumar C.S.
  - Harish N.
  - 'Eshanya '
  - Giridharan A.
  citation_count: '1'
  description: Edge computing is a networking technology focused on placing data processing
    closer to the data source to reduce latency and bandwidth usage. Edge computing
    refers to moving some operations from the cloud to more on-premises locations
    for example edge server, IoT device or a user's Personal Computer. The quantity
    of long-distance communication required between a client and server is reduced
    when computing is moved to the edge of the network. Edge computing is a new technique
    that was created to get over the drawbacks of cloud computing. It has advanced
    using 5g technology based on locally distributed computing, storage, and control
    services. The core of fifth-generation (5G) wireless networks and beyond is edge
    computing. The software used in simulation of the topologies is Cisco Packet Tracer
    (CPT) which is a multi-platform visual simulation tool created by Cisco Systems
    that enables users to build network topologies and replicate modern computer networks.
    A cloud and edge computing topology model are simulated and compared for their
    working for processing the data, from source to the destination. Mainly focusing
    on low-latency of the topology. After which real time application models are created
    and simulated in fields of agriculture for vermicomposting, and city planning
    considering water logging as a major issue.
  doi: 10.1109/IITCEE57236.2023.10091014
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2023 International Conference... Internet
    of Things and Edge Computing for Real Time Applications Publisher: IEEE Cite This
    PDF Chandana Sai Thondebhavi Shanthakumar; Neha Harish; Eshanya; Anandi Giridharan
    All Authors 1 Cites in Paper 290 Full Text Views Abstract Document Sections I.
    Cisco Packet Tracer II. Case Studies III. Proposed Architecture IV. Results and
    Conclusion Authors Figures References Citations Keywords Metrics Abstract: Edge
    computing is a networking technology focused on placing data processing closer
    to the data source to reduce latency and bandwidth usage. Edge computing refers
    to moving some operations from the cloud to more on-premises locations for example
    edge server, IoT device or a user''s Personal Computer. The quantity of long-distance
    communication required between a client and server is reduced when computing is
    moved to the edge of the network. Edge computing is a new technique that was created
    to get over the drawbacks of cloud computing. It has advanced using 5g technology
    based on locally distributed computing, storage, and control services. The core
    of fifth-generation (5G) wireless networks and beyond is edge computing. The software
    used in simulation of the topologies is Cisco Packet Tracer (CPT) which is a multi-platform
    visual simulation tool created by Cisco Systems that enables users to build network
    topologies and replicate modern computer networks. A cloud and edge computing
    topology model are simulated and compared for their working for processing the
    data, from source to the destination. Mainly focusing on low-latency of the topology.
    After which real time application models are created and simulated in fields of
    agriculture for vermicomposting, and city planning considering water logging as
    a major issue. Published in: 2023 International Conference on Intelligent and
    Innovative Technologies in Computing, Electrical and Electronics (IITCEE) Date
    of Conference: 27-28 January 2023 Date Added to IEEE Xplore: 10 April 2023 ISBN
    Information: DOI: 10.1109/IITCEE57236.2023.10091014 Publisher: IEEE Conference
    Location: Bengaluru, India Edge computing, also known as distributed computing
    architecture, allows data computation and storage at the network''s actual physical
    edge, as near to the end user as feasible. Early computing technologies had centralized
    applications that ran only on isolated computers. Personal computing was an early
    evolution of computing that allowed distributed applications to run locally. Cloud
    computing evolved from personal computing and allowed centralized applications
    running in data centers. Edge computing is the current technology which is an
    advancement to cloud computing and it enables centralized applications running
    close to users, either on the device itself or on the network edge. We already
    use devices that do edge computing every day, like smart speakers, watches and
    phones - devices which are locally collecting and processing data while touching
    the physical world. Internet of Things (IoT) devices, robots, vehicles and sensors
    can all be edge devices, when capable of communicating with the cloud and computed
    locally. The prevalence of Internet of Things (IoT) devices has led to a wide
    variety of edge computing application cases. In its most basic form, edge computing
    involves relocating some storage and processing capacity away from the main data
    center and toward the actual data source. Instead of sending unprocessed data
    to a centralized data center for processing and analysis, that work is now done
    where the data is really created, whether that be on the floor of a factory, in
    a retail establishment, a large utility, or all throughout a smart city. The only
    result of computing work at the edge is sent back to the primary data center for
    analysis or other human interaction, real-time business insights, device repair
    predictions, or other actionable results. Not all computing tasks call for the
    same design, hence the architectures required for those tasks must be suited for
    those workloads. Edge computing has become a convenient and important architecture
    for bringing computing and storage resources closer together, ideally in the same
    physical location as the data source. Edge computing has become more significant
    because it successfully tackles new network difficulties associated with transporting
    the enormous volumes of data that today''s enterprises produce and consume. It''s
    not just a quantity problem. Applications also rely on processing and responses,
    both of which are becoming more and more time-sensitive. Many researchers have
    studied and published literature about the concept of edge computing, its uses
    and its applications using Internet of Things. A few of them have been referred
    to, for designing the case studies being mentioned in the paper. Weisong Shi et
    al [1] have defined edge computing and published a number of case studies to help
    put your ideas into practice, including collaboration edge and cloud offload,
    smart home and smart city, and more. They discussed many issues and opportunities
    related to edge computing. Krittin Intharawijitr et al [2] have studied the effects
    of both latencies on latency-sensitive services in the MEC architecture. To study
    MEC in a realistic architecture, they also considered a centralized model that
    uses controllers to coordinate flows between users and mobile edge resources.
    Simulation results show that controller intervals and hop delays cause certain
    system deadlocks and errors. However, a tolerant system that removes latency limits
    and selects edge servers with the lowest overall latency can dramatically improve
    system performance. To reduce the weighted-sum delay of all devices in a cloud-edge
    collaboration system, Jinke Ren et al [3] have examined cooperative communication
    and compute resource allocation. The initial step was to define a delay minimization
    problem with communication and compute resource restrictions. The design of collaborative
    cloud and edge computing in future mobile computing systems is clarified by this
    study. The effects on the number of end users and the size of the work on edge
    computing are discussed by Qinling Zheng and Zhan Ping in their paper [4], and
    quantitative latency evaluations utilizing simulations of the edge architecture
    are presented. These evaluations show storage performance, memory capacity, and
    CPU utilization by the edge architecture. A study conducted by Hamed Rahimi et
    al [5] examines Edge Computing in 5G in-depth and defines it in light of the needs
    of applications with extremely low latency. They provide a hybrid architecture
    as a contribution that benefits from cutting-edge and environmentally friendly
    technologies (including D2D communication, Massive MIMO, SDN, and NFV) and has
    important properties like scalability, dependability, and ultra-low latency support.
    On the basis of agent-based simulations, the suggested architecture is assessed
    to show that it can meet requirements and can react quickly to high volume demands.
    In a paper published by Zhen Yuan Pan et al [6], a Greedy algorithm based technique,
    Low-latency Services Offloading Policy (LSOPG) for 5G edge computing was developed.
    The LSOPG method improves the frequency of congestion caused by queuing among
    users, achieves an optimal balance between delay and load balancing, and inherits
    the low complexity and high efficiency of the greedy algorithm. In order to define
    requirements and challenges of devices for diverse use cases, an article published
    by Morghan Hartmann et al [7] first seeks to evaluate current and developing edge
    computing architectures and approaches for health care applications. Additionally,
    they gave a thorough analysis of the data operations performed by edge computing,
    including transmission, encryption, authentication, classification, reduction,
    and prediction. The table below provides the major differences between the already
    existing technology cloud computing and the emerging technology, edge computing.
    Points of Difference Edge Computing Cloud Computing Operation Happens on the device
    used Happens on cloud platforms like Google Cloud Platform, Amazon EC2, Microsoft
    Azure etc. Approach Edge computing processes time sensitive data Cloud computing
    processes data that is not driven by time Advantage One can independently scale
    the network with every new device added to the system One can store a huge amount
    of data on the cloud that can be scaled and hosted over the internet and can be
    accessed on demand Use Case Edge computing is apt for applications that require
    very low latency Cloud computing is perfect and suitable for companies or businesses
    that require huge amount of data storage and scalable hosting providers Security
    This model requires robust security systems that include tackling attacks proactively
    and advanced authentication methods Cloud computing requires security that is
    not time sensitive and the security systems are managed by the cloud service providers
    Applications *Online Data Storage *Backup and Recovery *Big Data Analysis *Testing
    and Development *E-commerce Application *Autonomous vehicles *Remote monitoring
    of assets in the oil and gas industry *Smart grid *Virtualized radio networks
    and 5G (vRAN) SECTION I. Cisco Packet Tracer The software used to simulate the
    proposed edge computing models are open source software. Open source software
    (OSS) is computer software provided under a license that allows users to freely
    use, study, modify, and share the software and its source code for any reason.
    Open source software can be created in a collaborative and public way. Open-source
    software is a well-known example of open collaboration because any competent user
    is able to engage in its development online, thereby doubling the number of potential
    contributors. Public confidence in the software is facilitated by the ability
    to study and change the code. A. Cisco Packet Tracer Cisco Systems created Packet
    Tracer, a cross-platform visual simulation application that enables users to build
    network topologies and simulate contemporary computer networks. Using a command
    line interface, this software allows users to replicate the configuration of Cisco
    routers and switches. Packet Tracer''s drag-and-drop user interface allows users
    to add and remove virtual network components. The operating systems Linux, Windows,
    and MacOS all support Packet Tracer. There are also apps for iOS and Android.
    A “cable” symbolizes a physical link between two machines. Packet Tracer provides
    a variety of simulated Application Layer protocols in addition to fundamental
    routing with RIP, OSPF, EIGRP, and BGP. Packet Tracer also supports the Border
    Gateway Protocol as of version 5.3. It is a tool that can be used for collaboration
    in addition to modeling some characteristics of computer networks. With Packet
    Tracer 5.0, several users can connect various topologies together over a computer
    network using the multi-user system feature. SECTION II. Case Studies A. Edge
    Computing in Agriculture-Vermicomposting The waste treatment technology of vermiculture
    is significant. Vermicomposting is highly likely to affect microorganisms and
    nutrients for plant growth. The endresult of the decomposition is the vermicomposting
    process. There are a variety of worm species, including red earthworms such as
    white worms, wigglers, and others. The earthworms can produce a variety of rotting
    materials, wastematerials, food or vegetable waste, and vermicast for lowering
    pollutant levels and raising the level of nutrient saturation. We think automatic
    vermiculture is smart. The kit is useful in enhancing the soil quality. Earthworms
    just need the barest amount of attention, but they also require consistency and
    a regular routine. As a result, it appears that the smart vermiculture farming
    kit supports the autonomous monitoring and management of environmental factors
    like humidity, temperature, moisture, and watering. As a result, it helps farmers
    to plan ahead, manage their time more efficiently, and improve the quantity of
    earthworm composting. Fig. Cloud model-vermiculture Show All The image above,
    a cloud model of vermiculture is being designed and simulated. Components like
    thermostat, water level detector, humiture sensor are being used as per their
    function and are assigned with the IP address of the server then connected to
    a wireless router which connects all the components together which are operated
    from the computerthat is being connected. B. Edge Computing in City Planning-Water
    Logging Metropolitan cities experience water logging during heavy rains. This
    can be the result of poor drainage or low-lying locations. In many areas when
    there is severe rainfall for three-four hours, water enters homes, which resulted
    in significant loss. If we can foresee or foretell which area of a city will experience
    significant rain, we can then take the necessary precautions. In the majority
    of low lying places, the sensors can be fixed. As a result, when the water reaches
    a specific level, an alarm will sound, and information will be sent to the BBMP
    office as well as to everyone in that neighborhood. Fig. Cloud model - water logging
    Show All The model above shows a cloud model to deal with the water logging. The
    components are connected in the similar way assigning the server''s IP address
    to connect and are operated accordingly using the end device computer that is
    being connected. SECTION III. Proposed Architecture A. Edge Computing Architecture
    By utilizing cloud, edge data centers and, when necessary, embedded data storage
    directly on devices, edge computing brings storage and data processing closer
    to applications and client device. Applications are protected from disruptions
    in the central and local data centers by this layered strategy. As local connectivity
    improves, each tier uses it to synchronize data both within and between tiers,
    which is more dependable. Applications that are always quick and always on are
    powered by edge computing. Fig. Edge computing architecture Show All The top layer
    of the diagram above illustrates cloud data centers, which is made up of a central
    data center and connected local data centers. In an edge computing architecture,
    the cloud data centers continue to be essential because they are the final information
    repository. The edge layer is the one below that. Edge data centers and Internet
    of Things (IoT) gateways are both part of the edge layer. These operate on a local
    area network, which may be wireless, fiber-optic, 5G, or an earlier network like
    4G. Individual devices, IoT devices, smart phones, tablets, and laptops that people
    carry are all visible in the edge layer and they communicate with the edge data
    center. Devices can also communicate with one another through a private area network,
    such Bluetooth or RF. One of the major advantages of this architecture is that
    it essentially involves the major advantages of edge computing operations. It
    enhances the security and enhances the reliability and resiliency of the data.
    It also enhances the privacy protection and data security boosting the overall
    performance of the system minimizing the response time and latency. Reduces the
    data loss and with the modern-day technological application it can support AI/ML
    applications. Based on these important aspects real time application specific
    models are made with respect to water logging and vermiculture applications. B.
    Case Study 1-Simulation and Architecture Edge Computing in Agriculture-Vermicomposting
    Fig. Vermiculture model simulation using CPT Show All With the use of Cisco packet
    tracer, a real time vermiculture model is being proposed and simulated for which
    a wireless router along with a local server which is connected to the cloud are
    taken which are then assigned with IP address, assigned to the server. Later the
    GUI set up of the outer is done after which the IoT after which all the edge devices
    and edge things are connected. This can be operated on given conditions via the
    edge devices. The edgeof the cloud data center is eventually connected to the
    edge smartphone device which can be used alternatively over the edge laptop to
    operate and monitor the IoT devices i.e. the edge nodes. C. Case Study 2-Simulation
    and Architecture b. Edge Computing in City Planning-Water Logging Fig. Water logging
    model simulation using CPT Show All Water logging being a major un-noticed issue
    that requires immediate attention but is left unread nowadays. Using CPT, a general
    model in order to avoid water logging is being simulated where the water level
    monitor senses that water level and sends information to the IoT based water drain
    which, with the increase in water level will open the water drain automatically.
    The devices are connected in the similar way as implemented for vermiculture.
    A further implementation could be done on how the rainwater can be collected and
    saved and used for things like irrigation, watering the garden and even use it
    for everyday chores by filtering the collected water. SECTION IV. Results and
    Conclusion The introduction of cloud computing raised the bar for data analytics.
    The cloud''s interconnectedness made it possible to collect and analyze data more
    thoroughly. With edge computing, things have become even more efficient. As a
    result, business activities are now of higher quality. Edge computing is a potential
    solution for data-driven tasks, depending on the circumstances that demand incredibly
    quick responses and a great degree of flexibility. The graph below shows how the
    time consumption i.e. how the latency is being minimized by edge over cloud in
    processing the data from any source to destination. As the graph depicts, edge
    not just shows the improvement in the speed but also more advanced compared to
    the cloud. Hence using a simple edge architecture, considering the real time problems
    majorly important one''s which can be implemented using computing has been proposed
    and simulated using networking platforms like GNS3 and Cisco packet tracer. Fig.
    Graph comparing Cloud and Edge computing architecture performance Show All The
    graph below shows the simulation comparison of the cloud model of the case study
    vermiculture model. Fig. Performance comparison of edge and cloud computing models
    for time taken - vermiculture Show All The graph shows the time taken by edge
    computing and cloud computing architecture models for vermiculture. The graph
    clearly shows the less time taken to deliver the information from source to destination
    by the edge model in comparison to the cloud model. Similarly, graph below shows
    the comparison with respectto time for cloud models for water logging. Fig. Performance
    comparison of edge and cloud computing models for time taken - waterlogging Show
    All Further implementation of the proposed models can be done using different
    network simulation software with further real time implementations included to
    the models. Authors Figures References Citations Keywords Metrics More Like This
    Connecting Two Worlds: Physical Models and Graph Models of Wireless Network Topologies
    2011 IEEE International Conference on High Performance Computing and Communications
    Published: 2011 The computational aspects of estimating the efficiency of wireless
    networks'' topology 2017 IEEE Conference of Russian Young Researchers in Electrical
    and Electronic Engineering (EIConRus) Published: 2017 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the International Conference on Intelligent and Innovative
    Technologies in Computing, Electrical and Electronics, ICIITCEE 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Internet of Things and Edge Computing for Real Time Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 69 papers. The special focus in this conference
    is on Computing, Communications, and Cyber-Security. The topics include: An Approach
    for Cloud Security Using TPA- and Role-Based Hybrid Concept; Decision Tree Algorithm
    for Diagnosis and Severity Analysis of COVID-19 at Outpatient Clinic; CSBRCA:
    Cloud Security Breaches and Its Root Cause Analysis; a Mobile-Based Patient Surgical
    Appointment System Using Fuzzy Logic; implementation of Green Technology in Cloud
    Computing; concurrency Control in Distributed Database Systems: An In-Depth Analysis;
    house Pricing Prediction Based on Composite Facility Score Using Machine Learning
    Algorithms; Malicious Website Detection Based on URL Classification: A Comparative
    Analysis; conversion of Intermittent Water Supply to Continuous Water Supply of
    Chandigarh: A Case Study; attribute Selection, Sampling, and Classifier Methods
    to Address Class Imbalance Issues on Data Set Having Ratio Less Than Five; timely
    Prediction of Diabetes by Means of Machine Learning Practices; detection of Brain
    Tumor Using K-Means Clustering; on Efficient and Secure Multi-access Edge Computing
    for Internet of Things; Execution Survey and State of the Art of Different ML-Based
    Ensemble Classifiers Approach Contextual Analysis of Spam Remark Location; real-Time
    Eyesight Power Prediction Using Deep Learning Methods; an Unsupervised Machine
    Learning Approach to Prediction of Price for Taxi Rides; facial Landmark Features-Based
    Face Misclassification Detection System; predictive Model for Agriculture Using
    Markov Model; a Novel Compression Method for Transmitting Multimedia Data in Wireless
    Multimedia Sensor Networks; a Comparative Analysis of Edge Detection Using Soft
    Computing Techniques; a Comprehensive Study of Pose Estimation in Human Fall Detection;
    preface.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 3rd International Conference on Computing, Communications, and Cyber-Security,
    IC4S 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Vangipuram S.L.T.
  - Mohanty S.P.
  - Kougianos E.
  - Ray C.
  citation_count: '2'
  description: Groundwater overuse in different domains will eventually lead to global
    freshwater scarcity. To meet the anticipated demands, many governments worldwide
    are employing innovative and traditional techniques for forecasting groundwater
    availability by conducting research and studies. One challenging step for this
    type of study is collecting groundwater data from different sites and securely
    sending it to the nearby edges without exposure to hacking and data tampering.
    In the current paper, we send raw data formats from the Internet of Things to
    the Distributed Data Storage (DDS) and Blockchain (BC) edges. We use a distributed
    and decentralized architecture to store the statistics, perform double hashing,
    and implement access control through smart contracts. This work demonstrates a
    modern and innovative approach combining DDS and BC technologies to overcome traditional
    data sharing, and centralized storage, while addressing blockchain limitations.
    We have shown performance improvements with increased data quality and integrity.
  doi: 10.3390/s22228725
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 22 Issue 22 10.3390/s22228725 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editor Francesco Longo
    Subscribe SciFeed Recommended Articles Related Info Links More by Authors Links
    Article Views 1451 Citations 2 Table of Contents Abstract Introduction Novel Contributions
    Prior Related Works Sources for Groundwater Data A DDS and Blockchain Platform
    Water-Quality Data Management System Architecture Algorithms for DDS and Blockchain
    Based Framework G-DaM Implementation G-DaM Results Conclusions and Future Direction
    for Research Author Contributions Funding Conflicts of Interest References share
    Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessArticle G-DaM: A Distributed Data Storage with Blockchain Framework for
    Management of Groundwater Quality Data by Sukrutha L. T. Vangipuram 1,†, Saraju
    P. Mohanty 2,*,†, Elias Kougianos 2,‡ and Chittaranjan Ray 3,‡ 1 Department of
    Computer Science and Engineering, University of North Texas, Denton, TX 76203,
    USA 2 Department of Electrical Engineering, University of North Texas, Denton,
    TX 76203, USA 3 Department of Civil and Environmental Engineering, University
    of Nebraska-Lincoln, Lincoln, NE 68588, USA * Author to whom correspondence should
    be addressed. † These authors contributed equally to this work. ‡ These authors
    contributed equally to this work. Sensors 2022, 22(22), 8725; https://doi.org/10.3390/s22228725
    Submission received: 14 October 2022 / Revised: 3 November 2022 / Accepted: 7
    November 2022 / Published: 11 November 2022 (This article belongs to the Special
    Issue Lightweight Security Integrity and Confidentiality for Internet of Things
    (IoT)) Download keyboard_arrow_down     Browse Figures Versions Notes Abstract
    Groundwater overuse in different domains will eventually lead to global freshwater
    scarcity. To meet the anticipated demands, many governments worldwide are employing
    innovative and traditional techniques for forecasting groundwater availability
    by conducting research and studies. One challenging step for this type of study
    is collecting groundwater data from different sites and securely sending it to
    the nearby edges without exposure to hacking and data tampering. In the current
    paper, we send raw data formats from the Internet of Things to the Distributed
    Data Storage (DDS) and Blockchain (BC) edges. We use a distributed and decentralized
    architecture to store the statistics, perform double hashing, and implement access
    control through smart contracts. This work demonstrates a modern and innovative
    approach combining DDS and BC technologies to overcome traditional data sharing,
    and centralized storage, while addressing blockchain limitations. We have shown
    performance improvements with increased data quality and integrity. Keywords:
    smart agriculture; Internet of Agricultural Things (IoAT); Blockchain (BC); Distributed
    Data Storage (DDS); edge system; groundwater quality data management 1. Introduction
    Water acts as an essential supporting element of life. In total, 96% of the water
    resides in oceans, and the remaining 3% of freshwater comes from sources such
    as rain, streams, rivers, lakes, and groundwater. About 1.69% of the freshwater
    comes from the ground [1] and is used mainly for agriculture and industry, which
    has put more pressure on global water resources. As the population is predicted
    to grow in the coming decades, so is the increased demand for food and crop yields.
    Groundwater utilization has expanded rapidly through water withdrawals and central
    pivots for irrigation and domestic purposes. Our higher dependency on water will
    result in the reduction in groundwater and its availability for the dependent
    life systems. Soil absorbs rainwater to store water in the ground [1]; however,
    due to global warming, rainfall patterns have been changing, affecting the sinking
    amount of water and gradually decreasing the earth’s freshwater supply. Similarly,
    using fertilizers excessively may increase nitrate contamination due to leaching,
    and a possible reduction in groundwater availability [2,3]. Data are the primary
    driving force for science. Data for groundwater availability are collected from
    different sources, such as an aquifer, climate science, law, public policy, and
    hydro-geology, with the help of sensors. The sensors for collecting agricultural
    data for fields are referred to as part of the Internet of Agricultural Things
    (IoAT). IoAT devices collect the statistics with suitable sensors in their raw
    format to recognize the problems. The devices collect unlimited data 24/7, which
    is helpful for later analysis. However, the IoAT is useful for collecting data,
    but it comes with its constraints that are discussed in more depth in Section
    2. Research and study on multiple data contexts received from these IoAT devices
    are complicated; combining and integrating all of these into a single platform
    is a more difficult challenge. Food production can increase with unlimited water
    resources; hence, data collection on agricultural farms is crucial. The entities
    involved in sharing the knowledge and technology from the groundwater sectors
    are minimal, which raises new issues from a political point of view. The data
    collected help researchers to construct different visualization, simulation, and
    study models to analyze groundwater reserves and calculate water levels for the
    next generation. Although data gathering helps in a significant way, incorrect
    information can lead to misleading analyses. Researchers and experts are more
    worried about the authenticity of the data because they may have been tampered
    with and modified in the data path [4]. Using the blockchain is one possible solution
    for researchers to avoid data integrity and quality problems. Storage systems
    with a central design face issues such as Internet dependency risks in data confidentiality,
    single-point failures, latency problems, and security, and are more prone to data
    attacks. Information gathered from different sources comes in various formats
    that need to be brought under one mode for sharing and storing. Some of the challenges
    included in managing groundwater data are listed in Figure 1. Advanced technologies
    such as the blockchain and distributed data storage methods can provide several
    benefits to overcome the issues encountered. Figure 1. Groundwater data management
    challenges. The blockchain delivers a decentralized architecture that uses cryptographic
    hashes for security to create immutable blocks comprising data transactions ordered
    in chain blocks. These chains of blocks are equal in size and have timestamps
    embedded. To validate the data transactions and secure them from malicious attacks,
    the blockchain uses complex mining protocols [5]. Smart contracts execute logic
    and act as small services for application program interfaces to implement access
    control. Although blockchain is famous for its immutable data transfer, it could
    be perfect for such applications. High fees, massive energy requirements, and
    slow data validation during increased traffic are a few of its challenges. Therefore,
    we perform distributed data storage with the help of an Interplanetary File System
    (IPFS). Progress in employing these technologies is taking place in different
    fields such as smart agriculture [6] and intelligent medical things [7] to ensure
    greater security for sensitive data. This paper highlights blockchain’s and DDS’s
    plausible role in supporting groundwater data management. The current paper is
    presented in the following order. By combining and extracting meaningful information
    from different fields of the groundwater discipline, we establish the present
    work. In Section 2, the problems with the current groundwater data management
    systems are discussed along with solutions. Prior related work and sources for
    groundwater data are discussed in Section 3 and Section 4, respectively. A novel
    architecture for the proposed G-DaM and algorithms are presented in Section 5,
    and Section 6, respectively. The implementation of the system is detailed in Section
    7 followed by the validation of the system in Section 8. Finally, Section 9 presents
    the conclusions for the current paper, and also outlines future research. 2. Novel
    Contributions 2.1. Problem Definition In conventional data storage systems, latency
    issues, IoT limitations, higher mining times, time-bound storage, and higher transaction
    costs are some of the main problems that can arise. We introduce an intermediate
    edge embedded with DDS and blockchain technologies to take in more extensive data,
    avoid central issues and maintain privacy and immutability when sharing the groundwater
    records. We use an interplanetary file system for DDS and the Ethereum public
    blockchain in the current application to overcome all the above challenges. Next,
    we discuss some of the problems and itemized novel solutions. 2.2. Current IoAT
    Challenges Agro-things work extensively without pause 24/7 to collect groundwater
    data, consuming high energy. The data collected are vast, and if they are not
    sent for storage in databases, more statistics can be lost due to time-bound storage
    limitations, which could have been helpful for research. Most current agro-things
    use central and cloud systems for storage. If the data in a centralized model
    lead to incorrect statistics, there is a possibility that every other device connected
    can be corrupted. During data transmission, these things can lose data integrity,
    trust, and quality as they can be hacked and tampered with easily. Figure 2 shows
    the challenges that occur in IoAT, cloud, and central systems used in Smart Agriculture
    for groundwater data collection. IoAT machines cannot process data securely and
    can increase latency issues using traditional methods for storage. However, IoAT
    devices, cloud, and central storage systems have undergone improvement in terms
    of distributed storage systems and studies implementing energy-efficient strategies
    have been performed [8,9,10]. Our current work tries to implement distributed
    methods to overcome these issues. Figure 2. Current IoAT, cloud and central system
    challenges. 2.3. Importance of Data Quality in Groundwater Data Transmission Data
    that are accurate and of a high quality play an essential role in forecasting
    the threats and dangers that can help in avoiding future disasters for humanity.
    Contamination of groundwater is a severe threat, and a global issue which can
    be can be caused by chemicals, road salt, bacteria, viruses, medications, fertilizers,
    and fuel. Wrong data predictions of groundwater quality can lead to dangerous
    health hazards, degrade the quality of the environment and impact socioeconomic
    development. A discussion of real-time disasters that have occurred due to groundwater
    contamination to show the importance of quality data transmissions is available
    in [11]. People staying near the river Woburn in Massachusetts in 1969–1979 were
    affected due to river pollution with industrial solvents. There have been traces
    of high water contamination which causes various diseases, including leukemia,
    liver, kidney, prostate, and urinary cancer. To overcome the water crisis in the
    city of Flint, the pipeline has been shifted to the river of Flint from the Detroit
    River and Lake Huron. Due to the high content of lead and other contaminants in
    the drinking water, many health problems, such as skin lesions, hair loss, high
    lead levels in the blood, vision loss, memory loss, depression, and anxiety, were
    observed in the people. In New Delhi, most water pipelines are connected to the
    Yamuna river. It is a highly contaminated river, and the reasons for its contamination
    include pesticides, copper, zinc, and nickel, due to which people are facing health
    issues such as death, disease, cancer, and organ damage. 2.4. Why Blockchain in
    Data Transmission? With blockchain, data transmissions can be performed with increased
    trust and quality. The communication between the entities or the stakeholders
    between the data collecting fields and the end systems can be achieved more securely
    and authentically using the blockchain because it acts as a ledger system. Once
    we write data on the blockchain, it cannot be reverted or tampered with as it
    uses encryption techniques to calculate a hash of the data transmitted. Using
    this property as an advantage in securing the relevant statistics makes blockchain
    suitable for sharing the data. Data storage in blockchain uses a decentralized
    architecture to hinder centralized storage issues. Although it has many benefits
    in securing the information gathered, it is more costly to store data with blockchain
    because of the gas (mining) fees it requires for each transaction. The advantage
    of a decentralized architecture is that it will not have a severe effect if a
    single node fails because other nodes will continue to function. Through this,
    it maintains adequate redundancy within the network. The data gathered are distributed
    among nodes and encrypted so only the owner can view the data. The blockchain
    takes care of data using the following two techniques: sharding and swarming.
    Sharding allows the file to be divided into smaller chunks for a quicker transfer.
    Some percentage of the node is retained for sharding in each transaction. The
    participants do not receive the entire file; instead, they are sent a part of
    the file. Only the owner knows the locations of the shards through a private key
    which is also beneficial when discovering shards. Swarming is a technique that
    keeps all the shards together and helps in decreasing latency while retrieving
    the files from the nearest nodes [5]. 2.5. Past Incidents of Insecure Data in
    Water Plants In February 2021, the water treatment plant in Oldsmar, Florida,
    was attacked by a group of hackers who were able to gain access to the operations
    technology system. The attack mainly aimed to increase the sodium hydroxide content
    in the water from 100 parts per million to 11,100 parts per million. That attempt
    was prevented by an operator who stopped the attack by reversing the toxic levels
    in the water [12]. A hacker attempted to poison a water plant in San Francisco
    Bay Area in January 2021. The hacker had all the details of a former employee’s
    TeamViewer account with which he could delete all the programs required for water
    plant treatment [12]. 2.6. Problem Addressed in the Current Paper Groundwater
    data management challenges can be classified into storage, pre-processing, and
    secure sharing. Attributes such as integrity, availability, security, access,
    ingestion, metadata, transformation, and warehousing can be sub-categorical. Figure
    1 illustrates different kinds of data management issues. Central storage vulnerabilities.
    Disadvantages of the blockchain for slow speed, energy-draining, scaling, and
    price. 2.7. Solutions Proposed in the Current Paper DDS through IPFS for off-chain
    storage to evade blockchain limitations. A blockchain-based data storage solution
    to overcome IoAT challenges. Access control approaches through blockchain smart
    contracts. Achieving privacy by combining both DDS and blockchain technologies.
    2.8. State-of-the-Art Solutions For improving the quality, overcoming IoAT constraints,
    and decreasing the uncertainty of the data, unique blockchain technology is used
    for groundwater data sharing and storing. For bulk data to be stored and shared,
    DDS is used, providing increased security to the derived statistics. A state-of-the-art
    architecture is presented for the current G-DaM with dual hashing security included.
    A result log is shown for comparing transaction times, fees, and costs between
    traditional blockchain and blockchain with distributed storage systems. 3. Prior
    Related Works Water quality data are collected using different platforms. The
    information gathered in these applications plays an essential role for water managers
    and researchers in making correct decisions and further analyses. The system in
    [13] is designed with different modules to gather water quality and query data
    with statistical charts using a client–server architecture. It sends collected
    reports through traditional central systems. The study in [14] employs GIS (geographic
    information systems) for the management of water quality information. The data
    are interpreted and collected in the form of geographic data and stored in traditional
    database tables and spatial records. In recognizing the quality and quantity of
    the water in aqua agriculture, the approach in [15] is implemented using a big
    data platform built on the SpringBoot and JPA frameworks and a traditional database
    for storing and sharing the data among farmers. Others [16] use Autonomous Surface
    Vessels (ASVs) for capturing data in shorter time periods with lowered costs.
    The data are stored either by utilizing the ASV onboard software, which is not
    efficient for real-time visualization, or traditional central servers. The pH
    level is measured for getting water quality in the domestic supply [17]. The sensor
    gives information regarding the water’s quality and the tank’s water level near
    residential areas. The data collected are sent to cloud systems and to mobile
    users for alerting purposes. The application in [18] mainly concentrates on the
    security of the data gathered through the Internet of Things using blockchain
    at every level, i.e., from the device layer to the communication level. Real-time
    water quality data are collected in [19] to detect any violation records using
    blockchain and to ensure privacy and integrity in the data flow. With the help
    of an information system and centralized techniques, a client–server architecture
    with a single database sector is developed in [20]. As the groundwater data are
    stored in different geographical divisions, the paper introduces a single system
    for a more straightforward and accessible analysis. Other visualizations and analysis
    techniques are performed in [21] to compare two-dimensional and three-dimensional
    images with the help of fuzzy queries and relational databases. The database is
    used for storing important WebGIS water information that is collected from diverse
    sources. The storage for different groundwater data formats in [22] is completed
    using a distributed framework. The structure makes use of ArcIMS Services for
    spatial metadata handling. All the metadata management is achieved through central
    systems with the help of the RDF/XML platform and the J2EE environment. By using
    the web-based central system in [23], the groundwater data are composed and managed.
    It proposes a unified framework for collecting, storing, and sharing over a vast
    network of data workers and end-system users. While these methods for monitoring
    and managing water quality data increased the information quality and achieved
    a united structure, limitations still need to be addressed in the power usage,
    cost, computation, and access control areas. Some are solely designed using a
    single blockchain, increasing the cost and energy consumed, while others practice
    web services and are dependent on centralized servers for storage. Ref. [24] discusses
    the limitations of traditional data sharing, centralized storage, and blockchain
    more elaborately, along with a study on how the blockchain is helpful in mitigating
    these problems. Relying on the cloud for data processing is risky because the
    system can have a single point of failure and unknown accesses. As there is an
    increase in groundwater utilization, it is necessary to verify its availability
    for future generations. Accurate studies need to be performed based on the facts
    collected, so we utilize distributed storage strategies with blockchain for access
    control and integrity. As groundwater data are one of the most critical forms
    of data, authenticity and access permissions are required for sharing the data
    among stakeholders. Blockchain is an efficient way to share data when dealing
    with sensitive information. Its functionality is similar to an immutable ledger
    that keeps a log of every transaction in sequential order. The consensus mechanism
    in the blockchain further provides immutability, permanency, and anonymity to
    the groundwater records. It mitigates different threats such as tampering, repudiation,
    disclosure of the information, and denial of service, which need to be fulfilled
    for a higher quality of groundwater data. DDS supports storage in a decentralized
    way using peer-to-peer network models that share the file across different nodes
    or computers. The file is broken into smaller parts and distributed among a network
    of end systems to track the file with hashes. Table 1 presents different domains
    and data management strategies developed for information administration using
    diverse platforms and technologies. To the best of our knowledge, the current
    design combining DDS and Blockchain security is the first such attempt at groundwater
    data management. Table 1. Data management and storage approaches for water Quality.
    4. Sources for Groundwater Data The data can be collected using different techniques
    and platforms, such as remote sensing, multimedia, spatial, and other sources.
    The information gathered for nitrogen content in crops [25] is in a geospatial
    format, which differs from data in text or numerical formats. For securing and
    storing each of these types of data, experts use different methods. Figure 3 shows
    the available sites set up by the United States Geological Survey (USGS) for collecting
    water quality data in the state of Texas. These data-collecting centers record
    water quality and send the information to nearby institutes for making decisions
    and further research. For data scientists to suggest solutions, they must fully
    comprehend the water quality statistics and data origin. The U.S. Geological survey
    conducted in 2015 shows the water usage, which can be seen in Figure 4 [26]. The
    information gathered can be broadly categorized into structured and unstructured.
    The data in the structured format are in a table form, also called a relational
    database. In contrast, unstructured data include video, audio, text, and images
    that require a complicated structural design for sharing and storing. Figure 3.
    Water Quality Data Collection Sites of USGS -Texas. Figure 4. Groundwater and
    Water Quality Data Users. 4.1. Activities on Field One of the primary sources
    of data are observations collected during field operations. The activities include
    drilling, pumping, and monitoring operations. The information gathered with these
    techniques is robust in terms of accuracy. Drilling and pumping operations tend
    to be occasional, while monitoring is performed quarterly or less frequently [27].
    This type of data collection is structured and typically performed locally within
    an aquifer; although, the recent addition of sensors allows for off-site data
    collection. 4.2. Historical Historical data are in an unstructured format and
    contain legacy reports, physical maps, and text documents. Digitizing and transforming
    these sources of information into machine-readable data can create a new stream
    of more critical data [28]. 4.3. Remote Sensing This type of source forms data
    using primarily satellite, airborne, or ground-based instruments for observations
    [29]. They contain both structured and unstructured formats that are multi-dimensional,
    heterogeneous, and have increasingly voluminous datasets. 4.4. Computer Simulation
    Hydrological data are generated through computer models that use numeric methods
    and simulation techniques. Atmospheric models and land surface models apply complex
    mathematical equations to predict weather forecasts and integrate hydrological
    data with biological and radiation-based processes on land [30]. The source contains
    both structured and unstructured formats with multi-dimensional, heterogeneous,
    extensive data. 4.5. Web and Social Media With the emergence of the Internet,
    a new way of communication and transfer of information is practiced. Web and media
    can include text, images, videos, or audio, forming an unstructured data format
    [31]. Mostly, this source type is found on web pages and social media posts. 4.6.
    Internet of Things (IoT) Connected devices are intelligent equipment that can
    join each other and digital systems over the Internet. These “things” continually
    stream environmental statistics. IoT systems can generate and collect large amounts
    of data faster than conventional or manual data collection techniques. With increasing
    demands to make applications smart, intelligent things are also growing. IoT fields
    include city, home, agriculture, medical, and industrial fields. Smart agriculture
    is a field that involves different IoT Sensors to collect data on humidity, water
    range, light, etc. [32]. They gather information and connect to the farmer using
    mobile devices to detect farming field conditions remotely. Some of the smart
    developments are briefly discussed here to show their relevance. Ref. [33] presents
    a unique device for crop disease predictions, irrigation, and crop selection in
    an automatic method with a solar sensor node. It can also capture crop images
    with continuous sensing. Another innovative agricultural application [34] is a
    clever greenhouse to increase yield and adapt to farming changes with changing
    environments. With the help of smart IoT devices, medical statistics are also
    collected, where control sharing and access management are essential. With added
    blockchain immutability in [35], a smart pillow-Internet of Medical Things (IoMT)
    application is built for stress control and supervision. 4.7. Groundwater and
    Groundwater Quality Data User Domains Here, we discuss the receivers of the groundwater
    and the actors that benefit from the high quality groundwater data [36]. Private
    and public distributors distribute the water supply to the public through withdrawals
    and connect them to parks, swimming pools, fire departments, and wastewater treatments.
    These water supplies also include water distribution for residential and domestic
    needs for drinking, sprinkling, and washing. The agricultural division for growing
    fruits and vegetables to supply food for the world population is the most crucial
    recipient of groundwater and its quality data. The groundwater used in irrigation
    should be free from chemicals to obtain healthy produce. Livestock is another
    area that requires high levels of groundwater and quality data. The animals on
    the field require water for drinking, sanitation, and other hygienic purposes.
    Thermoelectric power is generated by sending water to turbines that circulate
    between heat exchangers to produce electricity. A huge percentage of water is
    also sent to industrial use for manufacturing daily usage products and is also
    essential for controlling the dust during the mining process. All these sectors
    utilize water as their primary source. Figure 5 shows the groundwater withdrawals
    across the United States. Figure 5. Groundwater withdrawals in United States.
    5. A DDS and Blockchain Platform Water-Quality Data Management System Architecture
    Measuring water quality is required as more groundwater is becoming contaminated
    through its overuse, storage tanks, pollution, septic tanks, uncontrolled harmful
    waste, and medical waste in drinking water supplies. Sensors are used to collect
    data and send them to end systems for sharing and storing. Different sources discussed
    in Section 4 are helpful in gathering and storing the information from their respective
    end stations. These end systems can also be referred to as edge system nodes that
    need to provide data integrity, privacy, storage, and security while transmitting
    the data. Each of these nodes participates by combining DDS storage and blockchain
    functionalities to create a unified and orchestrated method to manage groundwater
    data. 5.1. Interplanetary File System (IPFS)—DDS In Section 1, we discussed some
    of the limitations of blockchain for validating and storing large amounts of data;
    with this constraint, off-chain storage for information is a feasible solution.
    Deciding which information stays on-chain and which goes off-chain is essential.
    Storj1, FileCoin2, Sia3, and IPFS are some off-chain storage examples. Data can
    be kept secure using off-chain methods to distribute the files among various nodes
    using encryption and shredding techniques. The IPFS decentralized file-sharing
    platform recognizes the documents and folders through content. It mainly depends
    on the distributed Hash table (DHT) to recover the locations of the file and information
    regarding node connectivity. When a file gets uploaded to IPFS from the end station,
    it is divided into 256 KiloByte maximum length segments. IPFS blocks are referred
    to as segments to differentiate blockchain blocks from IPFS blocks [37]. Every
    segment is recognized using a cryptographic hash calculated according to its content,
    called a content identifier (CI). A Merkle-directed acyclic graph (Merkle DAG)
    depicts a complete file through its root hash and can be used to rebuild a file
    from its segments inside the IPFS. A DHT works on the principle of a distributed
    key-value store. It uses distance metrics along with node identifiers to store
    and reclaim the information quickly. When reading for the value, the end systems
    try to find other nodes close to the key and obtain the value/content. To write
    a value, the nodes establish already defined end stations that are most relative
    to the key and inform these nodes of the key attribute value, using buckets inside
    the network to track nodes [38]. IPFS makes use of S/Kademlia [39] for DHT. This
    secured Kademlia algorithm provides two distinct forms of information. Firstly,
    when a file is uploaded from the end station, this node registers itself as a
    file segment provider. Secondly, DHT provides information regarding how to connect
    to the node with the help of an identifier. In this way, the IPFS node appeals
    to the providers from DHT and links to retrieve a file. 5.2. BC-Ethereum Smart
    Contract Ethereum is one of the popular blockchain application development tools.
    Transactions in Ethereum are completed using a cryptocurrency called ether, and
    smart contracts are used to write the main application logic. The solidity programming
    language is used to design the contract, and when it compiles, a bytecode is generated
    that is understandable only by the Ethereum Virtual Machine (EVM). Smart contracts
    are mainly Turing complete and can be utilized for various purposes. Ethereum
    primarily works in a decentralized way that ensures that the control for executing
    is not in the hands of nodes and embeds trust using a consensus mechanism. With
    this trusted method, data in the transactions cannot be changed or modified. The
    access control procedures such as variables, mappings, and structures can be used
    in the solidity programming language and called using conditional statements.
    If these statements meet the norms, the state is not modified; if they don’t,
    the state returns to its original value. Inside the smart code, a state variable
    can be coined to assign a value to store on the blockchain. An owner state variable
    can be called inside the contract migrations and assigned to the msg.sender().
    The variable’s value is defined inside the constructor function and called on
    whenever the smart contract is created for the first time or deployed to the blockchain.
    As solidity is a statically typed language, we can declare a variable as the string
    datatype and enable the public to access the value outside of the contract [40].
    For writing and reading the values inside the state variable, the programming
    language provides functions such as set() and get() along with multiple access
    control functions such as amIOwner(), amIOwnerMultiple(), checkAccess(), and checkAccessMultiple().
    To make Ethereum’s states persistent, we can declare them constant. 5.3. Architecture
    A setup of the DDS-IPFS platform is developed between the data source and the
    blockchain to communicate with the smart contract inside the blockchain. It acts
    as a mediator for moving the transactions to the methods of smart contracts for
    taking control of the storage and communicating with the network gateways and
    DHTs. The currently proposed system G-DaM architecture is given in Figure 6. Here,
    the data traveling from the IPFS to the blockchain are represented as transactions.
    Figure 6. Proposed Blockchain Architecture for Groundwater Data Management with
    DDS. 5.3.1. Adding File When the end system submits a groundwater data file, the
    IPFS creates segments of the file with a corresponding Merkle DAG and content
    identifiers and provides the hash string as the output. The secured Kademlia protocol
    consists of subprotocols to identify and verify the node through Content Identifiers.
    Some nodes may be unreachable due to network address translators and firewalls;
    IPFS overcomes these nodes through filtering. Each object in IPFS storage includes
    two fields, one for the data and the other for links. The data field contains
    binary data, which are of a specific size. The links field is further divided
    into the link name, a hash of the linked object, and the linked object size. Every
    node or peer that has IPFS as the form of distributed storage maintains a routing
    table with links for other peers. A routing table decides where the moving data
    should be inside the network. 5.3.2. Linking IPFS Data to Ethereum Smart Contracts
    There are two types of accounts in Ethereum, namely externally owned accounts
    and contract accounts. With the help of private keys, Ethereum addresses, and
    digital signatures, the externally owned accounts can hold the ether cryptocurrency
    to perform transactions. The same follows with contract accounts, but the difference
    is that they are controlled through programming code. Private keys are at the
    core of the Ethereum accounts, and they determine the Ethereum address, referred
    to as the account. Access control and monitoring of the data are achieved through
    digital signatures created using private keys. To be included, the transaction
    inside the blockchain Ethereum transactions requires a valid digital signature.
    Any peer who obtains the private key can become the transaction owner; therefore,
    keys are stored in particular files and Ethereum wallet software such as metamask.
    Ethereum makes use of public-key cryptography. Registering the hash string file
    from IPFS inside the smart contract is carried out using addBlock functions, and
    the transactions are verified based on the CI’s. The calling set() function inside
    the contract writes the hash string file as a transaction to the block. Elliptic
    curve cryptography (ECC) multiplication is applied to the transaction data. ECC
    is a one-way function where the multiplication is performed in a single direction
    but is impractical to reverse. The private key owner can create public keys and
    share them with different nodes, realizing that no node calculates the function
    to obtain the private key. This arithmetic method provides secure digital signatures
    which make the transaction data tamper-resistant with total ownership and control
    of the contracts. The transactions are listed as a Merkle binary hash tree which
    can help to add new blocks to the previous chain. The protocol produces hashes
    in a bottom-up direction and avoids fake groundwater files from the beginning
    through a proof of work (PoW) consensus mechanism. The root hash on the tree acts
    as the digital footprint to make the transaction block valid. The PoW algorithm
    confirms transactions or the data in the blocks and adds them to the chain. This
    algorithm mainly uses mathematical puzzles that can be solved. Those who solve
    them are miners, and the process is mining. Once the hash string from IPFS is
    valid and added to the blockchain, it generates a transaction hash on the blockchain
    explorer etherscan to retrieve the file. 5.3.3. Retrieving the File Inside the
    smart contract, the get() function is defined and called to read the file whenever
    requested by the owner or nodes with the correct permissions. Once the required
    authorizations are provided, a groundwater user sector node can request and obtain
    the corresponding files. To achieve this, the user node checks for the transaction
    hash content identifier with the source checksum content identifier to retrieve
    and reassemble the file. If there are no authorizations provided in the contract,
    there is no reply to the request. 6. Algorithms for DDS and Blockchain Based Framework
    From the edge systems (EdS), the data move towards the IPFS, and from there to
    the blockchain, as stated in Algorithm 1. Public-key cryptography and SHA-256
    are used in distributed data storage for hashing the uploaded files. Both private
    and public keys are generated, respectively, for each edge system to control access,
    to provide unique messages called digital signatures and for signing the groundwater
    quality data file. The file uploaded to the edge system is given as FL. The react
    JS used for the front-end design oversees the file uploaded. Once the water quality
    data file is submitted, it is converted into the buffer (EdS), Buf file of each
    256 kB Buf265 KB. The buffer file is attached with the private key and is then
    signed. The IPFS digitally signs the hash string/hash message “h(Buf)” produced;
    and h denotes the hash function. The signed hash string is then called by the
    set() function in the smart contract. With the help of the elliptic curve digital
    signature algorithm (ecdsa), a signature output of the “h(Buf)” is generated.
    To order the Ethereum objects, an encoding technique called recursive length prefix
    (rlp) is used. pk represents the signing private-key of the blockchain, and e
    is the RLP encoded data. Fun keccak256, Fun signature represent the functions
    for the keccak-256 hash and signing algorithm, respectively. Once the data are
    hashed/signed twice, the smart contracts help in reading and writing the transaction
    for the blockchain using access rules. Algorithm 1 Data from Groundwater endsystems
    to IPFS and blockchain.   1: EdS, BC generate their respective Public and Private
    Keys (PuEdS, PrEdS) and (PuBC, PrBC)   2: EdS(FL)→Buf→Buf265 KB.   3: SC[set()]→Buf265
    KB→DDS.   4: The file gets hashed through cryptography method using SHA 256 to
    give distinct fingerprints represented as CI(Content Identifiers).   5: PuEdS
    = h(PrEdS * C), where C acts as a constant, * is a mathematical operation that
    is calculated in single direction and H is the secured hash function.   6: if
    FL==h(PrEdS * C)==h(Buf265 KB) then   7: Publishing h(Buf265 KB)→DDS, using IPFS
    client.   8: SC[get()] and SC[Publish()] functions to publish “h(Buf265 KB)” from
    DDS.   9: Signing “h(Buf265 KB)” with esdsa, Signature = Fun signature (Fun keccaK256(e),pkk).
    10: Attaching the ecdsa signature to the transaction. 11: if “h(Buf265 KB)” is
    signed with ecdsa algorithm then 12:     The hash maps in Sc are used for accessing
    the IPFS hash string towards ethereum accounts. 13:     Hash map has device owners,
    address and device id as key along with with hash string encrypted that is written
    on Blockchain. 14:     The write access policy checks for the validity of the
    data and functions in Sc help is publishing the encrypted data. 15:      if Device
    owner and address are related device id. then 16:          Runs the Write operation.
    17:      else 18:          Deletes Write operation. 19:      else 20:          Process
    End. 21:      else 22:          Process End. 23:      end if 24:     end if 25:
    end if 26: Repeat the steps from 1 through 26 every time edge system collects
    groundwater quality data. The steps for recovering the data from the blockchain
    to the user domains (Ud) are provided in Algorithm 2. The user domains should
    have the signature values and ordered transactions for retrieving the file. In
    the water quality data signed, private and public keys for creating the signatures
    are also present. The user domain ensures the water quality data are signed to
    authorize the signature and check if the hash functions have been compromised.
    Only the user domains with appropriate values can contact and receive the file.
    A complexity of 𝑂(1) [39] is required for validating and solving the cryptographic
    puzzles. Algorithm 2 Data from Blockchain to User Domains.   1: BC and Ud generate
    their respective Public and Private Keys (PuBC, PrBC) and (PuUd, PrUd).   2: The
    requester sends for data access request.   3: The access request gets signed by
    Requester’s private key (PrAr) and the signature gets attached along with data
    request.   4: The request for data access is concatenated with the signature an
    is then encrypted by public key of Edge system (PuEdS) for publishing from the
    client side Smart contract.   5: The request gets decrypted by the Edge System
    and uses signature for verifying the data integrity.   6: if Signature matches
    then   7:     The permission for reading the data is given to the requester.   8:     The
    owner, address and the id details of the device are provided by the requester.   9:     The
    owner, address, and id of the device are maintained in the smart contract hash
    map along with the registered user domains. 10:     if owner, address and id of
    requester matches hash map of smart contract then 11:      data can be accessed
    to read by the requester. 12:     else 13:      Declined the data access. 14:     else
    15:      Process End. 16:     end if 17: end if 18: Repeat the steps from 2 through
    18 every time there is a new user sector access request. 7. G-DaM Implementation
    Some dependencies are significant for the DDS application design, which are briefly
    discussed here. Ganache is a personal blockchain platform that is mainly used
    for deploying smart contracts, application development, and running tests locally
    that mirror actual public blockchain. Figure 7 shows ten free accounts provided
    by the mirror blockchain Ganache for developing distributed applications. Ganache
    initiates by setting up a platform for writing smart contracts with the help of
    a nodes package manager (Npm) and truffle framework (Tf). The local nodes are
    initiated with Npm, and Tf provides different tools for developing the present
    application. The tools in Tf help with smart contract management, testing in an
    automated way, contract migration and deployment, network management, running
    scripts for JS client code, and developing client-side code [41]. For the front-end
    design of the application, the react-java script (reactJS) framework is used,
    as shown in Figure 8. Figure 7. Ganache local blockchain. Figure 8. G-DaM User
    Interface. The Infura IPFS gateway has an ipfs-http-client package that can be
    installed using a local node. The package can be called from the front-end reactJS
    for attaining distributed storage for the current G-DaM application. Another essential
    package that is used for communicating Ethereum and local nodes is web3.js. The
    front end of the G-Dam system is connected to the backend blockchain by configuring
    the Tf to the Ganache host address 127.0.0.1:7545. A regular browser cannot be
    used for communicating with the blockchain; instead, a metamask extension browser
    is helpful. The metamask also handles personal accounts, funds, and fees for data
    transactions. The logic code inside the smart contract helps in interacting with
    the string data generated from IPFS which are forwarded to the blockchain. Testing
    is one of the crucial stages of application development. Blockchain testing plays
    a vital role since contract code execution on an actual blockchain will lead to
    higher risks due to its non-reverting property. The G-Dam application here is
    tested using Tf in local Ganache to verify its efficiency and deployed in the
    Ropsten test network for live setting performance testing without the use of real
    ether and mainnet tokens. 8. G-DaM Results We submit the water quality data file
    to the front-end to read the input in the form of a buffer, and the resulting
    IPFS hash string is delivered, as shown in Figure 9. Figure 9. File to buffer
    to hash. The metamask ethereum wallet acts as a connection medium between the
    user interface and Ganache. The hash string is generated from the front-end form
    linked to DDS-ipfs. Once the hash is received, the metamask asks to confirm the
    transaction to store the ipfs hash on the blockchain, which in turn provides a
    cryptographic transaction hash. Both the ipfs hash string output and the Ganache
    input are verified to be the same, as underlined in Figure 10a, and then deployed
    to ropsten testnet, which mirrors the functionality of the actual mainnet. Once
    deployed to the testnet, the transaction hash is provided along with the status,
    timestamp, block number, ether used, and the gas used, as shown in Figure 10b,c.
    The complete flow of data for the current G-DaM application is shown in Figure
    10. Figure 10. Dataflow from User Interface to Back-End Blockchain. (a) Hash verification
    & deploying to ropsten. (b) Transaction/Blockchain Hash. (c) Transaction Validating
    Time. Datasets The datasets we used for testing the current application are given
    in Table 2. These datasets comprise the water quality data for each state in the
    United States and are collected from the US Geological survey [42]. The datasets
    are initially compressed into a .zip format. We tested each data sample for its
    integrity, privacy, quality, and security through double hashing, one executed
    with ipfs and the other with the blockchain, as given in Table 3. Table 2. Datasets
    for G-DaM. Table 3. Water quality Data sharing with double hash refuge. The information
    regarding one ether(eth) price is $1098.84, and the mining time is 13.96 s for
    1 MB of data [43] as of 30 June 2022. For 1 KB of data to be shared and stored
    on the blockchain, it would require 0.032 ether fees [43]. Based on these facts,
    we calculated the transaction costs for all our water quality datasets and compared
    the prices between blockchain and blockchain with DDS, as shown in Figure 11.
    Figure 11. Comparing Tx-Cost for water quality data flow between blockchain-only
    and blockchain with DDS. 9. Conclusions and Future Direction for Research This
    paper provides a state-of-the-art design combining DDS and blockchain for the
    management of groundwater quality data. It solves various issues of central system
    challenges, blockchain latency, data integrity problems, privacy, and data quality
    issues. The blockchain uses ECC cryptographic puzzles on the data hashes received
    from the DDS, which acts as a form of extra protection for groundwater quality
    data. The DDS s/kademlia protocol avoids churn, eclipse, and Sybil attacks by
    inducing strong cryptographic signatures and hashing procedures. This paper also
    proposes a novel architecture and platform for stakeholders in groundwater quality
    data management and helps initialize digital agreements. For the control of access
    and data, the current paper makes use of public blockchain smart contracts. With
    the help of a private blockchain, the present application can be made more confidential
    and will have increased control over the quality of data flow. Author Contributions
    Methodology, S.L.T.V.; Project administration, S.P.M.; Writing—original draft,
    S.L.T.V.; Writing—review & editing, E.K. and C.R. All authors have read and agreed
    to the published version of the manuscript. Funding This research received no
    external funding. Conflicts of Interest The authors declare no conflict of interest.
    References Raghav. Groundwater: Origin, Sources and Other Details. 2014. Available
    online: https://www.geographynotes.com/essay/groundwater-origin-sources-and-other-details-with-diagram/620/
    (accessed on 12 July 2021). Pongpun, J.; Daniel, D.S.; Erin, M.H.; Chittaranjan,
    R. The long term effect of agricultural, vadose zone and climatic factors on nitrate
    contamination in Nebraska’s groundwater system. J. Contam. Hydrol. 2019, 220,
    33–48. [Google Scholar] [CrossRef] Exner, M.E.; Aaron; Hirsh, J.; Spalding, R.F.
    Nebraska’s groundwater legacy: Nitrate contamination beneath irrigated cropland.
    Adv. Earth Space Sci. 2014, 50, 4474–4489. [Google Scholar] [CrossRef] [Green
    Version] Fitch, P.; Brodaric, B.; Stenson, M.; Booth, N. Integrated Groundwater
    Data Management; Springer International Publishing: Berlin/Heidelberg, Germany,
    2016; pp. 667–692. [Google Scholar] [CrossRef] [Green Version] Puthal, D.; Malik,
    N.; Mohanty, S.P.; Kougianos, E.; Das, G. Everything You Wanted to Know About
    the Blockchain: Its Promise, Components, Processes, and Problems. IEEE Consum.
    Electron. Mag. 2018, 7, 6–14. [Google Scholar] [CrossRef] Ur Rahman, M.; Baiardi,
    F.; Ricci, L. Blockchain Smart Contract for Scalable Data Sharing in IoT: A Case
    Study of Smart Agriculture. In Proceedings of the 2020 IEEE Global Conference
    on Artificial Intelligence and Internet of Things (GCAIoT), Dubai, United Arab
    Emirates, 12–16 December 2020; pp. 1–7. [Google Scholar] [CrossRef] Vangipuram,
    S.; Mohanty, S.; Kougianos, E. CoviChain: A Blockchain Based Framework for Nonrepudiable
    Contact Tracing in Healthcare Cyber-Physical Systems during Pandemic Outbreaks.
    SN Comput. Sci. 2021, 2, 346. [Google Scholar] [CrossRef] [PubMed] Khan, S.U.;
    Min-Allah, N. A Goal Programming Based Energy Efficient Resource Allocation in
    Data Centers; Springer: Berlin/Heidelberg, Germany, 2011. [Google Scholar] [CrossRef]
    Zomaya, A.Y.; Lee, Y.C. Comparison and Analysis of Greedy Energy-Efficient Scheduling
    Algorithms for Computational Grids. In Energy-Efficient Distributed Computing
    Systems; John Wiley & Sons, Inc.: Hoboken, NJ, USA, 2012; pp. 189–214. [Google
    Scholar] [CrossRef] [Green Version] Zomaya, A.Y.; Lee, Y.C. Energy-Efficient Distributed
    Computing Systems (Wiley Series on Parallel and Distributed Computing); John Wiley
    & Sons, Inc.: Hoboken, NJ, USA, 2012. [Google Scholar] The Ohio State University.
    Water Contamination Disasters. 2016. Available online: https://u.osu.edu/waterpollution2367/water-pollution-crises/
    (accessed on 20 October 2022). Magill, J. U.S. Water Supply System Being Targeted
    by Cybercriminals. 2021. Available online: https://www.forbes.com/sites/jimmagill/2021/07/25/us-water-supply-system-being-targeted-by-cybercriminals/?sh=53b19c5328e7
    (accessed on 20 October 2022). Jin, H.; Feng, L.; Liang, R.; Xing, S. Design of
    urban and rural water resources information management system based on Delphi.
    In Proceedings of the 2011 Second International Conference on Mechanic Automation
    and Control Engineering, Hohhot, China, 15–17 July 2011; pp. 7284–7287. [Google
    Scholar] [CrossRef] Ma, D.; Cui, J. Design and realization of water quality information
    management system based on GIS. In Proceedings of the 2011 International Symposium
    on Water Resource and Environmental Protection, Xi’an, China, 20–22 May 2011;
    Volume 1, pp. 775–778. [Google Scholar] [CrossRef] Peng, Z.; Chen, Y.; Zhang,
    Z.; Qiu, Q.; Han, X. Implementation of Water Quality Management Platform for Aquaculture
    Based on Big Data. In Proceedings of the 2020 International Conference on Computer
    Information and Big Data Applications (CIBDA), Guiyang, China, 17–19 April 2020;
    pp. 70–74. [Google Scholar] [CrossRef] Beshah, W.T.; Moorhead, J.; Dash, P.; Moorhead,
    R.J.; Herman, J.; Sankar, M.S.; Chesser, D.; Lowe, W.; Simmerman, J.; Turnage,
    G. IoT Based Real-Time Water Quality Monitoring and Visualization System Using
    an Autonomous Surface Vehicle. In Proceedings of the OCEANS 2021: San Diego—Porto,
    San Diego, CA, USA, 20–23 September 2021; pp. 1–4. [Google Scholar] [CrossRef]
    Rathna, R.; Anbazhagu, U.V.; Mary Gladence, L.; Anu, V.M.; Sybi Cynthia, J. An
    Intelligent Monitoring System for Water Quality Management using Internet of Things.
    In Proceedings of the 2021 8th International Conference on Smart Computing and
    Communications (ICSCC), Kochi, Kerala, India, 1–3 July 2021; pp. 291–297. [Google
    Scholar] [CrossRef] Drăgulinescu, A.M.; Constantin, F.; Orza, O.; Bosoc, S.; Streche,
    R.; Negoita, A.; Osiac, F.; Balaceanu, C.; Suciu, G. Smart Watering System Security
    Technologies using Blockchain. In Proceedings of the 2021 13th International Conference
    on Electronics, Computers and Artificial Intelligence (ECAI), Pitesti, Romania,
    1–3 July 2021; pp. 1–4. [Google Scholar] [CrossRef] Alharbi, N.; Althagafi, A.;
    Alshomrani, O.; Almotiry, A.; Alhazmi, S. A Blockchain Based Secure IoT Solution
    for Water Quality Management. In Proceedings of the 2021 International Congress
    of Advanced Technology and Engineering (ICOTEN), Taiz, Yemen, 4–5 July 2021; pp.
    1–8. [Google Scholar] [CrossRef] Turganbaev, E.; Rakhmetullina, S.; Beldeubayeva,
    Z.; Krivykh, V. Information system of efficient data management of groundwater
    monitoring the Republic of Kazakhstan. In Proceedings of the 2015 9th International
    Conference on Application of Information and Communication Technologies (AICT),
    Rostov on Don, Russia, 14–16 October 2015. [Google Scholar] [CrossRef] Liu, Y.;
    Liu, X.; Liu, J.; Zhang, Y. The design and applications of services platform system
    for water data basing on WebGIS. In Proceedings of the 2010 2nd IEEE International
    Conference on Information Management and Engineering, Chengdu, China, 16–18 April
    2010. [Google Scholar] [CrossRef] Zhu, Y.; Zhu, S.; Yu, M. Study on groundwater
    data sharing based on metadata. In Proceedings of the 2005 IEEE International
    Geoscience and Remote Sensing Symposium, 2005. IGARSS ’05, Seoul, Korea, 29 July
    2005; Volume 2. [Google Scholar] [CrossRef] Takuya, I.; Sondoss, E.S.; Anthony,
    J. Design and Implementation of a Web-Based Groundwater Data Management System;
    Elsevier: Amsterdam, The Netherlands, 2013; Volume 93. [Google Scholar] [CrossRef]
    Gul, O. Blockchain-Enabled Internet of Things (IoTs) Platforms for Vehicle Sensing
    and Transportation Monitoring. In Machine Learning, Blockchain Technologies and
    Big Data Analytics for IoTs: Methods, Technologies and Applications; Institution
    of Engineering and Technology: London, UK, 2022; pp. 351–373. [Google Scholar]
    [CrossRef] Xia, Y.; Kwon, H.; Wander, M. Developing County Level Data of Nitrogen
    Fertilizer and Manure Inputs for Corn Production in the United States; Elsevier:
    Amsterdam, The Netherlands, 2021; Volume 309. [Google Scholar] [CrossRef] Grounwater
    Data. 2021. Available online: https://maps.waterdata.usgs.gov/mapper/nwisquery.html
    (accessed on 11 July 2021). Hugo, L.; Randall, C.; Lorne, E.; Graham, F. Review
    of Ground Water Quality Monitoring Network Design. J. Hydraul. Eng. 1992, 118,
    11–37. Available online: https://ascelibrary.org/doi/10.1061/(ASCE)0733-9429(1992)118:1(11)
    (accessed on 10 October 2022). Zhang, Z.; Moore, J.C. Mathematical and Physical
    Fundamentals of Climate Change; Elsevier: Amsterdam, The Netherlands, 2014. [Google
    Scholar] [CrossRef] Tang, Q.; Gao, H.; Lu, H.; Lettenmaier, D.P. Remote Sensing:
    Hydrology; SAGE: Thousand Oaks, CA, USA, 2009. [Google Scholar] [CrossRef] Hisashi,
    S.; Akihiko, I.; Akinori, I.; Takashi, I.; Etsushi, K. Current status and future
    of land surface models. Soil Sci. Plant Nutr. 2014, 61, 34–47. [Google Scholar]
    [CrossRef] [Green Version] Vasileios, L.; Nello, C. Nowcasting Events from the
    Social Web with Statistical Learning. Assoc. Comput. Mach. 2012, 3, 72. [Google
    Scholar] [CrossRef] [Green Version] Macaulay, T. RIoT Control: Understanding and
    Managing Risks and the Internet of Things; Elsevier: Amsterdam, The Netherlands,
    2016. [Google Scholar] [CrossRef] Udutalapally, V.; Mohanty, S.P.; Pallagani,
    V.; Khandelwal, V. sCrop: A Novel Device for Sustainable Automatic Disease Prediction,
    Crop Selection, and Irrigation in Internet-of-Agro-Things for Smart Agriculture.
    IEEE Sens. J. 2020, 21, 17525–17538. [Google Scholar] [CrossRef] Tripathy, P.K.;
    Tripathy, A.K.; Agarwal, A.; Mohanty, S.P. MyGreen: An IoT-Enabled Smart Greenhouse
    for Sustainable Agriculture. IEEE Consum. Electron. Mag. 2021, 10, 57–62. [Google
    Scholar] [CrossRef] Rachakonda, L.; Bapatla, A.K.; Mohanty, S.P.; Kougianos, E.
    SaYoPillow: Blockchain-Integrated Privacy-Assured IoMT Framework for Stress Management
    Considering Sleeping Habits. IEEE Trans. Consum. Electron. 2021, 67, 20–29. [Google
    Scholar] [CrossRef] Who Uses Groundwater? 2015. Available online: http://gwhub.srw.com.au/who-uses-groundwater
    (accessed on 14 July 2021). Tabora, V. Using IPFS for Distributed File Storage
    Systems. 2020. Available online: https://medium.com/0xcode/using-ipfs-for-distributed-file-storage-systems-61226e07a6f/
    (accessed on 26 June 2021). Maymounkov, P.; Eres, D. Kademlia: A Peer-to-Peer
    Information System Based on the XOR Metric; Springer: Berlin/Heidelberg, Germany,
    2002; Volume 2429. [Google Scholar] [CrossRef] Baumgart, I.; Mies, S. S/Kademlia:
    A practicable approach towards secure key-based routing. In IEEE Parallel and
    Distributed Systems; IEEE: Piscataway, NJ, USA, 2007. [Google Scholar] [CrossRef]
    Solidity 0.8.6 Documentation. 2018. Available online: https://docs.soliditylang.org/en/v0.8.6/
    (accessed on 26 June 2021). Andreas, M.A.; Gavin, W. Mastering Ethereum; O’Reilly:
    Sebastopol, CA, USA, 2018. [Google Scholar] Survey, U.G. Water Quality. 2022.
    Available online: https://waterdata.usgs.gov/usa/nwis/qw (accessed on 10 July
    2021). Ycharts. Ethreum Price. 2022. Available online: https://ycharts.com/indicators/ethereum_price
    (accessed on 14 August 2021). Publisher’s Note: MDPI stays neutral with regard
    to jurisdictional claims in published maps and institutional affiliations.  ©
    2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open
    access article distributed under the terms and conditions of the Creative Commons
    Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share
    and Cite MDPI and ACS Style Vangipuram, S.L.T.; Mohanty, S.P.; Kougianos, E.;
    Ray, C. G-DaM: A Distributed Data Storage with Blockchain Framework for Management
    of Groundwater Quality Data. Sensors 2022, 22, 8725. https://doi.org/10.3390/s22228725
    AMA Style Vangipuram SLT, Mohanty SP, Kougianos E, Ray C. G-DaM: A Distributed
    Data Storage with Blockchain Framework for Management of Groundwater Quality Data.
    Sensors. 2022; 22(22):8725. https://doi.org/10.3390/s22228725 Chicago/Turabian
    Style Vangipuram, Sukrutha L. T., Saraju P. Mohanty, Elias Kougianos, and Chittaranjan
    Ray. 2022. \"G-DaM: A Distributed Data Storage with Blockchain Framework for Management
    of Groundwater Quality Data\" Sensors 22, no. 22: 8725. https://doi.org/10.3390/s22228725
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations ads   2 PMC   2
    Scopus   2 Crossref   2 PubMed   2 Web of Science   2 Google Scholar   [click
    to view] Article Access Statistics Article access statistics Article Views 28.
    Dec 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb 26. Feb 7. Mar 17. Mar 0 500 1000 1500
    2000 For more information on the journal statistics, click here. Multiple requests
    from the same IP address are counted as one view.   Sensors, EISSN 1424-8220,
    Published by MDPI RSS Content Alert Further Information Article Processing Charges
    Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors
    For Reviewers For Editors For Librarians For Publishers For Societies For Conference
    Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles
    Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe
    to receive issue release notifications and newsletters from MDPI journals Select
    options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated
    Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'G-DaM: A Distributed Data Storage with Blockchain Framework for Management
    of Groundwater Quality Data'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Jan O.R.
  - Jo H.S.
  - Jo R.S.
  - Kua J.
  citation_count: '7'
  description: Natural disasters such as severe flooding can cause catastrophic losses
    to properties and human lives. Constant real-time water level monitoring prior
    to a flooding event can minimise damages and casualties. Many of the currently
    deployed water level monitoring systems typically use a combination of float-type
    or ultrasonic sensing, image processing and computer vision techniques. However,
    these systems incur high computing and hardware requirements, which hinder the
    deployment of such systems in resource-constrained and low-cost environments.
    The recent development of technologies empowered by the Internet of things (IoT)
    and edge computing have enabled real-time systems to be deployed at a significantly
    lower cost and a far more distributed manner. In this paper, we propose an architecture
    for flood monitoring using RGB-D cameras with stereoscopic capabilities to measure
    the water level in an open environment. Our system uses image preprocessing techniques
    to account for chromatic aberration due to overexposure, followed by postprocessing
    before the depth readings are extracted. Data processing and water level information
    extraction are entirely performed on an edge computing device, therefore greatly
    reducing the amount of data transmitted to the cloud server. We practically implemented
    and experimentally validated this system in the real world, under a wide range
    of weather and lighting conditions. Our results showed promising outcomes and
    demonstrated the applicability of our proposed system in a wider context.
  doi: 10.3390/fi14110308
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Future Internet All Article Types Advanced   Journals
    Future Internet Volume 14 Issue 11 10.3390/fi14110308 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Iwona Grobelna
    Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 3536 Citations 7 Table of Contents Abstract Introduction Background
    and Related Work Proposed Architecture for Real-Time Flood Monitoring Experimental
    Setup and Performance Evaluation Results and Analysis Conclusions and Future Work
    Author Contributions Funding Institutional Review Board Statement Informed Consent
    Statement Conflicts of Interest References Altmetric share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Real-Time Flood Monitoring with Computer Vision through Edge Computing-Based Internet
    of Things by Obaid Rafiq Jan 1, Hudyjaya Siswoyo Jo 1, Riady Siswoyo Jo 2 and
    Jonathan Kua 3,* 1 Faculty of Engineering, Computing and Science, Swinburne University
    of Technology Sarawak Campus, Kuching 93350, Sarawak, Malaysia 2 School of Engineering
    and Physical Sciences, Heriot-Watt University Malaysia Campus, Putrajaya 62200,
    Federal Territory of Putrajaya, Malaysia 3 School of Information Technology, Deakin
    University, Geelong 3220, Victoria, Australia * Author to whom correspondence
    should be addressed. Future Internet 2022, 14(11), 308; https://doi.org/10.3390/fi14110308
    Submission received: 21 September 2022 / Revised: 24 October 2022 / Accepted:
    25 October 2022 / Published: 28 October 2022 (This article belongs to the Special
    Issue Internet of Things and Cyber-Physical Systems) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Natural disasters such as severe flooding can
    cause catastrophic losses to properties and human lives. Constant real-time water
    level monitoring prior to a flooding event can minimise damages and casualties.
    Many of the currently deployed water level monitoring systems typically use a
    combination of float-type or ultrasonic sensing, image processing and computer
    vision techniques. However, these systems incur high computing and hardware requirements,
    which hinder the deployment of such systems in resource-constrained and low-cost
    environments. The recent development of technologies empowered by the Internet
    of things (IoT) and edge computing have enabled real-time systems to be deployed
    at a significantly lower cost and a far more distributed manner. In this paper,
    we propose an architecture for flood monitoring using RGB-D cameras with stereoscopic
    capabilities to measure the water level in an open environment. Our system uses
    image preprocessing techniques to account for chromatic aberration due to overexposure,
    followed by postprocessing before the depth readings are extracted. Data processing
    and water level information extraction are entirely performed on an edge computing
    device, therefore greatly reducing the amount of data transmitted to the cloud
    server. We practically implemented and experimentally validated this system in
    the real world, under a wide range of weather and lighting conditions. Our results
    showed promising outcomes and demonstrated the applicability of our proposed system
    in a wider context. Keywords: flood monitoring; computer vision; image processing;
    edge computing; Internet of things 1. Introduction Floods are known to cause catastrophic
    damages and disruptions to our daily life due to the magnitude and unpredictable
    nature of their occurrence. Besides causing physical damage, flood events often
    disrupt daily activities of the community such as causing traffic congestion,
    transport disruptions and power outages [1,2]. To minimise the negative impacts
    caused by natural disasters, a method to assess the conditions that may lead to
    the phenomenon is required. In the case of a flood, the level of water and its
    flow are crucial variables, hence it is necessary to monitor these variables to
    avert any danger. By having access to real-time information on flood risk and
    conditions, authorities will be able to perform timely preventive and corrective
    actions to minimise the disruptions caused by floods. Water level monitoring devices
    are often built with ultrasonic sensors, float-type sensors, capacitive sensors
    and pressure sensors. These sensors are categorised as contact sensors apart from
    ultrasonic sensors. Camera systems and ultrasonic sensors are both categorised
    as noncontact sensors, which perform measurements from a distance. While dedicated
    water level measurement sensors are straightforward and easy to implement, camera-based
    water level sensing has been gaining attention in recent years [3]. This is mainly
    because a camera-based setup can also serve as surveillance systems and similarly,
    existing surveillance cameras infrastructure can also be used to perform flood
    monitoring, which is beneficial in a smart city. In this paper, we propose and
    present a camera-based real-time flood monitoring system using computer vision
    techniques underlined by edge computing and IoT system approaches. We explore,
    design and deploy the proposed system in a real-world water level monitoring facility
    in a rural location. Our system leverages the advantages of camera-based system
    over the typical specialised noncamera systems. However, in a camera-based system,
    the raw information of images needs to be processed to extract essential information
    (e.g., water level). The image processing of these digital images is often performed
    on a computing device. Depending on the system setup, different computing paradigms
    are classified based on the location where the computational process is performed.
    In a cloud-computing setup, the raw image data are sent across the network to
    be processed on the computer located at a distant facility commonly known as a
    server. The process of transmitting raw data to the server becomes a challenge
    especially when the sensor node is located in a remote location where network
    connectivity is limited [4]. In contrast, in an edge-computing setup, the image
    data are processed in situ using the computing device located close to the sensor
    node which then extracts and transmits the important parameters to the server.
    With the advancement in computing technology, the edge computing device is becoming
    increasingly compact, powerful and power-efficient [5,6]. These features increase
    the viability of implementing edge-based processing tasks in areas where network
    bandwidth and power resources are scarce. The edge computing setup in the context
    of the Internet of things (IoT) enhances the capabilities of a real-time monitoring
    system in rural areas when extracting and processing important data are concerned
    [7,8,9] This paper aims to investigate the viability of utilising red, green,
    blue and depth (RGB-D) image sensor technology coupled with edge-based processing
    for the application of flood monitoring. The system setup is deployed and tested
    in a real-life scenario to further reinforce the feasibility of the implementation.
    In this paper, we make the following key contributions: Propose and design a novel
    RGB-D sensing- based technology for real-time water level monitoring under diverse
    weather and lighting conditions. Integrate edge-computing-based techniques and
    IoT-based system architecture for rapid image processing. Deploy the proposed
    architecture as a real-world system in an actual water level monitoring facility
    at a rural site where flooding is a common threat to the local community. The
    remaining sections of this paper are organised as follows. Section 2 covers the
    background and work related to vision-based water level detection and edge computing
    in IoT applications. Section 3 presents the architecture of the proposed system
    as well as the details of the system implementation. Section 4 discusses the implementation
    and experimental setup of the proposed system. Section 5 presents the experimental
    results and analysis followed by conclusions and future work in Section 6. 2.
    Background and Related Work In this section, we present the background information
    and related work in the field of vision-based water level measurements and edge
    computing in IoT applications. 2.1. Vision-based Water Level Measurements There
    is a plethora of work on flood monitoring based on camera systems. These range
    from combining various image processing techniques to determine the water level,
    to implementing deep neural networks for a higher classification accuracy. Measuring
    hydrological data under complex illumination conditions such as low lighting,
    water surface glare and artificial lighting obstructs the accuracy of measurement
    systems significantly. Image processing tasks can be interfered with when working
    in the aforementioned conditions, hence requiring more sophisticated/efficient
    image processing [10], data streaming [11,12,13,14] and network resource management
    techniques [15,16,17,18]. Some of the related works are summarised as follows.
    Zhang et al. [19] proposed to use near-infrared (NIR) imaging to tackle poor visibility
    while measuring the water level from a gauge by creating a digital template of
    the gauge for accurate readings. The conversion from the digital template to coordinates
    was done by orthorectifying the region of interest (ROI) image of the staff gauge,
    followed by a horizontal projection to locate the water line and finally converting
    to the water level with the physical resolution of the digital template. It achieves
    an accuracy of up to 1 cm. Similarly, Bruinink et al. [20] proposed a texture
    and numerical recognition technique to process the staff gauge features. It extracted
    numerical features from the staff gauge and trained a random forest classifier
    to determine the level of water, resulting in an overall accuracy of 97%. A cloud-based
    measurement system was proposed by Kim et al. [21], which also relied on a character
    recognition algorithm since it depended on a staff gauge to measure the readings.
    The subsequent technique used a correlation analysis of images and by determining
    the difference in the correlation coefficient, it identified the level of the
    water surface. The drawback for these techniques is the dependence on staff gauges
    to measure the water level, as without it, a digital template cannot be formed,
    and neither can a character recognition algorithm work. Lin et al. [22] proposed
    a robust solution that negated camera shivers and jitters. The water level detection
    was based on an ROI that was selected from the calibration points whereas the
    line was detected from a set of lines after applying a Hough transform [23]. Based
    on photogrammetry principles, the water line was calculated with the transformation
    from the pixel space to the real-world space. The water detection algorithm allowed
    the system to achieve an accuracy of 1 cm. The significance of this system was
    that it could detect and convert the water level accurately without a staff gauge
    due to its robust calibration. Studies without using a water gauge were presented
    by Ortigossa et al. and Li et al. [24,25]. Both studies relied on similar traditional
    image processing techniques by performing greyscale conversion, contrast adjustment
    and a binarisation of images. The lines were detected through the Hough transform
    algorithm performed on thresholded images before transforming the pixel coordinates
    to metric units. The work presented by Ortigossa et al. yielded poor results in
    the open environment due to the complex conditions and a low-resolution camera.
    In another study, Zhang et al. [26] proposed a methodology comprised of a web
    camera integrated with a NIR light and an optical bandpass filter to detect near-infrared
    light. It also consisted in utilising a staff gauge and the water level was obtained
    through monocular imaging by observing any abrupt changes in the horizontal projection
    curve. The novel implementation of a maximum mean difference (MMD) method detected
    the water level in the pixel space. The MMD was tested against Otsu’s thresholding
    [27] and order statistic filtering (OSF), outperforming the latter two techniques
    under complex illumination conditions. The implementation yielded an accuracy
    of up to 1 cm. Depth perception from single monocular images was regarded as an
    ill-posed problem [28]. To tackle the shortcomings, deep convolutional networks
    (ConvNets) [29] have been used in recent years and have yielded great success.
    A CNN-based method [30] was presented to determine the water level via boundary
    segmentation. Three different techniques were compared against each other: Otsu’s
    thresholding, dictionary learning and a CNN. The CNN outperformed the other two
    methods in determining the water level as it had a strong generalization ability
    and was capable of detecting and learning complex features. Furthermore, Jafari
    et al. [31] proposed a method to estimate water levels using time-lapse images
    and object-based image analysis (OBIA). The output images were semantically segmented
    based on a fully convolutional network (FCN) architecture whereby the water region
    was labelled, followed by an estimation of the water contours and a conversion
    of the pixel ratios to metric units. The method also incorporated a Laplacian
    detector to observe sudden changes in water level. Vision-based sensing technology
    has advanced over the years, especially when discussing RGB-D sensors, as it has
    become more accessible and even more affordable. Bung et al. [32] used a RealSense
    D435 depth camera to investigate complex free surface flows by measuring a hydraulic
    jump event and compared the results against multiple other sensors. The hydraulic
    jump surface profile and surface fluctuation amplitudes were measured in an experimental
    rectangular flume where the water jump height was altered. The RealSense camera
    measured the aerated flow and the air–water mixture with similar results to the
    characteristic upper-water-level limit of the air–water mixture. The study found
    that clear water surfaces remained undetected until water colouring was added,
    which significantly improved the detection of calm surfaces. Other areas where
    RealSense depth cameras have been used for is simultaneous localisation and mapping
    (SLAM), object tracking and object pose estimation. Bi et al. [33] discussed the
    deployment of a SLAM system on a micro aerial vehicle (MAV) using an R200 camera
    due to it being lightweight and capable of functioning indoors and outdoors at
    long range. Chen et al. [34] proposed an analysis procedure to solve object tracking
    and pose estimation using a single RealSense D400 depth camera. Using image segmentation
    and an iterative closest point (ICP) algorithm, the model created an ROI and performed
    pose estimation, respectively. It performed well under robust lighting and background
    conditions and comparatively better than a single-shot detector (SSD) model. 2.2.
    Edge Computing in IoT Applications In the modern world and modern wireless telecommunication,
    the Internet of things (IoT) has gained significant traction with billions of
    devices connected to the internet. The number of IoT devices is estimated to reach
    14.5 billion by the end of 2022, with the market size forecasted to grow to USD
    525 billion from 2022 to 2027 [35]. One of the biggest strengths of the IoT is
    the massive impact it has on the everyday life of users. It does not come as a
    surprise that the IoT has been included in the list of six “Disruptive Civil Technologies”
    with a potential influence on US national power. Some of the potential application
    domains of the IoT are transportation and logistics, healthcare and smart environments
    [36]. Within the IoT, there are several technologies involved such as radiofrequency
    identification (RFID), near-field communication (NFC), intelligent sensing, wireless
    sensor networks (WSNs) and cloud computing. The IoT essentially defines the upcoming
    generation of the Internet where physical objects will be accessed via the Internet.
    As current research stands, the use of RFID-based verification and identification
    in the logistics and retail industries has been rampant. Furthermore, the success
    of the IoT is dependent on standardisation, which will ultimately provide reliability,
    compatibility, interoperability and effectiveness at a global level. IoT devices
    can be implemented on a much larger scale in numerous industries and areas, where
    billions of things can be integrated seamlessly, and for that, objects in the
    IoT need to exchange information and be able to communicate autonomously [37].
    The number of layers in an IoT architecture is defined differently depending on
    the underlying technologies. However, from the recent literature, the basic model
    of the IoT is a five-layer architecture consisting of an object layer, object
    abstraction layer, service management layer, application layer and a business
    layer. The five-layer architecture is an ideal model since it encapsulates scalability
    and interoperability, both of which are important in IoT applications [38]. Prior
    to edge computing, data produced by IoT sensors were processed on the cloud. Cloud
    computing has been extremely cost-efficient and flexible in terms of computing
    power and storage capabilities. However, with the advancement in the IoT and the
    rapid development in mobile internet, cloud computing has been unable to meet
    the sophisticated demands of extremely low latency due to the additional load
    on the backhaul networks. The network bandwidth requirement to send all the data
    to the cloud generated by IoT applications is prohibitively high. To tackle the
    shortcomings, an edge computing paradigm has been thoroughly investigated, which
    processes the generated data at the network edge instead of transmitting them
    to centralised cloud servers. Edge computing is a much more suitable solution
    to be integrated with the IoT for a large user base and can be considered for
    future IoT infrastructure [39]. The edge computing architecture relies on the
    servers being closer to the end user or node and even though they possess less
    computational power than cloud servers, the latency and quality of service (QoS)
    provided are better. The edge-computing-based IoT satisfies the QoS requirements
    of time-sensitive applications and solves the bottleneck issues of network resources
    faced in the IoT [7]. Pan and McElhannon [40] discussed an open edge cloud model
    infrastructure which is necessary moving forward in an IoT world. An open edge
    cloud addresses local computing, networking resources and storage to aid resource-constrained
    IoT devices. Data generated by edge devices can be stored and preprocessed locally
    by the edge cloud. For loads or data that are beyond the capabilities of the IoT
    devices, the devices can offload tasks to the edge servers. This will enable latency
    control since the edge cloud is nearer to the nodes. Premsankar et al. [41] focused
    on edge computing for the IoT in the context of mobile gaming as it tests the
    edge computing paradigm. Their research performed an evaluation on an open-source
    3D arcade game whereby the mobile phone sent the game input to the server, which
    rendered the content of the game and returned it to the end-device. Their research
    concluded that hosting computational resources extremely close to the end-user
    was the only suitable route to achieve an acceptable quality of experience and
    to enable fast-paced real-time interaction. The work we present in this paper
    is different as we do not rely on extra external equipment such as staff gauges
    or light systems. It focuses on implementing a stereoscopic depth camera for hydrological
    data extraction purposes, while running on a compact edge computing device. 3.
    Proposed Architecture for Real-Time Flood Monitoring In this section, we present
    our proposed architecture for the real-time vision-based flood monitoring system.
    The architecture utilised the combination of a Y16 stream, which is a 16-bit greyscale
    format, and a depth stream from the RealSense D455 RGB-D camera [42]. The process
    began with the acquisition of images from the Y16 pipeline to determine the region
    of interest (ROI) of the target water surface. The ROI information was subsequently
    used for extracting the depth information from the depth stream. The RealSense
    depth camera is capable of activating multiple pipelines concurrently. Leveraging
    this feature, greyscale and depth pipelines were activated followed by different
    procedures to extract the depth readings. In this study, the stereoscopic capability
    of the RealSense D455 camera played an important role in measuring the water level
    without requiring external references such as a staff gauge or mapping pixel coordinates
    to physical dimensions. The RealSense D455 is the latest addition to the RS400
    line of stereo vision cameras manufactured by Intel. The camera package combines
    image sensors with hardware-accelerated on-board chips for calculating and perceiving
    depth information with metrical accuracy. The two infrared (IR) imagers are placed
    at a known fixed distance apart to measure the disparity between the depth of
    pixels in order to calculate and provide the distance of the said pixel from the
    camera. The matching algorithms of the RealSense 400 series are aggregated based
    on the values of the neighbouring pixels, which are then used to estimate the
    total depth measurement [43]. Since the pixel depth value is provided with a confidence
    score, if the confidence score does not meet the minimum expected value, the value
    is discarded, and the pixel depth is rendered invalid. The on-board processing
    capabilities of the camera, such as filtering and averaging, can be externally
    configured through the camera utility software or the RealSense Software Development
    Kit (SDK). The RealSense SDK is a cross-platform software development kit which
    allows the camera to be interfaced and configured through different popular programming
    languages such as C, C++, MATLAB and Python. In this study, the software system
    was developed using Python 3.6.9 running on an Ubuntu 18.04 operating system.
    Apart from the depth processing which happened on the camera’s ASIC chip itself,
    the rest of the algorithm was executed on an edge device. Figure 1 shows the system
    block diagram of the implementation. The detailed operation of each phase is further
    explained in the subsequent subsections. Figure 1. Block diagram of the proposed
    real-time flood monitoring system. 3.1. Phase 1: RGB Preprocessing RGB preprocessing
    was performed to adequately expose the water surface which remained underexposed.
    Through Otsu’s thresholding and clean-up operations, the underexposed regions
    were highlighted, after which the exposure levels were adjusted based on the underexposed
    coordinates, thus forming the ROI. The RGB stream shown in Figure 2a experienced
    chromatic aberration due to overexposure in the bottom right corner, while in
    contrast, the water streams were not exposed properly. To properly process the
    frame, the image was captured through the colour sensor in 16-bit greyscale format
    before being processed for thresholding. The 16-bit greyscale format captured
    65,000 different shades of grey, allowing for greater information in a frame when
    converted into a black and white image. After thresholding, regions which were
    not of interest were inverted to a black background colour by their coordinate
    area size, as shown in Figure 2b. Regions outside the ROI were cleaned up using
    a kernel that passed over the image, leaving a giant patch to determine the source
    of the ROI. The kernel size was set to be big enough to clean up noise outside
    the ROI. The ROI was selected based on the median values from the pool of coordinates
    in the giant white patch. The size of the ROI was insignificant as long as the
    streams were properly exposed for the depth extraction process. The selected ROI
    coordinates were fed into the depth pipeline to correctly expose the water streams
    before capturing the depth frames. The preprocessing stage was carried out during
    the day, since it was not needed at night due to the lack of light causing any
    interference with the depth pipeline or the autoexposure. Figure 2. (a) Captured
    frame from camera experiencing chromatic aberration in bottom right of the scene.
    (b) Thresholded region which forms the basis for the ROI coordinates. 3.2. Phase
    2: Postprocessing Filters The depth pipeline captured frames which were logged
    into a frameset required for postprocessing. The postprocessing flow implemented
    in this study followed the recommended sequence provided by Intel in the RealSense
    documentation [44]. The filters applied in this context were to clean up the image,
    smoothen the depth readings and to recover missing depth pixels as much as possible.
    The general flow of applying the filters is shown in Figure 3. Figure 3. Postprocessing
    flow. 3.2.1. Decimation Filter Since all the depth-generation processing was done
    on the chip on-board the camera, the rest of the postprocessing was configured
    on the platform used for the application. The decimation filter took in the raw
    depth frame of the scene and decimated it by a factor of n. Decimation in the
    context of image processing produces an approximation of the image as it compresses
    the desired frame. It typically involves lowpass filtering and subsampling of
    the data, which also aids in reducing the computational load as the computer does
    not need to process as many data [45,46]. In this case, the depth data were subsampled,
    thus yielding an image with a reduced size and resolution as denoted by Equation
    (1). A kernel size of n by n was applied and the depth frame was scaled down by
    the given factor. The kernel size selected in this case was [2 × 2], which was
    applied to the input resolution of the depth stream, i.e., 848 by 480 pixels.
    The resolution after the filter was applied was: [848,480]/2=[424,240] (1) 3.2.2.
    Spatial Filter Multiple parameters were tuneable under the spatial filter settings.
    The spatial filter performed several one-dimensional passes horizontally and vertically
    over the frame for enhancement. This was selected by the filter magnitude, which
    was set to a maximum value of 5, in this scenario—it was the number of filter
    iterations. The alpha and delta factors were selected as 0.5 and 1, respectively.
    The alpha factor was the exponential moving average whereas the delta factor was
    the threshold depth value for which any neighbouring pixel exceeding this threshold
    caused the smoothening effect to temporarily turn off. The primary purpose of
    the spatial filter was to preserve the edges. The spatial filter in the pyrealsense2
    library is an implementation of a domain transform, which efficiently performs
    the edge-preserving filtering of 2D images. The recursive equation for calculating
    the exponential moving average (EMA) is given below. 𝑆 𝑡 = ⎧ ⎩ ⎨     𝑌 1 ,𝑡=1
    𝛼 𝑌 𝑡 +(1−𝛼) 𝑆 𝑡−1 ,𝑡>1𝑎𝑛𝑑Δ=| 𝑆 𝑡 − 𝑆 𝑡−1 |< 𝛿 𝑡ℎ𝑟𝑒𝑠ℎ 𝑌 𝑡 ,𝑡>1𝑎𝑛𝑑Δ=| 𝑆 𝑡 − 𝑆 𝑡−1
    |> 𝛿 𝑡ℎ𝑟𝑒𝑠ℎ (2) 3.2.3. Temporal Filter The temporal filter averaged the frames
    and depth values depending on the number and history of previous frames. The alpha
    and delta factors also exist in this filter, serving the same purpose as in the
    spatial filters. The number of averaged frames in the frameset was 15, the alpha
    value was set to 0.4 and the delta factor was set to a maximum value of 100. The
    threshold value in the spatial filter was set to the lowest value so the edges
    remained sharp. The threshold value in the temporal filter was set to its maximum
    value in order to achieve stable depth readings. This combination allowed for
    sharp edges as well as stability in the frames by reducing high-frequency components
    in the scene. 3.2.4. Hole-Filling Filter In some areas of the scene, the camera
    was not confident about the depth of the pixel based on the confidence score,
    hence an invalid point was created which resulted in a depth reading of 0. To
    recover the depth from patches of invalid pixels, the hole-filling filter obtained
    depth pixels from neighbouring pixels and rectified it. The route chosen in this
    scenario was to take the depth data of the pixels closest to the sensor, instead
    of the farthest. The invalid patches shifted in position depending on the combination
    of water level and daylight; hence it helped to keep the depth readings valid
    for the water level extraction pipeline. 3.3. Phase 3: Crosshair Averaging The
    frame passed onto the depth extraction process in this module was the average
    of 15 processed frames over a period of 15 min. The 15 min averaging period was
    to match the on-site sensor measurement update rate which was used as the water
    level reading benchmark. As the sensor updated the data every 15 min, the values
    were averaged over that period to smoothen out any possible noise. To mimic the
    sensor averaging period, the pipeline for the RealSense depth extraction followed
    a similar principle. Each frame was captured and pushed into the frameset every
    60 s. Once 15 frames had been queued up, the entire postprocessing filters pipeline
    was activated, and the resulting frame was the processed frame. The downstream
    reference point was determined by the depth distance reading from the camera,
    which was used as the offset to add or subtract the excess values to determine
    the actual water level from. From the processed frame, an average depth reading
    of several pixels was taken in the formation of a crosshair to minimise the slight
    fluctuations across the measurement area. The crosshair was 10 pixels down and
    9 pixels across, with the centre pixel covered by the vertical down pass, as shown
    in Figure 4. The figure is the processed frame after being passed through the
    postprocessing pipeline as it has been smoothed out. The colour map going from
    dark blue to dark red displays the transition from the extremely close pixels
    to the furthest-away pixels, respectively. Figure 4. The crosshair which is averaged
    from the processed depth frame to obtain the final depth reading. 4. Experimental
    Setup and Performance Evaluation In this section, we describe our experimental
    setup and the environment in which we deployed our proposed system. After conducting
    laboratory experimental testing in a controlled environment, the system was installed
    on-site to validate the functionality of the proposed system. This section presents
    the on-site implementation of our proposed system. 4.1. Environment of System
    Testing Site The measurement site was a water gate in a rural setting located
    in Petrajaya, Sarawak, Malaysia. The gate itself acted as a barrier and as a controlled
    wall, functioning to regulate safe water levels between the sea water and the
    river stream. Figure 5 displays the water gate whereby the shaded platform houses
    the control system for the ultrasonic sensors, the RealSense camera and the gates
    themselves. Figure 5. Satellite view of the physical measurement site. 4.2. Hardware
    Implementation To effectively utilise edge computing, a processing powerhouse
    was required, ideally in a small form factor. The prototype was developed with
    the intention to be deployed in remote and rural areas. Therefore, the hardware
    setup was designed to achieve the utmost unsupervised automated operation in order
    to minimise physical site visits to perform maintenance or modification. As for
    the powerhouse for processing, a compact yet powerful NVIDIA Jetson Nano was used
    as the computing platform. For the RGB-D sensor, the Intel RealSense D455 was
    chosen as it could reliably provide both RGB and depth information through its
    on-board processing capabilities. The entire setup was connected to the Internet
    through a Teltonika industrial router, which provided a connection fail-over redundancy
    through a Long-Term Evolution (LTE) mobile network in case of disconnection from
    the main wired broadband network. Figure 6 displays the hardware topology. Figure
    6. System setup of our proposed computer vision and edge-computing-based IoT real-time
    flood monitoring architecture. The Intel RealSense D455 contains an in-body IR
    projector that allows depth perception in low-light situations, thus allowing
    the extraction of depth at night or in low-light conditions without additional
    hardware required. The camera was placed vertically below the platform and the
    electronics devices were housed in a weatherproof enclosure mounted on the platform.
    The physical setup of the computing platform, communication device and the RGB-D
    sensor are shown in Figure 7. The surface of the camera was parallel with the
    water surface as it provided the best viewing option of the water from the platform.
    Furthermore, whether the water level increased steadily or at a steep gradient
    towards the camera, the measurement accuracy increased, as it effectively shortened
    the distance between the camera sensors and the measuring object. The water gate
    itself housed a monitoring system which comprised ultrasonic sensors and a logging
    device. The data collected from this pre-existing system were used to verify the
    accuracy and reliability of the depth readings from the proposed system. Figure
    7. (a) The Jetson Nano enclosure which houses all the electrical components. (b)
    RealSense camera installation location. (c) Platform view of the installation
    site. 4.3. Bandwidth Measurements Another important aspect of this study focused
    on the comparison of bandwidth consumption requirements of an edge computing device
    and a cloud computing device. To perform the same processing steps on the cloud
    as was performed on the edge device, raw and uncompressed data were required to
    be sent to the cloud. For this study, the uncompressed and unfiltered data, which
    were Robot Operating System (ROS) bag format files, were uploaded to Google Drive
    via its application programming interface (API) accessed through Python. On the
    other hand, the final processed depth readings were uploaded onto the ThingSpeak
    Analytics dashboard via the representational state transfer (REST) API. The consumption
    of bandwidth was measured accordingly as shown in the block diagram in Figure
    8. The ROS file contained data required for the cloud server to process the depth
    readings, without which the depth extraction pipeline would not work on the cloud.
    Figure 8. Total bytes uploaded are recorded to determine the bandwidth consumed
    over the same testing period. To measure the time taken and bytes uploaded, respectively,
    the program initialised a performance counter and recorded it, immediately followed
    by measuring the network input/output data prior to uploading the ROS file. As
    the data were successfully uploaded, the network input/output counter was recorded,
    and the second performance counter was recorded. The time taken to upload the
    data was the difference between the two performance counters and the total bytes
    uploaded were the difference between the two network input/output counters. The
    same methodology was applied to the data uploaded to the IoT analytics dashboard,
    which was processed on the Jetson Nano edge device. 5. Results and Analysis The
    measurements were continuously taken over a 24 h period and divided into daylight
    hours (06:30–18:40) and night hours (18:40–6:30). The conditions during the measurements
    included clear skies, overcast and heavy precipitation. The measurement period
    was scattered over a few weeks to allow the system to be tested under diverse
    weather conditions. During the initial stage of data collection, it was found
    that some of the images captured by the sensor were affected by sun glare. The
    issue was tackled by applying polarising filters on the stereo imagers to eliminate
    the reflection from the water surface. The results presented were collected from
    the setup with the polarising filter applied. The testing results were compiled
    and analysed from two different angles. Firstly, the overall results (Figure 9,
    Figure 10, Figure 11 and Figure 12) were collectively analysed to understand the
    measurement performance during the daylight and night hours. The results in the
    graphs are segmented into day and night periods separated by the dashed line.
    Secondly, a separate analysis was performed on different scenarios to understand
    the effect of weather conditions on the measurement performance. Four sets of
    results in the form of graphs are presented in Figure 9, Figure 10, Figure 11
    and Figure 12 where different weather conditions are depicted in different colour
    shades accordingly. The variation in weather provided a natural and automatic
    diversification of the testing dataset which tested the performance of the depth
    perception. Figure 9. Water level with reference to MSL; scenario 1 displays consistent
    weather during each period. Figure 10. Water level with reference to MSL; scenario
    2 displays some variance in weather conditions. Figure 11. Water level with reference
    to MSL; scenario 3 displays normal fluctuations in weather conditions. Figure
    12. Water level with reference to MSL; scenario 4 displays readings under abrupt
    weather changes. 5.1. Daylight Readings The daylight (morning) readings collected
    consisted of 101 data points and the water level ranged from below −2 m to above
    2 m. The water level measurement was offset to match the pre-existing system which
    was calibrated against the mean sea level (MSL) during the construction of the
    water gate facility. The measurement data plots generated by the system against
    the ultrasonic sensor are shown in the figures below. The root-mean-square error
    (RMSE) during the daylight was 0.41 m averaged over the entire testing period.
    The breakdown of the RMSE for each weather condition for both day and night periods
    is tabulated in Table 1. Although the installation of the polariser significantly
    improved the stability and consistency of the results, the reflections appearing
    from different angles had the tendency to cause the camera to miscalculate the
    depth, resulting in a greater RMSE error during the day. Table 1. Collective RMSE
    error under all weather conditions. 5.2. Night Readings For the night readings,
    172 data points were collected in total. The readings under the complete absence
    of visible light and external sources resulted in a root-mean-square error (RMSE)
    of 0.2 m, significantly lower than that of daylight measurements. The absence
    of reflections and wayward lights and the activation of infrared light during
    the night provided a much more stable and accurate reading. The breakdown of the
    weather conditions for the collected results throughout the testing phase is presented
    in Table 1. The weather conditions varied constantly as the testing period progressed.
    During the dark hours, cloudy and fair weather conditions were indistinguishable,
    whereas fair during the daytime referred to clear skies with more sunlight exposure.
    To distinguish the different weather conditions, Weather Underground [47] was
    used to obtain historical data for each hour of the testing period. Since not
    all areas of a city experience the exact same weather, an error from mismatching
    the weather condition to its testing hour is possible. The results were segmented
    into different scenarios to better differentiate the conditions the system experienced
    in each period. Each testing period, the system experienced different weather
    conditions during different times of the day and night, and on multiple occasions
    throughout, the weather abruptly changed on-site. We observed that the readings
    were smoother during the night, with tiny fluctuations around the ground-truth
    ultrasonic sensor data. As the day started, the deviations and fluctuations grew
    larger. Across the three weather conditions, as can be seen from the graphs and
    the tables, a fair weather provided the most erroneous readings, both during the
    day and night. The RMSE, however, was only marginally greater than in rainy weather
    during the night, while it was significantly greater during the day. Cloudy conditions
    also covered the occurrence of partial clouds over the water region, hence the
    possibility of partial illumination and partial shading of the water region, which
    contributed to the high RMSE during the day. The biggest factor contributing to
    the high RMSE during the day was occlusion and glare, resulting from the reflections
    of the sunlight hitting the camera sensor off the water surface. Night readings
    showed a promising output which closely reflected the movement of the water level
    captured by the benchmarked ultrasonic sensor. On the contrary, clear skies during
    the day resulted in the largest RMSE, thus being responsible for a big percentage
    of the day’s large RMSE values. As the conditions became rainy and stormy, the
    readings became more accurate and resulted in the lowest RMSE during the day while
    being slightly more erroneous than the night readings. Nonetheless, when the rain
    poured down, the system captured depth accurately and consistently. The spatial
    filter continuously smoothened out the higher-frequency raindrops disturbing the
    water surface. The case for complex illumination conditions can be made for daytime
    readings rather than night readings, as the presence of reflections and glare
    under cloudy and fair weather conditions proved to be challenging for the camera
    to tackle. It is also important to note that the measurement spots of the ultrasonic
    sensor and camera sensors were located some distance apart. This situation was
    also one of the potential sources of error due to the dynamic movement of the
    water flow throughout the stream, which might have led to differences in water
    level readings. Moreover, due to the limitation of the on-site camera placement,
    shadows formed on the water surface due to the reflection of the sky, rendering
    the readings erroneous. The limitations due to the structural formation of the
    platform and water gate placed the system’s depth readings at a slight disadvantage,
    especially during the daylight testing period. 5.3. Scenario 1: Consistent Weather
    Scenario 1 showcases the performance of the system under consistent weather conditions
    during each testing period, i.e., rainy during the night and cloudy during the
    day. The weather remaining consistent allowed the system readings to be stable
    as there was relatively little fluctuation in the readings across both periods.
    In this scenario, the day RMSE was 0.23 m while the night RMSE was slightly lower
    at 0.2 m. 5.4. Scenario 2: Slight Fluctuations in Weather The system experienced
    some weather fluctuations in scenario 2, although all the fluctuations occurred
    during the night and none during the day period. Even with the fluctuation in
    weather condition, the system RMSE during the night was the lowest under this
    scenario, while during the day readings it was the highest. Even with the fluctuations
    in weather conditions, the system RMSE was 0.18 m, the lowest night-reading RMSE
    across all scenarios. On the contrary, the system RMSE during the day was 0.51
    m, the highest during the day periods across the testing period. 5.5. Scenario
    3: Normal Fluctuations in Weather The system experienced a moderate amount of
    weather fluctuations, slightly more than in scenario 2. Scenario 3’s monitoring
    period also extended longer than each of the previous two scenarios. Similar to
    scenario 2, the weather fluctuated multiple times during the night while during
    the day, it changed once from cloudy to rainy halfway during the day. The readings
    were most erroneous during the day when it was cloudy and much more stable when
    raining. Under this scenario, the system RMSE during the day was 0.33 m while
    during the night, the system RMSE was 0.2 m, one of the lowest across the entire
    testing period. 5.6. Scenario 4: Frequent Fluctuations in Weather In scenario
    4, the system experienced weather fluctuations more frequently and abruptly than
    the previous three scenarios. The weather fluctuated between all the three main
    categories during both testing periods. Most of the night period was fair, meaning
    clear skies, and that was also the most erroneous the readings were amongst all
    four scenarios as the system RMSE was 0.21 m. During the day, it fell within the
    category of one of the highest RMSEs at 0.48 m. 5.7. Bandwidth Consumption During
    the experimental period, an evaluation of the bandwidth usage was also conducted
    in order to compare the bandwidth requirements between cloud-based and edge-based
    processing technique. The total bandwidth consumed over the course of a single
    testing day for both computing models are presented in Figure 13 and Figure 14.
    The bandwidth capacity required for uploading the data to the cloud was approximately
    50,000 kilobits per second (Kbps) whereas uploading processed edge information
    to the dashboard maxed out at just over 12 Kbps. Figure 13. Bandwidth consumed
    uploading ROS bag files to a cloud storage (in the order of tens of Mbps). Figure
    14. Bandwidth consumed uploading processed edge readings to an IoT dashboard (in
    the order of Kbps). The results demonstrated the difference in bandwidth consumed
    for each of the computing strategies; the edge computing strategy proved to be
    more bandwidth efficient. The total data usage to upload edge data to the dashboard
    over a 24 h period was approximately 0.92 Mb whereas it took 11,106 Mb to upload
    raw data to cloud storage. The experiment showed that uploading uncompressed information
    to the cloud required approximately 12,072 times more data usage than uploading
    the processed data to the cloud dashboard. There was a massive data and bandwidth
    reduction in the edge pipeline, especially when the application involved real-time
    monitoring using image and video data. By utilising edge computing, the bandwidth
    consumption was greatly reduced thus alleviating stress off the network [48].
    This is beneficial for installing real-time monitoring applications in rural areas
    where network bandwidth is a major concern for IoT devices. 6. Conclusions and
    Future Work In this paper, we proposed, designed, implemented and deployed a novel
    real-time flood monitoring system driven by computer vision, edge computing and
    an IoT system architecture. We used RealSense D455’s stereo vision capabilities
    to extract depth from a complex medium such as water, which provided great insights
    into the application of RGB-D cameras in flood monitoring. The RGB-D camera was
    integrated with an edge computing device (Jetson Nano), to continuously process
    and log all critical incoming RGB-D image sensor data. The performance of the
    deployed system was analysed and compared with calibrated specialised ultrasonic
    sensor data. The system was deployed and tested under a variety of weather and
    lighting conditions in an actual flood mitigation facility. The experimental results
    demonstrated the promising potential of using the proposed solution to perform
    water level measurement using a computer-vision-based sensor. Results obtained
    during the night highlighted the stereoscopic capabilities of the RealSense D455,
    where it performed well under no visible lighting. Furthermore, running the prototype
    throughout the experimental period of more than 24 weeks demonstrated the reliability
    and capability of the proposed system. The implementation of a real-time flood
    monitoring system at the edge demonstrated the potential and benefits of installing
    similar systems in resource-constrained environments and without external measurement
    guide or additional hardware. Future work will investigate combining and superimposing
    RGB images, artefacts and cues onto depth maps to improve the overall accuracy
    and reliability of the system. To enable the system to be more dynamic and applicable
    at any water site, the incorporation of the internal inertial measurement unit
    (IMU) readings of the camera and frame subtraction techniques will be investigated.
    Our work can serve as a reference and further be expanded into different application
    domains, such as those in smart cities and smart manufacturing. For instance,
    the images captured by the image sensor could be used to detect illegal intrusion
    or vandalism attempt around the facility. Images and footage from multiple stereoscopic
    cameras at the same location could be utilised for sensor fusion. Additional stereoscopic
    depth cameras may be useful in providing information over a larger area and assist
    in differentiating among errors during various operational conditions, which may
    potentially be advantageous. Image information could also be used to detect river
    pollution or illegal dumping activities along the river streams which would be
    advantageous to the relevant authorities insofar as taking pre-emptive action
    for containing the effect of the pollution. Author Contributions Conceptualization,
    O.R.J. and H.S.J.; methodology, O.R.J. and H.S.J.; software, O.R.J.; validation,
    O.R.J., H.S.J., R.S.J. and J.K.; formal analysis, O.R.J.; investigation, O.R.J.
    and H.S.J.; resources, H.S.J.; data curation, O.R.J., H.S.J. and R.S.J.; writing—original
    draft preparation, O.R.J. and H.S.J.; writing—review and editing, H.S.J. and J.K.;
    visualization, O.R.J., H.S.J., R.S.J. and J.K.; supervision, H.S.J. and R.S.J.;
    project administration, H.S.J. All authors have read and agreed to the published
    version of the manuscript. Funding This research received no external funding.
    Institutional Review Board Statement Not applicable. Informed Consent Statement
    Not applicable. Conflicts of Interest The authors declare no conflict of interest.
    References Dib, A.; Estcourt, D. Victoria Floods: Power Outages Continue across
    State as Extreme Weather Cleanup Begins. Available online: https://www.theage.com.au/national/victoria/about-24-000-still-without-power-as-victoria-s-storm-recovery-continues-20210614-p580s4.html
    (accessed on 16 June 2022). Beers, L.M. Wild Weather Smashes Victoria with Roads
    Closed, Flash Flooding and Power Outages. Available online: https://7news.com.au/weather/melbourne-weather/wild-weather-smashes-victoria-with-roads-closed-trees-down-and-flash-flooding-c-3067406
    (accessed on 16 June 2022). Arshad, B.; Ogie, R.; Barthelemy, J.; Pradhan, B.;
    Verstaevel, N.; Perez, P. Computer Vision and IoT-Based Sensors in Flood Monitoring
    and Mapping: A Systematic Review. Sensors 2019, 19, 5012. [Google Scholar] [CrossRef]
    [PubMed] [Green Version] Mitra, A.; Biswas, S.; Adhikari, T.; Ghosh, A.; De, S.;
    Karmakar, R. Emergence of Edge Computing: An Advancement over Cloud and Fog. In
    Proceedings of the 2020 11th International Conference on Computing, Communication
    and Networking Technologies (ICCCNT), Kharagpur, India, 1–3 July 2020; pp. 1–7.
    [Google Scholar] [CrossRef] Baller, S.P.; Jindal, A.; Chadha, M.; Gerndt, M. DeepEdgeBench:
    Benchmarking Deep Neural Networks on Edge Devices. In Proceedings of the 2021
    IEEE International Conference on Cloud Engineering (IC2E), San Francisco, CA,
    USA, 4–8 October 2021; pp. 20–30. [Google Scholar] Hajder, P.; Rauch, Ł. Moving
    Multiscale Modelling to the Edge: Benchmarking and Load Optimization for Cellular
    Automata on Low Power Microcomputers. Processes 2021, 9, 2225. [Google Scholar]
    [CrossRef] Yu, W.; Liang, F.; He, X.; Hatcher, W.G.; Lu, C.; Lin, J.; Yang, X.
    A survey on the edge computing for the Internet of Things. IEEE Access 2017, 6,
    6900–6919. [Google Scholar] [CrossRef] Kua, J.; Armitage, G.; Branch, P. A Survey
    of Rate Adaptation Techniques for Dynamic Adaptive Streaming Over HTTP. IEEE Commun.
    Surv. Tutorials 2017, 19, 1842–1866. [Google Scholar] [CrossRef] Kua, J.; Loke,
    S.W.; Arora, C.; Fernando, N.; Ranaweera, C. Internet of things in space: A review
    of opportunities and challenges from satellite-aided computing to digitally-enhanced
    space living. Sensors 2021, 21, 8117. [Google Scholar] [CrossRef] Leduc, P.; Ashmore,
    P.; Sjogren, D. Technical note: Stage and water width measurement of a mountain
    stream using a simple time-lapse camera. Hydrol. Earth Syst. Sci. 2018, 22, 1–11.
    [Google Scholar] [CrossRef] [Green Version] Kua, J.; Armitage, G.; Branch, P.;
    However, J. Adaptive Chunklets and AQM for Higher-Performance Content Streaming.
    ACM Trans. Multimed. Comput. Commun. Appl. 2019, 15, 1–24. [Google Scholar] [CrossRef]
    [Green Version] Kua, J.; Armitage, G. Optimising DASH over AQM-Enabled Gateways
    Using Intra-Chunk Parallel Retrieval (Chunklets). In Proceedings of the 2017 26th
    International Conference on Computer Communication and Networks (ICCCN), Vancouver,
    BC, Canada, 31 July–3 August 2017; pp. 1–9. [Google Scholar] [CrossRef] Kua, J.;
    Armitage, G.; Branch, P. The Impact of Active Queue Management on DASH-Based Content
    Delivery. In Proceedings of the 2016 IEEE 41st Conference on Local Computer Networks
    (LCN), Dubai, United Arab Emirates, 7–10 November 2016; pp. 121–128. [Google Scholar]
    [CrossRef] Kua, J.; Armitage, G. Generating Dynamic Adaptive Streaming over HTTP
    Traffic Flows with TEACUP Testbed; Tech. Rep. A; Centre for Advanced Internet
    Architectures, Swinburne University of Technology: Melbourne, Australia, 2016;
    Volume 161216, p. 16. [Google Scholar] Kua, J.; Nguyen, S.H.; Armitage, G.; Branch,
    P. Using Active Queue Management to Assist IoT Application Flows in Home Broadband
    Networks. IEEE Internet Things J. 2017, 4, 1399–1407. [Google Scholar] [CrossRef]
    Kua, J.; Branch, P.; Armitage, G. Detecting bottleneck use of pie or fq-codel
    active queue management during dash-like content streaming. In Proceedings of
    the 2020 IEEE 45th Conference on Local Computer Networks (LCN), Sydney, NSW, Australia,
    16–19 November 2020; pp. 445–448. [Google Scholar] Kua, J. Understanding the Achieved
    Rate Multiplication Effect in FlowQueue-based AQM Bottleneck. In Proceedings of
    the 2021 IEEE 46th Conference on Local Computer Networks (LCN), Edmonton, AB,
    Canada, 4–7 October 2021; pp. 439–442. [Google Scholar] Kua, J.; Al-Saadi, R.;
    Armitage, G. Using Dummynet AQM-FreeBSD’s CoDel, PIE, FQ-CoDel and FQ-PIE with
    TEACUP v1.0 Testbed; Tech. Rep. A; Centre for Advanced Internet Architectures,
    Swinburne University of Technology: Melbourne, Australia, 2016; Volume 160708,
    p. 8. [Google Scholar] Zhang, Z.; Zhou, Y.; Liu, H.; Gao, H. In-situ water level
    measurement using NIR-imaging video camera. Flow Meas. Instrum. 2019, 67, 95–106.
    [Google Scholar] [CrossRef] Bruinink, M.; Chandarr, A.; Rudinac, M.; van Overloop,
    P.J.; Jonker, P. Portable, automatic water level estimation using mobile phone
    cameras. In Proceedings of the 2015 14th IAPR international conference on Machine
    Vision Applications (MVA), Tokyo, Japan, 18–22 May 2015; pp. 426–429. [Google
    Scholar] Kim, Y.; Park, H.; Lee, C.; Kim, D.; Seo, M. Development of a cloud-based
    image water level gauge. IT Converg. Pract. (INPRA) 2014, 2, 22–29. [Google Scholar]
    Lin, Y.T.; Lin, Y.C.; Han, J.Y. Automatic water-level detection using single-camera
    images with varied poses. Measurement 2018, 127, 167–174. [Google Scholar] [CrossRef]
    Hough, P.V. Method and Means for Recognizing Complex Patterns. U.S. Patent 3,069,654,
    18 December 1962. [Google Scholar] Ortigossa, E.S.; Dias, F.; Ueyama, J.; Nonato,
    L.G. Using digital image processing to estimate the depth of urban streams. In
    Proceedings of the Workshop of Undergraduate Works in Conjunction with Conference
    on Graphics, Patterns and Images (SIBGRAPI), Bahia, Brazil, 26–29 August 2015;
    pp. 26–29. [Google Scholar] Li, H.P.; Wang, W.; Ma, F.C.; Liu, H.L.; Lv, T. The
    water level automatic measurement technology based on image processing. In Applied
    Mechanics and Materials; Trans Tech Publ.: Bäch SZ, Switzerland, 2013; Volume
    303, pp. 621–626. [Google Scholar] Zhang, Z.; Zhou, Y.; Liu, H.; Zhang, L.; Wang,
    H. Visual Measurement of Water Level under Complex Illumination Conditions. Sensors
    2019, 19, 4141. [Google Scholar] [CrossRef] [PubMed] [Green Version] Otsu, N.
    A Threshold Selection Method from Gray-Level Histograms. IEEE Trans. Syst. Man
    Cybern. 1979, 9, 62–66. [Google Scholar] [CrossRef] [Green Version] Ren, H.; Gao,
    N.; Li, J. Monocular Depth Estimation with Traditional Stereo Matching Information.
    In Proceedings of the 2019 6th International Conference on Systems and Informatics
    (ICSAI), Shanghai, China, 2–4 November 2019; pp. 1319–1323. [Google Scholar] Xian,
    K.; Shen, C.; Cao, Z.; Lu, H.; Xiao, Y.; Li, R.; Luo, Z. Monocular relative depth
    perception with web stereo data supervision. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–23 June
    2018; pp. 311–320. [Google Scholar] Pan, J.; Yin, Y.; Xiong, J.; Luo, W.; Gui,
    G.; Sari, H. Deep Learning-Based Unmanned Surveillance Systems for Observing Water
    Levels. IEEE Access 2018, 6, 73561–73571. [Google Scholar] [CrossRef] Jafari,
    N.H.; Li, X.; Chen, Q.; Le, C.Y.; Betzer, L.P.; Liang, Y. Real-time water level
    monitoring using live cameras and computer vision techniques. Comput. Geosci.
    2021, 147, 104642. [Google Scholar] [CrossRef] Bung, D.B.; Crookston, B.M.; Valero,
    D. Turbulent free-surface monitoring with an RGB-D sensor: The hydraulic jump
    case. J. Hydraul. Res. 2021, 59, 779–790. [Google Scholar] [CrossRef] Bi, Y.;
    Li, J.; Qin, H.; Lan, M.; Shan, M.; Lin, F.; Chen, B.M. An MAV Localization and
    Mapping System Based on Dual Realsense Cameras. In Proceedings of the 2016 International
    Micro Air Vehicle Conference and Competition, Beijing, China, 17–21 October 2016;
    pp. 50–55. [Google Scholar] Chen, Y.C.; Weng, W.C.; Lin, S.W. A High Reliability
    3D Object Tracking Method for Robot Teaching Application. In IOP Conference Series:
    Materials Science and Engineering; IOP Publishing: Bristol, UK, 2019; Volume 644.
    [Google Scholar] Wegner, P. Global IoT Market Size Grew 22% in 2021. Available
    online: https://iot-analytics.com/iot-market-size/ (accessed on 16 June 2022).
    Atzori, L.; Iera, A.; Morabito, G. The internet of things: A survey. Comput. Netw.
    2010, 54, 2787–2805. [Google Scholar] [CrossRef] Li, S.; Xu, L.D.; Zhao, S. The
    internet of things: A survey. Inf. Syst. Front. 2015, 17, 243–259. [Google Scholar]
    [CrossRef] Al-Fuqaha, A.; Guizani, M.; Mohammadi, M.; Aledhari, M.; Ayyash, M.
    Internet of things: A survey on enabling technologies, protocols, and applications.
    IEEE Commun. Surv. Tutorials 2015, 17, 2347–2376. [Google Scholar] [CrossRef]
    Ai, Y.; Peng, M.; Zhang, K. Edge computing technologies for Internet of Things:
    A primer. Digital Commun. Netw. 2018, 4, 77–86. [Google Scholar] [CrossRef] Pan,
    J.; McElhannon, J. Future edge cloud and edge computing for internet of things
    applications. IEEE Internet Things J. 2017, 5, 439–449. [Google Scholar] [CrossRef]
    Premsankar, G.; Di Francesco, M.; Taleb, T. Edge computing for the Internet of
    Things: A case study. IEEE Internet Things J. 2018, 5, 1275–1284. [Google Scholar]
    [CrossRef] [Green Version] Intel® RealSense™ Depth Camera D455. Available online:
    https://www.intelrealsense.com/depth-camera-d455/ (accessed on 22 October 2022).
    Keselman, L.; Iselin Woodfill, J.; Grunnet-Jepsen, A.; Bhowmik, A. Intel(R) RealSense(TM)
    Stereoscopic Depth Cameras. In Proceedings of the 2017 IEEE Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW), Honolulu, HI, USA, 21–26 July
    2017; pp. 1267–1276. [Google Scholar] [CrossRef] Grunnet-Jepsen, A.; Tong, D.
    Depth Post-Processing FOR Intel® REALSENSE™ Depth Camera D400 Series. Available
    online: https://dev.intelrealsense.com/docs/depth-post-processing (accessed on
    1 August 2022). Ngan, K.N. Experiments on two-dimensional decimation in time and
    orthogonal transform domains. Signal Process. 1986, 11, 249–263. [Google Scholar]
    [CrossRef] Crochiere, R.; Rabiner, L. Interpolation and decimation of digital
    signals—A tutorial review. Proc. IEEE 1981, 69, 300–331. [Google Scholar] [CrossRef]
    Kuching, Malaysia Weather History. Available online: https://www.wunderground.com/history/daily/my/kuching/WBGG
    (accessed on 18 April 2022). Das, A.; Patterson, S.; Wittie, M. EdgeBench: Benchmarking
    Edge Computing Platforms. In Proceedings of the 2018 IEEE/ACM International Conference
    on Utility and Cloud Computing Companion (UCC Companion), Zurich, Switzerland,
    17–20 December 2018; pp. 175–180. [Google Scholar] [CrossRef] Publisher’s Note:
    MDPI stays neutral with regard to jurisdictional claims in published maps and
    institutional affiliations.  © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Jan, O.R.; Jo, H.S.; Jo, R.S.; Kua, J. Real-Time
    Flood Monitoring with Computer Vision through Edge Computing-Based Internet of
    Things. Future Internet 2022, 14, 308. https://doi.org/10.3390/fi14110308 AMA
    Style Jan OR, Jo HS, Jo RS, Kua J. Real-Time Flood Monitoring with Computer Vision
    through Edge Computing-Based Internet of Things. Future Internet. 2022; 14(11):308.
    https://doi.org/10.3390/fi14110308 Chicago/Turabian Style Jan, Obaid Rafiq, Hudyjaya
    Siswoyo Jo, Riady Siswoyo Jo, and Jonathan Kua. 2022. \"Real-Time Flood Monitoring
    with Computer Vision through Edge Computing-Based Internet of Things\" Future
    Internet 14, no. 11: 308. https://doi.org/10.3390/fi14110308 Note that from the
    first issue of 2016, this journal uses article numbers instead of page numbers.
    See further details here. Article Metrics Citations Crossref   6 Web of Science   5
    Scopus   7 Google Scholar   [click to view] Article Access Statistics Article
    access statistics Article Views 28. Dec 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb
    26. Feb 7. Mar 17. Mar 0k 1k 2k 3k 4k For more information on the journal statistics,
    click here. Multiple requests from the same IP address are counted as one view.   Future
    Internet, EISSN 1999-5903, Published by MDPI RSS Content Alert Further Information
    Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs
    at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Future Internet
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Real-Time Flood Monitoring with Computer Vision through Edge Computing-Based
    Internet of Things
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Feng L.
  - You Y.
  - Liao W.
  - Pang J.
  - Hu R.
  - Feng L.
  citation_count: '6'
  description: Water is the source of life and a very important part of life. However,
    the current water resources security is facing global challenges. Affected by
    climate change and human factors, how to quickly realize the continuous monitoring
    of water resources changes is crucial to the sustainable management of water resources.
    In recent years, the amount of multi-temporal remote sensing data has grown rapidly,
    which has caused problems such as weak single-computer processing for change detection
    and difficulty in mass image storage management. Traditional single-computer processing
    methods can no longer meet the needs of remote sensing big data change detection.
    It provides cutting-edge methods and means for monitoring large-scale changes
    in water resources. Therefore, a distributed parallel method for remote sensing
    image change detection based on cloud computing is proposed. Through this method,
    the change detection efficiency of massive remote sensing images is improved,
    and the high-resolution remote sensing image change detection based on fully connected
    conditional random fields is deeply analyzed. By summarizing the experimental
    data of this method and comparing with other data, it can be seen that when the
    parallelism is higher, the advantage of running time is more obvious. The performance
    improvement is also as high as 89.49%. Therefore, distributed parallel processing
    through cloud computing can complete the efficient change detection of massive
    remote sensing images without affecting the accuracy. This helps us to better
    monitor and monitor water resources at multiple scales.
  doi: 10.1016/j.egyr.2022.09.134
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Cloud computing and remote sensing
    images 3. Change monitoring based on remote sensing images 4. Data analysis of
    multi-scale changes in water environment 5. Summary Declaration of Competing Interest
    Acknowledgment Data availability References Show full outline Cited by (7) Figures
    (10) Show 4 more figures Tables (3) Table 1 Table 2 Table 3 Energy Reports Volume
    8, November 2022, Pages 13610-13620 Review Article Multi-scale change monitoring
    of water environment using cloud computing in optimal resolution remote sensing
    images Author links open overlay panel Lei Feng a, Yu You b, Weiling Liao c, Jiawei
    Pang a d, Ronghao Hu e, Li Feng c Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.egyr.2022.09.134
    Get rights and content Under a Creative Commons license open access Abstract Water
    is the source of life and a very important part of life. However, the current
    water resources security is facing global challenges. Affected by climate change
    and human factors, how to quickly realize the continuous monitoring of water resources
    changes is crucial to the sustainable management of water resources. In recent
    years, the amount of multi-temporal remote sensing data has grown rapidly, which
    has caused problems such as weak single-computer processing for change detection
    and difficulty in mass image storage management. Traditional single-computer processing
    methods can no longer meet the needs of remote sensing big data change detection.
    It provides cutting-edge methods and means for monitoring large-scale changes
    in water resources. Therefore, a distributed parallel method for remote sensing
    image change detection based on cloud computing is proposed. Through this method,
    the change detection efficiency of massive remote sensing images is improved,
    and the high-resolution remote sensing image change detection based on fully connected
    conditional random fields is deeply analyzed. By summarizing the experimental
    data of this method and comparing with other data, it can be seen that when the
    parallelism is higher, the advantage of running time is more obvious. The performance
    improvement is also as high as 89.49%. Therefore, distributed parallel processing
    through cloud computing can complete the efficient change detection of massive
    remote sensing images without affecting the accuracy. This helps us to better
    monitor and monitor water resources at multiple scales. Previous article in issue
    Next article in issue Keywords Cloud computingRemote sensing imageWater resourceEnvironmental
    monitoring 1. Introduction Water is an extremely precious resource and plays an
    indispensable role in people’s production and life. On the one hand, water is
    the basis for sustaining life; on the other hand, water is the basic resource
    guarantee for industrial manufacturing and agricultural production, and is crucial
    to maintaining regional climate stability and achieving regional sustainable development.
    In the context of globalization and climate change, water security is facing global
    challenges. Rapid and accurate monitoring of changes in water resources is of
    great significance for sustainable water resources management. In recent years,
    with the rapid development of remote sensing technology, on a global or regional
    scale, the multi-temporal information obtained by non-contact sensors has been
    detected and analyzed through image processing and mathematical model technology,
    and changes in water resources have been comprehensively monitored. Because remote
    sensing technology has the advantages of wide coverage, strong timeliness, large
    amount of information, and poor geographical conditions, it can monitor a large
    area of water resources in a very short period of time. Especially in remote mountainous
    areas, because humans cannot access, remote sensing technology is used to monitor
    changes in water resources in this area. In the past, due to the limitation of
    remote sensing data acquisition and processing capabilities, most researches were
    based on local computers. By selecting several periods of remote sensing images
    with better observation quality, water resources change research was carried out
    every three or five years. With the increasing number of satellite sensors operating
    in orbit, the amount of remote sensing data, including images with various temporal
    and spatial resolutions, has shown an exponential growth trend, providing data
    support for dynamic monitoring of the surface. Powerful storage and computing
    capabilities process massive data in parallel and make full use of all available
    remote sensing observations and improved ground feature extraction algorithms
    to achieve continuous monitoring of water resources in large areas and long-term
    sequences. Today, with the rapid development of information technology, cloud
    computing has become more and more used in various aspects. Quirita V proposes
    a new distributed architecture for supervised classification of large amounts
    of Earth observation data in a cloud computing environment, which supports distributed
    execution, network communication, and fault tolerance in a way that is transparent
    to users, consisting of three It consists of abstraction layers that support researchers
    from different fields of scientific research to define and implement applications
    (Quirita et al., 2017). Wei W proposed a cloud resource allocation model based
    on incomplete information Stackelberg game (CSAM-IISG) after analyzing the competition
    of multiple user environments in cloud services, which is used in cloud computing
    environment Hidden Markov Model (HMM) (Wei et al., 2018a). Wang S proposed a cloud
    data encryption algorithm based on file-level features. The algorithm integrates
    the hierarchical access structure into a single access structure, and uses the
    integrated access structure to realize the encryption of multi-layer files, and
    realizes and attributes The sharing of relevant data information saves the time
    of data storage and encryption (Wang et al., 2017). Tsai JL proposed an efficient
    mobile cloud-based distributed service authentication method, which provides security
    and convenience for mobile users to access multiple mobile cloud computing services
    from multiple service providers using only a single private key, which The security
    strength is based on a bilinear paired cipher system and dynamic nonce generation
    (Tsai and Lo, 2017). With the continuous development and progress of science and
    technology, remote sensing image technology has gradually matured, and it has
    been used in various places in our lives through technical support, bringing convenience.
    It can be seen from the data that Li P used multi-source Synthetic Aperture Radar
    (SAR) satellite images and optical satellite remote sensing images, used rule-based
    target information extraction technology, and combined with artificial visual
    correction to conduct five-year research on the coast of Jiaozhou Bay. A data
    collection, the results show that the coast of Jiaozhou Bay has grown by 43.5
    km of coastline in the past 20 years, and the area has decreased by 24.9 km2 (Li
    et al., 2020). The Boswell A study evaluates the efficacy of remote sensing as
    an alternative to field sampling techniques in detecting ground cover features
    (i.e. trees, shrubs, herb cover, litter, surfaces) and compares the results with
    field measurements collected by the Utah Department of Wildlife Resources For
    comparison, it is found that remote sensing can quickly and accurately assess
    pasture vegetation and other surface attributes (Boswell et al., 2017). Ling-Xia
    C introduced the basic principles and methods of remote sensing images in water
    environment detection, focused on analyzing the application of remote sensing
    images in water environment pollution detection and control, and summarized the
    development trend (Ling-Xia and Junli, 2017). Shen Q reviewed the latest research
    results on urban black and odorous water bodies, and carried out spatial resolution
    remote sensing monitoring and screening, which is helpful to comprehensively understand
    its spatial distribution and treatment process, and provide strong technical support
    for the prevention and control of urban black and odorous water bodies (Shen et
    al., 2017). General, the combination of cloud computing and remote sensing images
    can realize all-round and multi-dimensional monitoring of water resources changes,
    and provide scientific and technical means and important tools for understanding
    the continuous process of spatial changes in water resources and sustainable management
    of water resources in the study area data base. Remote sensing image detection
    using cloud computing can greatly improve the efficiency, and the detection performance
    can be improved by at least 93% through parallel algorithms. In the existing research,
    there have been many theoretical studies and achievements on cloud computing,
    remote sensing image service, remote sensing image data sharing, etc., but there
    are many studies on cloud computing and optimal resolution remote sensing image
    water environment. Scale change monitoring is still limited. The main purpose
    of this research is to study the storage and management methods of spatial data
    such as optimal resolution remote sensing images in the cloud computing environment
    according to the characteristics and application requirements of optimal resolution
    remote sensing images, so as to provide services for deeper high-performance computing
    applications. The main contents of this paper are: (1) In view of the shortcomings
    of spatial data storage and management technologies such as remote sensing images
    in traditional distributed environments, on the basis of integrating existing
    cloud platform technologies, combined with optimal resolution Based on the characteristics
    of remote sensing image applications, theoretical analysis of cloud computing
    and remote sensing images. (2) According to the relevant theory and calculation
    method, the multi-scale image segmentation technology is adopted. According to
    the characteristics of different scales of objects, the segmentation of multi-scale
    levels is completed, and the multi-scale features are used to detect changes.
    (3) Then, a distributed parallel remote sensing image change detection method
    based on cloud computing is proposed. Through this method, the change detection
    efficiency of massive remote sensing images is improved, and the changes of high-resolution
    remote sensing images based on fully connected conditional random fields are deeply
    analyzed detection. (4) Finally, by summarizing the experimental data of this
    method and comparing it with other data, it can be seen that the higher the degree
    of parallelism, the more obvious the advantage of running time. 2. Cloud computing
    and remote sensing images 2.1. Overview of cloud computing Cloud computing is
    a new type of distributed computing technology that combines traditional computer
    technology with modern network technology. It is based on advanced virtualization
    technology and allows network application infrastructure to be expanded at low
    cost (Wu et al., 2017). This is a commercial computing model that distributes
    computing work in a large number of computer resource libraries. Users can configure
    the computing power and services they need according to their needs. This model
    is called “cloud”. Simply put, “cloud” is A software and hardware business. “Cloud”
    has many advantages. First, its size and scalability are like a cloud in the sky.
    Although it has no fixed location, it exists objectively. Many companies will
    choose cloud services, such as Baidu. Using cloud services, massive data can be
    stored in the cloud without the need for additional servers. In addition, the
    computing mode of cloud computing is more complex than the traditional computing
    mode. It is a virtual computing resource pool, which mainly realizes the dynamic
    scheduling and management of tasks and resources, which is dynamic, heterogeneous
    and different. Cloud computing is a technology that integrates computer technology
    and modern Internet technology (Wang et al., 2018). Among these technologies,
    traditional computer technologies include: distributed computing, load balancing,
    network storage, grid computing, utility computing, virtualization, etc. (Sun
    et al., 2019). Cloud computing is divided into broad and narrow senses. In a broad
    sense, cloud computing refers to obtaining the required services, that is, the
    delivery and use of services; in a narrow sense, what cloud computing obtains
    is the corresponding hardware, platform, software, etc., that is, Provision and
    use of IT equipment and ancillary equipment. The basic principle block diagram
    of cloud computing is shown in Fig. 1: Cloud computing centralizes all computing
    resources and automatically manages them by software without human involvement,
    so that application providers do not need to worry about tedious details, and
    can focus more on their own business, which is conducive to innovation and cost
    reduction. Download : Download high-res image (227KB) Download : Download full-size
    image Fig. 1. Basic principle block diagram of cloud computing. 2.2. Cloud computing
    services Cloud computing mainly includes the following three service levels: Infrastructure
    as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS).
    IaaS is the lowest-level service, providing users with hardware infrastructure
    services such as processing, storage, and networking, so that users can deploy
    systems to run software. The implementation of cloud infrastructure as a service
    is shown in Fig. 2: PaaS is built on the infrastructure, which refers to the software
    development platform as a service. In addition to providing a development platform,
    it also provides technical support services for the platform, so that users can
    deploy their own software operating environment. Download : Download high-res
    image (163KB) Download : Download full-size image Fig. 2. Cloud infrastructure
    service implementation. On the basis of the first two layers, SaaS provides ready-made
    software applications for customers to use directly. 2.3. Remote sensing technology
    and remote sensing image Remote sensing technology is a comprehensive technology,
    it was proposed in the 1960s, and now it has penetrated into all aspects of the
    national economy, and its role has become increasingly prominent (Guo et al.,
    2019). As human beings have more and more understanding of the planet on which
    they live, remote sensing technology has become an important way for humans to
    obtain ground data through ground measurement technology (Liu et al., 2018). With
    the continuous development of remote sensing technology, remote sensing technology,
    aerospace technology, electronic computer technology and other technologies, remote
    sensing technology has become a new technology that provides earth observation
    data in a dynamic, fast, accurate and diverse manner. That only records the electromagnetic
    wave size of various ground objects is called remote sensing image. In remote
    sensing, it mainly refers to aerial photos and satellite photos. All remote sensing
    images have spatial resolution, spectral resolution, temporal resolution and radiometric
    resolution. At present, there are more and more types of sensors for remote sensing
    image collection, and the spatial resolution, temporal resolution, and spectral
    resolution are also getting higher and higher, which can meet the needs of different
    levels and fields (Wang and Wang, 2017). High-resolution remote sensing images
    have the following characteristics: massive, multi-dimensional, and integrated.
    With the gradual development of remote sensing technology to “three full”, “three
    high” and “three more”, as well as the continuous accumulation of various remote
    sensing data and the construction of spatial database, spatial data has been recorded
    in an all-round way, making remote sensing images in human life., production has
    been widely used (Wenjie et al., 2020). At the same time, remote sensing image
    processing technology has also made great progress, and a large number of high-quality
    commercial remote sensing image processing software have come out one after another,
    laying a solid foundation for the promotion of remote sensing image processing
    technology. Remote sensing technology has important application value in the fields
    of land use, crop yield estimation, resource survey, environmental monitoring,
    and regional analysis. In view of the key technologies of remote sensing technology
    and remote sensing images and the characteristics of multi-scale changes in the
    water environment, the optimal resolution remote sensing image access, optimal
    resolution remote sensing image map service, and high-performance computing service
    Experiment to verify the efficiency. 2.4. Introduction to remote sensing image
    data The resource-3 (ZY-3) satellite is China’s first independent civilian high-resolution
    3D mapping satellite. Its orbital parameters are shown in Table 1 below. It can
    collect high-resolution three-dimensional images and multi-spectral images, with
    the characteristics of long-term, continuous and stable, and can be applied to
    various fields of production, life, agriculture, water conservancy, urban planning
    and construction in the country. ZY-3 satellite is equipped with four cameras,
    including a front-viewing panchromatic TDICCD camera; two front-viewing and rear-viewing
    panchromatic TDICCD cameras; and a front-viewing multispectral camera. Therefore,
    three stereo images of the same area with different observation angles can be
    obtained, which can provide a lot of three-dimensional geometric information (Aslan
    et al., 2022). The specific parameters of the satellite payload are shown in Table
    2. Table 1. ZY-3 track parameters. Parameter Index Orbital parameters Sun synchronous
    regression orbit Track height 505.948 km Track inclination 97.42° Local time of
    descending intersection 10:30 AM Regression cycle 59 days Table 2. ZY-3 satellite
    payload parameters. Payload Spectral segment number Spectral range (nm) Spatial
    resolution (m) Width of cloth (km) Side sway capacity Revisit time (day) Forward
    looking camera – 500–800 3.5 52 ±32° 3–5 Rear view camera – 500–800 3.5 52 ±32°
    3–5 Front view camera – 500–800 2.1 50 ±32° 3–5 Multispectral camera 1 450–520
    5.8 52 ±32° 5 2 520–590 3 630–690 4 770–890 2.5. Remote sensing image data storage
    and management under cloud computing With the advent of the information age and
    the continuous launch of high-resolution and high-spatial-resolution sensors,
    the remote sensing data received by satellite ground stations increases at a geometric
    multiple speed, the acquisition time and update time are greatly reduced, and
    the timeliness is also It is continuously enhanced (Shen et al., 2019). If you
    use the existing system to build a service platform, you must build your own software
    and software system, and based on your own system, build a suitable remote sensing
    system, and call the resources provided by the service provider in the form of
    service. In recent years, with the rapid development of technology, cloud computing
    has become a part of people’s lives. In order to solve the above problems, researchers
    applied cloud computing technology to the field of remote sensing, and built a
    set of remote sensing platform based on cloud computing, which reduced a large
    number of equipment, software, software procurement, post-operation costs, and
    maintenance. It improves the stability of the system, reduces the investment of
    scientific researchers in information technology, and provides a flexible, fast
    and flexible experiment, development, application and testing platform for the
    development of remote sensing technology (Jin et al., 2018). Compared with the
    traditional remote sensing data system, the system makes full use of the characteristics
    of data storage and publishing to provide users with personalized and diversified
    information products; while cloud computing technology is to combine application
    software, storage and computing equipment, etc. Technology and cloud computing
    technology are integrated to build a set of remote cloud services suitable for
    user needs. Through a reasonable optimization algorithm, a better solution can
    be found that can achieve reasonable allocation of computing resources, load balancing,
    and reduce server power consumption. Some research institutions have developed
    a remote sensing cloud business platform based on GCloud by using the cloud computing
    operating system G-Cloud developed by themselves (Bruin and Floridi, 2017). The
    platform has initially realized the integration of computer infrastructure, remote
    sensing data, data processing and data processing functions. The overall architecture
    of the cloud computing operating system G-Cloud can be divided into four levels:
    ① The resource layer of the system includes the hardware and integrated resources
    of the system; ② At the functional level, it realizes the basic business of the
    system; ③ At the business level 4. The application layer is mainly for the operation
    process generated by commercial applications. The cloud computing operating system
    G-Cloud architecture is shown in Fig. 3: The functional layer and the service
    layer are located on the cloud computing platform, and apply the basic functions
    of cloud computing such as distributed data storage, high-speed parallel processing
    of data, dynamic management and scheduling of resource pools, and virtualization.
    The interface between the system and the cloud computing platform is shown in
    Fig. 4. Download : Download high-res image (564KB) Download : Download full-size
    image Fig. 3. Architecture diagram of remote sensing cloud service platform. Download
    : Download high-res image (177KB) Download : Download full-size image Fig. 4.
    Interface relationship between remote sensing cloud service platform and cloud
    computing platform. 3. Change monitoring based on remote sensing images Change
    detection refers to the repeated observation of changes in objects in the same
    geographical location. In remote sensing images, different forms of ground objects
    have different grayscale, texture, shape and other forms, and change detection
    is to analyze the ground objects with different attributes, and eliminate the
    changes caused by meaningless interference, so as to detect the changes caused
    by the meaningless interference. The change information of interest is extracted
    (Hirai et al., 2017). The change of topographic features is mainly manifested
    in the appearance and disappearance of the ground, and the change of spatial structure,
    and the change of image reflection and local texture caused by this change is
    related to atmospheric conditions, illumination, observation angle, soil water
    content and other factors. Affects are well differentiated. In order to overcome
    these disturbances, various image processing methods must be adopted, and an appropriate
    change detection method must be selected according to the actual situation. The
    change detection of remote sensing images needs to obtain multiple image data
    in different phases and the same area, exclude the differences caused by different
    imaging conditions on the multi-temporal images, and then use the change detection
    technology to analyze and obtain the detection results of the change range and
    change type. Range detection is the process of judging whether a pixel has changed,
    and change type detection is the process of determining the direction of change
    (Wang and Chunheng, 2019). The general technical process can be divided into four
    parts: (1) data preprocessing; (2) change information extraction, extracting change
    intensity and change direction information; (3) change threshold determination,
    distinguishing between changed and unchanged ranges, and distinguishing different
    Change type, wherein, the distinction of change type is usually carried out on
    the basis of the detection result of the change range, and the analysis is carried
    out for the change pixel; (4) Accuracy evaluation. As shown in Fig. 5, it is the
    general processing flow of remote sensing image change detection : (1) data preprocessing
    Download : Download high-res image (215KB) Download : Download full-size image
    Fig. 5. General processing flow of remote sensing image change detection. The
    main purpose of preprocessing is to make the geographic coordinates and feature
    attributes of the same objects at the same location in different time-phase images
    the same, while different objects show different feature attributes. Therefore,
    change detection requires comprehensive analysis of multi-source remote sensing
    data in the same geographic location, and requires special data preprocessing
    (Li et al., 2019). Remote sensing image preprocessing includes geometric correction,
    radiometric correction, image enhancement, image fusion, image mosaicking and
    cropping, etc. Among them, geometric correction and radiometric correction are
    very critical in change detection and have a great impact on detection accuracy.
    In general, in multiple time-phase images of the same area, the gray value of
    the same band is linear, and the errors of the atmosphere and the sensor are also
    linear, so it can be described by a regression equation which is (1) where is
    the gray value of the image to be corrected in the mth band, is the gray value
    of the image to be corrected in the mth band after radiometric correction, and
    is the slope and intercept of the regression equation of the mth band, respectively,
    which can be determined according to the reference image. Pseudo-invariant feature
    points (Pseudo-InvariantFeatures, PIFs) are extracted from the image to be corrected,
    that is, the unchanged correction reference point, which is obtained by regression
    calculation. The key of the linear relative radiometric correction method is how
    to extract the pseudo-invariant feature points accurately and automatically. (2)
    Change information extraction In the extraction of change information, the differences
    in spectral, texture, shape and other feature information of multi-temporal images
    are compared to obtain the intensity and orientation information of changes, which
    is the key to change detection. In this process, the characteristics of the image
    must be extracted and analyzed from the image. For traditional low- and medium-resolution
    image change detection, the methods for extracting change information mainly include
    image algebra method, image transformation method, classification detection method,
    etc. These three methods are currently the most in-depth and widely used change
    detection methods ; On the basis of traditional change detection methods, the
    current research methods are mainly divided into two categories: object-oriented
    change detection and change detection based on fusion of spatial information.
    (3) Change Threshold Determination Change threshold determination in remote sensing
    image change recognition based on change intensity and direction information,
    divides the range of change and unchanged range, and distinguishes the type of
    change. The k-means algorithm and the Otsu threshold method (Otsu method) are
    two effective and commonly used methods for determining the threshold value of
    the variation range. (4) Accuracy evaluation After outputting the change detection
    results, by visually interpreting the effect of qualitative analysis of the change
    detection result map, the integrity of the ground objects and the degree of salt
    and pepper noise can be roughly judged. Determine whether the test results are
    reliable and whether their accuracy meets the requirements. At present, the commonly
    used accuracy assessment methods include confusion matrix and receiver operating
    characteristic curve. Using confusion matrix to evaluate the accuracy, the method
    is to regard the detection result of the binary change range as a binary classification
    result, and calculate the confusion matrix, overall accuracy, Kappa coefficient
    and other accuracy indicators. Where is the number of pixels with true change
    and detected, is the number of pixels with true change but not detected, is the
    number of pixels with no change but detected change, is the number of pixels with
    no change and no change detected, and the total number of pixels that have actually
    changed is the data calculated in, can calculate the accuracy evaluation index
    of each variation range/type test result: ① The correct number of pixels in the
    detection result is: (2) ② The number of pixels with wrong detection results is:
    (3) ③ The false detection rate is: (4) ④ The missed detection rate is: (5) ⑤ The
    overall accuracy is: (6) ⑥ Kappa coefficient is: (7) ROC is a curve drawn by traversing
    thresholds of different sizes to obtain the false detection rate under different
    thresholds. It is not affected by the threshold method and can intuitively evaluate
    the binary change detection method. The closer the curve is to the upper left
    corner, the stronger the detection ability. The value of the area under the curve
    (AreaUnderCurve) is between 0.5–1.0, the better the detection effect, the larger
    the value, which is a quantitative evaluation index of the change detection ability.
    3.1. Common change detection methods 3.1.1. Traditional change detection methods
    The traditional change detection technology has been further developed in the
    research and application in the field of remote sensing. Among them, the target
    change detection based on high-resolution images and the change detection based
    on fusion of spatial information are based on traditional methods and combine
    new ideas. Introduced into the processing of images (Loew et al., 2018). The traditional
    image change detection technology mainly includes image algebra method, image
    transformation method, classification detection method and so on. (1) Image algebra
    method Change detection methods based on image algebraic method describe changes
    through algebraic features between images. In addition to the simplest image difference
    method, it also includes image ratio method, regression analysis method, thematic
    index method, spectral angle method, and change vector analysis method etc., can
    provide more different change information. ① Image difference method: (8) where
    b is a constant. ② Image ratio method: (9) ③ Regression analysis method: (10)
    Among them, a and c are constants. ④ Spectral angle method: (11) ⑤ Change vector
    analysis method: It is an extension of the image difference method. The change
    vector is constructed according to the difference of each band of the image. The
    length of the change vector is used to characterize the size of the change, and
    the angle is used to characterize the type of change. The basic principle of change
    vector analysis is shown in Fig. 6. The calculated change vector intensity is
    used to judge whether there is a change. Assuming that the spectral characteristics
    of the pixel at the position (i, j) of the time-phase image are expressed as,
    the measurement criterion of the change intensity can be defined as : (12) Download
    : Download high-res image (71KB) Download : Download full-size image Fig. 6. Change
    vector analysis method of change detection principle. In the formula, the size
    describes the size of the change degree of the pixel. ⑥ Thematic index method:
    By calculating the normalized vegetation index, normalized water body index and
    other thematic indexes as features for comparative analysis. (2) image transformation
    Using the statistical structure of the image and using the characteristics of
    the image to extract the image, the redundancy of the data is reduced, so that
    the change of the image is more obvious, thereby enhancing the change of the image
    (Gill and Buyya, 2018). Image transformation methods mainly include PCA, tasseled
    cap transformation, canonical correlation analysis, CCA, independent component
    analysis, etc. The use of principal component analysis method for change detection
    can reduce the data dimension and improve the change information; the TC method
    uses three components of intensity, greenness and humidity to strengthen; the
    CCA algorithm can overcome the time dependence to a certain extent and reduce
    the complexity of the data sex. However, it is difficult for the image transformation
    method to obtain the change type at the same time, and the description of the
    transformation information is lacking. (3) Classification detection method The
    classification recognition method compares the classification maps according to
    the classification results of the images. This method can solve the difference
    of imaging conditions, sensors, etc. from different sources, reduce the problem
    of lowering the accuracy of change detection due to registration errors, and simultaneously
    obtain information on the type of change (Huanjun et al., 2018). However, due
    to the accumulation of errors in image classification and the use of classification
    maps in change detection, image information is lost, which affects image detection.
    3.1.2. Multiscale image segmentation with region adjacency graphs On the basis
    of the region neighbor map, the bottom-up region fusion technique is used to separate
    the multi-scale images of the two temporal phase images (Yi et al., 2017). The
    first step is to perform pre-segmentation with simple linear iterative cluster
    segmentation (SLIC). The second step is to find the RAG based on the average spectral
    value in the superpixels, take the average spectral distance of adjacent points
    as a homogeneity measure, and determine the threshold for hierarchical division
    (Wei et al., 2018b). The RAG data structure consists of a set of vertices and
    edges, vertices correspond to patches, and edges connect adjacent patches. RAG
    is defined as G=(C, R), among them, represents M vertices, and the corresponding
    area is represented by each vertex; represents n edges in RAG, and the weight
    of adjacent areas refers to the average spectrum of adjacent patches The distance
    of the value, defined as: (13) where, and are the average spectral values of two
    adjacent spots, respectively. 3.2. Feature extraction (1) Spectral characteristics
    Because its spectral characteristics can directly reflect the information of the
    image, it plays a great role in the conventional image transformation detection.
    The spectral features mainly consider the gray value of each waveband of the image,
    namely blue (Blue), green (Green), red (Red) and near-infrared (NIR) bands. The
    spectral characteristic of the target after image segmentation adopts the spectral
    mean value, that is, the average value of the spectral values of all pixels of
    the image target, and the spectral characteristic of the target corresponds to
    each frequency band of the image. The spectral mean of the nth object at the kth
    level is: (14) Among them, m is the number of pixels in the object, and is the
    spectral value of the th pixel in the object. (2) Texture features Texture features
    can describe the regional characteristics of images and utilize the spatial information
    of high-resolution remote sensing images. Gray Level Co-occurrence Matrix (GLCM)
    has received extensive attention in the fields of remote sensing image classification,
    remote sensing change detection and image segmentation. By constructing GLCM,
    a total of 14 texture feature statistics can be calculated to calculate the pixel
    texture in the object. The mean value of the feature is used as the texture feature
    of each level object. Below we will briefly introduce five commonly used texture
    features : ① Mean: The formula that reflects the regularity of the texture is
    as follows (15) ② Entropy: The formula that expresses the complexity of the texture
    is as follows (16) ③ Difference: The formula that reflects the local contrast
    of the image is as follows (17) ④ Contrast: The formula that reflects the clarity
    of the image and the depth of the texture grooves is as follows (18) ⑤ Homogeneity:
    The formula reflecting the homogeneity of the texture is as follows (19) (3) Shape
    features Geometric invariance, also known as geometric moment, is a moment feature
    that can reflect the geometric characteristics of the image. Even if the high-resolution
    remote sensing image is transformed by translation, rotation, scaling, etc., it
    can still classify complex terrain objects important in processing and analysis.
    When the calculation window is large, the calculation formula of the p+q order
    moment M two-dimensional discrete image is : (20) Among them, then its p+q order
    center distance is expressed as: (21) In the formula, represent the centroid of
    the image in the horizontal and vertical directions, respectively. When p=q=0,
    the zero-order moment represents the total brightness of the image. Then, the
    obtained central moments of each order need to be normalized: (22) 4. Data analysis
    of multi-scale changes in water environment Remote sensing image data is essentially
    composed of image features and physical features. Therefore, there are two ways
    to extract information from remote sensing image data. The usual method is an
    image-centric method, such as obtaining objects of interest from aerial photos
    and satellite images by artificial interpretation; or using computers to process
    ground objects, make elevation maps, Generate digital raster images, etc., all
    around the image, and finally get a new image. The second way is a data-centric
    approach. Experiments are carried out by using a dataset of multi-temporal remote
    sensing images with 996*750 pixels, a total size of 17.0 MB, and the collection
    time of 2018 and 2019, which are denoted as T1 and T2, respectively. First, we
    conduct serial and parallel experiments for T1 and T2, respectively, taking the
    average of three runs to avoid the effect of errors on the results. In parallel
    experiments, 2, 4, 8, 16, and 32 shards were used for processing. Fig. 7 below
    shows the detection accuracy of the original data under different shards in detail.
    As can be seen from the above figure, the value of the second fragment is 0.95648,
    the value of the 32nd fragment is 0.9469, the overall detection accuracy of the
    parallel experiment does not fluctuate significantly, and is basically consistent
    with the serial detection results, which proves the correctness of change detection
    in the distributed environment of cloud computing. Download : Download high-res
    image (160KB) Download : Download full-size image Fig. 7. Overall accuracy of
    change detection under different parallelism. Next, the original image copy was
    expanded to 136 MB (8 times), 341 MB (20 times), 683 MB (40 times), named Image1,
    Image2 and Image3 respectively, and parallel computing was performed under different
    shards to analyze its running time., the experimental results are shown in Table
    3: From Table 3, the processing time of picture 1 at parallelism 1 is 325.25 to
    19.56 at parallelism 32; when processing picture 2, the processing time is reduced
    from 622.21 to 35.46; For picture 3, the processing time goes from 1213.04 to
    69.15. As a result, with the continuous expansion of the original image copy,
    the processing time of the image will be greatly reduced. Table 3. Experimental
    parallel processing time. Parallelism 1 2 4 8 16 32 Image1 Time (s) 325.25 266.12
    180.13 93.58 33.46 19.56 Image 2 Time (s) 622.21 507.82 362.89 148.92 81.45 35.46
    Image 3 Time (s) 1213.04 960.50 715.11 234.88 179.56 69.15 The speedup ratio can
    be obtained by calculation, as shown in Fig. 8 below: As can be seen from Fig.
    8, from the 2nd parallelism the value is 1.22, while the 32nd parallelism has
    a value of 17.55, the speedup ratio increases gradually with the increase of parallelism.
    However, when the number of shards is low, the acceleration effect is not obvious,
    and the effect is more obvious when the number of shards is high. Therefore, in
    the distributed processing environment of cloud computing, the higher the degree
    of parallel computing, the higher the I/O transmission efficiency. The faster
    the sharded data is read, the better the acceleration effect. Download : Download
    high-res image (232KB) Download : Download full-size image Fig. 8. Speedup ratio
    under different parallelism. At the same time, in order to test the ability of
    the parallel algorithm of change detection to process larger amounts of data,
    we continued to replicate and expand T1 and T2 into 2.0 GB (120 times) and 4.0
    GB (240 times) data sets, named Image4 and Image5 respectively, and perform distributed
    parallel experiments on these datasets, and the running time is shown in Fig.
    9 : Due to the complexity of the algorithm calculation process and the large memory
    overhead, limited by the scale of the experimental platform, large-scale data
    cannot be run with a small number of shards, so the number of shards in this experiment
    is 32. It can be seen from Fig. 9 that, when the original copy is 136 MB, the
    running time is 23, and when the original copy is 4.0 GB, the running time is
    413, thanks to the distributed computing capability of cloud computing, the distributed
    parallel algorithm for change detection can still achieve good processing results
    when the amount of data increases gradually. Download : Download high-res image
    (148KB) Download : Download full-size image Fig. 9. Running time of parallel algorithm
    under large amount of data. In general, cloud computing distributed parallel algorithms
    have the ability to process large-scale data while ensuring the detection accuracy,
    but some computing resources are still limited in the computing process and are
    not well utilized. Therefore, in order to further improve The processing efficiency
    of the algorithm requires optimal scheduling of the computing resources of the
    platform. The data statistics of the running time and power consumption under
    single-machine and parallel operation are carried out, and the results under different
    degrees of parallelism are quantitatively analyzed. The specific data are shown
    in Fig. 10: As can be seen from the above figure, in terms of algorithm completion
    time, compared with the traditional single-computer computing method, the task
    completion time of the cloud computing remote sensing image change detection algorithm
    is significantly reduced, and the performance is improved by 70.85%, 89.49% and
    93.87% respectively; in terms of power consumption, since the serial algorithm
    only runs on one virtual machine, it uses the least resources, so the power consumption
    is lower than that of the parallel algorithm of cloud computing. Therefore, the
    follow-up will find or optimize algorithms to reduce power consumption. Download
    : Download high-res image (368KB) Download : Download full-size image Fig. 10.
    Runtime and power consumption of single and parallel machines with different degrees
    of parallelism. On the basis of combining the respective characteristics of the
    existing cloud platforms, it is proposed to integrate the characteristics of multiple
    existing open source platforms for multi-scale monitoring of the water environment
    with optimal resolution remote sensing images. Based on the existing problems
    in the application, the idea of integrating multiple services into the same platform
    for management is proposed, and a multi-scale image segmentation technology of
    optimal resolution remote sensing images in a general cloud computing environment
    is designed. This experiment is very suitable for water environment. Multiscale
    monitoring. 5. Summary With the continuous development of remote sensing technology,
    the amount and complexity of data continue to increase, and the complexity and
    computational complexity of algorithm models for remote sensing data processing
    also increase exponentially. The application of cloud computing to the processing
    of remote sensing data to realize the parallel inversion of remote sensing images
    makes the application prospects of remote sensing technology in ecological environment
    monitoring, urban planning and other fields more extensive and in-depth, and can
    provide a real-time basis for ecological protection and policy formulation data.
    In this paper, multi-scale image segmentation technology is used to complete the
    segmentation of multiple scale levels according to the characteristics of different
    scales of objects, and use the features of multiple scales to detect changes.
    Although the method in this paper is not sensitive to the segmentation scale,
    the segmentation process requires multiple experiments to determine the optimal
    segmentation scale, and lacks a complete quantitative evaluation criterion. In
    addition, this research is mainly aimed at remote sensing data, and the research
    on distributed storage of vector data is not deep enough. In the future work,
    a cloud storage method suitable for vector data will be designed. At the same
    time, it is also necessary to research more algorithms for remote sensing image
    processing on this basis, reduce power consumption, and realize comprehensive
    monitoring of water environment. Declaration of Competing Interest The authors
    declare that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgment
    This research is supported by the key project of Chongqing’s technology innovation
    and application development “Research and application of water quality improvement
    and water ecological restoration technology in cross-border basins of Sichuan
    and Chongqing”, No. cstc2021jscx cylhX0013. Data availability Data will be made
    available on request. References Aslan et al., 2022 Aslan Sinem, Zennaro Federica,
    Furlan Elisa, Critto Andrea Recurrent neural networks for water quality assessment
    in complex coastal lagoon environments: A case study on the venice lagoon Environ.
    Model. Softw., 154 (2022), Article 105403 View PDFView articleView in ScopusGoogle
    Scholar Boswell et al., 2017 Boswell A., Petersen S., Roundy B. Rangeland monitoring
    using remote sensing: comparison of cover estimates from field measurements and
    image analysis AIMS Environ. Sci., 4 (1) (2017), pp. 1-16 CrossRefGoogle Scholar
    Bruin and Floridi, 2017 Bruin B.D., Floridi L. The ethics of cloud computing Sci.
    Eng. Ethics, 23 (1) (2017), pp. 21-39 Google Scholar Gill and Buyya, 2018 Gill
    S.S., Buyya R. A taxonomy and future directions for sustainable cloud computing:
    360 degree view Acm Comput. Surv., 51 (5) (2018), pp. 1-33 CrossRefGoogle Scholar
    Guo et al., 2019 Guo Q., Hu H.M., Li B. Haze and thin cloud removal using elliptical
    boundary prior for remote sensing image IEEE Trans. Geosci. Remote Sens., 57 (11)
    (2019), pp. 9124-9137 CrossRefView in ScopusGoogle Scholar Hirai et al., 2017
    Hirai T., Masuyyama H., Kasahara S. Performance analysis of large-scale parallel-distributed
    processing with backup tasks for cloud computing J. Ind. Manag. Optim., 10 (1)
    (2017), pp. 113-129 CrossRefGoogle Scholar Huanjun et al., 2018 Huanjun L., Yue
    P., Xin D. Soil organic matter content inversion model with remote sensing image
    in field scale of blacksoil area Trans. Chin. Soc. Agric. Eng., 34 (1) (2018),
    pp. 127-133 Google Scholar Jin et al., 2018 Jin L., Zhang Y., Chen X. Secure attribute-based
    data sharing for resource-limited users in cloud computing Comput. Secur., 72
    (JAN.) (2018), pp. 1-12 Google Scholar Li et al., 2019 Li J., Abdulmohsin H.A.,
    Hasan S.S., et al. Hybrid soft computing approach for determining water quality
    indicator: Euphrates river Neural Comput. Appl., 31 (2019), pp. 827-837 CrossRefGoogle
    Scholar Li et al., 2020 Li P., Pu S., Li Z. Coastline change monitoring of jiaozhou
    bay from multi-source sar and optical remote sensing images since 2000 Wuhan Daxue
    Xuebao (Xinxi Kexue Ban)/Geom. Inf. Sci. Wuhan Univ., 45 (9) (2020), pp. 1485-1492
    View in ScopusGoogle Scholar Ling-Xia and Junli, 2017 Ling-Xia C., Junli Z. Application
    of remote sensing image in water environment testing J. Mines, Metals Fuels, 65
    (2) (2017), pp. 101-110 Google Scholar Liu et al., 2018 Liu X., Xia Y., Yang W.
    Secure and efficient querying over personal health records in cloud computing
    Neurocomputing, 274 (jan.24) (2018), pp. 99-105 View PDFView articleGoogle Scholar
    Loew et al., 2018 Loew F., Biradar C., Dubovyk O. Regional-scale monitoring of
    cropland intensity and productivity with multi-source satellite image time series
    GISci. Remote Sensing, 55 (4) (2018), pp. 539-567 Google Scholar Quirita et al.,
    2017 Quirita V., Costa Gaopd, Happ P.N. A new cloud computing architecture for
    the classification of remote sensing data IEEE J. Sel. Top. Appl. Earth Observ.
    Remote Sensing, 10 (2) (2017), pp. 409-416 View in ScopusGoogle Scholar Shen et
    al., 2019 Shen Y., Chen J., Xiao L. Optimizing multiscale segmentation with local
    spectral heterogeneity measure for high resolution remote sensing images ISPRS
    J. Photogramm. Remote Sens., 157 (Nov.) (2019), pp. 13-25 View PDFView articleGoogle
    Scholar Shen et al., 2017 Shen Q., Zhu L., Cao H.Y. Remote sensing monitoring
    and screening for urban black and odorous water body: A review Ying yong sheng
    tai xue bao = j. appl. ecol. / Zhongguo sheng tai xue xue hui, Zhongguo ke xue
    yuan Shenyang ying yong sheng tai yan jiu suo zhu ban, 28 (10) (2017), pp. 3433-3439
    View in ScopusGoogle Scholar Sun et al., 2019 Sun K., Zhang J., Zhang Y. Roads
    and intersections extraction from high-resolution remote sensing imagery based
    on tensor voting under big data environment Wirel. Commun. Mob. Comput., 2019
    (4s) (2019), pp. 1-11 Google Scholar Tsai and Lo, 2017 Tsai J.L., Lo N.W. A privacy-aware
    authentication scheme for distributed mobile cloud computing services IEEE Syst.
    J., 9 (3) (2017), pp. 805-815 Google Scholar Wang and Chunheng, 2019 Wang Yu,
    Chunheng A selection criterion for the optimal resolution of ground-based remote
    sensing cloud images for cloud classification IEEE Trans. Geosci. Remote Sens.,
    57 (3) (2019), pp. 1358-1367 CrossRefView in ScopusGoogle Scholar Wang et al.,
    2018 Wang J., Ma C., Zhang N. Monitoring and evaluation of ecological environmental
    damage and recovery capability based on remote sensing image normalization index
    683 Shenyang Jianzhu Daxue Xuebao (Ziran Kexue Ban)/J. Shenyang Jianzhu Univ.
    (Nat. Sci.), 34 (4) (2018), pp. 676-683 View in ScopusGoogle Scholar Wang and
    Wang, 2017 Wang Y., Wang C. High resolution remote sensing image segmentation
    based on multi-features fusion Eng. Rev., 37 (3) (2017), pp. 289-297 Google Scholar
    Wang et al., 2017 Wang S., Zhou J., Member An efficient file hierarchy attribute-based
    encryption scheme in cloud computing IEEE Trans. Inf. Forensics Secur., 11 (6)
    (2017), pp. 1265-1277 CrossRefGoogle Scholar Wei et al., 2018a Wei W., Fan X.,
    Song H. Imperfect information dynamic stackelberg game based resource allocation
    using hidden Markov for cloud computing IEEE Trans. Serv. Comput., 11 (99) (2018),
    pp. 78-89 CrossRefView in ScopusGoogle Scholar Wei et al., 2018b Wei W., Fan X.,
    Song H. Imperfect information dynamic stackelberg game based resource allocation
    using hidden Markov for cloud computing IEEE Trans. Serv. Comput., 11 (99) (2018),
    pp. 78-89 CrossRefView in ScopusGoogle Scholar Wenjie et al., 2020 Wenjie, LIN,
    Yu High-resolution remote sensing image segmentation using minimum spanning tree
    tessellation and RHMRF-FCM algorithm J. Geodesy Geoinf. Sci., v.3 (01) (2020),
    pp. 54-65 Google Scholar Wu et al., 2017 Wu X., Liu T., Cheng Y. Dynamic monitoring
    of straw burned area using multi-source satellite remote sensing data Trans. Chin.
    Soc. Agric. Eng., 33 (8) (2017), pp. 153-159 View in ScopusGoogle Scholar Yi et
    al., 2017 Yi H., Chan J., Alpcan T. Using virtual machine allocation policies
    to defend against co-resident attacks in cloud computing IEEE Trans. Dependable
    Secure Comput., 14 (1) (2017), pp. 95-108 Google Scholar Cited by (7) Editorial:
    Smart energy infrastructures for smart cities 2023, Energy Reports Does rural
    tourism revitalize the countryside? An exploration of the spatial reconstruction
    through the lens of cultural connotations of rurality 2023, Journal of Destination
    Marketing and Management Show abstract T-UNet: Triplet UNet for Change Detection
    in High-Resolution Remote Sensing Images 2023, arXiv Remote Sensing and Image
    Processing Techniques for Water Environment Monitoring: A Case Study of the Beijing-Tianjin-Hebei
    Region 2023, Traitement du Signal Identity Consistency Construction for Visible-Infrared
    Person Re-identification in Cloud Environment 2023, Lecture Notes in Electrical
    Engineering Landscape ecological risk assessment and influencing factor analysis
    of basins in suburban areas of large cities – A case study of the Fuchunjiang
    River Basin, China 2023, Frontiers in Ecology and Evolution View all citing articles
    on Scopus © 2022 Published by Elsevier Ltd. Part of special issue Special Issue
    on Smart Energy Infrastructures for Smart Cities Edited by Zheng Xu, Vijayan Sugumaran,
    Neil Yen View special issue Recommended articles Biosensing of BCR/ABL fusion
    gene using an intensity-interrogation surface plasmon resonance imaging system
    Optics Communications, Volume 377, 2016, pp. 24-32 Jiangling Wu, …, Shijia Ding
    View PDF Simulation of an electronic equipment control method based on an improved
    neural network algorithm Energy Reports, Volume 8, 2022, pp. 13409-13416 Hao Liu,
    Wei Wang View PDF Dual-mode operation control of smart micro grid based on droop
    strategy Energy Reports, Volume 8, 2022, pp. 9017-9024 Bin Wang, Yupeng Sang View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 6 Captures
    Readers: 33 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Energy Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Multi-scale change monitoring of water environment using cloud computing
    in optimal resolution remote sensing images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Yao Z.
  - Zhao C.
  citation_count: '7'
  description: Missing data are quite common in the industrial field. Since most data
    driven methods used in these applications rely on complete and high-quality data
    set, it is important to handle the missing data problem. Also, the severity of
    missing data varies across factories, which means that a single factory could
    fail to handle missing data locally. With the rapid development of cloud–edge
    computing, different factories could work together to handle missing data problem
    by federated learning without sharing their private training data. However, popular
    federated imputation methods assume each edge, i.e., a factory, to be an equal
    participant during learning a central model in the cloud, and thus are unable
    to handle heterogeneous data across different clients, leading to slow convergence
    and degraded learning performance. In this paper, a federated transfer missing
    data imputation method (FedTMI) is proposed to address this dilemma. Firstly,
    edge models are built with traditional Generative Adversarial Imputation Nets
    (GAIN) trained on edge data sets and edge knowledge is extracted as knowledge
    vectors to identify variables which could provide room for performance improvement.
    Secondly, for a certain target edge, with edge models and edge knowledge being
    accumulated in the cloud, models from non-target edges are chosen as helper models
    following certain rules aided by the corresponding edge knowledge. The helper
    models could provide effective guidance for data imputation in the target edge.
    Thirdly, the target edge executes federated transfer learning with the selected
    helper models. Model knowledge of helper models is transferred to the target edge,
    forming an updated target edge model with its edge data. Case studies on steam-driven
    water pumps in thermal power plants show the feasibility of the proposed FedTMI.
    It outperforms the baseline method with model averaging (FedAvg), especially when
    the data are not independent and identically distributed (non-i.i.d.) across different
    edges.
  doi: 10.1016/j.jprocont.2022.08.004
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Problem statement and
    motivation 3. FedTMI: Knowledge aided federated transfer learning for industrial
    missing data imputation 4. Experiments 5. Conclusion CRediT authorship contribution
    statement Declaration of Competing Interest Acknowledgments References Show full
    outline Cited by (9) Figures (7) Show 1 more figure Tables (5) Table 1 Table 2
    Table 3 Table 4 Table 5 Journal of Process Control Volume 117, September 2022,
    Pages 206-215 FedTMI: Knowledge aided federated transfer learning for industrial
    missing data imputation Author links open overlay panel Zoujing Yao, Chunhui Zhao
    Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.jprocont.2022.08.004
    Get rights and content Highlights • Data non-i.i.d. problem is conquered with
    this federated transfer imputation method. • Knowledge is transferred to the target
    edge indirectly through helper models. • Communication between cloud and edges
    is not frequent in the designed method. • The proposed FedTMI outperforms three
    baseline methods in a real industrial process. Abstract Missing data are quite
    common in the industrial field. Since most data driven methods used in these applications
    rely on complete and high-quality data set, it is important to handle the missing
    data problem. Also, the severity of missing data varies across factories, which
    means that a single factory could fail to handle missing data locally. With the
    rapid development of cloud–edge computing, different factories could work together
    to handle missing data problem by federated learning without sharing their private
    training data. However, popular federated imputation methods assume each edge,
    i.e., a factory, to be an equal participant during learning a central model in
    the cloud, and thus are unable to handle heterogeneous data across different clients,
    leading to slow convergence and degraded learning performance. In this paper,
    a federated transfer missing data imputation method (FedTMI) is proposed to address
    this dilemma. Firstly, edge models are built with traditional Generative Adversarial
    Imputation Nets (GAIN) trained on edge data sets and edge knowledge is extracted
    as knowledge vectors to identify variables which could provide room for performance
    improvement. Secondly, for a certain target edge, with edge models and edge knowledge
    being accumulated in the cloud, models from non-target edges are chosen as helper
    models following certain rules aided by the corresponding edge knowledge. The
    helper models could provide effective guidance for data imputation in the target
    edge. Thirdly, the target edge executes federated transfer learning with the selected
    helper models. Model knowledge of helper models is transferred to the target edge,
    forming an updated target edge model with its edge data. Case studies on steam-driven
    water pumps in thermal power plants show the feasibility of the proposed FedTMI.
    It outperforms the baseline method with model averaging (FedAvg), especially when
    the data are not independent and identically distributed (non-i.i.d.) across different
    edges. Previous article in issue Next article in issue Keywords Data imputationFederated
    learningTransfer learningIndustrial process 1. Introduction High quality data
    are of much importance in building an effective model in industrial field since
    data-driven methods for tasks like fault diagnosis and process monitoring usually
    present degraded performance with incomplete data set. However, missing data are
    widespread especially in process industry because of failure or maintenance of
    hardware sensors. Errors in data transmission between sensors and database or
    in accessing database also account for missing data [1]. The demand for high quality
    data makes missing data a fundamental problem in the industrial field. An increasing
    number of algorithms are proposed due to the significance of addressing missing
    data, most of which are missing data imputation (MDI) methods. Traditional algorithms
    for MDI can be categorized into univariate imputation and multivariate imputation
    [1]. Univariate imputation such as mean imputation (MI) is simple and of high
    efficiency but will distort data distribution and form a spike on the imputed
    point. Multivariate imputation considers the difference in variable distributions,
    and hence plays a more important role for data missing. Most multivariate data
    imputation methods are based on machine learning. Algorithms such as principle
    component analysis (PCA) [2], slow feature analysis (SFA) [3], expectation–maximization
    (EM) [4], decision trees (DT) [5] and support vector machines (SVM) [6] can be
    applied to handle this problem. There are derived methods such as bayesian principle
    component analysis (BPCA) [7] and probabilistic principal component analysis (PPCA)
    [8] to perform missing data imputation as well. Seeking for local similarity structures
    among samples for imputing missing values, machine learning strategies like K-nearest
    neighbors (KNN) [9], local least squares (LLS) [10], iterative locally auto-weighted
    least squares (ILAW-LS) [11] are adopted in data imputation. These machine learning
    algorithms have simple modeling procedures but require complete data sets as training
    data. Recently, deep learning techniques have been applied to the problem of MDI
    for their advantage in fitting complex data distribution, including recurrent
    neural networks [12], [13], variational autoencoder (VAE) [14], [15], [16], [17],
    denoising autoencoder (DAE) [18], [19], [20] as well as multiple imputation with
    deep denoising autoencoder (MIDA) [21]. Deep transfer learning is also introduced.
    For example, a deep probabilistic transfer learning framework is proposed by Chai
    et al. [22] for soft sensor modeling with missing data. These methods could capture
    complex correlations in the data set with multiple layers of nonlinear computations
    using complete data in the training process. Since a complete data set is unavailable
    in some industrial field, generative adversarial network (GAN) is introduced in
    MDI [23], [24], [25]. Yoon et al. [23] proposed a method for imputing missing
    data with GAN and called it Generative Adversarial Imputation Nets (GAIN). The
    method can use incomplete data in the training process directly. The GAIN model
    obtains the complex distribution of the incomplete data set and then makes multiple
    imputations, capturing the uncertainty of the imputed values. Although there have
    been data imputation methods to deal with missing data, they all assume that all
    data sets are accessible. In other words, they could only be applied in a separate
    factory or in factories with shared data. But in reality, data are often stored
    in a distributed manner with limited access. Also, the severity of missing data
    varies across factories, which means that a factory could fail to handle missing
    data locally with the above MDI methods. With the rapid development of cloud–edge
    computing, different factories could work together to handle missing data problem.
    An intuitive approach is to use data from all factories directly to train an imputation
    model. But due to industry competition and privacy security, process data from
    different factories are not open. Many countries across the world are also strengthening
    laws in the protection of data security and privacy. To train models using decentralized
    training data in different factories, federated learning (FL) is emerging as a
    new framework not requiring the edges to transmit their raw data sets [26]. Zhou
    et al. [27] proposed FCGAI which follows a FedAvg [28] approach. FCGAI averages
    the model parameters from different clients during the training process. Different
    clients could have data sets with different distributions, which is known as data
    heterogeneity [29]. Consequently, the optimum of each edge’s loss function may
    be quite far from each other and the difference between the averaged result of
    the local optima and the global optimum naturally arises. The popular federated
    MDI methods such as FCGAI is thus unable to handle the non-i.i.d. problem in which
    data distribution varies in different edges. To overcome the shortcomings of the
    methods that use model averaging with data heterogeneity, several FL algorithms
    have been proposed. Li et al. [30] mitigated the adverse impact of data heterogeneity
    by incorporating a proximal term in the original objective. Wang et al. [31] eliminated
    the inconsistency of the mismatched objectives with normalized averaging. Deng
    et al. [32] decoupled parameter updating in edges and in the server with a snapshotting
    scheme. These are all modifications to FedAvg without changing its basic idea.
    Communication between central server and the cloud remains frequent and heterogeneous
    data are still not taken good care of to build models that fit different edges.
    A federated transfer imputation scheme is designed to solve this dilemma mentioned
    above, which includes three main steps, that is, edge imputation model establishment
    and edge knowledge extraction, knowledge aided helper model selection in the cloud
    and federated transfer learning for industrial missing data. First of all, edge
    models are built with traditional Generative Adversarial Imputation Nets (GAIN)
    trained on edge data sets. With each edge having its GAIN model, edge knowledge
    is extracted as edge knowledge vectors, making it possible to identify variables
    which could provide considerable room for performance improvement of data imputation.
    Edge models and edge knowledge are then uploaded to the cloud. Second, helper
    models are selected for each edge with the designed helper model selection scheme.
    For a certain target edge, models from non-target edges are chosen as helper models
    following certain rules aided by edge knowledge. These helper models could provide
    effective guidance for data imputation in the target edge, lowering the communication
    cost with limited numbers of helper models. Third, the target edge executes federated
    transfer learning with helper models to get an improved imputation model that
    fits its own data distribution. Knowledge about data distribution in non-target
    edges is transferred indirectly with the designed pseudo labeling scheme with
    a quantified reliability index for helper models. Compared with the FedAvg approach,
    communication cost between cloud and edges could be reduced significantly with
    improved imputation performance since the third step is executed locally without
    extra communication with the cloud. Accordingly, this paper makes the following
    major contributions • With the designed helper model selection scheme and federated
    learning algorithm, knowledge about data distribution of non-target edges is transferred
    to the target edge indirectly through helper models. • Compared with popular federated
    imputation methods, the proposed federated transfer data imputation method conquers
    data non-i.i.d. problem by introducing pseudo labels with quantified reliability
    index for helper models. The remainder of the paper is organized as follows. Section
    2 shows problem statement and motivation about our work. Section 3 presents the
    proposed method, followed by case studies on a power generating process in Section
    4. Lastly, conclusions and future works are given in Section 5. Download : Download
    high-res image (370KB) Download : Download full-size image Fig. 1. The schematic
    plot of industrial data imputation federated transfer learning problem. refers
    to a single factory holding data set . Imputation model for and edge knowledge
    vector could be uploaded to the cloud and then downloaded by other edges. It is
    noted that the cloud has no access to edge data sets. The goal is to make use
    of the models and knowledge to train better edge models across multiple decentralized
    edge devices. 2. Problem statement and motivation In this paper, we suppose that
    there are P factories that holds data sets which is shown as Fig. 1. is the number
    of samples in factory . Dim is the number of variables shared by all factories.
    It is noted that in the industrial imputation problem setting, the data owned
    by P factories is could be non-independent identically distributed (non-i.i.d.).
    Each factory cannot directly use data from other factories to train its imputation
    model. The cloud could get edge models and other information from different factories
    but has no access to edge data sets. The data quality and data distribution of
    data holders is diverse, although they have the same data features. For example,
    when data missing occurred, there may be different degrees of data missing. For
    each edge, we consider that the data comes from a Dim-dimensional space . The
    complete and true data set takes values in . The observed data contains missing
    data which is represented by NaN. Mask takes values in indicating whether the
    observed data point in X exists or misses. If exists, is 1, otherwise it is 0.
    The relation between , M and X can be presented by (1) The goal of federated transfer
    learning for industrial data imputation is to make use of the models and knowledge
    to train better edge models across multiple decentralized edge devices holding
    local data samples without exchanging them. Conventional MDI methods assuming
    that all data sets are accessible are only applicable when there is only one factory
    and no data sharing. Meanwhile, they could be unable to deal with missing data
    in highly missing variables. The existing federated MDI method takes the FedAvg
    approach to form a central model in the cloud with different factories working
    together to handle missing data problem. However, as is shown in Fig. 2, with
    varied data distribution and inconsistent model optimization directions on different
    edges, the aggregated model with FedAvg averaging the optimization direction could
    possibly fail to fit in edge data sets. This is known as client–drift [33]. Download
    : Download high-res image (151KB) Download : Download full-size image Fig. 2.
    The schematic plot of the optimization process with FedAvg on three edges. is
    the schematic plot of data distribution on edge i. Existing federated learning
    methods for missing data imputation such as FCGAI [27] aim to learn a single global
    model across the data with the traditional FedAvg approach, unable to deal with
    non-i.i.d. data. Transfer learning could be introduced to solve this problem [34].
    Models in different edges could be initialized and trained separately to learn
    different knowledge on their own data sets so that they could get distinct “expertise”.
    Such “expertise” could be transferred across edges via the central cloud in a
    more focused way by concentrating on specific variables which could provide considerable
    room for performance improvement. These specific variables could be recognized
    with certain knowledge, of which the most important information should be variables
    with high missing rates and the reconstruction performance for these variables
    with the current edge model. This could help in deciding what and how much information
    from other edges is needed for a certain target edge in the transfer learning
    process. To summarize, a helper model selection method and a federated transfer
    data imputation scheme is in need for better data imputation under federated settings
    with data heterogeneity. 3. FedTMI: Knowledge aided federated transfer learning
    for industrial missing data imputation The structure of knowledge aided federated
    learning for industrial missing data imputation proposed in this paper is shown
    in Fig. 3. Firstly, edge models Models are established on different edges along
    with edge knowledge EK analyzed and collected. Secondly, the cloud receives edge
    models and edge knowledge from all edges and selects helper models for each target
    edge according to EK. Thirdly and most importantly, the target edge downloads
    its helper models and performs federated transfer learning for missing data imputation
    to update its edge model. It is noted that for edge , the edge model training
    process for and the transfer learning process for only use the edge data set .
    The three main steps, that is, edge imputation model establishment and edge knowledge
    extraction in all edges, knowledge aided helper model selection in the cloud and
    federated transfer learning for missing data imputation, are explained in detail
    in the following subsections. Download : Download high-res image (327KB) Download
    : Download full-size image Fig. 3. Architecture of knowledge aided federated learning
    for industrial missing data imputation. Assuming that there are P factories. and
    the target edge is edge 1, the solid double arrows refer to communication between
    edges and the cloud. Edges send their initial edge models and edge information
    to the cloud and the cloud would send helper models to edge 1 after knowledge
    aided helper model selection. 3.1. Edge imputation model establishment and edge
    knowledge extraction Edge models in different edges are established with Generative
    Adversarial Imputation Nets (GAIN) [23] forming a series of generators and discriminators.
    In GAIN, the generator (G) observes some components of a real data vector, imputes
    the missing components conditioned on what is actually observed, and outputs a
    completed vector. The discriminator (D) then takes a completed vector and attempts
    to determine which components were actually observed and which were imputed. To
    ensure that D forces G to learn the desired distribution, there is a hint generator
    revealing to D partial information about which elements in the original sample
    are missing. This hint ensures that G does in fact learn to generate according
    to the true data distribution. The GAIN structure is shown in Fig. 4. After the
    establishment of edge imputation models, edge knowledge EK is extracted from each
    edge with an edge knowledge vector ek. ek is composed of the edge ID, the missing
    rates of different variables MR, the reconstruction of non-missing elements of
    different variables with the current edge imputation model and the number of edge
    samples N, as is shown in Eq. (2). Edge knowledge extraction makes it possible
    to identify variables where there could be considerable room for performance improvement
    of data imputation. (2) Download : Download high-res image (167KB) Download :
    Download full-size image Fig. 4. Structure of GAIN, including a generator, a discriminator
    and a hint generator. The dark gray blocks and light gray blocks represent data
    matrixes and the loss in GAIN respectively. The solid arrows represent data flow
    and the dash arrows represent back propagation. 3.2. Knowledge aided helper model
    selection With P clients participating in the whole cloud–edge system, communication
    cost would increase exponentially if all their models are involved in a federated
    transfer learning process of a target edge. A knowledge aided helper model selection
    method is thus proposed to decrease the communication cost between a target edge
    and the cloud. The main idea of the designed selection scheme is that only those
    models that are likely to have good imputation performance on variables with high
    missing rates should be selected for each specific target edge so that number
    of models to be transferred from the cloud to edges in a transfer learning process
    could be decreased. The selection should help the following transfer learning
    process to focus on specific variables which could provide considerable room for
    performance improvement. This could be achieved with edge knowledge having been
    extracted from all edges. Edge knowledge vectors eks are gathered together in
    the cloud, forming edge knowledge EK. While each edge can be the target edge in
    the later federated transfer learning process at one time, we assume that edge
    p is the target edge in the following discussion. For edge p, the cloud would
    compare with eks from other edges and choose complemental helper models accordingly.
    The selection process executes the following three steps: Step 1: find variables
    with relatively high missing rates of the target edge with and . is composed of
    serial numbers of variables with their missing rates higher than the corresponding
    missing rates of the variables in non-target edges; Step 2: find helper models
    with the help of , and . is composed of edge ID numbers with their reconstruction
    of non-missing elements of higher than that of edge p; Step 3: eliminate helper
    model ID from if their edge samples N is less than 1000. 3.3. Federated transfer
    learning for missing data imputation For edge p, we have indicating the edge ID
    of helper models and indicating which variables are the focus of the imputation
    helper models. If has more than 1 elements, randomly reserve one of them as the
    current helper model and perform transfer learning for edge p. If has 0 elements
    or all the elements in have been selected, the transfer learning process is completed
    for edge p. The cloud would convey the model knowledge of edge . Since it is the
    generator rather than the discriminator that learns the distribution of the data
    in each edge, the cloud only conveys parameters of the generators, namely . As
    data owned by different edges are non-i.i.d., the helper models selected cannot
    be fully trusted. Their performance on the edge data set should be taken into
    consideration in the transfer learning process. A weighted transfer learning method
    is thus designed to integrate model knowledge with care. Detailed explanations
    for the generator, the discriminator, the hint generator and the loss function
    are shown below. Generator module in the transfer learning process. The generator
    module has a local to be optimized and a fixed from the helper model. has knowledge
    about data distribution in edge . When data in the target edge and edge are non-i.i.d.,
    directly using parameters of in the transfer learning would cause model mismatching.
    So is used indirectly to generate part of imputed elements for the missing ones
    of variables in as is shown in Fig. 5. First of all, replace the missing values
    in with random noise and get as is shown in Eq. (3). takes and as inputs and outputs
    the estimated data matrix as is shown in Eq. (4). and is the whole training set
    in edge p. is the number of whole training samples in edge p. The here is similar
    to the noise variable introduced in standard GAN [35]. (3) (4) Download : Download
    high-res image (285KB) Download : Download full-size image Fig. 5. The schematic
    plot of pseudo labeling in FedTMI with helper generator and two elements in .
    Second, we use as the weight of the helper model on variable considering its reconstruction
    performance on the edge data set. It helps to figure out the reliability of model
    knowledge. is calculated as below (Eqs. (5)–(7)). is the number of non-missing
    elements of variable lv in . Since only one helper model is reserved in a federated
    learning round, the denominator in Eq. (7) should be greater than or equal to
    1. It is set to be 2 for simplicity. It is noted that in each federated transfer
    learning process with a helper model , is calculated once, which means there is
    low computation complexity. (5) (6) (7) Third, with X and M as the sampled batch
    data with batch size I in the training process, we define a matrix , where is
    randomly sampled from the set and equals 0. The proportion of 1 element in is
    . If equals 1 and equals 0, the corresponding element generated by is then taken
    as a pseudo label for the missing elements as is shown in Eqs. (8), (9). Consequently,
    M is changed to as is shown in Eq. (10). (8) (9) (10) Finally, with imputing part
    of the missing elements, uses pseudo labels as well as local data to perform missing
    data imputation in the model training process. The to be optimized takes and as
    inputs and outputs as is shown in Eqs. (11), (12). is the repaired matrix where
    vectors take the partial observation and replace missing values with the corresponding
    values of as is shown in Eq. (13). It is noted that in the testing process, pseudo
    labels are not used with equals 0. (11) (12) (13) Hint Generator in the transfer
    learning process. In the original GAIN model, the hint generator conveys partial
    information about the binary mask matrix M to the discriminator D so as to ensure
    that the generator G generates data according to the true data distribution [21].
    In the transfer learning process, the hint generator takes instead of M as its
    inputs and outputs , as is shown in Eq. (14). If does not contain “sufficient”
    information about , it cannot be guaranteed that G learns the data distribution
    of real data [36]. In the defined matrix , its elements are randomly sampled from
    the set. (14) Discriminator module in the transfer learning process. The discriminator
    D in the transfer learning process takes and as inputs and outputs . corresponds
    to the probability that is a component which is observed or pseudo labeled rather
    than imputed, varying from 0 to 1. This is shown in Eq. (15). D is to reconstruct
    the mask matrix . (15) The Loss Function of D in the transfer learning process.
    Since the goal of D is to distinguish which components are observed or imputed,
    the loss function of D should be a cross-entropy loss function. But with the hint
    matrix passing part of the information from , which means that when , D can easily
    identify whether the corresponding component is observed or imputed. Therefore,
    only when , namely , should the cross-entropy loss be included in the loss function,
    as is shown in Eq. (16). is the total number of zero elements in . (16) The Loss
    Function of G in the transfer learning process. The loss function of G consists
    of two parts. The first part of the loss function indicates whether the imputed
    value replacing the missing value has deceived D or not. Similar to the loss in
    D, only when is the loss included in the loss function as is shown in Eq. (17).
    The second part is the reconstruction error of the observed data ensuring that
    the distribution of the imputed data is similar to the real data distribution
    as is shown in Eq. (18). is the total number of zero elements in , that is, the
    number of observed elements plus the number of pseudo labeled elements. The total
    loss function of G is shown in Eq. (19). is the weight of reconstruction loss
    . (17) (18) (19) The detailed steps of the federated transfer learning process
    for edge are as follows: Step 1: initialize G and D with the target edge model;
    Step 2: randomly take I samples from the training set to obtain the required missing
    data matrix and its corresponding mask matrix ; Step 3: compute the weights of
    each helper model is the reconstruction of non-missing elements of variable on
    the target edge data set with helper model ; Step 4: imputing the missing elements
    of variables in with and randomly take a certain proportion of elements as pseudo
    labeled data with the corresponding elements in M changed to 1. The pseudo labeled
    data matrix is , the corresponding mask matrix is ; Step 5: Fix G and optimize
    D using Adam algorithm. Before the optimization, the imputed data matrix can be
    obtained with G. The hint matrix can be obtained with the hint generator. The
    discriminant matrix can be obtained with the D to be optimized. The calculation
    of the loss function is shown in Eq. (16); Step 6: Fix D and optimize G using
    Adam algorithm. Before the optimization, the discriminant matrix is obtained with
    D. The imputed matrix is obtained with the to-be-optimized G. The loss function
    is calculated as is shown in Eqs. (17)–(19). Then return to step 2 and proceed
    in sequence until the iteration number has reached a setting number, which is
    600 in our experiments. After the federated transfer learning process, the target
    edge gets its updated edge model including the edge generator G and discriminator
    D. Put and into G and we can get , the completed data. The high-quality complete
    data can thus be used in the following downstream applications such as soft sensing
    or condition monitoring. Compared with original local model, the updated edge
    model would be improved with knowledge focusing on from other edges. The quantified
    reliability index for helper models helps to fit data distribution in the target
    edge in the transfer learning process especially when data are non-i.i.d. Compared
    with the traditional FedAvg approach, the federated transfer approach proposed
    has a much lower communication cost between edges and the central server. Suppose
    that there are 10 edges and sending a model from one edge to the server has communication
    cost c, vice versa. The communication cost for 1000 optimizing iterations of the
    proposed transfer learning method on one edge would be 20c, while the cost of
    FedAvg would be 20,000c. Computation for model optimization is mainly performed
    on the edge rather than on the server, reducing the communication burden. 4. Experiments
    In this section, the proposed FedTMI is evaluated on the real-world data sets
    in thermal power plants. The first subsection is about the data sets preparation.
    The second subsection introduces algorithm implementation details including model
    settings and hyper-parameters. The last two subsections present the performance
    of FedTMI transfer learning on data sets under two typical scenarios. FedTMI is
    compared with baseline methods including local GAIN, centralized GAIN and FedAvg
    GAIN on the target edge data set separately. Local GAIN means that models are
    trained only on the target edge without communicating with the cloud. Centralized
    GAIN means that models are trained in the cloud supposing that all data sets are
    accessible. FedAvg GAIN means that models are trained with the traditional FedAvg
    approach. The two typical federated problem scenarios are as follows. The first
    scenario is that we simulate three data sets with one device. The simulated data
    sets thus have similar data distribution. Imputation results under this scenario
    could show model performance with independent and identically distributed (i.i.d.)
    data in different edges. The second scenario is that there are three data sets
    from three different devices whose data distribution may vary dramatically. Imputation
    results under this scenario could show model performance with non-i.i.d. data
    in different edges. 4.1. Data sets preparation The experimental data are collected
    from steam-driven water pumps in large-scale generator sets from different power
    plants. Large-scale generator sets (units above 300MW) generally use one electric
    feed-water pump and two steam-driven feed-water pumps. The electric feed-water
    pump is used for startup and shutdown and emergency backup. The adjustment of
    the electric feed-water pump mainly depends on the adjustment of the scoop tube
    of the hydraulic coupler. In our experiments, we simulated the federated learning
    problem scenario based on the collected data. We select data sets of pump A, B,
    C and D as our training and testing data sets. Detailed information of the 37
    variables in these data sets is shown in Table 1. Statistics of data sets of A,
    B, C and D is shown in Table 2. It is noted that data are missing at random. The
    basic missing rate is set to be 0.05. The commonly used model error evaluation
    index determination coefficient and root mean square error are taken as the evaluation
    criteria calculated as below. in the formula means the true value of the target
    parameter of sample . is the average value of the target parameter. is the estimated
    result of . (20) (21) Table 1. Variables in the electric feed-water pump data
    sets. Variable type Number of variables Description Valve Control Command (%)
    1 The control signal Valve position (%) 2 Valve positions of two inlet control
    valves Pressure (Mpa or kPa) 7 pump inlet pressure, outlet pressure, steam pressure,
    exhaust pressure, lubricating oil pressure, etc. Temperature (°C) 15 Water temperature,
    steam temperature, oil temperature, bearing temperature, etc. Vibration (um) 6
    Vibration signal on pump measuring points and bearing measuring points Displacement
    (um) 2 Axial displacement on measuring points Others 4 Machine speed (r/min),
    outlet flow (t/h), power (MV), low pressure regulating valve oil motor stroke
    (%) Table 2. Missing statistics of four data sets (pump A, B, C, D). Pump ID Sample
    number Variable number Basic missing rate A 26 350 37 0.05 B 26 225 37 0.05 C
    22 030 37 0.05 D 26 115 37 0.05 4.2. Model settings of FedTMI In all of our experiments,
    the GAIN model structure and the hyper-parameters are shown in Table 3, Table
    4 respectively. The local GAIN, centralized GAIN and FedAvg GAIN that are used
    to compare with FedTMI have the same base model structure. FCN means the full
    connected network, ReLU is the activation function Rectified Linear Unit, ELU
    is the exponential linear unit, Sigmoid is the logistic sigmoid function. Our
    neural networks’ parameter initialization strategy is Xavier normal in Pytorch.
    Table 3. Model architecture of generators and discriminators. Generator network
    Discriminator network FCN(input_dim, h_dim) FCN(2*data_size, h_dim) ReLU() ReLU()
    FCN (h_dim, h_dim) FCN (h_dim, h_dim) ReLU() ReLU() FCN (h_dim, data_size) FCN
    (h_dim, data_size) ELU() Sigmoid() Table 4. The Hyper-parameter configuration
    of local/centralized GAIN, FedAvg and FedTMI. Hyper-parameter Local/centralized
    GAIN FedAvg FedTMI Local initialization epochs 5000 5000 5000 Federated transfer
    update epochs – – 600 Federated centralized update epochs – 600 – Local update
    epochs – 10 – Adam step size Noise input dim 37 37 37 Hidden layer dimension 37
    37 37 Mini-batch size 500 500 500 10 10 10 Hint rate 0.9 0.9 0.9 4.3. Federated
    transfer learning with i.i.d. data The first federated problem scenario is that
    we simulate three i.i.d. data sets with one pump (A). Data sets are divided by
    ratio 2:1:1 and the corresponding edge number are set to be 1, 2 and 3 separately.
    Edge 4 has the same original data with edge 3. The missing rates of variable 31
    and variable 32 is set to be 0.6 on edge 1. The missing rates of variable 30 is
    set to be 0.9 on edge 4. In this scenario, we examine the feasibility of FedTMI
    on edge 1. According to the knowledge aided helper model selection scheme, for
    edge 1, we get , , in step 1. In step 2, with , we get , and . It is obvious that
    . In step 3, no edge ID is removed from since they have relatively adequate data.
    Edge models from edge 2 and edge 3 are selected as helper models of edge 1. Table
    5 shows the results of model performance with the local GAIN approach on their
    local edges and model performance with FedTMI on edge 1 compared with the centralized
    GAIN and FedAvg GAIN approach in the first scenario and the second scenario. Fig.
    6 shows the imputation loss on missing elements of in training iterations with
    FedTMI and Fig. 7 is visualization of pseudo labeled samples in the two scenarios
    after dimension reduction. Results for Scenario 1 in Table 5 show that model with
    the proposed FedTMI approach performs better in data imputation of variable 31
    and slightly worse in data imputation of variable 32 compared with centralized
    GAIN and local GAIN. Since data sets on the three edges are similar ones from
    one pump, parameters and the optimization directions in the three edge models
    should be similar as well. However, model with the FedAvg approach performs badly
    in data imputation on variable 31 with an 83.41% reduced for imputation performance
    on missing elements compared with local GAIN. This shows that although generators
    (Gs) in three edges learn similar data distribution, aggregation of model parameters
    can still result in model mismatch, so the imputation performance of FedTMI is
    much better than that of FedAvg GAIN. The updated edge model with FedTMI fits
    its local data set and learns data distribution from non-target edges with as
    the quantitative index indicating helper model reliability for a particular variable.
    In this scenario, for helper model , and . For helper model , and . Fig. 7(a)
    and (b) show the visualization of pseudo labeled samples in several iterations
    in the first scenario after dimension reduction. Since data sets on three edges
    in the first scenario have a similar data distribution, and are both reliable
    for labeling missing elements in in scenario 1 with i.i.d. data. Fig. 6(a) and
    (b) show the change of imputation loss in the FedTMI training process in scenario
    1. Since local GAIN in scenario 1 has achieved relatively good imputation performance
    with at least 24.90% higher than that of local GAIN in scenario 2 on missing data
    imputation of , improvement of model performance in the federated transfer learning
    process is not significant. It suggests that although the reliability indexes
    of helper models are high, and have little influence in improving the imputation
    performance when local GAIN is good enough. Table 5. Comparison of data imputation
    performance for missing elements in and average performance on all variables in
    two scenarios. Methods local GAIN local GAIN local GAIN FedAvg GAIN Centralized
    GAIN FedTMI local GAIN local GAIN local GAIN FedAvg GAIN Centralized GAIN FedTMI
    Edge ID 1 2 3 1 1 1 1 2 3 1 1 1 EC S1 31 NM 0.8404 0.9319 0.9076 0.2038 0.8528
    0.8586 0.0475 0.0330 0.0353 0.1060 0.0456 0.0447 32 NM 0.8608 0.8961 0.8981 0.8003
    0.8751 0.8690 0.0430 0.0415 0.0380 0.0516 0.0408 0.0418 31 M 0.7971 0.8309 0.8090
    0.1322 0.7986 0.8067 0.0520 0.0472 0.0547 0.1075 0.0518 0.0507 32 M 0.8557 0.8632
    0.8564 0.8194 0.8570 0.8545 0.0460 0.0483 0.0506 0.0515 0.0458 0.0488 Avg NM 0.9829
    0.9832 0.9813 0.0000 0.9821 0.9884 0.0347 0.0363 0.0377 0.3103 0.0355 0.0325 Avg
    M 0.9782 0.9736 0.9712 0.0000 0.9778 0.9803 0.0481 0.0452 0.0467 0.3099 0.0485
    0.0449 S2 31 NM 0.7569 0.8784 0.8486 0.1621 0.6911 0.7739 0.0416 0.0392 0.0589
    0.0771 0.0468 0.0401 32 NM 0.7613 0.8746 0.8671 0.2504 0.6851 0.7935 0.0387 0.0376
    0.0609 0.0686 0.0443 0.0360 31 M 0.6529 0.7303 0.5226 0.1528 0.5108 0.6666 0.0476
    0.0547 0.0744 0.1024 0.0566 0.0467 32 M 0.6387 0.6108 0.6705 0.1462 0.4957 0.6653
    0.0452 0.0595 0.0695 0.0963 0.0535 0.0436 Avg NM 0.9836 0.9792 0.9320 0.0000 0.9753
    0.9865 0.0338 0.0416 0.0588 0.2373 0.0415 0.0312 Avg M 0.9565 0.9510 0.8782 0.0000
    0.9356 0.9580 0.0522 0.0634 0.0783 0.2400 0.0632 0.0519 “EC” standards for evaluation
    criterion. “S1” means scenario 1 with i.i.d. data. “S2” means scenario 2 with
    non-i.i.d. data. “31” is the index number of the variable for rear bearing vibration.
    “32” is the index number of the variable for front bearing vibration. “NM” means
    that the results show estimation performance on non-missing elements. “M” means
    that the results show imputation performance on missing elements. “Avg” means
    that the average results for all variables. Download : Download high-res image
    (1MB) Download : Download full-size image Fig. 6. Imputation loss on missing elements
    of in training iterations with FedTMI. Download : Download high-res image (865KB)
    Download : Download full-size image Fig. 7. Visualization of pseudo labeled samples
    in the two scenarios after dimension reduction. 4.4. Federated transfer learning
    with non-i.i.d. data The second federated problem scenario is that there are three
    data sets from three different pumps (B, C and D) whose data are non-i.i.d. The
    corresponding edge number are set to be 1, 2 and 3 separately. Edge 4 has the
    same original data with edge 3. Similar to the first scenario, the missing rates
    of variable 31 and variable 32 is set to be 0.6 on edge 1. The missing rates of
    variable 30 is set to be 0.9 on edge 4. In this scenario, we examine the feasibility
    of FedTMI on edge 1 as is in Section 4.3. According to the knowledge aided helper
    model selection scheme, for edge 1, we get , , in step 1. In step 2, with , we
    get , and . It is obvious that . In step 3, no edge ID is removed from . Edge
    models from edge 2 and edge 3 are selected as helper models of edge 1. Results
    for scenario 2 in Table 5 show that the FedTMI model performs better than comparison
    models with at least 2.10% higher on missing data imputation of . It is obvious
    that the FedAvg GAIN has a deteriorated performance compared with the local GAIN
    with at least 76.60% lower for missing data imputation in . Centralized GAIN fails
    to surpass FedTMI in this non-i.i.d. setting too with its imputation performance
    worse than that of local GAIN. Since data sets on the three edges are all from
    pumps in different factories, parameters and the optimization directions in the
    three local models varies. The FedAvg approach simply computes the average of
    the model parameters without considering that the aggregated model may not fit
    each edge. Data distribution learned by centralized GAIN with all data sets does
    not fit the specific distribution in the target edge as well. FedTMI conquers
    this problem by passing helper model knowledge with pseudo labels of variables
    in . Fig. 6(c) and (d) show the change of imputation loss in the FedTMI training
    process in scenario 2. It can be seen that the model is optimized with the imputation
    performance of improved in the transfer learning process. Compared with imputation
    results in scenario 1, we can conclude that when local GAIN has not achieved good
    imputation performance, the role of helper models would be more significant in
    the transfer learning process. For helper model , and . For helper model , and
    . is higher when is chosen as the helper model compared with . Fig. 7(c) and (d)
    show the visualization of pseudo labeled samples in several iterations in the
    second scenario after dimension reduction. Apparently, there are more pseudo labels
    generated by , which means that more knowledge about data distribution on edge
    2 is transferred to edge 1 in scenario 2 with non-i.i.d. data. 5. Conclusion This
    paper presents FedTMI, a knowledge aided federated transfer learning approach
    for industrial missing data imputation concerning data privacy. In the proposed
    method, data non-i.i.d. problem is dealt with in the federated transfer data imputation
    scheme. Pseudo labels for variables with high missing rates in the target edge
    are generated with selected helper models and the number of pseudo labels is controlled
    with quantified reliability index. Thus, model in the target edge is improved
    with knowledge from other edges and still fits its own data distribution. Furthermore,
    the proposed knowledge aided federated transfer learning approach is able to transfer
    knowledge between edges efficiently without heavy communication. Experiments on
    the real-world data sets in thermal power plants show that FedTMI outperforms
    local GAIN, centralized GAIN and FedAvg GAIN especially in cases where there are
    non-i.i.d. data, its imputation performance improved by at least 2.10%. While
    this knowledge aided federated transfer learning approach is effective for the
    selected target edge, models of source edges are not improved in this process.
    Only relationship between the target edge and its helper models is described with
    the quantified reliability index. Future work could model the complex edge relationship
    under the federated learning framework and train personalized models accordingly.
    CRediT authorship contribution statement Zoujing Yao: Conceptualization, Methodology,
    Software, Validation, Writing – original draft. Chunhui Zhao: Conceptualization,
    Methodology, Funding acquisition, Project administration, Supervision, Writing
    – review & editing. Declaration of Competing Interest The authors declare that
    they have no known competing financial interests or personal relationships that
    could have appeared to influence the work reported in this paper. Acknowledgments
    The authors gratefully acknowledge the support from the following foundations:
    the National Natural Science Foundation of China (No. 62125306, No. 62133003),
    the Fundamental Research Funds for the Central Universities (Zhejiang University
    NGICS Platform), and the Research Project of the State Key Laboratory of Industrial
    Control Technology, Zhejiang University, China (ICT2021A15). References [1] Ehrlinger
    L., Grubinger T., Varga B., Pichler M., Natschläger T., Zeindl J. Treating missing
    data in industrial data analytics 2018 Thirteenth International Conference on
    Digital Information Management (ICDIM), IEEE (2018), pp. 148-155 CrossRefView
    in ScopusGoogle Scholar [2] Josse J., Pagès J., Husson F. Multiple imputation
    in principal component analysis Adv. Data Anal. Classif., 5 (3) (2011), pp. 231-246
    CrossRefView in ScopusGoogle Scholar [3] Zheng J., Zhao C., Gao F. Retrospective
    comparison of several typical linear dynamic latent variable models for industrial
    process monitoring Comput. Chem. Eng., 157 (2022), Article 107587 View PDFView
    articleView in ScopusGoogle Scholar [4] Dempster A.P., Laird N.M., Rubin D.B.
    Maximum likelihood from incomplete data via the em algorithm J. R. Stat. Soc.
    Ser. B Stat. Methodol., 39 (1) (1977), pp. 1-22 CrossRefGoogle Scholar [5] Stekhoven
    D.J., Bühlmann P. Missforest——non-parametricmissing value imputation for mixed-type
    data Bioinformatics, 28 (1) (2012), pp. 112-118 CrossRefView in ScopusGoogle Scholar
    [6] Nwulu N.I. Evaluation of machine learning classification algorithms & missing
    data imputation techniques 2017 International Artificial Intelligence and Data
    Processing Symposium (IDAP), IEEE (2017), pp. 1-5 Google Scholar [7] Audigier
    V., Husson F., Josse J. Multiple imputation for continuous variables using a bayesian
    principal component analysis J. Stat. Comput. Simul., 86 (11) (2016), pp. 2140-2156
    CrossRefView in ScopusGoogle Scholar [8] Qu L., Li L., Zhang Y., Hu J. Ppca-based
    missing data imputation for traffic flow volume: A systematical approach IEEE
    Trans. Intell. Transp. Syst., 10 (3) (2009), pp. 512-522 View in ScopusGoogle
    Scholar [9] Marchang N., Tripathi R. Knn-st: Exploiting spatio-temporal correlation
    for missing data inference in environmental crowd sensing IEEE Sens. J., 21 (3)
    (2020), pp. 3429-3436 Google Scholar [10] Tran C.T., Zhang M., Andreae P., Xue
    B., Bui L.T. An effective and efficient approach to classification with incomplete
    data Knowl.-Based Syst., 154 (2018), pp. 1-16 View PDFView articleView in ScopusGoogle
    Scholar [11] Yu Z., Li T., Horng S.-J., Pan Y., Wang H., Jing Y. An iterative
    locally auto-weighted least squares method for microarray missing value estimation
    IEEE Trans. Nanobiosci., 16 (1) (2016), pp. 21-33 CrossRefGoogle Scholar [12]
    Yu W., Zhao C. Concurrent analytics of temporal information and local correlation
    for meticulous quality prediction of industrial processes J. Process Control,
    107 (2021), pp. 47-57 View PDFView articleView in ScopusGoogle Scholar [13] Yoon
    J., Zame W.R., van der Schaar M. Estimating missing data in temporal data streams
    using multi-directional recurrent neural networks IEEE Trans. Biomed. Eng., 66
    (5) (2018), pp. 1477-1490 Google Scholar [14] Feng L., Zhao C., Sun Y. Dual attention-based
    encoder–decoder: A customized sequence-to-sequence learning for soft sensor development
    IEEE Trans. Neural Netw. Learn. Syst., 32 (8) (2020), pp. 3306-3317 Google Scholar
    [15] McCoy J.T., Kroon S., Auret L. Variational autoencoders for missing data
    imputation with application to a simulated milling circuit IFAC-PapersOnLine,
    51 (21) (2018), pp. 141-146 View PDFView articleView in ScopusGoogle Scholar [16]
    Boquet G., Morell A., Serrano J., Vicario J.L. A variational autoencoder solution
    for road traffic forecasting systems: Missing data imputation, dimension reduction,
    model selection and anomaly detection Transp. Res. C, 115 (2020), Article 102622
    View PDFView articleView in ScopusGoogle Scholar [17] Chai Z., Zhao C., Huang
    B. Variational progressive-transfer network for soft sensing of multirate industrial
    processes IEEE Trans. Cybern. (2021), 10.1109/TCYB.2021.3090996 Google Scholar
    [18] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and composing
    robust features with denoising autoencoders, in: Proceedings of the 25th international
    conference on Machine learning, 2008, pp. 1096–1103. Google Scholar [19] Abiri
    N., Linse B., Edén P., Ohlsson M. Establishing strong imputation performance of
    a denoising autoencoder in a wide range of missing data problems Neurocomputing,
    365 (2019), pp. 137-146 View PDFView articleView in ScopusGoogle Scholar [20]
    Yu W., Zhao C. Robust monitoring and fault isolation of nonlinear industrial processes
    using denoising autoencoder and elastic net IEEE Trans. Control Syst. Technol.,
    28 (3) (2020), pp. 1083-1091 CrossRefView in ScopusGoogle Scholar [21] Gondara
    L., Wang K. Multiple imputation using deep denoising autoencoders (2017) arXiv
    preprint arXiv:1705.02737 Google Scholar [22] Chai Z., Zhao C., Huang B., Chen
    H. A deep probabilistic transfer learning framework for soft sensor modeling with
    missing data IEEE Trans. Neural Netw. Learn. Syst. (2021), pp. 1-12 Google Scholar
    [23] Yoon J., Jordon J., Schaar M. Gain: Missing data imputation using generative
    adversarial nets International Conference on Machine Learning, PMLR (2018), pp.
    5689-5698 View in ScopusGoogle Scholar [24] Luo Y., Cai X., Zhang Y., Xu J., et
    al. Multivariate time series imputation with generative adversarial networks Advances
    in Neural Information Processing Systems (2018), pp. 1596-1607 View in ScopusGoogle
    Scholar [25] S. Yoon, S. Sull, Gamin: generative adversarial multiple imputation
    network for highly missing data, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2020, pp. 8456–8464. Google Scholar [26]
    Kairouz P., McMahan H.B., Avent B., Bellet A., Bennis M., Bhagoji A.N., Bonawitz
    K., Charles Z., Cormode G., Cummings R., et al. Advances and open problems in
    federated learning (2019) arXiv preprint arXiv:1912.04977 Google Scholar [27]
    Zhou X., Liu X., Lan G., Wu J. Federated conditional generative adversarial nets
    imputation method for air quality missing data Knowl.-Based Syst., 228 (2021),
    Article 107261 View PDFView articleView in ScopusGoogle Scholar [28] McMahan B.,
    Moore E., Ramage D., Hampson S., y Arcas B.A. Communication-efficient learning
    of deep networks from decentralized data Artificial Intelligence and Statistics,
    PMLR (2017), pp. 1273-1282 View in ScopusGoogle Scholar [29] Sattler F., Wiedemann
    S., Müller K.-R., Samek W. Robust and communication-efficient federated learning
    from non-iid data IEEE Trans. Neural Netw. Learn. Syst., 31 (9) (2019), pp. 3400-3413
    Google Scholar [30] Li T., Sahu A.K., Zaheer M., Sanjabi M., Talwalkar A., Smith
    V. Federated optimization in heterogeneous networks Proc. Mach. Learn. Syst.,
    2 (2020), pp. 429-450 Google Scholar [31] Wang J., Liu Q., Liang H., Joshi G.,
    Poor H.V. Tackling the objective inconsistency problem in heterogeneous federated
    optimization (2020) arXiv preprint arXiv:2007.07481 Google Scholar [32] Deng Y.,
    Kamani M.M., Mahdavi M. Distributionally robust federated averaging (2021) arXiv
    preprint arXiv:2102.12660 Google Scholar [33] Karimireddy S.P., Kale S., Mohri
    M., Reddi S., Stich S., Suresh A.T. Scaffold: Stochastic controlled averaging
    for federated learning International Conference on Machine Learning, PMLR (2020),
    pp. 5132-5143 Google Scholar [34] Zhao C. Perspectives on nonstationary process
    monitoring in the era of industrial artificial intelligence J. Process Control,
    116 (2022), pp. 255-272 View PDFView articleView in ScopusGoogle Scholar [35]
    Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville
    A., Bengio Y. Generative adversarial nets Adv. Neural Inf. Process. Syst., 27
    (2014), pp. 2672-2680 Google Scholar [36] Liu X., Wang X., Zou L., Xia J., Pang
    W. Spatial imputation for air pollutants data sets via low rank matrix completion
    algorithm Environ. Int., 139 (2020), Article 105713 View PDFView articleView in
    ScopusGoogle Scholar Cited by (9) Fusing consensus knowledge: A federated learning
    method for fault diagnosis via privacy-preserving reference under domain shift
    2024, Information Fusion Show abstract Facing spatiotemporal heterogeneity: A
    unified federated continual learning framework with self-challenge rehearsal for
    industrial monitoring tasks 2024, Knowledge-Based Systems Show abstract Federated
    knowledge amalgamation with unbiased semantic attributes under cloud–edge collaboration
    for heterogeneous fault diagnosis 2023, Journal of Process Control Show abstract
    Finding trustworthy neighbors: Graph aided federated learning for few-shot industrial
    fault diagnosis with data heterogeneity 2023, Journal of Process Control Show
    abstract Fed-MIWAE: Federated Imputation of Incomplete Data Via Deep Generative
    Models 2024, SSRN Fed-MIWAE: Federated Imputation of Incomplete Data via Deep
    Generative Models 2023, arXiv View all citing articles on Scopus View Abstract
    © 2022 Elsevier Ltd. All rights reserved. Recommended articles High-dimensional,
    slow-time-varying process monitoring technique based on adaptive eigen subspace
    extraction method Journal of Process Control, Volume 117, 2022, pp. 122-131 Xiaowei
    Feng, …, Jiayu Luo View PDF A supervised multisegment probability density analysis
    method for incipient fault detection of quality indicator Journal of Process Control,
    Volume 116, 2022, pp. 53-63 Yang Tao, …, Shuai Tan View PDF Active nonstationary
    variables selection based just-in-time co-integration analysis and slow feature
    analysis monitoring approach for dynamic processes Journal of Process Control,
    Volume 117, 2022, pp. 112-121 Jian Huang, …, Yuri A.W. Shardt View PDF Show 3
    more articles Article Metrics Citations Citation Indexes: 4 Captures Readers:
    10 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Journal of Process Control
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'FedTMI: Knowledge aided federated transfer learning for industrial missing
    data imputation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wu H.
  - Liu Q.
  - Liu X.
  - Zhang Y.
  - Yang Z.
  citation_count: '8'
  description: Internet of Things (IoT) has been rapidly developed in recent years,
    being well applied in the fields of Environmental Surveillance, Smart Grid, Intelligent
    Transportation, and so on. As one of the typical earth-based meteorological observation
    methods, networked Doppler weather radars, i.e. the Internet of weather Radars
    (IoR) can detect the signals of large-area water particles in the atmosphere with
    high resolution, but suffer from beam blockage due to surrounded mountains, buildings,
    as well as other obstacles. In addition, how to establish a distributed platform
    for large-scale radar data analytics becomes critical and challenging, especially
    considering optimised strategies on the storage, processing and exchange of radar
    raw data, beam/echo signal, and final products etc. In this paper, an edge-assisted
    cloud framework is proposed to facilitate effective and proficient communication
    and progression, where echo signal from a single site radar can be analysed and
    pre-processed at the edge, and then trained in the cloud with elastic resources
    and distributed learning ability. A Residual Concatenate Fully Convolutional Network
    (RC-FCN) is presented for beam blockage correction, which is integrated into the
    framework to be compared with other deep learning models, including FCN, ResNet,
    VGG, etc. According to experiment results, better performance and efficiency have
    been achieved using the proposed framework and its fitted RC-FCN model.
  doi: 10.1007/s11280-021-00988-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home World Wide Web Article An edge-assisted
    cloud framework using a residual concatenate FCN approach to beam correction in
    the internet of weather radars Published: 21 January 2022 Volume 25, pages 1923–1949,
    (2022) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    World Wide Web Aims and scope Submit manuscript Hao Wu, Qi Liu , Xiaodong Liu,
    Yonghong Zhang & Zhiyun Yang  395 Accesses 9 Citations 1 Altmetric Explore all
    metrics Abstract Internet of Things (IoT) has been rapidly developed in recent
    years, being well applied in the fields of Environmental Surveillance, Smart Grid,
    Intelligent Transportation, and so on. As one of the typical earth-based meteorological
    observation methods, networked Doppler weather radars, i.e. the Internet of weather
    Radars (IoR) can detect the signals of large-area water particles in the atmosphere
    with high resolution, but suffer from beam blockage due to surrounded mountains,
    buildings, as well as other obstacles. In addition, how to establish a distributed
    platform for large-scale radar data analytics becomes critical and challenging,
    especially considering optimised strategies on the storage, processing and exchange
    of radar raw data, beam/echo signal, and final products etc. In this paper, an
    edge-assisted cloud framework is proposed to facilitate effective and proficient
    communication and progression, where echo signal from a single site radar can
    be analysed and pre-processed at the edge, and then trained in the cloud with
    elastic resources and distributed learning ability. A Residual Concatenate Fully
    Convolutional Network (RC-FCN) is presented for beam blockage correction, which
    is integrated into the framework to be compared with other deep learning models,
    including FCN, ResNet, VGG, etc. According to experiment results, better performance
    and efficiency have been achieved using the proposed framework and its fitted
    RC-FCN model. Similar content being viewed by others DenMerD: a feature enhanced
    approach to radar beam blockage correction with edge-cloud computing Article Open
    access 05 February 2024 LAPRNet: Lightweight Airborne Particle Removal Network
    for LiDAR Point Clouds Chapter © 2024 Ground radar precipitation estimation with
    deep learning approaches in meteorological private cloud Article Open access 15
    April 2020 1 Introduction Weather radar is an important equipment of weather observation
    in meteorological department. In order to make the observation more accurate and
    timely, the deployment of weather radar is very intensive, especially in coastal
    areas. The weather radar outputs data every six minutes generally. The amount
    of data generated by weather radar network is very huge. Therefore, it is an important
    and difficult problem to save massive radar data orderly. At the same time, the
    data detected by different radar stations are copyrighted. In the process of transmission
    and storage, it is necessary to ensure the security and privacy of radar data.
    On the other hand, there are many quality problems in the direct output data of
    weather radar. When directly applied to the quantitative analysis method of meteorological
    operation, it is easy to cause obvious errors. Common quality problems include
    non precipitation target blocking, ground clutter and signal attenuation. Therefore,
    before the data are put into use, the quality control operations such as blocking
    correction, clutter removal and attenuation correction are needed. In this paper,
    an edge-assisted cloud framework based Residual Concatenate Fully Convolutional
    Neural Network (RC-FCN) is proposed, which can achieve the correction of beam
    blockage independent of terrain data and ensure the access performance of radar
    data. Blocking correction is regarded as an image in-painting problem in this
    study. The system adopts the combination of edge and cloud computing. The edge
    part is each radar station which is mainly responsible for detection, data storage
    and preprocessing. The cloud computing part is high performance server hardware
    system optimized for artificial intelligence and remote access software system.
    In this way, the whole system can harmonize the contradiction of weather radar
    AI system in data storage, data transmission, and efficient invoking of AI algorithms.
    In this study, the problem of weather radar beam blockages is mainly solved. The
    blocking part of radar image is just similar to the missing data of image. RC-FCN
    is a multilayer neural network with image restoration function. It consists of
    an convolutional encoder and a deconvolutional decoder network. During training,
    the input of network is the blockages contained weather radar images. The output
    of the network is the repaired image corresponding to the blocking positions.
    The label is the real image corresponding to the blocking positions. After training
    with large amount data finished, the network can automatically fill in the missing
    data of the blocking part according to the context information of the radar image.
    In fact, the training process of the network is to find the relationship of data
    distributions between the blocked and normal part in weather radar images. Experiment
    shows that the method in this paper is superior to the traditions in critical
    success index (CSI), false alarm rate (FAR) and probability of detection (POD)
    [26]. Due to the different models and principles of radar equipment, the traditional
    methods need to adjust the algorithm parameters according to the actual situation
    to adapt different types of radar-based data. The method proposed in this paper
    is based on radar image. Radar image is the advanced product of radar system,
    which has strong generality. The training of this method is based on radar image.
    Therefore, this can be applied to almost all weather radar systems without parameter
    adjustment of specific equipment. This method is universal and convenient. 2 Related
    work 2.1 Edge and cloud combined computing Compared with cloud computing, edge
    computing has its unique advantages. For example, it can achieve faster network
    service response to meet the basic needs of the industry in real-time business
    [1, 12, 46], application intelligence [38], security and privacy protection [42].
    Edge computing technologies have been widely used in the field of Internet of
    Things (IoTs) [8, 23, 30, 35], Internet of Vehicles (IoVs) [19, 40, 41], smart
    city [31] and so on. Atmospheric detection and weather monitoring are special
    IoTs application scenarios [18]. The network of Radars (IoRs) is also a special
    type of IoTs. Because the scanning range of a weather radar is limited, usually
    covering a radius of about one hundred to four hundred kilometres. A network composed
    of multiple radars is needed to completely cover a certain area. Each radar generates
    several GB of data every day. How to process so much data become a challenging
    problem. The radar node for lightweight data processing and the comprehensive
    analysis processed by cloud high-performance computer will be a better solution.
    At present, there are many computing systems that combine the advantages of edge
    computing and cloud computing. Zamora-Izquierdo [45] proposed an exchangeable
    low-cost hardware based and multi software platform supported edge computing platform
    for Precision Agriculture management. The local cyber-physical systems can gather
    information and execute control actions. The cloud platform can collect information
    for analysis. Wu [36] proposed a distributed deep learning-driven task offloading
    algorithm for mobile devices, edge cloud server, and central cloud server. The
    algorithm can give the optimised decisions in the mixture edge and cloud computing
    environments. Edge computing and cloud computing are not opposite, but more often
    to make up for their shortcomings. Duc [5] surveys problems in joint edge-cloud
    environments. The research shows that machine learning technologies can solve
    the scheduling and allocation of computing resources in the environment. Miao
    [21] also uses LSTM model to predict the task offloading and migration in mobile-edge
    cloud computing and reduce the task delay. Network security and efficiency are
    also important features of cloud computing and edge computing. Wang [34] proposed
    a trust evaluation mechanism and a service parameter template combined architecture
    that improve the security and efficiency of IoT-Cloud systems. 2.2 Typical beam
    blockage correction methods Due to the obstruction of tall buildings, trees and
    other obstacles near the radar, the missing or deviation of echo data detected
    by the radar is common. Even the slightly block, the electromagnetic wave emitted
    by the radar can not propagate forward completely, which make the echo be weak
    (partial block) or disappears (complete block). In order to reduce the influence
    of beam blocking on radar data, most research methods are based on digital elevation
    model (DEM). Wang [9]takes Quzhou radar in Zhejiang, China as an example to study
    the reliability of using ASTER GDEM V2 and SRTM3 V4 data, to simulate the radar
    beam blockages. However, DEM data is not enough to completely determine the areas
    which are blocked for its low update frequency. It is difficult to get the latest
    DEM data in time. Therefore, many researchers proposed other methods to detect
    the blockages. McRoberts [20] proposed a new spatial analysis technique to objectively
    identify the area of precipitation estimation affected by beam blockage. Li [14]
    proposed a method based on spatial distribution statistics and fuzzy logic, which
    can detect clutter and beam blocking on the ground independent of DEM data. However,
    this method only deleted the target object, and did not correct the beam blockages
    and clutter. 2.3 Relevant image inpainting methods The block correction problem
    can be considered as an image inpainting problem. The parts of blocked can be
    regarded as the missing part of images. In recent years, deep learning has achieved
    very obvious step forward in the field of image in-painting [4, 13, 33]. Cai [2]
    proposes a GAN based model, which can generate multiple possibilities for image
    in-painting task. Liu [16] proposed to use partial convolution to solve the problem
    of color difference and blurriness in image restoration. Yu [44] proposed a method
    which can compose the new image structure, and use the surrounding image features
    to help the process of network training. Song [28] proposes a method, which divides
    the image restoration task into inference and translation, and uses deep learning
    network to model each step. In order to make the repaired region image smoother
    and clearer, Nazeri [22] proposed a new image restoration algorithm, which can
    repair the missing region more precisely. In recent years, due to the excellent
    performance of deep learning and the AI computing capacity increase [24], the
    field of meteorological research gradually began to use this technology and other
    related methods to solve related problems, such as precipitation prediction [32],
    analysis and prediction of ENSO phenomenon [6], landslide early warning [15].
    They all achieved good results. Shi [25, 39] proposed a new method of combining
    convolution with LSTM network and a Trajectory GRU model to achieve the prediction
    of time series data of weather radar images which has achieved good results. Because
    this kind of thinking is the first in the field of meteorology, many researchers
    have carried out many follow-up studies based on this [11, 37, 43]. Besides the
    precipitation forecast, deep learning has also achieved good results in the field
    of ENSO analysis and prediction. The study of Ham [6] which utilized the convolution
    method not only improves the prediction skills of ENSO, but also fully proves
    the feasibility of the application of deep learning and other methods in the big
    data of Geoscience. In general, deep learning technology is developing rapidly
    in the field of image in-painting, and has made great progress in recent years.
    Therefore, by applying similar technologies to block correction, it has great
    potential and is expected to be a better correction results. However, due to the
    occlusion of objects, atmospheric refraction, equipment failure and other reasons,
    some of the radar data are missing or abnormal. In order to solve the problems,
    this study focuses on the correction of radar data missing due to various reasons.
    3 An edge-assisted cloud framework for internet of weather radars Figure 1 shows
    the structure of the Edge-Assisted Cloud Framework for Internet of Weather Radars
    proposed in this study. Due to the limited detection range of weather radar, it
    needs hundreds of weather radars to achieve the meteorological detection within
    the territory for a vast country or region. The detection network composed of
    hundreds weather radars is called radar network. If each radar is regarded as
    an Internet of Things device, such a network can also be regarded as an Internet
    of Radars, which is a special type Internet of Things that characterized by huge
    amount of big data. At the same time, because of the copyright of radar data,
    it needs a way to manage such a large amount of radar data in a centralized way
    and ensure that the data of each radar can not be stolen and can only be accessed
    by the authorized users. The cloud platform is mainly responsible for the training
    of network model. At the same time, it also manages the computing resources and
    provide secure remote call services. For example, encrypted interfaces, distributed
    maintenance of deep learning models, software runtime environment, etc. For privacy
    reasons, the cloud does not accept the original radar data from the edges. Only
    the preprocessed radar image data sets need to be uploaded to the cloud platform
    for AI model training. After the training, the models will be downloaded to each
    edge end for other services. Fig. 1 An edge-assisted cloud-assisted framework
    for Internet of Weather Radars Full size image 3.1 Distributed edge computing
    for radar data preprocessing and storage In this study, all the data detected
    by the radars are stored in their own station. The computing system fo each edge
    radar station is mainly responsible for data preprocessing, for example, data
    unification, data normalization, coordinate transform and sample expansion, etc.
    Besides, after the training of artificial intelligence model. In addition, the
    edge side can download the models from the cloud and run the model locally. In
    general, the edge side is mainly responsible for lightweight computing tasks.
    The framework mainly includes the following modules: Authority Management: This
    module is mainly used to verify whether the request sent by each user is legal,
    so as to prevent users or malicious attackers from abusing resources. Through
    authority management, the security of the server can be guaranteed to a certain
    extent. At the same time, the permission management can distinguish the use space
    of each user and prevent the training/testing tasks on the server from being too
    chaotic. Feature Extraction: This module provides various feature extraction methods
    for feature extraction. The interface needs to select the specified training data
    set and parameters for training. After feature learning, the interface generates
    the corresponding network model and stores it for the next calculation call. Training
    and Testing: This module contains training and testing processes. In the training
    process, users need to specify the feature extraction network and training data
    set, and different models can be trained to obtain different parameters. The test
    process needs to specify the detailed model and test data. All prediction results
    of the model will be stored in the system database. Users can query the detailed
    analysis results through task ID. Assisting Utilities: The module is used to manage
    training and test data, as well as test and training results. It mainly includes
    the following eight functions: upload and download of training and test data sets,
    download of test data results, query of existing data sets, query of network model
    training progress, query of training progress, query of trained network model,
    query of trained classifier knowledge and query of classifier test progress. 3.2
    Deep Learning models and high performance computing for remote invocation Fig.
    2 Cloud platform structure for Internet of Weather Radars Full size image Figure
    2 shows the cloud platform framework of this study. This framework uses restful
    style Java EE API and ICE (Internet Communications Engine) to achieve the process
    of remote sending and defining parameters to the deep learning model, invoking
    the specified Python deep learning program modules, obtaining the response results
    and other necessary functions. The framework is running on a high performance
    computing and storage system for the needs of giant GPU accelerated computing
    and big data storing ability. On this basis, the framework needs various software
    running environment, such as Java EE for network services, Python and various
    deep learning frameworks for deep learning algorithm, Hadoop and MySQL database
    system for distributed storage. The built-in weather data processing algorithms
    are running on the environment. The details of the algorithms are introduced in
    Section 4. In order to achieve the orderly access and privacy control of radar
    data, these algorithms are wrapped by the interfaces. Users can access these built-in
    algorithms through them with the help of SSL encrypted Internet of Weather Radars
    transmission network to manage and process radar data. The cloud platform mainly
    includes model training and testing process. In the training process, the user
    needs to specify the feature extraction network and training data set. Different
    network training results in different radar correction models. In the test process,
    the correction knowledge of the correction algorithm and the preprocessed data
    set are required to complete the correction of radar data. The detailed results
    will be stored in the system database, and users can query the detailed correction
    results through the IDs of results. The reason why the cloud can also invoke models
    going testing stage is that some edge equipment have no ability to run the models
    Fig. 3. 3.3 ICE-based distributed interfaces between edge and cloud ICE is similar
    to socket communication technology. It deals with all the underlying network interface
    programming, so that developers do not have to consider the details such as opening
    network connection, serialization and de-serialization of network data transmission,
    number of attempts of connection failure, etc. ICE describes the interface of
    a service through neutral language, which is independent of the specific programming
    language, SLICE (Specification Language for ICE), so as to separate object interface
    and its implementation. The client and server can use different programming languages
    and the communication is efficient and safe. This method can cross platform and
    ensure the security of user data. With the help of ICE, the results of ICE program
    can be better called directly by the programming language used on the client side,
    so that the programming style is consistent. In the AI computing environment,
    the security of the system is also very important [3]. All communication channels
    can be configured with various industrial level security protocols,such as AES
    and RSA, to ensure the communication security in this process. Fig. 3 Cloud platform
    structure for Internet of Weather Radars Full size image In this study, ICE framework
    technology is used to access restful style APIs, provide interfaces for users
    to access services, and ensure the security of user data while cross platform.
    Using ice framework, the results of ice program can be better called directly
    by the programming language used on the client side, so as to keep the programming
    style consistent. The communication process between the server and the client
    under the ice framework is shown in Figure 4. Fig. 4 Communication process between
    server and client in ICE framework Full size image 4 Detailed system design of
    edge-assisted computing in radar stations 4.1 Weather radar data preprocessing
    Deep learning relies heavily on big data and requires enough samples to train
    the network. However, there are various problems in the radar data observed naturally.
    For example, the sample distribution is uneven, and the image quality is poor.
    Before training the model, it is necessary to preprocess the original radar data
    which can help make the training successfully. Radar has a large amount of data
    because of its wide range, large quantity and high degree of automation. However,
    the radar data with blocking is relatively small, which is not enough to support
    the deep learning model network for training. Therefore, it is necessary to expand
    the radar data with blocking properly by some means so as to train the model.
    Before expanding the data, we need to find out the causes and characteristics
    of blocking. For the reflectivity image of Doppler weather radar, the radar is
    located in the centre of the image. As the radar rotates, it emits electromagnetic
    waves. Radar draws reflectivity image by receiving echo signal in a period. Figures
    5 and 6 shows two class weather radar reflectivity images which are from NUIST
    and Guangzhou weather radar stations. Different colours represent the reflected
    radar echo intensity of water vapour particles in the air. However, it can be
    observed that there are some significant lack of area which are circled with red
    and purple lines. These areas can be called beam blockages. The red line circle
    area has a small blocking range, which usually presents as a small range of echo
    data missing. The common causes for this phenomenon is that there are small objects
    in this direction, such as trees and buildings, which block the propagation of
    electromagnetic wave. The area circled by purple lines is a large area of signal
    loss. Most of the reason for this phenomenon is that there is a huge object block
    in the direction of electromagnetic wave emission, such as mountains, huge buildings.
    Fig. 5 Beam blockage samples of NUIST weather radar station. The red line circles
    the beam blockage caused by ground clutter and the blue line circled is ground
    blocking Full size image Fig. 6 Beam blockage samples of Guangzhou weather radar
    station. The red line circled the beam blockages are caused by ground clutter
    Full size image Besides the blockages of radial data, sometimes the radar signal
    will also appear some abnormalities caused by noise from ground. This will lead
    to redundant radial data in radar image. Figure 6 mainly shows a typical case
    of data redundancy.The areas circled by red lines are the abnormal parts of the
    data.This kind of signal is not necessary for weather radar, which is not necessary
    for the field of weather research. Therefore, for this kind of signal, it is also
    regarded as the lack of radar data, which is considered as a kind of beam blocking
    situation in this paper. To solve the above problems, the following Algorithm
    1 is adopted in this paper. It shows the main steps of data preprocessing. The
    input of the algorithm is origin weather radar observation images series, and
    the output is the transformed dataset, including manually masked inputs and ground
    truth labels. The \\(\\theta\\) in the algorithm means the rotation angle of radar
    scanning which is 360 as usual. The D means the distance of scanning distance.
    4.2 Data normalization Different colours and textures are usually used to represent
    the echo reflectivity with different intensities. This helps the human’s eyes
    to observe different intensity detail information. But for the deep learning model,
    the complex and unnecessary texture features will increase the training difficulty
    and convergence speed of the network, and eventually lead to the reduction of
    model output accuracy. On the other hand, different types of radars or different
    radar signal image processing programs use different colour codes when generating
    radar signal images. As we can see in the Figures 5 and 6, the white background
    radar images and the black background radar images are from the different radar.
    They used different standards to visualize radar signals. Signal intensities are
    represented by different colours according to its value. But what kind of colour
    represents what kind of intensity, these two radars adopt different schemes. In
    order to improve the generality of this research model, it is necessary to unify
    the radar intensity and colour representation under the same standard before the
    model training. On the other hand, radar echo reflectivity value is float data
    usually. When the radar image is generated, these floating-point numbers will
    approximate to the closest colour threshold range according to the colour standard,
    and use the colour value to represent the intensity value of this range in the
    image. The number of colour standard is about 15-20 in weather radar field. In
    this paper, the colours of echo intensities value from different radar will be
    transport into the same colour space. It means that all the pixels value of the
    radar images will be represent with the same standard. Pixel values will be evenly
    distributed between 0-255 according to the same standard in the form of gray images.
    In this way, the model can be used in the radar data of one station after being
    trained by the radar data of another one. Figures 7 and 8 show the normalization
    progresses of Guangzhou station and NUIST station. The general processing steps
    are similar. The first step is decoloring. In this step, colour images are transformed
    into gray scale images. The second step is normalization. In this step, the pixel
    values of gray scale images are redistributed according to one-to-one correspondence,
    evenly distributed between 0-16. In this study, echo data from two radar stations
    just have 16 intensity segments. So they can be represent by the values as (255,
    239, 223, 207, 191, 175, 159, 143, 127, 111, 95, 79, 63, 47, 31, 15). A large
    value indicates a high intensity of radar reflected signal. Finally, we use 1-16
    to index these regularized values as their classification numbers. Besides, the
    number 0 is used to represent the null area where the area with no radar echo.
    Fig. 7 Data normalization of NUIST weather radar station Full size image Fig.
    8 Data normalization of Guangzhou weather radar station Full size image The specific
    regularization rules of pixel colour values are shown in Tables 1 and 2. It should
    be noted that the intensity colour of radar image in Guangzhou station is represented
    by a kind of texture. Each intensity segment contains multiple colour values that
    do not repeat each other. Table 1 Colour Transform Rules of NUIST Station Radar
    Images Full size table Table 2 Colour Transform Rules of Guangzhou Station Radar
    Images Full size table 4.3 The transform from polar to Cartesian Coordinate System
    Deep neural network usually contains a large number of parameters, which will
    occupy a very large memory capacity. The more parameters, the slower the training
    speed of the network. Therefore, in the design of the network, it is important
    to save memory as much as possible. The input scale of the network has a great
    influence on the network parameters. The more dimension of input parameters, the
    more layers and parameters of network are needed to get good training effect.
    However, the valid data of the original radar image exists in the rectangular
    image as a circle. Effective information only takes up the part of the radar scanning
    circle in the images centre. So, it is necessary to drop the invalid data of the
    images. The drop method of the radar image redundant data is opposite to that
    of radar image construction. Generally, the weather radar takes the radar station
    as the centre, fixes a certain elevation angle, rotates while transmitting electromagnetic
    wave. In fact, the original radar data is the echo reflectivity intensity data
    obtained according to each rotation angle. The round visual radar image is processed
    based on the echo intensity data from different angles. Therefore, in this paper,
    the images are expanded from the centres which is opposite of the construction
    progress. The interval angle of expanding is 1 degree and the expanded radius
    is 250 pixels. Therefore, the final 500 \\(\\times\\) 500 radar image will be
    expanded into a 360 \\(\\times\\) 250 rectangular image. The x-axis of the original
    image is the east-west direction, and the y-axis is the north-south direction.
    The data in transformed image are only valid radar scanning data. The x-axis of
    the transformed image is the distance between the reflected object and the radar
    transmitting point, and the y-axis is the rotation angle of the radar. This process
    is equivalent to transforming radar image from polar coordinate system to Cartesian
    rectangular coordinate. The amount of data after redundancy removal is 36% of
    the original data. From this method, the valid data is retained, and the amount
    of data is greatly reduced. The Figures 9 and 10 shows the changes before and
    after data processing. On the other hand, it can be seen that the ground clutter
    and ground blocking areas have changed from fan-shaped areas to rectangular areas.
    This change will also help to blockage corrections in next steps. Fig. 9 Coordinate
    system transform of NUIST weather radar station Full size image Fig. 10 Coordinate
    system transform of Guangzhou weather radar station Full size image 4.4 Sample
    expansion method In the case of natural observation, the possibility of weather
    radar beam blockages is relatively small. The data of beam blockages is not enough
    to support the deep learning training for large amount of data demanding. Therefore,
    in this study, large-scale training data will be generated manually according
    to the block data of natural observation. There is a difference between the handcrafted
    data generated by program and the data observed by nature. However, the manual
    blockage areas are much larger and positions are more random than that of natural
    observation. As shown in the figures on the left of Figures 9 and 10, most of
    the obstructions are fan-shaped on the radar image in natural observation. After
    the above transformation, these fan-shaped blockages are almost perfectly transformed
    into rectangles in the Angle-Distance coordinate system. According to this phenomenon,
    the rectangle is used to cover the original data to simulate the beam blockage
    in natural observation. The positions and widths are generated randomly with uniform
    distribution. The starting indexes of block areas in y-axis direction are between
    [0, 360] and the heights are between [1, 18]. The two cases of manual blockages
    and original images are shown in Figure 11. The black rectangular areas circled
    in red are the artificial blocking areas. Fig. 11 Two manual block samples of
    Guangzhou Station Full size image 5 The model design in cloud computing platform
    5.1 Residual concatenate fully convolutional neural network In this study, weather
    radar beam blockage correction is regarded as a classification problem. The model
    can classify all the unknown pixels at the same time in the condition of known
    areas. The classification rules of the model are obtained by big data training
    progress. A detailed description of the model is given below. 5.1.1 Network structure
    Fully Convolutional Network (FCN) [17] is a widely used model in the field of
    image semantic segmentation. It can classify each pixel in the image and determine
    the category of each point. Its segmentation effect has been remarkably improved
    compared with traditional methods. However, if the FCN model is directly applied
    to radar image correction, the correction effect is not fine enough and the accuracy
    is low. Based on the idea of FCN model, an improved model which names Residual
    Concatenate Fully Convolutional Neural Network (RC-FCN) is proposed for radar
    image correction. Figure 12 shows the basic structure of RC-FCN. It consists of
    an encoder and a decoder. The encoder part is composed of continuous residual
    convolution modules and the decoder part is composed of continuous de-convolution
    modules. The structure of each residual convolution module are same. As the feature
    map passes through each residual convolution module, its size will change to half
    of its original size. Detail introduction of residual convolution module will
    be introduced in next part. Starting from the input layer, the feature map will
    be reduced to 1/32 of the original size after five times of residual convolution
    module. After several convolutions, the feature map will be de-convoluted five
    times again.In contrast to convolution, each de-convolution module will enlarge
    the size of the input feature map by twice. After five consecutive de-convolution,
    the feature map of the last convolution output is enlarged 32 times. The output
    will be restored to the original size. It should be noted that in order to improve
    the reconstruction performance of the feature map in the de-convolution process,
    the input feature map of each de-convolution operation contains not only the feature
    map output from the previous stage, but also the convolution feature map with
    the same size in symmetrical position. These two kinds of feature maps connect
    the two tensors together by means of concatenate. This structure is similar to
    FCN, but FCN only connects x32, x16 and x8. In this paper, through experiments,
    it is found that concatenating each set of symmetric convolution and de-convolution
    feature maps can improve the image reconstruction accuracy. In this paper, this
    kind of network is called CFCN. The letter, C, indicates that the concatenation
    between the encoder network and the decoder network is complete. In this way,
    it can effectively improve the network image reconstruction ability. After encoder
    network coding, the decoded output image of decoder is much more close to the
    original image. The detailed comparison can be seen in Table 3, and the specific
    analysis will be described in the next section. Since the length and width of
    the original image are not necessarily 32 times, the image may not be able to
    be restored to the same size as the original image after five consecutive downsizing
    and five successive upsizing. Therefore, when the image is input the first convolution
    module, the length and width of the image should be adjusted by adjusted convolution
    module to the nearest size of which can be divided by 32 with no remainder. In
    this paper, the input image is 250 pixels wide and 360 pixels high. After adaptive
    convolution, the output image size is 256\\({\\times }\\)384. In this model, the
    image correction problem is regarded as a segmentation problem, and the essence
    is to classify each pixel one by one. Therefore, cross entropy loss is used as
    the loss function in this model. In order to achieve this conversion, each pixel
    of the image needs to be one-hot coded. As this is a 17 classification problem,
    each pixel is transformed into a one-hot vector with 17 dimensions. The loss function
    formula is shown in Formula (1). $$\\begin{aligned} {\\text {Loss}}(\\mathbf {x},\\mathbf
    {y})=\\sum _{N}^{i=0}loss(x_{i},y_{i}) \\end{aligned}$$ (1) Where \\(\\mathbf
    {x}\\) represents all predicted pixels and \\(\\mathbf {y}\\) represents the index
    of real pixel class. N represents the number of pixels to be predicted. Detail
    of \\(loss(x_{i},y_{i})\\) is shown as Formula (2). $$\\begin{aligned} {\\text
    {loss}}(x, \\text{ class})=-\\log \\left( \\frac{\\exp (x[{\\text {class}}])}{\\sum
    _{j} \\exp (x[j])}\\right) \\end{aligned}$$ (2) Where \\(\\text{ class }\\) means
    the index of pixel class which is from 0-17. The \\(x[{\\text {class}}]\\) is
    negative log likelihood loss, which is calculated as Formula (3). It can be understood
    as next. For example, suppose \\(x=[1,2,3]\\), \\(class=2\\), then \\(f(x,class)=-x[2]=-3\\).
    $$\\begin{aligned} f(x,class)=-x[class] \\end{aligned}$$ (3) Fig. 12 Residual
    concatenate fully convolutional neural network Full size image 5.1.2 Residual
    convolutional module structure A large number of experiments show that the residual
    convolution can effectively predict the blocking areas in the radar images. Detailed
    results can be seen in Table 4. Experiments show that, compared with VGG [27],
    GoogLeNet [29], DenseNet [10], the ResNet [7] has the best performance in image
    in-painting. In this study, the convolution module in the encoder is designed
    in the form of residual network. The structure of the residual module is shown
    in Figure 13. In this progress, input is represented by x, and supposing that
    the target mapping which want to learn is f(x). The part in the dotted line box
    in the Figure 13 needs to fit the residual mapping \\(f(x)-x\\) of identical mapping.
    Residual mapping is often easier to optimize in practice because of its skipping
    connections. When the target mapping f(x) is very close to identical mapping,
    the residual mapping is also easy to capture the subtle fluctuations of identical
    mapping. With residual module, inputs can forward propagate faster through the
    residual connections across layers. Fig. 13 Residual module structure Full size
    image 6 Experiment In this study, a lot of experiments have been done in early
    stage, and the performance of different models in image reconstruction and local
    blocking area prediction are studied. 6.1 Training and test detail During the
    training stages, continuous weather radar image data are used. The data is from
    Guangzhou weather radar station detecting from 2011 to 2014 and NUIST weather
    radar station in the form of image. The data set mixes the images from two radar
    stations. The purpose of this is to improve the universality of the model after
    training, so that the model can correct the data from the two radar stations.
    Because the data to be processed by the model come from different radars in practical
    application, multi-source data need to be used for training, so that the model
    can have higher practical value. The images have been preprocessed in the way
    of Section 3, and the blocking areas are generated randomly by the program. A
    total of 10000 weather radar images were used in the experiments, including 8000
    for training and 2000 for testing. The size of the image used in the experiment
    is 360 height and 250 width. The position and width of the blocking areas are
    random. The minimum width is 3 pixels and the maximum is 16 pixels. The same data
    set was used for training and testing in all experiments. However, the number
    of training epochs of each model is different. General training until the loss
    value is no longer significantly reduced. The left right of Figure 14 shows the
    change of accuracy of training data set and test data set with the number of training
    epochs. The right side of Figure 14 shows the change of loss during training.
    It can be seen from the figure that after about 800 epochs, the changes of accuracy
    and loss are significantly slowed down, but the overall change is still going
    better. However, in this process, sometimes the accuracy suddenly drops or the
    loss increases suddenly. With the continuous training, the frequency of this sudden
    change has a decreasing trend. In order to avoid this kind of accidental deterioration
    during testing, the model parameters are saved when the test accuracies of the
    test set reach the best value during the 1000 training epochs. Fig. 14 Training
    and test accuracy and training loss changes Full size image In the stage of network
    training, many groups of hyperparameters are tried in order to find the best parameters.
    For RC-FCN model, many attempts show that when the learning rate is 0.001, the
    mini-batch is 8, and the stochastic gradient descent algorithm is used for optimization,
    the prediction result of the model is the best. However, these hyperparameters
    are limited by the computational power of this study. The GPU video card memory
    used in the experiment is only 11GB, which can not accommodate a larger mini-batch.
    Maybe a larger mini-batch can get better training results. 6.2 RC-FCN experiment
    comparison The RC-FCN method proposed in this study is a deep improvement of FCN.
    Therefore, in the process of research, comparative experiments for each improvement
    have been carried out. At the same time, the results of other classical convolution
    neural networks in this study are also compared. The experiments mainly focus
    on two aspects, one is the ability of radar image restoration, the other is the
    ability of radar image beam blockage correction. 6.2.1 Evaluation criterion Beam
    blockage correction of weather radar is implemented from the perspective of image
    restoration and image segmentation methods. The accuracy, structural similarity
    (SSIM) and correlation are used to evaluate the effects of different models. On
    the other hand, because the network proposed in this study is mainly to solve
    the weather radar beam blockage correction problem, the standards in meteorological
    industry, which are CSI, FAR and POD, are used to evaluate the correction effects
    of each intensity segment. Detailed formulation can be found in [25, 26]. 6.2.2
    Overall radar image restoration Fig. 15 A sample of beam blockage correction.
    The left first image is the ground truth. The second is the input image, which
    can be seen a rectangle black area in the top, that is the area to be corrected.
    The third is the output from RC-FCN. The corrected area can refer to the position
    corresponding to the white area of the forth. The horizontal axis is the radial
    distance. The coverage radius of radar data used in this study is 250km. The longitudinal
    axis is the radar deflection angle, a total of 360 integers Full size image Figure
    15 shows a set of radar images corrected by RC-FCN. Visual observation shows that
    the network model can reconstruct the original image well. Without careful observation,
    it is difficult to find the difference between input and output in the unblocked
    area. In order to show the image reconstruction ability of the RC-FCN more quantitatively,
    the results of evaluation criterion and the comparison with other networks are
    given in detail in Table 3. This table also gives the reasons why FCN can be used
    as the basis for weather radar image correction. It can be seen from the table
    that FCN has excellent effect in image reconstruction. No matter CSI, FAR, POD,
    Accuracy, Correlation or SSIM, FCN ranked first or second. Therefore, based on
    FCN, this study attempts to propose a new deep learning network with excellent
    image reconstruction ability and good correction effect. However, the original
    FCN image reconstruction effect is not ideal, and the restored image appears mosaic
    like. Therefore, all the feature maps of the symmetric positions of FCN are concatenated,
    which is called CFCN in this study. The experimental results show that although
    CFCN does not achieve the best results in CSI, FAR and POD, it has achieved better
    results in overall accuracy, correlation and SSIM than FCN. But most importantly,
    CFCN has excellent performance in image reconstruction. It can reconstruct the
    original radar image completely without mosaic after encoding and decoding the
    input. Table 3 Overall accuracies of radar image reconstruction Full size table
    6.2.3 Radar beam blockage correction The ability of image reconstruction is not
    enough. The focus of this study is to achieve the data correction of blocking
    areas, that is, prediction. On the basis of CFCN, we try to add various convolution
    modules in the process of this study, such as dense module, residual module, inception
    module and VGG structure. Experiments show that the coding and decoding network
    based on ResNet is the best in image correction task. Therefore, this study combines
    DenseNet and ResNet to improve the network structure of CFCN. It should be noted
    that DenseNet’s concatenation idea has been achieve in CFCN. CFCN has concatenated
    the feature maps of symmetric part of coder and decoder. Table 4 shows the accuracy
    comparison of RC-FCN and other networks in radar beam blockage correction task.
    Experiments show that the coding and decoding network based on ResNet has achieved
    remarkable results in image correction task. After adding residual module to CFCN,
    some of the RC-FCN’s evaluation criterion indexes achieve the best results, and
    some achieve the second best results. Most importantly, the improved RC-FCN has
    achieved the best or second best results in the accuracy, correlation and SSIM.
    Fig. 16 Detail comparison of beam blockage correction Full size image Figure 16
    shows the comparison of the corrected results of blocking area. Each graph shows
    the radial data of radar echo reflectivity at one angle. The horizontal axis is
    the radial distance, and the vertical axis is the normalized reflectivity intensity.
    The blue line is the true value, and the red line is the predicted value of the
    network. Through observation, it can be found that the fit of predicted value
    and real value is high. Network can accurately predict the trend of data changes.
    There are some errors, but overall, the error is small. Although RC-FCN failed
    to achieve the best or second best performance in every index, it achieved good
    fusion of radar image reconstruction task and radar image beam blockage correction
    task. The skipping connection added in the network also helps to improve the accuracy
    of the correction task. However, the prediction results of the model still have
    some disadvantages. As can be seen from Figure 15, there is a sense of smearing
    in the target area of image prediction. This phenomenon shows that the general
    trend of the data is predicted correctly, but the precision is not high enough.
    This conclusion can be verified from Figure 16. In this figure, the red curve
    is the predicted value and the blue curve is the real value. It can be seen that
    the two curves overlap in general, but the complete overlap is relatively small.
    In addition, there are a few pixels that can not be effectively predicted. For
    example, in row 39 to row 42 of Figure 16, the data at about the 180th position
    do not be accurately predicted, and the predicted value is greatly different from
    the real value. Table 4 Accuracies of corrected areas from generated images Full
    size table 7 Conclusion In this study, a new edge-assisted cloud computing framework
    using RC-FCN is proposed. Its main task is to correct the beam blocking areas
    of radar images in the environment of Internet of Weather Radars. The framework
    is based on an edge-assisted cloud computing architecture, which can achieve large-scale
    weather radar beam blocking correction under the premise of high performance and
    elastic. A large number of comparative experiments show that the RC-FCN proposed
    in this paper can effectively correct the radar beam blockage areas and reconstruct
    the radar image. In addition, the most innovative part of the network is the use
    of semantic segmentation idea to achieve the correction of radar images. In order
    to meet the needs of semantic segmentation, a lot of preprocessing work has been
    done in this study, such as pixel value normalization and coordinate transformation.
    All processes are integrated into the edge-assisted cloud system which can be
    invoked remotely in a flexible and efficient way. References Bi, R., Liu, Q.,
    Ren, J., Tan, G.: Utility aware offloading for mobile-edge computing. Tsinghua
    Science and Technology 26(2), 239–250 (2020) Article   Google Scholar   Cai, W.,
    Wei, Z.: Piigan: Generative adversarial networks for pluralistic image inpainting.
    IEEE Access 8, 48451–48463 (2020) Article   Google Scholar   Chen, H., Zhang,
    Y., Cao, Y., Xie, J.: Security issues and defensive approaches in deep learning
    frameworks. Tsinghua Science and Technology 26(6), 894–905 (2021) Article   Google
    Scholar   Chen, Y., Liu, L., Tao, J., Xia, R., Zhang, Q., Yang, K., Xiong, J.,
    Chen, X.: The improved image inpainting algorithm via encoder and similarity constraint.
    The Visual Computer pp. 1–15 (2020) Duc, T.L., Leiva, R.G., Casari, P., Östberg,
    P.O.: Machine learning methods for reliable resource provisioning in edge-cloud
    computing: A survey. ACM Computing Surveys (CSUR) 52(5), 1–39 (2019) Article   Google
    Scholar   Ham, Y.G., Kim, J.H., Luo, J.J.: Deep learning for multi-year enso forecasts.
    Nature 573(7775), 568–572 (2019) Article   Google Scholar   He, K., Zhang, X.,
    Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 770–778
    (2016) He, Q., Cui, G., Zhang, X., Chen, F., Deng, S., Jin, H., Li, Y., Yang,
    Y.: A game-theoretical approach for user allocation in edge computing environment.
    IEEE Transactions on Parallel and Distributed Systems 31(3), 515–529 (2020). https://doi.org/10.1109/TPDS.2019.2938944
    Article   Google Scholar   Hongyan, W., Liping, L., Liping, H.: Beam blockage
    studies of doppler weather radar in mountainous region of zhejiang. Plateau Meteorology
    33(6), 1737–1747 (2014) Google Scholar   Huang, G., Liu, Z., Van Der Maaten, L.,
    Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of
    the IEEE conference on computer vision and pattern recognition, pp. 4700–4708
    (2017) Kim, S., Hong, S., Joh, M., Song, S.k.: Deeprain: Convlstm network for
    precipitation prediction using multichannel radar data. Preprint at arXiv:1711.02316
    (2017) Lai, P., Qiang, H., Abdelrazek, M., Chen, F., Hosking, J., Grundy, J.,
    Yun, Y.: Optimal edge user allocation in edge computing with variable sized vector
    bin packing. Springer, Cham (2018) Book   Google Scholar   Li, J., Wang, N., Zhang,
    L., Du, B., Tao, D.: Recurrent feature reasoning for image inpainting. In: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7760–7768
    (2020) Li, N., Wang, Z., Sun, K., Chu, Z., Leng, L., Lv, X.: A quality control
    method of ground-based weather radar data based on statistics. IEEE Transactions
    on Geoscience and Remote Sensing 56(4), 2211–2219 (2017) Article   Google Scholar   Li,
    P.C., Yu, T.T.: Landslide early warning with rainfall data from correcting weather
    radar reflectivity using machine learning. In: EGU General Assembly Conference
    Abstracts, p. 19265 (2020) Liu, G., Reda, F.A., Shih, K.J., Wang, T.C., Tao, A.,
    Catanzaro, B.: Image inpainting for irregular holes using partial convolutions.
    In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 85–100
    (2018) Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for
    semantic segmentation. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 3431–3440 (2015) Mabrouki, J., Azrour, M., Dhiba,
    D., Farhaoui, Y., El Hajjaji, S.: Iot-based data logger for weather monitoring
    using arduino-based wireless sensor networks with remote graphical application
    and alerts. Big Data Mining and Analytics 4(1), 25–32 (2021) Article   Google
    Scholar   Malek, Y.N., Najib, M., Bakhouya, M., Essaaidi, M.: Multivariate deep
    learning approach for electric vehicle speed forecasting. Big Data Mining and
    Analytics 4(1), 56–64 (2021) Article   Google Scholar   McRoberts, D.B., Nielsen-Gammon,
    J.W.: Detecting beam blockage in radar-based precipitation estimates. Journal
    of Atmospheric and Oceanic Technology 34(7), 1407–1422 (2017) Article   Google
    Scholar   Miao, Y., Wu, G., Li, M., Ghoneim, A., Al-Rakhami, M., Hossain, M.S.:
    Intelligent task prediction and computation offloading based on mobile-edge cloud
    computing. Future Generation Computer Systems 102, 925–931 (2020) Article   Google
    Scholar   Nazeri, K., Ng, E., Joseph, T., Qureshi, F.Z., Ebrahimi, M.: Edgeconnect:
    Generative image inpainting with adversarial edge learning. Preprint at arXiv:1901.00212
    (2019) Rafique, W., Qi, L., Yaqoob, I., Imran, M., Rasool, R.U., Dou, W.: Complementing
    iot services through software defined networking and edge computing: A comprehensive
    survey. IEEE Communications Surveys & Tutorials 22(3), 1761–1804 (2020) Article   Google
    Scholar   Ren, Z., Liu, Y., Shi, T., Xie, L., Zhou, Y., Zhai, J., Zhang, Y., Zhang,
    Y., Chen, W.: Aiperf: Automated machine learning as an ai-hpc benchmark. Big Data
    Mining and Analytics 4(3), 208–220 (2021) Article   Google Scholar   Shi, E.,
    Li, Q., Gu, D., Zhao, Z.: A method of weather radar echo extrapolation based on
    convolutional neural networks. In: International Conference on Multimedia Modeling,
    pp. 16–28. Springer (2018) Shi, X., Gao, Z., Lausen, L., Wang, H., Yeung, D.Y.,
    Wong, W.k., Woo, W.c.: Deep learning for precipitation nowcasting: A benchmark
    and a new model. In: Advances in neural information processing systems, pp. 5617–5627
    (2017) Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
    image recognition. Preprint at arXiv:1409.1556(2014) Song, Y., Yang, C., Lin,
    Z., Liu, X., Huang, Q., Li, H., Jay Kuo, C.C.: Contextual-based image inpainting:
    Infer, match, and translate. In: Proceedings of the European Conference on Computer
    Vision (ECCV), pp. 3–19 (2018) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed,
    S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with
    convolutions. In: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1–9 (2015) Tian, H., Xu, X., Lin, T., Cheng, Y., Qian, C., Ren,
    L., Bilal, M.: Dima: Distributed cooperative microservice caching for internet
    of things in edge computing by deep reinforcement learning. World Wide Web pp.
    1–24 (2021) Tong, Z., Ye, F., Yan, M., Liu, H., Basodi, S.: A survey on algorithms
    for intelligent computing and smart city applications. Big Data Mining and Analytics
    4(3), 155–172 (2021) Article   Google Scholar   Trebing, K., Stanczyk, T., Mehrkanoon,
    S.: Smaat-unet: Precipitation nowcasting using a small attention-unet architecture.
    Pattern Recognition Letters (2021) Wang, N., Ma, S., Li, J., Zhang, Y., Zhang,
    L.: Multistage attention network for image inpainting. Pattern Recognition 106,
    107448 (2020) Article   Google Scholar   Wang, T., Zhang, G., Liu, A., Bhuiyan,
    M.Z.A., Jin, Q.: A secure iot service architecture with an efficient balance dynamics
    based on cloud and edge computing. IEEE Internet of Things Journal 6(3), 4831–4843
    (2018) Article   Google Scholar   Wei, D., Ning, H., Shi, F., Wan, Y., Xu, J.,
    Yang, S., Zhu, L.: Dataflow management in the internet of things: Sensing, control,
    and security. Tsinghua Science and Technology 26(6), 918–930 (2021) Article   Google
    Scholar   Wu, H., Zhang, Z., Guan, C., Wolter, K., Xu, M.: Collaborate edge and
    cloud computing with distributed deep learning for smart city internet of things.
    IEEE Internet of Things Journal 7(9), 8099–8110 (2020) Article   Google Scholar   Wu,
    K., Shen, Y., Wang, S.: 3d convolutional neural network for regional precipitation
    nowcasting. Journal of Image and Signal Processing 7(4), 200–212 (2018) Article   Google
    Scholar   Xia, X., Chen, F., He, Q., Grundy, J.C., Abdelrazek, M., Jin, H.: Cost-effective
    app data distribution in edge computing. IEEE Transactions on Parallel and Distributed
    Systems 32(1), 31–44 (2021). https://doi.org/10.1109/TPDS.2020.3010521 Article   Google
    Scholar   Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.:
    Convolutional lstm network: A machine learning approach for precipitation nowcasting.
    In: Advances in neural information processing systems, pp. 802–810 (2015) Xu,
    X., Fang, Z., Qi, L., Zhang, X., He, Q., Zhou, X.: Tripres: Traffic flow prediction
    driven resource reservation for multimedia iov with edge computing. ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM) 17(2), 1–21 (2021)
    Article   Google Scholar   Xu, X., Fang, Z., Zhang, J., He, Q., Yu, D., Qi, L.,
    Dou, W.: Edge content caching with deep spatiotemporal residual network for iov
    in smart city. ACM Transactions on Sensor Networks (TOSN) 17(3), 1–33 (2021) Article   Google
    Scholar   Xu, X., Huang, Q., Zhu, H., Sharma, S., Zhang, X., Qi, L., Bhuiyan,
    M.Z.A.: Secure service offloading for internet of vehicles in sdn-enabled mobile
    edge computing. IEEE Transactions on Intelligent Transportation Systems 22(6),
    3720–3729 (2020) Article   Google Scholar   Yao, G., Liu, Z., Guo, X., Wei, C.,
    Li, X., Chen, Z.: Prediction of weather radar images via a deep lstm for nowcasting.
    In: 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE
    (2020) Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image
    inpainting with contextual attention. In: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 5505–5514 (2018) Zamora-Izquierdo,
    M.A., Santa, J., Martínez, J.A., Martínez, V., Skarmeta, A.F.: Smart farming iot
    platform based on edge and cloud computing. Biosystems engineering 177, 4–17 (2019)
    Article   Google Scholar   Zhang, W., Chen, X., Jiang, J.: A multi-objective optimization
    method of initial virtual machine fault-tolerant placement for star topological
    data centers of cloud systems. Tsinghua Science and Technology 26(1), 95–111 (2020)
    Article   Google Scholar   Download references Acknowledgements This work has
    received funding from the Key Laboratory Foundation of National Defence Technology
    under Grant 61424010208, National Natural Science Foundation of China (No. 41911530242
    and 41975142), 5150 Spring Specialists (05492018012 and 05762018039), Major Program
    of the National Social Science Fund of China (Grant No. 17ZDA092), 333 High-Level
    Talent Cultivation Project of Jiangsu Province (BRA2018332), Royal Society of
    Edinburgh, UK and China Natural Science Foundation Council (RSE Reference: 62967_Liu_2018_2)
    under their Joint International Projects funding scheme and basic Research Programs
    (Natural Science Foundation) of Jiangsu Province (BK20191398 and BK20180794).
    Funding Included in the Acknowledgements. Author information Authors and Affiliations
    School of Computer and Software, Engineering Research Center of Digital Forensics,
    Ministry of Education, Nanjing University of Information Science and Technology,
    Nanjing, China Hao Wu & Zhiyun Yang School of Computer and Software, Nanjing University
    of Information Science and Technology, Nanjing, China Qi Liu School of Computing,
    Edinburgh Napier University, Edinburgh, Scotland Xiaodong Liu Jiangsu Collaborative
    Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET),
    Nanjing University of Information Science Technology, Nanjing, China Yonghong
    Zhang Corresponding author Correspondence to Qi Liu. Ethics declarations Conflict
    of interest The authors declare that they have no conflict of interest. Additional
    information Hao Wu and Qi Liu contribute equally to the article. This article
    belongs to the Topical Collection: Special Issue on Resource Management at the
    Edge for Future Web, Mobile, and IoT Application Guest Editors: Qiang He, Fang
    Dong, Chenshu Wu, and Yun Yang. Rights and permissions Reprints and permissions
    About this article Cite this article Wu, H., Liu, Q., Liu, X. et al. An edge-assisted
    cloud framework using a residual concatenate FCN approach to beam correction in
    the internet of weather radars. World Wide Web 25, 1923–1949 (2022). https://doi.org/10.1007/s11280-021-00988-y
    Download citation Received 19 July 2021 Revised 03 November 2021 Accepted 15 December
    2021 Published 21 January 2022 Issue Date September 2022 DOI https://doi.org/10.1007/s11280-021-00988-y
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Edge computing Internet of radars Residual concatenate Beam
    blockage correction Weather radar Use our pre-submission checklist Avoid common
    mistakes on your manuscript. Associated Content Part of a collection: Special
    Issue on Resource Management at the Edge for Future Web, Mobile and IoT Applications
    Sections Figures References Abstract Introduction Related work An edge-assisted
    cloud framework for internet of weather radars Detailed system design of edge-assisted
    computing in radar stations The model design in cloud computing platform Experiment
    Conclusion References Acknowledgements Funding Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: World Wide Web
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An edge-assisted cloud framework using a residual concatenate FCN approach
    to beam correction in the internet of weather radars
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ali A.A.
  - Saadi S.M.
  - Mahmood T.M.
  - Mostafa S.A.
  citation_count: '1'
  description: This paper proposes a smart water grids network (SWGN) architecture
    that combines the advantages of fog computing, internet of things (IoT), long
    range wide area network (LoRaWAN), and Software-defined networking (SDN). The
    main aims of the SWG architecture are to optimize data routing and monitor water
    supply and quality in real-time. SWGN handles a vast amount of data that is collected
    by IoT devices from different points related to water supply and quality. The
    data is processed in a distributed way by a number of fog servers that are located
    at the edge of the network. The fog controllers are deployed at the fog layer
    in order to take action locally for frequent events. The cloud layer has a cloud
    controller to take actions globally for infrequent events. The LoRaWAN provides
    communication technology that allows devices to operate regularly. The SDN technology
    decouples network traffic to control data routing decisions efficiently. A primitive
    evaluation under the Mininet emulator, focusing on SDN, shows the feasibility
    and efficiency of the architecture.
  doi: 10.11591/eei.v11i3.3227
  full_citation: '>'
  full_text: '>

    "USER Username Password Remember me CITATION ANALYSIS Dimensions Google Scholar
    Scholar Metrics Scinapse Scopus QUICK LINKS Editorial Boards Reviewers Abstracting
    and Indexing Guide of Authors Online Papers Submission Peer Review Process Publication
    Fee Publication Ethics Visitor Statistics DOI Deposit Report Contact Us HOW TO
    SUBMIT   JOURNAL CONTENT Search Search Scope      All Authors Title Abstract Index
    terms Full Text      Browse By Issue By Author By Title INFORMATION For Readers
    For Authors For Librarians ARTICLE TOOLS How to cite item HOME ABOUT LOGIN REGISTER
    SEARCH CURRENT ARCHIVES ANNOUNCEMENTS Home > Vol 11, No 3 > Adil Ali A smart water
    grid network for water supply management systems Ali Adil Ali, Saadi Mohammed
    Saadi, Tameem Mohammed Mahmood, Salama A. Mostafa  Abstract  This paper proposes
    a smart water grids network (SWGN) architecture that combines the advantages of
    fog computing, internet of things (IoT), long range wide area network (LoRaWAN),
    and Software-defined networking (SDN). The main aims of the SWG architecture are
    to optimize data routing and monitor water supply and quality in real-time. SWGN
    handles a vast amount of data that is collected by IoT devices from different
    points related to water supply and quality. The data is processed in a distributed
    way by a number of fog servers that are located at the edge of the network. The
    fog controllers are deployed at the fog layer in order to take action locally
    for frequent events. The cloud layer has a cloud controller to take actions globally
    for infrequent events. The LoRaWAN provides communication technology that allows
    devices to operate regularly. The SDN technology decouples network traffic to
    control data routing decisions efficiently. A primitive evaluation under the Mininet
    emulator, focusing on SDN, shows the feasibility and efficiency of the architecture.  Keywords  Cloud
    computing; Fog computing; Smart water grid; Water quality monitoring internet
    of things  Full Text: PDF   DOI: https://doi.org/10.11591/eei.v11i3.3227 Refbacks
    There are currently no refbacks.     Bulletin of EEI Stats Bulletin of Electrical
    Engineering and Informatics (BEEI) ISSN: 2089-3191, e-ISSN: 2302-9285 This journal
    is published by the Institute of Advanced Engineering and Science (IAES) in collaboration
    with Intelektual Pustaka Media Utama (IPMU)."'
  inline_citation: '>'
  journal: Bulletin of Electrical Engineering and Informatics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A smart water grid network for water supply management systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhang D.K.
  - Li Y.H.
  - Zi C.Y.
  - Zhang Y.Q.
  - Yang R.
  - Tian G.C.
  - Zhao W.B.
  citation_count: '1'
  description: As a critical fossil energy, lignite has a huge resource, wide distribution,
    and a low comprehensive utilization rate. Investigations regarding the molecular
    structure model of lignite are beneficial for pre-judging the chemical reaction
    mechanism and reaction path of lignite in pyrolysis, liquefaction and gasification,
    thereby improving its comprehensive utilization. Eshan lignite was studied by
    Fourier transform infrared spectroscopy, 13C Nuclear magnetic resonance spectroscopy
    and X-ray photoelectron spectroscopy in this paper. Moreover, the structural unit
    parameters of carbon, oxygen and nitrogen of Eshan lignite were obtained. According
    to these parameters, the molecular structure model of Eshan lignite was established
    and optimized by using the quantum chemical modeling method in the Gaussian 09
    computing platform. The results indicate that the content of aromatic carbon and
    aliphatic carbon is 39.20% and 49.51%, respectively. In detail, the aromatic carbon
    structure mainly includes benzene and naphthalene, and the ratio of aromatic bridgehead
    carbon to surrounding aromatic carbon is 0.07. The aliphatic carbon structure
    mainly contains methylene, methyl and oxy-aliphatic carbon. Furthermore, the oxygen
    atoms mainly exist in hydroxyl, ether oxygen, carboxyl and carbonyl. Moreover,
    the nitrogen structure mainly involves pyridine. Based on the results of ultimate
    analysis and 13C nuclear magnetic resonance spectroscopy analysis, the molecular
    formula of Eshan lignite was calculated as C153H137O35N2 after eliminating the
    influence of water by thermogravimetric experiment. The initial structural model
    of Eshan lignite was constructed via the connecting structural unit. The PM 3
    basis set of semi-empirical method and density functional theory M06-2X/3-21G
    basis set were used to optimize the initial molecular configuration. The optimized
    model has obvious three-dimensional characteristics. Among these, the aromatic
    rings arrange irregularly in space, and the distance between every aromatic ring
    is far. The aromatic carbon structures are mainly connected by methylene, ether
    oxygen, carbonyl ester and aliphatic ring. The oxygen functional groups mainly
    distributed at the edge of molecular and aliphatic structures possess many side
    chains. The simulated infrared spectrum of the molecular model was obtained by
    analyzing the vibration frequency of the optimized molecular model, and it agrees
    with the experimental infrared spectrum well, representing the accuracy and rationality
    of the molecular structure model of Eshan lignite. This molecular structure model
    is conducive to understanding the physicochemical properties of Eshan lignite
    more intuitively and revealing its macroscopic properties. Meanwhile, the molecular
    structure model can provide a theory basis for further research on lignite pyrolysis,
    liquefaction and gasification.
  doi: 10.3964/j.issn.1000-0593(2022)04-1293-06
  full_citation: '>'
  full_text: '>

    "检测到您正在使用 Safari 浏览器，可能影响导出功能的正常使用，建议您下载 Google Chrome 、 Microsoft Edge 、 Firefox
    。 X 学习中心 应用 会员 登录 / 注册 简 繁 搜索 首页 > 期刊导航 > 光谱学与光谱分析 > 2022年4期 > 峨山褐煤的分子结构和分子模拟
    DOI: 10.3964/j.issn.1000-0593(2022)04-1293-06 峨山褐煤的分子结构和分子模拟 张殿凯 1 李艳红 1 訾昌毓 1
    张远琴 1 杨荣 1 田国才 2 赵文波 1 1.昆明理工大学化学工程学院,云南 昆明 6505002.昆明理工大学省部共建复杂有色金属资源清洁利用国家重点实验室,云南
    昆明 650093 在线阅读 下载 引用 收藏 分享 打印 摘要： 作为重要的化石能源,褐煤资源潜力巨大、分布广泛但综合利用率低.研究褐煤的分子结构模型,有助于预测褐煤在热解、液化和气化过程中的化学反应机理及反应路径,进而提高褐煤的综合应用水平.以云南峨山褐煤为研究对象,利用傅里叶变换红外光谱、13
    C核磁共振波谱及X射线光电子能谱等分析测试方法,获取了峨山褐煤的含碳、含氧及含氮结构参数.在此基础上,借助Gaussian 09计算平台,采用量子化学建模的方法构建并优化了峨山褐煤的分子结构模型.研究结果表明:峨山褐煤的芳碳率为39.20％,芳香碳结构主要为苯和萘,且芳香桥头碳与周边碳的比值
    χb为0.07;脂碳率为49.51％,脂肪碳结构主要为亚甲基,季碳和氧接脂碳;氧原子主要存在于羟基、醚氧、羰基和羧基结构中;含氮结构则以吡啶为主.基于元素分析、13
    C核磁共振波谱分析,又经过热重实验消除褐煤中残余水分的影响后,计算... 关键词： 峨山褐煤光谱分析量子化学计算分子模型模拟FTIR 分类号： TQ536.9(煤化学及煤的加工利用)
    资助基金： 国家自然科学基金 ( 21766013 ) 昆明理工大学分析测试基金 ( 2020M20192208021 ) 在线出版日期： 2022-04-18
    （万方平台首次上网日期，不代表论文的发表时间） 页数： 6 ( 1293-1298 ) 英文信息 引证文献 (1) 仅看全文 排序： 发表时间 被引频次 [1]
    王雪峰,曹敏敏,王荀,等.钴基催化剂催化矿井乏风甲烷与锅炉燃煤共燃性能[J].煤炭学报.2023,48(8).DOI:10.13225/j.cnki.jccs.2022.0799
    . 同项目论文 21766013:国家自然科学基金 [1] 张殿凯 , 李艳红 , 王苗 ,等. 氧化法提取褐煤腐植酸的研究进展 [J]. 应用化工 . 2021
    ,50(10).2851-2855,2860. DOI: 10.3969/j.issn.1671-3206.2021.10.047 . [2] 阳超琴 ,
    张权 , 顾丽莉 ,等. 比较法在精馏实验教学中的应用 [J]. 实验室研究与探索 . 2020 ,39(3).160-162,167. DOI: 10.3969/j.issn.1006-7167.2020.03.034
    . [3] 张远琴 , 李艳红 , 訾昌毓 ,等. 弥勒褐煤提取腐植酸的工艺优化研究 [J]. 应用化工 . 2020 ,49(6).1348-1353.
    DOI: 10.3969/j.issn.1671-3206.2020.06.005 . [4] 李瑾 , 王平艳 , 李艳丽 ,等. 火电厂周边土壤污染及农作物重金属的累积特征评价
    [J]. 昆明理工大学学报（自然科学版） . 2020 ,45(3).87-92,127. DOI: 10.16112/j.cnki.53-1223/n.2020.03.011
    . [5] 张殿凯 , 李艳红 , 常丽萍 ,等. 弥勒褐煤结构特征及其分子模型构建 [J]. 燃料化学学报 . 2021 ,49(6).727-734.
    DOI: 10.19906/j.cnki.JFCT.2021026 . [6] 彭昭霞 , 李艳红 , 梁光兵 ,等. 蔗渣灰制备SiO2及其亚甲基蓝吸附性能
    [J]. 环境科学与技术 . 2020 ,43(10).77-84. DOI: 10.19672/j.cnki.1003-6504.2020.10.010
    . [7] 罗翠娟 , 张登峰 , 赵春鹏 ,等. 含气页岩中水分赋存与分布的研究进展 [J]. 化工进展 . 2019 ,38(6).2726-2737.
    DOI: 10.16085/j.issn.1000-6613.2018-2051 . [8] 訾昌毓 , 李艳红 , 赵文波 ,等. 粉煤灰合成分子筛的研究
    [J]. 硅酸盐通报 . 2018 ,37(12).4001-4006. [9] 彭昭霞 , 李艳红 , 陈亿琴 ,等. 生物质灰硅资源高附加值利用的研究进展
    [J]. 生物质化学工程 . 2020 ,54(2).61-66. DOI: 10.3969/j.issn.1673-5854.2020.02.009 .
    [10] 梁光兵 , 李艳红 , 张远琴 ,等. 磁响应吸油材料的研究进展 [J]. 材料导报 . 2019 ,33(23).3999-4007. DOI:
    10.11896/cldb.18110136 . 1 2 评论 您当前未登录！去登录 光谱学与光谱分析 EI CSTPCD 北大核心 SCI ISSN：1000-0593
    年,卷(期)：2022,42(4) 月卡 - 期刊畅读卡 - ¥68 季卡 - 期刊畅读卡 - ¥128 年卡 - 期刊畅读卡 - ¥199 年卡 - 超级文献套餐
    - ¥499 查重 - 个人文献检测 - 快速入口 开通阅读并同意 《万方数据会员(个人)服务协议》 相关主题 峨山褐煤 光谱分析 量子化学计算 分子模型
    模拟FTIR 相关学者 赵文波 昆明理工大学 张远琴 昆明理工大学 帮助 客户服务 问卷调查 关于我们 公司首页 加入我们 网站地图 官方店铺 网络出版服务许可证：(署)网出证(京)字第072号
    药品医疗器械网络信息服务备案：(京)网药械信息备字（2023）第 00470 号 信息网络传播视听节目许可证 许可证号：0108284 万方数据知识服务平台--国家科技支撑计划资助项目（编号：2006BAH03B01）
    万方数据学术资源发现获取服务系统[简称：万方智搜] V3.0 证书号：软著登字第11363462号 京ICP证：010071 京公网安备11010802020237号
    京ICP备08100800号-1 ©北京万方数据股份有限公司 万方数据电子出版社 在线客服 客服电话：4000115888 客服邮箱：service@wanfangdata.com.cn
    违法和不良信息举报电话：4000115888 举报邮箱：problem@wanfangdata.com.cn 举报专区：https://www.12377.cn/
    个人文献 检测入口 万方检测 京东店铺 手机版 联系 客服"'
  inline_citation: '>'
  journal: Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Molecular Structure and Molecular Simulation of Eshan Lignite
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Weinman J.
  citation_count: '0'
  description: 'A range of options exist to implement today''s digital architectures.
    Although an oversimplification, we can think of these options along a spectrum:
    At one extreme is a single instance ''data center,'' which might be an enterprise
    data center or a colocation facility, or perhaps even a single server or rack
    containing a database or application. Adjacent to that option is a set of perhaps
    dozens of hyperscale cloud facilities, with perhaps hundreds of thousands of servers,
    typically geographically distributed. As we continue through various layers of
    ''fog,'' we hit the near edge, which might include facilities in major metros,
    and then the far edge, which might include computing nodes located on every street
    corner, or spaced several per mile along a traffic corridor such as a superhighway.
    These then typically tie to endpoint elements, such as devices or things-for example,
    smart meters, irrigation sensors, smartphones, smart TVs, or connected vehicles.
    Sometimes individual processors will be aggregated into a system, such as an autonomous
    vehicle with a hundred or more micro-processors, or perhaps its entertainment
    system, or a network of systems, such as a factory including its robots, quality
    inspection systems, materials handling systems, and fire/smoke detection sensors.'
  doi: 10.1109/CloudContinuum57429.2022.00002
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2022 Cloud Continuum Trade-Offs Along the
    Cloud-Edge Continuum Publisher: IEEE Cite This PDF Joe Weinman All Authors 5 Full
    Text Views Abstract Document Sections Service and Data Proximity Latency Local
    Vs. Global Data Sovereignty, Control, and Privacy Data Management Practicality
    Show Full Outline Authors Figures References Keywords Metrics Abstract: A range
    of options exist to implement today''s digital architectures. Although an oversimplification,
    we can think of these options along a spectrum: At one extreme is a single instance
    “data center,” which might be an enterprise data center or a colocation facility,
    or perhaps even a single server or rack containing a database or application.
    Adjacent to that option is a set of perhaps dozens of hyperscale cloud facilities,
    with perhaps hundreds of thousands of servers, typically geographically distributed.
    As we continue through various layers of “fog,” we hit the near edge, which might
    include facilities in major metros, and then the far edge, which might include
    computing nodes located on every street corner, or spaced several per mile along
    a traffic corridor such as a superhighway. These then typically tie to endpoint
    elements, such as devices or things-for example, smart meters, irrigation sensors,
    smartphones, smart TVs, or connected vehicles. Sometimes individual processors
    will be aggregated into a system, such as an autonomous vehicle with a hundred
    or more micro-processors, or perhaps its entertainment system, or a network of
    systems, such as a factory including its robots, quality inspection systems, materials
    handling systems, and fire/smoke detection sensors. Published in: 2022 Cloud Continuum
    Date of Conference: 05-05 December 2022 Date Added to IEEE Xplore: 05 February
    2024 ISBN Information: DOI: 10.1109/CloudContinuum57429.2022.00002 Publisher:
    IEEE Conference Location: Los Alamitos, CA, USA Along the spectrum from one single
    instance data center to a trillion or more connected endpoints, some would argue
    that on-demand, pay-per-use, consoli-dated hyperscale facilities (the “cloud”,
    the “center,” or the”core”) are the best choice to solve today''s demanding IT
    challenges; others would say that a decentralized edge is the only solution. The
    truth is that there are benefits and disadvantages to each, and finding the right
    weighting of components by considering the tradeoffs along the cloud-fog-edge
    continuum (Figure 1) will typically be the best choice for many business and technology
    challenges. We will focus here on the general characteristics of cloud vs. edge.
    Each of the accepted elements of a “cloud” can generate benefits. For example,
    on-demand resource provisioning in the presence of variable demand generates benefits
    through right-sized capac-ity, since too many resources leads to waste and thus
    unnecessary costs, and too few resources drives opportunity costs through the
    missed revenue, profit, or other benefits associated with the application that
    is executing on that capacity.1 Finer granularity of that capacity for shorter
    intervals can lead to less waste; just as it''s cheaper to buy a glass of wine
    than the cha-teau, and less costly to run an appliance for a shorter period of
    time.2 In a computing context, granularity gets finer as we move from monolithic
    systems to servers to cores to virtual machines to containers to microservices.
    Pay-per-use can lead to benefits even with a unit cost premium, since although
    one may pay more for resources that are employed, one doesn''t pay for resources
    that are not used, unlike with fixed, owned capacity3 However, all of these characteristics-on-demand
    provisioning, fine resource granularity, shorter intervals, and pay-per-use pricing-are
    feasible at the edge, even if they originated with the cloud. Thus, we must look
    to other characteristics to understand the tradeoffs between cloud and edge. We
    also must consider atypical variations. For example, the center may be a single
    facility, such as a solitary enterprise data center serving global demand from
    customers, employees, and partners. However, the edge also may be a single location,
    such as a stand-alone facility that does image processing for railroad car maintenance
    deep in a rural area. Typically, though, we think of the cloud/core/center as
    a few, hyperscale, consolidated facilities; and the edge as a globally geo-dispersed
    set of smaller locations. Figure 1. The cloud-fog-edge continuum. Show All Service
    and Data Proximity The edge is in proximity to where sensor data is collected
    and used. On the other hand, centralized facili-ties can be in proximity to data
    and related application components or services. Proximity can be unhelpful, as
    in the case of a localized disaster such as a hurri-cane or flood that destroys,
    say, an original and its only replica, but is generally good, for example, to
    reduce data transport costs, minimize bandwidth requirements, and minimize latency.
    Edge The edge is in close proximity or even co-located with the user devices and
    things that include sensors that collect data and the actuators that causally
    interact with the real world. This includes smart phones, tablets, smart TVs,
    connected soccer balls, connected auton-omous and non-autonomous vehicles, drones,
    robots, flexible manufacturing cells, video surveillance systems, irrigation sensors,
    connected trash receptacles, and so on. As video typically drives the majority
    of data collection, and resolutions and frame rates increase, local processing
    can have numerous advantages. Cloud The cloud is in close proximity to-in fact,
    likely co-located with-other applications, services, and/ or single-instance or
    master replicas of cloud-based data lakes or data warehouses (or lakehouses).
    It is also likely to be in close proximity to major backbone network transport
    and interconnection facilities, compared to many edge locations that may be constrained
    by local wireline, wireless, or satellite access network capacity and capability.
    Proximity is not just a function of geographic distance, but network latency,
    which can be impacted by routing, congestion, bandwidth limitations, and outages.
    Latency Edge Latency from endpoints to the cloud may be too long for many applications.
    In the worst case, a remote cloud may be 150 milliseconds or more round trip from
    a given endpoint. Thus, an application that needs time at the endpoint, time at
    the remote server or serv-ers, and time to traverse the distance between them
    may take too long to meet user needs or application requirements. Especially for
    applications such as con-nected autonomous vehicles communicating with each other
    or with pedestrians or roadside infrastructure, low latency is critically important.
    A few hun-dred milliseconds of additional latency to communi-cate, perhaps via
    multiple round trips, may literally be a life-or-death matter. Many other applications
    can also benefit from low latency at the edge. These may be stateful and perhaps
    require local data which may be ephemeral or long-lived, say, to authenticate
    user access to certain services. Or they may be ephemeral and event-driven, say,
    image processing a license plate for upload to a criminal tracking or toll collecting
    database. Multiple image sensors may need to efficiently and rapidly col-laborate
    to correctly identify the vehicle. In any event, performant interactions between
    sensors, actuators, processors, and data may be critical. Cloud However, latency
    between various application components, microservices, or data spanning a distrib-uted
    edge may also be an issue. Applications and data have different characteristics,
    and thus the advan-tages and disadvantages of the cloud can differ. A single monolithic
    application running in a legacy or high-performance computing environment can
    benefit from tight coupling between modules. For example, such an application
    might exploit a hypercube or fat-tree network topology implemented in a high-performance
    backplane in a single physical system. But even a distributed architecture in
    a hyperscale cloud can have performance optimized via latency reduction enabled
    by proximity. For example, today, a typical application will be architected as
    a set of microservices interacting with each other across APls. Having all connected
    microservices co-located at a single cloud location or rack can minimize inter-service
    latency, and thereby maximize performance, through-put, and response time. At
    an even higher level, a typical workflow may consist of dozens of these applications
    or services. For example, the workflow to process a customer order may involve
    image capture, data validation against a customer, product, and/or geographic
    database, payment authorization, entry into a manufacturing planning system, and
    so forth. Even a few milliseconds extra delay may not seem like much, until one
    realizes that this may be multiplied by hundreds or millions of orders. Local
    Vs. Global We live in a diverse world. There are different coun-tries, on different
    continents, with different climates and geological characteristics. Even within
    any coun-try, there are subdivisions such as provinces, states, and special administrative
    regions. Then there are urban, suburban, and rural areas. Many localities have
    unique risks, challenges, economies, cost structures, environmental priorities,
    and legal and regulatory frameworks. Edge Rather than a one-size fits all approach
    globally, edge resources can vary to fit local requirements. For example, in a
    city, edge nodes can be very dense and sup-port 5G mmWave communications and high
    data volume of multiple Terabits per second per square kilo-meter. Backhaul can
    be high-capacity fiber. In a more rural area, a tradeoff can be made between cost,
    coverage, and density, perhaps using 4G LTE Advanced or 5G mid-band wireless with
    its better propagation characteristics and larger radius of coverage. Instead
    of fiber backhaul, integrated access and backhaul or a wireless or microwave mesh
    can be used for cover-age. Points of presence can be adapted to local conditions-for
    example, degree of ruggedness against heat or humidity. Cloud Cloud services can
    take advantage of the unique ben-efits of a specific region, and “broadcast” those
    ben-efits around the world. For example, data centers in Iceland can be more green
    than in other locations, because they can be powered by geothermal heat and cooled
    much of the year by the ambient environment, leading to a net-zero carbon footprint.
    These data cen-ters can run applications and services supporting needs around
    the world, for the limited marginal cost of networking. This is advantageous compared,
    to, say, a data center local to its users that is powered by coal-generated electricity
    and also needs to power its cooling systems. Data Sovereignty, Control, and Privacy
    There are a variety of unique, innovative ideas for siting processing, storage,
    and/or networking resources. For example, Facebook had examined solar-powered
    planes.4 Google had explored hot-air balloons with Project Loon,5 and floating
    or platformed offshore data centers are in service.6,7 However, most data centers
    are located on land, within the jurisdiction of nation-states with particular
    societal objectives and constraints embodied in laws, statutes, regulations, and
    executive orders. Edge Resources located at the edge can conform to regu-lations
    such as those that mandate that no personal data can leave a given geographic
    region, a boon for user privacy. In the case of three letter government agencies
    or military groups, the ideal approach is not just to have a firewall between
    the compute and data resources, but to air-gap them entirely to preserve security.
    Such control, privacy, and sovereignty are thus best maintained via local resources,
    such as the edge and/or endpoints. Of course, connected edge resources can still
    be attacked. For example, a steel mill in Germany suffered severe damage after
    hackers infiltrated its control systems, thus letting a furnace overheat and causing
    physical destruction.8 Even air-gapped resources can be attacked. The vector to
    carry out an attack on a uranium processing facility in Natanz, Iran, was the
    Stuxnet virus, brought in on a USB drive.9 Cloud Resources located in centralized
    fashion can be sub-poenaed and/or forensically analyzed by the appropriate legal
    authority. For example, the U.S. National Security Agency has had a program called
    “Prism” that allegedly enabled the NSA to have direct access to servers belonging
    to Google, Facebook, and Apple.10 This enabled direct government access to many
    types of data such as search queries and the contents of email and files. In the
    case of “keyword warrants,” a government can broadly request information on any-one
    who has used specific search terms, and for “geo-fence warrants,” a cloud or network
    service provider may have to share information concerning anyone within a given
    geographic region where a crime was committed. Although there may be diverse opinions
    on the benefits or concerns of such actions, from a technology perspective it
    is clear that having such data within a jurisdiction makes it easier to legally
    access.11 Data Management Practicality Sufficiently large datasets may be too
    large to store in their entirety at the edge, or too large to transport in their
    entirety to the cloud. Edge Endpoints are the first location for data capture
    via sensors, and the edge is typically the first stop on data''s journey to other
    destinations, such as peer endpoints or to the cloud. We say “typically the first
    stop,” because endpoints may be configured into a peer-to-peer mesh that packets
    traverse before they reach the edge. The volume of data that is captured may far
    exceed the capacity of access or edge networks to transport in a useful amount
    of time, requiring compression or selection of relevant subsets of the data. Cloud
    Consider the size of the index that Google builds as it crawls the web to enable
    search query processing. It has been estimated to be “hundreds of billions” of
    web pages amounting to “well over 100,000,000 gigabytes.“12 It is obviously not
    possible to store this much data in today''s popular endpoints, such as smartphones
    with 128GB of total storage, or even today''s edge nodes. As device and edge capac-ity
    grow, so will the size of databases such as that index, meaning that this constraint
    is unlikely to ever vanish. Data Replication Costs The larger that a collection
    of data is, and the more times that it is replicated, the higher the costs of
    storing and managing that data. On the other hand, if the data can successfully
    be partitioned, total storage capacity requirements and costs won''t vary very
    much between a few locations that aggregate the data and a multitude of locations
    with small portions of the data and few replicas. Edge The degree of replication
    vs. partitioning determines the overhead-if any-associated with edge storage.
    As an example, consider the storage requirements for 100GB of personal data such
    as family movies and photos. It requires 100GB whether stored in a smartphone,
    at the edge, or in the cloud, if only one copy is stored. On the other hand, replicating
    the same content multiple times takes a lot of aggregate storage. For example,
    there are tens of thousands of different types of devices running Android, for
    a total of over 3 billion devices.13 Depending on the version of the OS, it may
    need 10, 20, or even 30GB on each device, leading to a total storage of roughly
    100,000,000,000GB, or 100 exabytes. Cloud Suppose that instead of Android, we
    consider the scores of petabytes for Google''s search index. If we tried to store
    it in every one of the 3 billion Android endpoints, or the tens of billions of
    connected devices, it would obviously be very costly, if there were even that
    much storage capacity in the world. It is thus self-evident that massive data
    sets are unlikely to be replicated unnecessarily-there must be a compelling reason
    such as performance, control, or privacy to drive such replication. Statistical
    Effects on Resource Utilization Demand for processing typically varies over time
    for most applications and services. Workloads as diverse as ecommerce and connected
    vehicles have circadian rhythms, since very few people shop online or drive in
    the middle of the night-although these patterns may change as, say, underutilized
    autono-mous vehicle resources become pressed into ser-vice to make deliveries.
    Demand also varies based on events, such as the Olympics, or seasonal factors,
    such as tax-filing deadlines, Singles'' Day, or Black Friday. Generally speaking,
    the coefficient of variation-a measure of variance relative to a given mean level
    of demand-becomes lower as the number of independent aggregate workloads is increased-indicating
    that the aggregate demand is smoother, that is, has less variability. Such statistical
    multiplexing effects-such as serving diverse, uncorrelated demand from a pool
    of common resources-display well-known behaviors as the number of demands increases.14
    Edge Single workloads typically have great variability, whether it is playing
    Wordle once a day for a few min-utes, or doing a monthly backup for eight hours.
    The edge is typically an aggregation point for multiple workloads, such as image
    processing across multiple quality control inspection points in a factory or video
    stream processing of license plates or traffic congestion, or intermittently connected
    endpoints, such as vehicles connecting to base stations or other highly dense
    edge processing facilities. Interestingly, the 1/ n − − √ measure of coefficient
    of variation of the aggregate of n workloads implies that the reduction in variablity
    is steepest when n is smallest.15 In other words, not that many independent workloads
    need to be aggregated to achieve substantial capacity benefits. Cloud Hyperscale
    facilities that aggregate the most independent workloads have the lowest demand
    variability, because peaks in some workloads correspond to val-leys in others.
    Of course, not all real-world workloads are independent and uncorrelated, due
    to correlative factors such as circadian and seasonal rhythms. However, there
    will still clearly be benefits compared to the variability of a single workload,
    and, in the case of hyperscale clouds unused capacity can easily be filled with
    deferrable or discretionary work, incented via dynamic pricing. Statistical Effects
    on Profitability There is a direct relationship between resource utili-zation
    and profitability, therefore there are tradeoffs between cloud and edge that depend
    on their capac-ity, their cost structure, and the statistics of workloads running
    on their compute and network resources.16 For example, a facility that only runs
    at 50% utilization must recover not just the cost of a used resource, but also
    the cost of the matching unused resource. In such a case, if a server costs, say,
    1torunperday,includingleasing,depreciation,installation,maintenance,power,andsoon,itmustbe
    2 per day just to break even, not including sales, general, and administrative
    costs. A facility that runs at 100% uti-lization can make money, conceptually,
    only charging $1.01 per day. Of course, other factors, such as powering down or
    otherwise idling unused servers, consoli-dating workloads on virtualized servers,
    storage hier-archies, volume discounts, better PUE (Power Usage Effectiveness),
    etc., impact these calculations, but the general principle remains. The statistics
    of storage utilization also may make a difference. Sometimes storage requirements
    are monotonically nondecreasing, as more and more data is collected. Sometimes
    they vary up and down, as with a test/dev workload. In any event, the greater
    the workload variability, the greater the aggregate variability. The greater the
    number of workloads of a given variance, the smoother the aggregate behaves. Edge
    Edge resources can achieve higher utilization levels than most endpoints, but
    not as high as clouds, as described above. However, “profitability” may not be
    a major concern. A factory with a private edge and perhaps private 5G may have
    lower utilization than a cloud, but also does not need to spend money to fund
    a cloud provider''s profits. A city may choose to deploy edge resources to help
    with public safety, public health, emergency services, or to address the digital
    divide. A user deploying a residential gateway in their home is probably more
    concerned with functionality than with “making a profit” on family members Internet
    usage. Cloud Clouds and their hyperscale facilities that multiplex massive numbers
    of workloads and then also fill in any valleys with deferrable and discretionary
    work-loads will achieve the highest possible utilization levels. According to
    one estimate, cloud servers oper-ate at 65% utilization, whereas on-premises-such
    as facilities that are likely to combine fewer, less diverse workloads-are more
    likely to operate at 12-18% utilization.17 Statistical Multiplexing Efffcts on
    Resource Availability Automobiles typically have a 25% safety margin on tires
    (the spare tire in the trunk in addition to the four pressed into service) and
    a 5 to 10% safety margin on fuel (the reserve capacity after the meter shows “empty”).
    Similarly, data centers and networks are typically overprovisioned to provide
    a safety margin above and beyond what might be expected during “normal” peak demand.
    This can help ensure service continu-ity even during unexpected spikes, or during
    the fail-ure of some components. The degree of overprovisioning can depend on
    the variability of demand, the criticality of the workload, the supply chain availability
    of resources and other factors.18 Edge Edge resources disaggregate the overcapacity,
    or safety margin. Each resource must be engineered to allow for unplanned capacity
    issues or component fail-ures, the same way that every car must carry its own
    spare tire. This is the case even though the vast major-ity of these tires are
    never needed at any given time. On the other hand, if edge resources are config-ured
    into a mesh and can share capacity, then their geographic proximity can reduce
    these requirements, the same way that a neighborhood can get by with only one
    homeowner owning, say, a chain saw, and sharing it with neighbors when needed.
    Cloud Clouds require much less capacity to meet the same safety margins. The reason
    is that a demand spike in a few workloads is likely to be complemented by a demand
    trough in a few other workloads and near-average demand in others. (Both the analysis
    of cloud overhead capacity requirements and edge overhead capacity requirements
    assumes that the work-loads are independent and uncorrelated, an assumption which
    is sometimes violated due to circadian, weekly, or seasonal correlations). Clouds
    can also share capacity, either by running excess workloads in a different location
    or even a different cloud.19 Resilience and Security Clouds or edge nodes from
    a single given provider or using a given stack are subject to systemic outages-for
    example, due to a software bug, human error in updating configurations,20 or an
    unexpected clash of algorithms. Often, however, clouds and edge nodes differ in
    their resilience in the face of different events or failure modes. Edge Edge nodes
    may be located in a server room, a cor-ner of the factory, a street corner, along
    a traffic cor-ridor, or on a remote farm or oil pipeline. This makes them difficult
    to physically secure, because there are too many locations to guard, and often,
    they are space-constrained. On the other hand, loss of one or even several edge
    nodes is likely to be a drop in the bucket in terms of total processing capacity
    for an edge network. However, services offered may be impacted substantially within
    the localized service area of those lost nodes. For example, loss of nodes near
    the corner of Elm and Main St. may impact vehicle collision avoid-ance at that
    intersection. Cloud Cloud data centers and other centralized facilities can and
    do have excellent physical security. They typically have few windows, and have
    man traps, video surveillance, and even armed guards, sometimes from spe-cial
    services. On the other hand, a large data center is a large target for a flood,
    hurricane, tornado, lightning strike, power outage, or terrorist attack. In other
    areas, cloud security can have advan-tages. For example, the high access bandwidth
    of these facilities can better defend against Distributed Denial of Service attacks.
    The largest such attack as of this writing was a 3.47 Terabit per second DDoS
    attack from 10,000 servers, and was successfully stopped by a major cloud provider.21
    There are numerous other factors that may affect the balance of tradeoffs, such
    as provisioning, operations, configuration, administration, monitoring, maintenance,
    and management of architecture components. However, the bottom line is that various
    applications and organizations may benefit more from centralization or more from
    decentralization. That said, the exponential growth in data capture at endpoints
    is inexorably tilting that balance to the edge. Authors Figures References Keywords
    Metrics More Like This Demand Response in Data Centers: Integration of Server
    Provisioning and Power Procurement IEEE Transactions on Smart Grid Published:
    2019 RRect: A Novel Server-Centric Data Center Network with High Power Efficiency
    and Availability IEEE Transactions on Cloud Computing Published: 2020 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2022 Cloud Continuum, Cloud-Continuum 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Trade-Offs Along the Cloud-Edge Continuum
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 23 papers. The topics discussed include: designing
    neuromorphic architectures: towards an ultra-low power AI; creating an edge-to-cloud
    computing continuum: status and perspective; observability of software computing
    systems: challenges and opportunities; challenges in real-time scheduling for
    energy harvesting embedded systems; optimizing deep learning application for edge
    computing; toward memory-centric scheduling for prem task on multicore platforms,
    when processor assignments are specified; embedded Raspberry Pi vehicle detection
    and classification using single shot detector and dilated multi-column convolutional
    neural network; a combination of multi and univariate anomaly detection in urban
    irrigation systems; an innovative smart and sustainable low-cost irrigation system
    for smallholder farmers'' communities; and Bayesian convolutional neural networks
    for image classification with uncertainty estimation.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 2022 3rd International Conference on Embedded and Distributed Systems,
    EDiS 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 2022 3rd International Conference on Embedded and Distributed Systems, EDiS
    2022
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Goleva R.
  - Sokullu R.
  - Kadrev V.
  - Savov A.
  - Mihaylov S.
  - Garcia N.
  citation_count: '1'
  description: Well-being and agriculture works of people living not only in villages
    but also in towns nowadays, the healthy living environment at home, in the park,
    in the sports center, at work, local and remote monitoring of different parameters
    of the body, house, garden, the greenhouse is a matter of increased interest from
    families. Regardless of the size of the family, garden, villas, and fields, there
    is a need for support of integrated smart services in real-time and near-real-time
    forming an adaptable family software-defined Personal Enhanced Living Environment
    4.0 network configurable on the top of the existing public and private infrastructure.
    The scale of the network, the variety of Internet of Things parts, and the distributed
    edge and cloud computing services that are fragmented nowadays need to be integrated.
    The raw data created, data storage and data processing require a common edge-to-dew-to-fog-to-cloud
    approach and clear correlation to the related sectors such as water, land, house,
    factory, environment, parks, and infrastructure management. In this paper, an
    integrated approach toward services for different types of users based on the
    previously defined scenarios is presented. The services are software-defined and
    use existing infrastructure orchestrated resources that are unified, and allocated
    appropriately to support the functional and non-functional requirements. Private
    and public parts of the data, data flows, data space, and capacity for processing
    are considered.
  doi: 10.1007/978-3-031-17292-2_11
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Computer
    Science and Education in Computer Science CSECS 2022: Computer Science and Education
    in Computer Science pp 126–141Cite as Home Computer Science and Education in Computer
    Science Conference paper Real-Time and Near-Real-Time Services in Distributed
    Environment for IoT – Edge – Cloud Computing Implementation in Agriculture and
    Well-Being Rossitza Goleva , Radosveta Sokullu , Vassil Kadrev, Alexandar Savov,
    Svetoslav Mihaylov & Nuno Garcia   Conference paper First Online: 03 November
    2022 309 Accesses Part of the book series: Lecture Notes of the Institute for
    Computer Sciences, Social Informatics and Telecommunications Engineering ((LNICST,volume
    450)) Abstract Well-being and agriculture works of people living not only in villages
    but also in towns nowadays, the healthy living environment at home, in the park,
    in the sports center, at work, local and remote monitoring of different parameters
    of the body, house, garden, the greenhouse is a matter of increased interest from
    families. Regardless of the size of the family, garden, villas, and fields, there
    is a need for support of integrated smart services in real-time and near-real-time
    forming an adaptable family software-defined Personal Enhanced Living Environment
    4.0 network configurable on the top of the existing public and private infrastructure.
    The scale of the network, the variety of Internet of Things parts, and the distributed
    edge and cloud computing services that are fragmented nowadays need to be integrated.
    The raw data created, data storage and data processing require a common edge-to-dew-to-fog-to-cloud
    approach and clear correlation to the related sectors such as water, land, house,
    factory, environment, parks, and infrastructure management. In this paper, an
    integrated approach toward services for different types of users based on the
    previously defined scenarios is presented. The services are software-defined and
    use existing infrastructure orchestrated resources that are unified, and allocated
    appropriately to support the functional and non-functional requirements. Private
    and public parts of the data, data flows, data space, and capacity for processing
    are considered. Keywords Sensors Data sharing Edge Dew Fog Cloud computing technologies
    Smart agriculture Smart cities Smart home Smart well-being Access provided by
    University of Nebraska-Lincoln. Download conference paper PDF 1 Introduction Nowadays
    the work of the so-called smart systems is specific with its fragmentation. Regardless
    of using cloud services and the Internet of Things (IoT), devices the applications
    are separated and do not exchange data with few exceptions. The requirements of
    the market for an integrated simple and highly customizable service for a Personal
    Enhanced Living Environment are high. However, the Personal Enhanced Living Environment
    4.0 should be integrable, sharing, importing, and exporting data with a clear
    view of the privacy of the data. It is also expected to be distributed by nature
    and uses: 1) local, smart dust level computing facilities like gateways and controllers
    for different sensor devices at home, park, field, and factory; 2) dew computing
    level facilities such as home computers, tablets, smartphones; 3) fog computing
    facilities formed by the regional data centers; 4) cloud computing facilities
    formed by the global data centers [1]. In many papers, the smart dust and dew
    computing levels are covered by the common term edge computing. In some papers,
    even fog computing infrastructure is considered as part of the edge. The distribution
    of the data, data flows, clear identification of the private and public data,
    resources to be used at different computing levels, and orchestration and sharing
    of these resources are a matter of intensive development [2]. The new paradigm
    of distributed resources orchestration is expected to allow fully distributed
    processing of the data and better utilization of the network nodes. Well-being
    is a term that is considered often to be related to the Body Area Networks and
    body monitoring in general. However, in the context of this work, the term is
    enhanced with new features related to the living environment, and many things
    known as home automation, building automation, and smart agriculture. Smart environments
    and smart cities influence the complexity of the services prepared for the well-being
    platforms. Very often such platforms are also considered to be data-driven [3,
    4], i.e., changing behavior depending on the data. Another important point in
    the creation of the services for the Personal Living Environment 4.0 is the distinction
    between the real-time and near-real-time services for the customers. The term
    real-time is tricky and is interpreted by different authors differently. In this
    paper, there is a definition of services based on a clear explanation of the real-time
    requirements with a limit of round trip delay of 150 ms. This is the perception
    of the human beings for the delay. One could say that a service is offered in
    near-real-time when the round trip delay is close to 150 ms, i.e. up to one second.
    All services with a round trip delay bigger than one second are considered to
    be working in non-real-time. Finally, the new services could not be developed
    without the digitalization of sectors that feed the Personal Enhanced Living Environment
    4.0 with multidisciplinary data. However, many of the sectors are still too fragmented
    and use proprietary technology solutions. The structure of this paper contains
    a literature review of the recent papers in the field, a clear classification
    of the end-users of the platforms and expected services, classification of the
    services based on the functional and non-functional requirements and scenarios
    investigated, possible network architecture and technologies to be implemented
    and preliminary results in the controlled and real environment. 2 State of the
    Art The new term Personal Enhanced Living Environment 4.0 that is proposed in
    this paper is an extension of the previously well-defined platforms for Enhanced
    Living Environment, Ambient Assisted Living, and Personal Defined Networks. Many
    details of such a platform are already defined and worldwide experimented. However,
    the existence of a sustainable and customizable integrated solution is still missing.
    The main problems are the fragmentation of the business, different levels of digitalization
    of the sectors, the use of too many proprietary solutions, and the lack of appropriate
    data sharing rules between systems and platforms. In this paper, there is a trail
    to integrate different proprietary and non-proprietary solutions, create circumstances
    for data sharing, distributed resource orchestration, and allocation, and distributed
    data storage and processing. The integration of the agriculture and living environment
    will require the use of short-range and long-range sensor technologies. Data collected
    is often missing, corrupted, or late and appropriate processing could suffer from
    the lack of data and processing capacity. The existence of Long Range (LoRa) sensor
    technology is already proven to be implementable in the open space environment
    like sea coast, crop fields, meadows, forests, rivers, and smart cities [5]. The
    range of the technology is about 40 km. However, the limits in the power of transmission
    have happened to shorten this distance. This is the case in Europe. To avoid the
    development of infrastructure in rural places or in places where installation
    is almost impossible, part of the equipment and especially the gateways and controllers
    are developed to fly on drones or be carried by mountain vehicles, or transported
    by boats, or trucks. The efficiency of the sensor implementations in agriculture
    is of vital importance as it reflects the water use and reuse, smart ways to grow
    crops and manage the production, prediction of possible disasters, and ways to
    avoid disasters, i.e., making the agriculture more sustainable [6, 7]. A rich
    analysis of the possible scenarios in smart agriculture could be seen in [8].
    The work combines the use of different fixed and mobile approaches for data collection,
    the possibility to monitor different parameters of the crops, and has an implementation
    of artificial intelligence in data processing. The proposed in this paper modular
    approach to making the gateways interoperable with multiple sensor technologies
    at the same time is more universal. A similar approach toward smart agriculture
    is also presented in [9]. A rich analysis of the climate influence on smart agriculture
    is presented in [10]. The vast implementation of artificial intelligence in data
    processing and data correlation is well demonstrated. The problems in the typical
    living environments seem not to be similar to the ones in smart agriculture [11,
    12]. In many cases, the sensors are working in short range and the transmission
    power could be limited easily. However, when the Personal Enhanced Living Environment
    4.0 is distributed towards the parks, public transport, yards, and shops the problem
    with infrastructure interoperability, the distance of the transmission and collection
    and sharing of different types of data is again a matter of high importance. The
    aim is also to go further and create a cooperative solution for living and working
    environments as they could not be separated so easily during the last pandemic
    years because most of the people work from home. The problems in urban environments
    as in smart cities [13] are in the existence of too many and highly dependable
    parameters of different systems that could be analyzed. This creates circumstances
    for the use of multiple often diverse solutions. An approach to the data analyses
    is presented in [14]. The approaches require often the processing of the images
    obtained from different sources and careful correlation between the data obtained
    from the image analyses as seen in [15]. Work with medical devices, the necessity
    to use the network for healthcare is analyzed in many papers over the last decade
    such as [16]. Implementation of machine learning in activity recognition is demonstrated
    in [17]. Enhanced Living Environment platform architecture and testing could be
    seen in [18] and [19]. General approaches to the definition of IoT scenarios are
    taken into consideration in [20]. 3 Users’ Requirements The requirements of the
    users towards their personal defined network are usually high and depend very
    much on the culture, background, traditions, and habits of the people. The living
    and working environment is always expected to be comfortable and to consider additional
    features of life and work. The personal environment is part of the living and
    working environment of the people in the house and colleagues in the office. Therefore,
    data sharing and data correlation between different parameters need special consideration.
    The idea for Enhanced Personal Living Environment 4.0 is presented in Fig. 1.
    The proposal is to create a circumstance for the use of the integrated approach
    from any place and at any time, i.e., transparent in space and time. Recently,
    thanks to the vast digitalization of different sectors and increased use of IoT
    and cloud computing in houses, companies, parks, and factories it is possible
    to define and specify the requirements and main functions of platforms such as
    Living Environment 4.0, Working Environment 4.0, Personal Living Environment 4.0.
    The requests from the market for such a service are high. However, the solutions
    offered are still fragmented and proprietary by design without an appropriate
    level for data sharing and data correlation. Many of the services in the Personal
    Enhanced Living Environment 4.0 have to work in real-time, while quite a big amount
    of services also work in near-real-time. The meaning of these terms depends very
    much on the context to be implemented. For example, the alarms for falling, fires,
    floods, and high personal temperatures are expected to be created and processed
    in real-time. Parameters like temperature in the garden, soil humidity, and particle
    matters in the room and street might be detected and monitored in near-real-time.
    It is important to mention that in many cases the limit of 150 ms is relaxed.
    There are many examples such as Voice over IP communication, sensors-to-cloud
    communication, sensor-to-server communication where the limit could be up to 5
    s, and even more. Many IoT implementations when the sensors are sleeping exchange
    the data within the intervals of 5 s due to the specific features of the technology.
    Fig. 1. The idea of the enhanced personal living environment 4.0. Full size image
    All services need to be supported transparently in space and time using standard
    interfaces, protocols, and data formats, i.e., Wi-Fi, 3G, 4G, Ethernet, 5G interfaces,
    MQTT (Message Queuing Telemetry Transport), or similar protocols, and JSON (JavaScript
    Object Notation) or other similar data formats. 4 Primary, Secondary, and Tertiary
    Users The classification of the users is an important issue as they will require
    different functionality and be implemented over different infrastructure facilities
    and resources. From the point of the service use, one may distinguish three types
    of users: primary, secondary, and tertiary. Primary users are direct users of
    the services provided. These are people and their living and working environment.
    They also have: Their own Personal Enhanced Living Environment for the body Optional
    Personal Enhanced Living Environments of their relatives/ patients for the doctors
    and caring people, customers for management of specified part of the data House
    living environment access Garden living environment access Working living environment
    access School/shops, other public places living environment access Car/ infrastructure
    living environment access Primary users could support: Partial orchestration of
    resources Import of common data Export of part of their private data Secondary
    users have the same functionality as the primary users having additionally the
    possibility to extend the monitored devices and persons such as family members,
    colleagues, places, environment, and devices. Secondary users have: Their Enhanced
    Living Environment Personal Enhanced Living Environments of their relatives/ patients
    for the doctors and caring people, customers for management of specified part
    of the data House living environment Working living environment Car/public transport
    infrastructure living environment Secondary users should support also data sharing
    on their data and the data of the monitored people and network places. Tertiary
    users have additional roles in configuration management, support, insurance management,
    social analyses, medical data analyses, and public services analyses made for
    the communes, governmental and non-governmental organizations. Their network should
    support; Their Enhanced Living Environment Personal Enhanced Living Environments
    of their customers for configuration, statistics, management, monitoring of part
    of the data Infrastructure use and resources orchestration Infrastructure development
    Definition and specification of policies for infrastructure development and use
    of data Tertiary users share data on the infrastructure, many types of publicly
    available data for the climate, infrastructure, facilities, utilities, community
    policies, etc. 5 Scenarios Implemented at IoT Level Last decade many scenarios
    on the IoT level have been implemented in diverse fields and sectors. In [21]
    and [22] there is a detailed scenarios’ specifications for the home, person, and
    garden management. All scenarios are explored in a real environment. Scenarios
    are classified here for simplicity and used for the service creation process as
    well. Groups of scenarios to be implemented in the Personal Enhanced Living Environment
    4.0 platform are classified shortly in next sections. 5.1 House Automation Management,
    Office/ Factory Building Automation Management, Crops Management in the House,
    in the Greenhouse Common devices to be implemented in the house and garden could
    be summarised as doors and windows guarding, security systems, and utility devices
    for electricity, water, and gas supply. The results expected are data and appropriate
    visualization of the data for monitoring and management as well as setting of
    appropriate alarms for power failures, fire, flooding, gas leakage, etc. Implementation
    of the machine learning on the data collected at dew, fog, and cloud computing
    level could lead to the prediction of alarms and raising/decreasing of alerting
    level to the end-users. Many parts of the rooms in the houses and offices and
    halls in the factories could be a matter of passive and active monitoring and
    management as lamps, recuperators, heaters, coolers, humidifiers, dehumidifiers,
    purifiers, curtains, windows, doors, sockets, air quality sensors, and many others.
    5.2 Crops Management in the Yard, in the Park, in the Fields People in the villages,
    people living in the houses with yeards grow different crops. The Personal Enhanced
    Living Environment 4.0 implementation needs to be extended by IoT solutions for
    crop management in short and long distances. The water supply system may work
    depending on the environmental conditions such as sunlight, rain, the humidity
    of the air, soil, type of the soil, type of crops, and many others. Water sources
    need additional management for the levels, capacities, number of pumps, electricity
    supply, type of sensors and actuators implemented, etc. Additionally, the monitoring
    services could be optimized based on machine learning and passive/active on-site
    experiments. The aim is to optimize the use of energy, water, and electricity
    and reach an efficient and sustainable solution that is specific for every customer
    and climate zone. 5.3 Healthcare Scenarios Scenarios for healthcare implement
    sensors for both parameters measurements and management as well as the definition
    of different levels of notifications, messaging, alarms, and reports of the person
    under care. Types of the sensors could be classified into sensors for the body
    parameters (blood sugar, temperature, skin resistance, activity) and actuators
    for management such as panic cords, voice commands, noise sensing, alert levels,
    etc. 6 Functional and Nonfunctional Requirements of the Personal Enhanced Living
    Environment 4.0 The data collected from different parts of the distributed environment
    could be a matter of additional analyses and additional functions could be defined
    accordingly. The main functional requirements of the Personal Enhanced Living
    Environment 4.0 could be summarized as: measuring data monitoring data reporting
    data storing data processing data correlation of data visualization of data export
    of data import of data transformation of data verification of data validation
    of data comparison of data prediction of events analyses of events creation of
    alarm notification acknowledgment requests request for data hiding private data
    sharing public data collection of resources orchestration of resources allocation
    of resources unification of resources release of resources Non-functional requirements
    could be summarised as making the Personal Enhanced Living Environment 4.0: adaptable
    and scalable identifying resources having policies for resource allocation and
    release keeping the performance parameters identifying the missing data, missing
    parameters, and missing parts of the infrastructure being capable to perform a
    self-test of the infrastructure having self-configuration capabilities. 7 Services
    for Primary, Secondary, and Tertiary Users Services for the primary, secondary,
    and tertiary end-users are different. While the basic services are directly related
    to the functional requirements the added value monolithic services are combining
    many features and functions with the data from the existing infrastructure. Therefore,
    some additional services could be defined as additional functions of the platforms
    such as: Analyses of the efficiency of the electricity use Redistribution of the
    electricity using predefined algorithms or machine learning algorithms Security
    smart service Added value smart service for the garden and crop fields that are
    specific to the crops and environment Additional services for the thermal comfort
    of the crops in the greenhouses Value-added service for the light control Value-added
    service for the air quality Smart care adaptable services controlling medicines,
    body parameters, additional prescriptions, activities, training, dietary prescriptions
    Value-added multicast services for alarms and alerts Services for the control
    of the swarm behavior In Fig. 2 the main menu for the main agriculture management
    service is shown. There are submenus for the management of the greenhouse, irrigation
    system, water supply basin, greenhouse heating system, environmental parameters
    for the yard, and the technical parameters of the sensor IoT network. In the figure,
    the Signal-to-Noise Ratio (SNR) and Received Signal Strength Indication (RSSI)
    are related to the quality of wireless connection to the sensors. In Fig. 3 the
    submenu for the greenhouse is presented with measurements of the temperature and
    relative humidity (SM or soil moisture in the figure). Services for the secondary
    users are enhanced by possibilities to: add additional persons for monitoring
    drop persons for monitoring add a place for monitoring drop a place for monitoring
    activate/ deactivate service share data define policies for data sharing import
    data perform additional data visualization and analyses manage alarms and events
    implement machine learning for the prediction of events Services for the tertiary
    users are mostly related to the: data visualization data correlation data collection
    data analyses policy definitions policy implementation data validation data sharing
    correlation between the data and legislation service adaptivity and sustainability
    service scaling Fig. 2. The main menu of the greenhouse IoT system management.
    Full size image Fig. 3. Greenhouse submenu. Full size image 8 Network Architecture
    The network infrastructure is heterogeneous and supports both functional and non-functional
    requirements for the services. The adaptivity and the performance of the services
    depend very much on the existing infrastructure. The main home implementation
    with ZigBee and EnOcean sensors is presented in Fig. 4. Fig. 4. Home automation
    infrastructure (PLC - Programmable Logic Controller, HMI – Human-Machine Interface),
    Full size image The edge/smart dust-to-dew-to-fog-to-cloud computing is presented
    by sensors, gateways, controllers, local servers, and Internet connectivity. The
    software-defined and personal-defined networks on the top of the existing infrastructure
    have an interface locally or through the Internet using fixed or mobile devices.
    Data flows, data storage, and data processing are distributed among network devices.
    Figure 5 presents an automation infrastructure for the greenhouse, water irrigation,
    water supply, boiler for water heating, and mobile computing devices. The presented
    Personal Enhanced Living Environment 4.0 platform has currently more than 500
    sensors of different types and technologies, fixed and mobile gateways, some of
    them flying on a drone, controllers, and local and remote servers for data storage
    and processing. It is capable to raise alarms, uses the algorithms with the parameters
    obtained by the machine learning algorithms, monitors the infrastructure for its
    performance parameters such as batteries levels, link quality (Fig. 6), telegrams
    delays, telegram losses, validates the vitality of the data collected and is capable
    to predict some events as increased consumption, non-typical behavior, the necessity
    for reaction, lack of notifications and many others. Fig. 5. Garden automation
    (SCADA - Supervisory control and data acquisition). Full size image There is a
    need to mention explicitly that the validity of the data from sensors and removal
    of the outliers in the time series obtained from the sensors is also an important
    topic. It is expected that the data acquisition algorithms and machine learning
    classification algorithms could support the creation of a valid data sets at the
    edge. Creation of ontologies like SAREF (Smart Applications REFerence) and woking
    on a common data representation formats like JSON (JavaScript Object Notation)
    aim to support the standardization process of the data representations, flows
    and storing and enforce the data sharing. Fig. 6. Link quality to the sensor of
    a door, SNR between 6:08 and 6:18 pm. Full size image Reservation of the connections
    is another important issue in the technologies presented. Most of the sensors
    are working in multicasting mode or in a mesh retransmission mode. This is the
    natural sensor technologies way to support connectivity reservation. Some of the
    sensors are working in point-to-point mode. There is no reservation in the connections
    in these cases. However, the variety of the sensors, the multivariate analyses,
    the data dependencies and/or machine learning and data acquisition algorithms
    are capable to solve the problem of missing data. There is yet another way to
    support data reservation on the fly using so-called peer ports at application
    layer. The technology is implementatble at the edge, fog of cloud computing level.
    9 Conclusions and Future Work Plans Services in the Personal Enhanced Living Environment
    4.0 have been highlighted and classified towards primary, secondary, and tertiary
    end-users, real-time and non-real-time performance, and distributed orchestration
    of the data at the smart dust, dew, fog, and cloud computing. All services are
    based on the experimented scenarios and added value services have been identified
    based on the results from real environment implementations. The aim is to create
    more smart services based on machine learning algorithms that could predict events
    and manage risks in the distributed environment and implement also the distributed
    resource orchestration. The analyses continue with a more accurate estimation
    of the storage capacity, data flows and traffic of data flows, data processing
    requirements at edge/fog/cloud level and possible mapping to the resource allocation
    algorithms to make the services more sustainable. References Goleva, R., Mihaylov,
    S.: European Catalogue of ICT Water Standards and Specifications, p. 128. https://ec.europa.eu/digital-single-market/en/news/european-catalogue-ict-water-standards-and-specifications;
    https://op.europa.eu/es/publication-detail/-/publication/36d88e41-d6c3-11ea-adf7-01aa75ed71a1/language-es,
    https://doi.org/10.2759/39178, ISBN 978-92-76-17850-7. Last update: 5 August 2020,
    Shaping Europe’s digital future, REPORT / STUDY, Team responsible, Smart Mobility
    and Living (Unit H.5), Directorate-General for Communications Networks, Content
    and Technology (European Commission) (2020) Goleva, R., Asenova, P., Mihaylov,
    S., Siderov, D., Kadrev, V.: Sensor2Fog2Cloud2Application Platform for Smart Agriculture
    and Water Management. In: International Conference on Computer Science and Education
    in Computer Science CSECS 2020, Sofia, 5 Sept. 2020, ISSN 1313-8624, p. 5 (2020)
    Google Scholar   Saiz-Rubio, V., Rovira-Más, F.: From smart farming towards agriculture
    5.0: a review on crop data management. J. of Agronomy 10, 207 (2020). https://doi.org/10.3390/agronomy10020207,
    www.mdpi.com/journal/agronomy Anikwe, C.V., et al.: Mobile and wearable sensors
    for data-driven health monitoring system: State-of-the-art and future prospect.
    J. Expert Systems with Applications 202, 117362, ISSN 0957-4174 (2022). https://doi.org/10.1016/j.eswa.2022.117362.
    https://www.sciencedirect.com/science/article/pii/S095741742200714X Peinl, P.,
    Goleva, R., Ackoski, J.: Advanced system for the prevention and early detection
    of forest fires (ASPires). In: The35th ACM/SIGAPP Symposium on Applied Computing
    (SAC ’20), March 30-April 3, Brno, Czech Republic. ACM, New York, NY, USA, 4,
    pp. 1200–1203 (2020). https://doi.org/10.1145/3341105.3374052 Roy, R., Aslekar,
    A.: IoT in farm productivity enhancement. In: International Conference on Decision
    Aid Sciences and Applications (DASA), pp. 1034–1039 (2022). https://doi.org/10.1109/DASA54658.2022.9765273
    Basu, B., Kal, S., Ghosh, U., Datta, R.: SoftDrone: softwarized 5G assisted drone
    networks for dynamic resource sharing using machine learning techniques. J. Comp.
    Electri. Eng. 101, 107962, ISSN 0045-7906 (2022). https://doi.org/10.1016/j.compeleceng.2022.107962
    Elsayed Said Mohamed, E.S., Belal, A.A., Abd-Elmabod, S.K., El-Shirbeny, M.A.,
    Gad, A., Zahran, M.B.: Smart farming for improving agricultural management. The
    Egyptian J. Remote Sensing and Space Sci. 24(3 and Part 2), 971–981, ISSN 1110-9823
    (2021). https://doi.org/10.1016/j.ejrs.2021.08.007 Quy, V.K., et al.: IoT-enabled
    smart agriculture: architecture, applications, and challenges. Appl. Sci. 12,
    3396 (2022). https://doi.org/10.3390/app12073396 Article   Google Scholar   Rosenstock,
    T.S., Nowak, A., Girvetz, E. (eds.): The Climate-Smart Agriculture. Papers Published
    by Springer International Publishing (January 2019). https://doi.org/10.1007/978-3-319-92798-5,
    ISBNs 978-3-31-992797-8, 978-3-31-992798-5 IC1303 – Algorithms: Architectures
    and Platforms for Enhanced Living Environments (AAPELE) (2021). https://www.cost.eu/actions/IC1303/
    Dobre, C., Ganchev, I., Garcia, N., Goleva, R.: Introduction to enhanced living
    environment. In: Goleva, R., Ganchev, I., Dobre, C., Garcia, N., Valderrama, C.
    (eds.) Enhanced Living Environments: From Models to Technologies. The Institution
    of Engineering and Technology, pp. 1–20, ISBN: 978-1-78561-211-4 (2017) Google
    Scholar   Wu, M., Yan, B., Huang, Y., Sarker, M.N.I.: Big data-driven urban management:
    potential for urban sustainability. Land 2022 11, 680 (2022). https://doi.org/10.3390/land11050680
    Mavrevski, R., Milanov, P., Traykov, M., Pencheva, N.: Assessment of different
    model selection criteria by generated experimental data. WSEAS Transactions on
    Computers, ISSN / E-ISSN: 1109-2750 / 2224-2872, vol 16, Art. #30, pp. 260–268
    (2017) Google Scholar   Laskov, L.M.: Methods for document image de-warping. Astronomical
    and Astrophysical Transactions (AApTr), Cambridge Scientific Publishers 30(4),
    511–522 (2020) Google Scholar   Ivanov, I.E., Gueorguiev, V., Georgieva, D., Nenova,
    M., Ivanov, B.: Risk-based testing approach for medical devices software. In:
    Proceedings of the Technical University – Sofia 70(4), 41–48, ISSN 1311-0829 (2020).
    https://doi.org/10.47978/TUS.2020.70.04.025 Zdravevski, E., et al.: Improving
    activity recognition accuracy in ambient assisted living systems by automated
    feature engineering. IEEE Access J. PP(99), 5262–5280 (2017). https://doi.org/10.1109/ACCESS.2017.2684913
    Goleva, R., et al.: AALaaS and ELEaaS Platforms. In: Goleva, R., Ganchev, I.,
    Dobre, C., Garcia, N., Valderrama, C. (eds.) Enhanced Living Environments: From
    Models to Technologies. The Institution of Engineering and Technology, pp. 207–243,
    ISBN: 978-1-78561-211-4 (2017) Google Scholar   Autexier, S., et al.: End-Users
    AAL and ELE Service Scenarios in Smart Personal Environment. In: Goleva, R., Ganchev,
    I., Dobre, C., Garcia, N, Valderrama, C. (eds.) Enhanced Living Environments:
    From Models to Technologies. The Institution of Engineering and Technology, pp.
    101-131, ISBN: 978-1-78561-211-4 (2017) Google Scholar   Simeonova, T.: Development
    of Internet of Things. Monography. Aseventsi Publisher, Sofia, p. 362, ISBN 978-619-7586-25-1
    (2021) Google Scholar   2GreenHome web page: https://2greenhome.com/en/smart-home/.
    Accessed May 2022 Goleva, R., Kadrev, V., Savov, A., Koleva, Z., Draganov, P.,
    Mihaylov, S.: Scenarios to use sensors in agriculture, home, office, well-being.
    In: International Conference on Computer Science and Education in Computer Science
    CSECS 2021, Sofia, p. 7, ISSN 1313-8624 (18 Sept. 2021). https://www.ceeol.com/search/article-detail?id=978468
    Download references Acknowledgments This work is developed under support of multiple
    projects as: a) 2020–2021 Research and Development of Prototype of Multifunctional
    IIo LPWAN Communication Model for Wireless Data Transmission of Ecological Parameters
    Using Sensors – TeleEco, National Innovation Fund, Bulgaria; b) 2020–2024, CA19130
    - Fintech and Artificial Intelligence in Finance - Towards a transparent financial
    industry, https://www.cost.eu/actions/CA19130/; c) 2018–2021, Theoretical Models
    for Digital Agriculture Development, National Fund for Research, Bulgaria; d)
    2018–2019, 5P, Research and Development of a Prototype of the Platform for Continuous
    Monitoring of Vital Parameters and Processes, National Innovation Fund, Bulgaria;
    e) 2017–2019, European Civil Protection and Humanitarian Aid Operations, Advanced
    systems for prevention & early detection of forest fires 2016/PREV/03 (ASPIRES),
    granted by European Commission; f) 2013–2017, COST Action IC1303, Algorithms,
    Architectures and Platforms for Enhanced Living Environments (AAPELE), vice chair,
    granted by European Commission; g) 2022 Flying Forest Fire Fighting, Open Call
    project to Innovation Boosted by Small Flying Objects (UFO), granted by European
    Commission; h) 2022 Prototype Design of the LoRa® Repeater for Unified Signals
    and Channel Selection Management (LIOREPLICON), National Innovation Fund, Bulgaria.
    Author information Authors and Affiliations New Bulgarian University, Montevideo
    str. 21, Sofia, Bulgaria Rossitza Goleva Ege University, Izmir, Turkey Radosveta
    Sokullu New Bulgarian University, Montevideo str. 21, Sofia, Bulgaria Vassil Kadrev
    Comicon Ltd., Sofia, Bulgaria Alexandar Savov New Bulgarian University, Sofia,
    Bulgaria Svetoslav Mihaylov Computer Science Department of the University of Beira
    Interior, Covilhã, Portugal Nuno Garcia Corresponding author Correspondence to
    Rossitza Goleva . Editor information Editors and Affiliations Boston University,
    Boston, MA, USA Tanya Zlateva New Bulgarian University, Sofia, Bulgaria Rossitza
    Goleva Rights and permissions Reprints and permissions Copyright information ©
    2022 ICST Institute for Computer Sciences, Social Informatics and Telecommunications
    Engineering About this paper Cite this paper Goleva, R., Sokullu, R., Kadrev,
    V., Savov, A., Mihaylov, S., Garcia, N. (2022). Real-Time and Near-Real-Time Services
    in Distributed Environment for IoT – Edge – Cloud Computing Implementation in
    Agriculture and Well-Being. In: Zlateva, T., Goleva, R. (eds) Computer Science
    and Education in Computer Science. CSECS 2022. Lecture Notes of the Institute
    for Computer Sciences, Social Informatics and Telecommunications Engineering,
    vol 450. Springer, Cham. https://doi.org/10.1007/978-3-031-17292-2_11 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-17292-2_11 Published
    03 November 2022 Publisher Name Springer, Cham Print ISBN 978-3-031-17291-5 Online
    ISBN 978-3-031-17292-2 eBook Packages Computer Science Computer Science (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Download book PDF Download book
    EPUB Sections Figures References Abstract Introduction State of the Art Users’
    Requirements Primary, Secondary, and Tertiary Users Scenarios Implemented at IoT
    Level Functional and Nonfunctional Requirements of the Personal Enhanced Living
    Environment 4.0 Services for Primary, Secondary, and Tertiary Users Network Architecture
    Conclusions and Future Work Plans References Acknowledgments Author information
    Editor information Rights and permissions Copyright information About this paper
    Publish with us Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes of the Institute for Computer Sciences, Social-Informatics
    and Telecommunications Engineering, LNICST
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Real-Time and Near-Real-Time Services in Distributed Environment for IoT
    – Edge – Cloud Computing Implementation in Agriculture and Well-Being
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lv T.
  - Yu M.
  - Han Y.
  - Chen Y.
  citation_count: '1'
  description: Accurate ship tracking is one of the important means to implement accurate
    ship monitoring and reduce traffic accidents. The traditional ship tracking is
    based on the single sensor source such as VTS, AIS or CCTV to realize the data
    processing. The accuracy of single data source is not high, and it is difficult
    to meet the requirements of water transportation management. In order to improve
    the accuracy of ship tracking, a distributed ship tracking algorithm based on
    particle filter is proposed, and it integrates AIS and CCTV data. At the edge
    of CCTV, YOLO technology is used to identify ships and the identified status information
    is returned to the cloud. by the distributed parallel particle filter, the cloud
    realizes the prediction of ship state from AIS data, and the correction of ship
    state from CCTV data. The experimental results show that the proposed algorithm
    can track the ship state more effective and accurate than the traditional ship
    tracking algorithm.
  doi: 10.1109/ISCER55570.2022.00056
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2022 International Symposium ... A distributed
    ship tracking algorithm based on particle filter Publisher: IEEE Cite This PDF
    Taizhi Lv; Miao Yu; Yu Han; Yong Chen All Authors 1 Cites in Paper 42 Full Text
    Views Abstract Document Sections I. Introduction II. Related Work III. Distributed
    Ship Tracking Algorithm IV. Experiments V. Conclusion Authors Figures References
    Citations Keywords Metrics Abstract: Accurate ship tracking is one of the important
    means to implement accurate ship monitoring and reduce traffic accidents. The
    traditional ship tracking is based on the single sensor source such as VTS, AIS
    or CCTV to realize the data processing. The accuracy of single data source is
    not high, and it is difficult to meet the requirements of water transportation
    management. In order to improve the accuracy of ship tracking, a distributed ship
    tracking algorithm based on particle filter is proposed, and it integrates AIS
    and CCTV data. At the edge of CCTV, YOLO technology is used to identify ships
    and the identified status information is returned to the cloud. by the distributed
    parallel particle filter, the cloud realizes the prediction of ship state from
    AIS data, and the correction of ship state from CCTV data. The experimental results
    show that the proposed algorithm can track the ship state more effective and accurate
    than the traditional ship tracking algorithm. Published in: 2022 International
    Symposium on Control Engineering and Robotics (ISCER) Date of Conference: 18-20
    February 2022 Date Added to IEEE Xplore: 03 October 2022 ISBN Information: DOI:
    10.1109/ISCER55570.2022.00056 Publisher: IEEE Conference Location: Changsha, China
    Funding Agency: SECTION I. Introduction Water transportation has the advantages
    of large volume and low cos, and occupies an important position in the transportation
    industry [1]. Intelligent management of waterway transportation is the key for
    the sustainable development of waterway transportation, and information perception
    is the most basic and core work [2]. Maritime administration, wharf, ship owner
    and other institutions and departments are focusing on integrating perceptual
    data such as VTS (Vessel Traffic Services), AIS (Automatic Identification System)
    and CCTV (Closed Circuit Television) to realize ship dynamic tracking, dynamic
    management of ship sailing order, electronic monitoring of important water areas.
    Accidents occurred in water transportation lead a lot of losses every year, and
    effective ship tracking is an important means to reduce traffic accidents. VTS,
    AIS, CCTV and other perceptual data all play an auxiliary role in ship tracking,
    but the single data source has its own bottlenecks. There are some problems in
    the environment perception by a single sensor source, such as missed detection,
    low detection accuracy and low target signal-to-noise ratio [3]–[4]. In order
    to perceive the environment more accurately, ship tracking needs to use information
    fusion technology to process the multi-source heterogeneous information. For improving
    the accuracy of ship tracking, a distributed ship tracking algorithm based on
    particle filter is proposed. The algorithm realizes ship tracking by integrating
    AIS data and CCTV data which takes advantage of complementary data from different
    sources. The particle filter algorithm based on stream computing is used to realizes
    ship tracking by distributed parallel processing. SECTION II. Related Work A.
    Ship Tracking Based on Particle Filter Particle filter (PF) algorithm originates
    from Monte Carlo method, and can be used in any form of state space model. It
    provides an effective solution for the analysis of nonlinear dynamic systems,
    and has attracted extensive attention in the fields of target tracking, signal
    processing and automatic control [5]. The core idea of particle filter algorithm
    is to use the weighted sum of a series of random samples to represent the posterior
    probability density, and approximate the integration operation by summation [6].
    The main flow of particle filter is shown in Figure 1. Figure 1. The flowchart
    of particle filter Show All The main process of ship tracking algorithm based
    on particle filter is the same as that of the particle filter algorithm. There
    are specific definitions in the state transition equation, observation equation
    and particle information. In ship tracking process, each particle contains the
    coordinate information, motion information and feature information. Since the
    ship motion is relatively uniform, it is assumed to be a linear Gaussian system,
    and the state transition equation is shown in formula 1. x t =g( x t −1 , u t
    )=[ x x t−1 +ΔT( v c cos( θ x t−1 ) y x t−1 +ΔT( v t sin( θ x t−1 ) ]+ε (1) View
    Source Where x t is the Gauss coordinate ( x x t−1 , y x l−1 ) after Gauss-Kruger
    projection [7] at time t, u t is the control data, including the speed and heading
    of the ship, which is acquired from AIS data. ΔT is the sampling period, and ε
    is the control noise. The observation equation is shown in formula 2. z t ( m
    j )=h( X t , m j )=[ x t ( m j ) y t ( m j ) ] (2) View Source Where X t represents
    the ship position based on the world coordinate system. m j represents the j−th
    camera, and x t ( m j ) represents the image position of the j−th camera based
    on the image plane coordinate system. In order to achieve the coordinate consistency
    of control data and observation data, the transformation between the world coordinate
    system and imaging plane coordinate system needs to be realized. In order to realize
    this conversion, it includes the world coordinate system, camera coordinate system,
    image coordinate system and pixel coordinate system. The conversion between the
    four coordinate systems is shown in Figure 2. Figure 2. Coordinate system transformation
    Show All B. Ship Tracking System Ship tracking system mainly includes ship feature
    matching, ship search, ship status update and ship feature update. Feature matching
    is to extract the visual features of candidate regions (such as gray features,
    color features, contour features) and judge the probability that the candidate
    regions belong to the target by matching or classification. Ship search is a search
    method to find the target ship in the current image. Ship state update is the
    target tracking, which updates ship state by the ship motion model and observation
    model. Ship feature updating is to adapt to the feature changes of ships in the
    process of movement. Template updating and online classification are common methods
    of appearance model updating. SECTION III. Distributed Ship Tracking Algorithm
    A. System Architecture A distributed ship tracking system is constructed by the
    edge-cloud collaboration computing and stream computing technology. The system
    architecture is shown in Figure 3. Figure 3. The system architecture based on
    distributed ship tracking algorithm Show All In this architecture, the edge is
    responsible for acquiring and preprocessing sensor data, and transmitting the
    data to the cloud by Kafka message queue. The cloud realizes ship tracking based
    on distributed parallel computing cluster, and it is realized by Flink platform
    which is distributed processing engine for data stream [8]. HDFS (Hadoop Distributed
    File System) provides a distributed file system for ship tracking to store ship
    status, ship feature, particle information, etc. The application coordination
    service is implemented through Zookeeper which provides the consistency services
    for particle filter distributed parallel computing [9]. In Flink platform, the
    master node is implemented by JobManager, and the resource scheduling is implemented
    by ResourceManager. The work node is responsible for executing the calculation
    tasks of each particle prediction and update [10]. B. Image Target Detection Based
    on YOLO-V4 Yolo is an object recognition and location algorithm based on deep
    neural network [11]. Based on the original target detection architecture, Y 010-v4
    algorithm is optimized in varying degrees from the aspects of data processing,
    backbone network, network training, activation function, loss function and so
    on. In the Y 010-v4 framework, ship target recognition is solved as a regression
    problem. By dividing the image into fixed size grids, the confidence of the ship
    target and the probability of belonging to different ships are predicted in each
    grid, and the ship position and the ship target frame are regressed on the whole
    image. The process of ship target recognition is shown in Figure 4. Figure 4.
    The flowchart of ship target detection based on yolo-v4 Show All C. Distributed
    Parallel Particle Filter Based on Stream Computing The iterative update time directly
    rely on the number of particles. Each particle needs to maintain a separate ship
    state which requires large storage space and computing time. The particle number
    determines whether the algorithm can effectively represent the posterior probability
    distribution and ensure the diversity of particles. Too few particles make the
    distribution of particles to be far away the real state. Increasing the particle
    number will increase the computing time, and it causes that the ship tracking
    cannot meet the real-time requirements. In order to ensure the particles number
    and real-time computing at the same time, a distributed parallel particle filter
    algorithm based on flow computing is used to track the ship. The algorithm processing
    flow is shown in Figure 5. Figure 5. The flow of distributed parallel particle
    filter process Show All Firstly, the sensor data is transmitted to the cloud by
    the message queue. ReadTask reads the sensor data from the Kafka message queue,
    and distributes the data to the UpdateTask actuator to update the particles. The
    updated particles are distributed to the SampleTask and ResampleTask actuators
    to realize particle sampling and resampling. Finally, the ship tracking results
    are output by the WriteTask actuator. SECTION IV. Experiments In order to validate
    the performance of the proposed algorithm, experiments are designed to compare
    the single AIS data, single CCTV data and the proposed method. These experiments
    are carried out with ships sailing at the Yangtze River. The server with GPU is
    deployed at the bayonet for real-time ship target recognition, and a streaming
    computing cluster is built in the cloud for ship tracking. The cloud uses six
    servers with 32 cores to build the streaming computing by Flink platform and related
    components. Figure 6 shows a result of a ship tracking with different sensing
    sources. Figure 6. The experimental results for three methods Show All The red
    border indicates the ship tracking result with AIS data, the yellow line indicates
    the ship tracking result with CCTV data, and the green box indicates the ship
    tracking result with fusion AIS and CCTV data. It can be seen that the fused result
    is more accurate. Based on the edge-cloud cooperation and cloud distributed computing,
    the calculation time of single ship tracking is 1.67 seconds where 100 particles
    are used, and it can basically meet the real-time requirements. In order to verify
    the influence of different particle numbers on the ship tracking, two ships in
    the Yangtze River are selected for experiments by using the proposed algorithm
    and the traditional particle filter algorithm. The experimental results are shown
    in Table 1 and Table 2. Table I. Tracking errors with different particle numbers
    Too few particles will cause the ship tracking fail to meet the actual target
    trajectory, and the error will be significantly reduced when the particle number
    increases. It can be seen from Table 2 that the computing time will increase significantly
    with the increment of particle number. For the traditional particle filter algorithm,
    it is difficult to meet the real-time requirements where the particle number is
    too large. By distributed parallel computing based on stream computing, the proposed
    algorithm increases the computing time less where the particle number increases.
    If more servers are deployed, the execution time can be controlled at a lower
    level even if more particles are added. Table II. Computing time with different
    particle numbers SECTION V. Conclusion The safe navigation of ships needs the
    support of monitoring and detection technology, and timely early warning is the
    guarantee of safe driving. Ship tracking technology is one of the important means
    for marine intelligent management. It can effectively strengthen shipping management
    and reduce traffic accidents. CCTV, AIS, VTS and other perceptual data can be
    used for ship tracking processing. How to integrate multi-source heterogeneous
    data has become an important research direction of ship tracking technology. The
    technologies of edge-cloud cooperation and multi-source heterogeneous data fusion
    are used in the proposed algorithm. By integrating CCTV and AIS data by particle
    filter and carrying out distributed computing based on streaming computing, it
    only ensures the real-time performance, but also improves the accuracy of ship
    tracking. ACKNOWLEDGMENT This work was financially supported by the China postdoctoral
    science foundation (2019M651844), the natural science project of Jiangsu colleges
    and universities (21KJB580007), the funding of Jiangsu postdoctoral science foundation
    (2018K035C), the excellent scientific and technological innovation team of Jiangsu
    colleges and universities (Maritime big data team), the young academic leaders
    for Jiangsu colleges and universities QingLan project, and excellent teaching
    team of Jiangsu colleges and universities QingLan project (innovative teaching
    team of software technology specialty). Authors Figures References Citations Keywords
    Metrics More Like This Distributed Artificial Intelligence Empowered by End-Edge-Cloud
    Computing: A Survey IEEE Communications Surveys & Tutorials Published: 2023 On
    the use of artificial intelligence techniques in intelligent transportation systems
    2018 IEEE Wireless Communications and Networking Conference Workshops (WCNCW)
    Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings - 2022 International Symposium on Control Engineering and Robotics,
    ISCER 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A distributed ship tracking algorithm based on particle filter
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 45 papers. The topics discussed include: smart
    watch activity recognition using plot image analysis; single user activity recognition
    with density activity abstraction graphics algorithm; a multi-round bilinear-map-based
    secure password hashing scheme; proximity-based anomaly detection in securing
    water treatment; comparative analysis of deep learning models for network intrusion
    detection systems; negative sampling in variational autoencoders; leveraging fog
    computing for geographically distributed smart cities; greedy algorithm for edge-based
    nested community detection; clustering-based customer representation learning
    from dynamic transactional data; development of machine learning based model for
    anomaly detection and fault cause diagnosis of assets in petrochemical industries;
    evaluation of machine learning methods for predicting rainfall in Bangladesh;
    machine learning techniques applied to Bangla crime news classification; deep
    learning-based anomaly detection for imaging in autonomous vehicles; and neural
    network-based prediction for lateral acceleration of vehicles.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 2022 IEEE 2nd Conference on Information Technology and Data Science, CITDS
    2022 - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 2022 IEEE 2nd Conference on Information Technology and Data Science, CITDS
    2022 - Proceedings
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
