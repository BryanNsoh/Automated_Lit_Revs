- analysis: '>'
  authors:
  - Kumar P.
  - Raghavendran S.
  - Silambarasan K.
  - Kannan K.S.
  - Krishnan N.
  citation_count: '3'
  description: Farming has a plethora of difficult responsibilities, and plant monitoring
    is one of them. There is also an urgent need to increase the number of alternative
    techniques for detecting plant diseases, which is now lacking. The agriculture
    and agricultural support sectors in India provide employment for the great majority
    of the country’s people. In India, the agricultural production of the country
    is directly connected to the country’s economic growth rate. In order to sustain
    healthy plant development, a variety of processes must be followed, including
    consideration of environmental factors and water supply management for the optimal
    production of crops. It is inefficient and uncertain in its outcomes to use the
    traditional method of watering a lawn. The devastation of more than 18% of the
    world’s agricultural produce is caused by disease attacks on an annual basis.
    Because it is difficult to execute these activities manually, identifying plant
    diseases is essential to decreasing losses in the agricultural product business.
    In addition to diagnosing a wide range of plant ailments, our method also includes
    the identification of infections as a prophylactic step. Below is a detailed description
    of a farm-based module that includes numerous cloud data centers and data conversion
    devices for accurately monitoring and managing farm information and environmental
    elements. This procedure involves imaging the plant’s visually obvious signs in
    order to identify disease. It is recommended that the therapy be used in conjunction
    with an application to minimize any harm. Increased productivity as a result of
    the suggested approach would help both the agricultural and irrigation sectors.
    The plant area module is fitted with a mobile camera that captures images of all
    of the plants in the area, and all of the plants’ information is saved in a database,
    which is accessible from any computer with Internet access. It is planned to record
    information on the plant’s name, the type of illness that has been afflicted,
    and an image of the plant. In a wide range of applications, bots are used to collect
    images of various plants as well as to prevent disease transmission. To ensure
    that all information given is retained on the Internet, data is collected and
    stored in cloud storage as it becomes essential to regulate the condition. According
    to our findings from our research on wide images of healthy and ill fruit and
    plant leaves, real-time diagnosis of plant leaf diseases may be done with 98.78%
    accuracy in a laboratory environment. We utilized 40,000 photographs and then
    analyzed 10,000 photos to construct a DCDM deep learning model, which was then
    used to train additional models on the data set. Using a cloud-based image diagnostic
    and classification service, consumers may receive information about their condition
    in less than a second on average, with the process requiring only 0.349 s on average.
  doi: 10.1007/s10661-022-10561-3
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Environmental Monitoring
    and Assessment Article Mobile application using DCDM and cloud-based automatic
    plant disease detection Published: 28 October 2022 Volume 195, article number
    44, (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Environmental Monitoring and Assessment Aims and scope Submit manuscript Parasuraman
    Kumar, Srinivasan Raghavendran, Karunagaran Silambarasan, Kaliaperumal Senthamarai
    Kannan & Nallaperumal Krishnan  536 Accesses 3 Citations Explore all metrics Abstract
    Farming has a plethora of difficult responsibilities, and plant monitoring is
    one of them. There is also an urgent need to increase the number of alternative
    techniques for detecting plant diseases, which is now lacking. The agriculture
    and agricultural support sectors in India provide employment for the great majority
    of the country’s people. In India, the agricultural production of the country
    is directly connected to the country’s economic growth rate. In order to sustain
    healthy plant development, a variety of processes must be followed, including
    consideration of environmental factors and water supply management for the optimal
    production of crops. It is inefficient and uncertain in its outcomes to use the
    traditional method of watering a lawn. The devastation of more than 18% of the
    world’s agricultural produce is caused by disease attacks on an annual basis.
    Because it is difficult to execute these activities manually, identifying plant
    diseases is essential to decreasing losses in the agricultural product business.
    In addition to diagnosing a wide range of plant ailments, our method also includes
    the identification of infections as a prophylactic step. Below is a detailed description
    of a farm-based module that includes numerous cloud data centers and data conversion
    devices for accurately monitoring and managing farm information and environmental
    elements. This procedure involves imaging the plant’s visually obvious signs in
    order to identify disease. It is recommended that the therapy be used in conjunction
    with an application to minimize any harm. Increased productivity as a result of
    the suggested approach would help both the agricultural and irrigation sectors.
    The plant area module is fitted with a mobile camera that captures images of all
    of the plants in the area, and all of the plants’ information is saved in a database,
    which is accessible from any computer with Internet access. It is planned to record
    information on the plant’s name, the type of illness that has been afflicted,
    and an image of the plant. In a wide range of applications, bots are used to collect
    images of various plants as well as to prevent disease transmission. To ensure
    that all information given is retained on the Internet, data is collected and
    stored in cloud storage as it becomes essential to regulate the condition. According
    to our findings from our research on wide images of healthy and ill fruit and
    plant leaves, real-time diagnosis of plant leaf diseases may be done with 98.78%
    accuracy in a laboratory environment. We utilized 40,000 photographs and then
    analyzed 10,000 photos to construct a DCDM deep learning model, which was then
    used to train additional models on the data set. Using a cloud-based image diagnostic
    and classification service, consumers may receive information about their condition
    in less than a second on average, with the process requiring only 0.349 s on average.
    Similar content being viewed by others Plant disease detection using drones in
    precision agriculture Article Open access 28 March 2023 Medicinal Plant Identification
    in Real-Time Using Deep Learning Model Article Open access 07 December 2023 Machine
    learning for leaf disease classification: data, techniques and applications Article
    Open access 18 October 2023 Introduction Agricultural pesticides are sprayed on
    ill plants in order to enhance yield and profitability; nevertheless, farmers
    must be properly monitored at all phases of the process. For a number of reasons,
    farmers are unable to maintain a constant gaze on their crops while working. Incorporate
    a farm robot that can autonomously identify damaged crops and automatically apply
    the pesticides necessary to treat the diseased crop, thereby significantly decreasing
    the amount of effort required by farmers to handle the issue. Working agriculture
    robots can move through a paddock, trying to identify weed growth and sprayers
    chemical in which they are required, even bottom to a specific leaf. The robot
    also tries to learn what’s not recognized as a weed and tries to avoid watering
    on a flourishing crop. Production of raw food products that are essential for
    the existence of all humans on this planet is what agriculture is defined to be.
    Agriculture has played a key role in human history for thousands of years and
    is still going strong now. Since its start, a number of technological breakthroughs
    have helped to increase the rate of growth in this area. Agriculture employs around
    one-third of the world’s total human population, according to some estimates.
    Following several technological advancements, industrialized countries have implemented
    various automation technologies to reduce the amount of human labor required.
    However, as a result of these technological advancements, developing countries
    are experiencing some difficulties in the agricultural sector. India has the eighth
    largest agricultural land area in the world, yet it has the 84th greatest kilogram
    yields per hectare of land, according to the United Nations Food and Agriculture
    Organization. Although agriculture employs more than half of the country’s workforce,
    it is not the most profitable sector. Low farming techniques and poor soil quality,
    as well as a lack of education and understanding of contemporary agricultural
    methods and technology, are all contributing factors to the current state of agriculture
    in the USA and throughout the world. Farming is a method for which farmers work
    with environment instead of against, and where origin is incorporated into agriculture.
    It employs ecosystem-based approaches, which commonly require more understanding
    and human labor per acre than chemical-based strategies to homogeneity and basic
    production. Farmers nowadays are not yet aware of the impact that various meteorological
    variables, such as moisture and temperature, have on their crop yields. Although
    there are no efficient or cost-effective means of employing the same to enhance
    efficiency and production, mobile phones and wireless Internet access are becoming
    increasingly widespread across the country. These are the subjects that contemporary
    researchers are most interested in: the study of many various types of plant illnesses,
    the finding of treatments, and the increase of yields. In order to prevent diseases
    from spreading across the plant and to monitor the facility on a regular basis,
    an automated system must be installed. Images were processed in order to estimate
    the amount of illness that had spread throughout the leaf and to treat the leaves
    with the necessary pesticides. Various procedures, such as detecting the disease
    kind and spraying pesticides with the assistance of a Cloud Data Center, might
    be included in this category. It is claimed that cloud data is used to interface
    between a mobile camera that takes photographs and a display that saves the images
    to a remote location in the cloud. The constant monitoring of crops and the streaming
    of information are two of the most crucial components of agricultural production.
    Following the collection of images, the following steps of image analysis are
    carried out: acquisition, preprocessing, segmentation, grouping, and other phases
    of image analysis shown in Table 1. Table 1 Summarizes the findings of several
    research projects Full size table This can minimize the amount of work necessary
    on the farm while simultaneously increasing output. The concept of employing image
    processing technology through the Python OpenCV platform is utilized for leaf
    disease diagnostics through disease detection, and the OpenCV platform is used
    to do this. OpenCV is a big open library for machine learning, deep learning,
    and image analysis, and now it plays a major part in includes the practice, which
    is crucial in today’s modern systems. It can process pictures and videos to detect
    objects, faces, or even human writing skills. The OpenCV platform is used to accomplish
    this. In most cases, the identification of leaf sheets is done by the use of distinguishing
    features such as pattern, color, texture, and disease kind, among other things.
    Because it helps them to minimize their efforts and working hours, farmers gain
    from this automation. Our approach allows us to distinguish between healthy and
    diseased leaves, and the farm robot then sprays pesticides into the sick leaves
    to prevent them from becoming sick. Modern equipment for automatically diagnosing
    and categorizing herbal diseases do not exist, according to the old technique
    of diagnosis and classification. If this problem is not handled immediately, it
    will result in a decrease in the number and quality of agricultural products,
    as well as a reduction in revenue. It is critical to undertake continuous agricultural
    monitoring on big farms, and this necessitates the hiring of more employees and
    professionals. For professional guidance from specialists in their area, farmers
    may be forced to travel considerable distances. Agrarian production is the backbone
    of our economy and one of the most significant human activities in our nation.
    An agrarian society, also recognized as an agricultural society, is any group
    whose economy depends on the creation and maintenance of crop production and farmland.
    Growing the property is the main symbol of income in an agricultural society.
    Agricultural production is use of agriculture and animal products to sustainable
    way improve person’s existence. Food products, fuel sources, natural fiber, and
    natural resources are the 4 types. Crops and living creature products are being
    used to grow food, pet food, and non-food products destined for human consumption.
    If we look at the agricultural sector, water is by far the most often utilized
    resource. A large quantity of water is saved by irrigation, which is a win-win
    situation. It is possible that on larger farms, the ancient manual irrigation
    method employed on agricultural land may necessitate the hiring of skilled employees.
    Plant diseases and their symptoms Agricultural pesticides are sprayed on ill plants
    in order to enhance yield and profitability; nevertheless, farmers must be properly
    monitored at all phases of the process. For a number of reasons, farmers are unable
    to maintain a constant gaze on their crops while working. Incorporate a farm robot
    that can autonomously identify damaged crops and automatically apply the pesticides
    necessary to treat the diseased crop, thereby significantly decreasing the amount
    of effort required by farmers to handle the issue. Production of raw food products
    that are essential for the existence of all humans on this planet is what agriculture
    is defined to be. Agriculture has played a key role in human history for thousands
    of years and is still going strong now. Since its start, a number of technological
    breakthroughs have helped to increase the rate of growth in this area. Agriculture
    employs around one-third of the world’s total human population, according to some
    estimates. Following several technological advancements, industrialized countries
    have implemented various automation technologies to reduce the amount of human
    labor required. However, as a result of these technological advancements, developing
    countries are experiencing some difficulties in the agricultural sector. India
    has the eighth largest agricultural land area in the world, yet it has the 84th
    greatest kilogram yields per hectare of land, according to the United Nations
    Food and Agriculture Organization. Although agriculture employs more than half
    of the country’s workforce, it is not the most profitable sector. Low farming
    techniques and poor soil quality, as well as a lack of education and understanding
    of contemporary agricultural methods and technology, are all contributing factors
    to the current state of agriculture in the USA and throughout the world. Farmers
    nowadays are not yet aware of the impact that various meteorological variables,
    such as moisture and temperature, have on their crop yields. Although there are
    no efficient or cost-effective means of employing the same to enhance efficiency
    and production, mobile phones and wireless Internet access are becoming increasingly
    widespread across the country. These are the subjects that contemporary researchers
    are most interested in: the study of many various types of plant illnesses, the
    finding of treatments, and the increase of yields. To prevent diseases from spreading
    across the plant shown in Figs. 1 and 2 and to monitor the facility regularly,
    an automated system must be installed. Fig. 1 Affected leaf types Full size image
    Fig. 2 Types of leaves Full size image Images were processed in order to estimate
    the amount of illness that had spread throughout the leaf and to treat the leaves
    with the necessary pesticides. Various procedures, such as detecting the disease
    kind and spraying pesticides with the assistance of a Cloud Data Center, might
    be included in this category. It is claimed that cloud data is used to interface
    between a mobile camera that takes photographs and a display that saves the images
    to a remote location in the cloud. The constant monitoring of crops and the streaming
    of information are two of the most crucial components of agricultural production.
    Following the collection of images, the following steps of image analysis are
    carried out: acquisition, preprocessing, segmentation, grouping, and other phases
    of image analysis. This can minimize the amount of work necessary on the farm
    while simultaneously increasing output. The concept of employing image processing
    technology through the Python OpenCV platform is utilized for leaf disease diagnostics
    through disease detection, and the OpenCV platform is used to do this. The OpenCV
    platform is used to accomplish this. In most cases, the identification of leaf
    sheets is done by the use of distinguishing features such as pattern, color, texture,
    and disease kind, among other things. Because it helps them to minimize their
    efforts and working hours, farmers gain from this automation. Our approach allows
    us to distinguish between healthy and sick leaves, and the farm robot subsequently
    sprays pesticides into the diseased leaves using the information it has gleaned
    from the process. Following that, a review of the foundations of bacterial, viral,
    and fungal diseases and infections is presented to the students. Bacterial illnesses,
    often known as bacteria, may produce a range of symptoms, including plant overcrowding,
    leaf spots, scabies, and carcinoma, among other things (cancerous tumor). Bacterial
    infections present with signs and symptoms that are quite similar to those of
    fungal diseases. The most frequent symptom of a bacterial illness is leaf spot,
    which is caused by a bacterium. Disease caused by a virus: Viruses are responsible
    for a number of different diseases in plants. The recognition and analysis of
    viral infections are not tough tasks when it comes to viruses in general. Crouching
    leaves, yellowed foliage, and slowed development in the plant are all symptoms
    of the viral disease. A number of viruses, including the tobacco mosaic and tomato
    virus, the potato virus, the cauliflower mosaic virus, and others, are among the
    most dangerous viral diseases that may harm crops shown in Fig. 3. Fig. 3 a Bacterial
    diseases, b viral diseases, c fungal diseases, d mixed diseases, e leaves’ effect
    Full size image Automated classification and identification of plant diseases
    is made feasible via the use of machine learning (ML) algorithms, which are extremely
    beneficial in the classification and identification process. Plant health evaluations
    and the prediction of early-stage plant disease outbreaks are made easier with
    the use of machine learning. As time passed, new machine learning models, such
    as the SVM, VGG, R-FCN, Faster R-CNN, and SDD, were developed to replace the old
    ones. Researchers utilized them in their experiments on image identification and
    classification, which they carried out for a number of reasons, including identifying
    faces in photographs. Several of these technologies are employed in agricultural
    system automation to increase the efficiency of the system. The current development
    of cloud-based services and effective deep learning algorithms has fueled the
    search for a practical and scalable solution to agricultural issues. We discovered
    that the bulk of the photos in the PlantVillage dataset are either gray or white;
    however, the situation in the actual world is different and may consist of a range
    of different hues and degrees of gray, depending on the scenario. This issue has
    the potential to cause low accuracy or erroneous prediction in models that have
    only been trained on a consistent background. This research challenge was addressed
    using a photograph from a Plant Village dataset that is publicly available to
    the public (Ferentinos, 2018), as well as photos collected from Tarnab Farm. To
    address this research problem (Agriculture Research Institute, Pakistan), in order
    to categorize and detect various illnesses, the deep convolutional neural network
    (DCNN) was trained and tested in the cloud environment. The DCNN is based on the
    DeepLens Classification and Detection Model (DCDM). DeepLens is a configurable
    video camera which allows customers to test with machine learning, artificial
    intelligence, and the Internet of Things. The DeepLens camera links to AI services
    hosted on AWS’ cloud platform. An object detection model can determine which of
    a known group of items is visible as well as provide information about their places
    within the image. Object detection is a computer vision procedure for detecting
    and locating objects in the image or video. Particularly, object recognition draws
    boundary boxes around objects in the scene, enabling us to evaluate in which these
    objects are in a provided scenario. A DCDM training program was completed in order
    to develop a scalable and efficient real-time classification and identification
    model on the Internet of Things (IoT) device known as AWS DeepLens. By definition,
    when you start creating a training job by using API, Sage Maker reproduces the
    data set on ML compute resources. Set the S3DataDistributionType field to ShardedByS3Key
    to have Sage Maker recreate a subset of data on each ML calculate instance. The
    low-level SDK could be used to set the above field. An image sensor with a resolution
    of 4 MP and Deep Learning (DL) technology is utilized for the integration and
    execution of machine learning–related projections in Table 2. Table 2 The CAE
    network’s layer-by-layer configuration Full size table Related works In the current
    state of the study, the detection of plant sickness in a complex environment is
    primarily concerned with three aspects: image segmentation, feature extraction,
    and disease diagnosis. Image segmentation The most difficult problem in a complicated
    environment is identifying how to segment images while leaving sick plants intact,
    as the fundamental goal of image division is to distinguish symptom information
    from background information. Image segmentation is the process of dividing up
    an electronic image into different subgroups recognized as image objects that
    can make it easier for users of the image and thus end up making image processing
    easier. It has been the subject of extensive research by a large number of researchers.
    In 2017, Ali et al. used the Delta E color variation method to distinguish the
    region impacted by the disease, and they published their findings in the journal
    Scientific Reports in 2017. In general, image segmentation shown in Fig. 4 is
    accomplished by the use of four fundamental approaches, which are discussed in
    further depth in the next paragraph (Ni et al., 2019). A convolutional neural
    network is a type of neural network that is specially designed to handle pixel
    data in pattern recognition systems. A convolutional neural network is a deep
    learning algorithm that really can start taking in a picture, allocate importance
    to different aspects in the picture, and differentiate one from another. Fig.
    4 Segment image Full size image Some scientists use a combination of the region
    of interest approach and additional photo segmentation techniques to create their
    images. Image segmentation is a crucial stage of the picture identification system
    since it extracts the items of interest for the further processing including such
    characterization or appreciation. Image segmentation is often used in training
    to categorize image pixels. To offer an example, Kao et al. (2019) claimed in
    their article that the co-evolutionary autoencoder was used to identify the image’s
    ROI as the background filter, claiming that this was done with the help of the
    background filter. It is only meant for the division of geographical regions that
    the second approach is used. The study by Pujari et al. (2013) states that photographs
    were split into several sections, each of which had a distinct significance, and
    photographs were extracted from each section (Chopda et al., 2018). Akram et al.
    (2017) pioneered a novel image processing paradigm that combines synchronous real-time
    processing in addition to traditional image processing. Increasing contrast and
    vector characteristics may be achieved by splitting the image into three color
    spaces, and it is possible to identify a particularly notable location by separating
    the image into three color spaces (Chad et al., 2017). Several other researchers
    have chosen to segment and identify images by the application of deeper learning
    techniques, which they characterize as follows: It has been hypothesized by Marko
    et al. (2019) that a two-stage technique, based on the creation of a target detection
    algorithm, can be used to optimize plant disease image identification accuracy.
    Finally, the barrier is frequently crossed in the segmentation process itself.
    An article by Li et al. (2019) was released on January 19, 2018, in which they
    described their work on developing a gray histogram-based multi-level thresholding
    technique for photo segmentation. Mohamed and Diego suggested a novel multi-target
    metaheuristic (Pujari et al., 2013) that was based on a multi-object optimization
    method in the segment gray image (Li et al., 2019), which was based on a multi-object
    optimization approach in the segment gray image (Li et al., 2019). Multi-objective
    optimization is a branch of numerous decision-making that needs to deal with arithmetical
    optimization algorithms involving different optimization methods that must be
    adapted at the very same time. A single reality should not be overlooked, notwithstanding
    the abovementioned circumstances. Color information in a complicated context is
    difficult to interpret, and machine vision algorithms based on color, ROI, and
    threshold are inefficient for real-world applications because of this difficulty.
    Feature extraction Identification of plant diseases is complicated for a variety
    of reasons, which complicates the process of extracting plant disease. Feature
    extraction is a dimension reduction method of reducing an original batch of original
    data to more manageable chunks for handling. The large number of factors in such
    large data sets requires a huge number of computer power to process. The tactile
    properties of an image include texture patterns, form, color, movement, and other
    aspects that are critical for the extraction of illness symptoms from an image,
    among other things (Luna et al., 2018; Silva Abade et al., 2019). Color and texture
    traits were used to build a method for extracting disease regions from photographs,
    which Raza and his partners developed (Dhiraj & Ghattamaraju, 2019). Utilizing
    the variance in the D–s decision rules for the theory of D–S evidence, a method
    has been developed for the extraction of the Dempster–Shafer (D–S) theory of proofs
    and multi-feature fusion using the variance in the D–s decision rules for the
    theory of D–S evidence (Pertot et al., 2012). One such technique is local binary
    patterns (LBPs), which uses a locally calculated LBP value to convert an image
    to grayscale and evaluate its R and G channels while considering the entire region
    (Kao et al., 2019). Li and colleagues (2019) investigated an IoT extraction of
    characteristics for the smart city that was based on the deep migration model,
    which they referred to as “deep migration extraction.” We can respond to and engage
    with the music by extracting essential audio functions from the song and incorporating
    them into the visuals in the shown figure as Fig. 5a and b (Patil & Kumar, 2016).
    The LBP’s first algorithmic step is to produce an intermediate picture that effectively
    identifies the actual image by highlighting facial characteristics. To achieve
    this, the algorithm utilizes a template matching theory based on the parametric
    radial distance and neighborhood. Recent research has presented a slew of innovative
    techniques for the implementation of function extraction, and many more are now
    in the process of being created. Meziani et al. (2019), for example, devised two
    novel spectral estimators that are resilient to non-Gaussian, nonlinear, and non-stationary
    signals, and which were entrusted with the arduous task of extracting important
    and unique characteristics from the electroencephalogram (EEG). Furthermore, Liu
    et al. demonstrated that utilizing the residual neural network and the enhanced
    signal-to-trim ratio, it was possible to remove the high-dimensional time-frequency
    spectrum characteristics. Xu et al. (2019a) have developed a technique for extractor
    functions based on the Hilbert marginal spectrum, which they believe will help
    to minimize the wear of milling tools. Fig. 5 a–c Viewing the function map of
    a DCDM sample leaf convolution layer Full size image Disease identification The
    development and testing of a large number of approaches is underway in order to
    provide reliable findings in the field of suitable identification. It was feasible
    to use class labels for training photos and to construct a fine-grained image
    classification system (Hu et al., 2017) by employing an identification model,
    which was then implemented. The process of classifying and trying to label organizations
    of pixel resolution or vectors inside an image based on particular rules is known
    as visual classification. One or even more spectrograph or levels of quality can
    be used to create the classification law. An approach to capture plant disease
    leaf (Raza et al., 2018) has been described by Zhang et al. (Raza et al., 2018),
    shown in Fig. 6. Fig. 6 Input, grayscale, and segment images Full size image Images
    that are based on hybrid clustering are displayed. Pandit et al. released a study
    in 2017 in which they described a content-based image recovery system that was
    utilized for the extraction of texture characteristics and mean values, as well
    as the classification of support vector machines. Developing grading systems,
    as well as image analysis algorithms for the extraction and identification of
    numerous features shown in Fig. 6, was the major objective of this project. As
    a result of the aforementioned study, further cutting-edge techniques to more
    consistently and accurately identifying the illness have recently been created
    (Yang et al., 2019). Researchers have discovered that a unique approach based
    on the selection of images and brief written descriptions has allowed non-experts
    to detect plant illnesses that may be accessed remotely from a personal computer,
    a smartphone, or a digital assistant. Pertot et al. (2012) use mobile phones to
    capture bad harvests in real time as they occur. Additionally, mobile devices
    are being utilized to segment the leaf image and identify patches using better
    clustering of K-means, which is accomplished through the usage of mobile devices
    (Mangalraj et al., 2019). An image detecting microscope based on synergistic evaluation
    of texture and form attributes, as well as the tree-confusion matrix decision
    (Coulibaly et al., 2019), has been suggested by Yang et al. (2019). Another advantage
    of the neural network is that it can be used in a number of ways to diagnose illnesses,
    which is another advantage. According to Chad et al. (2017), a technique for automatically
    diagnosing plant disease in photos of maize plants was created by utilizing photographs
    obtained in the field. Ni and colleagues (Zhang et al., 2019a) utilized a deep-seated
    neural network to train 1632 maize kernel images and then used the deep-seated
    neural network to build an automated maize detector that could detect corn (Akram
    et al., 2017). In order to detect rice illnesses, Lu et al. created a DCNN method,
    which was published in the journal Science (Vasan et al., 2020). Using deep learning,
    Zhang et al. developed a system for identifying images of agricultural machinery,
    which they published in the journal Nature in 2015. The deep neural convolution
    network was developed by Zhang et al. (2018) to increase the precision of maize
    leaf disease diagnosis, the process of determining the presence of a disorder,
    situation, or damage based on its signs or symptoms. Health records, medical examination,
    and exams such as blood work, imaging studies, and tissue samples could be used
    to assist in the diagnosis (Lu et al., 2017). Two deep learning–based identification
    architectures, AlexNet and VGG-16 net, have been created (Xu et al., 2019b) and
    images have been incorporated into both of these designs. AlexNet can identify
    objects were off, and the large percentage of its top five classifications for
    each image are acceptable. Coulibaly and colleagues (Zhang et al., 2016) have
    suggested a strategy for building an identification system that takes use of transfer
    training for feature extraction. Deep learning in plant identifying diseases can
    prevent the drawbacks of genetic manipulation of disease point features, start
    making plant pathogen feature extraction extra objective, and enhance study effectiveness
    and future technologies’ transition speed (Iqbal et al., 2018). This technique
    is based on transfer training for feature extraction. AlexNet increases the speed
    by six times whereas the preserving the same precision. To deal with overfitting,
    use dropouts rather than normalization. Even so, with a dropout rate of 0.5, the
    learning curve is increased (Zhang et al., 2019b). The high hardware resource
    requirements, as well as the high-quality, conventional neural network models
    and huge volumes of training data sets, need the use of a substantial amount of
    time during the training process, which is not beneficial to the model under consideration.
    It is proposed that a transfer learning model be used in conjunction with a previously
    trained model to train the model utilizing illness leaves for the purposes of
    this study. Three distinct techniques were utilized to arrive at the aforementioned
    conclusions, including segmentation of the leaf image, extraction of features
    from leaf lesions, and detection of leaf diseases, among others. According to
    earlier statements, there are still several obstacles to overcome in order to
    identify plant illnesses in such a complicated environment. Image processing techniques
    and feature extraction techniques In this section of the article, the methods
    for extracting the various features that are used by the classification strategy
    are discussed in detail. As a starting point, the background must be eliminated
    from the image pixels, which are responsible for making up the plant component
    of the image. In order to make things easier, it is assumed that the plant component
    has been photographed against a background that is significantly brighter or darker
    than the plant section under consideration. When shooting a citrus fruit on a
    tree, this criterion would not be necessary because the fruit’s brilliant color
    would make it easy to distinguish it from the tree’s other green foliage, which
    would make the image unnecessary. It would be considerably more difficult, on
    the other hand, to isolate a single leaf from a tree in a photograph, and it would
    demand the deployment of an image processing strategy that may be significantly
    more complicated than, for example, the segmentation technique used for the rest
    of the image regions. One of two techniques can be used to distinguish the white
    backdrop from the rest of the image. To begin, the grayscale version of the image
    is used (background separation method 1 (BSM1)), and any pixels with a gray level
    larger than a threshold Bg are considered to be in the background, unless otherwise
    specified. A similar assumption is made with respect to the second approach (BSM2),
    which is that the three color levels of the RGB backdrop pixels are relatively
    close together and have a very high value (for example, > 200) in comparison to
    the first technique (BSM1). When the values of R = 255, G = 255, and B = 255 are
    all equal, the color white is obtained. When a photograph is taken in the shadow,
    on the other hand, the backdrop gets somewhat darker and the color levels fall
    substantially lower, albeit they still remain about equal in this case as well.
    It is determined that a pixel I with color levels Ri, Gi, and Bi is the background
    if all of the following conditions are satisfied: $$\\begin{array}{l}R_{i} > \\,
    c_{1} , \\, G_{i} > \\, c_{1} , \\, B_{i} > \\, c_{1} , \\, \\left| {R_{i} - \\,
    G_{i} } \\right| \\, < \\, c_{2} , \\\\ \\left| {R_{i} - \\, B_{i} } \\right|
    \\, < \\, c_{2} , \\, \\left| {B_{i} - \\, G_{i} } \\right| \\, < \\, c_{2}\\end{array}$$
    It is not required to know the exact values of the constants c1 and c2, as their
    significance is not in any way determined by them. In the preceding example, selecting
    c1 = 200 and c2 = 30 resulted in good outcomes in all of the cases that were investigated.
    Also, if the backdrop is darker (for instance, black), it may be possible to distinguish
    between the plant component under investigation and other plant components by
    seeing how close their grayscale or color levels are to one another. The downside
    of using low-complexity background separation is that it is less accurate. The
    problem with this method is that if the color of a plant part’s region is near
    to the color of the backdrop, it may be mistaken for the backdrop and be identified
    as such. If the confused region does not extend beyond the boundary of the plant
    part, the question of whether the confused zone is a part of the plant part or
    a part of the backdrop may be readily answered. In order to do this, it is necessary
    to determine if the confusable zone is contained inside the perimeter of the inspected
    leaf or fruit. Disease detection conventional techniques One of the most essential
    components is the use of digital imaging and machine learning. The other two are
    the detection of plant disease and categorization of plants. Images are captured,
    noise reduction is applied, images are segmented, and characteristics are manually
    extracted. Finally, machine learning algorithms are used to the images after they
    have been processed. For ailment characterization, AI models are utilized to arrange
    illnesses dependent on what they look like in an image. It is shown in Fig. 7
    that the basic technique to identifying and categorizing plant diseases is described
    in the next section. The many components, such as the pre-processed image, will
    contain operations such as image processing, among other things. Fig. 7 Approach
    for disease detection and classification Full size image In a general method,
    each stage includes steps such as filtering, noise reduction, and image scaling.
    Many different approaches may be used to segment an image, including edge detection
    (such as that used by Sobel and Canny), clustering using k-means, thresholding
    using otsu, and other techniques. Extracting functions from data can be accomplished
    through the use of histograms with oriented gradients and robust speed up, color
    and structural features, LBP, and various classification methods, such as the
    NB classifier, SVM, DT, Boosting Trees, R FR, NN, and logistic regression, among
    other techniques, to name a few results to build in Table 3. Table 3 The suggested
    hybrid model’s layer-by-layer specifications Full size table Difference between
    machine learning and deep learning Deep learning is a component of machine learning,
    but the manner in which the data is fed into the system is essential to its effectiveness.
    The detailed discussion of structured data models and machine learning technologies,
    in which profound learning is based on the layer of ANN, is presented in this
    chapter. The majority of machine learning approaches rely on manually gathering
    features from data in order to detect plant illnesses and classify them. They
    naturally prepare for them while they are immersed in their studies and learning
    shown in above Fig. 8. Fig. 8 Disease detection and categorization two distinct
    techniques Full size image Cloud transfer learning A machine learning concept
    called transfer learning (TL) describes a technique that collects basic knowledge
    to solve a specific problem and then uses that information to tackle other problems
    that are more or less comparable [49]. Transfer learning is a term that refers
    to the process of taking basic knowledge and applying it to other problems. This
    approach compels us to utilize any critical problem that cannot be solved because
    there is insufficient data to do so in order to keep up with the times. Therefore,
    the amount of data that is suppressed for training and testing has been reduced
    to the same level as previously, and the data should be provided separately [50].
    A substantial amount of time and a big number of data are required to train CNN.
    In some instances, such as when the dataset is small, it is feasible that employing
    TL is a beneficial technique. For the DCDM training, we started from the ground
    up and built TL from the ground up. It was determined that leveraging the Amazon
    Cloud infrastructure and Cloud DeepLens would be the most effective way to overcome
    the scalability limitations. For the creation and deployment of applications on
    the Amazon cloud, the Amazon cloud architecture makes data gathering, data transfer,
    and computing resources easier than they would be otherwise. It is possible to
    access a wide range of services and applications using cloud computing. A software
    engineering platform assists with the construction, preparation, and deployment
    of the system, and machine learning models are validated as they are developed.
    In addition to Cloud Services, the trained model may be used on other suitable
    platforms, such as Cloud DeepLens, if they are available. Proposed technology
    Furthermore, while no research has yet resulted in the development of a farmer-friendly
    intelligent gadget, this notion paves the way for smart Internet of Things–based
    mobile apps that may detect early plant illnesses that are recognized by farmers
    and help them increase crop yields. Plant illness may be difficult to treat, which
    is why many people are afraid to visit a farm or even have an outdoor garden because
    of the difficulties in treating plants that get ill as a consequence of plant
    disease. The objective of this research is to detect sick plants using real-time
    data obtained from plant leaves, which will be used in future research. In order
    to achieve this aim, the DCDM, which is a deep learning technique, is employed.
    A small amount of post-processing was done to the images, with rotation and image
    brightness calibration being the most noticeable changes. It is possible to diagnose
    any leaf, regardless of whether it is healthy or diseased, and regardless of the
    species. In keeping with the IoT idea, a mobile application for this project is
    being developed that can image crops that are acquired on mobile devices and provide
    output diagnostics. Because of the application result, it is possible to carry
    out agricultural operations that are suited for the current circumstances. Figure
    9 is the whole structure of proposed work. Fig. 9 Proposed plant disease detection
    architecture Full size image Step 1 It is necessary to collect classification
    model training data in order to begin the application procedure at the beginning.
    The resultant image may be used to access and evaluate numerous other models stored
    in a database, which can then be used to treat the patient further: Distance of
    the photo–1ft Field–white sheet Hardware for the camera–Oneplus5T 16mpf/1.7 (usually
    a smartphone may be used for a camera) No image taken–100 (healthy 50, illness
    50) equal distribution Step 2 Pre-processing raw data is a critical step in ensuring
    that the model’s accuracy and reliability are as high as possible. A higher amount
    of input data is required in order to accomplish a deep learning paradigm. The
    following steps are using near increase the records: The dataset has been expanded
    to include 400 images as a result of image rotation. With the use of the brightness
    modification approach, the data set has been expanded to 3600 images: Problem
    modeling based on the data collected: The architecture of Convolution’s architecture
    for healthy and sick plants is intended to be categorized. This stage is focused
    to the development of the most appropriate model for the given circumstance. The
    detailed explanation is provided in the following sections. If the plants are
    suffering from illness, the categorization shows the severity of the ailment.
    Image classification in real time and model validation: The validation data position
    is use to evaluate the piece of the CNN. Activation features and optimizer are
    tuned to ensure that the model suffers the least amount of loss. Activation optimizers
    are algorithms or methodologies which are used to modify the characteristics of
    the neural network, such as weight training and learning rate, in order to reduce
    losses. By reducing the purpose, optimization techniques are used to solve the
    optimization problems. Step 3 Smart, farmer-friendly Internet of Things mobile
    application An inverted version of the photograph recorded by the mobile device
    is sent to the server in advance of the image being displayed. The features of
    the image recorded are extracted in the following phase, and the model we have
    created is classified according to the image and transferred to a predefined output
    format for end-users to use. The application depicted in Fig. 3 is one in which
    the user is invited to take an image of a leaf using the graphical user interface.
    A graphical user interface is a software program that allows individuals to interact
    with a computer by using symbols, graphic metaphors, and devices. The graphical
    interface is now the industry standard, and its elements are becoming distinctive
    cultural artifacts in their very own right. Before the photo is transmitted to
    the trained server model for processing, it must first be processed by the trained
    server model. Following that, the model predicts and generates the results of
    the experiment. Whenever a function provides this result, the end-user can inspect
    it in order to determine whether or not the leaf is healthy. End-users will receive
    this result as a result of the function’s operation. In the event that this is
    not the case, a diagnosis will be established. Following the design of the model,
    the mobile device serves as a client and the host serves as a server, with the
    host serving as the client. It is possible to utilize the localhost server at
    this moment to replicate the cloud server environment for the sake of the current
    testing processes. The connection between the client and server must first be
    established on a local level before the connection can be formed between them.
    A function gets this information and displays it to the end-user, allowing them
    to judge whether the leaf is healthy or not, as indicated by the forecast label,
    and whether the leaf is healthy or not, as indicated by the forecast label. If
    the test is positive, the results will reveal what sort of sickness the animal
    is suffering from. In this case, the mobile device acts as a client, while the
    local host acts as a server, as defined by client-server architecture. To simulate
    the current cloud server arrangement as part of the current testing phase, the
    local host server will be used to conduct testing. In the beginning, the client-server
    connection is formed on a local computer. When utilizing this design, the application
    browser serves as the client, giving users with many choices for exploring and
    evaluating the models and analyzers that have been made accessible. A POST request
    is used to upload the photo to a server’s uploads folder after that, and the image
    gets posted whenever the user clicks on the Predict button. When the server receives
    the image, it immediately begins processing it and putting the necessary packs
    and trained models into memory. The script runs the model a second time, this
    time using the image data that was supplied, and the anticipated results are returned
    to the application via the web interface. When using the program’s graphical user
    interface, users may verify the accuracy of the model as well as other information
    by clicking on the analyzer button located at the bottom of the screen. Considering
    that the HTTP solicitation and reaction are the correspondence convention for
    this application, the HTTP model incorporates each of the qualities of this application.
    Adaptability is made conceivable because of the way that the application is made
    out of a few autonomous modules, which implies that the user interface can be
    upgraded without influencing the backend (Python), as well as the other way around,
    and that the backend contents can be changed without influencing the front end.
    Deep learning development model This review incorporated the refinement of the
    pre-prepared convolutional neural organizations, which were then used to prepare
    the model (CNNs). Google, Inc. utilized three CNNs in this review: VGG16 with
    23 layers, VGG16 with 20 layers, and VGG16 with 20 layers (from the Visual Geometry
    Group at the Department of Engineering Science at the University of Oxford) Inception-V3
    has 159 levels and is positioned number ten (from Google, Inc.) 12, and MobileNetV2
    with a sum of 88 layers (likewise from Google, Inc.) 13. Roughly 1.28 million
    pictures (1000 thing classifications) from the 2014 ImageNet Large Scale Visual
    Recognition Challenge12 were utilized to prepare all CNNs in anticipation of the
    opposition. Because of the way that the recognizable proof issue in this review
    was impressively unique in relation to the trouble in ImageNet, we utilized exchange
    learning and tweaking of the first CNN loads for the 2-class plants laterality
    arrangement work. From that point forward, the model’s last two levels were thawed
    to consider more preparing to occur. The models start with a worldwide normal
    pooling layer, which is trailed by a group standardization layer, a dropout layer
    (50%), and two thick layers, all of which add to the last model’s yield (512 hubs
    and 1 hub). The feature maps’ measurements are lowered by pooling layers. As a
    consequence, it decreases the time to learn and the number of calculations done
    in the network. A 224,224 lattice was delivered by introducing the picture pixels
    to fill it with values going from 0 to 1, bringing about a sum of 224,224 lines
    and segments. This was done to make the organizations match the ones that were
    at that point pre-prepared. It was chosen to apply information increase methods
    to work on the dataset. These strategies included arbitrary level flipping, shading
    immersion, splendor, and difference varieties, in addition to other things. Preparing
    with a minibatch slope drop of size 32 and an Adam enhancer learning pace of 0.0001
    brought about further developed combination. Profound learning models and calculations
    were made utilizing the TensorFlow system (Google; variant 2.1.0) in mix with
    the Keras API, which is accessible from the Apache Software Foundation (v.2.2.4).
    Practically, each of the models were prepared on a Ubuntu 16.04 working framework
    running on an Intel Core i7-2700 K CPU running at 4.66 GHz, 128 GB of RAM, and
    a NVIDIA GTX 1080 Ti GPU with 12 GB of memory, with every one of the models being
    prepared on a Ubuntu 16.04 working framework. Verification of the DL model The
    slope-weighted class activation mapping (CAM) method was utilized to acquire a
    more complete arrangement and exhibition of the DL model. This method was utilized
    to underscore the region where the DL model might be generally centered on the
    discovery of plant laterality. Class activation maps are a strong classification
    technique used during machine learning. It facilitates the researcher to inspect
    the picture to be classified and decide which components of the picture helped
    contribute more to the model’s final result. Class activation maps are a simple
    method to get the racist and discriminatory input images used by a CNN to define
    a particular class in the picture. CAM is a strategy that was created by the University
    of California, Berkeley. A harsh restriction guide of the picture’s fundamental
    locales is created by a CNN after it has wrapped up handling a picture utilizing
    the class-explicit inclination data that comes into the last convolutional layer
    of the CNN’s preparing. Utilizing this technique, it is feasible to decipher network
    yield and decide if the organization has taken in any significant provisions over
    the long haul. It was additionally conceivable to assess the qualities of the
    extricated various leveled highlights from the DL model14 utilizing a strategy
    known as t-disseminated stochastic neighbor implanting (t-SNE). This procedure
    (512 components from the InceptionV3 model) took into account instinctive assessment.
    In this review, t-SNE was used as an apparatus for picturing high-dimensional
    information, for example, the DL includes that were utilized in the examination.
    By changing over similitudes between information focuses, you can decrease the
    Kullback–Leibler dissimilarity of the joint likelihood between the low-dimensional
    implanting and the high-dimensional information, just as the Kullback–Leibler
    difference of the joint likelihood between the low-dimensional inserting and the
    high-dimensional information. Comparisons between humans and machines Furthermore,
    we led human–machine correlations and ensure that the DL model was just about
    as precise as could really be expected. It was important to analyze the reference
    decisions delivered by the DL models with the reference decisions made by human
    graders in the investigation; along these lines, the outside testing dataset (76
    essential look pictures) was utilized. In this review, three ophthalmologists
    with shifting degrees of clinical experience (L.JH with 1 year of involvement,
    C.XP with 10 years of involvement, and H.YP with 20 years of involvement) were
    told to settle on autonomous choices about each testing picture in the wake of
    being dazed to the information assortment measure. We additionally investigated
    and ordered the misclassified photos utilizing CAM that was created from the outer
    testing dataset, zeroing in on the three most usually saw highlights: (1) plants
    with existing together plant conditions, for example, evolutional posies, trichomegaly,
    or misled lashes; (2) plants with other photograph conditions, like overexposure
    or low openness and askew; and (3) plants with coinciding plant conditions and
    coinciding plant conditions and coinciding plant conditions and coinciding (erroneous
    named). Statistical investigation To lead the measurable examinations, the Python
    (form 3.6) and Scikit learn libraries were utilized related to the R programming
    language (Anaconda Python, Continuum Analytics). Execution was assessed as far
    as precision, affectability, and particularity, just as two-sided 95% certainty
    spans, in addition to other things. It was stated that the conditions for computing
    the exactness, affectability, and explicitness were supported by the way that
    $$Accuracy = \\frac{TP + TN}{{TP + FP + TN + FN}}$$ (1) $$Sensitivity = \\frac{TP}{{TP
    + FN}}$$ (2) $$Specificity = True\\frac{TN}{{TN + FP}}$$ (3) The terms genuine
    up-sides (TP), genuine negatives (TN), bogus up-sides (FP), and bogus negatives
    (FN) are characterized as follows: When we check out this model, we can see that
    our model created the appropriate estimates, while our model made the inaccurate
    expectations (TP and TN). Table 4 shows the after-effects of figuring esteems
    from the disarray grid for the 80–20 split subsequent to registering values from
    the disarray network. Table 4 Report on the classification and performance of
    models Full size table Demonstrative exactness condition 1 examinations are characterized
    as those where the precision, affectability condition 2, and particularity condition
    3 of a symptomatic test are accounted for in accordance with the Standards for
    Reporting of Diagnostic Accuracy Studies (STARD) 15. A methodological quality
    research determines the ability of one or even more diagnostic tests to maintain
    a high degree participant in the study as having a particular goal. The viability
    of our DL model was surveyed utilizing estimations of the space under the beneficiary
    working trademark bend (AUC), which were utilized to measure its exhibition. DCDM
    structure We made a CNN model with two convolutional layers, one as an info layer
    and the other as a yield layer, and prepared it. [i1, i2,…, ir] and O = [o1, o2,..,..,
    goodness] address the info and yield vectors, separately, where r shows the number
    of things in the information highlight set and h indicates the number of classes,
    and I addresses the number of components in the information include set. The essential
    objective of the organization is to find a compacted portrayal of the dataset.
    At the end of the day, it endeavors to move toward the character work F, which
    is characterized as the accompanying: $$F_{W, B} \\left( I \\right)\\sim 1$$ (4)
    where W and B are the loads and predisposition vectors for the entire organization,
    individually. The enactment work f in the covered up and yield neurons is picked
    to be a log sigmoid capacity, as found in the chart. It is characterized as follows:
    The strategic capacity in the t space is a specific example of the log sigmoid
    capacity s, which is characterized as follows: $$s\\left( t \\right) = \\frac{1}{{1
    + e^{ - t} }}$$ (5) The decision boundaries in the feature space are defined by
    the weights of the DCDM network, and the discriminating surfaces that arise from
    this process may categorize complicated boundaries. During the training process,
    these weights are adjusted to correspond to each new training picture that is
    presented. In general, supplying the DCDM model with more pictures improves the
    model’s ability to detect plant diseases with greater accuracy. For training the
    DCDM model, we utilized the back-propagation technique, which has a computational
    cost that is proportional to the amount of time spent. $$\\sum \\left( {w_{i,j}
    *Y_{j} } \\right) + \\mu_{i}$$ (6) Assuming the associations between neuron j
    and I are weighted, the load on the associations between neuron j and the I are
    equivalent to the yield worth of neuron j; and the limit incentive for a neuron
    is equivalent to the pattern contribution to neuron I when there are no extra
    data sources. On the off chance that the worth of w_(i,j) is negative, it is named
    as an inhibitory worth and is taken out from thought since it lessens all out
    input. The preparation calculation is isolated into two sections: the forward
    stage and the opposite stage. The organization’s loads stay consistent during
    the forward stage, while the info information is communicated through the organization
    layer by layer during the converse stage. Not set in stone that the forward stage
    has finished when the mistake signal ei estimations meet as follows: $$e_{i} =
    \\left( {d_{i} - o_{i} } \\right)$$ (7) For this situation, di addresses the planned
    (target) and oi addresses the real yields of the preparation picture, separately.
    It is during the retrogressive stage that the blunder signal ei is communicated
    across the organization the other way to the forward stage. This progression includes
    the use of blunder adjustments to the DCDM organization’s loads to limit ei. For
    every neuron weight, we used the slope plunge first-request iterative enhancement
    strategy to decide ∆wi,j the weight change. The strategy is portrayed as follows:
    $$\\Delta w_{i,j} = - n\\frac{\\delta \\varepsilon \\left( n \\right)}{{\\delta
    e_{j} \\left( n \\right)}}y_{i} \\left( n \\right)$$ (8) The intermediate output
    of the preceding neuron n is denoted by the symbol \\({y}_{i}(n)\\), wherein is
    the learning rate and (n) denotes the error signal in the whole output. The value
    of \\(\\epsilon (n)\\) is determined as follows: $$\\epsilon \\left(n\\right)=\\frac{1}{2}\\sum_{j}{e}_{j}^{2}\\left(n\\right)$$
    (9) Results and outcomes It is built as a mobile application that incorporates
    the capacity of sickness identification using deep learning, following the suggested
    strategy for developing a disease application. Farmers and those planning to enter
    the farming and gardening sectors will be better equipped to detect illnesses
    early on and take preventative steps to avoid the spread of diseases and achieve
    the desired output with the help of this application. This application must also
    be submitted as part of the process. By training the model on a range of pre-processed
    datasets and using a number of different optimizers and activation functions,
    the optimal performance of the model may be determined. In order to save the model
    with the hyper parameters that have been changed, it is stored as the final optimal
    model. Using this model, it is possible to predict sickness in leaves that have
    not before been observed. Table 5 compares the testing accuracy of several existing
    methods with the proposed work, as well as the number of training parameters used
    in the experiments (Fig. 10). Table 5 Different types of classification results
    Full size table Fig. 10 Real field and controlled environment image sample results
    Full size image Table 6 presents the accuracy performance metric for each data
    split, as well as the overall performance measure. After every 10 epochs of training,
    the values are displayed on the screen. When data split is 80–20 and epoch size
    is 50, the bolded number reflects the maximum degree of precision. Table 6 Models’
    classification and performance are discussed in a detailed report Full size table
    The precision and misfortune for both preparing and testing/approving for every
    age in every one of the three stages. These charts were made dependent on an information
    split of 80–20%. The precision diagram portrays how exactness for both preparing
    and testing rises logically over the long haul prior to having a tendency to unite
    on a specific point after the preparation stage. It additionally exhibits that,
    after 40 ages, the adjustment of exactness diminishes as the approval precision
    is by all accounts like the preparation exactness, showing that the model is turning
    out to be more exact. Along these lines, the right diagram outlines how the misfortune
    starts to diminish continuously when the model gains from a specific dataset.
    It takes 43 ages for the deficiency of approval information to become steady,
    and the deficiency of approval information will in general fall inside a specific
    reach noted in Fig. 11. Fig. 11 Accuracy and loss diagram in training and validation
    Full size image Comparison of CNN structures In this part, a visual comparison
    of several CNN architectures created using the DCDM method is presented. It is
    important to train the model on multiple architectures to determine which architecture
    is the most appropriate for a certain target application. For the classification
    challenge, the designs that we chose are some of the best-performing architectures
    on the market. For each trained architecture, we were able to achieve performance
    accuracy of 90% or above. AlexNet’s design has the lowest accuracy of 92.43%,
    which is the lowest of any architecture tested. In comparison to other architectures,
    this architecture is regarded to be the smallest and most basic architecture available.
    However, it continues to provide us with accuracy levels of over 90%. The VGG16
    and VGG19 designs are almost identical, except for a few small differences and
    a variable number of layers. They have a long history of reliably beating the
    opposition in characterization undertakings. Concerning our testing dataset, they
    give us an exactness of 94.05% and an accuracy of 96.89% individually. Like SqueezeNet,
    the structures of DenseNet and SqueezeNet additionally performed well, with exactness
    paces of 94.67% and 96.99%, individually. Squeeze Net is a neural network that
    uses design guidelines to decrease the number of variables, most noticeably by
    utilizing fire components that squeeze parameters utilizing 1 × 1 convolution
    layer. DenseNets have several key advantages; they solve the back to simpler,
    enhance feature propagation, promote retouch, and reduce the amount of data substantially.
    The ResNet50 engineering is notable for its capacity to perform well on huge datasets,
    and this is valid. This gadget has an aggregate of 50 layers, each having a one-of-a-kind
    interconnection. With an exactness of 97.85%, this plan procured the third-best
    positioning in our rundown of top structures. The plan of DarkNet conveys accuracy
    that is tantamount to that of the DCDM technique. It achieves an accuracy of 98.21%,
    placing it in second place among the best architectures, whereas DCDM architecture
    outperformed all others, earning the first spot among the best architectures with
    an accuracy of 98.78%. A visual representation of the findings for each architecture
    is provided in Fig. 12, which is given in Table 7. For the testing dataset, we
    tested the performance of one design against the other. When doing the comparison,
    the accuracy of Eq. (3) was utilized as an assessment metric of accuracy. A comparison
    of each design has also been conducted in terms of the amount of time it consumes,
    which results in the amount of time necessary for training. On the next page,
    you can get the average time for each training epoch. Our design consumes less
    time since it takes less computing time, resulting in the shortest average training
    time possible. It demonstrates that our design is the most efficient in terms
    of performance as well as computation efficiency. Fig. 12 Average accuracy of
    each model of CNN Full size image Table 7 Accuracy of disease classifications
    Full size table Probability: 0.9878 Disease Name: Tomato__Tomato_YellowLeaf__Curl_Virus
    Remedies: In that cultivators save their pepper seed. They ought to never save
    seed from cross breed assortments. The regular intersection of peppers is higher
    than that of tomatoes (typically under 10% yet as high as 25% in outrageous cases).
    Segregation by distance (a few hundred feet) and area upwind ought to be adequate.
    Conclusions The device may be intended to detect particular ailments that are
    most often found in farm environments, if it is developed in this manner. The
    system reports on each plant on a daily basis based on continuous inspection.
    The assessment of the sorts of illnesses prevalent in each institution, according
    to our early results, has resulted in an improvement in the overall performance.
    The bot has been created in order to maintain track of a wide variety of plants
    and prevent the spread of disease. At the moment, the focus of research is on
    illness detection systems that are based on image or video frames. It is the intention
    of the authors to have our systems interact with one another in order to increase
    the efficacy of sickness detection. An additional use for our method is in a new
    dimension of mobile disease recognition applications, which may aid ordinary people
    in comprehending any ailment by using their cell phones’ camera to record photos
    of it. This would allow them to better grasp any illness. Quad copters, which
    operate in areas where people are unable to enter their field of view, may benefit
    from our technology to protect their skin from infection. References Akram, T.,
    Naqvi, S. R., Haider, S. A., & Kamran, M. (2017). Towards real-time crops surveillance
    for disease classification: Exploiting parallelism in computer vision. Computers
    & Electrical Engineering, 59, 15–26. Article   Google Scholar   Chad, D., Tyr,
    W. H., Chen, S., et al. (2017). Automated identification of northern leaf blight-infected
    maize plants from field imagery using deep learning. Phytopathology, 107, 1426–1432.
    Article   Google Scholar   Chohan, M., Khan, A., Chohan, R., Katpar, S. H., &
    Mahar, M. S. (2020). Plant disease detection using deep learning. International
    Journal of Recent Technology and Engineering, 9(1), 909–914. Google Scholar   Chopda,
    J., Raveshiya, H., Nakum, S., & Nakrani, V. (2018). Cotton crop disease detection
    using Decision Tree Classifier. 2018 International Conference on Smart City and
    Emerging Technology (ICSCET) (pp. 1–5). IEEE. Google Scholar   Coulibaly, S.,
    Kamsu-Foguem, B., Kamissoko, D., & Traore, D. (2019). Deep neural networks with
    transfer learning in millet crop images. Computers in Industry, 108, 115–120.
    Article   Google Scholar   da Silva Abade, A., de Almeida, A. P. G., & de Barros
    Vidal, F. (2019). Plant diseases recognition from digital images using multichannel
    convolutional neural networks. VISIGRAPP. Google Scholar   de Luna, R. G., Dadios,
    E. P., & Bandala, A. A. (2018). Automated image capturing system for deep learning-based
    tomato plant leaf disease detection and recognition. TENCON 2018–2018 IEEE Region
    10 Conference (pp. 1414–1419). IEEE. Chapter   Google Scholar   Dhiraj, Biswas,
    R., & Ghattamaraju, N. (2019). An effective analysis of deep learning-based approaches
    for audio-based feature extraction and its visualization. Multimedia Tools and
    Applications, 78(17), 23949–23972. Article   Google Scholar   Ferentinos, K. P.
    (2018). Deep learning models for plant disease detection and diagnosis. Computers
    and Electronics in Agriculture, 145, 311–318. Article   Google Scholar   Hu, M.,
    Bu, X., Sun, X., Yu, Z., & Zheng, Y. (2017). Rape plant disease recognition method
    of multi-feature fusion based on D-S evidence theory. Mathematical and Computational
    Applications, 22(1), 18. Article   Google Scholar   Iqbal, Z., Khan, M. A., Sharif,
    M., Shah, J. H., ur Rehman, M. H., & Javed, K. (2018). An automated detection
    and classification of citrus plant diseases using image processing techniques:
    a review. Computers and Electronics in Agriculture, 153, 12–32. Article   Google
    Scholar   Kao, I. -H., Hsu, Y. -W., Yang, Y. -Z., Chen, Y. -L., Lai, Y. -H., &
    Perng, J. -W. (2019). Determination of Lycopersicon maturity using convolutional
    autoencoders. Scientia Horticulturae, 256, 108538. Article   Google Scholar   Khamparia,
    A., Saini, G., Gupta, D., Khanna, A., Tiwari, S., & de Albuquerque, V. H. C. (2020).
    Seasonal crops disease prediction and classification using deep convolutional
    encoder network. Circuits, Systems, and Signal Processing, 39(2), 818–836. Article   Google
    Scholar   Li, D., Deng, L., Lee, M., & Wang, H. (2019). IoT data feature extraction
    and intrusion detection system for smart cities based on deep migration learning.
    International Journal of Information Management, 49, 533–545. Article   Google
    Scholar   Lu, Y., Yi, S., Zeng, N., Liu, Y., & Zhang, Y. (2017). Identification
    of rice diseases using deep convolutional neural networks. Neurocomputing, 267,
    378–384. Article   Google Scholar   Mangalraj, P., Sivakumar, V., Karthick, S.,
    Haribaabu, V., Ramraj, S., & Samuel, D. J. (2019). A review of multi-resolution
    analysis (MRA) and multi-geometric analysis (MGA) tools used in the fusion of
    remote sensing images. Circuits, Systems, and Signal Processing, 39(6), 3145–3172.
    Article   Google Scholar   Marko, A., Mirjana, K., Srdjan, S., Andras, A., & Darko,
    S. (2019). Solving current limitations of deep learning-based approaches for plant
    disease detection. Symmetry-Baseline, 11(7), 939. Article   Google Scholar   Meziani,
    A., Djouani, K., Medkour, T., & Chibani, A. (2019). A Lasso quantile periodogram
    based feature extraction for EEG-based motor imagery. Journal of Neuroscience
    Methods, 328, 108434. Article   Google Scholar   Mohameth, F., Bingcai, C., &
    Sada, K. A. (2020). Plant disease detection with deep learning and feature extraction
    using plant village. Journal of Computer and Communications, 8(6), 10–22. Article   Google
    Scholar   Mohanty, S. P., Hughes, D. P., & Salathé, M. (2016). Using deep learning
    for image-based plant disease detection. Frontiers in Plant Science, 7, 1419.
    Article   Google Scholar   Ni, C., Wang, D., Vinson, R., Holmes, M., & Tao, Y.
    (2019). Automatic inspection machine for maize kernels based on deep convolutional
    neural networks. Biosystems Engineering, 178, 131–144. Article   Google Scholar   Patil,
    J. K., & Kumar, R. (2016). Analysis of content-based image retrieval for plant
    leaf diseases using color, shape and texture features. Engineering in Agriculture,
    Environment and Food, 10, 69–78. Article   Google Scholar   Pertot, I., Kuflik,
    T., Gordon, I., Freeman, S., & Elad, Y. (2012). Identification: A web-based tool
    for visual plant disease identification, a proof of concept with a case study
    on strawberry. Computers and Electronics in Agriculture, 84, 144–154. Article   Google
    Scholar   Pujari, D., Yakkundimath, R., & Byadgi, A. S. (2013). Grading and classification
    of anthracnose fungal disease of fruits based on statistical texture features.
    International Journal of Advanced Science and Technology, 52, 121–132. Google
    Scholar   Raza, M., Sharif, M., Yasmin, M., Khan, M. A., Saba, T., & Fernandes,
    S. L. (2018). Appearance-based pedestrians’ gender recognition by employing stacked
    autoencoders in deep learning. Future Generation Computer Systems, 88, 28–39.
    Article   Google Scholar   Sanga, S. L., Machuve, D., & Jomanga, K. (2020, June).
    Mobile-based deep learning models for banana disease detection. Engineering, Technology
    & Applied Scientific Research, 10(3), 5674–5677. Tiwari, D., Ashish, M., Gangwar,
    N., Sharma, A., Patel, S., & Bhardwaj, S. (2020, May). Potato leaf disease detection
    using deep learning. In 2020 4th International Conference on Intelligent Computing
    and Control Systems (ICICCS) (pp. 461–466). IEEE. Vasan, D., et al. (2020). IMCFN:
    Image-based malware classification using fine-tuned convolutional neural network
    architecture. Computer Networks, 171, 107138. Article   Google Scholar   Xu, C.,
    Chai, Y., Li, H., Shi, Z., Zhang, L., & Liang, Z. (2019a). A feature extraction
    method for the wear of milling tools based on the Hilbert marginal spectrum. Machining
    Science and Technology, 23, 847–868. Article   Google Scholar   Xu, Y., Ding,
    H., Xue, Y., & Guan, J. (2019b). High-dimensional feature extraction of sea clutter
    and target signal for intelligent maritime monitoring network. Computer Communications,
    147, 76–84. Article   Google Scholar   Yang, N., Qian, Y., El-Misery, H. S., Zhang,
    R., Wang, A., & Tang, J. (2019). Rapid detection of rice disease using microscopy
    image identification based on the synergistic judgment of texture and shape features
    and decision tree-confusion matrix method. Journal of the Science of Food and
    Agriculture, 99(14), 6589–6600. Article   CAS   Google Scholar   Zhang, S., You,
    Z., & Wu, X. (2019a). Plant disease leaf image segmentation based on superpixel
    clustering and EM algorithm. Neural Computing and Applications, 31(S2), 1225–1232.
    Article   Google Scholar   Zhang, X., Qiao, Y., Meng, F., Fan, C., & Zhang, M.
    (2018). Identification of maize leaf diseases using improved deep convolutional
    neural networks. IEEE Access, 6, 30370–30377. Article   Google Scholar   Zhang,
    Y., Wei, X. -S., Wu, J., et al. (2016). Weakly supervised fine-grained categorization
    with part-based image representation. IEEE Transactions on Image Processing, 25(4),
    1713–1725. Article   Google Scholar   Zhang, Z., Liu, H., Meng, Z., & Chen, J.
    (2019b). Deep learning-based automatic recognition network of agricultural machinery
    images. Computers and Electronics in Agriculture, 166, 104978. Article   Google
    Scholar   Download references Author information Authors and Affiliations Centre
    for Information Technology and Engineering, Manonmaniam Sundaranar University,
    Abishekaptti, Tirunelveli, Tamilnadu, 627 012, India Parasuraman Kumar, Srinivasan
    Raghavendran, Karunagaran Silambarasan & Nallaperumal Krishnan Department of Computer
    Science and Engineering, Vel Tech Rangarajan Dr, Sagunthala R&D Institute of Science
    and Technology, Chennai, Tamilnadu, 600 062, India Srinivasan Raghavendran Department
    of Statistics, Manonmaniam Sundaranar University, Abishekaptti, Tirunelveli, Tamilnadu,
    627 012, India Kaliaperumal Senthamarai Kannan Corresponding author Correspondence
    to Srinivasan Raghavendran. Ethics declarations Conflict of interest The authors
    declare no competing interests. Additional information Publisher''s Note Springer
    Nature remains neutral with regard to jurisdictional claims in published maps
    and institutional affiliations. Rights and permissions Springer Nature or its
    licensor holds exclusive rights to this article under a publishing agreement with
    the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Kumar, P., Raghavendran, S., Silambarasan, K. et al. Mobile application
    using DCDM and cloud-based automatic plant disease detection. Environ Monit Assess
    195, 44 (2023). https://doi.org/10.1007/s10661-022-10561-3 Download citation Received
    05 November 2021 Accepted 31 December 2021 Published 28 October 2022 DOI https://doi.org/10.1007/s10661-022-10561-3
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Plant diseases Cloud data center Cloud storage Mobile camera
    Agricultural product sector DeepLens Classification and Detection Model (DCDM)
    Use our pre-submission checklist Avoid common mistakes on your manuscript. Associated
    Content Part of a collection: Special Issue: Recent Technological Advances in
    Bio-sensors for Soil and Plant Monitoring Sections Figures References Abstract
    Introduction Related works Disease detection conventional techniques Proposed
    technology Results and outcomes Conclusions References Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Environmental Monitoring and Assessment
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Mobile application using DCDM and cloud-based automatic plant disease detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
