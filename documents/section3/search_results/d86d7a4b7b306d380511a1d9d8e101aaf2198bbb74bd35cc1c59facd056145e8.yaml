- DOI: https://doi.org/10.3390/rs12162659
  analysis: '>'
  authors:
  - Bing Lu
  - Phuong D. Dao
  - Jiangui Liu
  - Yuhong He
  - Jiali Shang
  citation_count: 360
  full_citation: '>'
  full_text: ">\nremote sensing  \nReview\nRecent Advances of Hyperspectral Imaging\n\
    Technology and Applications in Agriculture\nBing Lu 1, Phuong D. Dao 1,2\n, Jiangui\
    \ Liu 3, Yuhong He 1,* and Jiali Shang 3\n1\nDepartment of Geography, Geomatics\
    \ and Environment, University of Toronto Mississauga,\n3359 Mississauga Road,\
    \ Mississauga, ON L5L 1C6, Canada; bing.lu@mail.utoronto.ca (B.L.);\nphuong.dao@mail.utoronto.ca\
    \ (P.D.D.)\n2\nSchool of the Environment, University of Toronto, 33 Willcocks\
    \ Street, Toronto, ON M5S 3E8, Canada\n3\nAgriculture and Agri-Food Canada, 960\
    \ Carling Avenue, Ottawa, ON K1A 0C6, Canada;\njiangui.liu@canada.ca (J.L.); jiali.shang@Canada.ca\
    \ (J.S.)\n*\nCorrespondence: yuhong.he@utoronto.ca\nReceived: 12 July 2020; Accepted:\
    \ 16 August 2020; Published: 18 August 2020\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\
    \n\x01\x02\x03\x04\x05\x06\a\nAbstract:\nRemote sensing is a useful tool for monitoring\
    \ spatio-temporal variations of crop\nmorphological and physiological status and\
    \ supporting practices in precision farming. In comparison\nwith multispectral\
    \ imaging, hyperspectral imaging is a more advanced technique that is capable\n\
    of acquiring a detailed spectral response of target features. Due to limited accessibility\
    \ outside of\nthe scientiﬁc community, hyperspectral images have not been widely\
    \ used in precision agriculture.\nIn recent years, diﬀerent mini-sized and low-cost\
    \ airborne hyperspectral sensors (e.g., Headwall\nMicro-Hyperspec, Cubert UHD\
    \ 185-Fireﬂy) have been developed, and advanced spaceborne\nhyperspectral sensors\
    \ have also been or will be launched (e.g., PRISMA, DESIS, EnMAP, HyspIRI).\n\
    Hyperspectral imaging is becoming more widely available to agricultural applications.\
    \ Meanwhile,\nthe acquisition, processing, and analysis of hyperspectral imagery\
    \ still remain a challenging research\ntopic (e.g., large data volume, high data\
    \ dimensionality, and complex information analysis). It is\nhence beneﬁcial to\
    \ conduct a thorough and in-depth review of the hyperspectral imaging technology\n\
    (e.g., diﬀerent platforms and sensors), methods available for processing and analyzing\
    \ hyperspectral\ninformation, and recent advances of hyperspectral imaging in\
    \ agricultural applications. Publications\nover the past 30 years in hyperspectral\
    \ imaging technology and applications in agriculture were\nthus reviewed.\nThe\
    \ imaging platforms and sensors, together with analytic methods used in\nthe literature,\
    \ were discussed. Performances of hyperspectral imaging for diﬀerent applications\n\
    (e.g., crop biophysical and biochemical properties’ mapping, soil characteristics,\
    \ and crop classiﬁcation)\nwere also evaluated. This review is intended to assist\
    \ agricultural researchers and practitioners to\nbetter understand the strengths\
    \ and limitations of hyperspectral imaging to agricultural applications\nand promote\
    \ the adoption of this valuable technology. Recommendations for future hyperspectral\n\
    imaging research for precision agriculture are also presented.\nKeywords: precision\
    \ agriculture; remote sensing; hyperspectral imaging; platforms and sensors;\n\
    analytical methods; crop properties; soil characteristics; classiﬁcation of agricultural\
    \ features\n1. Introduction\nThe global agricultural sector is facing increasing\
    \ challenges posed by a range of stressors,\nincluding a rapidly growing population,\
    \ the depletion of natural resources, environmental pollution,\ncrop diseases,\
    \ and climate change. Precision agriculture is a promising approach to address\
    \ these\nchallenges through improving farming practices, e.g., adaptive inputs\
    \ (e.g., water and fertilizer),\nensured outputs (e.g., crop yield and biomass),\
    \ and reduced environmental impacts. Remote sensing\nRemote Sens. 2020, 12, 2659;\
    \ doi:10.3390/rs12162659\nwww.mdpi.com/journal/remotesensing\nRemote Sens. 2020,\
    \ 12, 2659\n2 of 44\nis capable of identifying within-ﬁeld variability of soils\
    \ and crops and providing useful information for\nsite-speciﬁc management practices\
    \ [1,2]. There are two types of remote sensing technologies given the\nsource\
    \ of energy, passive (e.g., optical) and active remote sensing (e.g., LiDAR and\
    \ Radar). Passive\noptical remote sensing is usually further divided into two\
    \ groups based on the spectral resolutions\nof sensors, multispectral and hyperspectral\
    \ remote sensing [3]. Multispectral imaging is facilitated\nby collecting spectral\
    \ signals in a few discrete bands, each spanning a broad spectral range from tens\n\
    to hundreds of nanometers. In contrast, hyperspectral imaging detects spectral\
    \ signals in a series of\ncontinuous channels with a narrow spectral bandwidth\
    \ (e.g., typically below 10 nm); therefore, it can\ncapture ﬁne-scale spectral\
    \ features of targets that otherwise could be compromised [4].\nMultispectral\
    \ images (e.g., Landsat, Sentinel 2, and SPOT images) have been widely used in\n\
    agricultural studies to retrieve various crop and soil attributes, such as crop\
    \ chlorophyll content,\nbiomass, yield, and soil degradation [5–10]. However,\
    \ due to the limitations in spectral resolution,\nthe accuracy of the retrieved\
    \ variables is often limited, and early signals of crop stresses (e.g., nutrient\n\
    deﬁciency, crop disease) cannot be eﬀectively detected in a timely manner [11].\n\
    Hyperspectral\nimages (e.g., Hyperion, CASI, and Headwall Micro-Hyperspec) with\
    \ hundreds of bands can capture\nmore detailed spectral responses; hence, it is\
    \ more capable of detecting subtle variations of ground\ncovers and their changes\
    \ over time. Therefore, hyperspectral imagery can be used to address the\naforementioned\
    \ challenges and facilitate more accurate and timely detection of crop physiological\n\
    status [12,13]. Previous studies have also demonstrated the superior performance\
    \ of hyperspectral\nover multispectral images in monitoring vegetation properties,\
    \ such as estimating the leaf area\nindex (LAI) [14], discriminating crop types\
    \ [15], retrieving crop biomass [16], and assessing leaf\nnitrogen content [17].\
    \ Despite its outstanding performance, hyperspectral imaging has been utilized\n\
    comparatively less in operational agricultural applications in the past few decades\
    \ due to the high cost\nof the sensors and imaging missions, and various technical\
    \ challenges (e.g., low signal-to-noise ratio\nand large data volume) [18–21].\
    \ Although ground-based hyperspectral reﬂectance data can be quickly\nmeasured\
    \ using a spectroradiometer (e.g., ASD Field Spec, Analytical Spectral Devices\
    \ Inc., Boulder,\nCO, USA) and have been widely used for observing canopy- and\
    \ leaf-level spectral features [22–24],\nsuch ground-based measurements are limited\
    \ to a few numbers of ﬁeld sites, and they cannot capture\nspatial variability\
    \ across large areas. In contrast, hyperspectral imaging sensors are more convenient\
    \ to\nacquire spatial variability of spectral information across a region.\nIn\
    \ recent years, a wide range of mini-sized and low-cost hyperspectral sensors\
    \ have been developed\nand are available for commercial use, such as Micro- and\
    \ Nano-Hyperspec (Headwall Photonics Inc.,\nBoston, MA, USA), HySpex VNIR (HySpex,\
    \ Skedsmo, Skjetten, Norway), and FireﬂEYE (Cubert GmbH,\nUlm, Germany) [11,25].\
    \ These sensors can be mounted on manned or unmanned airborne platforms\n(e.g.,\
    \ airplanes, helicopters, and unmanned aerial vehicles (UAVs)) for acquiring hyperspectral\
    \ images\nand supporting various monitoring missions [13,26,27]. In addition,\
    \ new spaceborne hyperspectral\nsensors have been launched recently, such as the\
    \ DESIS—launched in 2018 [28]—and PRISMA—\nlaunched in 2019 [29]—or will be launched\
    \ in the next few years, such as EnMAP, with scheduled\nlaunching in 2020 [30,31].\
    \ Overall, increasingly more airborne or spaceborne hyperspectral images\nhave\
    \ become available, bringing unprecedented opportunities for better monitoring\
    \ of ground targets,\nespecially for better investigation of crop and soil variabilities\
    \ and supporting precision agriculture.\nTherefore, a literature search was performed\
    \ to examine if more research in using hyperspectral\nimaging for agricultural\
    \ purposes had been published in recent years. Both Web of Science and\nGoogle\
    \ Scholar were used for conducting the literature search with topics or keywords,\
    \ including\nhyperspectral, imaging, agriculture, or farming, and publication\
    \ over a 30-year time span (1990 to 2020).\nThe searched results were further\
    \ veriﬁed to ensure that each publication falls within the scope of\nhyperspectral\
    \ imaging for agriculture applications. It was found that there was an increasing\
    \ number\nof publications in recent years that used hyperspectral imaging for\
    \ agricultural applications (Figure 1).\nSubstantially more studies have been\
    \ published in the recent decade (e.g., 245 articles published in\n2011–2020)\
    \ than that in the previous one (e.g., 97 published in 2001–2010).\nRemote Sens.\
    \ 2020, 12, 2659\n3 of 44\nRemote Sens. 2020, 12, x FOR PEER REVIEW \n3 of 43\
    \ \n \n \nFigure 1. The number of publications that utilized hyperspectral imaging\
    \ for agriculture applications \n(by May 2020). \nThis review is designed to focus\
    \ on the acquisition, processing, and analysis of hyperspectral \nimagery for\
    \ different agricultural applications. The review is organized in the following\
    \ main aspects: \n(1) Hyperspectral imaging platforms and sensors, (2) methods\
    \ for processing and analyzing \nhyperspectral images, and (3) hyperspectral applications\
    \ in agriculture (Table 1). Regarding imaging \nplatforms, different types, including\
    \ satellites, airplanes, helicopters, fixed-wing UAVs, multi-rotor \nUAVs, and\
    \ close-range platforms (e.g., ground or lab based), have been used. These platforms\
    \ acquire \nimages with different spatial coverage, spatial resolution, temporal\
    \ resolution, operational \ncomplexity, and mission cost. It will be beneficial\
    \ to summarize various platforms in terms of these \nfeatures to support the selection\
    \ of the appropriate one(s) for different monitoring purposes. After \nraw hyperspectral\
    \ imagery is acquired, pre-processing is the step for obtaining accurate spectral\
    \ \ninformation. Several procedures need to be carried out during pre-processing\
    \ (usually implemented \nin a specialized remote sensing software), including\
    \ radiometric calibration, spectral correction, \natmospheric correction, and\
    \ geometric correction. Although these are standard processing steps for \nmost\
    \ satellite imagery, it still can be challenging to perform on many airborne hyperspectral\
    \ images \ndue to different technical issues (e.g., the requirement of high-accuracy\
    \ Global Positioning System \n(GPS) signals for proper geometric correction, the\
    \ measurement of real-time solar radiance for \naccurate spectral correction).\
    \ There are no standardized protocols for all sensors due to the limited \navailability\
    \ of hyperspectral imaging in the past and the fact that the new mini-sized and\
    \ low-cost \nhyperspectral sensors in the market are from different manufacturers\
    \ with varying sensor \nconfigurations. Various approaches have been used in previous\
    \ studies to address these challenges \n[12,19,32,33]. Therefore, it is essential\
    \ to review these approaches to support other researchers for \nmore accurate\
    \ and efficient hyperspectral image processing. After pre-preprocessing, such\
    \ as \ncalibration and correction, spectral information extraction (e.g., band\
    \ selection and dimension \nreduction) can be performed to further improve the\
    \ usability of the hyperspectral image. Techniques \nfor these procedures are\
    \ reviewed in this study. \n \nFigure 1. The number of publications that utilized\
    \ hyperspectral imaging for agriculture applications\n(by May 2020).\nThis review\
    \ is designed to focus on the acquisition, processing, and analysis of hyperspectral\n\
    imagery for diﬀerent agricultural applications. The review is organized in the\
    \ following main aspects:\n(1) Hyperspectral imaging platforms and sensors, (2)\
    \ methods for processing and analyzing\nhyperspectral images, and (3) hyperspectral\
    \ applications in agriculture (Table 1). Regarding imaging\nplatforms, diﬀerent\
    \ types, including satellites, airplanes, helicopters, ﬁxed-wing UAVs, multi-rotor\n\
    UAVs, and close-range platforms (e.g., ground or lab based), have been used. These\
    \ platforms\nacquire images with diﬀerent spatial coverage, spatial resolution,\
    \ temporal resolution, operational\ncomplexity, and mission cost. It will be beneﬁcial\
    \ to summarize various platforms in terms of these\nfeatures to support the selection\
    \ of the appropriate one(s) for diﬀerent monitoring purposes. After raw\nhyperspectral\
    \ imagery is acquired, pre-processing is the step for obtaining accurate spectral\
    \ information.\nSeveral procedures need to be carried out during pre-processing\
    \ (usually implemented in a specialized\nremote sensing software), including radiometric\
    \ calibration, spectral correction, atmospheric correction,\nand geometric correction.\
    \ Although these are standard processing steps for most satellite imagery,\nit\
    \ still can be challenging to perform on many airborne hyperspectral images due\
    \ to diﬀerent technical\nissues (e.g., the requirement of high-accuracy Global\
    \ Positioning System (GPS) signals for proper\ngeometric correction, the measurement\
    \ of real-time solar radiance for accurate spectral correction).\nThere are no\
    \ standardized protocols for all sensors due to the limited availability of hyperspectral\n\
    imaging in the past and the fact that the new mini-sized and low-cost hyperspectral\
    \ sensors in the\nmarket are from diﬀerent manufacturers with varying sensor conﬁgurations.\
    \ Various approaches have\nbeen used in previous studies to address these challenges\
    \ [12,19,32,33]. Therefore, it is essential to\nreview these approaches to support\
    \ other researchers for more accurate and eﬃcient hyperspectral\nimage processing.\
    \ After pre-preprocessing, such as calibration and correction, spectral information\n\
    extraction (e.g., band selection and dimension reduction) can be performed to\
    \ further improve the\nusability of the hyperspectral image. Techniques for these\
    \ procedures are reviewed in this study.\nRemote Sens. 2020, 12, 2659\n4 of 44\n\
    Table 1. Topics reviewed in this article.\nProcedures of Applying\nHyperspectral\
    \ Imagery\nImage Acquisition\nImage Processing and Analysis\nImage Applications\n\
    Review Focuses\nPlatforms:\n- Satellites\n- Airplanes\n- UAVs\n- Close-range platforms\n\
    Sensors:\n- EO-1 Hyperion\n- AVIRIS\n- CASI\n- Headwall Hyperspec etc.\nPre-processing:\n\
    - Geometric and radiometric\ncorrection etc.\n- Dimension reduction\n- Band selection\n\
    Analytical Methods:\n- Empirical regression\n- Radiative transfer modelling\n\
    - Machine learning and\ndeep learning\nSpeciﬁc Applications:\n- Estimating crop\
    \ biochemical and biophysical properties\n- Evaluating crop nutrient status\n\
    - Classifying imagery to identify crop types, growing stages,\nweeds/invasive\
    \ species, stress/disease\n- Retrieving soil moisture, fertility, and other physical\
    \ or\nchemical properties\nRemote Sens. 2020, 12, 2659\n5 of 44\nWith pre-processed\
    \ hyperspectral images, a robust and eﬃcient analytical method is required\nfor\
    \ analyzing the tremendous amount of information contained in the images (e.g.,\
    \ spectral, spatial,\nand textural features) and extracting target properties\
    \ (e.g., crop and soil characteristics). Previous\nstudies have used a suite of\
    \ analytical methods, including empirical regression (e.g., linear regression,\n\
    partial least square regression (PLSR), and multi-variable regression (MLR)),\
    \ radiative transfer\nmodelling (RTM, e.g., PROSPECT and PROSAIL), machine learning\
    \ (e.g., random forest (RF)),\nand deep learning (e.g., convolutional neural network\
    \ (CNN)) [34–37]. These methods have been\ndeveloped based on diﬀerent theories\
    \ and have diﬀerent operational complexity, computation eﬃciency,\nand performance\
    \ accuracy. Therefore, it is essential to review the strengths and limitations\
    \ of these\nmethods and help to choose the appropriate one(s) for speciﬁc research\
    \ purposes. Using hyperspectral\ninformation, researchers have investigated a\
    \ wide range of agricultural features. Some popular ones\ninclude crop water content,\
    \ LAI, chlorophyll and nitrogen contents, pests and disease, plant height,\nphenological\
    \ information, soil moisture, and soil organic matter content [11,38]. It will\
    \ also be valuable\nto review the performances of hyperspectral imaging in these\
    \ studies and further explore the potential\nof this technology for monitoring\
    \ other agricultural features. Lastly, challenges of using hyperspectral\nimaging\
    \ for precision agriculture, together with future research directions, are discussed.\
    \ A few\nprevious review articles have discussed some of these topics to some\
    \ extent [11,38,39]. More details\nand contributions of this review will be discussed\
    \ in each speciﬁc section. Overall, this review aims to\nexamine the main procedures\
    \ in collecting and utilizing hyperspectral images for diﬀerent agricultural\n\
    applications, to further understand the strengths and limitations of hyperspectral\
    \ technology, and to\npromote the faster adoption of this valuable technology\
    \ in precision farming.\n2. Hyperspectral Imaging Platforms and Sensors\nHyperspectral\
    \ sensors can be mounted on different platforms, such as satellites, airplanes,\n\
    UAVs, and close-range platforms, to acquire images with different spatial and\
    \ temporal resolutions.\nPlatforms used in the literature were identified and\
    \ summarized over the publication years, aiming to\nfind, if any, the platforms\
    \ that had been used more frequently in a specific time period, and the results\n\
    are shown in Figure 2. Airplanes have been the most widely used platforms for\
    \ hyperspectral imaging\nin agriculture (Figure 2). Approximately 30 articles\
    \ that used airplanes were published every five years\nstarting from 2001 (e.g.,\
    \ 27 publications in 2001–2005 and 38 in 2006–2010). In comparison, satellite-based\n\
    hyperspectral imaging has been used less frequently; approximately 20 or fewer\
    \ articles were published\nin all five-year periods. UAVs are popular platforms\
    \ for remote sensing and have been widely used in\nthe last decade for hyperspectral\
    \ imaging in agriculture (e.g., more than 20 publications in 2011–2015 and\n2016–2020).\
    \ Close-range platforms have been the most widely used in the last five years\
    \ (i.e., 2016–2020),\nwith 49 publications (Figure 2). The review in this section\
    \ is structured based on different platforms,\nincluding satellites, airplanes,\
    \ UAVs, and close-range platforms. In contrast to previous articles reviewing\n\
    hyperspectral platforms [20,38,39], the review in this section focuses more on\
    \ recent advancements of\nimaging platforms (e.g., UAVs, helicopters, and close\
    \ range) and their applications to precision farming\n(e.g., weed classification,\
    \ fine-scale evaluation of crop health, pests, and disease).\n2.1. Satellite-Based\
    \ Hyperspectral Imaging\nCompared with a large number of satellite-based multispectral\
    \ sensors (e.g., Landsat,\nSPOT, WorldView, QuickBird, Sentinel-2), there are\
    \ signiﬁcantly fewer hyperspectral sensors.\nEO-1 Hyperion, PROBA-CHRIS, and TianGong-1\
    \ [40] are a few examples of the available satellite\nhyperspectral sensors [20].\
    \ EO-1 Hyperion is the most widely used satellite-based hyperspectral\nsensor\
    \ for agriculture (e.g., more than 40 publications). It collects data in the visible,\
    \ near-infrared,\nand shortwave infrared ranges with a spectral resolution of\
    \ 10 nm and a spatial resolution of 30 m.\nMore sensor speciﬁcations of EO-1 Hyperion\
    \ are given in Table 2. The sensor was in operation\nfrom 2000 to 2017, which\
    \ corresponds to the period having more publications using satellite-based\nhyperspectral\
    \ imaging (e.g., 2006 to 2020 in Figure 2). The use of Hyperion data has been\
    \ reported in a\nRemote Sens. 2020, 12, 2659\n6 of 44\nvariety of agricultural\
    \ studies for monitoring diﬀerent crop and soil properties, including detecting\n\
    crop disease [41,42], estimating crop properties (e.g., chlorophyll, LAI, biomass)\
    \ [43–45], assessing crop\nresidues [46,47], classifying crop types [48], and\
    \ investigating soil features [49,50]. A few featured ones\ninclude Wu et al.\
    \ [45], who estimated vegetation chlorophyll content and LAI in a mixed agricultural\n\
    ﬁeld using Hyperion data and evaluated spectral bands that are sensitive to these\
    \ vegetation properties.\nCamacho Velasco et al. [48] used Hyperion hyperspectral\
    \ imagery and diﬀerent classiﬁcation algorithms\n(e.g., spectral angle mapper\
    \ and adaptive coherence estimator) for identifying ﬁve types of crops\n(e.g.,\
    \ oil palm, rubber, grass for grazing, citrus, and sugar cane) in Colombia. Gomez\
    \ et al. [49] predicted\nsoil organic carbon (SOC) using both spectroradiometer\
    \ data and a Hyperion hyperspectral image,\nand they found that using Hyperion\
    \ data resulted in a lower accuracy compared with results derived\nfrom spectroradiometer\
    \ data.\nRemote Sens. 2020, 12, x FOR PEER REVIEW \n6 of 43 \n \nFigure 2. Number\
    \ of publications that used different hyperspectral imaging platforms over time.\
    \ \n2.1. Satellite-Based Hyperspectral Imaging \nCompared with a large number\
    \ of satellite-based multispectral sensors (e.g., Landsat, SPOT, \nWorldView,\
    \ QuickBird, Sentinel-2), there are significantly fewer hyperspectral sensors.\
    \ EO-1 \nHyperion, PROBA-CHRIS, and TianGong-1 [40] are a few examples of the\
    \ available satellite \nhyperspectral sensors [20]. EO-1 Hyperion is the most\
    \ widely used satellite-based hyperspectral \nsensor for agriculture (e.g., more\
    \ than 40 publications). It collects data in the visible, near-infrared, \nand\
    \ shortwave infrared ranges with a spectral resolution of 10 nm and a spatial\
    \ resolution of 30 m. \nMore sensor specifications of EO-1 Hyperion are given\
    \ in Table 2. The sensor was in operation from \n2000 to 2017, which corresponds\
    \ to the period having more publications using satellite-based \nhyperspectral\
    \ imaging (e.g., 2006 to 2020 in Figure 2). The use of Hyperion data has been\
    \ reported in \na variety of agricultural studies for monitoring different crop\
    \ and soil properties, including detecting \ncrop disease [41,42], estimating\
    \ crop properties (e.g., chlorophyll, LAI, biomass) [43–45], assessing \ncrop\
    \ residues [46,47], classifying crop types [48], and investigating soil features\
    \ [49,50]. A few \nfeatured ones include Wu et al. [45], who estimated vegetation\
    \ chlorophyll content and LAI in a \nmixed agricultural field using Hyperion data\
    \ and evaluated spectral bands that are sensitive to these \nvegetation properties.\
    \ Camacho Velasco et al. [48] used Hyperion hyperspectral imagery and \ndifferent\
    \ classification algorithms (e.g., spectral angle mapper and adaptive coherence\
    \ estimator) for \nidentifying five types of crops (e.g., oil palm, rubber, grass\
    \ for grazing, citrus, and sugar cane) in \nColombia. Gomez et al. [49] predicted\
    \ soil organic carbon (SOC) using both spectroradiometer data \nand a Hyperion\
    \ hyperspectral image, and they found that using Hyperion data resulted in a lower\
    \ \naccuracy compared with results derived from spectroradiometer data \nStudies\
    \ have also been conducted to compare the performances of Hyperion hyperspectral\
    \ \nimagery with multispectral imagery for estimating crop properties or classifying\
    \ crop types. For \ninstance, Mariotto et al. [15] compared Hyperion hyperspectral\
    \ imagery with Landsat multispectral \nimagery for the estimation of crop productivity\
    \ and the classification of crop types. The authors \nreported better performances\
    \ of using hyperspectral imagery than using Landsat imagery for both \nresearch\
    \ purposes. Similarly, Bostan et al. [51] compared Hyperion hyperspectral imagery\
    \ with \nLandsat multispectral imagery for crop classification and also found\
    \ that higher classification \naccuracy can be achieved by using hyperspectral\
    \ imagery. \nFigure 2. Number of publications that used diﬀerent hyperspectral\
    \ imaging platforms over time.\nStudies have also been conducted to compare the\
    \ performances of Hyperion hyperspectral imagery\nwith multispectral imagery for\
    \ estimating crop properties or classifying crop types. For instance,\nMariotto\
    \ et al. [15] compared Hyperion hyperspectral imagery with Landsat multispectral\
    \ imagery for\nthe estimation of crop productivity and the classiﬁcation of crop\
    \ types. The authors reported better\nperformances of using hyperspectral imagery\
    \ than using Landsat imagery for both research purposes.\nSimilarly, Bostan et\
    \ al. [51] compared Hyperion hyperspectral imagery with Landsat multispectral\n\
    imagery for crop classiﬁcation and also found that higher classiﬁcation accuracy\
    \ can be achieved by\nusing hyperspectral imagery.\nRemote Sens. 2020, 12, 2659\n\
    7 of 44\nTable 2. Speciﬁcations of commonly used hyperspectral sensors [11,20,52–56].\n\
    Satellite-Based\nAirplane-Based\nUAV-Based *\nSensor\nHyperion\nPROBA-CHRIS\n\
    AVIRIS\nCASI\nAISA\nHyMap\nHeadwall\nHyperspec\nUHD\n185-Fireﬂy\nSpectral range\
    \ (nm)\n357–2576\n415–1050\n400–2500\n380–1050\n(CASI-1500)\n400–970\n(Eagle)\n\
    440–2500\n400–1000\n(VNIR)\n450–950\nNumber of spectral bands\n220\n19\n63\n224\n\
    288\n244\n128\n270 (Nano)\n324 (Micro)\n138\nSpectral Resolution (nm)\n10\n34\n\
    17\n10\n<3.5\n3.3\n15\n6 (Nano) 2.5\n(Micro)\n4\nOperational altitudes (km)\n\
    705 (swath 7.7 km)\n830 (swath 14 km)\n1–20\n<0.15\nSpatial resolution (m)\n30\n\
    17\n36\n1–20\n0.01–0.5\nTemporal resolution (days)\n16–30\n8\nDepends on ﬂight\
    \ operations (hours to days)\nOrganization\nNASA, USA\nESA, UK\nJet Propulsion\n\
    Laboratory,\nUSA\nItres,\nCanada\nSpecim,\nFinland\nIntegrated\nSpectronics,\n\
    Australia\nHeadwall\nPhotonics,\nUSA\nCubert GmbH,\nGermany\nNumber of publications\n\
    41\n9\n18\n22\n20\n12\n9\n6\n* UAV-based sensors typically can also be mounted\
    \ on airplanes for imaging.\nRemote Sens. 2020, 12, 2659\n8 of 44\nPROBA-CHRIS\
    \ is another commonly used satellite-based hyperspectral sensor that was launched\n\
    in 2001.\nSpeciﬁc studies, such as Verger et al. [57], utilized PROBA-CHRIS data\
    \ for retrieving\nLAI, the fraction of vegetation cover (fCover), and the fraction\
    \ of absorbed photosynthetically\nactive radiation (FAPAR) in an agricultural\
    \ ﬁeld. Antony et al. [58] identiﬁed three growth stages\nof wheat using multi-angle\
    \ PROBA-CHRIS images and found the optimal view angles for the\nidentiﬁcation.\
    \ Casa et al. [59] evaluated the performance of airborne Multispectral Infrared\
    \ Visible\nImaging Spectrometer (MIVIS) data and spaceborne PROBA-CHRIS data for\
    \ investigating soil texture,\nand they found that these two data have similar\
    \ performances, although the PROBA-CHRIS data have\na lower spatial resolution.\n\
    There are a few other satellite-based hyperspectral sensors that have not been\
    \ commonly used\nin an agricultural environment. For instance, Hyperspectral Imager\
    \ (HySI) is a hyperspectral sensor\nequipped on the Indian Microsatellite-1 (IMS-1)\
    \ launched in 2008 [60]. It collects spectral signals in the\nrange of 400–950\
    \ nm with a spatial resolution of 550 m at nadir [61]. HySI imagery has been used\
    \ to\nmap diﬀerent agricultural features, such as soil moisture and soil salinity\
    \ [62]. It has also been used for\ncrop classiﬁcation [63]. However, this data\
    \ has not been widely used in precision farming, which is\nprobably due to the\
    \ low spatial resolution and limited data availability. The Hyperspectral Imager\n\
    for the Coastal Ocean (HICO) is another spaceborne hyperspectral sensor that takes\
    \ images with a\nspectral range from 380 to 960 nm at a spatial resolution of\
    \ 90 m [64]. This sensor was mainly designed\nto sample the coastal ocean and\
    \ operated from 2009 to 2015.\nIn recent years, several spaceborne hyperspectral\
    \ sensors have been launched or scheduled for\nlaunching in the next few years.\
    \ For instance, the German Aerospace Center (DLR) Earth Sensing\nImaging Spectrometer\
    \ (DESIS), a hyperspectral sensor mounted on the International Space Station,\n\
    was launched in 2018 [65]. This sensor acquires images in the range from 400 to\
    \ 1000 nm with a spectral\nresolution of 2.5 nm and a spatial resolution of 30\
    \ m. The Hyperspectral Imager Suite (HISUI) is a\nJapanese hyperspectral sensor\
    \ that is also onboard the International Space Station [66]. It was launched\n\
    in 2019 and collects data in the range from 400 to 2500 nm with a spatial resolution\
    \ of 20 m and a\ntemporal resolution of 2 to 60 days [20]. Hyperspectral Precursor\
    \ and Application Mission (PRISMA)\nis an Italian hyperspectral mission with the\
    \ sensor launched in March 2019. Its spectral resolution is\n12 nm in the range\
    \ of 400-2500 nm (~250 bands in visible to shortwave infrared). Its hyperspectral\n\
    imagery has a spatial resolution of 30 and 5 m for the panchromatic band [67].\
    \ The Environmental\nMapping and Analysis Program (EnMAP) is a German hyperspectral\
    \ satellite mission that is still in\nthe development and production phase [68].\
    \ The EnMAP sensor will collect data from the visible to\nthe shortwave infrared\
    \ range with a spatial resolution of 30 m. It is planned to be launched in 2020.\n\
    The Spaceborne Hyperspectral Applicative Land and Ocean Mission (SHALOM) is a\
    \ joint mission by\nIsraeli and Italian space agencies, and the satellite is scheduled\
    \ to be launched in 2022 [69]. This sensor\nwill collect hyperspectral images\
    \ with a spatial resolution of 10 m in the spectral range of 400–2500 nm\nand\
    \ panchromatic images with a spatial resolution of 2.5 m [70]. HyspIRI is another\
    \ hyperspectral\nmission that is also at the study stage [71]. This sensor will\
    \ collect data in the 380 to 2500 nm range\nwith an interval of 10 nm and a spatial\
    \ resolution of 60 m.\nAlthough the actual PRISMA, EnMAP, and HyspIRI data are\
    \ not yet available, researchers\nhave simulated the images using other data and\
    \ tested the performance of the simulated images for\ninvestigating diﬀerent vegetation\
    \ and soil features. For instance, Malec et al. [72], Siegmann et al. [73],\n\
    and Locherer et al. [74] simulated EnMAP imagery using diﬀerent airborne or spaceborne\
    \ images and\napplied the simulated images for investigating diﬀerent crop and\
    \ soil properties. Bachmann et al. [75]\nproduced an image using the EnMAP’s end-to-end\
    \ simulation tool and examined the uncertainties\nassociated with spectral and\
    \ radiometric calibration. Castaldi et al. [76] simulated data of four\ncurrent\
    \ (EO-1 ALI and Hyperion, Landsat 8 Operational Land Imager (OLI), Sentinel-2\
    \ MultiSpectral\nInstrument (MSI)) and three forthcoming (EnMAP, PRISMA, and HyspIRI)\
    \ sensors using a soil spectral\nlibrary and compared their performance for estimating\
    \ soil properties. Castaldi et al. [77] used PRISMA\nRemote Sens. 2020, 12, 2659\n\
    9 of 44\ndata that were simulated with lab-measured spectral data for estimating\
    \ clay content and attempted to\nreduce the inﬂuence of soil moisture on the estimation\
    \ of clay.\nPrevious studies have conﬁrmed the good performance of satellite-based\
    \ hyperspectral sensors for\nstudying agricultural features; however, several\
    \ factors could potentially aﬀect the broad applications\nof these data in precision\
    \ farming, including the spatial resolution, temporal resolution, and data quality.\n\
    The detection and monitoring of many agricultural features, such as crop disease,\
    \ pest infestation,\nand nutrient status, require high spatial and temporal resolution.\n\
    Most of the satellite-based\nhyperspectral sensors have medium spatial resolutions,\
    \ such as 17 or 36 m for PROBA-CHRIS;\n30 m for Hyperion, PRISMA, and EnMAP, DESIS;\
    \ and 60 m for HyspIRI. Previous studies have\nindicated that such spatial resolutions\
    \ are not suﬃcient for precision farming applications [20,49].\nTo overcome such\
    \ limitations, researchers have attempted to pansharpen hyperspectral images,\
    \ aiming\nto improve spatial resolution [73,78–80]. Loncan et al. [81] also reviewed\
    \ diﬀerent pansharpening\nmethods for generating high-spatial resolution hyperspectral\
    \ images.\nTemporal resolution is another factor that could potentially limit\
    \ the applications of satellite-based\nhyperspectral images to precision agriculture.\
    \ Most of the satellite-based sensors have a long revisit\ncycle (e.g., typically\
    \ around two weeks), and thus early signals of crop stress (e.g., disease and\n\
    pest) may be missed. This limitation can be further aggravated by unfavorable\
    \ weather conditions\n(e.g., cloud contamination). Lastly, low data quality is\
    \ also an issue that can aﬀect the performance of\nsatellite-based hyperspectral\
    \ imaging for investigating agricultural features. A low signal-to-noise ratio\n\
    is a well-known issue of Hyperion data (e.g., in the shortwave infrared (SWIR)\
    \ range), which has aﬀected\nthe accuracy of retrieving diﬀerent agricultural\
    \ features [20]. For instance, Asner and Heidebrecht [82],\nGomez et al. [49],\
    \ and Weng et al. [83] found that the low signal-to-noise ratio inﬂuenced the\
    \ accuracies\nof estimating non-photosynthetic vegetation and soil cover, soil\
    \ organic matter, and soil salinity,\nrespectively. Future satellite-based hyperspectral\
    \ missions are expected to solve the data quality issue.\n2.2. Airplane-Based\
    \ Hyperspectral Imaging\nAirborne hyperspectral imaging has been widely used to\
    \ collect hyperspectral imagery for\ndiﬀerent monitoring purposes (e.g., for agriculture\
    \ or forestry). The ﬁrst hyperspectral sensor was an\nairborne visible/infrared\
    \ imaging spectrometer (AVIRIS) that was developed and utilized in 1987 [84].\n\
    It collects spectral signals in 224 bands in the visible to SWIR range (Table\
    \ 2). Researchers have applied\nAVIRIS data to help understand a wide range of\
    \ agricultural features, such as investigating vegetation\nproperties (e.g., yield,\
    \ LAI, chlorophyll, and water content) [85–88], analyzing soil properties [89],\n\
    evaluating crop health or identifying pest infestation [90–92], and mapping crop\
    \ area or agricultural\ntillage practices [93,94].\nBesides AVIRIS, the Compact\
    \ Airborne Spectrographic Imager (CASI), Hyperspectral Mapper\n(HyMap), and AISA\
    \ Eagle are also widely used airborne hyperspectral sensors (Table 2). For instance,\n\
    CASI images have been used for estimating crop chlorophyll content [95], investigating\
    \ crop cover\nfraction [96], classifying weeds [97], and delineating management\
    \ zones [2]. The HyMap imagery\nhas been applied to examining crop biophysical\
    \ and biochemical variables (e.g., LAI, chlorophyll and\nwater content) [98–100],\
    \ detecting plant stress signals [101], and investigating the spatial patterns\
    \ of\nSOC [102]. Regarding AISA Eagle imagery, Ryu et al. [35] and Cilia et al.\
    \ [103] used this data for\nestimating crop nitrogen content, and Ambrus et al.\
    \ [104] used it for estimating biomass.\nSeveral other airborne hyperspectral\
    \ sensors have also been used in previous studies. For instance,\nAVIS images\
    \ were used for investigating a range of vegetation characteristics (e.g., biomass\
    \ and\nchlorophyll) [105], Probe-1 hyperspectral images were used for investigating\
    \ crop residues [106],\nRDACS-H4 hyperspectral images were used for detecting\
    \ crop disease [34], AHS-160 hyperspectral\nsensor was used for mapping SOC [107],\
    \ the SWIR Hyper Spectral Imaging (HSI) sensor was used for\nestimating soil moisture\
    \ [108], the Pushbroom Hyperspectral Imager (PHI) was used for estimating\nwinter\
    \ wheat LAI [109], and airborne prism experiment (APEX) data were used for studying\
    \ the\nrelationship between SOC in croplands and the spectral signals [110].\n\
    Remote Sens. 2020, 12, 2659\n10 of 44\nMost of the aforementioned airborne hyperspectral\
    \ images have been acquired by airplanes at\nmedium to high altitude (e.g., 1–4\
    \ km altitude for CASI, 20 km for AVIRIS), and the acquired images\ngenerally\
    \ having high to medium spatial resolution, such as 4 m for CASI imagery, 5 m\
    \ for HyMap,\nand 20 m for AVIRIS [111–113]. Such spatial resolutions are appropriate\
    \ for mapping many crop and\nsoil features. However, image acquisition usually\
    \ needs to be scheduled months or even years in\nadvance, and ﬂight missions are\
    \ expensive [19]. Furthermore, for some speciﬁc applications, such as\ninvestigating\
    \ species-level or community-level features (e.g., identiﬁcation of weeds or early\
    \ signal\nof crop disease), images with very high spatial resolutions (e.g., sub-meter)\
    \ are preferred [114,115].\nIn addition, due to the unstable nature of airplanes\
    \ as imaging platforms, a gimbal or high-accuracy\ninertial measurement unit (IMU)\
    \ will be required to compensate for the orientation change of the\nairplanes\
    \ or recording the orientation information for subsequent image correction, respectively.\n\
    These factors limited the full application of airborne hyperspectral imaging in\
    \ precision agriculture.\nManned helicopters have also been used as platforms\
    \ for hyperspectral imaging and investigation\nof vegetation features [27,116].\
    \ Helicopters have more ﬂexible ﬂight heights (e.g., 100 m–2 km) than\nairplanes\
    \ and are capable of acquiring high-spatial-resolution images (e.g., sub-meter)\
    \ over large\nareas. An aviation company with a manned helicopter is generally\
    \ needed for the imaging task,\nwhich requires extra funding support and far advanced\
    \ pre-scheduling.\n2.3. UAV-Based Hyperspectral Imaging\nUAV has become a popular\
    \ platform in recent years for remote sensing data acquisition,\nespecially for\
    \ multispectral imaging using digital cameras or multispectral sensors. With the\
    \ increased\navailability of lightweight hyperspectral sensors, researchers have\
    \ experimented on mounting these\nsensors on UAVs to acquire high-spatial-resolution\
    \ hyperspectral imagery [19,117]. Diﬀerent types\nof UAVs, including multi-rotors,\
    \ helicopters, and ﬁxed wings, have been utilized in previous studies\n(Figure\
    \ 3). Compared with manned airplanes and helicopters, UAVs are capable of acquiring\
    \ high-\nspatial-resolution images with a much lower cost and have high ﬂexibility\
    \ in terms of scheduling a\nﬂight mission [118]. Several speciﬁc agricultural\
    \ applications of UAV-based hyperspectral imaging are\nsummarized in Table 3.\n\
    Remote Sens. 2020, 12, x FOR PEER REVIEW \n10 of 43 \nand 20 m for AVIRIS [111–113].\
    \ Such spatial resolutions are appropriate for mapping many crop and \nsoil features.\
    \ However, image acquisition usually needs to be scheduled months or even years\
    \ in \nadvance, and flight missions are expensive [19]. Furthermore, for some\
    \ specific applications, such as \ninvestigating species-level or community-level\
    \ features (e.g., identification of weeds or early signal \nof crop disease),\
    \ images with very high spatial resolutions (e.g., sub-meter) are preferred [114,115].\
    \ \nIn addition, due to the unstable nature of airplanes as imaging platforms,\
    \ a gimbal or high-accuracy \ninertial measurement unit (IMU) will be required\
    \ to compensate for the orientation change of the \nairplanes or recording the\
    \ orientation information for subsequent image correction, respectively. \nThese\
    \ factors limited the full application of airborne hyperspectral imaging in precision\
    \ agriculture. \nManned helicopters have also been used as platforms for hyperspectral\
    \ imaging and investigation of \nvegetation features [27,116]. Helicopters have\
    \ more flexible flight heights (e.g., 100 m–2 km) than \nairplanes and are capable\
    \ of acquiring high-spatial-resolution images (e.g., sub-meter) over large \n\
    areas. An aviation company with a manned helicopter is generally needed for the\
    \ imaging task, \nwhich requires extra funding support and far advanced pre-scheduling.\
    \  \n2.3. UAV-Based Hyperspectral Imaging \nUAV has become a popular platform\
    \ in recent years for remote sensing data acquisition, \nespecially for multispectral\
    \ imaging using digital cameras or multispectral sensors. With the \nincreased\
    \ availability of lightweight hyperspectral sensors, researchers have experimented\
    \ on \nmounting these sensors on UAVs to acquire high-spatial-resolution hyperspectral\
    \ imagery [19,117]. \nDifferent types of UAVs, including multi-rotors, helicopters,\
    \ and fixed wings, have been utilized in \nprevious studies (Figure 3). Compared\
    \ with manned airplanes and helicopters, UAVs are capable of \nacquiring high-spatial-resolution\
    \ images with a much lower cost and have high flexibility in terms of \nscheduling\
    \ a flight mission [118]. Several specific agricultural applications of UAV-based\
    \ \nhyperspectral imaging are summarized in Table 3. \n \nFigure 3. Hyperspectral\
    \ UAV systems used in previous agricultural studies. Figures were reproduced \n\
    with permission from the corresponding publishers: (a) MDPI [119], (b) MDPI [120],\
    \ (c) MDPI [121], \nand (d) SPIE [122]. \nTable 3. Example applications of UAV-based\
    \ hyperspectral imaging in agriculture. \nApplications \nPrevious \nStudies \n\
    Research Focuses \nEstimating LAI \nand chlorophyll \nYu et al. [37] \nEstimated\
    \ a range of vegetation phenotyping variables \n(e.g., LAI and leaf chlorophyll)\
    \ using UAV-based \nhyperspectral imagery and radiative transfer modelling. \n\
    (a) \n(b) \n(d) \n(c) \nFigure 3. Hyperspectral UAV systems used in previous agricultural\
    \ studies. Figures were reproduced\nwith permission from the corresponding publishers:\
    \ (a) MDPI [119], (b) MDPI [120], (c) MDPI [121],\nand (d) SPIE [122].\nRemote\
    \ Sens. 2020, 12, 2659\n11 of 44\nTable 3. Example applications of UAV-based hyperspectral\
    \ imaging in agriculture.\nApplications\nPrevious Studies\nResearch Focuses\n\
    Estimating LAI and\nchlorophyll\nYu et al. [37]\nEstimated a range of vegetation\
    \ phenotyping variables\n(e.g., LAI and leaf chlorophyll) using UAV-based\nhyperspectral\
    \ imagery and radiative transfer modelling.\nEstimating biomass\nHonkavaara et\
    \ al. [123]\nMounted a hyperspectral sensor and a consumer-level\ncamera on a\
    \ UAV for estimating biomass in a wheat and\na barley ﬁeld.\nYue et al. [124]\n\
    Utilized UAV-based hyperspectral images for estimating\nwinter wheat above-ground\
    \ biomass.\nEstimating nitrogen\ncontent\nPölönen et al. [125]\nUsed lightweight\
    \ UAVs for collecting hyperspectral\nimages and estimated crop biomass and nitrogen\
    \ content.\nKaivosoja et al. [126]\nApplied UAV-based hyperspectral imagery to\
    \ investigate\nbiomass and nitrogen contents in a wheat ﬁeld.\nAkhtman et al.\
    \ [127]\nUtilized UAV-based hyperspectral images for estimating\nnitrogen content\
    \ and phytomass in corn and wheat ﬁelds\nand monitored temporal variations of\
    \ these properties.\nEstimating water\ncontent\nIzzo et al. [128]\nEvaluated water\
    \ content in the commercial vineyard\nusing UAV-based hyperspectral images and\
    \ determined\nwavelengths sensitive to canopy water content.\nClassifying weeds\n\
    Scherrer et al. [129]\nClassiﬁed herbicide-resistant weeds in diﬀerent crop\n\
    ﬁelds (e.g., barley, corn, and dry pea) using both ground-\nand UAV-based hyperspectral\
    \ imagery.\nDetecting disease\nBohnenkamp et al. [119]\nUsed both ground- and\
    \ UAV-based hyperspectral images\nfor detecting yellow rust in wheat.\nVarious\
    \ lightweight hyperspectral sensors have been developed in recent years and can\
    \ be\nmounted on UAVs. Examples of sensors include the widely-used Headwall Micro-\
    \ and Nano-Hyperspec\nVNIR [12,13,26,128], UHD 185-Fireﬂy [53,130], the PIKA II\
    \ sensor [19,32], and the HySpex VNIR [25,131].\nThese hyperspectral sensors contain\
    \ more than 100 bands in the visible-near infrared spectral range\n(Table 2).\
    \ These sensors are small and compact (1–2 kg), thus they can be deployed quickly\
    \ on various\nmanned or unmanned remote sensing platforms. Previous studies conducted\
    \ by Adão et al. [11] and\nLodhi et al. [52] also compared and summarized various\
    \ lightweight hyperspectral sensors.\nA large number of factors need to be considered\
    \ in the application of UAV-based hyperspectral\nimaging, ranging from sensor\
    \ setup and data collection, to image processing. Saari et al. [122] tested\n\
    the feasibility of a UAV-based hyperspectral imaging system for agricultural and\
    \ forest applications\nand discussed several challenges regarding the imaging\
    \ technology (e.g., hardware requirements\nand system settings).\nAasen et al.\
    \ [132] focused on the calibration of images collected with a\nframe-based sensor\
    \ and discussed several challenges related to the use of UAV-based hyperspectral\n\
    imaging for vegetation and crop investigation (e.g., the payload of UAV, signal-to-noise\
    \ ratio, and\nspectral calibration). Habib et al. [120] attempted to perform orthorectiﬁcation\
    \ of UAV-acquired\npushbroom-based hyperspectral imagery with frame-based RGB\
    \ images over an agricultural ﬁeld.\nAdão et al. [11] reviewed applications of\
    \ UAV-based hyperspectral imaging in agriculture and forestry\nand listed several\
    \ hyperspectral sensors that can be mounted on UAVs. The authors also discussed\n\
    several challenges in collecting and analyzing UAV-based hyperspectral imagery,\
    \ such as radiometric\nnoise, the low quality of UAV georeferencing, and a low\
    \ signal-to-noise ratio.\nUAV-based hyperspectral imaging has become more popular\
    \ in recent years; therefore, it is critical to\nreview its strengths and limitations.\
    \ To explore more features of this technology, this section of the review\nis\
    \ not limited to agricultural applications alone. Different types of UAVs have\
    \ been used as hyperspectral\nimaging platforms, with the two most widely used\
    \ as multi-rotors [130,133,134] and ﬁxed-wing\nplanes [33,120,135]. Slow ﬂights\
    \ at low altitudes are preferred to achieve high-spatial-resolution\nhyperspectral\
    \ imagery with a high signal-to-noise ratio. Thus, a multi-rotor is more competitive\
    \ than\nRemote Sens. 2020, 12, 2659\n12 of 44\nﬁxed-wing planes for hyperspectral\
    \ imaging in terms of ﬂight operation. Speciﬁcally, the multi-rotor\nallows for\
    \ a low ﬂight altitude, ﬂexible ﬂight speed, and vertical takeoﬀ and landing,\
    \ while the\nﬁxed wing requires a minimum ﬂight altitude, speed, and, sometimes,\
    \ accessories for takeoﬀ and\nlanding (e.g., runway, launcher, and parachute).\
    \ A hyperspectral imaging system, which consists\nof a hyperspectral sensor, a\
    \ data processing unit, a GPS, and an IMU, has a considerable weight\n(e.g., 1–3\
    \ kg), thus bringing challenges to the payload capacity of the UAV system and\
    \ its battery\nendurance. The multi-rotors are generally powered by high-performance\
    \ batteries (e.g., LiPo), and most\nhave a short endurance (e.g., less than 20\
    \ min). The endurance can be as short as 3 min [12]. In contrast,\nmany ﬁxed-wing\
    \ UAVs are powered by fuel, thus having a much longer endurance (e.g., 1–10 h)\
    \ [19,135].\nHowever, these ﬁxed-wing planes are mostly large and heavy (e.g.,\
    \ a 5 m wingspan and 14 kg take-oﬀ\nweight) [135], and thus bring challenges to\
    \ the ﬂight operation. Using UAV, researchers need to consider\nthe UAV SWaP (size,\
    \ weight, and power), geographical coverage, time aloft, altitude, and other variables.\n\
    In addition to the challenges in building a UAV system and performing ﬂight operations,\
    \ researchers\nlikely need to apply for ﬂight permission from an aviation authority\
    \ (e.g., Special Flight Operations\nCertiﬁcate (SFOC) from Transport Canada),\
    \ and purchase suitable UAV ﬂight insurance [136]. UAV size\nand weight are essential\
    \ parameters to consider in these processes. Furthermore, the UAVs are required\n\
    to be visible during ﬂight missions, so that the pilot can maintain constant visual\
    \ contact with the\naircraft. This could create a major challenge when ﬂying over\
    \ a large area, a hilly area, or an area\nwith forests.\n2.4. Close-Range (Ground-\
    \ or Lab-Based) Hyperspectral Imaging\nClose-range hyperspectral imaging, including\
    \ ground (Figure 4a–c) or lab based (Figure 4d,e),\nis an emerging technology\
    \ in recent years, and it is capable of acquiring super-high-spatial-resolution\n\
    (e.g., cm or sub-cm level) hyperspectral imagery [137–139]. Therefore, this imaging\
    \ technology can\nbe used for investigating ﬁne-scale (e.g., leaf and canopy level)\
    \ vegetation features and thus greatly\nsupport the investigation of crop growing\
    \ status and detection of early signs of crop stress (e.g., disease,\nweeds, or\
    \ nutrition deﬁciency). Sensors are mounted on moving or static platforms (e.g.,\
    \ linear\nstages, scaﬀolds, or trucks) that can be deployed indoors or outdoors\
    \ for collecting images. Lamps\n(e.g., halogen lamp) or the sun are used as light\
    \ sources in these platforms, respectively.\nResearchers have utilized diﬀerent\
    \ types of platforms and hyperspectral sensors for collecting\nsuper-high-spatial-resolution\
    \ hyperspectral imagery to study diﬀerent agricultural features, as shown\nin\
    \ Table 4.\nTable 4. Example applications of close-range hyperspectral imaging\
    \ in previous studies.\nApplications\nPrevious Studies\nResearch Focuses\nInvestigating\n\
    biochemical\ncomponents\nFeng et al. [140]\nDesigned a hyperspectral imaging system\
    \ that consists of a\nHeadwall hyperspectral camera, a halogen lamp, a computer,\n\
    and a translation stage and used this system for taking images of\nrice leaves\
    \ to study leaf chlorophyll distribution.\nMohd Asaari et al. [141]\nMounted a\
    \ visible and near-infrared HIS camera in a\nhigh-throughput plant phenotyping\
    \ platform for evaluating\nplant water status and detecting early stage signs\
    \ of plant\ndrought stress.\nZhu et al. [142]\nInstalled a hyperspectral camera\
    \ and halogen lamp on a moving\nstage and used this imaging system for estimating\
    \ sugar and\nnitrogen contents in tomato leaves.\nDetecting crop\ndisease\nMorel\
    \ et al. [143]\nUsed a HySpex hyperspectral camera installed in a close-range\n\
    imaging system for investigating black leaf streak disease in\nbanana leaves.\n\
    Nagasubramanian et al. [144]\nIntegrated a Pika XC hyperspectral line imaging\
    \ scanner and\nhalogen illumination lamps for taking images of soybeans and\n\
    monitoring fungal disease.\nRemote Sens. 2020, 12, 2659\n13 of 44\nTable 4. Cont.\n\
    Applications\nPrevious Studies\nResearch Focuses\nIdentifying\nvegetation\nspecies\
    \ or\nweeds\nEddy et al. [139]\nMounted a hyperspectral sensor on a boom arm that\
    \ was\ninstalled on a truck for acquiring images at 1 m above the\nground and\
    \ applied the hyperspectral images to classifying\nweeds in diﬀerent crop ﬁelds.\n\
    Lopatin et al. [145]\nInstalled an AISA Eagle imaging spectrometer on a scaﬀold\
    \ at\nthe height of 2.5 m above ground, aiming to collect hyperspectral\nimagery\
    \ in a grassland area for classifying grassland species.\nPhenotyping\nBehmann\
    \ et al. [146]\nUtilized hyperspectral cameras and a close-range 3D laser\nscanner\
    \ that were mounted on a linear stage for collecting\nhyperspectral images and\
    \ 3D point models, respectively, and\nused these two datasets for generating hyperspectral\
    \ 3D plant\nmodels for better monitoring plant phenotyping features.\nMonitoring\
    \ soil\nproperties\nAntonucci et al. [147]\nAttempted to estimate copper concentration\
    \ in contaminated\nsoils using hyperspectral images that were acquired from a\n\
    lab-based spectral scanner.\nMalmir et al. [137]\nCollected close-range soil images\
    \ using Pika XC2 hyperspectral\ncamera that was mounted on a linear stage and\
    \ used the\nhyperspectral imagery for investigating soil macro- and\nmicro-elements.\n\
    Overall, the close-range hyperspectral imaging platform is capable of acquiring\
    \ super-high-\nspatial-resolution hyperspectral imagery that is critical for investigating\
    \ ﬁne-scale crop or soil features.\nThese features provide detailed information\
    \ about the plant’s biophysical and biochemical processes\nand how plants respond\
    \ to environmental stresses and diseases. However, the image collection and\n\
    processing also suﬀer from diﬀerent issues, such as uninformative variability\
    \ caused by the interaction\nof light with the plant structure (i.e., illumination\
    \ eﬀects), inﬂuences of shadows, and expanding\napplications of the platform to\
    \ a large scale [141,146]. Further research in these areas is warranted.\nRemote\
    \ Sens. 2020, 12, x FOR PEER REVIEW \n13 of 43 \n \nIdentifying \nvegetation \n\
    species or weeds \nEddy et al. [139] \nMounted a hyperspectral sensor on a boom\
    \ arm that \nwas installed on a truck for acquiring images at 1 m \nabove the\
    \ ground and applied the hyperspectral \nimages to classifying weeds in different\
    \ crop fields.  \nLopatin et al. [145] \nInstalled an AISA Eagle imaging spectrometer\
    \ on a \nscaffold at the height of 2.5 m above ground, aiming to \ncollect hyperspectral\
    \ imagery in a grassland area for \nclassifying grassland species. \nPhenotyping\
    \ \nBehmann et al. \n[146] \nUtilized hyperspectral cameras and a close-range\
    \ 3D \nlaser scanner that were mounted on a linear stage for \ncollecting hyperspectral\
    \ images and 3D point models, \nrespectively, and used these two datasets for\
    \ \ngenerating hyperspectral 3D plant models for better \nmonitoring plant phenotyping\
    \ features. \nMonitoring soil \nproperties \nAntonucci et al. \n[147] \nAttempted\
    \ to estimate copper concentration in \ncontaminated soils using hyperspectral\
    \ images that \nwere acquired from a lab-based spectral scanner.  \nMalmir et\
    \ al. [137] \nCollected close-range soil images using Pika XC2 \nhyperspectral\
    \ camera that was mounted on a linear \nstage and used the hyperspectral imagery\
    \ for \ninvestigating soil macro- and micro-elements. \nOverall, the close-range\
    \ hyperspectral imaging platform is capable of acquiring super-high-\nspatial-resolution\
    \ hyperspectral imagery that is critical for investigating fine-scale crop or\
    \ soil \nfeatures. These features provide detailed information about the plant’s\
    \ biophysical and biochemical \nprocesses and how plants respond to environmental\
    \ stresses and diseases. However, the image \ncollection and processing also suffer\
    \ from different issues, such as uninformative variability caused \nby the interaction\
    \ of light with the plant structure (i.e., illumination effects), influences of\
    \ shadows, \nand expanding applications of the platform to a large scale [141,146].\
    \ Further research in these areas \nis warranted. \n \nFigure 4. Close-range imaging\
    \ platforms used in previous studies. Figures were reproduced with \npermission\
    \ from corresponding publishers: (a) American Society for Photogrammetry and Remote\
    \ \nSensing (ASPRS), Bethesda, Maryland, asprs.org [139]; (b) SPIE [148]; (c)\
    \ Elsevier [138]; (d) Springer \nNature [144]; (e) Elsevier [149]. \nIn summary,\
    \ different hyperspectral imaging platforms, including satellites, airplanes,\
    \ \nhelicopters, UAVs, and close-range, have different advantages and disadvantages\
    \ for applications in \nprecision agriculture. Detailed comparisons of these platforms\
    \ for agricultural applications are \nshown in Table 5. In brief, satellite-based\
    \ systems provide images covering large areas but suffer from \n(a) \n(b) \n(c)\
    \ \n(d) \n(e) \nFigure 4. Close-range imaging platforms used in previous studies.\
    \ Figures were reproduced with\npermission from corresponding publishers: (a)\
    \ American Society for Photogrammetry and Remote\nSensing (ASPRS), Bethesda, Maryland,\
    \ asprs.org [139]; (b) SPIE [148]; (c) Elsevier [138]; (d) Springer\nNature [144];\
    \ (e) Elsevier [149].\nIn summary, diﬀerent hyperspectral imaging platforms, including\
    \ satellites, airplanes, helicopters,\nUAVs, and close-range, have diﬀerent advantages\
    \ and disadvantages for applications in precision\nagriculture. Detailed comparisons\
    \ of these platforms for agricultural applications are shown in\nTable 5. In brief,\
    \ satellite-based systems provide images covering large areas but suﬀer from medium\n\
    spatial resolution and limited data availability (e.g., a limited number of operating\
    \ sensors and long\nRemote Sens. 2020, 12, 2659\n14 of 44\nrevisit time). Airplane-\
    \ and helicopter-based imaging platforms acquire data with suitable spatial\n\
    coverage and resolution for most of the agricultural applications. However, they\
    \ are limited by\na high mission cost and scheduling challenges and thus are not\
    \ suitable for repeated monitoring.\nUAV-based systems are capable of acquiring\
    \ high-spatial resolution images repeatedly and have high\nﬂexibility. However,\
    \ they can only cover a small area due to the limited battery endurance and aviation\n\
    regulations. The close-range imaging systems are capable of obtaining super-high-spatial-resolution\n\
    images, but they can only be used at leaf or canopy levels. Therefore, the following\
    \ factors should be\ntaken into consideration when selecting a platform for a\
    \ speciﬁc research project: spatial resolution\nneeded for the study, ﬂight area\
    \ and ﬂight endurance, weight of the imaging system, platform payload\ncapacity,\
    \ ﬂight safety and regulations, operation ﬂexibility, and cost.\nTable 5. Comparison\
    \ of hyperspectral imaging platforms.\nSatellites\nAirplanes\nHelicopters\nFixed-Wing\n\
    UAVs\nMulti-Rotor\nUAVs\nClose-Range\nPlatforms\nExample\nPhotos\nRemote Sens.\
    \ 2020, 12, x FOR PEER REVIEW \n14 of 43 \n \nmedium spatial resolution and limited\
    \ data availability (e.g., a limited number of operating sensors \nand long revisit\
    \ time). Airplane- and helicopter-based imaging platforms acquire data with suitable\
    \ \nspatial coverage and resolution for most of the agricultural applications.\
    \ However, they are limited \nby a high mission cost and scheduling challenges\
    \ and thus are not suitable for repeated monitoring. \nUAV-based systems are capable\
    \ of acquiring high-spatial resolution images repeatedly and have \nhigh flexibility.\
    \ However, they can only cover a small area due to the limited battery endurance\
    \ and \naviation regulations. The close-range imaging systems are capable of obtaining\
    \ super-high-spatial-\nresolution images, but they can only be used at leaf or\
    \ canopy levels. Therefore, the following factors \nshould be taken into consideration\
    \ when selecting a platform for a specific research project: spatial \nresolution\
    \ needed for the study, flight area and flight endurance, weight of the imaging\
    \ system, \nplatform payload capacity, flight safety and regulations, operation\
    \ flexibility, and cost. \nTable 5. Comparison of hyperspectral imaging platforms.\
    \ \n \nSatellites \nAirplanes \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor\
    \ \nUAVs \nClose-\nRange \nPlatforms \nExample \nPhotos \n \n(Photo: \nSwales\
    \ \nAerospace) \n \n \n \n \n(Photo: \nASPRS) \nOperational \nAltitudes \n400–700\
    \ \nkm \n1–20 km \n100 m–2 km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large\
    \ \nMedium—\nlarge \nMedium \nSmall—\nmedium \nSmall \nVery small \ne.g., one\
    \ \nHyperion \nscene \ncovers 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers\
    \ \n~100 km2 \n~10 km2 \n~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution\
    \ \n20–60 m \n1–20 m \n0.1–1 m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution\
    \ \nDays to \nweeks \nDepends on flight operations (hours to days) \nFlexibility\
    \ \nLow (e.g., \nfixed \nrepeating \ncycles) \nMedium (e.g., limited by the \n\
    availability of aviation \ncompany) \nHigh \nOperational \nComplexity \nLow (Final\
    \ \ndata \nprovided \nto users) \nMedium (Depends on who \noperates the sensor,\
    \ users or \ndata vendors) \nHigh (users typically operate sensors \nand need\
    \ to set up hardware and \nsoftware properly) \nApplicable \nScales \nRegional—\n\
    global \nLandscape—regional \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting\
    \ \nFactors \nWeather \n(e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed,\
    \ unstable \nillumination conditions \nShort battery \nendurance (e.g., 10–30\
    \ \nmin), flight \nregulations \nPlatform \ndesign and \noperation \n(Photo: Swales\n\
    Aerospace)\nRemote Sens. 2020, 12, x FOR PEER REVIEW \n14 of 43 \n \nmedium spatial\
    \ resolution and limited data availability (e.g., a limited number of operating\
    \ sensors \nand long revisit time). Airplane- and helicopter-based imaging platforms\
    \ acquire data with suitable \nspatial coverage and resolution for most of the\
    \ agricultural applications. However, they are limited \nby a high mission cost\
    \ and scheduling challenges and thus are not suitable for repeated monitoring.\
    \ \nUAV-based systems are capable of acquiring high-spatial resolution images\
    \ repeatedly and have \nhigh flexibility. However, they can only cover a small\
    \ area due to the limited battery endurance and \naviation regulations. The close-range\
    \ imaging systems are capable of obtaining super-high-spatial-\nresolution images,\
    \ but they can only be used at leaf or canopy levels. Therefore, the following\
    \ factors \nshould be taken into consideration when selecting a platform for a\
    \ specific research project: spatial \nresolution needed for the study, flight\
    \ area and flight endurance, weight of the imaging system, \nplatform payload\
    \ capacity, flight safety and regulations, operation flexibility, and cost. \n\
    Table 5. Comparison of hyperspectral imaging platforms. \n \nSatellites \nAirplanes\
    \ \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor \nUAVs \nClose-\nRange \n\
    Platforms \nExample \nPhotos \n \n(Photo: \nSwales \nAerospace) \n \n \n \n \n\
    (Photo: \nASPRS) \nOperational \nAltitudes \n400–700 \nkm \n1–20 km \n100 m–2\
    \ km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large \nMedium—\nlarge \nMedium\
    \ \nSmall—\nmedium \nSmall \nVery small \ne.g., one \nHyperion \nscene \ncovers\
    \ 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers \n~100 km2 \n~10 km2 \n\
    ~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution \n20–60 m \n1–20 m \n0.1–1\
    \ m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution \nDays to \nweeks \n\
    Depends on flight operations (hours to days) \nFlexibility \nLow (e.g., \nfixed\
    \ \nrepeating \ncycles) \nMedium (e.g., limited by the \navailability of aviation\
    \ \ncompany) \nHigh \nOperational \nComplexity \nLow (Final \ndata \nprovided\
    \ \nto users) \nMedium (Depends on who \noperates the sensor, users or \ndata\
    \ vendors) \nHigh (users typically operate sensors \nand need to set up hardware\
    \ and \nsoftware properly) \nApplicable \nScales \nRegional—\nglobal \nLandscape—regional\
    \ \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting \nFactors \nWeather \n\
    (e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed, unstable \nillumination\
    \ conditions \nShort battery \nendurance (e.g., 10–30 \nmin), flight \nregulations\
    \ \nPlatform \ndesign and \noperation \nRemote Sens. 2020, 12, x FOR PEER REVIEW\
    \ \n14 of 43 \n \nmedium spatial resolution and limited data availability (e.g.,\
    \ a limited number of operating sensors \nand long revisit time). Airplane- and\
    \ helicopter-based imaging platforms acquire data with suitable \nspatial coverage\
    \ and resolution for most of the agricultural applications. However, they are\
    \ limited \nby a high mission cost and scheduling challenges and thus are not\
    \ suitable for repeated monitoring. \nUAV-based systems are capable of acquiring\
    \ high-spatial resolution images repeatedly and have \nhigh flexibility. However,\
    \ they can only cover a small area due to the limited battery endurance and \n\
    aviation regulations. The close-range imaging systems are capable of obtaining\
    \ super-high-spatial-\nresolution images, but they can only be used at leaf or\
    \ canopy levels. Therefore, the following factors \nshould be taken into consideration\
    \ when selecting a platform for a specific research project: spatial \nresolution\
    \ needed for the study, flight area and flight endurance, weight of the imaging\
    \ system, \nplatform payload capacity, flight safety and regulations, operation\
    \ flexibility, and cost. \nTable 5. Comparison of hyperspectral imaging platforms.\
    \ \n \nSatellites \nAirplanes \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor\
    \ \nUAVs \nClose-\nRange \nPlatforms \nExample \nPhotos \n \n(Photo: \nSwales\
    \ \nAerospace) \n \n \n \n \n(Photo: \nASPRS) \nOperational \nAltitudes \n400–700\
    \ \nkm \n1–20 km \n100 m–2 km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large\
    \ \nMedium—\nlarge \nMedium \nSmall—\nmedium \nSmall \nVery small \ne.g., one\
    \ \nHyperion \nscene \ncovers 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers\
    \ \n~100 km2 \n~10 km2 \n~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution\
    \ \n20–60 m \n1–20 m \n0.1–1 m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution\
    \ \nDays to \nweeks \nDepends on flight operations (hours to days) \nFlexibility\
    \ \nLow (e.g., \nfixed \nrepeating \ncycles) \nMedium (e.g., limited by the \n\
    availability of aviation \ncompany) \nHigh \nOperational \nComplexity \nLow (Final\
    \ \ndata \nprovided \nto users) \nMedium (Depends on who \noperates the sensor,\
    \ users or \ndata vendors) \nHigh (users typically operate sensors \nand need\
    \ to set up hardware and \nsoftware properly) \nApplicable \nScales \nRegional—\n\
    global \nLandscape—regional \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting\
    \ \nFactors \nWeather \n(e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed,\
    \ unstable \nillumination conditions \nShort battery \nendurance (e.g., 10–30\
    \ \nmin), flight \nregulations \nPlatform \ndesign and \noperation \nRemote Sens.\
    \ 2020, 12, x FOR PEER REVIEW \n14 of 43 \n \nmedium spatial resolution and limited\
    \ data availability (e.g., a limited number of operating sensors \nand long revisit\
    \ time). Airplane- and helicopter-based imaging platforms acquire data with suitable\
    \ \nspatial coverage and resolution for most of the agricultural applications.\
    \ However, they are limited \nby a high mission cost and scheduling challenges\
    \ and thus are not suitable for repeated monitoring. \nUAV-based systems are capable\
    \ of acquiring high-spatial resolution images repeatedly and have \nhigh flexibility.\
    \ However, they can only cover a small area due to the limited battery endurance\
    \ and \naviation regulations. The close-range imaging systems are capable of obtaining\
    \ super-high-spatial-\nresolution images, but they can only be used at leaf or\
    \ canopy levels. Therefore, the following factors \nshould be taken into consideration\
    \ when selecting a platform for a specific research project: spatial \nresolution\
    \ needed for the study, flight area and flight endurance, weight of the imaging\
    \ system, \nplatform payload capacity, flight safety and regulations, operation\
    \ flexibility, and cost. \nTable 5. Comparison of hyperspectral imaging platforms.\
    \ \n \nSatellites \nAirplanes \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor\
    \ \nUAVs \nClose-\nRange \nPlatforms \nExample \nPhotos \n \n(Photo: \nSwales\
    \ \nAerospace) \n \n \n \n \n(Photo: \nASPRS) \nOperational \nAltitudes \n400–700\
    \ \nkm \n1–20 km \n100 m–2 km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large\
    \ \nMedium—\nlarge \nMedium \nSmall—\nmedium \nSmall \nVery small \ne.g., one\
    \ \nHyperion \nscene \ncovers 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers\
    \ \n~100 km2 \n~10 km2 \n~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution\
    \ \n20–60 m \n1–20 m \n0.1–1 m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution\
    \ \nDays to \nweeks \nDepends on flight operations (hours to days) \nFlexibility\
    \ \nLow (e.g., \nfixed \nrepeating \ncycles) \nMedium (e.g., limited by the \n\
    availability of aviation \ncompany) \nHigh \nOperational \nComplexity \nLow (Final\
    \ \ndata \nprovided \nto users) \nMedium (Depends on who \noperates the sensor,\
    \ users or \ndata vendors) \nHigh (users typically operate sensors \nand need\
    \ to set up hardware and \nsoftware properly) \nApplicable \nScales \nRegional—\n\
    global \nLandscape—regional \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting\
    \ \nFactors \nWeather \n(e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed,\
    \ unstable \nillumination conditions \nShort battery \nendurance (e.g., 10–30\
    \ \nmin), flight \nregulations \nPlatform \ndesign and \noperation \nRemote Sens.\
    \ 2020, 12, x FOR PEER REVIEW \n14 of 43 \n \nmedium spatial resolution and limited\
    \ data availability (e.g., a limited number of operating sensors \nand long revisit\
    \ time). Airplane- and helicopter-based imaging platforms acquire data with suitable\
    \ \nspatial coverage and resolution for most of the agricultural applications.\
    \ However, they are limited \nby a high mission cost and scheduling challenges\
    \ and thus are not suitable for repeated monitoring. \nUAV-based systems are capable\
    \ of acquiring high-spatial resolution images repeatedly and have \nhigh flexibility.\
    \ However, they can only cover a small area due to the limited battery endurance\
    \ and \naviation regulations. The close-range imaging systems are capable of obtaining\
    \ super-high-spatial-\nresolution images, but they can only be used at leaf or\
    \ canopy levels. Therefore, the following factors \nshould be taken into consideration\
    \ when selecting a platform for a specific research project: spatial \nresolution\
    \ needed for the study, flight area and flight endurance, weight of the imaging\
    \ system, \nplatform payload capacity, flight safety and regulations, operation\
    \ flexibility, and cost. \nTable 5. Comparison of hyperspectral imaging platforms.\
    \ \n \nSatellites \nAirplanes \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor\
    \ \nUAVs \nClose-\nRange \nPlatforms \nExample \nPhotos \n \n(Photo: \nSwales\
    \ \nAerospace) \n \n \n \n \n(Photo: \nASPRS) \nOperational \nAltitudes \n400–700\
    \ \nkm \n1–20 km \n100 m–2 km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large\
    \ \nMedium—\nlarge \nMedium \nSmall—\nmedium \nSmall \nVery small \ne.g., one\
    \ \nHyperion \nscene \ncovers 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers\
    \ \n~100 km2 \n~10 km2 \n~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution\
    \ \n20–60 m \n1–20 m \n0.1–1 m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution\
    \ \nDays to \nweeks \nDepends on flight operations (hours to days) \nFlexibility\
    \ \nLow (e.g., \nfixed \nrepeating \ncycles) \nMedium (e.g., limited by the \n\
    availability of aviation \ncompany) \nHigh \nOperational \nComplexity \nLow (Final\
    \ \ndata \nprovided \nto users) \nMedium (Depends on who \noperates the sensor,\
    \ users or \ndata vendors) \nHigh (users typically operate sensors \nand need\
    \ to set up hardware and \nsoftware properly) \nApplicable \nScales \nRegional—\n\
    global \nLandscape—regional \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting\
    \ \nFactors \nWeather \n(e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed,\
    \ unstable \nillumination conditions \nShort battery \nendurance (e.g., 10–30\
    \ \nmin), flight \nregulations \nPlatform \ndesign and \noperation \nRemote Sens.\
    \ 2020, 12, x FOR PEER REVIEW \n14 of 43 \n \nmedium spatial resolution and limited\
    \ data availability (e.g., a limited number of operating sensors \nand long revisit\
    \ time). Airplane- and helicopter-based imaging platforms acquire data with suitable\
    \ \nspatial coverage and resolution for most of the agricultural applications.\
    \ However, they are limited \nby a high mission cost and scheduling challenges\
    \ and thus are not suitable for repeated monitoring. \nUAV-based systems are capable\
    \ of acquiring high-spatial resolution images repeatedly and have \nhigh flexibility.\
    \ However, they can only cover a small area due to the limited battery endurance\
    \ and \naviation regulations. The close-range imaging systems are capable of obtaining\
    \ super-high-spatial-\nresolution images, but they can only be used at leaf or\
    \ canopy levels. Therefore, the following factors \nshould be taken into consideration\
    \ when selecting a platform for a specific research project: spatial \nresolution\
    \ needed for the study, flight area and flight endurance, weight of the imaging\
    \ system, \nplatform payload capacity, flight safety and regulations, operation\
    \ flexibility, and cost. \nTable 5. Comparison of hyperspectral imaging platforms.\
    \ \n \nSatellites \nAirplanes \nHelicopters \nFixed-\nWing \nUAVs \nMulti-\nRotor\
    \ \nUAVs \nClose-\nRange \nPlatforms \nExample \nPhotos \n \n(Photo: \nSwales\
    \ \nAerospace) \n \n \n \n \n(Photo: \nASPRS) \nOperational \nAltitudes \n400–700\
    \ \nkm \n1–20 km \n100 m–2 km \n<150 m \n<10 m \nSpatial \nCoverage \nVery large\
    \ \nMedium—\nlarge \nMedium \nSmall—\nmedium \nSmall \nVery small \ne.g., one\
    \ \nHyperion \nscene \ncovers 42 \nkm × 7.7 \nkm \nA 10-min flight/operation covers\
    \ \n~100 km2 \n~10 km2 \n~5 km2 \n~0.5 km2 \n~0.005 km2 \nSpatial \nResolution\
    \ \n20–60 m \n1–20 m \n0.1–1 m \n0.01–0.5 m \n0.0001–0.01 \nm \nTemporal \nResolution\
    \ \nDays to \nweeks \nDepends on flight operations (hours to days) \nFlexibility\
    \ \nLow (e.g., \nfixed \nrepeating \ncycles) \nMedium (e.g., limited by the \n\
    availability of aviation \ncompany) \nHigh \nOperational \nComplexity \nLow (Final\
    \ \ndata \nprovided \nto users) \nMedium (Depends on who \noperates the sensor,\
    \ users or \ndata vendors) \nHigh (users typically operate sensors \nand need\
    \ to set up hardware and \nsoftware properly) \nApplicable \nScales \nRegional—\n\
    global \nLandscape—regional \nCanopy—landscape \nLeaf—\ncanopy \nMajor \nLimiting\
    \ \nFactors \nWeather \n(e.g., rain \nand \nclouds) \nUnfavorable flight \nheight/speed,\
    \ unstable \nillumination conditions \nShort battery \nendurance (e.g., 10–30\
    \ \nmin), flight \nregulations \nPlatform \ndesign and \noperation \n(Photo:\n\
    ASPRS)\nOperational\nAltitudes\n400–700 km\n1–20 km\n100 m–2 km\n<150 m\n<10 m\n\
    Spatial\nCoverage\nVery large\nMedium—large\nMedium\nSmall—medium\nSmall\nVery\
    \ small\ne.g., one\nHyperion scene\ncovers 42 km ×\n7.7 km\nA 10-min ﬂight/operation\
    \ covers\n~100 km2\n~10 km2\n~5 km2\n~0.5 km2\n~0.005 km2\nSpatial\nResolution\n\
    20–60 m\n1–20 m\n0.1–1 m\n0.01–0.5 m\n0.0001–0.01 m\nTemporal\nResolution\nDays\
    \ to weeks\nDepends on ﬂight operations (hours to days)\nFlexibility\nLow (e.g.,\
    \ ﬁxed\nrepeating cycles)\nMedium (e.g., limited by the\navailability of aviation\n\
    company)\nHigh\nOperational\nComplexity\nLow (Final data\nprovided to\nusers)\n\
    Medium (Depends on who\noperates the sensor, users or\ndata vendors)\nHigh (users\
    \ typically operate sensors and need\nto set up hardware and software properly)\n\
    Applicable\nScales\nRegional—\nglobal\nLandscape—regional\nCanopy—landscape\n\
    Leaf—canopy\nMajor\nLimiting\nFactors\nWeather (e.g.,\nrain and clouds)\nUnfavorable\
    \ ﬂight\nheight/speed, unstable\nillumination conditions\nShort battery endurance\
    \ (e.g.,\n10–30 min), ﬂight regulations\nPlatform\ndesign and\noperation\nImage\n\
    Acquisition\nCost\nLow to medium\nHigh (typically requires hiring\nan aviation\
    \ company to ﬂy)\nHigh (If need to cover a large area)\nNumber of\npublications\
    \ *\n59\n133\n3\n4\n38\n79\n* The number of publications was counted based on\
    \ which speciﬁc platform was used in each of the\nliterature reviewed.\n3. Methods\
    \ for Processing and Analyzing Hyperspectral Images\nHyperspectral images acquired\
    \ by diﬀerent platforms and sensors are typically provided in a\nraw format (e.g.,\
    \ digital numbers) that needs to be pre-processed (e.g., atmospheric, radiometric,\n\
    and spectral corrections) to retrieve accurate spectral information. Afterward,\
    \ diﬀerent approaches can\nbe used for analyzing the hyperspectral information\
    \ and investigating various agricultural features\nRemote Sens. 2020, 12, 2659\n\
    15 of 44\n(e.g., crop and soil properties). A few commonly used methods include\
    \ linear regression, advanced\nregression (e.g., PLSR), machine learning and deep\
    \ learning (e.g., RF, CNN), and radiative transfer\nmodelling (e.g., PROSPECT\
    \ and PROSAIL). Researchers have used one or more of these methods for\ninvestigations\
    \ of diﬀerent agricultural features. In this section, the review is arranged based\
    \ on the\ndiﬀerent methods used in the studies.\n3.1. Pre-Processing of Hyperspectral\
    \ Images\nTypical processing of hyperspectral imagery includes geometric correction,\
    \ orthorectiﬁcation,\nradiometric correction, and atmospheric correction. For\
    \ satellite- and airplane-based hyperspectral\nimages, the geometric and orthorectiﬁcation\
    \ correction are generally performed by data providers,\nand the radiometric and\
    \ atmospheric corrections can be done following standard image processing steps\n\
    available in remote sensing software. For UAV-based images, in contrast, the users\
    \ need to conduct\nthese processing steps and decide on appropriate processing\
    \ methods and associated parameters.\nFor instance, a digital elevation model\
    \ (DEM) and ground control points (GCPs) are usually needed\nfor performing the\
    \ orthorectiﬁcation and geometric correction [12]. If the sensor mounted on UAV\n\
    is pushbroom based, accurate sensor orientation information recorded by an IMU\
    \ will be needed\nfor these corrections, and the IMU needs to be integrated into\
    \ the UAV and well-calibrated [12,27].\nSoftware packages commonly used in previous\
    \ studies for performing these corrections on UAV-based\nhyperspectral images\
    \ include ENVI (Exelis Visual Information Solutions, Boulder, CO, USA) and\nPARGE\
    \ (ReSe Applications Schläpfer, Wil, Switzerland) [12,26,117].\nRadiometric correction\
    \ is conducted to convert image digital numbers to radiance using calibration\n\
    coeﬃcients that are provided by the sensor manufacturer [11]. These coeﬃcients\
    \ may need to be updated\nover time due to the degradation of spectral materials\
    \ used to construct the hyperspectral sensors.\nRegarding atmospheric correction,\
    \ although the UAVs are ﬂown at low altitudes, the signals acquired\nare still\
    \ subjective to the inﬂuence of various atmospheric absorptions and scatterings,\
    \ such as oxygen\nabsorption at 760nm; water absorption near 820, 940, 1140, 1380,\
    \ and 1880 nm; and carbon dioxide\nabsorption at 2010 and 2060 nm [12,13,26,150].\
    \ Therefore, atmospheric correction is critical for obtaining\ngood-quality spectral\
    \ information. However, Adão et al. [11] suggest that this process might be skipped\n\
    if the UAVs are operated close to the ground. Therefore, the application of atmospheric\
    \ correction will\ndepend on speciﬁc ﬂight missions and research purposes (e.g.,\
    \ ﬂight altitudes, if atmosphere-inﬂuenced\nspectral bands are needed). Software\
    \ or methods commonly used in previous studies for performing\natmospheric correction\
    \ on UAV-based hyperspectral images include the MODTRAN model (Spectral\nSciences\
    \ Inc.), ENVI FLAASH (L3Harris Geospatial), PCI Geomatica (PCI Geomatics Corporate),\n\
    SMARTS model (Solar Consulting Services), and empirical line correction [12,19,27,32,33,116].\n\
    Hyperspectral images typically have hundreds of bands, and many of them are highly\
    \ correlated.\nTherefore, dimension reduction is also an essential procedure to\
    \ consider in the pre-processing of\nhyperspectral imagery. Many previous studies\
    \ using hyperspectral imagery have discussed the\nchallenges of data redundancy\
    \ and have used diﬀerent methods for dimension reduction. For instance,\nMiglani\
    \ et al. [151] performed principal component analysis (PCA) on hyperspectral images\
    \ and\nindicated that 99% of the information could be explained in the ﬁrst 10\
    \ principal components.\nAmato et al. [152] discussed a few previous methods of\
    \ dimension reduction, such as PCA, minimum\nnoise fraction (MNF), and singular\
    \ value decomposition (SVD), and proposed a dimension reduction\nalgorithm based\
    \ on discriminant analysis for supervised classiﬁcation. Teke et al. [38] reviewed\n\
    several dimension reduction methods and summarized them based on transformation\
    \ techniques.\nThenkabail et al. [153] discussed the problems of high dimensionality\
    \ and listed a number of spectral\nbands that are more important for investigating\
    \ crop features. Sahoo et al. [4] reviewed diﬀerent\nmethods for dimension reduction,\
    \ such as PCA, uniform feature design (UMD), wavelet transforms,\nand artiﬁcial\
    \ neural networks (ANNs), and discussed their features of operation. Wang et al.\
    \ [154]\nproposed an auto-encoder-based dimensionality reduction method that is\
    \ a deep learning-based\napproach. Of these diﬀerent methods, the wavelet transform\
    \ is one of the most widely used ones for\nRemote Sens. 2020, 12, 2659\n16 of\
    \ 44\ndimension reduction. This technique decomposes a signal into a series of\
    \ scaled versions of the mother\nwavelet function and allows the variation of\
    \ the wavelet based on the frequency information to extract\nlocalized features\
    \ (e.g., local spectral variation) [155,156]. It has also been successfully used\
    \ for image\nfusion, feature extraction, and image classiﬁcation [156–158].\n\
    In addition to dimensionality reduction, band sensitivity analysis and band selection\
    \ have also\nbeen widely used in hyperspectral remote sensing to reduce the data\
    \ size by selecting only the bands\nthat are sensitive to the object of interest.\
    \ Diﬀerent algorithms have been proposed in previous studies\nfor band selection,\
    \ such as a fast volume-gradient-based method that is an unsupervised method and\n\
    removes the most redundant band successively based on the gradient of volume [159],\
    \ a column subset\nselection-based method that maximizes the volume of the selected\
    \ subset of columns (i.e., bands)\nand is robust to noisy bands [160], and a manifold\
    \ ranking-based salient band selection method that\nputs band vectors in manifold\
    \ space and selects a band-based ranking that can tackle the problem of\ninappropriate\
    \ measurement of the band diﬀerence [161]. With the sensitivity analysis, previous\
    \ studies\nhave identiﬁed spectral bands that are sensitive to diﬀerent crop properties,\
    \ for instance, ~515, ~550,\n~570, ~670, 700–740, ~800, and ~855 nm for investigating\
    \ chlorophyll content; ~405, ~515, ~570, ~705,\nand ~720 nm for evaluating nitrogen\
    \ status; ~970, ~1180, ~1245, ~1450, and ~1950 nm for assessing\nwater content;\
    \ ~682, ~855, ~910, ~970, ~1075, ~1245, ~1518, ~1725, and ~2260 nm for estimating\n\
    biomass; and ~550, ~682, ~855, ~1075, ~1180, ~1450, and ~1725 nm for crop classiﬁcation\
    \ [36,44,153,162].\nOverall, pre-processing is an essential step for improving\
    \ the quality of hyperspectral images and\npreparing for further data analysis.\
    \ After the pre-processing, the analytical methods to be discussed\nbelow can\
    \ be used for analyzing the hyperspectral information and investigating various\
    \ agricultural\nfeatures on the ground.\n3.2. Empirical Relationships\nLinear\
    \ regression is a widely used method for analyzing hyperspectral imagery and retrieving\n\
    target information (e.g., crop and soil properties). Both spectral reﬂectance\
    \ and vegetation indices can\nbe used as predictor variables in establishing a\
    \ linear relationship. For instance, using spectral bands,\nFinn et al. [108]\
    \ built linear regressions between ﬁeld-measured soil moisture data and the spectral\n\
    reﬂectance of collected hyperspectral imagery and identiﬁed bands that have stronger\
    \ correlations with\nsoil moisture. More studies have used vegetation indices\
    \ in the regression for a better performance as\nsome indices can enhance the\
    \ signal of targeted features and minimize the background noise. Some of\nthe\
    \ previous studies are shown in Table 6.\nTable 6. Selected previous studies utilized\
    \ linear regression and hyperspectral vegetation indices for\ninvestigating agricultural\
    \ features.\nApplications\nPrevious Studies\nResearch Focuses\nEstimating leaf\n\
    chlorophyll and\nnitrogen content\nOppelt and Mauser [105]\nUtilized the Chlorophyll\
    \ Absorption Integral (CAI), Optimized\nSoil-Adjusted Vegetation Index (OSAVI),\
    \ and hyperspectral\nNormalized Diﬀerence Vegetation Index (h NDVI) for estimating\n\
    leaf chlorophyll and nitrogen content from hyperspectral\nimagery and evaluated\
    \ the performance of each of the indices.\nWu et al. [45]\nTested a range of vegetation\
    \ indices (e.g., NDVI, Simple Ratio\n(SR), and Triangular Vegetation Index (TVI))\
    \ for retrieving\nvegetation chlorophyll content and LAI from Hyperion images\n\
    and determined the indices that produced high accuracies.\nCilia et al. [103]\n\
    Utilized the Double-peak Canopy Nitrogen Index (DCNI) and\nModiﬁed Chlorophyll\
    \ Absorption Ratio Index/Modiﬁed\nTriangular Vegetation Index 2 (MCARI/MTVI2)\
    \ for estimating\nnitrogen content, as well as the Transformed Chlorophyll\nAbsorption\
    \ in Reﬂectance Index (TCARI), MERIS Terrestrial\nChlorophyll Index (MTCI) and\
    \ Triangular Chlorophyll Index\n(TCI) for estimating leaf pigments.\nRemote Sens.\
    \ 2020, 12, 2659\n17 of 44\nTable 6. Cont.\nApplications\nPrevious Studies\nResearch\
    \ Focuses\nEstimating LAI and\nbiomass\nXie et al. [109]\nEvaluated a range of\
    \ vegetation indices, such as the modiﬁed\nsimple ratio index (MSR), NDVI, a newly\
    \ proposed index\nNDVI-like (which resembles NDVI), modiﬁed triangular\nvegetation\
    \ index (MTVI2), and modiﬁed soil adjusted vegetation\nindex (MSAVI) for estimating\
    \ winter wheat LAI from\nhyperspectral images.\nAmbrus et al. [104]\nTested the\
    \ NDVI and Red Edge Position (REP) for estimating\nﬁeld-scale winter wheat biomass.\n\
    Richter et al. [98]\nExamined a range of techniques (e.g., index-based empirical\n\
    regression, radiative transfer modelling, and artiﬁcial neural\nnetwork) for estimating\
    \ crop biophysical variables (e.g., LAI and\nwater content) in terms of operational\
    \ agricultural applications\nwith airborne Hymap data and discussed the unique\
    \ features of\neach technique.\nEstimating nitrogen\ncontent\nNevalainen et al.\
    \ [163]\nUtilized 28 published vegetation indices (e.g., Chlorophyll\nAbsorption\
    \ Ratio Index (CARI) and Normalized Diﬀerence Red\nEdge (NDRE)) for estimating\
    \ oat nitrogen and identiﬁed the\nbest-performing one.\nDetecting crop\ndisease\n\
    Huang et al. [164]\nExamined the performance of the photochemical reﬂectance\n\
    index (PRI) for estimating the disease index of wheat yellow rust\nusing canopy\
    \ reﬂectance data and then applied the regression\non an airborne hyperspectral\
    \ imagery for mapping the\ndisease-aﬀected areas.\nCopenhaver et al. [34]\nCalculated\
    \ a range of vegetation indices (e.g., NDVI and red\nedge position index) for\
    \ detecting crop disease and compared\nthe eﬀectiveness of these indices.\nEstimating\
    \ crop\nresidue cover\nGalloza and Crawford [47]\nUtilized the Normalized Diﬀerence\
    \ Tillage Index (NDTI) and\nCellulose Absorption Index (CAI), together with ALI,\
    \ Hyperion,\nand airborne hyperspectral (SpecTIR) data, for estimating crop\n\
    residue cover for conservation tillage application.\nCrop classiﬁcation\nThenkabail\
    \ et al. [44]\nUtilized both spectral bands and vegetation indices for\nclassifying\
    \ diﬀerent crop types and estimating vegetation\nproperties and evaluated the\
    \ performance diﬀerence of using\nvarious bands or indices.\nOverall, linear regression\
    \ has been commonly used for estimating a wide range of crop or soil\nproperties.\
    \ It is easy to establish, and most of the index-based regressions generated satisfactory\n\
    accuracies. However, there are several potential issues associated with this approach,\
    \ such as the large\nnumber of indices available and it is unknown which performs\
    \ better, regression may be very sensitive\nto data size and quality, and the\
    \ saturation problem of indices [36,165]. It is thus critical to consider\nthese\
    \ potential issues and adopt appropriate solutions when establishing linear regressions\
    \ with\nhyperspectral data. For instance, selecting appropriate vegetation indices\
    \ with targeted crop or soil\nvariables is recommended. Researchers have evaluated\
    \ a wide range of hyperspectral vegetation indices\nfor diﬀerent research purposes.\
    \ Haboudane et al. [166] examined 11 hyperspectral vegetation indices for\nestimating\
    \ crop chlorophyll content. Main et al. [167] investigated 73 vegetation indices\
    \ for estimating\nchlorophyll content in crop and savanna tree species. Peng and\
    \ Gitelson [168] tested 10 multispectral\nindices and 4 hyperspectral indices\
    \ for quantifying crop gross primary productivity. Croft et al. [169]\nanalyzed\
    \ 47 hyperspectral indices for estimating the leaf chlorophyll content of diﬀerent\
    \ tree species.\nZhou et al. [170] evaluated eight hyperspectral indices for estimating\
    \ the canopy-level wheat nitrogen\ncontent. Tong and He [165] evaluated 21 multispectral\
    \ and 123 hyperspectral vegetation indices for\ncalculating the grass chlorophyll\
    \ content at both the leaf and canopy scales. Yue et al. [171] examined\n54 hyperspectral\
    \ vegetation indices for estimating winter wheat biomass. Indices performed diﬀerently\n\
    Remote Sens. 2020, 12, 2659\n18 of 44\nin these studies; thus, it is suggested\
    \ to evaluate the top-performed ones in these studies and select the\none that\
    \ generates the highest accuracy.\nTo deal with issues of linear regression, advanced\
    \ regression, such as MLR and PLSR, has also\nbeen commonly used in previous research\
    \ for estimating crop and soil properties [172,173]. Compared\nwith linear regression,\
    \ the advanced regression models mostly use multiple predictor variables in the\n\
    model to achieve a higher accuracy. PLSR is one of the most widely used models\
    \ for investigating\ncrop properties using hyperspectral images, such as Ryu et\
    \ al. [35], Jarmer [99], Siegmann et al. [73],\nand Yue et al. [124] used PLSR\
    \ and hyperspectral images for estimating diﬀerent crop biophysical and\nbiochemical\
    \ variables (e.g., LAI, biomass, chlorophyll, content, fresh matter, and nitrogen\
    \ contents).\nThomas et al. [100] examined PLSR for retrieving the biogas potential\
    \ from hyperspectral images and\nevaluated the inﬂuence of imaging time on retrieval\
    \ accuracy. Regarding soil features, Gomez et al. [49],\nVan Wesemael et al. [107],\
    \ Hbirkou et al. [102], and Castaldi et al. [110] built a PLSR model for estimating\n\
    the SOC content using hyperspectral images. Zhang et al. [50] used PLSR for estimating\
    \ a wide range\nof soil properties (e.g., soil moisture, soil organic matter,\
    \ clay, total carbon, phosphorus, and nitrogen\ncontent) from hyperspectral imagery\
    \ and identiﬁed factors that may aﬀect the model accuracy\n(e.g., low signal-to-noise\
    \ ratio, spectral overlap of diﬀerent soil features). Casa et al. [59] used the\n\
    PLSR model and diﬀerent hyperspectral imagery for investigating soil textural\
    \ features and evaluated\nvarious factors (e.g., spectral range and resolution,\
    \ soil moisture, geolocation error) inﬂuencing the\nmodel performance.\nThe PLSR\
    \ model is implemented in Python and R [174,175] and is widely used in many research\n\
    areas, including forests [176], grasslands [177], and waters [178]. This model\
    \ performed well in diﬀerent\nstudies owning to its strengths in dealing with\
    \ a large number of inter-correlated predictor variables\n(i.e., by converting\
    \ them to a few non-correlated latent variables), addressing the data noise challenge,\n\
    and tackling the over-ﬁtting problem [171,179]. Diﬀerent techniques have also\
    \ been conﬁrmed to\nbe eﬃcient for improving the accuracy of the PLSR model, such\
    \ as incorporating diﬀerent types of\npredictor variables in the model (e.g.,\
    \ spectral bands, indices, textural variables), utilizing predicted\nresidual\
    \ error sum of squares (PRESS) statistics for determining the optimal number of\
    \ latent variables,\nand feature evaluation for selecting more important predictor\
    \ variables in the model [36]. It is thus\ncritical to carefully examine these\
    \ techniques for achieving the optimal model accuracy.\n3.3. Radiative Transfer\
    \ Modelling\nRadiative transfer modelling is a physically based approach that\
    \ uses physical laws to\nsimulate the interaction of electromagnetic radiation\
    \ with vegetation (e.g., reﬂection, transmission,\nand absorption) [180]. The\
    \ RTMs simulate vegetation spectra (e.g., leaf reﬂectance and transmittance)\n\
    using vegetation biophysical and biochemical properties (e.g., chlorophyll and\
    \ water contents) in\nthe forward mode, and for inversion of these variables from\
    \ spectral measurements in the inverse\nmode [181]. PROSAIL is one of the most\
    \ widely used RTMs. This model is an integration of the\nleaf-level PROSPECT model\
    \ and canopy-level SAIL model and is capable of simulating canopy\nreﬂectance\
    \ using leaf properties (e.g., chlorophyll and water contents), canopy structural\
    \ parameters\n(e.g., LAI and leaf angle), and soil reﬂectance [18].\nPROSAIL has\
    \ also been used in agricultural environments for investigating crop and soil\n\
    properties. For instance, Casa and Jones [182] inverted PROSAIL and a ray-tracing\
    \ canopy model\nwith spectroradiometer-measured hyperspectral reﬂectance data\
    \ and imaging spectrometer-acquired\nhyperspectral image data, respectively, for\
    \ estimating canopy LAI and evaluated factors inﬂuencing\nthe estimation accuracy\
    \ (e.g., the non-homogeneous surface caused by the crop row structure).\nRichter\
    \ et al. [98] utilized PROSAIL for estimating LAI, fCover, canopy chlorophyll,\
    \ and water content\nfrom hyperspectral images and compared its performance to\
    \ other methods (e.g., artiﬁcial neural\nnetwork). Richter et al. [183] applied\
    \ PROSAIL to investigate similar vegetation variables and analyzed\nthe accuracy\
    \ and eﬃciency of this method. Wu et al. [184] examined the sensitivity of vegetation\
    \ indices\nto vegetation chlorophyll content using simulated results from the\
    \ PROSPECT model and suggested\nRemote Sens. 2020, 12, 2659\n19 of 44\na few well-performed\
    \ indices. Locherer et al. [74] attempted to estimate vegetation LAI using the\n\
    PROSAIL model and multi-source hyperspectral images and tested several techniques\
    \ (e.g., diﬀerent\ncost functions and types of averaging methods) used for the\
    \ inversion process. Yu et al. [37] estimated\na range of vegetation phenotyping\
    \ variables (e.g., LAI and leaf chlorophyll) using hyperspectral\nimagery and\
    \ PROSAIL and examined the sensitivity of diﬀerent spectral ranges to the parameters\
    \ in\nthe PROSAIL model.\nCompared with the regression models discussed in previous\
    \ sections, the RTMs have been less\nused in the literature for investigating\
    \ agricultural features due mainly to their high model complexity\nand computational\
    \ intensity. For instance, a wide range of parameters need to be considered in\n\
    RTM (e.g., chlorophyll, carotenoids, water contents, leaf area index, leaf angles,\
    \ solar angles, and soil\nreﬂectance, along with other parameters, in the PROSAIL\
    \ model) and the users need to use diﬀerent\ntechniques (e.g., merit function,\
    \ look-up table) to facilitate the forward and inversion operations of\nthe model.\
    \ In addition, it costs much more computing time than the regression models to\
    \ achieve the\npredictions of target vegetation variables. However, it is also\
    \ well known that the regression models\ntend to be site and time speciﬁc and\
    \ are not readily transferable to other geographical regions or\ndiﬀerent times\
    \ over the site [166]. In contrast, RTM is a more transferable approach owning\
    \ to the\nfact that it is established based on physical laws and does not require\
    \ training data for rebuilding\nthe model. In addition, RTM is capable of estimating\
    \ a range of vegetation properties in one model,\nwhile regression models typically\
    \ can only estimate one variable [36,185].\n3.4. Machine Learning and Deep Learning\n\
    Machine learning algorithms, including support vector machine regression (SVM)\
    \ and RF,\nare powerful tools for analyzing hyperspectral information since they\
    \ can process a large number of\nvariables (e.g., spectral reﬂectance and vegetation\
    \ indices) eﬃciently [186]. Machine learning has been\nwidely used in the remote\
    \ sensing ﬁeld for estimating properties of ground features or classifying\ndiﬀerent\
    \ ground covers [36,114,187]. Researchers have also used diﬀerent machine learning\
    \ algorithms\nand hyperspectral images for agricultural applications. SVM has\
    \ been a commonly used algorithm\nin previous research for prediction or classiﬁcation\
    \ purposes. For instance, Honkavaara et al. [123]\nestimated crop biomass using\
    \ SVM and UAV-acquired hyperspectral imagery. Bostan et al. [51] utilized\nSVM\
    \ for classifying diﬀerent crop types and achieved high classiﬁcation accuracy.\
    \ Ran et al. [93]\nused KNN and SVM classiﬁers for investigating tillage practices\
    \ in agricultural ﬁelds and compared\ntheir performances. RF is another commonly\
    \ used algorithm for investigating agricultural features\nwith hyperspectral imagery.\
    \ For instance, Gao et al. [188] successfully classiﬁed weed and maize\nusing\
    \ RF and lab-based hyperspectral images. Using ground-based hyperspectral reﬂectance\
    \ data\nacquired by an ASD spectroradiometer, Siegmann and Jarmer [189] evaluated\
    \ the performance of\nRF, SVM, and PLSR for estimating crop LAI and conﬁrmed the\
    \ good performance of RF. Similarly,\nusing hyperspectral reﬂectance, Adam et\
    \ al. [190] attempted to detect maize disease with the RF model.\nOverall, machine\
    \ learning models generally have robust performances for investigating agricultural\n\
    features using hyperspectral imagery.\nDeep learning is a subset of machine learning\
    \ and extends machine learning by adding more\n“depth” (i.e., hierarchical representation\
    \ of the dataset) in the model [191,192]. It is a popular approach\nin recent\
    \ years for recognizing patterns in remote sensing images and thus for investigating\
    \ various\nground features. Deep learning has been commonly used in the remote\
    \ sensing ﬁeld for image\nclassiﬁcation, such as land cover classiﬁcation [193–195]\
    \ and the identiﬁcation of ground features\n(e.g., buildings) [196]. Deep learning\
    \ has also been applied to precision farming to solve complicated\nissues. Existing\
    \ studies are, for example, investigating the estimation of crop yield using CNN\n\
    and multispectral images together with climate data [197], plant disease detection\
    \ using CNN and\nsmartphone-acquired images [198], crop classiﬁcation using 3-D\
    \ CNN and multi-temporal multispectral\nimages [199], and classiﬁcation of agricultural\
    \ land cover using deep recurrent neural network and\nmulti-temporal SAR images\
    \ [200]. Kamilaris and Prenafeta-Boldú [191] reviewed applications of deep\nRemote\
    \ Sens. 2020, 12, 2659\n20 of 44\nlearning in agriculture and food production,\
    \ although not all studies used remote sensing images.\nSingh et al. [201] reviewed\
    \ a range of deep learning methods and their applications, speciﬁcally in plant\n\
    phenotyping. Up to now, deep learning has not been well explored for processing\
    \ and analyzing remote\nsensing images, especially hyperspectral images, for agricultural\
    \ applications. Considering the capacity\nof deep learning for studying feature\
    \ patterns in images and the rich information in hyperspectral\nimagery, the integration\
    \ of the two has a wide range of agricultural applications (e.g., crop classiﬁcation,\n\
    weed monitoring, crop disease detection, and plant stress evaluation). Further\
    \ research in these areas\nis warranted.\nMachine learning or deep learning is\
    \ capable of processing multi-source and multi-type data [202].\nFor instance,\
    \ besides multi-type remote sensing images (e.g., optical, thermal, LiDAR, and\
    \ Radar), other\nsources of data, such as weather, irrigation, and historical\
    \ yield information, can also be incorporated in\nthe modelling process for a\
    \ possibly better evaluation of targeted agricultural features [203]. Although\n\
    machine learning and deep learning models are powerful, it is also critical to\
    \ keep in mind that these\nmodels require large-quantity and high-quality training\
    \ samples to achieve robust performances [202].\nInsuﬃcient training datasets\
    \ or data with issues (e.g., data incompleteness, noise, and biases) may\ncause\
    \ undesired model performances.\nIn summary, diﬀerent analytical methods (e.g.,\
    \ linear regression, advanced regression,\nmachine learning and deep learning,\
    \ and RTM) have diﬀerent levels of complexity, performance,\nand transferability.\
    \ More detailed comparisons on these methods are listed in Table 7. Overall, linear\n\
    regression is the easiest method to use, and its performance is generally acceptable,\
    \ although this\nmethod can be highly inﬂuenced by the choice of predictor variables\
    \ and quality of the sample data.\nThe advanced regression (e.g., PLSR) mostly\
    \ performs better than the linear regression since it involves\nmultiple variables\
    \ in the model and is less sensitive to data noise. RTM (e.g., PROSAIL) is capable\
    \ of\nproducing multiple data products (e.g., chlorophyll, water, and LAI) with\
    \ reasonably high accuracies.\nOne essential advantage of this method is its high\
    \ transferability. However, this method has the highest\ncomplexity as it requires\
    \ a wide range of parameters and extensive programming. In terms of machine\n\
    learning, many algorithms, such as RF and SVM, are well established and mostly\
    \ performed well in\nprevious studies. Some programming and model adjustments\
    \ are needed for this method to achieve\noptimal performance. Deep learning is\
    \ a relatively new method and is increasingly popular in recent\nyears. Appropriate\
    \ model design and programming are critical for this approach. It also requires\
    \ a\nsubstantial amount of training data and computing resources to achieve a\
    \ good model performance.\nRemote Sens. 2020, 12, 2659\n21 of 44\nTable 7. Comparison\
    \ of diﬀerent analytical methods.\nMethods\nLinear Regression\nAdvanced Regression\n\
    Radiative Transfer\nModelling\nMachine Learning\nDeep Learning\nParameters typically\
    \ used\nin the model\n- One predictor\nvariable (e.g.,\nreﬂectance or\nvegetation\
    \ index)\n- Response variable\n(e.g., chlorophyll)\n- Multiple\npredictor variables\n\
    - Response variable\n- Parameters in the model\n(e.g., the number of\nlatent variables\
    \ in PLSR)\n- A wide range of\npredictor variables (e.g.,\nleaf biophysical and\n\
    biochemical properties)\n- Parameters in the model\n(e.g., absorption\ncoeﬃcients,\
    \ the\nrefractive index of leaf\nmaterial in PROSAIL)\n- Multiple\npredictor variables\n\
    - Response variable\n- Parameters in the\nmodel (e.g., number\nof trees in the\n\
    RF model)\n- Predictor variables\nas input layers\n- Sizes and weights\nof layers\n\
    - Number of layers\nfor calculating\nModel complexity\nLow\nMedium\nHigh\nMedium\n\
    High\nModel performance\nLow—high\n(depend on predictor\nvariable used)\nMedium—high\n\
    Medium—high\nMedium—high\nMedium—high\nTransferability in time and\ngeographical\
    \ location\nLow\nLow\nHigh\nLow\nHigh\nTypical agricultural\napplications\nPrediction\
    \ of agricultural variables (e.g., yield, LAI)\nPrediction of agricultural variables\n\
    Classiﬁcation of agricultural features\nApplication\nrecommendations\n- Test a\
    \ range of\npredictor variables\nand identify the best\nperformed one\n- Check\
    \ data noise in\nthe training samples\n- Involve diﬀerent types\nof variables\
    \ (e.g.,\nspectral and textural)\n- Check contributions of\nvariables to the model\n\
    - Tuning model\nparameters to achieve\noptimal performance\n- Collect a set of\n\
    vegetation biophysical\nand\nbiochemical parameters\n- Adjust the model to\nimprove\n\
    calculating eﬃciency\n- Involve diﬀerent\ntypes of variables\n(e.g., spectral\n\
    and textural)\n- Tuning model\nparameters to\nachieve\noptimal performance\n-\
    \ Optimize\nmodel conﬁgurations\n- Large size of\ntraining samples\nRemote Sens.\
    \ 2020, 12, 2659\n22 of 44\n4. Hyperspectral Applications in Agriculture\nHyperspectral\
    \ imaging has been used in agriculture for a wide range of purposes, including\n\
    estimating crop biochemical properties (e.g., chlorophyll, carotenoids, and water\
    \ contents) and\nbiophysical properties (e.g., LAI, biomass) for understanding\
    \ vegetation physiological status and\npredicting yield, evaluating crop nutrient\
    \ status (e.g., nitrogen deﬁciency), monitoring crop disease,\nand investigating\
    \ soil properties (e.g., soil moisture, soil organic matter, and soil carbon).\
    \ Previous\nstudies have also summarized some of the above-mentioned applications\
    \ of hyperspectral remote\nsensing in precision agriculture [4,84]. In this section,\
    \ we will thus focus more on recent hyperspectral\nstudies and summarize these\
    \ studies according to speciﬁc applications.\n4.1. Estimation of Crop Biochemical\
    \ and Biophysical Properties\nOne important hyperspectral application in agriculture\
    \ is monitoring crop conditions through\nthe retrieval of crop biochemical and\
    \ biophysical properties [8,99]. For instance, the leaf chlorophyll\ncontent is\
    \ an essential biochemical property inﬂuencing the vegetation photosynthetic capacity\
    \ and\ncontrolling crop productivity [99]. In previous studies, Oppelt and Mauser\
    \ [105] collected AVIS data\nto retrieve the chlorophyll and nitrogen contents\
    \ in a winter wheat ﬁeld. Similarly, Moharana and\nDutta [43] used Hyperion data\
    \ to estimate the contents of these two biochemical components in a rice\nﬁeld.\
    \ LAI, on the other hand, is a fundamental vegetation biophysical parameter and\
    \ is highly related\nto crop biomass and yield [98]. Previous studies have used\
    \ hyperspectral remote sensing to estimate\nthe LAI of diﬀerent crops, and some\
    \ of the example studies are shown in Table 8.\nTable 8. Selected previous studies\
    \ estimating LAI for diﬀerent crop types using hyperspectral images.\nCrops\n\
    Previous Studies\nResearch Focuses\nWinter wheat\nXie et al. [109]\nEstimated\
    \ canopy LAI in a winter wheat ﬁeld using airborne\nhyperspectral imagery and\
    \ proposed a new vegetation index for\nimproved estimation accuracy.\nSiegmann\
    \ et al. [73]\nRetrieved LAI of two wheat ﬁelds using EnMAP images and\nattempted\
    \ to pan-sharp the images aiming to improve the\nspatial resolution of LAI products.\n\
    Barley\nJarmer [99]\nRetrieved a range of canopy variables from barley, including\n\
    LAI, chlorophyll, water, and fresh matter content using HyMap\ndata and established\
    \ an eﬃcient approach for monitoring the\nspatial patterns of crop variables.\n\
    Rice\nYu et al. [37]\nInvestigated LAI, leaf chlorophyll content, canopy water\n\
    content, and dry matter content using UAV-based hyperspectral\nimagery, aiming\
    \ to understand the growing status of rice.\nMixed\nagricultural\nﬁelds\nRichter\
    \ et al. [98]\nEstimated crop LAI and water content with airborne HyMap\ndata\
    \ aiming to support operational agricultural practices (e.g.,\nirrigation management\
    \ and crop stress detection) in the context\nof the EnMap hyperspectral mission.\n\
    Wu et al. [45]\nEstimated chlorophyll content and LAI in a mixed agricultural\n\
    ﬁeld (e.g., corns, chestnuts trees, and tea plants) using Hyperion\ndata and identiﬁed\
    \ spectral bands and vegetation indices that\ngenerated the highest accuracy.\n\
    Verger et al. [57]\nEstimated LAI, fCover, and FAPAR in an agricultural site with\n\
    diﬀerent crops using PROBA-CHRIS data.\nLocherer et al. [74]\nEstimated LAI in\
    \ mixed crop ﬁelds using EnMAP data and\ncompared the result accuracy to that\
    \ of LAI estimation with\nairborne data.\nRemote Sens. 2020, 12, 2659\n23 of 44\n\
    In addition to the above-mentioned vegetation biochemical and biophysical properties,\
    \ crop water\ncontent is a critical parameter for revealing water stress. Richter\
    \ et al. [98] attempted to estimate the\nwater content in maize, sugar beet, and\
    \ winter wheat using airborne HyMap data. Moharana and\nDutta [204] investigated\
    \ the water stress in a rice ﬁeld and its variations using Hyperion images and\n\
    indicated that the remote sensing-estimated water content matched well with ﬁeld-observed\
    \ data.\nIzzo et al. [128] evaluated the water status in a commercial vineyard\
    \ using UAV-based hyperspectral\ndata and determined wavelengths sensitive to\
    \ the canopy water content. Sahoo et al. [4] discussed the\napplications of hyperspectral\
    \ remote sensing data for evaluating water features in crops and listed\nseveral\
    \ vegetation indices for calculating the water content.\nIt can be found from\
    \ the literature review that many previous studies have focused on estimating\n\
    the crop chlorophyll content, LAI, and water content using hyperspectral imagery,\
    \ while other\nimportant crop properties, such as carotenoids, that are sensitive\
    \ to plant stress are less explored.\nIn addition, crop production is inﬂuenced\
    \ by all of these vegetation properties (e.g., chlorophyll,\nwater, and LAI).\
    \ Besides investigating the spatial and temporal variations of each property,\
    \ it is also\ncritical to evaluate the relationships between these properties\
    \ and further understand how they aﬀect\ncrop growth and crop production.\nEstimating\
    \ crop biomass and forecasting yield are also important applications of remote\
    \ sensing,\nas they will contribute to the understanding of crop productivity\
    \ and implementing suitable\nmanagement measures [126]. Yue et al. [124] utilized\
    \ UAV-based hyperspectral images for estimating the\nabove-ground biomass of winter\
    \ wheat. Yang [205] and Mariotto et al. [15] utilized both multispectral\nand\
    \ hyperspectral data to estimate crop yield and found that the hyperspectral imagery-based\
    \ model\nperformed better. In addition, crop residues left in the ﬁeld are critical\
    \ materials protecting soil\nfrom water and wind erosion and inﬂuencing soil biochemical\
    \ processes. Previous studies, such as\nBannari et al. [106], Galloza and Crawford\
    \ [47], Bannari et al. [46], have used diﬀerent hyperspectral\nimages for the\
    \ estimation of crop residues on farmlands\nBeyond the estimation of crop biomass\
    \ and residue, one further research topic is investigating\nbioenergy (e.g., biogas),\
    \ which can be generated from the crop biomass. Thomas et al. [100] attempted\n\
    to estimate the amount of biogas that can be generated per unit of biomass using\
    \ airborne HyMap\ndata and achieved satisfactory results. Overall, hyperspectral\
    \ imagery has contributed greatly to the\nestimation of crop biomass, yield, and\
    \ other related features (e.g., bioenergy, crop residues). Since crop\nbiomass\
    \ and yield are highly aﬀected by agricultural practices (e.g., watering and nutrition\
    \ treatment),\ninvolving these practice data, together with hyperspectral imagery,\
    \ in the model can potentially\ngenerate better results. More research in this\
    \ area is warranted.\n4.2. Evaluating Crop Nutrient Status\nPrecision farming\
    \ involves evaluating the crop nutrient status and providing recommendations\n\
    on site-speciﬁc resource management according to crop needs [206]. Such an approach\
    \ is critical for\nimproving the resource use eﬃciency and reducing environmental\
    \ impacts [4,103]. Previous studies\nhave used hyperspectral images for estimating\
    \ the nitrogen content of diﬀerent crop types, as shown\nin Table 9.\nTable 9.\
    \ Selected previous studies estimating the nitrogen content for diﬀerent crop\
    \ types using\nhyperspectral images.\nCrop types\nPrevious Studies\nResearch Focuses\n\
    Corn\nAkhtman et al. [127]\nUsed UAV-based hyperspectral images for estimating\
    \ nitrogen\ncontent and phytomass in corn and wheat ﬁelds and monitored the\n\
    temporal variation of these properties.\nGoel et al. [207]\nCollected hyperspectral\
    \ images in a cornﬁeld with diﬀerent nitrogen\ntreatments and weed controls aiming\
    \ to evaluate to what extent the\nspectral signals can identify diﬀerent nitrogen\
    \ treatments, weed\ncontrols, or their interactions.\nRemote Sens. 2020, 12, 2659\n\
    24 of 44\nTable 9. Cont.\nCrop types\nPrevious Studies\nResearch Focuses\nCilia\
    \ et al. [103]\nEstimated nitrogen concentration and dry mass in an experimental\n\
    maize ﬁeld using airborne hyperspectral imagery, aiming to\nquantify the nitrogen\
    \ deﬁcit and provide a variable rate fertilization\nmap. The authors also suggested\
    \ a way to evaluate the minimum\namount of nitrogen to apply without reducing\
    \ crop yield and avoid\nexcessive fertilization.\nQuemada et al. [208]\nEvaluated\
    \ plant nitrogen status in a maize ﬁeld using airborne\nhyperspectral images and\
    \ developed nitrogen fertilizer\nrecommendations.\nWheat\nKoppe et al. [209]\n\
    Attempted to investigate wheat nitrogen status and aboveground\nbiomass using\
    \ hyperspectral and radar images and to evaluate\nspectral signatures of wheats\
    \ under diﬀerent nitrogen treatments.\nKaivosoja et al. [126]\nUsed UAV-based\
    \ hyperspectral imagery to investigate nitrogen\ncontent and absolute biomass\
    \ in a wheat ﬁeld and evaluated the\ndegree of nitrogen shortage on the date of\
    \ image acquisition. In this\nresearch, historical farming data, including a yield\
    \ map and a spring\nfertilization map, were used for estimating the optimal amount\
    \ of\nfertilizer to be applied in diﬀerent areas of the ﬁeld.\nCastaldi et al.\
    \ [210]\nEstimated nitrogen content in wheat using multi-temporal\nsatellite-based\
    \ multispectral and hyperspectral images and found\nthat the band selection aﬀected\
    \ estimation accuracy at diﬀerent\nphenological stages.\nRice\nMoharana and Dutta\
    \ [43]\nCollected Hyperion images for monitoring nitrogen and chlorophyll\ncontents\
    \ in rice and investigated the performance of diﬀerent\nspectral indices.\nRyu\
    \ et al. [35]\nUsed airborne hyperspectral images and multivariable analysis to\n\
    estimate nitrogen content in rice at the heading stage.\nZheng et al. [211]\n\
    Tried to monitor rice nitrogen status using UAV-based hyperspectral\nimages and\
    \ tested the performance of diﬀerent vegetation indices\nfor estimating the nitrogen\
    \ content.\nZhou et al. [212]\nEstimated leaf nitrogen concentration of rice using\
    \ close-range\nhyperspectral images and tested if the variations of the spatial\n\
    resolution of the imagery aﬀect the estimation accuracy.\nOther crops\n(i.e.,\
    \ barley,\npotato,\ncabbage,\ntomato,\nsugarcane,\nand cacao)\nNasi et al. [213]\n\
    Evaluated the performance of using airborne hyperspectral images\nand photogrammetric\
    \ features for estimating crop nitrogen content\nand biomass in a barley ﬁeld\
    \ and a grassland site, and examined if\nthe integration of spectral and plant\
    \ height information can improve\nthe estimation results.\nNigon et al. [214]\n\
    Examined nitrogen stress in potato ﬁelds using airborne\nhyperspectral imagery\
    \ and identiﬁed spectral indices that are\nsensitive to nitrogen content.\nChen\
    \ et al. [215]\nEstimated nitrogen content in cabbage seedlings using close-range\n\
    hyperspectral images and identiﬁed sensitive wavelengths for the\nestimation.\n\
    Zhu et al. [142]\nInvestigated soluble sugar, total nitrogen, and their ratio\
    \ in tomato\nleaves using close-range hyperspectral images and tested data\nfusion\
    \ analysis techniques for improving the investigation accuracy.\nMiphokasap and\n\
    Wannasiri [216]\nCollected Hyperion images for investigating spatial variations\
    \ of\nsugarcane canopy nitrogen concentration and attempted to identify\nthe nutrient\
    \ deﬁcient areas for corresponding treatments.\nMalmir et al. [217]\nAttempted\
    \ to evaluate nutrient status (e.g., nitrogen, phosphorus,\nand potassium) of\
    \ cacao leaves using close-range hyperspectral\nimages and examined inﬂuences\
    \ of band selection on the evaluation\naccuracy.\nRemote Sens. 2020, 12, 2659\n\
    25 of 44\nOverall, owing to the large amount of spectral information in hyperspectral\
    \ imagery, crop nutrient\nstatus can be evaluated with high accuracies, and a\
    \ corresponding fertilizer treatment plan can be\nproposed to achieve optimal\
    \ crop productions. However, it is also essential to keep in mind that there is\n\
    a wide range of factors, such as soil moisture, soil type, and topographic conditions,\
    \ that can impact crop\ngrowth and production. A more comprehensive treatment\
    \ plan that takes into consideration both the\ncrop nutrient status and other\
    \ inﬂuencing factors can make a greater contribution to crop production.\n4.3.\
    \ Classifying Imagery to Identify Crop Types, Growing Stages, Weeds/Invasive Species,\
    \ and Stress/Disease\nBesides quantifying crop properties, hyperspectral images\
    \ have also been used for classiﬁcation\npurposes, such as differentiating crop\
    \ types, identifying crop growing stages, classifying weeds or invasive\nspecies,\
    \ and detecting disease [218]. Examples of previous studies are shown in Table\
    \ 10. Diﬀerent\nagricultural land covers or crop types have diﬀerent spectral\
    \ characteristics; hence, hyperspectral\nimages can contribute greatly to the\
    \ classiﬁcation of these agricultural features.\nTable 10. Selected previous studies\
    \ for the classification of agricultural features using hyperspectral images.\n\
    Applications\nPrevious Studies\nResearch Focuses\nClassiﬁcation of\ncrop types\n\
    Camacho Velasco et al. [48]\nUtilized Hyperion data and diﬀerent classiﬁcation\
    \ algorithms\n(e.g., spectral angle mapper and adaptive coherence estimator)\n\
    for identifying ﬁve types of crops (e.g., oil palm, rubber, grass\nfor grazing,\
    \ citrus, and sugar cane) in Colombia.\nBostan et al. [51]\nClassiﬁed diﬀerent\
    \ crop and land cover types (e.g., maize,\ncotton, urban, water, barren rock,\
    \ and other crop types) using\nLandsat 8 multispectral and EO-1 Hyperion hyperspectral\n\
    images and indicated that hyperspectral imagery performed\nbetter than the multispectral\
    \ imagery.\nAmato et al. [152]\nAssessed the potential of PRISMA data for classifying\
    \ diﬀerent\nagricultural land uses (e.g., soybean, corn, and sugar beet) and\n\
    evaluated the contribution of spectral bands to image\nsegmentation and classiﬁcation.\n\
    Nigam et al. [91]\nPerformed crop classiﬁcation over homogeneous and\nheterogeneous\
    \ agriculture and horticulture areas with airborne\nAVIRIS images and assessed\
    \ crop health at the ﬁeld scale.\nSahoo et al. [4]\nReviewed a few previous studies\
    \ that used hyperspectral\nimages for classiﬁcation purposes and indicated the\
    \ robustness\nof hyperspectral imagery for classifying diﬀerent crop types and\n\
    diﬀerent crop phonological stages.\nOther\nclassiﬁcations\n(e.g., growth stages\n\
    and agricultural\ntillage practices)\nAntony et al. [58]\nApplied multi-angle\
    \ PROBA-CHRIS data for classifying\ndiﬀerent growth stages of wheat.\nRan et al.\
    \ [93]\nAttempted to detect agricultural tillage practices using\nhyperspectral\
    \ imagery with diﬀerent classiﬁcation models and\nidentiﬁed the best performing\
    \ one.\nTeke et al. [38]\nDiscussed the application of spectral libraries for\
    \ classiﬁcation\npurposes and listed several spectral libraries available\nworldwide.\
    \ The authors also indicated the limitations of using a\nspectral library, such\
    \ as the spectral varieties within the same\nspecies or land cover, and highlighted\
    \ the importance of having\ngeographically speciﬁc libraries\nWeed infestation\
    \ is a severe issue in agricultural ﬁelds and could substantially aﬀect crop growth\n\
    and yield. Identifying and mapping weeds in agricultural ﬁelds using remote sensing\
    \ will contribute\ngreatly to variable rate treatment in the ﬁelds [219]. Researchers\
    \ have utilized diﬀerent remote sensing\ndata and methods for weed mapping, as\
    \ shown in Table 11. Overall, the identiﬁcation of weeds\ntypically requires a\
    \ high spatial resolution since many weeds are small in size and mixed with crops.\n\
    Remote Sens. 2020, 12, 2659\n26 of 44\nUAV-based and close-range hyperspectral\
    \ imaging is capable of acquiring high-spatial-resolution\nimages, and thus has\
    \ high potential to contribute to weed detection.\nTable 11. Selected previous\
    \ studies for detecting weeds using diﬀerent hyperspectral imaging platforms.\n\
    Platforms\nPrevious Studies\nResearch Focuses\nAirborne\nGoel et al. [97]\nAttempted\
    \ to detect weed infestation in a cornﬁeld that\nhad diﬀerent nitrogen treatments\
    \ using airborne\nhyperspectral imagery and found the diﬀerent nitrogen\ntreatments\
    \ aﬀected the classiﬁcation accuracy of weed.\nKarimi et al. [220]\nPerformed\
    \ combinations of diﬀerent nitrogen treatment\nrates and weed management practices\
    \ in a cornﬁeld and\ntried to classify these combinations with airborne\nhyperspectral\
    \ images.\nClose range\nZhang et al. [221]\nDeveloped a close-range weed sensing\
    \ system using\nhyperspectral images for classifying tomato and weeds\nand tested\
    \ its performance in diﬀerent environments.\nEddy et al. [139]\nUsed a ground-based\
    \ hyperspectral imaging system for\nclassifying weeds in canola, pea, and wheat\
    \ crops and\nevaluated the applicability of this approach for real-time\ndetection\
    \ of weeds in the ﬁeld.\nEddy et al. [222]\nUsed hyperspectral image data as well\
    \ as secondary\nproducts with reduced bands to classify weeds and\nachieved good\
    \ accuracy.\nLiu et al. [223]\nClassiﬁed carrot and weeds using a ground-based\n\
    hyperspectral imaging system and evaluated the number\nof spectral bands needed\
    \ to achieve a good classiﬁcation\naccuracy.\nMultiple platforms\nScherrer et\
    \ al. [129]\nAttempted to classify herbicide-resistant weeds in\ndiﬀerent crop\
    \ ﬁelds (e.g., barley, corn, and dry pea) using\nboth ground- and UAV-based hyperspectral\
    \ imagery and\ndiscussed factors inﬂuencing classiﬁcation accuracy (e.g.,\ncrop\
    \ type, plant age, and illumination condition).\nReview studies\nLÓPEZ-Granados\
    \ [224]\nDiscussed the high potential of hyperspectral remote\nsensing images\
    \ for mapping weeds but also indicated\nthe limitations of this technology due\
    \ to the high cost of\ndata collection.\nMonitoring crop disease is highly important\
    \ to growers trying to reduce economic and yield\nlosses [38]. Hyperspectral imaging\
    \ collects signals at fine spectral resolutions (e.g., less than 10-nm\nintervals),\
    \ and thus can possibly detect early symptoms of crop disease and support timely\n\
    interventions [225].\nPrevious studies have used hyperspectral images for detecting\
    \ diseases in\ndiﬀerent types of groups (Table 12). Overall, hyperspectral signals\
    \ are sensitive to the variations of\ncrop growth status (e.g., caused by disease\
    \ or stress) and thus can indicate the occurrence of crop\ndisease or stress.\
    \ However, considering that crop status can be aﬀected by other factors (e.g.,\
    \ nutrient\ndeﬁciency), repeat imaging and analysis together with robust modelling\
    \ would be critical for accurate\nand timely detection of crop disease or stress.\n\
    Remote Sens. 2020, 12, 2659\n27 of 44\nTable 12. Selected previous studies for\
    \ detecting disease in diﬀerent crops using hyperspectral images.\nCrops\nPrevious\
    \ Studies\nResearch Focuses\nWheat\nBohnenkamp et al. [119]\nUsed both ground-\
    \ and UAV-based hyperspectral imaging\nplatforms for detecting yellow rust in\
    \ wheat and evaluated\nfactors inﬂuencing the detection (e.g., measurement distance,\n\
    spectral features to use).\nBauriegel et al. [226]\nTargeted the infestation of\
    \ wheat by Fusarium and attempted to\ndetect this disease using hyperspectral\
    \ remote sensing data, and\nconsequently suggested that farmers need to deal with\
    \ infected\ncrops separately from healthy crops.\nZhang et al. [227]\nAttempted\
    \ to detect the Fusarium head blight in winter wheat\nsimilarly using close-range\
    \ hyperspectral imaging and\nsuggested that this is a stable and feasible way\
    \ to monitor this\ndisease using low-altitude remote sensing.\nCorn\nCopenhaver\
    \ et al. [34]\nUsed airborne hyperspectral images to detect the signal of\nOstrinia\
    \ nubilalis in a cornﬁeld (e.g., via monitoring rate of plant\nsenescence) and\
    \ tested the performance of this approach\nthroughout the growing season.\nSoybean\n\
    Nagasubramanian et al. [144]\nTried to detect charcoal rot in soybeans using close-range\n\
    hyperspectral imaging and identiﬁed wavelength ranges that\nare sensitive to this\
    \ disease.\nSugarcane\nApan et al. [41]\nDetected sugarcane areas aﬀected by orange\
    \ rust disease using\nHyperion data and developed speciﬁc vegetation indices that\n\
    are sensitive to the disease.\nMustard\nDutta et al. [42]\nDelineated mustard\
    \ areas inﬂuenced by diseases using Hyperion\nimages and evaluated the performance\
    \ of diﬀerent indices.\nReview\nstudies\nLowe et al. [218]\nFocused on hyperspectral\
    \ imaging and reviewed some of its\napplications in detecting and classifying\
    \ crop disease and stress.\nThomas et al. [225]\nReviewed the contributions of\
    \ hyperspectral imaging to the\ndetection of plant disease and discussed diﬀerent\
    \ factors (e.g.,\nlight and wind) that may limit its wide applications.\nMahlein\
    \ et al. [228]\nReviewed previous studies using remote sensing for detecting\n\
    plant disease, but not limited to hyperspectral imaging.\n4.4. Retrieving Soil\
    \ Moisture, Fertility, and Other Physical or Chemical Properties\nAgricultural\
    \ soil properties, including soil moisture, soil organic matter, soil salinity,\
    \ and roughness,\nare important factors inﬂuencing crop growth and ﬁnal production\
    \ [7]. Hyperspectral remote sensing\ncan contribute greatly to the investigation\
    \ of these factors. For instance, estimating soil moisture is\none of the most\
    \ popular research topics. Finn et al. [108] estimated soil moisture at three\
    \ diﬀerent\ndepths using airborne hyperspectral images and linear regression and\
    \ discussed the contributions and\nlimitations of hyperspectral remote sensing\
    \ for soil moisture studies. Casa et al. [229] investigated\nsoil water, clay,\
    \ and sand contents using a fusion of CHRIS-PROBA images and soil geophysical\
    \ data.\nShoshany et al. [7] summarized four main approaches for estimating soil\
    \ moisture content: (1) Radar\ntechniques; (2) radiation balance and surface temperature\
    \ calculations; (3) reﬂectance in the visible,\nNIR, and SWIR ranges; and (4)\
    \ integrative methods using multiple spectral ranges. Although soil\nmoisture\
    \ can be estimated using optical remote sensing data, it is often aﬀected by the\
    \ plant ground\ncover. Integrating multi-type remote sensing data, e.g., SAR and\
    \ thermal data, can possibly generate\nmore accurate estimates.\nSOC is a critical\
    \ component of soil fertility, which highly controls both the growth and yield\
    \ of\ncrops. Hyperspectral data provide ﬁne spectral details that are critical\
    \ for the estimation of SOC content.\nPrevious studies have used hyperspectral\
    \ images collected by diﬀerent platforms for investigating\nSOC (Table 13). Overall,\
    \ hyperspectral imagery has a high potential for the estimation of soil organic\n\
    Remote Sens. 2020, 12, 2659\n28 of 44\nmatter and carbon. However, similar to\
    \ the evaluation of soil moisture, the investigation of soil organic\nmatter and\
    \ carbon can be highly inﬂuenced by vegetation cover. Therefore, collecting hyperspectral\n\
    images in non-growing seasons could be a solution.\nTable 13. Selected previous\
    \ studies for estimating soil organic carbon using hyperspectral images\nacquired\
    \ by diﬀerent platforms.\nPlatforms\nPrevious Studies\nResearch Focuses\nSatellites\n\
    Zhang et al. [50]\nUtilized EO-1 Hyperion images for estimating several soil\n\
    properties, including soil moisture, soil organic matter, total\ncarbon, total\
    \ phosphorus, total nitrogen, and clay content. The\nauthors also found the inﬂuence\
    \ of spectral resolution on the\nperformance of retrieval models.\nCasa et al.\
    \ [230]\nAssessed soil organic matter and soil texture at the ﬁeld scale\nusing\
    \ CHRIS-PROBA images and produced uniform soil zones\nfor supporting irrigation\
    \ management.\nAirplanes\nHbirkou et al. [102]\nAttempted to estimate SOC in agricultural\
    \ ﬁelds using airborne\nHyMap images and tested the inﬂuences of soil surface\n\
    conditions on the estimation, aiming to support soil\nmanagement in precision\
    \ farming.\nGedminas and Martin [231]\nTried to map soil organic matter using\
    \ airborne hyperspectral\nimagery in combination with topographic information\
    \ extracted\nfrom LiDAR image and evaluated the correlation between soil\norganic\
    \ matter and various spectral bands.\nCastaldi et al. [110]\nInvestigated the\
    \ relationship between SOC in croplands and\nspectral signals using a soil database\
    \ and then estimated SOC in\ntheir study sites using airborne hyperspectral imagery.\
    \ With this\napproach, the authors attempted to reduce the amount of new\ndata\
    \ collection in the ﬁeld or lab.\nVan Wesemael et al. [107]\nDiscussed the impacts\
    \ of vegetation cover on soil and the\nestimation of SOC from remote sensing data\
    \ and attempted to\nuse spectral unmixing techniques to estimate the fraction\
    \ of\nvegetation cover and then estimate the soil carbon content using\nthe residue\
    \ soil spectra.\nMultiple\nplatforms\nGomez et al. [49]\nEstimated SOC using both\
    \ lab-based hyperspectral reﬂectance\ndata and Hyperion image data and found that\
    \ using the\nlab-acquired reﬂectance data can generate more accurate results\n\
    than using the Hyperion data. At the same time, the Hyperion\ndata can generate\
    \ a SOC map that matches ﬁeld observations\nand thus can also be used for prediction.\n\
    Hyperspectral remote sensing data have also been used for estimating other soil\
    \ features, as shown\nin Table 14. It can be found from these studies that hyperspectral\
    \ images can be used for studying a\nwide range of soil features. Diﬀerent soil\
    \ features inﬂuence the spectral signals in diﬀerent bands and\nwith diﬀerent\
    \ magnitudes, while some of these inﬂuences may be spectrally overlapped. Therefore,\n\
    when investigating a speciﬁc soil feature, it is critical to collect a suitable\
    \ number of soil samples with\nother soil features generally controlled.\nRemote\
    \ Sens. 2020, 12, 2659\n29 of 44\nTable 14. Selected previous studies for investigating\
    \ diﬀerent soil features using hyperspectral images.\nSoil Features\nPrevious\
    \ Studies\nResearch Focuses\nSoil texture\nCasa et al. [59]\nInvestigated soil\
    \ texture using airborne MIVIS and spaceborne\nPROBA-CHRIS hyperspectral images\
    \ and discussed their\nperformance and limitation (e.g., lack of SWIR band).\n\
    Soil nitrogen\nSong et al. [232]\nUsed airborne hyperspectral images for evaluating\
    \ the impact of\nsoil nitrogen applications and variable-rate fertilization on\
    \ winter\nwheat growth. The authors also indicated that the variable-rate\nfertilization\
    \ in the ﬁeld could reduce the growing diﬀerence of\nwinter wheat caused by the\
    \ spatial variations of soil nitrogen.\nCopper\nconcentration\nAntonucci et al.\
    \ [147]\nAttempted to estimate in soil using lab-based hyperspectral\nmeasurement\
    \ and achieved good accuracy.\nPotassium\ncontent\nWang et al. [233]\nEvaluated\
    \ potassium content in cinnamon soil using close-range\nhyperspectral imaging\
    \ aiming to better understand soil fertility and\nindicated the good performance\
    \ of this approach when the\npotassium content is high (i.e., ≥ 100 mg/kg).\n\
    CO2 leaks\nMcCann et al. [234]\nDetected CO2 leaks from the soil by monitoring\
    \ vegetation stress\nsignals using multi-temporal hyperspectral images.\nIn summary,\
    \ hyperspectral imaging has been successfully applied to a wide range of agricultural\n\
    applications, as reviewed above, and summarized in Table 15.\nFuture research\
    \ directions are\nalso suggested.\nRemote Sens. 2020, 12, 2659\n30 of 44\nTable\
    \ 15. Hyperspectral applications in agriculture.\nPrevious Focuses\nSuggested\
    \ Future Research Directions\nCrop biochemical and biophysical properties\n- Leaf\
    \ area index\n- Chlorophyll content\n- Water content\n- Fraction of vegetation\
    \ cover\n- Fresh/dry biomass, crop residue\n- Yield\n- Vegetation properties related\
    \ to crop stress (e.g., carotenoids)\n- Relationships between diﬀerent properties\
    \ and how they\naﬀect crop growth\nCrop nutrient status\n- Nitrogen content\n\
    - Other nutrients (e.g., phosphorus, magnesium, and boron\netc.) that may limit\
    \ crop growth\n- Optimized treatment plan targeting diﬀerent limiting factors\n\
    Classiﬁcation\nClassiﬁcation of:\n- Crop types\n- Soil types\n- Growing stages\
    \ (i.e., crop phenological features)\nClassiﬁcation and detection of stressors:\n\
    - Weeds or invasive species\n- Disease/stress aﬀected areas\n- Improvement of\
    \ classiﬁcation methods (e.g., advanced\nalgorithms) for target features\n- Fusion\
    \ and application of multi-type and multi-temporal\nremote sensing data\n- Further\
    \ exploration of UAV and close-range imaging for\nbetter identiﬁcation of ﬁne-scale\
    \ signals\nSoil properties\n- Soil moisture\n- Soil organic matter\n- Soil salinity\n\
    - Soil roughness\n- Separation of spectral signals from soil and vegetation for\n\
    better assessing soil features\n- Fusion and application of multi-type remote\
    \ sensing data to\ncapture diﬀerent soil information\n- Further exploration of\
    \ close-range sensing for investigating\nsoil properties.\nAgro-ecosystem\n- Less\
    \ explored using hyperspectral image\n- Ecosystem services\n- Biodiversity\n-\
    \ Adverse eﬀects of agricultural practices on the environments\nRemote Sens. 2020,\
    \ 12, 2659\n31 of 44\n5. Conclusions and Recommendations\nHyperspectral imaging\
    \ has great potential for applications in agriculture, particularly precision\n\
    agriculture, owing to ample spectral information sensitive to diﬀerent plant and\
    \ soil biophysical and\nbiochemical properties. Multiple platforms, including\
    \ satellites, airplanes, UAVs, and close-range\nplatforms, have become more widely\
    \ available in recent years for collecting hyperspectral images with\ndiﬀerent\
    \ spatial, temporal, and spectral resolutions. These platforms also have diﬀerent\
    \ strengths and\nlimitations in terms of spatial coverage, ﬂight endurance, ﬂexibility,\
    \ operational complexity, and cost.\nThese factors need to be considered when\
    \ choosing imaging platform(s) for speciﬁc research purposes.\nFurther technological\
    \ developments are also needed to overcome some of the limitations, such as the\n\
    short battery endurance in UAV operations and high cost of hyperspectral sensors.\n\
    Diﬀerent analytical methods, such as linear regression, advanced regression, machine\
    \ learning,\ndeep learning, and RTM, have been explored in previous studies for\
    \ analyzing the tremendous amount\nof information in hyperspectral images for\
    \ investigating diﬀerent agricultural features. Previous\nstudies have mainly\
    \ used the regression approach, while more physically based methods, such as\n\
    RTM, have been less explored. Deep learning and eﬀective big-data analytics are\
    \ powerful tools for\nrecognizing patterns in remote sensing data. Together with\
    \ hyperspectral imagery, deep learning\nmodels have high potential to support\
    \ the monitoring of a wide range of agricultural features. Diﬀerent\nanalytical\
    \ methods have diﬀerent advantages and disadvantages, and thus it is critical\
    \ to compare\nthese methods for speciﬁc research (e.g., requirements of accuracy\
    \ and computing eﬃciency) and\nchoose an optimal approach. In addition, image\
    \ spectral information has been commonly used as\nvariables for prediction or\
    \ classiﬁcation tasks, while other information, such as texture, has been less\n\
    explored. Further, some other sources of data, such as weather, irrigation records,\
    \ and historical yield\ninformation, can also be used in some of the analytical\
    \ methods (e.g., machine learning and deep\nlearning) for better monitoring of\
    \ crop features. More research in these ﬁelds is also warranted.\nHyperspectral\
    \ imaging has been successfully applied in a wide range of agricultural applications,\n\
    including estimating crop biochemical and biophysical properties; evaluating crop\
    \ nutrient and stress\nstatus; classifying or detecting crop types, weeds, and\
    \ diseases; and investigating soil characteristics.\nPrevious studies have focused\
    \ on discussing one or two of the many factors impacting crop growth\nperformance\
    \ and productivity, and thus cannot evaluate crop status and growth-limiting factors\n\
    comprehensively. It is important to integrate these factors to achieve a better\
    \ understanding of their\ninter-relationships for optimal crop production and\
    \ environmental protection. Besides, previous studies\nusing hyperspectral imaging\
    \ have mainly targeted investigating crop growth, aiming to improve\ncrop yield,\
    \ while less research has focused on understanding the ecosystem side of crop\
    \ production\n(e.g., ecosystem services and biodiversity). Further research in\
    \ these areas is warranted.\nAuthor Contributions: Conceptualization, J.S., J.L.,\
    \ Y.H., B.L. and P.D.D.; methodology, B.L., P.D.D. and Y.H.;\ninvestigation, B.L.;\
    \ writing—original draft preparation, B.L.; writing—review and editing, P.D.D.,\
    \ J.S., J.L. and\nY.H.; project administration, J.S., J.L. and Y.H.; funding acquisition,\
    \ Y.H. All authors have read and agreed to the\npublished version of the manuscript.\n\
    Funding: This work was funded by the Natural Sciences and Engineering Research\
    \ Council of Canada (NSERC)\nunder Discovery Grant RGPIN-386183 to Professor Yuhong\
    \ He.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nAbbreviations\n\
    ALI\nAdvanced Land Imager\nAPEX\nAirborne Prism Experiment\nAVIS\nAirborne Visible\
    \ Near-Infrared Imaging Spectrometer\nAVIS\nAirborne Visible Near-Infrared Imaging\
    \ Spectrometer\nAVIRIS\nAirborne Visible/Infrared Imaging Spectrometer\nANN\n\
    Artiﬁcial Neural Networks\nCAI\nCellulose Absorption Index\nRemote Sens. 2020,\
    \ 12, 2659\n32 of 44\nCAI\nChlorophyll Absorption Integral\nCARI\nChlorophyll\
    \ Absorption Ratio Index\nCASI\nCompact Airborne Spectrographic Imager\nCHRIS\n\
    Compact High Resolution Imaging Spectrometer\nCNN\nConvolutional Neural Network\n\
    DEM\nDigital Elevation Model\nDESIS\nDlr Earth Sensing Imaging Spectrometer\n\
    DCNI\nDouble-Peak Canopy Nitrogen Index\nEnMAP\nEnvironmental Mapping And Analysis\
    \ Program\nFAPAR\nFraction Of Absorbed Photosynthetically Active Radiation\nfCover\n\
    Fraction Of Vegetation Cover\nGCPs\nGround Control Points\nHSI\nHyper Spectral\
    \ Imaging\nHySI\nHyperspectral Imager\nHICO\nHyperspectral Imager For The Coastal\
    \ Ocean\nHISUI\nHyperspectral Imager Suite\nHyspIRI\nHyperspectral Infrared Imager\n\
    HyMap\nHyperspectral Mapper\nh NDVI\nHyperspectral Normalized Diﬀerence Vegetation\
    \ Index\nPRISMA\nHyperspectral Precursor And Application Mission\nIMU\nInertial\
    \ Measurement Unit\nLAI\nLeaf Area Index\nMTCI\nMeris Terrestrial Chlorophyll\
    \ Index\nMNF\nMinimum Noise Fraction\nMCARI/MTVI2\nModiﬁed Chlorophyll Absorption\
    \ Ratio Index/Modiﬁed Triangular Vegetation Index 2\nMSR\nModiﬁed Simple Ratio\
    \ Index\nMSAVI\nModiﬁed Soil Adjusted Vegetation Index\nMTVI2\nModiﬁed Triangular\
    \ Vegetation Index\nMIVIS\nMultispectral Infrared Visible Imaging Spectrometer\n\
    MSI\nMultispectral Instrument\nMLR\nMulti-Variable Regression\nNDRE\nNormalized\
    \ Diﬀerence Red Edge\nNDTI\nNormalized Diﬀerence Tillage Index\nOLI\nOperational\
    \ Land Imager\nOSAVI\nOptimized Soil-Adjusted Vegetation Index\nPLSR\nPartial\
    \ Least Square Regression\nPRI\nPhotochemical Reﬂectance Index\nPRESS\nPredicted\
    \ Residual Error Sum Of Squares\nPCA\nPrincipal Component Analysis\nPHI\nPushbroom\
    \ Hyperspectral Imager\nRTM\nRadiative Transfer Modelling\nRF\nRandom Forest\n\
    REP\nRed Edge Position\nSWIR\nShortwave Infrared\nSR\nSimple Ratio\nSVD\nSingular\
    \ Value Decomposition\nSOC\nSoil Organic Carbon\nSHALOM\nSpaceborne Hyperspectral\
    \ Applicative Land And Ocean Mission\nSFOC\nSpecial Flight Operations Certiﬁcate\n\
    SVM\nSupport Vector Machine Regression\nTCARI\nTransformed Chlorophyll Absorption\
    \ In Reﬂectance Index\nTCI\nTriangular Chlorophyll Index\nTVI\nTriangular Vegetation\
    \ Index\nUMD\nUniform Feature Design\nUAV\nUnmanned Aerial Vehicle\nRemote Sens.\
    \ 2020, 12, 2659\n33 of 44\nReferences\n1.\nWeiss, M.; Jacob, F.; Duveiller, G.\
    \ Remote sensing for agricultural applications: A meta-review. Remote Sens.\n\
    Environ. 2020, 236, 111402. [CrossRef]\n2.\nLiu, J.; Miller, J.R.; Haboudane,\
    \ D.; Pattey, E.; Nolin, M.C. Variability of seasonal CASI image data products\n\
    and potential application for management zone delineation for precision agriculture.\
    \ Can. J. Remote Sens.\n2005, 31, 400–411. [CrossRef]\n3.\nJensen, J.R. Remote\
    \ Sensing of the Environment: An Earth Resource Perspective; Prentice Hall: Upper\
    \ Saddle\nRiver, NJ, USA, 2006.\n4.\nSahoo, R.N.; Ray, S.S.; Manjunath, K.R. Hyperspectral\
    \ remote sensing of agriculture. Curr. Sci. 2015, 108,\n848–859.\n5.\nAlonso,\
    \ F.G.; Soria, S.L.; Gozalo, J.C. Comparing two methodologies for crop area estimation\
    \ in Spain using\nLandsat TM images and ground-gathered data. Remote Sens. Environ.\
    \ 1991, 35, 29–35. [CrossRef]\n6.\nMcNairn, H.; Champagne, C.; Shang, J.; Holmstrom,\
    \ D.; Reichert, G. Integration of optical and Synthetic\nAperture Radar (SAR)\
    \ imagery for delivering operational annual crop inventories. ISPRS J. Photogramm.\n\
    2009, 64, 434–449. [CrossRef]\n7.\nShoshany, M.; Goldshleger, N.; Chudnovsky,\
    \ A. Monitoring of agricultural soil degradation by remote-sensing\nmethods: A\
    \ review. Int. J. Remote Sens. 2013, 34, 6152–6181. [CrossRef]\n8.\nHunt, E.R.;\
    \ Daughtry, C.S.T. What good are unmanned aircraft systems for agricultural remote\
    \ sensing and\nprecision agriculture? Int. J. Remote Sens. 2018, 39, 5345–5376.\
    \ [CrossRef]\n9.\nThenkabail, P.S. Biophysical and yield information for precision\
    \ farming from near-real-time and historical\nLandsat TM images. Int. J. Remote\
    \ Sens. 2003, 24, 2879–2904. [CrossRef]\n10.\nShang, J.; Liu, J.; Ma, B.; Zhao,\
    \ T.; Jiao, X.; Geng, X.; Huﬀman, T.; Kovacs, J.M.; Walters, D. Mapping spatial\n\
    variability of crop growth conditions using RapidEye data in Northern Ontario,\
    \ Canada. Remote Sens. Environ.\n2015, 168, 113–125. [CrossRef]\n11.\nAdão, T.;\
    \ Hruška, J.; Pádua, L.; Bessa, J.; Peres, E.; Morais, R.; Sousa, J. Hyperspectral\
    \ Imaging: A Review\non UAV-Based Sensors, Data Processing and Applications for\
    \ Agriculture and Forestry. Remote Sens. 2017,\n9, 1110. [CrossRef]\n12.\nLucieer,\
    \ A.; Malenovský, Z.; Veness, T.; Wallace, L. HyperUAS-imaging spectroscopy from\
    \ a multirotor\nunmanned aircraft system. J. Field Robot. 2014, 31, 571–590. [CrossRef]\n\
    13.\nGonzalez-Dugo, V.; Hernandez, P.; Solis, I.; Zarco-Tejada, P. Using High-Resolution\
    \ Hyperspectral and\nThermal Airborne Imagery to Assess Physiological Condition\
    \ in the Context of Wheat Phenotyping.\nRemote Sens. 2015, 7, 13586–13605. [CrossRef]\n\
    14.\nLee, K.; Cohen, W.B.; Kennedy, R.E.; Maiersperger, T.K.; Gower, S.T. Hyperspectral\
    \ versus multispectral data\nfor estimating leaf area index in four diﬀerent biomes.\
    \ Remote Sens. Environ. 2004, 91, 508–520. [CrossRef]\n15.\nMariotto, I.; Thenkabail,\
    \ P.S.; Huete, A.; Slonecker, E.T.; Platonov, A. Hyperspectral versus multispectral\n\
    crop-productivity modeling and type discrimination for the HyspIRI mission. Remote\
    \ Sens. Environ. 2013,\n139, 291–305. [CrossRef]\n16.\nMarshall, M.; Thenkabail,\
    \ P. Advantage of hyperspectral EO-1 Hyperion over multispectral IKONOS,\nGeoEye-1,\
    \ WorldView-2, Landsat ETM+, and MODIS vegetation indices in crop biomass estimation.\n\
    ISPRS J. Photogramm. 2015, 108, 205–218. [CrossRef]\n17.\nSun, J.; Yang, J.; Shi,\
    \ S.; Chen, B.; Du, L.; Gong, W.; Song, S. Estimating Rice Leaf Nitrogen Concentration:\n\
    Inﬂuence of Regression Algorithms Based on Passive and Active Leaf Reﬂectance.\
    \ Remote Sens. 2017, 9, 951.\n[CrossRef]\n18.\nDarvishzadeh, R.; Matkan, A.A.;\
    \ Ahangar, A.D. Inversion of a radiative transfer model for estimation of rice\n\
    canopy chlorophyll content using a lookup-table approach. IEEE J.-STARS 2012,\
    \ 5, 1222–1230. [CrossRef]\n19.\nHruska, R.; Mitchell, J.; Anderson, M.; Glenn,\
    \ N.F. Radiometric and geometric analysis of hyperspectral\nimagery acquired from\
    \ an unmanned aerial vehicle. Remote Sens. 2012, 4, 2736–2752. [CrossRef]\n20.\n\
    Transon, J.; d’Andrimont, R.; Maugnard, A.; Defourny, P. Survey of Hyperspectral\
    \ Earth Observation\nApplications from Space in the Sentinel-2 Context. Remote\
    \ Sens. 2018, 10, 157. [CrossRef]\n21.\nLodhi, V.; Chakravarty, D.; Mitra, P.\
    \ Hyperspectral Imaging System: Development Aspects and Recent\nTrends. Sens.\
    \ Imaging 2019, 20, 1–24. [CrossRef]\nRemote Sens. 2020, 12, 2659\n34 of 44\n\
    22.\nHatﬁeld, J.L.; Prueger, J.H. Value of Using Diﬀerent Vegetative Indices to\
    \ Quantify Agricultural Crop\nCharacteristics at Diﬀerent Growth Stages under\
    \ Varying Management Practices. Remote Sens. 2010, 2,\n562–578. [CrossRef]\n23.\n\
    Zhang, H.; Lan, Y.; Suh, C.P.C.; Westbrook, J.; Clint Hoﬀmann, W.; Yang, C.; Huang,\
    \ Y. Fusion of\nremotely sensed data from airborne and ground-based sensors to\
    \ enhance detection of cotton plants.\nComput. Electron. Agric. 2013, 93, 55–59.\
    \ [CrossRef]\n24.\nMahajan, G.R.; Pandey, R.N.; Sahoo, R.N.; Gupta, V.K.; Datta,\
    \ S.C.; Kumar, D. Monitoring nitrogen,\nphosphorus and sulphur in hybrid rice\
    \ (Oryza sativa L.) using hyperspectral remote sensing. Precis. Agric.\n2017,\
    \ 18, 736–761. [CrossRef]\n25.\nSkauli, T.; Goa, P.E.; Baarstad, I.; Loke, T.\
    \ A compact combined hyperspectral and polarimetric imager. In\nProceedings of\
    \ the Society of Photo-Optical Instrumentation Engineers, Stockholm, Sweden, 5\
    \ October 2006;\nDriggers, R.G., Huckridge, D.A., Eds.; SPIE-INT SOC Optical Engineering:\
    \ Bellingham, WA, USA, 2016;\nVolume 6395, pp. 44–51.\n26.\nZarco-Tejada, P.J.;\
    \ Suarez, L.; Gonzalez-Dugo, V. Spatial resolution eﬀects on chlorophyll ﬂuorescence\
    \ retrieval\nin a heterogeneous canopy using hyperspectral imagery and radiative\
    \ transfer simulation. IEEE Geosci.\nRemote Soc. 2013, 10, 937–941. [CrossRef]\n\
    27.\nLu, B.; He, Y.; Dao, P.D. Comparing the Performance of Multispectral and\
    \ Hyperspectral Images for Estimating\nVegetation Properties. IEEE J. STARS 2019,\
    \ 12, 1784–1797. [CrossRef]\n28.\nISS Utilization: MUSES-DESIS (Multi-User System\
    \ for Earth Sensing) with DESIS instrument. Available\nonline: https://directory.eoportal.org/web/eoportal/satellite-missions/content/-/article/iss-muses\
    \ (accessed on\n3 August 2020).\n29.\nPRISMA (Hyperspectral Precursor and Application\
    \ Mission). Available online: https://directory.eoportal.\norg/web/eoportal/satellite-missions/p/prisma-hyperspectral#launch\
    \ (accessed on 3 August 2020).\n30.\nSatellite Missions Database. Available online:\
    \ https://directory.eoportal.org/web/eoportal/satellite-missions\n(accessed on\
    \ 10 November 2019).\n31.\nEnMAP (Environmental Monitoring and Analysis Program).\
    \ Available online: https://directory.eoportal.org/\nweb/eoportal/satellite-missions/e/enmap\
    \ (accessed on 3 August 2020).\n32.\nMitchell, J.J.; Glenn, N.F.; Anderson, M.O.;\
    \ Hruska, R.C.; Halford, A.; Baun, C.; Nydegger, N. Unmanned\nAerial Vehicle (UAV)\
    \ hyperspectral remote sensing for dryland vegetation monitoring. In Proceedings\n\
    of the 2012 4th Workshop on Hyperspectral Image and Signal Processing: Evolution\
    \ in Remote Sensing\n(WHISPERS), Shanghai, China, 4–7 June 2012; pp. 1–10.\n33.\n\
    Zarco-Tejada, P.J.; Guillén-Climent, M.L.; Hernández-Clemente, R.; Catalina, A.;\
    \ González, M.R.; Martín, P.\nEstimating leaf carotenoid content in vineyards\
    \ using high resolution hyperspectral imagery acquired from\nan unmanned aerial\
    \ vehicle (UAV). Agric. Forest Meteorol. 2013, 171, 281–294. [CrossRef]\n34.\n\
    Copenhaver, K.; Hellmich, R.; Hunt, T.; Glaser, J.; Sappington, T.; Calvin, D.;\
    \ Carroll, M.; Fridgen, J. Use of\nspectral vegetation indices derived from airborne\
    \ hyperspectral imagery for detection of European corn\nborer infestation in Iowa\
    \ corn plots. J. Econ. Entomol. 2008, 101, 1614–1623.\n35.\nRyu, C.; Suguri, M.;\
    \ Umeda, M. Multivariate analysis of nitrogen content for rice at the heading\
    \ stage using\nreﬂectance of airborne hyperspectral remote sensing. Field Crops\
    \ Res. 2011, 122, 214–224. [CrossRef]\n36.\nLu, B.; He, Y. Evaluating Empirical\
    \ Regression, Machine Learning, and Radiative Transfer Modelling for\nEstimating\
    \ Vegetation Chlorophyll Content Using Bi-Seasonal Hyperspectral Images. Remote\
    \ Sens. 2019,\n11, 1979. [CrossRef]\n37.\nYu, F.; Xu, T.; Du, W.; Ma, H.; Zhang,\
    \ G.; Chen, C. Radiative transfer models (RTMs) for ﬁeld phenotyping\ninversion\
    \ of rice based on UAV hyperspectral remote sensing. Int. J. Agric. Biol. Eng.\
    \ 2017, 10, 150–157.\n38.\nTeke, M.; Deveci, H.S.; Haliloglu, O.; Gurbuz, S.Z.;\
    \ Sakarya, U. A short survey of hyperspectral remote\nsensing applications in\
    \ agriculture. In Proceedings of the 2013 6th International Conference on Recent\n\
    Advances in Space Technologies (RAST), Istanbul, Turkey, 12–14 June 2013; IEEE:\
    \ New York, NY, USA, 2013;\npp. 171–176.\n39.\nDale, L.M.; Thewis, A.; Boudry,\
    \ C.; Rotar, I.; Dardenne, P.; Baeten, V.; Pierna, J.A.F. Hyperspectral\nImaging\
    \ Applications in Agriculture and Agro-Food Product Quality and Safety Control:\
    \ A Review.\nAppl. Spectrosc. Rev. 2013, 48, 142–159. [CrossRef]\n40.\nTiangong/Shenzhou:\
    \ China’s Human Spaceﬂight Program/Tianzhou Cargo Spaceship. Available online:\n\
    https://directory.eoportal.org/web/eoportal/satellite-missions/t/tiangong (accessed\
    \ on 3 August 2020).\nRemote Sens. 2020, 12, 2659\n35 of 44\n41.\nApan, A.; Held,\
    \ A.; Phinn, S.; Markley, J. Detecting sugarcane ‘orange rust’ disease using EO-1\
    \ Hyperion\nhyperspectral imagery. Int. J. Remote Sens. 2004, 25, 489–498. [CrossRef]\n\
    42.\nDutta, S.; Bhattacharya, B.K.; Rajak, D.R.; Chattopadhayay, C.; Patel, N.K.;\
    \ Parihar, J.S. Disease detection in\nmustard crop using eo-1 hyperion satellite\
    \ data. J. Indian Soc. Remote 2006, 34, 325–330. [CrossRef]\n43.\nMoharana, S.;\
    \ Dutta, S. Spatial variability of chlorophyll and nitrogen content of rice from\
    \ hyperspectral\nimagery. ISPRS J. Photogramm. 2016, 122, 17–29. [CrossRef]\n\
    44.\nThenkabail, P.S.; Mariotto, I.; Gumma, M.K.; Middleton, E.M.; Landis, D.R.;\
    \ Huemmrich, K.F. Selection\nof Hyperspectral Narrowbands (HNBs) and Composition\
    \ of Hyperspectral Twoband Vegetation Indices\n(HVIs) for Biophysical Characterization\
    \ and Discrimination of Crop Types Using Field Reﬂectance and\nHyperion/EO-1 Data.\
    \ IEEE J. STARS 2013, 6, 427–439. [CrossRef]\n45.\nWu, C.; Han, X.; Niu, Z.; Dong,\
    \ J. An evaluation of EO-1 hyperspectral Hyperion data for chlorophyll content\n\
    and leaf area index estimation. Int. J. Remote Sens. 2010, 31, 1079–1086. [CrossRef]\n\
    46.\nBannari, A.; Staenz, K.; Champagne, C.; Khurshid, K. Spatial Variability\
    \ Mapping of Crop Residue Using\nHyperion (EO-1) Hyperspectral Data. Remote Sens.\
    \ 2015, 7, 8107–8127. [CrossRef]\n47.\nGalloza, M.S.; Crawford, M. Exploiting\
    \ multisensor spectral data to improve crop residue cover estimates\nfor management\
    \ of agricultural water quality. In Proceedings of the IEEE Geoscience and Remote\
    \ Sensing\nSociety Symposium, Vancouver, BC, Canada, 24–29 July 2011; IEEE: New\
    \ York, NY, USA, 2011; pp. 3668–3671.\n48.\nCamacho Velasco, A.; Vargas García,\
    \ C.A.; Arguello Fuentes, H. A comparative study of target detection\nalgorithms\
    \ in hyperspectral imagery applied to agricultural crops in Colombia. Revista\
    \ Tecnura 2016, 20,\n86–99. [CrossRef]\n49.\nGomez, C.; Rossel, R.A.V.; McBratney,\
    \ A.B. Soil organic carbon prediction by hyperspectral remote sensing\nand ﬁeld\
    \ vis-NIR spectroscopy: An Australian case study. Geoderma 2008, 146, 403–411.\
    \ [CrossRef]\n50.\nZhang, T.; Li, L.; Zheng, B. Estimation of agricultural soil\
    \ properties with imaging and laboratory spectroscopy.\nJ. Appl. Remote Sens.\
    \ 2013, 7, 73587. [CrossRef]\n51.\nBostan, S.; Ortak, M.A.; Tuna, C.; Akoguz,\
    \ A.; Sertel, E.; Ustundag, B.B. Comparison of classiﬁcation accuracy\nof co-located\
    \ hyperspectral & multispectral images for agricultural purposes. In Proceedings\
    \ of the 2016\nFifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics),\
    \ Tianjin, China, 18–20 July\n2016; IEEE: New York, NY, USA, 2016; pp. 1–4.\n\
    52.\nLodhi, V.; Chakravarty, D.; Mitra, P. Hyperspectral Imaging for Earth Observation:\
    \ Platforms and Instruments.\nJ. Indian Inst. Sci. 2018, 98, 429–443. [CrossRef]\n\
    53.\nAasen, H.; Bolten, A. Multi-temporal high-resolution imaging spectroscopy\
    \ with hyperspectral 2D imagers -\nFrom theory to application. Remote Sens. Environ.\
    \ 2018, 205, 374–389. [CrossRef]\n54.\nJia, X.; Li, S.; Ke, S.; Hu, B. Overview\
    \ of spaceborne hyperspectral imagers and the research progress in\nbathymetric\
    \ maps. In Proceedings of the Second Target Recognition and Artiﬁcial Intelligence\
    \ Summit\nForum. International Society for Optics and Photonics, Shenyang, China,\
    \ 28–30 August 2019; SPIE-INT SOC\nOptical Engineering: Bellingham, WA, USA, 2020.\n\
    55.\nHeadwall Hyperspectral Sensors. Available online: https://www.headwallphotonics.com/hyperspectral-\n\
    sensors (accessed on 8 May 2020).\n56.\nPullanagari, R.R.; Kereszturi, G.; Yule,\
    \ I. Integrating Airborne Hyperspectral, Topographic, and Soil Data for\nEstimating\
    \ Pasture Quality Using Recursive Feature Elimination with Random Forest Regression.\
    \ Remote Sens.\n2018, 10, 1117. [CrossRef]\n57.\nVerger, A.; Baret, F.; Camacho,\
    \ F. Optimal modalities for radiative transfer-neural network estimation of\n\
    canopy biophysical characteristics: Evaluation over an agricultural area with\
    \ CHRIS/PROBA observations.\nRemote Sens. Environ. 2011, 115, 415–426. [CrossRef]\n\
    58.\nAntony, R.; Ray, S.S.; Panigrahy, S. Discrimination of wheat crop stage using\
    \ CHRIS/PROBA multi-angle\nnarrowband data. Remote Sens. Lett. 2011, 2, 71–80.\
    \ [CrossRef]\n59.\nCasa, R.; Castaldi, F.; Pascucci, S.; Palombo, A.; Pignatti,\
    \ S. A comparison of sensor resolution and calibration\nstrategies for soil texture\
    \ estimation from hyperspectral remote sensing. Geoderma 2013, 197, 17–26. [CrossRef]\n\
    60.\nKumar, A.S.K.; Samudraiah, D.R.M. Hyperspectral Imager Onboard Indian Mini\
    \ Satellite-1. In Optical\nPayloads for Space Missions; Qian, S., Ed.; John Wiley\
    \ & Sons: Hoboken, NJ, USA, 2015; Volume 6, pp. 141–160.\n61.\nIMS-1 (Indian Microsatellite-1).\n\
    Available online: https://directory.eoportal.org/web/eoportal/satellite-\nmissions/i/ims-1\
    \ (accessed on 31 March 2020).\n62.\nRaval, M.S. Hyperspectral Imaging: A Paradigm\
    \ in Remote Sensing. CSI Commun. 2014, 7, 7–9.\nRemote Sens. 2020, 12, 2659\n\
    36 of 44\n63.\nKhobragade, A.N.; Raghuwanshi, M.M. Contextual Soft Classiﬁcation\
    \ Approaches for Crops Identiﬁcation\nUsing Multi-sensory Remote Sensing Data:\
    \ Machine Learning Perspective for Satellite Images. In Artiﬁcial\nIntelligence\
    \ Perspectives and Applications; Springer: Cham, Switzerland, 2015; pp. 333–346.\n\
    64.\nHyperspectral Imager for the Coastal Ocean. Available online: http://hico.coas.oregonstate.edu/\
    \ (accessed on\n1 April 2020).\n65.\nKrutz, D.; Müller, R.; Knodt, U.; Günther,\
    \ B.; Walter, I.; Sebastian, I.; Säuberlich, T.; Reulke, R.; Carmona, E.;\nEckardt,\
    \ A.; et al. The Instrument Design of the DLR Earth SensingImaging Spectrometer\
    \ (DESIS). Sensors\n2019, 19, 1622. [CrossRef]\n66.\nISS Utilization: HISUI (Hyperspectral\
    \ Imager Suite). Available online: https://eoportal.org/web/eoportal/\nsatellite-missions/content/-/article/iss-utilization-hisui-hyperspectral-imager-suite-#launch\
    \ (accessed on 1\nApril 2020).\n67.\nPignatti, S.; Palombo, A.; Pascucci, S.;\
    \ Romano, F.; Santini, F.; Simoniello, T.; Umberto, A.; Vincenzo, C.;\nAcito,\
    \ N.; Diani, M.; et al. The PRISMA hyperspectral mission: Science activities and\
    \ opportunities for\nagriculture and land monitoring. In Proceedings of the 2013\
    \ IEEE International Geoscience and Remote\nSensing Symposium-IGARSS, Melbourne,\
    \ VIC, Australia, 21–26 July 2013; pp. 4558–4561.\n68.\nEnMap Hyperspectral Imager.\n\
    Available online:\nhttp://www.enmap.org/index.html (accessed on 1\nDecember 2019).\n\
    69.\nFeingersh, T.; Ben-Dor, E. SHALOM—A Commercial Hyperspectral Space Mission.\
    \ In Optical Payloads for\nSpace Missions; Qian, S.E., Ed.; John Wiley & Sons,\
    \ Ltd.: Hoboken, NJ, USA, 2015; pp. 247–263.\n70.\nPandey, P.C.; Manevski, K.;\
    \ Srivastava, P.K.; Petropoulos, G.P. The Use of Hyperspectral Earth Observation\n\
    Data for Land Use/Cover Classiﬁcation: Present Status, Challenges, and Future\
    \ Outlook. In Hyperspectral\nRemote Sensing of Vegetation, 2nd ed.; Thenkabail,\
    \ P.S., Lyon, J.G., Huete, A., Eds.; CRC Press: Boca Raton, FL,\nUSA, 2018; Volume\
    \ 4.\n71.\nHyspIRI Mission Study. Available online: https://hyspiri.jpl.nasa.gov/\
    \ (accessed on 1 August 2020).\n72.\nMalec, S.; Rogge, D.; Heiden, U.; Sanchez-Azofeifa,\
    \ A.; Bachmann, M.; Wegmann, M. Capability of Spaceborne\nHyperspectral EnMAP\
    \ Mission for Mapping Fractional Cover for Soil Erosion Modeling. Remote Sens.\
    \ 2015,\n7, 11776–11800. [CrossRef]\n73.\nSiegmann, B.; Jarmer, T.; Beyer, F.;\
    \ Ehlers, M. The Potential of Pan-Sharpened EnMAP Data for the Assessment\nof\
    \ Wheat LAI. Remote Sens. 2015, 7, 12737–12762. [CrossRef]\n74.\nLocherer, M.;\
    \ Hank, T.; Danner, M.; Mauser, W. Retrieval of Seasonal Leaf Area Index from\
    \ Simulated EnMAP\nData through Optimized LUT-Based Inversion of the PROSAIL Model.\
    \ Remote Sens. 2015, 7, 10321–10346.\n[CrossRef]\n75.\nBachmann, M.; Makarau,\
    \ A.; Segl, K.; Richter, R. Estimating the Inﬂuence of Spectral and Radiometric\n\
    Calibration Uncertainties on EnMAP Data Products—Examples for Ground Reﬂectance\
    \ Retrieval and\nVegetation Indices. Remote Sens. 2015, 7, 10689–10714. [CrossRef]\n\
    76.\nCastaldi, F.; Palombo, A.; Santini, F.; Pascucci, S.; Pignatti, S.; Casa,\
    \ R. Evaluation of the potential of the\ncurrent and forthcoming multispectral\
    \ and hyperspectral imagers to estimate soil texture and organic carbon.\nRemote\
    \ Sens. Environ. 2016, 179, 54–65. [CrossRef]\n77.\nCastaldi, F.; Palombo, A.;\
    \ Pascucci, S.; Pignatti, S.; Santini, F.; Casa, R. Reducing the Inﬂuence of Soil\n\
    Moisture on the Estimation of Clay from Hyperspectral Data: A Case Study Using\
    \ Simulated PRISMA Data.\nRemote Sens. 2015, 7, 15561–15582. [CrossRef]\n78.\n\
    Ghasrodashti, E.; Karami, A.; Heylen, R.; Scheunders, P. Spatial Resolution Enhancement\
    \ of Hyperspectral\nImages Using Spectral Unmixing and Bayesian Sparse Representation.\
    \ Remote Sens. 2017, 9, 541. [CrossRef]\n79.\nYang, J.; Li, Y.; Chan, J.; Shen,\
    \ Q. Image Fusion for Spatial Enhancement of Hyperspectral Image via Pixel\nGroup\
    \ Based Non-Local Sparse Representation. Remote Sens. 2017, 9, 53. [CrossRef]\n\
    80.\nZhao, Y.; Yang, J.; Chan, J.C. Hyperspectral Imagery Super-Resolution by\
    \ Spatial-Spectral Joint Nonlocal\nSimilarity. IEEE J. STARS 2014, 7, 2671–2679.\
    \ [CrossRef]\n81.\nLoncan, L.; Almeida, L.B.; Bioucas-Dias, J.M.; Briottet, X.;\
    \ Chanussot, J.; Dobigeon, N.; Fabre, S.; Liao, W.;\nLicciardi, G.A.; Simões,\
    \ M.; et al. Hyperspectral pansharpening: A review. IEEE Geosci. Remote Sens.\
    \ Mag.\n2015, 3, 27–46. [CrossRef]\n82.\nAsner, G.P.; Heidebrecht, K.B. Imaging\
    \ spectroscopy for desertiﬁcation studies: Comparing aviris and eo-1\nhyperion\
    \ in argentina drylands. IEEE Trans. Geosci. Remote 2003, 41, 1283–1296. [CrossRef]\n\
    Remote Sens. 2020, 12, 2659\n37 of 44\n83.\nWeng, Y.; Gong, P.; Zhu, Z. A Spectral\
    \ Index for Estimating Soil Salinity in the Yellow River Delta Region of\nChina\
    \ Using EO-1 Hyperion Data. Pedosphere 2010, 20, 378–388. [CrossRef]\n84.\nMulla,\
    \ D.J. Twenty ﬁve years of remote sensing in precision agriculture: Key advances\
    \ and remaining\nknowledge gaps. Biosyst. Eng. 2013, 114, 358–371. [CrossRef]\n\
    85.\nJacquemoud, S.; Baret, F.; Andrieu, B.; Danson, F.M.; Jaggard, K. Extraction\
    \ of vegetation biophysical\nparameters by inversion of the PROSPECT + SAIL models\
    \ on sugar beet canopy reﬂectance data. Application\nto TM and AVIRIS sensors.\
    \ Remote Sens. Environ. 1995, 52, 163–172. [CrossRef]\n86.\nGat, N.; Erives, H.;\
    \ Fitzgerald, G.J.; Kaﬀka, S.R.; Maas, S.J. Estimating sugar beet yield using\
    \ AVIRIS-derived\nindices. In Summaries of the 9th JPL Airborne Earth Science\
    \ Workshop. Unpaginated CD; Jet Propulsion Laboratory:\nPasadena, CA, USA, 2000.\n\
    87.\nEstep, L.; Terrie, G.; Davis, B. Crop stress detection using AVIRIS hyperspectral\
    \ imagery and artiﬁcial neural\nnetworks. Int. J. Remote Sens. 2004, 25, 4999–5004.\
    \ [CrossRef]\n88.\nCheng, Y.; Ustin, S.L.; Riano, D.; Vanderbilt, V.C. Water content\
    \ estimation from hyperspectral images and\nMODIS indexes in Southeastern Arizona.\
    \ Remote Sens. Environ. 2008, 112, 363–374. [CrossRef]\n89.\nPalacios-Orueta,\
    \ A.; Ustin, S.L. Remote Sensing of Soil Properties in the Santa Monica Mountains\
    \ I. Spectral\nAnalysis. Remote Sens. Environ. 1998, 65, 170–183. [CrossRef]\n\
    90.\nGat, N.;\nErives, H.;\nMaas, S.J.;\nFitzgerald, G.J. Application of low altitude\
    \ AVIRIS imagery\nof agricultural ﬁelds in the San Joaquin Valley,\nCA, to precision\
    \ farming.\nIn The 8th JPL\nAirborne Earth Science Workshop; Academia:\nPasadena,\
    \ CA, USA, 1999; pp. 145–150.\nAvailable\nonline: https://www.researchgate.net/publication/2434575_Application_Of_Low_Altitude_Aviris_Imagery_\n\
    Of_Agricultural_Fields_In_The_San_Joaquin_Valley_Ca_To_Precision_Farming (accessed\
    \ on 11 July 2020.).\n91.\nNigam, R.; Tripathy, R.; Dutta, S.; Bhagia, N.; Nagori,\
    \ R.; Chandrasekar, K.; Kot, R.; Bhattacharya, B.K.;\nUstin, S. Crop type discrimination\
    \ and health assessment using hyperspectral imaging. Curr. Sci. 2019, 116,\n1108–1123.\
    \ [CrossRef]\n92.\nShivers, S.W.; Roberts, D.A.; McFadden, J.P. Using paired thermal\
    \ and hyperspectral aerial imagery to quantify\nland surface temperature variability\
    \ and assess crop stress within California orchards. Remote Sens. Environ.\n2019,\
    \ 222, 215–231. [CrossRef]\n93.\nRan, Q.; Li, W.; Du, Q.; Yang, C. Hyperspectral\
    \ image classiﬁcation for mapping agricultural tillage practices.\nJ. Appl. Remote\
    \ Sens. 2015, 9, 97298. [CrossRef]\n94.\nShivers, S.W.; Roberts, D.A.; McFadden,\
    \ J.P.; Tague, C. Using Imaging Spectrometry to Study Changes in\nCrop Area in\
    \ California’s Central Valley during Drought. Remote Sens. 2018, 10, 1556. [CrossRef]\n\
    95.\nHaboudane, D.; Miller, J.R.; Tremblay, N.; Zarco-Tejada, P.J.; Dextraze,\
    \ L. Integrated narrow-band vegetation\nindices for prediction of crop chlorophyll\
    \ content for application to precision agriculture. Remote Sens. Environ.\n2002,\
    \ 81, 416–426. [CrossRef]\n96.\nLiu, J.; Miller, J.R.; Haboudane, D.; Pattey,\
    \ E.; Hochheim, K. Crop fraction estimation from casi hyperspectral\ndata using\
    \ linear spectral unmixing and vegetation indices. Can. J. Remote Sens. 2008,\
    \ 34, S124–S138.\n[CrossRef]\n97.\nGoel, P.K.; Prasher, S.O.; Landry, J.; Patel,\
    \ R.M.; Viau, A.A. Hyperspectral image classiﬁcation to detect weed\ninfestations\
    \ and nitrogen status in corn. Trans. ASAE 2003, 46, 539.\n98.\nRichter, K.; Hank,\
    \ T.; Mauser, W. Preparatory analyses and development of algorithms for agricultural\n\
    applications in the context of the EnMAP hyperspectral mission. In Proceedings\
    \ of the Remote Sensing\nfor Agriculture, Ecosystems, and Hydrology XII. International\
    \ Society for Optics and Photonics, Toulouse,\nFrance, 22 October 2010; pp. 782407–7824011.\n\
    99.\nJarmer, T. Spectroscopy and hyperspectral imagery for monitoring summer barley.\
    \ Int. J. Remote Sens. 2013,\n34, 6067–6078. [CrossRef]\n100. Thomas, U.; Philippe,\
    \ D.; Christian, B.; Franz, R.; Frédéric, M.; Martin, S.; Miriam, M.; Lucien,\
    \ H. Retrieving\nthe Bioenergy Potential from Maize Crops Using Hyperspectral\
    \ Remote Sensing. Remote Sens. 2013, 5,\n254–273.\n101. Mewes, T.; Franke, J.;\
    \ Menz, G. Spectral requirements on airborne hyperspectral remote sensing data\
    \ for\nwheat disease detection. Precis. Agric. 2011, 12, 795–812. [CrossRef]\n\
    102. Hbirkou, C.; Pätzold, S.; Mahlein, A.; Welp, G. Airborne hyperspectral imaging\
    \ of spatial soil organic carbon\nheterogeneity at the ﬁeld-scale. Geoderma 2012,\
    \ 175–176, 21–28. [CrossRef]\nRemote Sens. 2020, 12, 2659\n38 of 44\n103. Cilia,\
    \ C.; Panigada, C.; Rossini, M.; Meroni, M.; Busetto, L.; Amaducci, S.; Boschetti,\
    \ M.; Picchi, V.;\nColombo, R. Nitrogen Status Assessment for Variable Rate Fertilization\
    \ in Maize through Hyperspectral\nImagery. Remote Sens. 2014, 6, 6549–6565. [CrossRef]\n\
    104. Ambrus, A.; Burai, P.; Lénárt, C.; Enyedi, P.; Kovács, Z. Estimating biomass\
    \ of winter wheat using narrowband\nvegetation indices for precision agriculture.\
    \ J. Cent. Eur. Green Innov. 2015, 3, 13–22.\n105. Oppelt, N.; Mauser, W. Hyperspectral\
    \ monitoring of physiological parameters of wheat during a vegetation\nperiod\
    \ using AVIS data. Int. J. Remote Sens. 2004, 25, 145–159. [CrossRef]\n106. Bannari,\
    \ A.; Pacheco, A.; Staenz, K.; McNairn, H.; Omari, K. Estimating and mapping crop\
    \ residues cover\non agricultural lands using hyperspectral and IKONOS data. Remote\
    \ Sens. Environ. 2006, 104, 447–459.\n[CrossRef]\n107. Van Wesemael, B.; Tychon,\
    \ B.; Bartholomeus, H.; Kooistra, L.; van Leeuwen, M.; Stevens, A.; Ben-Dor, E.\
    \ Soil\nOrganic Carbon mapping of partially vegetated agricultural ﬁelds with\
    \ imaging spectroscopy. Int. J. Appl.\nEarth Obs. 2011, 13, 81–88.\n108. Finn,\
    \ M.P.; Lewis, M.D.; Bosch, D.D.; Giraldo, M.; Yamamoto, K.; Sullivan, D.G.; Kincaid,\
    \ R.; Luna, R.;\nAllam, G.K.; Kvien, C.; et al.\nRemote Sensing of Soil Moisture\
    \ Using Airborne Hyperspectral Data.\nGisci. Remote Sens. 2011, 48, 522–540. [CrossRef]\n\
    109. Xie, Q.; Huang, W.; Liang, D.; Chen, P.; Wu, C.; Yang, G.; Zhang, J.; Huang,\
    \ L.; Zhang, D. Leaf Area\nIndex Estimation Using Vegetation Indices Derived From\
    \ Airborne Hyperspectral Images in Winter Wheat.\nIEEE J. STARS 2014, 7, 3586–3594.\
    \ [CrossRef]\n110. Castaldi, F.; Chabrillat, S.; Jones, A.; Vreys, K.; Bomans,\
    \ B.; van Wesemael, B. Soil Organic Carbon Estimation\nin Croplands by Hyperspectral\
    \ Remote APEX Data Using the LUCAS Topsoil Database. Remote Sens. 2018,\n10, 153.\
    \ [CrossRef]\n111. Luo, S.; Wang, C.; Xi, X.; Zeng, H.; Li, D.; Xia, S.; Wang,\
    \ P. Fusion of Airborne Discrete-Return LiDAR and\nHyperspectral Data for Land\
    \ Cover Classiﬁcation. Remote Sens. 2016, 8, 3. [CrossRef]\n112. Mart, L.; Tard,\
    \ A.; Pal, V.; Arbiol, R. Atmospheric correction algorithm applied to CASI multi-height\n\
    hyperspectral imagery. Parameters 2006, 1, 4.\n113. AVIRIS Data—New Data Acquisitions.\
    \ Available online: https://aviris.jpl.nasa.gov/data/newdata.html\n(accessed on\
    \ 1 August 2020).\n114. Lu, B.; He, Y. Species classiﬁcation using Unmanned Aerial\
    \ Vehicle (UAV)-acquired high spatial resolution\nimagery in a heterogeneous grassland.\
    \ ISPRS J. Photogramm. 2017, 128, 73–85. [CrossRef]\n115. Casa, R.; Pascucci,\
    \ S.; Pignatti, S.; Palombo, A.; Nanni, U.; Harfouche, A.; Laura, L.; Di Rocco,\
    \ M.; Fantozzi, P.\nUAV-based hyperspectral imaging for weed discrimination in\
    \ maize. In Precision Agriculture ‘19; Staﬀord, J.V.,\nEd.; Wageningen Academic\
    \ Publishers: Wageningen, The Netherlands, 2019; pp. 24–35.\n116. Dao, P.D.; He,\
    \ Y.; Lu, B. Maximizing the quantitative utility of airborne hyperspectral imagery\
    \ for studying\nplant physiology: An optimal sensor exposure setting procedure\
    \ and empirical line method for atmospheric\ncorrection. Int. J. Appl. Earth Obs.\
    \ 2019, 77, 140–150. [CrossRef]\n117. Capolupo, A.; Kooistra, L.; Berendonk, C.;\
    \ Boccia, L.; Suomalainen, J. Estimating plant traits of grasslands\nfrom UAV-acquired\
    \ hyperspectral images: A comparison of statistical approaches. ISPRS Int. J.\
    \ Geo Inf. 2015,\n4, 2792–2820. [CrossRef]\n118. Lu, B.; He, Y. Optimal spatial\
    \ resolution of Unmanned Aerial Vehicle (UAV)-acquired imagery for species\nclassiﬁcation\
    \ in a heterogeneous grassland ecosystem. Gisci. Remote Sens. 2018, 55, 205–220.\
    \ [CrossRef]\n119. Bohnenkamp, D.; Behmann, J.; Mahlein, A. In-Field Detection\
    \ of Yellow Rust in Wheat on the Ground\nCanopy and UAV Scale. Remote Sens. 2019,\
    \ 11, 2495. [CrossRef]\n120. Habib, A.; Han, Y.; Xiong, W.; He, F.; Zhang, Z.;\
    \ Crawford, M. Automated Ortho-Rectiﬁcation of UAV-Based\nHyperspectral Data over\
    \ an Agricultural Field Using Frame RGB Imagery. Remote Sens. 2016, 8, 796.\n\
    [CrossRef]\n121. Honkavaara, E.; Saari, H.; Kaivosoja, J.; Pölönen, I.; Hakala,\
    \ T.; Litkey, P.; Mäkynen, J.; Pesonen, L. Processing\nand assessment of spectrometric,\
    \ stereoscopic imagery collected using a lightweight UAV spectral camera\nfor\
    \ precision agriculture. Remote Sens. 2013, 5, 5006–5039. [CrossRef]\n122. Saari,\
    \ H.; Pellikka, I.; Pesonen, L.; Tuominen, S.; Heikkila, J.; Holmlund, C.; Makynen,\
    \ J.; Ojala, K.; Antila, T.\nUnmanned Aerial Vehicle (UAV) operated spectral camera\
    \ system for forest and agriculture applications.\nIn Proceedings of the Remote\
    \ Sensing for Agriculture, Ecosystems, and Hydrology XIII. International Society\n\
    for Optics and Photonics, Prague, Czech Republic, 6 October 2011; Volume 8174.\n\
    Remote Sens. 2020, 12, 2659\n39 of 44\n123. Honkavaara, E.; Kaivosoja, J.; Mäkynen,\
    \ J.; Pellikka, I.; Pesonen, L.; Saari, H.; Salo, H.; Hakala, T.; Marklelin, L.;\n\
    Rosnell, T. Hyperspectral reﬂectance signatures and point clouds for precision\
    \ agriculture by light weight\nUAV imaging system. ISPRS Ann. Photogramm. Remote\
    \ Sens. Spat. Inf. Sci. 2012, 7, 353–358. [CrossRef]\n124. Yue, J.; Yang, G.;\
    \ Li, C.; Li, Z.; Wang, Y.; Feng, H.; Xu, B. Estimation of Winter Wheat Above-Ground\
    \ Biomass\nUsing Unmanned Aerial Vehicle-Based Snapshot Hyperspectral Sensor and\
    \ Crop Height Improved Models.\nRemote Sens. 2017, 9, 708. [CrossRef]\n125. Pölönen,\
    \ I.; Saari, H.; Kaivosoja, J.; Honkavaara, E.; Pesonen, L. Hyperspectral imaging\
    \ based biomass and\nnitrogen content estimations from light-weight UAV. In Proceedings\
    \ of the Remote Sensing for Agriculture,\nEcosystems, and Hydrology XV. International\
    \ Society for Optics and Photonics, Dresden, Germany, 16 October\n2013; p. 88870J.\n\
    126. Kaivosoja, J.; Pesonen, L.; Kleemola, J.; Pölönen, I.; Salo, H.; Honkavaara,\
    \ E.; Saari, H.; Mäkynen, J.; Rajala, A.\nA case study of a precision fertilizer\
    \ application task generation for wheat based on classiﬁed hyperspectral\ndata\
    \ from UAV combined with farm history data. In Proceedings of the SPIE Remote\
    \ Sensing, Dresden,\nGermany, 24–26 September 2013; pp. 1–11.\n127. Akhtman, Y.;\
    \ Golubeva, E.; Tutubalina, O.; Zimin, M. Application of hyperspectural images\
    \ and ground data\nfor precision farming. Geogr. Environ. Sustain. 2017, 10, 117–128.\
    \ [CrossRef]\n128. Izzo, R.R.; Lakso, A.N.; Marcellus, E.D.; Bauch, T.D.; Raqueno,\
    \ N.G.; van Aardt, J. An initial analysis of\nreal-time sUAS-based detection of\
    \ grapevine water status in the Finger Lakes Wine Country of Upstate\nNew York.\
    \ In Proceedings of the Autonomous Air and Ground Sensing Systems for Agricultural\
    \ Optimization and\nPhenotyping IV; International Society for Optics and Photonics:\
    \ Baltimore, MD, USA, 2019.\n129. Scherrer, B.; Sheppard, J.; Jha, P.; Shaw, J.A.\
    \ Hyperspectral imaging and neural networks to classify\nherbicide-resistant weeds.\
    \ J. Appl. Remote Sens. 2019, 13, 044516. [CrossRef]\n130. Yue, J.; Feng, H.;\
    \ Jin, X.; Yuan, H.; Li, Z.; Zhou, C.; Yang, G.; Tian, Q. A Comparison of Crop\
    \ Parameters\nEstimation Using Images from UAV-Mounted Snapshot Hyperspectral\
    \ Sensor and High-Deﬁnition Digital\nCamera. Remote Sens. 2018, 10, 1138. [CrossRef]\n\
    131. Dalponte, M.; Orka, H.O.; Gobakken, T.; Gianelle, D.; Naesset, E. Tree Species\
    \ Classiﬁcation in Boreal Forests\nwith Hyperspectral Data. IEEE Trans. Geosci.\
    \ Remote 2013, 51, 2632–2645. [CrossRef]\n132. Aasen, H.; Bendig, J.; Bolten,\
    \ A.; Bennertz, S.; Willkomm, M.; Bareth, G. Introduction and preliminary results\n\
    of a calibration for full-frame hyperspectral cameras to monitor agricultural\
    \ crops with UAVs. Int. Arch.\nPhotogramm. Remote Sens. Spat. Inf. Sci. 2014,\
    \ XL-7, 1–8. [CrossRef]\n133. Zhu, W.; Sun, Z.; Huang, Y.; Lai, J.; Li, J.; Zhang,\
    \ J.; Yang, B.; Li, B.; Li, S.; Zhu, K.; et al. Improving Field-Scale\nWheat LAI\
    \ Retrieval Based on UAV Remote-Sensing Observations and Optimized VI-LUTs. Remote\
    \ Sens.\n2019, 11, 2456. [CrossRef]\n134. Zhao, J.; Zhong, Y.; Hu, X.; Wei, L.;\
    \ Zhang, L. A robust spectral-spatial approach to identifying heterogeneous\n\
    crops using remote sensing imagery with high spectral and spatial resolutions.\
    \ Remote Sens. Environ. 2020,\n239, 111605. [CrossRef]\n135. Zarco-Tejada, P.J.;\
    \ González-Dugo, V.; Berni, J.A.J. Fluorescence, temperature and narrow-band indices\n\
    acquired from a UAV platform for water stress detection using a micro-hyperspectral\
    \ imager and a thermal\ncamera. Remote Sens. Environ. 2012, 117, 322–337. [CrossRef]\n\
    136. Lu, B.; He, Y.; Liu, H.H.T. Mapping vegetation biophysical and biochemical\
    \ properties using unmanned\naerial vehicles-acquired imagery. Int. J. Remote\
    \ Sens. 2018, 39, 5265–5287. [CrossRef]\n137. Malmir, M.; Tahmasbian, I.; Xu,\
    \ Z.; Farrar, M.B.; Bai, S.H. Prediction of soil macro- and micro-elements in\n\
    sieved and ground air-dried soils using laboratory-based hyperspectral imaging\
    \ technique. Geoderma 2019,\n340, 70–80. [CrossRef]\n138. Van de Vijver, R.; Mertens,\
    \ K.; Heungens, K.; Somers, B.; Nuyttens, D.; Borra-Serrano, I.; Lootens, P.;\n\
    Roldan-Ruiz, I.; Vangeyte, J.; Saeys, W. In-ﬁeld detection of Altemaria solani\
    \ in potato crops using\nhyperspectral imaging. Comput. Electron. Agric. 2020,\
    \ 168, 105106. [CrossRef]\n139. Eddy, P.R.; Smith, A.M.; Hill, B.D.; Peddle, D.R.;\
    \ Coburn, C.A.; Blackshaw, R.E. Hybrid segmentation -\nArtiﬁcial Neural Network\
    \ classiﬁcation of high resolution hyperspectral imagery for Site-Speciﬁc Herbicide\n\
    Management in agriculture. Photogramm. Eng. Remote Sens. 2008, 74, 1249–1257.\
    \ [CrossRef]\n140. Feng, H.; Chen, G.; Xiong, L.; Liu, Q.; Yang, W. Accurate Digitization\
    \ of the Chlorophyll Distribution\nof Individual Rice Leaves Using Hyperspectral\
    \ Imaging and an Integrated Image Analysis Pipeline.\nFront. Plant Sci. 2017,\
    \ 8, 1238. [CrossRef]\nRemote Sens. 2020, 12, 2659\n40 of 44\n141. Asaari, M.S.M.;\
    \ Mishra, P.; Mertens, S.; Dhondt, S.; Inzé, D.; Wuyts, N.; Scheunders, P. Close-range\n\
    hyperspectral image analysis for the early detection of stress responses in individual\
    \ plants in a\nhigh-throughput phenotyping platform. ISPRS J. Photogramm. 2018,\
    \ 138, 121–138. [CrossRef]\n142. Zhu, W.; Li, J.; Li, L.; Wang, A.; Wei, X.; Mao,\
    \ H. Nondestructive diagnostics of soluble sugar, total nitrogen\nand their ratio\
    \ of tomato leaves in greenhouse by polarized spectra–hyperspectra Introduction\
    \ to the pls\nPackage l data fusion. Int. J. Agric. Biol. Eng. 2020, 13, 189–197.\n\
    143. Morel, J.; Jay, S.; Féret, J.; Bakache, A.; Bendoula, R.; Carreel, F.; Gorretta,\
    \ N. Exploring the potential of\nPROCOSINE and close-range hyperspectral imaging\
    \ to study the eﬀects of fungal diseases on leaf physiology.\nSci. Rep. 2018,\
    \ 8, 1–13. [CrossRef] [PubMed]\n144. Nagasubramanian, K.; Jones, S.; Singh, A.K.;\
    \ Sarkar, S.; Singh, A.; Ganapathysubramanian, B. Plant disease\nidentiﬁcation\
    \ using explainable 3D deep learning on hyperspectral images. Plant Methods 2019,\
    \ 15, 98.\n[CrossRef] [PubMed]\n145. Lopatin, J.; Fassnacht, F.E.; Kattenborn,\
    \ T.; Schmidtlein, S. Mapping plant species in mixed grassland\ncommunities using\
    \ close range imaging spectroscopy. Remote Sens. Environ. 2017, 201, 12–23. [CrossRef]\n\
    146. Behmann, J.; Mahlein, A.; Paulus, S.; Dupuis, J.; Kuhlmann, H.; Oerke, E.;\
    \ Plümer, L. Generation and\napplication of hyperspectral 3D plant models: Methods\
    \ and challenges. Mach. Vis. Appl. 2016, 27, 611–624.\n[CrossRef]\n147. Antonucci,\
    \ F.; Menesatti, P.; Holden, N.M.; Canali, E.; Giorgi, S.; Maienza, A.; Stazi,\
    \ S.R. Hyperspectral Visible\nand Near-Infrared Determination of Copper Concentration\
    \ in Agricultural Polluted Soils. Commun. Soil\nSci. Plan. 2012, 43, 1401–1411.\
    \ [CrossRef]\n148. Wan, P.; Yang, G.; Xu, B.; Feng, H.; Yu, H. Geometric Correction\
    \ Method of Rotary Scanning Hyperspectral\nImage in Agriculture Application. In\
    \ Proceedings of the Conferences of the Photoelectronic Technology\nCommittee\
    \ of the Chinese Society of Astronautics, Beijing, China, 13–15 May 2014.\n149.\
    \ Yeh, Y.; Chung, W.; Liao, J.; Chung, C.; Kuo, Y.; Lin, T. Strawberry foliar\
    \ anthracnose assessment by\nhyperspectral imaging. Comput. Electron. Agric. 2016,\
    \ 122, 1–9. [CrossRef]\n150. Liu, Y.; Wang, T.; Ma, L.; Wang, N. Spectral calibration\
    \ of hyperspectral data observed from a\nhyperspectrometer loaded on an Unmanned\
    \ Aerial Vehicle platform.\nIEEE J. Sel.\nTop.\nAppl.\nEarth\nObs. Remote Sens.\
    \ 2014, 7, 2630–2638.\n151. Miglani, A.; Ray, S.S.; Pandey, R.; Parihar, J.S.\
    \ Evaluation of EO-1 hyperion data for agricultural applications.\nJ. Indian Soc.\
    \ Remote 2008, 36, 255–266. [CrossRef]\n152. Amato, U.; Antoniadis, A.; Carfora,\
    \ M.F.; Colandrea, P.; Cuomo, V.; Franzese, M.; Pignatti, S.; Serio, C.\nStatistical\
    \ Classiﬁcation for Assessing PRISMA Hyperspectral Potential for Agricultural\
    \ Land Use.\nIEEE J. STARS 2013, 6, 615–625. [CrossRef]\n153. Thenkabail, P.S.;\
    \ Gumma, M.K.; Teluguntla, P.; Mohammed, I.A. Hyperspectral remote sensing of\
    \ vegetation\nand agricultural crops. Photogramm. Eng. Remote Sens. J. Am. Soc.\
    \ Photogramm. 2014, 80, 697–709.\n154. Wang, Y.; Yao, H.; Zhao, S. Auto-encoder\
    \ based dimensionality reduction. Neurocomputing 2016, 184, 232–242.\n[CrossRef]\n\
    155. Hsu, P.; Tseng, Y.; Gong, P. Dimension Reduction of Hyperspectral Images\
    \ for Classiﬁcation Applications.\nGeogr. Inf. Sci. 2002, 8, 1–8. [CrossRef]\n\
    156. Abdolmaleki, M.; Fathianpour, N.; Tabaei, M. Evaluating the performance of\
    \ the wavelet transform in\nextracting spectral alteration features from hyperspectral\
    \ images. Int. J. Remote Sens. 2018, 39, 6076–6094.\n[CrossRef]\n157. Cao, X.;\
    \ Yao, J.; Fu, X.; Bi, H.; Hong, D. An Enhanced 3-D Discrete Wavelet Transform\
    \ for Hyperspectral\nImage Classiﬁcation. IEEE Geosci. Remote Soc. 2020, 1–5.\
    \ [CrossRef]\n158. Prabhakar, T.V.N.; Geetha, P. Two-dimensional empirical wavelet\
    \ transform based supervised hyperspectral\nimage classiﬁcation. ISPRS J. Photogramm.\
    \ 2017, 133, 37–45. [CrossRef]\n159. Geng, X.; Sun, K.; Ji, L.; Zhao, Y. A Fast\
    \ Volume-Gradient-Based Band Selection Method for Hyperspectral\nImage. IEEE Trans.\
    \ Geosci. Remote 2014, 52, 7111–7119. [CrossRef]\n160. Wang, C.; Gong, M.; Zhang,\
    \ M.; Chan, Y. Unsupervised Hyperspectral Image Band Selection via Column\nSubset\
    \ Selection. IEEE Geosci. Remote Soc. 2015, 12, 1411–1415. [CrossRef]\n161. Wang,\
    \ Q.; Lin, J.; Yuan, Y. Salient Band Selection for Hyperspectral Image Classiﬁcation\
    \ via Manifold Ranking.\nIEEE Trans. Neural Netw. Learn. Syst. 2016, 27, 1279–1289.\
    \ [CrossRef]\nRemote Sens. 2020, 12, 2659\n41 of 44\n162. Thenkabail, P.S.; Smith,\
    \ R.B.; De Pauw, E. Hyperspectral vegetation indices and their relationships with\n\
    agricultural crop characteristics. Remote Sens. Environ. 2000, 71, 158–182. [CrossRef]\n\
    163. Nevalainen, O.; Hakala, T.; Suomalainen, J.; Kaasalainen, S. Nitrogen concentration\
    \ estimation with\nhyperspectral LiDAR. ISPRS Ann. Photogramm. Remote Sens. Spat.\
    \ Inf. Sci. 2013, 2, 205–210. [CrossRef]\n164. Huang, W.; Lamb, D.W.; Niu, Z.;\
    \ Zhang, Y.; Liu, L.; Wang, J. Identiﬁcation of yellow rust in wheat using\nin-situ\
    \ spectral reﬂectance measurements and airborne hyperspectral imaging. Precis.\
    \ Agric. 2007, 8, 187–197.\n[CrossRef]\n165. Tong, A.; He, Y. Estimating and mapping\
    \ chlorophyll content for a heterogeneous grassland: Comparing\nprediction power\
    \ of a suite of vegetation indices across scales between years. ISPRS J. Photogramm.\
    \ 2017, 126,\n146–167. [CrossRef]\n166. Haboudane, D.; Tremblay, N.; Miller, J.R.;\
    \ Vigneault, P. Remote estimation of crop chlorophyll content using\nspectral\
    \ indices derived from hyperspectral data. IEEE T. Geosci. Remote 2008, 46, 423–437.\
    \ [CrossRef]\n167. Main, R.; Cho, M.A.; Mathieu, R.; O’Kennedy, M.M.; Ramoelo,\
    \ A.; Koch, S. An investigation into robust\nspectral indices for leaf chlorophyll\
    \ estimation. ISPRS J. Photogramm. 2011, 66, 751–761. [CrossRef]\n168. Peng, Y.;\
    \ Gitelson, A.A. Remote estimation of gross primary productivity in soybean and\
    \ maize based on\ntotal crop chlorophyll content. Remote Sens. Environ. 2012,\
    \ 117, 440–448. [CrossRef]\n169. Croft, H.; Chen, J.M.; Zhang, Y. The applicability\
    \ of empirical vegetation indices for determining leaf\nchlorophyll content over\
    \ diﬀerent leaf and canopy structures. Ecol. Complex. 2014, 17, 119–130. [CrossRef]\n\
    170. Zhou, X.; Huang, W.; Kong, W.; Ye, H.; Luo, J.; Chen, P. Remote estimation\
    \ of canopy nitrogen content in\nwinter wheat using airborne hyperspectral reﬂectance\
    \ measurements. Adv. Space Res. 2016, 58, 1627–1637.\n[CrossRef]\n171. Yue, J.;\
    \ Feng, H.; Yang, G.; Li, Z. A comparison of regression techniques for estimation\
    \ of above-ground\nwinter wheat biomass using near-surface spectroscopy. Remote\
    \ Sens. 2018, 10, 66. [CrossRef]\n172. Hansen, P.M.; Schjoerring, J.K. Reﬂectance\
    \ measurement of canopy biomass and nitrogen status in wheat crops\nusing normalized\
    \ diﬀerence vegetation indices and partial least squares regression. Remote Sens.\
    \ Environ.\n2003, 86, 542–553. [CrossRef]\n173. Nguyen, H.T.; Lee, B. Assessment\
    \ of rice leaf growth and nitrogen status by hyperspectral canopy reﬂectance\n\
    and partial least square regression. Eur. J. Agron. 2006, 24, 349–356. [CrossRef]\n\
    174. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel,\
    \ O.; Blondel, M.; Prettenhofer, P.;\nWeiss, R.; Dubourg, V.; et al. Scikit-learn:\
    \ Machine learning in Python. Mach. Learn. 2011, 12, 2825–2830.\n175. Mevik, B.;\
    \ Wehrens, R. Introduction to the PLS Package. Help Sect. “Pls” Package R Studio\
    \ Softw; R Found. Stat.\nComput.: Vienna, Austria, 2015; pp. 1–23.\n176. Asner,\
    \ G.P.; Martin, R.E.; Anderson, C.B.; Knapp, D.E. Quantifying forest canopy traits:\
    \ Imaging spectroscopy\nversus ﬁeld survey. Remote Sens. Environ. 2015, 158, 15–27.\
    \ [CrossRef]\n177. Kiala, Z.; Odindi, J.; Mutanga, O. Potential of interval partial\
    \ least square regression in estimating leaf area\nindex. S. Afr. J. Sci. 2017,\
    \ 113, 40–48. [CrossRef]\n178. Wang, Z.; Kawamura, K.; Sakuno, Y.; Fan, X.; Gong,\
    \ Z.; Lim, J. Retrieval of Chlorophyll-a and Total Suspended\nSolids Using Iterative\
    \ Stepwise Elimination Partial Least Squares (ISE-PLS) Regression Based on Field\n\
    Hyperspectral Measurements in Irrigation Ponds in Higashihiroshima, Japan. Remote\
    \ Sens. 2017, 9, 264.\n[CrossRef]\n179. Mehmood, T.; Ahmed, B. The diversity in\
    \ the applications of partial least squares: An overview. J. Chemometr.\n2016,\
    \ 30, 4–17. [CrossRef]\n180. Jacquemoud, S.; Baret, F. PROSPECT—A model of leaf\
    \ optical-properties spectra. Remote Sens. Environ. 1990,\n34, 75–91. [CrossRef]\n\
    181. Jacquemoud, S.; Bacour, C.; Poilve, H.; Frangi, J.P. Comparison of four radiative\
    \ transfer models to simulate\nplant canopies reﬂectance: Direct and inverse mode.\
    \ Remote Sens. Environ. 2000, 74, 471–481. [CrossRef]\n182. Casa, R.; Jones, H.G.\
    \ Retrieval of crop canopy properties: A comparison between model inversion from\n\
    hyperspectral data and image classiﬁcation. Int. J. Remote Sens. 2004, 25, 1119–1130.\
    \ [CrossRef]\n183. Richter, K.; Hank, T.; Atzberger, C.; Locherer, M.; Mauser,\
    \ W. Regularization strategies for agricultural\nmonitoring: The EnMAP vegetation\
    \ analyzer (AVA). In Proceedings of the 2012 IEEE International Geoscience\nand\
    \ Remote Sensing Symposium, Munich, Germany, 22–27 July 2012; pp. 6613–6616.\n\
    184. Wu, C.; Wang, L.; Niu, Z.; Gao, S.; Wu, M. Nondestructive estimation of canopy\
    \ chlorophyll content using\nHyperion and Landsat/TM images. Int. J. Remote Sens.\
    \ 2010, 31, 2159–2167. [CrossRef]\nRemote Sens. 2020, 12, 2659\n42 of 44\n185.\
    \ Darvishzadeh, R.; Atzberger, C.; Skidmore, A.; Schlerf, M. Mapping grassland\
    \ leaf area index with airborne\nhyperspectral imagery: A comparison study of\
    \ statistical approaches and inversion of radiative transfer\nmodels. ISPRS J.\
    \ Photogramm. 2011, 66, 894–906. [CrossRef]\n186. Breiman, L. Random forests.\
    \ Mach. Learn. 2001, 45, 5–32. [CrossRef]\n187. Were, K.; Bui, D.T.; Dick, O.B.;\
    \ Singh, B.R. A comparative assessment of support vector regression, artiﬁcial\n\
    neural networks, and random forests for predicting and mapping soil organic carbon\
    \ stocks across an\nAfromontane landscape. Ecol. Indic. 2015, 52, 394–403. [CrossRef]\n\
    188. Gao, J.; Nuyttens, D.; Lootens, P.; He, Y.; Pieters, J.G. Recognising weeds\
    \ in a maize crop using a random\nforest machine-learning algorithm and near-infrared\
    \ snapshot mosaic hyperspectral imagery. Biosyst. Eng.\n2018, 170, 39–50. [CrossRef]\n\
    189. Siegmann, B.; Jarmer, T. Comparison of diﬀerent regression models and validation\
    \ techniques for the\nassessment of wheat leaf area index from hyperspectral data.\
    \ Int. J. Remote Sens. 2015, 36, 4519–4534.\n[CrossRef]\n190. Adam, E.; Deng,\
    \ H.; Odindi, J.; Abdel-Rahman, E.M.; Mutanga, O. Detecting the Early Stage of\
    \ Phaeosphaeria\nLeaf Spot Infestations in Maize Crop Using In Situ Hyperspectral\
    \ Data and Guided Regularized Random\nForest Algorithm. J. Spectrosc. 2017, 2017,\
    \ 1–8. [CrossRef]\n191. Kamilaris, A.; Prenafeta-Boldú, F.X. Deep learning in\
    \ agriculture: A survey. Comput. Electron. Agric. 2018,\n147, 70–90. [CrossRef]\n\
    192. Yuan, Q.; Shen, H.; Li, T.; Li, Z.; Li, S.; Jiang, Y.; Xu, H.; Tan, W.; Yang,\
    \ Q.; Wang, J.; et al. Deep learning\nin environmental remote sensing: Achievements\
    \ and challenges. Remote Sens. Environ. 2020, 241, 111716.\n[CrossRef]\n193. Sharma,\
    \ A.; Liu, X.; Yang, X. Land cover classiﬁcation from multi-temporal, multi-spectral\
    \ remotely sensed\nimagery using patch-based recurrent neural networks. Neural\
    \ Netw. 2018, 105, 346–355. [CrossRef]\n194. Zhang, C.; Sargent, I.; Pan, X.;\
    \ Li, H.; Gardiner, A.; Hare, J.; Atkinson, P.M. Joint Deep Learning for land\n\
    cover and land use classiﬁcation. Remote Sens. Environ. 2019, 221, 173–187. [CrossRef]\n\
    195. Rezaee, M.; Mahdianpari, M.; Zhang, Y.; Salehi, B. Deep Convolutional Neural\
    \ Network for Complex Wetland\nClassiﬁcation Using Optical Remote Sensing Imagery.\
    \ IEEE J. STARS 2018, 11, 3030–3039. [CrossRef]\n196. Xu, Y.; Wu, L.; Xie, Z.;\
    \ Chen, Z. Building Extraction in Very High Resolution Remote Sensing Imagery\
    \ Using\nDeep Learning and Guided Filters. Remote Sens. 2018, 10, 144. [CrossRef]\n\
    197. Kuwata, K.; Shibasaki, R. Estimating crop yields with deep learning and remotely\
    \ sensed data. In Proceedings\nof the 2015 IEEE International Geoscience and Remote\
    \ Sensing Symposium (IGARSS), Milan, Italy, 26–31\nJuly 2015; pp. 858–861.\n198.\
    \ Mohanty, S.P.; Hughes, D.P.; Salathé, M. Using Deep Learning for Image-Based\
    \ Plant Disease Detection.\nFront. Plant Sci. 2016, 7, 1419. [CrossRef] [PubMed]\n\
    199. Ji, S.; Zhang, C.; Xu, A.; Shi, Y.; Duan, Y. 3D Convolutional Neural Networks\
    \ for Crop Classiﬁcation with\nMulti-Temporal Remote Sensing Images. Remote Sens.\
    \ 2018, 10, 75. [CrossRef]\n200. Ndikumana, E.; Ho Tong Minh, D.; Baghdadi, N.;\
    \ Courault, D.; Hossard, L. Deep Recurrent Neural Network\nfor Agricultural Classiﬁcation\
    \ using multitemporal SAR Sentinel-1 for Camargue, France. Remote Sens. 2018,\n\
    10, 1217. [CrossRef]\n201. Singh, A.K.; Ganapathysubramanian, B.; Sarkar, S.;\
    \ Singh, A. Deep Learning for Plant Stress Phenotyping:\nTrends and Future Perspectives.\
    \ Trends Plant Sci. 2018, 23, 883–898. [CrossRef]\n202. Chlingaryan, A.; Sukkarieh,\
    \ S.; Whelan, B. Machine learning approaches for crop yield prediction and\nnitrogen\
    \ status estimation in precision agriculture: A review. Comput. Electron. Agric.\
    \ 2018, 151, 61–69.\n[CrossRef]\n203. Song, X.; Zhang, G.; Liu, F.; Li, D.; Zhao,\
    \ Y.; Yang, J. Modeling spatio-temporal distribution of soil moisture\nby deep\
    \ learning-based cellular automata model. J. Arid Land 2016, 8, 734–748. [CrossRef]\n\
    204. Moharana, S.; Dutta, S. Estimation of water stress variability for a rice\
    \ agriculture system from space-borne\nhyperion imagery. Agr. Water Manag. 2019,\
    \ 213, 260–269. [CrossRef]\n205. Yang, C. Airborne Hyperspectral Imagery for Mapping\
    \ Crop Yield Variability. Geogr. Compass 2009, 3,\n1717–1731. [CrossRef]\n206.\
    \ Zimdahl, R.L. Six Chemicals That Changed Agriculture; Academic Press: Cambridge,\
    \ MA, USA, 2015.\nRemote Sens. 2020, 12, 2659\n43 of 44\n207. Goel, P.K.; Prasher,\
    \ S.O.; Landry, J.A.; Patel, R.M.; Bonnell, R.B.; Viau, A.A.; Miller, J.R. Potential\n\
    of airborne hyperspectral remote sensing to detect nitrogen deﬁciency and weed\
    \ infestation in corn.\nComput. Electron. Agric. 2003, 38, 99–124. [CrossRef]\n\
    208. Quemada, M.; Gabriel, J.; Zarco-Tejada, P. Airborne Hyperspectral Images\
    \ and Ground-Level Optical Sensors\nAs Assessment Tools for Maize Nitrogen Fertilization.\
    \ Remote Sens. 2014, 6, 2940–2962. [CrossRef]\n209. Koppe, W.; Laudien, R.; Gnyp,\
    \ M.L.; Jia, L.; Li, F.; Chen, X.; Bareth, G. Deriving winter wheat characteristics\n\
    from combined radar and hyperspectral data analysis. In Proceedings of the Geoinformatics,\
    \ Wuhan, China,\n28–29 October 2006; Remotely Sensed Data and Information. SPIE-INT\
    \ SOC Optical Engineering: Bellingham,\nWA, USA, 2006.\n210. Castaldi, F.; Castrignano,\
    \ A.; Casa, R. A data fusion and spatial data analysis approach for the estimation\
    \ of\nwheat grain nitrogen uptake from satellite data. Int. J. Remote Sens. 2016,\
    \ 37, 4317–4336. [CrossRef]\n211. Zheng, H.; Zhou, X.; Cheng, T.; Yao, X.; Tian,\
    \ Y.; Cao, W.; Zhu, Y. Evaluation of a uav-based hyperspectral\nframe camera for\
    \ monitoring the leaf nitrogen concentration in rice. In Proceedings of the IEEE\
    \ International\nSymposium on Geoscience and Remote Sensing IGARSS, Beijing, China,\
    \ 10–15 July 2016; pp. 7350–7353.\n212. Zhou, K.; Cheng, T.; Zhu, Y.; Cao, W.;\
    \ Ustin, S.L.; Zheng, H.; Yao, X.; Tian, Y. Assessing the Impact of Spatial\n\
    Resolution on the Estimation of Leaf Nitrogen Concentration Over the Full Season\
    \ of Paddy Rice Using\nNear-Surface Imaging Spectroscopy Data. Front. Plant Sci.\
    \ 2018, 9, 964. [CrossRef] [PubMed]\n213. Nasi, R.; Viljanen, N.; Kaivosoja, J.;\
    \ Alhonoja, K.; Hakala, T.; Markelin, L.; Honkavaara, E. Estimating Biomass\n\
    and Nitrogen Amount of Barley and Grass Using UAV and Aircraft Based Spectral\
    \ and Photogrammetric 3D\nFeatures. Remote Sens. 2018, 10, 1082. [CrossRef]\n\
    214. Nigon, T.J.; Mulla, D.J.; Rosen, C.J.; Cohen, Y.; Alchanatis, V.; Knight,\
    \ J.; Rud, R. Hyperspectral aerial imagery\nfor detecting nitrogen stress in two\
    \ potato cultivars. Comput. Electron. Agric. 2015, 112, 36–46. [CrossRef]\n215.\
    \ Chen, S.; Chen, C.; Wang, C.; Yang, I.; Hsiao, S. Evaluation of nitrogen content\
    \ in cabbage seedlings using\nhyper-spectral images. In Proceedings of the Optics\
    \ East, Boston, MA, USA, 9–12 September 2007; p. L7610.\n216. Miphokasap, P.;\
    \ Wannasiri, W. Estimations of Nitrogen Concentration in Sugarcane Using Hyperspectral\n\
    Imagery. Sustainability 2018, 10, 1266. [CrossRef]\n217. Malmir, M.; Tahmasbian,\
    \ I.; Xu, Z.; Farrar, M.B.; Bai, S.H. Prediction of macronutrients in plant leaves\
    \ using\nchemometric analysis and wavelength selection. J. Soil. Sediment. 2020,\
    \ 20, 249–259. [CrossRef]\n218. Lowe, A.; Harrison, N.; French, A.P. Hyperspectral\
    \ image analysis techniques for the detection and\nclassiﬁcation of the early\
    \ onset of plant disease and stress. Plant Methods 2017, 13, 80. [CrossRef]\n\
    219. Kingra, P.K.; Majumder, D.; Singh, S.P. Application of Remote Sensing and\
    \ Gis in Agriculture and Natural\nResource Management Under Changing Climatic\
    \ Conditions. Agric. Res. J. 2016, 53, 295. [CrossRef]\n220. Karimi, Y.; Prasher,\
    \ S.O.; McNairn, H.; Bonnell, R.B.; Dutilleul, P.; Goel, R.K. Classiﬁcation accuracy\
    \ of\ndiscriminant analysis, artiﬁcial neural networks, and decision trees for\
    \ weed and nitrogen stress detection in\ncorn. Trans. ASAE 2005, 48, 1261–1268.\
    \ [CrossRef]\n221. Zhang, Y.; Slaughter, D.C.; Staab, E.S. Robust hyperspectral\
    \ vision-based classiﬁcation for multi-season weed\nmapping. ISPRS J. Photogramm.\
    \ 2012, 69, 65–73. [CrossRef]\n222. Eddy, P.R.; Smith, A.M.; Hill, B.D.; Peddle,\
    \ D.R.; Coburn, C.A.; Blackshaw, R.E. Weed and crop discrimination\nusing hyperspectral\
    \ image data and reduced bandsets. Can. J. Remote Sens. 2014, 39, 481–490. [CrossRef]\n\
    223. Liu, B.; Li, R.; Li, H.; You, G.; Yan, S.; Tong, Q. Crop/Weed Discrimination\
    \ Using a Field Imaging Spectrometer\nSystem. Sensors 2019, 19, 5154. [CrossRef]\
    \ [PubMed]\n224. LÓPEZ-Granados, F. Weed detection for site-speciﬁc weed management:\
    \ Mapping and real-time approaches.\nWeed Res. 2011, 51, 1–11. [CrossRef]\n225.\
    \ Thomas, S.; Kuska, M.T.; Bohnenkamp, D.; Brugger, A.; Alisaac, E.; Wahabzada,\
    \ M.; Behmann, J.; Mahlein, A.\nBeneﬁts of hyperspectral imaging for plant disease\
    \ detection and plant protection: A technical perspective.\nJ. Plant Dis. Protect.\
    \ 2018, 125, 5–20. [CrossRef]\n226. Bauriegel, E.; Giebel, A.; Geyer, M.; Schmidt,\
    \ U.; Herppich, W.B. Early detection of Fusarium infection in\nwheat using hyper-spectral\
    \ imaging. Comput. Electron. Agric. 2011, 75, 304–312. [CrossRef]\n227. Zhang,\
    \ N.; Pan, Y.; Feng, H.; Zhao, X.; Yang, X.; Ding, C.; Yang, G. Development of\
    \ Fusarium head blight\nclassiﬁcation index using hyperspectral microscopy images\
    \ of winter wheat spikelets. Biosyst. Eng. 2019,\n186, 83–99. [CrossRef]\n228.\
    \ Mahlein, A.; Oerke, E.; Steiner, U.; Dehne, H. Recent advances in sensing plant\
    \ diseases for precision crop\nprotection. Eur. J. Plant Pathol. 2012, 133, 197–209.\
    \ [CrossRef]\nRemote Sens. 2020, 12, 2659\n44 of 44\n229. Casa, R.; Castaldi,\
    \ F.; Pascucci, S.; Basso, B.; Pignatti, S. Geophysical and Hyperspectral Data\
    \ Fusion\nTechniques for In-Field Estimation of Soil Properties. Vadose Zone J.\
    \ 2013, 12, vzj2012.0201. [CrossRef]\n230. Casa, R.; Castaldi, F.; Pascucci, S.;\
    \ Pignatti, S. Potential of hyperspectral remote sensing for ﬁeld scale soil\n\
    mapping and precision agriculture applications. Ital. J. Agron. 2012, 7, 43. [CrossRef]\n\
    231. Gedminas, L.; Martin, S. Soil Organic Matter Mapping Using Hyperspectral\
    \ Imagery and Elevation Data.\nIn IEEE Aerospace Conference Proceedings; IEEE:\
    \ Big Sky, MT, USA, 2019.\n232. Song, X.; Yan, G.; Wan, J.; Liu, L.; Xue, X.;\
    \ Li, C.; Huang, W. Use of airborne hyperspectral imagery to\ninvestigate the\
    \ inﬂuence of soil nitrogen supplies and variable-rate fertilization to winter\
    \ wheat growth.\nIn Proceedings of the SPIE, Florence, Italy, 11 October 2007.\n\
    233. Wang, W.; Li, Z.; Wang, C.; Zheng, D.; Du, H. Prediction of Available Potassium\
    \ Content in Cinnamon Soil\nUsing Hyperspectral Imaging Technology. Spectrosc.\
    \ Spect. Anal. 2019, 39, 1579–1585.\n234. McCann, C.; Repasky, K.S.; Lawrence,\
    \ R.; Powell, S. Multi–temporal mesoscale hyperspectral data of\nmixed agricultural\
    \ and grassland regions for anomaly detection. ISPRS J. Photogramm. 2017, 131,\
    \ 121–133.\n[CrossRef]\n© 2020 by the authors. Licensee MDPI, Basel, Switzerland.\
    \ This article is an open access\narticle distributed under the terms and conditions\
    \ of the Creative Commons Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  pdf_link: https://www.mdpi.com/2072-4292/12/16/2659/pdf?version=1597839578
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Recent Advances of Hyperspectral Imaging Technology and Applications in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.4314/wsa.v33i2.49049
  analysis: '>'
  authors:
  - Moreshnee Govender
  - K.T. Chetty
  - H. H. Bulcock
  citation_count: 206
  full_citation: '>'
  full_text: ">\nAvailable on website http://www.wrc.org.za\nISSN 0378-4738 = Water\
    \ SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA (on-line)\n145\nA review\
    \ of hyperspectral remote sensing and its application \nin vegetation and water\
    \ resource studies\nM Govender1*, K Chetty2 and H Bulcock2 \n1CSIR Natural Resources\
    \ and the Environment, c/o School of Environmental Sciences, University of KwaZulu-Natal,\
    \ \nPrivate Bag X01, Scottsville 3209, South Africa\n2 School of Bioresources\
    \ Engineering and Environmental Hydrology, University of KwaZulu-Natal, Private\
    \ Bag X01, \nScottsville 3209, South Africa\nAbstract\nMultispectral imagery has\
    \ been used as the data source for water and land observational remote sensing\
    \ from airborne and \nsatellite systems since the early 1960s. Over the past two\
    \ decades, advances in sensor technology have made it possible for \nthe collection\
    \ of several hundred spectral bands. This is commonly referred to as hyperspectral\
    \ imagery. This review details \nthe differences between multispectral and hyperspectral\
    \ data; spatial and spectral resolutions and focuses on the application \nof hyperspectral\
    \ imagery in water resource studies and, in particular the classification and\
    \ mapping of land uses and vegeta-\ntion.\nKeywords: hyperspectral, multispectral,\
    \ spectral resolution, spatial resolution, vegetation classification, water \n\
    resources\nIntroduction\nWater is one of the most valuable and essential resources\
    \ that \nform the basis of all life. With the ever-increasing human \npopulation,\
    \ there is constant stress exerted on water resources \n(McGwire et al., 2000).\
    \ Accurate monitoring and assessment of \nour water resources is necessary for\
    \ sustained water resource \nmanagement. Earth observation data have formed the\
    \ basis for \nacquiring data remotely for many years (Landgrebe, 1999) and \n\
    are viewed as a time- and cost-effective way to undertake large-\nscale monitoring\
    \ (Okin et al., 2001), which can be used to deter-\nmine the quality, quantity\
    \ and geographic distribution of this \nresource. \n \nMultispectral imagery has\
    \ been used as the data source \nfor water and land observational remote sensing\
    \ from airborne \nand satellite systems since the 1960s (Landgrebe, 1999). Multi-\n\
    spectral systems commonly collect data in three to six spectral \nbands in a single\
    \ observation from the visible and near-infrared \nregion of the electromagnetic\
    \ spectrum. This crude spectral cat-\negorisation of the reflected and emitted\
    \ energy from the earth \nis the primary limiting factor of multispectral sensor\
    \ systems. \nOver the past two decades, advances in sensor technology have \n\
    overcome this limitation of earth observation systems, with the \ndevelopment\
    \ of hyperspectral sensor technologies. Hyperspec-\ntral systems have made it\
    \ possible for the collection of several \nhundred spectral bands in a single\
    \ acquisition, thus producing \nmany more detailed spectral data. However, with\
    \ the advances in \nhyperspectral technologies practical issues related to increased\
    \ \nsensor or imager costs, data volumes and data-processing costs \nand times\
    \ would need to be considered especially for operational \nmodes. \n \nThis review\
    \ details the differences between multispec-\ntral and hyperspectral data, highlights\
    \ commonly used remote \nsensing terminology, and focuses on the use of hyperspectral\
    \ \nimagery in water resource studies and, in particular vegetation \napplications.\
    \ \nDifferences between multispectral and hyper-\nspectral data\nMultispectral\
    \ airborne and satellite systems have been employed \nfor gathering data in the\
    \ fields of agriculture and food produc-\ntion, geology, oil and mineral exploration,\
    \ geography and urban \nto non-urban localities (Landgrebe, 1999). The advantage\
    \ of \nusing satellite remote sensing systems was to provide both the \nsynoptic\
    \ view space provides and the economies of scale, since \ndata over large areas\
    \ could be gathered quickly and economi-\ncally from such platforms (Landgrebe,\
    \ 1999).\n \nMultispectral remote sensing systems use parallel sensor \narrays\
    \ that detect radiation in a small number of broad wave-\nlength bands. According\
    \ to Smith (2001a), most multispectral \nsatellite systems measure between three\
    \ and six spectral bands \nwithin the visible to middle infrared region of the\
    \ electromag-\nnetic spectrum. There are, however, some systems that use one \n\
    or more thermal infrared bands. Multispectral remote sensing \nallows for the\
    \ discrimination of different types of vegetation, \nrocks and soils, clear and\
    \ turbid water, and selected man-made \nmaterials (Smith, 2001a). To obtain data\
    \ of a higher spectral res-\nolution compared to multispectral data, hyperspectral\
    \ sensors \non board satellites or airborne hyperspectral imagers are used \n\
    (Smith, 2001b). \n \nHyperspectral remote sensing imagers acquire many, very \n\
    narrow, contiguous spectral bands throughout the visible, near-\ninfrared, mid-infrared,\
    \ and thermal infrared portions of the \nelectromagnetic spectrum. Hyperspectral\
    \ sensors typically col-\nlect 200 or more bands enabling the construction of\
    \ an almost \ncontinuous reflectance spectrum for every pixel in the scene. \n\
    Contiguous, narrow bandwidths characteristic of hyperspectral \n*  To whom all\
    \ correspondence should be addressed. \n +27 33 260 5276; fax: +27 33 260 5266;\
    \ \ne-mail: MGovender@csir.co.za \nReceived 24 August 2006; accepted in revised\
    \ form 24 November 2006.\n146\nAvailable on website http://www.wrc.org.za\nISSN\
    \ 0378-4738 = Water SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA (on-line)\n\
    data allow for in-depth examination of earth surface features \nwhich would otherwise\
    \ be ‘lost’ within the relatively coarse \nbandwidths acquired with multispectral\
    \ scanners.\n \nOver the past decade, extensive research and development \nhas\
    \ been carried out in the field of hyperspectral remote sens-\ning. Now with commercial\
    \ airborne hyperspectral imagers such \nas CASI and Hymap and the launch of satellite-based\
    \ sensors \nsuch as Hyperion, hyperspectral imaging is fast moving into \nthe\
    \ mainstream of remote sensing and applied remote sensing \nresearch studies.\
    \ Hyperspectral images have found many appli-\ncations in water resource management,\
    \ agriculture and environ-\nmental monitoring (Smith, 2001a). It is important\
    \ to remember \nthat there is not necessarily a difference in spatial resolution\
    \ \nbetween hyperspectral and multispectral data but rather in their \nspectral\
    \ resolutions.\nFundamentals of image resolution\nRemotely sensed images are characterised\
    \ by their spatial and \nspectral resolutions. The terms spatial and spectral\
    \ resolution \nrepresent pixels of an image displayed in a geometric relation-\n\
    ship to one another; and variations within pixels as a function of \nwavelength\
    \ respectively.  These fundamentals of image resolu-\ntion are explained below.\n\
    Spectral resolution\nSpectral resolution refers to the number and width of the\
    \ por-\ntions of the electromagnetic spectrum measured by the sensor. A \nsensor\
    \ may be sensitive to a large portion of the electromagnetic \nspectrum but have\
    \ poor spectral resolution if it captures a small \nnumber of wide bands. A sensor\
    \ that is sensitive to the same \nportion of the electromagnetic spectrum but\
    \ captures many \nsmall bands within that portion would have greater spectral\
    \ \nresolution. The objective of finer spectral sampling is to enable \nthe analyst,\
    \ human or computer, to distinguish between scene \nelements. Detailed information\
    \ about how individual elements \nin a scene reflect or emit electromagnetic energy\
    \ increases the \nprobability of finding unique characteristics for a given ele-\n\
    ment which allows for better distinction from other elements \nin the scene (Jensen,\
    \ 1996). Multispectral remote sensing sys-\ntems record energy over several separate\
    \ wavelength ranges \nat various spectral resolutions. Hyperspectral sensors,\
    \ detect \nhundreds of very narrow spectral bands throughout the visible, \nnear-infrared,\
    \ and mid-infrared portions of the electromagnetic \nspectrum. A distinct advantage\
    \ of their very high spectral reso-\nlution facilitates fine discrimination between\
    \ different targets \nbased on their spectral response in each of the narrow bands\
    \ \n(Landgrebe, 1999).\nSpatial resolution\nSpatial resolution defines the level\
    \ of spatial detail depicted in \nan image. This may be described as a measure\
    \ of the smallness \nof objects on the ground that may be distinguished as sepa-\n\
    rate entities in the image, with the smallest object necessarily \nbeing larger\
    \ than a single pixel. In this sense, spatial resolution \nis directly related\
    \ to image pixel size. The spatial property of \nan image is a function of the\
    \ design of the sensor in terms of \nits field of view and the altitude at which\
    \ it operates above the \nsurface (Smith, 2001b). Each of the detectors in a remote\
    \ sensor \nmeasures the energy received from a finite patch on the ground \nsurface.\
    \ The smaller the individual patches are, the higher the \nspatial resolution\
    \ and the more detail one can spatially interpret \nfrom the image (Smith, 2001b).\
    \ Currently hyperspectral imagery \nacquired with satellite systems are usually\
    \ in the order of 30 m \nor finer whereas airborne systems generally acquire higher\
    \ spa-\ntial resolution data usually in the order of 5 m or finer.\nContiguous\
    \ spectral signatures\nAn understanding of spectral signatures is essential in\
    \ the \nunderstanding and interpretation of a remotely sensed image. \nDifferent\
    \ materials are discriminated by wavelength-depend-\nent absorptions, and these\
    \ images of reflected solar energy are \nknown as spectral signatures. The property\
    \ that is used to quan-\ntify these spectral signatures is called spectral reflectance.\
    \ This \nis a ratio of the reflected energy to incident energy as a function \n\
    of wavelength (Smith, 2001b). The graph of the spectral reflect-\nance of an object\
    \ as a function of wavelength is termed the spec-\ntral reflectance curve (Lillesand\
    \ and Kiefer, 1999).  \n \nThe configuration of the spectral reflectance curves\
    \ is \nimportant in the determination of the wavelength region(s) in \nwhich remote\
    \ sensing data is acquired as the spectral reflect-\nance curves give insight\
    \ into the spectral characteristics of an \nobject (Lillesand and Kiefer, 1999).\
    \ Spectral signatures obtained \nfrom multispectral images are discrete compared\
    \ to the contigu-\nous signatures obtained from hyperspectral images. Contiguous\
    \ \nspectral signatures allow for detailed analysis through the detec-\ntion of\
    \ surface materials and their abundances, as well as infer-\nences of biological\
    \ and chemical processes. \n \nThe three basic contiguous spectral reflectance\
    \ signatures of \nearth features used are those for green vegetation, dry bare\
    \ soil \nand clear water. Figure 1 shows the average reflectance curves \nfor\
    \ each feature. \n \nIn green vegetation, the valleys in the visible portion of\
    \ the \nspectrum are determined by the pigmentation of the plant. For \nexample,\
    \ chlorophyll absorbs strongly in the blue (450 nm) and \nred (670 nm) regions,\
    \ also known as the chlorophyll absorption \nbands. Chlorophyll is the primary\
    \ photosynthetic pigment in \ngreen plants (Smith, 2000b). This is the reason\
    \ for the human \neye perceiving healthy vegetation as green, due to the strong\
    \ \nabsorption of the red and blue wavelengths and the reflection of \nthe green\
    \ wavelengths. When the plant is subjected to stress that \nhinders normal growth\
    \ and chlorophyll production, there is less \nadsorption in the red and blue regions\
    \ and the amount of reflec-\ntion in the red waveband increases (Smith, 2000b).\n\
    \ \nThe spectral reflectance signature illustrates a dramatic \nincrease in the\
    \ reflection for healthy vegetation at around  \nFigure 1\nSpectral signatures\
    \ for dry bare soil, green vegetation and \nclear water (Smith, 2000b)\nAvailable\
    \ on website http://www.wrc.org.za\nISSN 0378-4738 = Water SA Vol. 33 No. 2 April\
    \ 2007\nISSN 1816-7950 = Water SA (on-line)\n147\n700 nm. In the NIR between 700\
    \ and 1 300 nm, a plant leaf will \ntypically reflect between 40 to 50%, the rest\
    \ is transmitted, with \nonly about 5% being adsorbed. Structural variability\
    \ in leaves \nin this range allows one to differentiate between species, even\
    \ \nthough they might look the same in the visible region (Lillesand \nand Kiefer,\
    \ 1999). Beyond 1 300 nm the incident energy upon \nthe vegetation is largely\
    \ absorbed or reflected with very little \ntransmittance of energy. Three strong\
    \ water absorption bands \nare noted at 1 400, 1 900 and 2 700 nm.\n \nThe spectral\
    \ curve for bare soil shows far less variation in \nreflectance compared to that\
    \ of green vegetation. This is due to \nthe factors that affect soil reflectance\
    \ acting over less specific \nspectral bands. Factors that affect soil reflectance\
    \ include mois-\nture content, soil texture, surface roughness, presence of iron\
    \ \noxide, and organic matter content. The factors that influence soil \nreflectance\
    \ are complex, variable and interrelated (Lillesand and \nKiefer, 1999).  \n \n\
    The most distinctive characteristic of water is the absorption \nin the NIR and\
    \ beyond. There are various conditions of water \nbodies that manifest themselves\
    \ in the visible wavelengths. There \nare complex energy-matter interactions at\
    \ these wavelengths \nthat depend on a number of factors. These factors include\
    \ the \ninteraction with the water surface and material suspended in the \nwater.\
    \ Clear water reflects the greatest at 600 nm. The presence \nof organic and inorganic\
    \ materials greatly influences the trans-\nmittance of the water and therefore\
    \ the reflectance is dramati-\ncally affected. For example, a water body with\
    \ high amounts of \nsuspended sediments will reflect better than ‘clear’ water.\
    \ An \nincrease in chlorophyll will decrease the reflectance in the blue \nwavelengths\
    \ and increase in the green. This can be useful in the \ndetection of algae in\
    \ water using remote sensing (Lillesand and \nKiefer, 1999).\nApplications of\
    \ hyperspectral remote sensing in \nwater resources\nRemote sensing technology\
    \ has been widely used in water \nresource applications (Gitelson and Merzlyak,\
    \ 1996; Zagolski et \nal., 1996; Asner, 1998; McGwire et al., 2000; Stone et al.,\
    \ 2001; \nCoops et al., 2002; Underwood et al., 2003) and in particular \nhyperspectral\
    \ remote sensing is emerging as the more in-depth \nmeans of investigating spatial,\
    \ spectral and temporal varia-\ntions in order to derive more accurate estimates\
    \ of information \nrequired for water resource applications. This section briefly\
    \ \nhighlights applications of hyperspectral remote sensing in water \nresources,\
    \ and is followed by a detailed review of the methods \nand applications of land-\
    \ use and vegetation classification. \nFlood detection and monitoring are constrained\
    \ by the ina-\nbility to obtain timely information of water conditions from \n\
    ground measurements and airborne observations at sufficient \ntemporal and spatial\
    \ resolutions. Satellite remote sensing allows \nfor timely investigation of areas\
    \ of large regional extent and \nprovides frequent imaging of the region of interest\
    \ (Felipe et \nal., 2006). Until recently, near real-time flood detection was\
    \ not \npossible, but with sensors such as Hyperion on board the EO-\n1 satellite\
    \ this has been vastly improved (Felipe et al., 2006). \nAccording to research\
    \ conducted by Felipe et al. (2006) auto-\nmated spacecraft technology reduced\
    \ the time to detect and \nreact to flood events to a few hours. Advances in remote\
    \ sensing, \nhave resulted in the investigation of early warning systems with\
    \ \npotential global applications. Most recent studies from NASA \nand the US\
    \ Geological Survey are utilising satellite observations \nof rainfall, rivers\
    \ and surface topography into early warning sys-\ntems (Brakenridge et al., 2006).\
    \ Specifically, scientists are now \nemploying satellite microwave sensors to\
    \ gauge discharge from \nrivers by measuring changes in river widths and satellite\
    \ based \nestimates of rainfall to improve warning sytems (Brakenridge et \nal.,\
    \ 2006). Procedure for the detection of flooded areas with sat-\nellite data were\
    \ also investigated by Glaber and Reinartz (2002). \nMoisture classes in flood\
    \ plain areas in relation to high water \nchanges, the accumulation of sediments\
    \ and silts for different \nland-use classes and erosive impacts of floods were\
    \ investigated \n(Glaber and Reinartz, 2002). The estimation of discharge and\
    \ \nflood hydrographs from hydraulic information obtained from \nremotely sensed\
    \ data was assessed by Roux and Dartus (2006). \nRemote sensed images as used\
    \ to estimate the hydraulic charac-\nteristics which are then applied in routing\
    \ modules to generate \na flood wave in a synthetic river channel. Optimisation\
    \ methods \nare used to minimise discrepancies between simulations and \nobservations\
    \ of flood extent fields to estimate river discharge \n(Roux and Dartus, 2006).\n\
    Detection of water quality conditions and parameters is one \nof the major advantages\
    \ of hyperspectral remote sensing tech-\nnologies. Hyperspectral reflectance has\
    \ been widely used to \nassess water quality conditions of many open water aquatic\
    \ \necosystems. This includes classifying the trophic status of lakes \n(Koponen\
    \ et al., 2002; Thiemann and Kaufmann, 2002) and \nestuaries (Froidefond et al.,\
    \ 2002) characterising algal blooms \n(Stumpf, 2001) and assessment of ammonia\
    \ dynamics for wet-\nland treatments (Tilley et al., 2003).  Predictors of total\
    \ ammonia \nconcentrations using remotely sensed hyperspectral signatures \nof\
    \ macrophytes in order to monitor changes in wetland water \nquality were developed\
    \ by Tilley et al. (2003). Hyperspectral \nspectrometers have also proved useful\
    \ in determining the total \nsuspended matter, chlorophyll content (Hakvoort et\
    \ al., 2002; \nVos et al., 2003) and total phosphorus (Koponen et al., 2002).\
    \ \nMuch research has been undertaken in the estimation of chloro-\nphyll content\
    \ from remotely sensed images which is then used as \nan estimate for monitoring\
    \ algal content and hence water quality. \nSince wavelengths corresponding with\
    \ the peak reflectance of \nblue-green and green algae are close together it is\
    \ more difficult \nto differentiate between them. However, hyperspectral imagers\
    \ \nallow for improved detection of chlorophyll and hence algae, \ndue to the\
    \ narrow spectral bands which are acquired between \n450 nm and 600 nm. (Hakvoort\
    \ et al., 2002). Estimation and \nmapping of water quality constituents such as\
    \ concentrations of \ndissolved organic matter, chlorophyll or total suspended\
    \ matter \nfrom optical remote sensing technologies have proved to be use-\nful\
    \ and successful and are being investigated for operational use \n(Hakvoort et\
    \ al., 2002).\nWetland mapping has gained increased recognition for the abil-\n\
    ity to improve quality of ecosystems (Schmidt and Skidmore, \n2003). Sustainable\
    \ management of any ecosystem requires, \namong other information, a thorough\
    \ understanding of vegeta-\ntion species distribution. Hyperspectral imagery has\
    \ been used \nto remotely delineate wetland areas and classify hydrophytic \n\
    vegetation characteristics of these ecosystems (Schmidt and \nSkidmore, 2003;\
    \ Becker et al., 2005). Research undertaken by \nSchmidt and Skidmore (2003) promoted\
    \ the use of high spatial \nand spectral resolution data for improved mapping\
    \ of salt marsh \nvegetation of similar structure. The hyperspectral analysis\
    \ \nidentified key regions of the electromagnetic spectrum which \nprovided detailed\
    \ information for discriminating between and \nidentifying different wetland species\
    \ (Schmidt and Skidmore, \n2003). Becker et al. (2005) performed a similar study\
    \ based on \n148\nAvailable on website http://www.wrc.org.za\nISSN 0378-4738 =\
    \ Water SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA (on-line)\ncoastal\
    \ wetland plant communities which are spatially complex \nand heterogeneous. This\
    \ study also emphasised the impor-\ntance of hyperspectral imagery for identifying\
    \ and differentiat-\ning vegetation spectral properties from narrow spectral bands\
    \ \nfocussing on the visible and near-infrared regions (Becker et al., \n2005).\
    \ A number of studies have investigated the potential of \nproviding timely data\
    \ for mapping and monitoring submerged \naquatic vegetation which has been identified\
    \ as one of the most \nimportant aspects of ecosystem restoration and reconstruction\
    \ \n(Lin and Liquan, 2006). Such species have been termed ecologi-\ncal engineering\
    \ species and the quantification of their coverage \nand spectral reflectance\
    \ properties is currently being researched \n(Lin and Liquan, 2006). \nMeasures\
    \ of plant physiology and structure such as leaf area \nindex, water content,\
    \ plant pigment content, canopy architecture \nand density have been investigated\
    \ extensively over the past dec-\nade (Asner, 1998; Datt, 1998; Ceccato et al.,\
    \ 2001; Gitelson et al., \n2002; Champagne et al., 2003; Gupta et al., 2003; Merzlyak\
    \ et \nal., 2003; Pu et al., 2003; D’Urso et al., 2004; Schlerf et al., 2005;\
    \ \nStimson et al., 2005; Chun-Jiang et al., 2006) using hyperspec-\ntral imagery.\
    \ These applications investigate the spectral reflect-\nance properties of plants,\
    \ identifying key spectral wavebands \nrelated to specific plant physiological\
    \ and structural character-\nistics, hence deriving sensitive vegetation spectral\
    \ indices for \ntheir non-destructive estimation. Remote sensing data have been\
    \ \nexploited to estimate canopy characteristics by using empirical \napproaches\
    \ based on spectral indices (D’Urso et al., 2004; Sch-\nlerf et al., 2005). Analysis\
    \ of hyperspectral remote sensing data \nhas been carried out to estimate LAI\
    \ for agricultural crops and \nforests. Schlerf et al. (2005) investigated several\
    \ narrow band \nand broad band vegetation indices in order to explore whether\
    \ \nhyperspectral data may improve the estimation of biophysical \nvariables such\
    \ as LAI, canopy crown and crown volume when \ncompared to multispectral analyses.\
    \ The spectral and spatial \ninformation content of the satellite data was exploited\
    \ to vali-\ndate canopy reflectance models such as PROSPECT and SAILH \n(D’Urso\
    \ et al., 2004). Results obtained for the crops under inves-\ntigation encourage\
    \ the use of canopy reflectance models in the \ninverse mode in order to retrieve\
    \ other vegetation parameters \nsuch as chlorophyll content, dry matter and canopy\
    \ geometrical \ncharacteristics like mean leaf inclination angle (D’Urso et al.,\
    \ \n2004). The accurate estimation of plant water status and plant \nwater stress\
    \ is essential to the integration of remote sensing into \nprecision agricultural\
    \ and forestry management. The potential to \nspectrally estimate plant physiological\
    \ properties over relatively \nlarge areas, and to predict plant water status\
    \ and plant water \nstress was demonstrated by Champagne et al., 2003 for agri-\n\
    cultural crops; and Stimson et al., 2005 and Eitel al., 2006 for \nforestry species.\
    \ Their results indicate the potential use of veg-\netation   spectral indices\
    \ derived from various scales of remote \nsensing data for determining plant physiological\
    \ properties and \ncharacteristics. These studies amongst others clearly indicate\
    \ \nthe improved estimates of plant physiological and structural \ncharacteristics\
    \ from hyperspectral data, allowing for much more \ndetailed spectral analyses\
    \ and hence more accurate estimates.\nEvapotranspiration (ET) estimates are essential\
    \ in a wide range \nof water resource applications such as water and energy balance\
    \ \ncomputations, in irrigation schemes, reservoir water losses, \nrunoff prediction,\
    \ meteorology and climatology (Medina et al., \n1998). ET cannot be estimated\
    \ directly from satellite observa-\ntions; however, hyperspectral remote sensing\
    \ can provide a good \nestimate of components of energy balance algorithms used\
    \ to \nderive spatial estimates of ET. Earlier studies have focused on \nthe estimation\
    \ of evaporative fraction (EF) rather than on ET as \nthe estimation of available\
    \ radiant energy was difficult to obtain. \nAccording to Batra et al (2006) an\
    \ estimation of EF is defined \nas a ratio of ET and available radiant energy\
    \ and has been esti-\nmated successfully using AVHRR and MODIS data. Several\n\
    Several \nrecent studies (Medina et al. 1998; Kite and Droogers, 2000; \nLoukas\
    \ et al. 2005; Batra et al. 2006; Eichinger, et al. 2006) have \nbeen conducted\
    \ using more detailed hyperspectral data, ancil-\nlary surface data and atmospheric\
    \ data for improved spatial esti-\nmates of ET. The availability of water, radiant\
    \ energy and the \nremoval of water vapour away from the surface are the major\
    \ \nfactors that control ET. However these factors in turn depend \non other variables\
    \ such as soil moisture, land surface tempera-\nture, air temperature, and vegetation\
    \ cover, vapour pressure, \nand wind speed which may vary between regions, seasons,\
    \ and \ntime of day. Generally these factors are accounted for by using a \ncombination\
    \ of remote sensing data, ancillary surface data and \natmospheric data for the\
    \ estimation of ET values (Batra et al., \n2006), and has lead to extensive measurements\
    \ of surface fluxes, \nmeteorological and soil variables (Wang et al., 2006).\
    \ Batra et al \n(2006) successfully estimated ET based on the extension of the\
    \ \nPriestly-Taylor equation and the relationship between remotely \nsensed surface\
    \ temperature and vegetation spectral indices.\nLand-use and vegetation classification\n\
    Land-use and vegetation classification is generally performed \nusing supervised\
    \ and unsupervised classification methods \nwhich are commonly available in most\
    \ data processing systems. \nThe key difference in the two methods lies in the\
    \ training stage \nof supervised classification which involves identifying areas\
    \ of \nspecific spectral attributes for each land-cover or land-use type \nof\
    \ interest to the analyst. In comparison unsupervised image \nclassification into\
    \ spectral classes is based solely on the natural \ngroupings from the image values.\
    \ Furthermore, remote sensing \napplications and specifically land-use or vegetation\
    \ classifica-\ntion is seldom done without some form of ground truthing or \n\
    collection of reference data. Ground-based spectral measure-\nments are commonly\
    \ done using portable, field spectrometers. \nField spectrometers are utilised\
    \ in forestry, agriculture and other \nenvironmental studies; and the spectral\
    \ signatures obtained can \nbe used in classification and mapping of vegetation,\
    \ mapping \nof ecosystem productivity, crop type or yield mapping, and in \nthe\
    \ detection of plant stress for water resource operations and \nmanagement. \n\
    Applications\nTraditional methods for landscape-scale vegetation mapping \nrequire\
    \ expensive, time-intensive field surveys. Remotely \nsensed data for the classification\
    \ and mapping of vegetation \nprovide a detailed accurate product in a time- and\
    \ cost-effec-\ntive manner. The availability of satellite and airborne hyper-\n\
    spectral data with its increased spatial and more critically fine \nspectral resolution\
    \ offers an enhanced potential for the classifi-\ncation and mapping of land use\
    \ and vegetation. Due to the large \nnumber of wavebands, image processing is\
    \ able to capitalise \non both the biochemical and structural properties of vegetation\
    \ \n(Underwood et al., 2003). The need for exploring these spectral \nproperties\
    \ is particularly important when we consider the limi-\ntations of using traditionally\
    \ available wavebands, where most \nof the land cover is grouped and identification\
    \ of individual \nspecies is difficult.\nAvailable on website http://www.wrc.org.za\n\
    ISSN 0378-4738 = Water SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA\
    \ (on-line)\n149\n \nVegetation has unique spectral signatures. Vegetation spec-\n\
    tra are often used for the training stage of image classification. \nThe spectral\
    \ reflectance signatures of healthy vegetation have \ncharacteristic shapes that\
    \ are dictated by various plant attributes. \nIn the visible portion of the spectrum,\
    \ the curve is governed by \nabsorption effects of chlorophyll and other leaf\
    \ pigments.  Leaf \nstructure varies significantly between plant species, and\
    \ can also \nchange as a result of stress. Thus species type, plant stress and\
    \ \ncanopy state can all affect near-infrared reflectance and mak-\ning it difficult\
    \ to distinguish between different vegetation types \n(Smith, 2001a), as illustrated\
    \ in Fig. 2.\n \nSeveral studies have derived band ratios or spectral indices\
    \ \nwhich are useful for emphasising certain physiological features, \nand can\
    \ be used to distinguish between different vegetation \nwithin a mosaic of other\
    \ land uses (McGwire et al., 2000; Under-\nwood et al., 2003; Yamano et al., 2003;\
    \ Galvao et al., 2005). \nCommonly used indices include normalised difference\
    \ vegeta-\ntion index, soil-adjusted vegetation index, and modified soil-\nadjusted\
    \ vegetation index (McGwire et al., 2000). Vegetation \nindices are also used\
    \ to mask out vegetated areas from remote \nsensed imagery which are then used\
    \ in the classification process \n(Underwood et al., 2003; Yamano et al., 2003;\
    \ Koch et al., 2005). \nSimilarly, principal component analysis (PCA) has been\
    \ applied \nas a data enhancement method when analysing remote sensing \nimages\
    \ to distinguish between vegetated and non-vegetation \nareas (Castro-Esau et\
    \ al., 2004). These techniques facilitate the \nclassification of vegetated areas,\
    \ especially over large spatial \nscales where the landscape is unknown. \n \n\
    Classification algorithms or statistical classifiers such as \nspectral angle\
    \ mapper, Gaussian maximum likelihood and par-\nallelepiped classifier use reflectance\
    \ spectra as reference data \nfor identifying classes from both multispectral\
    \ and hyperspec-\ntral images. Reference spectra are measured or collected from\
    \ \npure and single image pixels or larger training areas. The qual-\nity or ‘pureness’\
    \ of the reference spectra is an important factor \nthat defines the classification\
    \ results (Castro-Esau et al., 2004). \nImproved land-cover and land-use classification\
    \ is expected \nfrom hyperspectral data due to the improved quality in the ref-\n\
    erence spectra. Examples of these classification algorithms or \nstatistical classifiers\
    \ are discussed below.\nGaussian maximum likelihood estimates of the parameters\
    \ are \ncomputed, and individual pixels are assigned to the class which \nmaximises\
    \ the likelihood function of the dataset. As a pixel by \npixel method this approach\
    \ does not take contextual informa-\ntion about the neighbouring classes into\
    \ account in labelling a \npixel. However, increased information provided by the\
    \ spatial \nextent of the classes of the neighbours tends to mitigate the \neffects\
    \ of noise, isolated pixels, and individual pixels (Castro-\nEsau, 2004).\nSpectral\
    \ angle mapper classifies by comparing the spectral \nangles between the reflectance\
    \ spectrum of the classified pixel \nand the reference spectrum obtained from\
    \ training data or a \nspectral library. Each pixel is assigned to a class according\
    \ to \nthe lowest spectral angle value (Kruce et al., 1993). \nParallelepiped\
    \ classification is a decision rule method based on \nthe standard deviation from\
    \ the mean of each defined and trained \nclass. A threshold of each class signature\
    \ is used to determine \nif a given pixel falls within a class. Pixels which fall\
    \ inside the \nparallelepiped are assigned to the class; however, those within\
    \ \nmore than one class are grouped into an overlap class. Pixels \nungrouped\
    \ are considered as unclassified (Lillesand and Kiefer, \n1999). \nDiscussion\
    \ and conclusion\nRecent advances in remote sensing have led the way for the \n\
    development of hyperspectral sensors and the application of \nhyperspectral data.\
    \ Hyperspectral remote sensing is a relatively \nnew technology to South Africa\
    \ that is being used in the detec-\ntion and identification of minerals, terrestrial\
    \ vegetation, and \nman-made materials as well as in the field of water resources\
    \ \nand environmental applications (Nagy and Jung, 2005). \n \nThe availability\
    \ of hyperspectral data has overcome the \nconstraints and limitations of low\
    \ spectral and spatial resolution \nimagery, and discreet spectral signatures.\
    \ Hyperspectral images \nprovide high spectral resolution data, with many narrow\
    \ con-\ntiguous spectral bands allowing for detailed applications.\n \nThis review\
    \ highlights the vast extent to which hyperspec-\ntral technologies have been\
    \ applied to the water resource and \nenvironmental sectors. In particular, the\
    \ remotely sensed clas-\nsification and mapping of land use and vegetation has\
    \ been \nadopted internationally, since traditional methods of classifying \n\
    and mapping land use and vegetation require expensive, time-\nintensive field\
    \ surveys. Remotely sensed data allow for the clas-\nsification and mapping of\
    \ vegetation in a time and cost-effective \nmanner. \n \nDifferent processing\
    \ and statistical techniques such as \nspectral vegetation indices and principal\
    \ component analy-\nsis are often used as data enhancement methods to mask  \n\
    Figure 2\nReflectance spectra of different \ntypes of green vegetation com-\n\
    pared to a spectral signature for \nsenescent leaves (Smith, 2001a)\n150\nAvailable\
    \ on website http://www.wrc.org.za\nISSN 0378-4738 = Water SA Vol. 33 No. 2 April\
    \ 2007\nISSN 1816-7950 = Water SA (on-line)\nvegetated areas within remotely sensed\
    \ images. Vegetated areas \nare then classified using one or more recognised classification\
    \ \nalgorithms such as spectral angle mapper, Gaussian maximum \nlikelihood and\
    \ parallelepiped, resulting in more accurate prod-\nucts borne from hyperspectral\
    \ data sources. As hyperspectral \nimaging techniques evolve and are introduced\
    \ into new fields \ntheir primary contribution will be in exploring and developing\
    \ \nnew applications primarily on the selection of optimal spectral \nband parameters\
    \ i.e. band position and widths. Practical issues \nof sensor costs, data volumes\
    \ and data processing mechanisms \nwill need to addressed for operational use,\
    \ and thus still favour \nmultispectral systems.\n \nIn general this review illustrates\
    \ the enhanced capability \nof hyperspectral technologies in vegetation and water\
    \ resource \nstudies and, allows water resource managers to make informed \nmanagement\
    \ decisions with the relevant detail in an efficient \ntimeframe.\nReferences\n\
    ASNER GP (1998) Biophysical and biochemical sources of variability \nin canopy\
    \ reflectance. Remote Sens. Environ. 64 234-253.\nBATRA N, ISLAM S, VENTURINI\
    \ V, BISHT G and JIANG L (2006) \nEstimation of comparison of evapotranspiration\
    \ from MODIS and \nAVHRR sensors for clear sky days over the Southern Great Plains.\
    \ \nRemote Sens. Environ. 103 1-15.\nBECKER BL, LUSCH DP and QI J (2005) Identifying\
    \ optimal spectral \nbands from in-situ measurements of Great Lakes coastal wetlands\
    \ \nusing second derivative analysis. Remote Sens. Environ. 97 238-\n248.\nBRACKENRIDGE\
    \ R, ANDERSON E and NGHIEM SV (2006) Satel-\nlite microwave detection and measurement\
    \ of river floods.  NASA \nSpring Annual General Conference 2006. www.nasa.gov/vision/\n\
    earth/lookingatearth/springagu_2006.html (Accessed 4 October \n2006).\nCASTRO-ESAU\
    \ KL, SANCHEZ-AZOFEIFA GA and CAELLI T \n(2004) Discrimination of lianas and trees\
    \ with leaf level hyperspec-\ntral data. Remote  Sens.  Environ. 90 353-372. \n\
    CECCATO P, FLASSE S, TARANTOLA S, JACQUEMOUD S and \nGREGOIRE JM (2001) Detecting\
    \ vegetation leaf water content \nusing reflectance in the optical domain. Remote\
    \  Sens. Environ. 77 \n22-33.\nCHAMPAGNE CM, STAENZ K, BANNARI A, MCNAIRN H and\
    \ \nDEGUISE JC (2003) Validation of a hyperspectral curve fitting \nmodel for\
    \ the estimation of water content of agricultural canopies. \nRemote Sens. Environ.\
    \ 87 148-160.\nCHUN-JIANG Z, JI-HUA W, LIANG-YUN L, WEN-JIANG H and \nQI-FA Z\
    \ (2006) Relationship of 2100-2300nm spectral characteris-\ntics of wheat canopy\
    \ to leaf area index and leaf N as affected by leaf \nwater content. Pedosphere\
    \ 16 333-338.\nCOOPS N, DURY S, SMITH ML, MARTIN M and OLLINGER S \n(2002) Comparison\
    \ of green leaf eucalypt spectra using spectral \ndecomposition. Austral. J. Bot.\
    \ 50 567-576.\nDATT B (1998) Remote sensing of chlorophyll a, chlorophyll b, chloro-\n\
    phyll a+b, and total carotenoid content in eucalyptus leaves. Remote \nSens. Environ\
    \ 66 111-121.\nD’URSO GD, DINI L, VUOLO F, ALONSO L and GUANTER L (2004) \nRetrieval\
    \ of leaf area index by inverting hyperspectral multiangular \nCHRIS PROBA data\
    \ from SPARC 2003. Proc. 2nd  CHRIS Proba \nWorkshop. 28 to 30 April, ESA/ESRIN,\
    \ Frascati Italy. \nFELIPE IP, DOHM JM, BAKER VR, DOGGETT T, DAVIES AG, \nCASTANO\
    \ R, CHIEN S, CICHY B, GREELEY R, SHERWOOD \nR, TRAN D, RABIDEAU G (2006) Flood\
    \ detection and monitor-\ning with the autonomous sciencecraft experiment onboard\
    \ EO-1. \nRemote Sens. Environ. 101 463-481.\nEICHINGER WE, COOPER DI, HIPPS LE,\
    \ KUSTAS WP, NEALE \nCMU and PRUEGER JH (2006) Spatial and temporal variation\
    \ in \nevapotranspiration using Raman Lidar. Adv. Water Resour. 29 369-\n381.\n\
    FROIDEFOND J, GARDEL L, GUIRAL D, PARRA M and TERNON \nJ (2002) Spectral remote\
    \ sensing reflectances of coastal waters in \nFrench Guiana under the Amazon influence.\
    \ Remote Sens. Environ. \n80 225-232.\nGALVAO LS, FORMAGGIO, AR and TISOT DA (2005)\
    \ Discrimina-\ntion of sugarcane varieties in South Eastern Brazil with EO-1 Hype-\n\
    rion data. Remote Sens.  Environ. 94 523-534.\nGITELSON AA and MERZLYAK MN (1996)\
    \ Signature analysis of \nleaf reflectance spectra: algorithm development for\
    \ remote sensing \nof chlorophyll. J. Plant Physiol. 148 494-500.\nGITELSON AA,\
    \ ZUR Y, CHIVKUNOVA, OB and MERZLYAK MN \n(2002) Assesing carotenoid content in\
    \ plant leaves with reflectance \nspectroscopy. Photochem. Photobiol. 75 272-281.\n\
    GLABER C and REINARTZ, P (2004) Multitemporal and multispec-\ntral remote sensing\
    \ approach for flood detection in the Elbe-Mulde \nregion 2002. Acta Hydrochim.\
    \ Hydrobiol. 33 (5).\nGUPTA RK, VIJAYAN D and PRASAD TS (2003) Comparative analy-\n\
    sis of red-edge hyperspectral indices. Adv. Space Res. 32 2217-\n2222.\nHAKVOORT\
    \ H, DE HAAN J, JORDANS R, VOS R, PETERS S and \nRIJKEBOER M (2002) Towards airborne\
    \ remote sensing of water \nquality in the Netherlands-validation and error analysis.\
    \ J. Photogr. \nRemote Sens. 57 171-183.\nJENSEN, JR (1996) Introductory Digital\
    \ Image Processing: A Remote \nSensing Perspective. Prentice-Hall, New Jersey.\n\
    KITE G and DROOGERS P (2000) Comparing evapotranspiration esti-\nmates from satellites,\
    \ hydrological models and field data. J. Hydrol. \n229 3-18.\nKOCH M, INZANA J\
    \ and EL BAZ F (2005) Applications of hyperion \nhyperspectral and aster multispectral\
    \ data in characterizing vegeta-\ntion for water resource studies in arid lands.\
    \ Geol. Remote Sens. 16 to \n19 October 2005, Salt Lake City Annual Meeting. \
    \ Paper No. 44-5. \nKOPONEN S, PULLIAINEN J, KALLIO K and HALLIKAINEN M \n(2002)\
    \ Lake water quality classification with airborne hyperspectral \nspectrometer\
    \ and simulated MERIS data. Remote Sens. Environ. 79 \n51-59.\nKRUCE F, LEFKOFF\
    \ A, BOARDMAN J, HEIDEBRECHT K, SHA-\nPIRO A, BARLOON P and GOETZ A (1993) The\
    \ spectral image \nprocessing system (SIPS) interactive visualization and analysis\
    \ of \nimaging spectrometer data. Remote  Sens. Environ. 44 145-163.\nLANDGREBE\
    \ D (1999) On information extraction principles for \nhyperspectral data. Cybernetics\
    \  28 part c, 1, 1-7.\nLANDGREBE D (1999) Some fundamentals and methods for hyper-\n\
    spectral image data analysis. Systems and Technologies for Clinical \nDiagnostics\
    \ and Drug Discovery II, 3603. 6 pp.\nLILLESAND TM and KIEFER RW (1999) Remote\
    \ Sensing and Image \nInterpretation. John Wiley & Sons, Inc. New Jersey.\nLIN\
    \ Y and LIQUAN Z (2006) Identification of the spectral characteris-\ntics of submerged\
    \ plant Vallisneria spiralis. Acta Ecol. Sin. 26 1005-\n1011.\nLOUKAS A, VASILIADES\
    \ L, DOMENIKIOTIS C and DALEZIOS \nNR (2005) Basin wide actual evapotranspiration\
    \ using NOAA\x12\nBasin wide actual evapotranspiration using NOAA\x12\nAVHRR satellite\
    \ data. Phys. Chem. Earth 30 69-79.\nMCGWIRE K, MINOR T and FENSTERMAKER L (2000)\
    \ Hyper-\nspectral mixture modeling for quantifying sparse vegetation cover \n\
    in arid environments. Remote Sens. Environ. 72 360-374.\nMEDINA JL, CAMACHO E,\
    \ RECA J, LOPEZ R and ROLDAN J (1998) \nDetermination and analysis of regional\
    \ evapotranspiration in Spain \nbased on remote sensing and GIS. Phys. Chem. Earth\
    \ 23 427-432.\nMERZLYAK MN, SOLOVCHENKO AE and GITELSON, AA (2003) \nReflectance\
    \ spectral features and non-destructive estimation of \nchlorophyll, carotenoid\
    \ and anthocyanin content in apple fruit. Post-\nharvest Biol. Technol. 27 197-211.\
    \ \nNAGY Z and JUNG A (2005) A case study of the anthropogenic impact \non the\
    \ catchment of Mogyorod-brook, Hungary. Phys. Chem. Earth \n30 588-597.\nOKIN\
    \ GS, ROBERTS DA, MURRAY B, OKIN WJ (2001) Practical \nlimits on hyperspectral\
    \ vegetation discrimination in arid and semi-\narid environments. Remote Sens.\
    \ Environ. 77 212-225.\nPU R, GE S, KELLY NM and GONG P (2003) Spectral absorption\
    \ fea-\ntures as indicators of water status in coast live oak leaves. Int. J.\
    \ \nRemote Sens. 24 1799-1810.\nAvailable on website http://www.wrc.org.za\nISSN\
    \ 0378-4738 = Water SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA (on-line)\n\
    151\nROUX H and DARTUS D (2006) Use of parameter optimization to esti-\nmate a\
    \ flood wave: potential applications to remote sensing of rivers. \nJ. Hydrol.\
    \ 328 258-266.\nSCHLERF M, ATZBERGER C and HILL J (2005) Remote sensing of \n\
    forest biophysical variables using HyMap imaging spectrometer \ndata.Remote Sens.\
    \ Environ. 95 177-194.\nSCHMIDT KS and SKIDMORE AK (2003) Spectral discrimination\
    \  \nof vegetation types in a coastal wetland. Remote Sens. Environ. 85 \n92-108.\n\
    SMITH RB (2001a) Introduction to hyperspectral imaging. www.\nmicroimages.com\
    \  (Accessed 11/03/2006).\nSMITH RB (2001b) Introduction to remote sensing of\
    \ the environment. \nwww.microimages.com  (Accessed 24/03/2006).\nSTIMSON HC,\
    \ BRESHEARS DD, USTIN SL and KEFAUVER SC \n(2005) Spectral sensing of foliar water\
    \ conditions in two co-occur-\nring conifer species: Pinus edulis and Juniperus\
    \ monosperma. \nRemote Sens. Environ.  96 108-118.\nSTONE C, CHISHOLM L and COOPS\
    \ N (2001) Spectral reflectance \ncharacteristics of eucalypt foliage damaged\
    \ by insects. Aust. J. Bot. \n49 687-698.\nSTUMPF RP (2001) Applications of satellite\
    \ ocean color sensors for \nmonitoring and predicting harmful algal blooms. Hum.\
    \ Ecol. Risk \nAssess. 7 1363-1368. \nTHIEMANN S and KAUFMANN H (2002) Lake water\
    \ quality moni-\ntoring using hyperspectral airborn data – a semiempirical multisen-\n\
    sor and multitemporal approach for the Mecklenburg Lake District, \nGermany. Remote\
    \ Sens. Environ. 81 228-237.\nTILLEY DR, AHMED M, SON, JH and BADRINARAYANAN H\
    \ \n(2003) Hyperspectral reflectance of emergent macrophytes as an \nindicator\
    \ of water column ammonia in an oligohaline, subtropical \nmarsh. Eco. Eng. 21\
    \ 153-163.\nUNDERWOOD E, USTIN S and DIPIETRO D (2003) Mapping non-\nnative plants\
    \ using hyperspectral imagery. Remote Sens. Environ. \n86 150-161.\nVOS RJ, HAKVOORT\
    \ JHM, JORDANS, RWJ and IBELINGS BW \n(2003) Multiplatform optical monitoring\
    \ of eutrophication in tempo-\nrally and spatially variable lakes.  Sci. Total\
    \ Environ. 312 221-243.\nWANG K, LI Z and CRIBB M (2006) Estimation of evaporative\
    \ fraction \nfrom a combination of day and night land surface temperatures and\
    \ \nNDVI: A new method to determine the Priestly-Taylor parameter. \nRemote Sens.\
    \ Environ. 102 293-305.\nYAMANO H, CHEN J and TAMURA M (2003) Hyperspectral identifi-\n\
    cation of grassland vegetation in Xilinahot, Inner Mongolia, China. \nInt. J.\
    \ Remote Sens. 24 3171-3178.\nZAGOLSKI F, PINEL V, ROMIER J, ALCAYDE D, GASTELLU-\n\
    ETCHEGORRY JP, GIORDANO G, MARTY G, MOUGIN E and \nJOFFRE R (1996) Forest canopy\
    \ chemistry with high spectral reso-\nlution remote sensing. Int. J. Remote Sens.\
    \ 17 1107-1128.\n152\nAvailable on website http://www.wrc.org.za\nISSN 0378-4738\
    \ = Water SA Vol. 33 No. 2 April 2007\nISSN 1816-7950 = Water SA (on-line)\n"
  inline_citation: '>'
  journal: Water SA
  limitations: '>'
  pdf_link: https://www.ajol.info/index.php/wsa/article/download/49049/35397
  publication_year: 2009
  relevance_score1: 0
  relevance_score2: 0
  title: A review of hyperspectral remote sensing and its application in vegetation
    and water resource studies
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/rs11111373
  analysis: '>'
  authors:
  - Jaafar Abdulridha
  - Özgur Batuman
  - Yiannis Ampatzidis
  citation_count: 137
  full_citation: '>'
  full_text: ">\nremote sensing  \nArticle\nUAV-Based Remote Sensing Technique to\
    \ Detect\nCitrus Canker Disease Utilizing Hyperspectral\nImaging and Machine Learning\n\
    Jaafar Abdulridha 1, Ozgur Batuman 2 and Yiannis Ampatzidis 1,*\n1\nAgricultural\
    \ and Biological Engineering department, Southwest Florida Research and Education\
    \ Center,\nUniversity of Florida, IFAS, 2685 SR 29 North Immokalee, FL 34142,\
    \ USA; ftash@uﬂ.edu\n2\nDepartment of Plant Pathology, Southwest Florida Research\
    \ and Education Center, University of Florida,\nIFAS, 2685 SR 29 North Immokalee,\
    \ FL 34142, USA; obatuman@uﬂ.edu\n*\nCorrespondence: i.ampatzidis@uﬂ.edu; Tel.:\
    \ +01-239-658-3451\nReceived: 29 April 2019; Accepted: 6 June 2019; Published:\
    \ 8 June 2019\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\
    \nAbstract: A remote sensing technique was developed to detect citrus canker in\
    \ laboratory conditions\nand was veriﬁed in the grove by utilizing an unmanned\
    \ aerial vehicle (UAV). In the laboratory,\na hyperspectral (400–1000 nm) imaging\
    \ system was utilized for the detection of citrus canker in\nseveral disease development\
    \ stages (i.e., asymptomatic, early, and late symptoms) on Sugar Belle\nleaves\
    \ and immature (green) fruit by using two classiﬁcation methods: (i) radial basis\
    \ function (RBF)\nand (ii) K nearest neighbor (KNN). The same imaging system mounted\
    \ on an UAV was used to detect\ncitrus canker on tree canopies in the orchard.\
    \ The overall classiﬁcation accuracy of the RBF was\nhigher (94%, 96%, and 100%)\
    \ than the KNN method (94%, 95%, and 96%) for detecting canker in\nleaves. Among\
    \ the 31 studied vegetation indices, the water index (WI) and the Modiﬁed Chlorophyll\n\
    Absorption in Reﬂectance Index (ARI and TCARI 1) more accurately detected canker\
    \ in laboratory\nand in orchard conditions, respectively. Immature fruit was not\
    \ a reliable tissue for early detection of\ncanker. However, the proposed technique\
    \ successfully distinguished the late stage canker-infected\nfruit with 92% classiﬁcation\
    \ accuracy. The UAV-based technique achieved 100% classiﬁcation accuracy\nfor\
    \ identifying healthy and canker-infected trees.\nKeywords: citrus; canker; disease\
    \ detection; hyperspectral imaging; neural networks; vegetation\nindices\n1. Introduction\n\
    Citrus bacterial canker (CBC), caused by Xanthomonas citri subsp. citri (Xcc;\
    \ syn. X. axonopodis\npv. citri), is a serious disease of citrus worldwide [1,2].\
    \ Symptoms include necrotic, raised lesions\nwith yellow halos on fruit, leaves,\
    \ and twigs [3]. The bacterium is dispersed by wind and rain and\nprefers humid-wet\
    \ climates [1]. On severely infected trees, the pathogen can cause severe premature\n\
    leaf and fruit drop, twig dieback, blemished fruit, and tree decline, resulting\
    \ in signiﬁcant economic\nimpacts [3]. Late symptoms of this disease may appear\
    \ within only a few months from the infection.\nVisually, a plant may look healthy,\
    \ but in fact, the bacterial growth stages take a few months to\nshow symptoms.\
    \ The disease may spread widely in diﬀerent ways, including natural events such\n\
    as storms and hurricanes [4]. Contaminated equipment can also spread this disease\
    \ [5]. Another\ncommon way that this disease is spread is by transportation of\
    \ diseased or seemingly healthy trees\nto regions that were previously not infected.\
    \ CBC is treated with copper applications and is most\neﬀective when used on new\
    \ ﬂush before the infection. It is necessary to apply copper multiple times\n\
    in order to maintain control of CBC during the most susceptible stages of leaf\
    \ and fruit growth [6].\nThe asymptomatic stage of this disease is the critical\
    \ stage to treat a tree, either by using copper-based\nRemote Sens. 2019, 11,\
    \ 1373; doi:10.3390/rs11111373\nwww.mdpi.com/journal/remotesensing\nRemote Sens.\
    \ 2019, 11, 1373\n2 of 22\npesticide and/or bactericide. This, in turn, can prevent\
    \ the disease from spreading throughout the\nentire grove. Detection of Xcc in\
    \ plants relies on culturing and polymerase chain reaction (PCR)\ntechniques [7,8].\
    \ Although PCR diagnosis can be rapid, there is a risk for false negative results\
    \ due to\nthe presence of PCR inhibitors in the samples [6]. Culture-based diagnosis\
    \ is more reliable but can take\nup to a week to get results and requires samples\
    \ to be brought into the lab for diagnosis. Thus, an early\nand accurate disease\
    \ detection method, before the late symptoms appear, is needed.\nSeveral remote\
    \ sensing technologies and techniques (e.g., machine vision, hyperspectral imaging)\n\
    have been developed to scout agricultural ﬁelds and detect pests, diseases, and\
    \ disorders [9,10]. These\nsystems generate substantial datasets; the analysis\
    \ of these datasets require advanced algorithms\nand high computational power.\
    \ Machine learning methods provide a powerful tool to analyze such\nbig datasets\
    \ (e.g., hyperspectral data) for plant disease detection. For example, Ashourloo\
    \ et al. [11]\nutilized three machine learning methods (partial least square regression,\
    \ support vector regression, and\nGaussian process regression) to detect wheat\
    \ leaf rust disease with high classiﬁcation accuracy. Zhang\net al. [12] studied\
    \ two imaging techniques, hyperspectral and multispectral, to detect and distinguish\n\
    apples with wind damage, insect damage, bruises, decay, hail, russeting, spot,\
    \ scar, stem, and calyx.\nThe hyperspectral imaging technique achieved 93.6% accuracy\
    \ and the multispectral imaging obtained\n91.4% overall accuracy. Behmann et al.\
    \ [13] measured the hyperspectral reﬂectance of table grapes\n(seven cultivars)\
    \ to predict some physico-chemical (pH, total acidity, and solid soluble content)\
    \ and\nsensory indices. They obtained high correlation (80–94%) between hyperspectral\
    \ reﬂectance values\nand physicochemical indices. Abdulridha et al. [14] detected\
    \ laurel wilt disease with a six band\nmultispectral camera in order to diﬀerentiate\
    \ it from other biotic and abiotic stress factors such as N and\nFe eﬃciency,\
    \ and phytophthora root rot disease. The classiﬁcation results reached up to 100%\
    \ in some\ncases. Lu et al. [15] also detected anthracnose crown rot in strawberries\
    \ utilizing spectroradiometric\ntechnology in asymptomatic and late development\
    \ stage utilizing three classiﬁcation discriminant\nanalysis algorithms (ﬁsher\
    \ discriminant analysis, FDA; stepwise discriminant analysis, SDA; and\nK-nearest\
    \ neighbor algorithm, KNN). The classiﬁcation results were 71.3%, 70.5%, and 73.6%\
    \ for SDA,\nFDA, and KNN, respectively.\nUnmanned aerial vehicles (UAVs) have\
    \ recently been utilized in precision agriculture for weed\nand disease detection,\
    \ vegetation coverage detection and assessment, and nutrient status and growth\n\
    vigor assessment [16,17]. Zhang et al. [18] monitored the growth of turf grass\
    \ in diﬀerent trails by\nusing a UAV and several vegetation indices (e.g., Normalized\
    \ Diﬀerence Vegetation Index (NDVI),\nVisible Atmospherically Resistant Index\
    \ VARI). Albetis et al. [19] developed a UAV-based multispectral\ntechnique to\
    \ distinguish between two diseases with similar symptoms, Flavescence dorée and\
    \ grapevine\ntrunk disease, in vineyards. The infected (based on this detection\
    \ technique) plants were removed\nafter detection [20]. Several studies in remote\
    \ sensing recommended the use of UAVs for precision\nagricultural applications\
    \ [16,21–23].\nIn citrus, Qin et al. [24] developed a method to detect citrus\
    \ canker, greasy spots, insect damage,\nmelanose, scab, and wind scar on grapefruits\
    \ by analyzing wavelengths between 450 nm and 930 nm\nthrough hyperspectral imaging\
    \ with a classiﬁcation accuracy of 95% under indoor conditions. Similarly,\nWeng\
    \ et al. [25] successfully utilized a hyperspectral technology to determine Huanglongbing\
    \ (HLB)\nin the asymptomatic and symptomatic development stages. By comparing\
    \ the reﬂectance of these\nstages with other nutrient deﬁciency symptoms, they\
    \ were able to distinguish the diseased (early\nand late stages) from non-diseased\
    \ plants. Furthermore, they combined hyperspectral imaging with\ncarbohydrate\
    \ metabolic analysis for HLB detection in diﬀerent seasons and cultivars. The\
    \ results were\nmore than 94% accurate in all tested periods and seasons. Sharif\
    \ et al. [26] analyzed several citrus\ndiseases and infections using imaging techniques\
    \ to distinguish between anthracnose, black spot,\ncanker, scab, greening (HLB),\
    \ and melanose symptoms. They were able to detect various types of lesion\non\
    \ the citrus fruits and leaves. The classiﬁcation accuracy, using a multiclass\
    \ support vector machine\n(SVM), was 94%. Zhang et al. [27] achieved promising\
    \ results utilizing non-destructive methods and\nseveral classiﬁcation techniques\
    \ to detect citrus diseases and disorders on fruits. Using tree-type SVM\nRemote\
    \ Sens. 2019, 11, 1373\n3 of 22\nmodels, the classiﬁcation accuracies of healthy,\
    \ HLB, melanose, oil spot, wind scar, leaf miner, and rust\nmite were 98.4%, 90.8%,\
    \ 95.2%, 92.0%, 90.8%, 95.2%, and 96.8%, respectively. The proposed appropriate\n\
    waveband selection methods were, therefore, very eﬀective in extracting features\
    \ of citrus fruit with\nthese blemishes. Sankaran et al. [28] used a portable\
    \ spectrometer (visible-near-infrared) to detect\nHLB disease in citrus trees;\
    \ reﬂectance data was analyzed as ﬁrst and second derivatives, and the\noverall\
    \ classiﬁcation accuracy of the detection system was more than 90%. Similarly,\
    \ Mishra et al. [29]\nutilized visible-near infrared spectroscopy and three classiﬁcation\
    \ methods (k-nearest neighbor, logistic\nregression, and support vector machines)\
    \ to detect HLB in tree canopies with more than 90% accuracy.\nSankaran and Ehsani\
    \ [30] developed a technique to distinguish healthy citrus leaves from leaves\n\
    aﬀected with canker and HLB by using portable spectroscopy (visible-near infrared\
    \ and mid-infrared)\nand two classiﬁers (quadratic discriminant analysis and k-nearest\
    \ neighbor). They achieved more than\n90% classiﬁcation accuracy.\nNevertheless,\
    \ previous studies have all been focused on detecting canker disease on mature\n\
    (orange color) citrus fruit (grapefruit and orange) and mainly in laboratory conditions\
    \ [24,31]. The\nobjectives of this study were to (i) detect canker disease on\
    \ tangerine citrus Sugar Belle leaves and\nimmature (green) fruits in asymptomatic,\
    \ early, and late disease development stages in laboratory\nconditions and later\
    \ in the orchard (outdoors) by utilizing an UAV-based hyperspectral imaging\n\
    technique; and (ii) classify and select the optimal vegetation indices to detect\
    \ canker in the above\ndisease stages in both indoor (laboratory) and outdoor\
    \ (UAV-based technique) environments. To the\nbest of our knowledge, we are the\
    \ ﬁrst to develop a UAV-based technique to detect citrus canker in the\ngrove\
    \ and to identify citrus canker on leaves (indoors and outdoors) and on immature\
    \ (green) fruits.\n2. Materials and Methods\n2.1. Experimental Site and Sample\
    \ Collection\nTangerine Sugar Belle leaves and immature (green) fruits infected\
    \ with canker disease and healthy\nleaves and fruits were collected from an experimental\
    \ orchard at the University of Florida’s Southwest\nFlorida Research and Education\
    \ Center (SWFREC), Immokalee, Florida, USA, for laboratory assessment\non October\
    \ 2018. Four trees were selected, and 10 leaves were collected from each tree\
    \ in diﬀerent\ndisease severity stages including (i) asymptomatic stage (leaves\
    \ without visible symptom); (ii) early\nstage (symptoms appear as slightly raised,\
    \ small, blister-like chlorotic lesions); and (iii) late stage\n(lesions turn\
    \ tan and then brown, and the edges appear water-soaked and develop a yellow halo)\n\
    (Table 1 and Figure 1a–d). Immature (green) fruits were collected from the same\
    \ orchard in the\nasymptomatic stage (Figure 1f) and the late stage (Figure 1g).\
    \ The UAV data was collected in the\nsame ﬁeld (October 2018, between 10 a.m.\
    \ to 2 p.m. to minimize variations of light intensity during\nexperiments).\n\
    Table 1. Number of leaves collected for each category studied in this work.\n\
    Category\nNumber of Leaves\nThe Symptoms of Leaves\nHealthy leaves (from the greenhouse)\n\
    40\nNo symptoms\nAsymptomatic stage of canker disease\n40\nNo symptoms with a\
    \ yellow halo.\nEarly stage of canker disease\n40\nTiny lesion\nLate stage of\
    \ canker disease\n40\nLarge dark and brown lesion\nRemote Sens. 2019, 11, 1373\n\
    4 of 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n4 of 23 \n \n \nFigure 1.\
    \ Example of Sugar Belle leaves (upper panel) representing four categories: (a)\
    \ healthy; (b) \nasymptomatic; (c) early, and (d) leaves with late canker developmental\
    \ stages. Lower panel is \nexamples of Sugar Belle immature (green) fruits representing\
    \ three categories: (e) non-infected; (f) \nasymptomatic, and (g) fruits with\
    \ late canker developmental stages. \n2.2. Bacterium Isolation \nA plant pathologist\
    \ carefully collected the leaves and fruits from citrus plants in grove, and to\
    \ \nconfirm Xanthomonas axonopodis bacterial infection, a polymerase chain reaction\
    \ (PCR) analysis \nwas conducted on the selected leaves and fruits. Healthy leaves\
    \ and fruits were obtained from potted \nplants grown in greenhouse conditions.\
    \  \nTissue was excised from leaf and fruit samples with or without CBC lesions\
    \ (Figure 1). Early and \nlate CBC lesions were compared to asymptomatic tissue.\
    \ Healthy leaves and fruits from citrus plants \ngrown in greenhouse conditions\
    \ were used as negative control. The citrus tissue was then macerated \nin 100\
    \ µL of nuclease-free water to release bacterial cells from tissue. This homogenate\
    \ was used in \nPCR assays. \n2.3. Polymerase Chain Reaction (PCR) Analysis \n\
    XACF/XACR primer pairs [7] were used to test for Xcc, and primer pairs 27F/1492R\
    \ [32] were \nused as a control to test for bacteria. All amplifications were\
    \ carried out in a final volume of 25 µL \ncontaining 12.5 µL of DreamTaq Green\
    \ PCR Master Mix (ThermoFisher, Waltham, MA), 1 µL of each \nprimer (Table 1),\
    \ 9.5 µL of nuclease free water, and 1 µL of the macerated tissue/water mix. Reactions\
    \ \nwere run for an initial denaturation step of 5 min at 95˚C followed by 35\
    \ cycles of 15 s at 95˚C, 30 s at \n60˚C, and 30 s at 72˚C, and final extension\
    \ of 7 min at 72˚C. Eight microliters of the PCR reaction was \nFigure 1. Example\
    \ of Sugar Belle leaves (upper panel) representing four categories: (a) healthy;\
    \ (b)\nasymptomatic; (c) early, and (d) leaves with late canker developmental\
    \ stages. Lower panel is examples\nof Sugar Belle immature (green) fruits representing\
    \ three categories: (e) non-infected; (f) asymptomatic,\nand (g) fruits with late\
    \ canker developmental stages.\n2.2. Bacterium Isolation\nA plant pathologist\
    \ carefully collected the leaves and fruits from citrus plants in grove, and to\n\
    conﬁrm Xanthomonas axonopodis bacterial infection, a polymerase chain reaction\
    \ (PCR) analysis was\nconducted on the selected leaves and fruits. Healthy leaves\
    \ and fruits were obtained from potted\nplants grown in greenhouse conditions.\n\
    Tissue was excised from leaf and fruit samples with or without CBC lesions (Figure\
    \ 1). Early and\nlate CBC lesions were compared to asymptomatic tissue. Healthy\
    \ leaves and fruits from citrus plants\ngrown in greenhouse conditions were used\
    \ as negative control. The citrus tissue was then macerated\nin 100 µL of nuclease-free\
    \ water to release bacterial cells from tissue. This homogenate was used in\n\
    PCR assays.\n2.3. Polymerase Chain Reaction (PCR) Analysis\nXACF/XACR primer pairs\
    \ [7] were used to test for Xcc, and primer pairs 27F/1492R [32] were\nused as\
    \ a control to test for bacteria. All ampliﬁcations were carried out in a ﬁnal\
    \ volume of 25 µL\ncontaining 12.5 µL of DreamTaq Green PCR Master Mix (ThermoFisher,\
    \ Waltham, MA), 1 µL of each\nprimer (Table 1), 9.5 µL of nuclease free water,\
    \ and 1 µL of the macerated tissue/water mix. Reactions\nwere run for an initial\
    \ denaturation step of 5 min at 95 ◦C followed by 35 cycles of 15 s at 95 ◦C,\
    \ 30 s at\nRemote Sens. 2019, 11, 1373\n5 of 22\n60 ◦C, and 30 s at 72 ◦C, and\
    \ ﬁnal extension of 7 min at 72 ◦C. Eight microliters of the PCR reaction\nwas\
    \ separated out on 1.0% agarose gel, stained with Apex Safe DNA Gel Stain (Genesee\
    \ Scientiﬁc,\nSan Diego, CA), and visualized on a UV gel imager. Samples with\
    \ a 561 bp-band was considered a\nCBC positive sample. Selected CBC-positive bands\
    \ were excised from the gel and sequenced at MC\nLab (San Francisco, CA) for conﬁrmation.\n\
    2.4. Indoor Hyperspectral Data Collection\nHyperspectral data was collected by\
    \ a benchtop imaging system comprised of a Pika L 2.4 camera,\na mounting tower,\
    \ lights, a scanning stage, a power supply, and a SpectrononPro control software\n\
    (Spectronon Pro, Resonon, Bozeman, MT) (Figure 2). The same hyperspectral camera\
    \ was used indoors\nand outdoors after replacing lenses to cover a 400–1000 nm\
    \ spectral range. Resonon’s hyperspectral\nimagers (RHI) are line-scan imagers\
    \ (also referred to as push-broom imagers). Two-dimensional\nimages are created\
    \ by gathering the image line by line while the sample is interpreting relative\
    \ to the\ncamera. This is typically accomplished by placing the sample on a linear\
    \ translation stage. Focus and\ncalibration sheets were used to set the stage\
    \ speed and imager frame rate. White panels and dark lines\nwith black covers\
    \ were utilized for the calibration. The white panel used in laboratory conditions\n\
    was made of polyethylene plastic (type 822; Spectronon Pro, Resonon, Bozeman,\
    \ MT). The regions of\ninterest (RoIs) were chosen manually by randomly selecting\
    \ six spectral scans from each leaf and fruit\nto avoid any bias (Figure 3).\n\
    \ \nseparated out on 1.0% agarose gel, stained with Apex Safe DNA Gel Stain (Genesee\
    \ Scientific, San \nDiego, CA), and visualized on a UV gel imager. Samples with\
    \ a 561 bp-band was considered a CBC \npositive sample. Selected CBC-positive\
    \ bands were excised from the gel and sequenced at MC Lab \n(San Francisco, CA)\
    \ for confirmation.  \nTable 1. Number of leaves collected for each category studied\
    \ in this work. \nCategory \nNumber of Leaves \nThe Symptoms of Leaves \nHealthy\
    \ leaves (from the greenhouse) \n40 \nNo symptoms \nAsymptomatic stage of canker\
    \ disease \n40 \nNo symptoms with a yellow halo. \nEarly stage of canker disease\
    \  \n40 \nTiny lesion  \nLate stage of canker disease  \n40 \nLarge dark and brown\
    \ lesion  \n2.4. Indoor Hyperspectral Data Collection  \nHyperspectral data was\
    \ collected by a benchtop imaging system comprised of a Pika L 2.4 \ncamera, a\
    \ mounting tower, lights, a scanning stage, a power supply, and a SpectrononPro\
    \ control \nsoftware (Spectronon Pro, Resonon, Bozeman, MT) (Figure 2). The same\
    \ hyperspectral camera was \nused indoors and outdoors after replacing lenses\
    \ to cover a 400–1000 nm spectral range. Resonon’s \nhyperspectral imagers (RHI)\
    \ are line-scan imagers (also referred to as push-broom imagers). Two-\ndimensional\
    \ images are created by gathering the image line by line while the sample is interpreting\
    \ \nrelative to the camera. This is typically accomplished by placing the sample\
    \ on a linear translation \nstage. Focus and calibration sheets were used to set\
    \ the stage speed and imager frame rate. White \npanels and dark lines with black\
    \ covers were utilized for the calibration. The white panel used in \nlaboratory\
    \ conditions was made of polyethylene plastic (type 822; Spectronon Pro, Resonon,\
    \ \nBozeman, MT). The regions of interest (RoIs) were chosen manually by randomly\
    \ selecting six \nspectral scans from each leaf and fruit to avoid any bias (Figure\
    \ 3). \n \nFigure 2. Hyperspectral Resonon Pika L imaging system that was used\
    \ for indoor data collection with \nall components shown including tower, lights,\
    \ stage, software, and a leaf sample. \nFigure 2. Hyperspectral Resonon Pika L\
    \ imaging system that was used for indoor data collection with\nall components\
    \ shown including tower, lights, stage, software, and a leaf sample.\nRemote Sens.\
    \ 2019, 11, 1373\n6 of 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n6 of 23\
    \ \n \n \nFigure 3. The indoor data collection system that was used for regions\
    \ of interest (RoI) collection from \ncitrus canker-infected Sugar Belle tangerine\
    \ tissue samples. (a) Selection of RoIs on leaves and (b) \nimmature fruits are\
    \ indicated with white circles. Each sample (leaf or fruit) generates six spectral\
    \ \nsignatures by six randomly selected RoIs. (c) Leaf images showing canker symptoms\
    \ on leaves with \na false color of leaves with lesions in an infrared, blue (450–485\
    \ nm), green (500–565 nm), and red \n(625–740 nm) wavelengths. \n2.5. Outdoor\
    \ Hyperspectral Data Collection  \nHyperspectral data was collected by using a\
    \ UAV (DJI Matrice 600, Pro Hexacopter) and the \nsame hyperspectral camera, Resonon\
    \ Pika L 2.4 (Figure 4a), as in the indoor procedure. The UAV-\nbased imaging\
    \ system includes (i) a Resonon Pika L 2.4 hyperspectral camera (Spectronon Pro,\
    \ \nResonon, Bozeman, MT); (ii) visible-near infrared (V-NIR) objective lenses\
    \ for the Pika L camera with \na focal length of 23 mm, field of view (FOV) of\
    \ 13.1 degrees, and instantaneous field of view (IFOV) \nof 0.52 mrad; and (iii)\
    \ a global positioning system (GPS) and the inertial measurement unit IMU (DJI)\
    \ \nflight control system for multi-rotor aircraft, to record sensor position\
    \ and orientation. Data was \ncollected at 30 m above the ground with a speed\
    \ of 1.5 m/h. The region of interest was chosen \nrandomly from the developed\
    \ 2D maps, and the files were saved in KML format. The positions of the \ninfected\
    \ trees were known (leaves were collected and identified by PCR). \nThe maps and\
    \ images were analyzed by the Spectronon software after hyperspectral data were\
    \ \nacquired. Calibration corrections were performed using Resonon hyperspectral\
    \ data analysis \nsoftware (Spectronon Pro, Resonon, Bozeman, MT). Georectification\
    \ and radiometric correction \nplugins, from the Spectronon Pro software, were\
    \ used to correct the GPS/IMU and the radiometric \ndata, respectively. The regions\
    \ of interest were selected manually (and randomly) for each tree, and \n20 spectral\
    \ scans were performed to ensure that the entire canopy was covered spectrally\
    \ (Figure 4). \nThe regions of interest were then exported as a text file and\
    \ processed using SPSS software (SPSS \n13.0, Inc., Chicago; Microsoft Corp.,\
    \ Redmond, WA). Pixel-based reflectance data was mixed for each \nclass.  \nFigure\
    \ 3. The indoor data collection system that was used for regions of interest (RoI)\
    \ collection from\ncitrus canker-infected Sugar Belle tangerine tissue samples.\
    \ (a) Selection of RoIs on leaves and (b)\nimmature fruits are indicated with\
    \ white circles. Each sample (leaf or fruit) generates six spectral\nsignatures\
    \ by six randomly selected RoIs. (c) Leaf images showing canker symptoms on leaves\
    \ with\na false color of leaves with lesions in an infrared, blue (450–485 nm),\
    \ green (500–565 nm), and red\n(625–740 nm) wavelengths.\n2.5. Outdoor Hyperspectral\
    \ Data Collection\nHyperspectral data was collected by using a UAV (DJI Matrice\
    \ 600, Pro Hexacopter) and the same\nhyperspectral camera, Resonon Pika L 2.4\
    \ (Figure 4a), as in the indoor procedure. The UAV-based\nimaging system includes\
    \ (i) a Resonon Pika L 2.4 hyperspectral camera (Spectronon Pro, Resonon,\nBozeman,\
    \ MT); (ii) visible-near infrared (V-NIR) objective lenses for the Pika L camera\
    \ with a focal\nlength of 23 mm, ﬁeld of view (FOV) of 13.1 degrees, and instantaneous\
    \ ﬁeld of view (IFOV) of\n0.52 mrad; and (iii) a global positioning system (GPS)\
    \ and the inertial measurement unit IMU (DJI)\nﬂight control system for multi-rotor\
    \ aircraft, to record sensor position and orientation. Data was\ncollected at\
    \ 30 m above the ground with a speed of 1.5 m/h. The region of interest was chosen\
    \ randomly\nfrom the developed 2D maps, and the ﬁles were saved in KML format.\
    \ The positions of the infected\ntrees were known (leaves were collected and identiﬁed\
    \ by PCR).\nThe maps and images were analyzed by the Spectronon software after\
    \ hyperspectral data were\nacquired. Calibration corrections were performed using\
    \ Resonon hyperspectral data analysis software\n(Spectronon Pro, Resonon, Bozeman,\
    \ MT). Georectiﬁcation and radiometric correction plugins, from\nthe Spectronon\
    \ Pro software, were used to correct the GPS/IMU and the radiometric data, respectively.\n\
    The regions of interest were selected manually (and randomly) for each tree, and\
    \ 20 spectral scans\nwere performed to ensure that the entire canopy was covered\
    \ spectrally (Figure 4). The regions of\ninterest were then exported as a text\
    \ ﬁle and processed using SPSS software (SPSS 13.0, Inc., Chicago;\nMicrosoft\
    \ Corp., Redmond, WA). Pixel-based reﬂectance data was mixed for each class.\n\
    Remote Sens. 2019, 11, 1373\n7 of 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW\
    \ \n7 of 23 \n \n \nFigure 4. The unmanned aerial vehicle (UAV)-based data collection\
    \ system that was used for regions \nof interest (RoI) collection from citrus\
    \ canker-infected Sugar Belle tangerine trees in the orchard. (a) \nUAV-based\
    \ data collection system; (b) random selection of twenty RoIs from citrus trees\
    \ infected with \ncitrus canker. \n2.6. Data Analysis  \nData obtained from the\
    \ Resonon imaging system can be categorized into three types as described \nbelow.\
    \ \n2.6.1. Raw data \nAfter capturing the images, the raw data was preprocessed\
    \ to eliminate the noise of the \ninstrument, illumination, and model reflectivity.\
    \ For this, the raw data was calibrated by an imager \ncalibration pack (ICP)\
    \ with the Radiance Conversion plugin in Spectronon software. \n2.6.2. Reflectance\
    \ \nDuring airborne (i.e., UAV) data collection, a gray panel, made from highly\
    \ durable woven \npolyester fabric (type 822; Spectronon Pro, Resonon, Bozeman,\
    \ MT) (Figure 4a–b), was utilized to \ncalibrate the collected data. The reﬂection\
    \ values are only accurate when the solar illumination \n(clouds, sun angle, etc.)\
    \ does not change during the data collection. To achieve this consistency, we\
    \ \nonly collected data in a 10 a.m. to 2 p.m. window of the day during our experiments.\
    \ Data were \nconverted from radiance to reﬂectance using the Spectronon Reﬂectance\
    \ Conversion from the \nSpectrally Flat Reference Cube plugin software.  \n2.6.3.\
    \ Data analysis procedure  \nThe Resonon Pika L imaging system was utilized in\
    \ both outdoor (airborne) and indoor \n(benchtop) systems. In the benchtop system,\
    \ images were analyzed using the Spectronon Pro \nsoftware. Figure 3 presents\
    \ an example of the selection of RoIs for leaves (Figure 3a) and fruit (Figure\
    \ \n3b). This procedure generates six spectral signatures for each leaf and fruit.\
    \ Figure 3c shows the false \ncolor of leaves with lesions in an infrared, blue\
    \ (450–485 nm), green (500–565 nm), and red (625–740 \nnm) leaf image. Figure\
    \ 4 presents an example of the RoIs for the UAV-based system (Figure 4b) and \n\
    their spectral signatures.  \n2.7. Vegetation Indices \nFigure 4. The unmanned\
    \ aerial vehicle (UAV)-based data collection system that was used for regions\n\
    of interest (RoI) collection from citrus canker-infected Sugar Belle tangerine\
    \ trees in the orchard.\n(a) UAV-based data collection system; (b) random selection\
    \ of twenty RoIs from citrus trees infected\nwith citrus canker.\n2.6. Data Analysis\n\
    Data obtained from the Resonon imaging system can be categorized into three types\
    \ as\ndescribed below.\n2.6.1. Raw data\nAfter capturing the images, the raw data\
    \ was preprocessed to eliminate the noise of the instrument,\nillumination, and\
    \ model reﬂectivity. For this, the raw data was calibrated by an imager calibration\n\
    pack (ICP) with the Radiance Conversion plugin in Spectronon software.\n2.6.2.\
    \ Reﬂectance\nDuring airborne (i.e., UAV) data collection, a gray panel, made\
    \ from highly durable woven\npolyester fabric (type 822; Spectronon Pro, Resonon,\
    \ Bozeman, MT) (Figure 4a–b), was utilized to\ncalibrate the collected data. The\
    \ reﬂection values are only accurate when the solar illumination (clouds,\nsun\
    \ angle, etc.) does not change during the data collection. To achieve this consistency,\
    \ we only\ncollected data in a 10 a.m. to 2 p.m. window of the day during our\
    \ experiments. Data were converted\nfrom radiance to reﬂectance using the Spectronon\
    \ Reﬂectance Conversion from the Spectrally Flat\nReference Cube plugin software.\n\
    2.6.3. Data analysis procedure\nThe Resonon Pika L imaging system was utilized\
    \ in both outdoor (airborne) and indoor (benchtop)\nsystems. In the benchtop system,\
    \ images were analyzed using the Spectronon Pro software. Figure 3\npresents an\
    \ example of the selection of RoIs for leaves (Figure 3a) and fruit (Figure 3b).\
    \ This procedure\ngenerates six spectral signatures for each leaf and fruit. Figure\
    \ 3c shows the false color of leaves with\nlesions in an infrared, blue (450–485\
    \ nm), green (500–565 nm), and red (625–740 nm) leaf image. Figure 4\npresents\
    \ an example of the RoIs for the UAV-based system (Figure 4b) and their spectral\
    \ signatures.\nRemote Sens. 2019, 11, 1373\n8 of 22\n2.7. Vegetation Indices\n\
    Based on literature review, the most common factors that can be estimated using\
    \ the visible\nand near-infrared channels are chlorophyll content, water stress,\
    \ and cell structure [33,34]. In this\nstudy, 31 vegetation indices (Vis) were\
    \ utilized and evaluated to detect citrus canker (Table 2). These\nindices were\
    \ chosen to identify citrus canker disease based on chlorophyll reduction and\
    \ leaf cell\ndamage caused by bacteria. The spectral reﬂectance was collected\
    \ in diﬀerent stages of disease\nsymptom development (asymptomatic, early, and\
    \ late stage), and healthy leaves were used as a control.\nVIs measurements will\
    \ vary from one stage to another based on severity of the disease (i.e., size\
    \ and\nnumber of lesion).\nTable 2. List of vegetation indices (VIs) utilized\
    \ to detect citrus canker in this study\nVegetation Indexes\nEquation\nReferences\n\
    Water Index (WI)\nWI = R900\nR970\nPenuelas et al. [33]\nModiﬁed Chlorophyll Absorption\
    \ in\nReﬂectance Index (mCARI 1)\nmCARI 1 = 1.2[(2.5*R761 − R651) − 1.3(R761 −\
    \ R581)]\nHaboudane et al. [34]\nModiﬁed Triangular Vegetation Index1\n(MTVI 1)\n\
    MTVI 1 = 1.2[1.2(1.2(R761 − R581) − 2.5(R651 − R581)]\nHaboudane et al. [34]\n\
    Modiﬁed Triangular Vegetation Index2\n(MTVI 2)\nMTVI 2 =\n1.5[1.2(R761−R581)−2.5(R651−R581)]\n\
    SQ[(2∗R761+1)ˆ2−(6∗R761−5∗SQ(R651)−0.5]\nHaboudane et al. [34]\nSimple Ratio Index\
    \ (SR 900)\nSR 900 = R900\nR651\nJordan [35]\nSimple Ratio Index (SR 850)\nSR\
    \ 850 = R850\nR651\nJordan [35]\nGreen NDVI (GNDVI)\nGNDVI = (NIR850−G580)\n(NIR850+G580)\n\
    Gitelson and Merzlyak [36]\nPhotochemical Reﬂectance Index (PRI)\nPRI = (R531−R570)\n\
    (R531+R570)\nGamon et al. [37]\nRatio Analysis of reﬂectance Spectral\nChlorophyll\
    \ a (RARSa)\nRARSa = R675\nR700\nChappelle et al. [38]\nRatio Analysis of reﬂectance\
    \ Spectral\nChlorophyll b (RARSb)\nRARSb =\nR675\n(R700×R650)\nChappelle et al.\
    \ [38]\nRatio analysis of reﬂectance spectra (RARSc)\nRARSc = R760\nR500\nChappelle\
    \ et al. [38]\nPigment speciﬁc simple ratio (PSSRa)\nPSSRa = R800\nR680\nBlackburn\
    \ [39]\nNormalized diﬀerence vegetation index 780\n(NDVI 780)\nNDVI 780 = R780−R670\n\
    R780+R670\nRaun et al. [40]\nStructure Insensitive Pigment Index (SIPI)\nSIPI\
    \ = (R840−R450)\n(R840−R670)\nPenuelas et al. [41]\nNormalized chlorophyll pigment\
    \ ratio index\n(NCPI)\nNCPI = (R670−R450)\n(R670−R450)\nPenuelas et al. [42]\n\
    Normalized phaeophytinization index\n(NPQI)\nNPQI = (R415−R435)\n(R415−R435)\n\
    Barnes et al. [43]\nPlant Senescence Reﬂectance Index (PSRI)\nPSRI = (R660−R510)\n\
    (R760)\nPenuelas et al. [44]\nThe ratio of WI and ND (WI/ND)\nWI/ND = R750\nR600\n\
    Hunt et al. [45]\nTransform chlorophyll absorption in\nreﬂectance index (TCARI)\n\
    TCARI = 3[(R740 − R651) − 0.2(R740 −\nR581)(R740/R651)]\nHaboudane et al. [46]\n\
    Green Vegetation (VIGreen)\nVIGreen = (R760−R651)\n(R760+R651)\nGitelson et al.\
    \ [47]\nRed-Edge Vegetation Stress Index 1 (RVS 1)\nRVS 1 =\n\x14 (R651+Red Edge\
    \ 750)\n2\n\x15\n− Red Edge 733\nMerton [48]\nRed-Edge Vegetation Stress Index\
    \ 2 (RVS 2)\nRVS 2 =\n\x14 (R651+Red Edge 750)\n2\n\x15\n− Red Edge 751\nMerton\
    \ [48]\nTriangle Vegetation Index (TVI)\nTVI = 0.5[120*(R761 − R581) − 200(R651\
    \ − R581)]\nBroge [49]\nRenormalized Diﬀerence Vegetation Index\n(RDVI)\nRDVI\
    \ =\n(R761−R651)\nSQ(R761+R651)\nRoujean et al. [50]\nNormalized diﬀerence vegetation\
    \ index 850\n(NDVI 850)\nNDVI 850 = (R850−R651)\n(R850+R651)\nRaun et al. [40]\n\
    Simple Ratio Index (SR 761)\nSR 761 = R761\nR651\nJordan [35]\nNormalized diﬀerence\
    \ vegetation index 761\n(NDV 761)\nNDVI 761 = (R761−R651)\n(R761+R651)\nRaun [40]\n\
    Plant Pigment ratio (PPR)\nPPR = (R550−R450)\n(R550+R450)\nMetternicht [51]\n\
    Water Stress and Canopy Temperature\n(NWI 2)\nNWI 2 = R970−R850\nR970+R850\nBabar\
    \ et al. [52]\nNitrogen Reﬂectance Index (NRI)\nNRI = (R570−R670)\n(R570+R670)\n\
    Bausch and Duke [53]\nAnthocyanin Reﬂectance Index (ARI)\nARI =\n\x10\n1\nR550\n\
    \x11\n−\n\x10\n1\nR700\n\x11\nGitelson [54]\n2.8. Spectral Data Classiﬁcation\
    \ Methods\nFor the classiﬁcation analysis, SPSS software was utilized to analyze\
    \ and classify the spectral\ndata of the four categories of leaves: (i) healthy,\
    \ (ii) asymptomatic, (iii) early, and (iv) late disease\ndevelopment stages. The\
    \ classiﬁcation of immature fruit was conducted between three categories:\nRemote\
    \ Sens. 2019, 11, 1373\n9 of 22\n(a) healthy, (b) asymptomatic, and (c) late disease\
    \ development stages. Two classiﬁcation methods were\nutilized to analyze the\
    \ collected data and classify the above categories: (I) the neural network Radial\n\
    Basis Function (RBF); and (II) the K-nearest neighbor (KNN). These methods were\
    \ chosen because of\ntheir high classiﬁcation accuracy in similar studies [15,55].\n\
    2.8.1. Neural Network Radial Basis Function (RBF)\nRBF is an artiﬁcial neural\
    \ network that performs supervised machine learning. It is a more\ncomplicated\
    \ process than a simple linear classiﬁer, and it can analyze a substantial amount\
    \ of data.\nRBF is considered an excellent function classiﬁer for spectral reﬂectance\
    \ data [56]. Generally, in neural\nnetworks, the back-propagation technique is\
    \ utilized to adjust the weights of the network to improve\nclassiﬁcation accuracy\
    \ [57]. In this study, the full dataset was randomly split into three datasets\n\
    including 70% training, 20% testing, and 10% hold out, based on [58]. The input\
    \ layers were healthy,\nasymptomatic, and early and late disease development stages.\
    \ A holdout cross-validation method was\nutilized to validate the results. Cross-validation\
    \ classiﬁes all variables.\n2.8.2. K-Nearest Neighbor (KNN)\nKNN is a widely used\
    \ machine learning algorithm that works well on simple recognition problems\n\
    in supervised learning environments [59]. It is one of the most straightforward\
    \ classiﬁcation algorithms,\nand it can be used for classiﬁcation and regression\
    \ predictive problems providing highly competitive\nresults. In KNN, each neighbor\
    \ is assigned with a contribution weight so that the nearer neighbors\ncontribute\
    \ more than the distant ones to the average. The “K” in KNN denotes the number\
    \ of the\nnearest neighbors used in the classiﬁcation. In general, KNN is easy\
    \ to use and works well with little\nprior knowledge of the data distribution\
    \ [60]. Euclidean distance strategy is the most common method\nto calculate the\
    \ variations between samples characterized as vector inputs [61]. In this study,\
    \ the\ntraining data set was 70%, and 30% was used as testing data.\n3. Results\n\
    3.1. Indoor Imaging Technique\nThe indoor imaging technique that was used in this\
    \ study allowed us to determine spectral\nsignatures of leaves with each disease\
    \ development stage in laboratory conditions (Figure 5).\nFigure 5 shows the spectral\
    \ signatures of the four categories—healthy, asymptomatic, early, and\nlate disease\
    \ development stages—for leaves in laboratory conditions. The healthy leaves presented\n\
    a higher reﬂectance in the green bands (548 nm) with a peak of 23% reﬂectance.\
    \ In the same band,\nthe asymptomatic stage produced a 19% peak and the early\
    \ symptomatic stage a peak of 17%.\nThe reﬂectance curves of those three stages\
    \ were similar. In the red bands, later stages showed a higher\nreﬂectance (Figure\
    \ 5).\nThe classiﬁcation rates of healthy and canker-infected citrus tissues (leaves\
    \ and fruits) were\ndetermined (Figure 6). All classiﬁcation rates were high in\
    \ all categories (between healthy and\nnon-healthy leaves and fruit), with the\
    \ exception of the classiﬁcation between healthy and asymptomatic\nimmature fruit\
    \ (Figure 6b). The RBF classiﬁcations have higher classiﬁcations than the KNN\
    \ in\nall categories (Figure 6). The most critical classiﬁcation determined was\
    \ between a healthy and\nan asymptomatic leaf, which achieved 96% and 94% accuracy\
    \ with the RBF and KNN methods,\nrespectively. These classiﬁcation accuracies\
    \ were considered high enough to distinguish between\nhealthy and asymptomatic\
    \ stages.\nRemote Sens. 2019, 11, 1373\n10 of 22\nRemote Sens. 2019, 11, x FOR\
    \ PEER REVIEW \n11 of 23 \n \n \nFigure 5. The spectral signatures of the four\
    \ categories—healthy, asymptomatic, early, and late \ndisease development stages—for\
    \ leaves in laboratory conditions. Spectral reflectance signatures \nproduced\
    \ by the indoor leaf analysis are shown with different colored lines. \nThe classification\
    \ rates of healthy and canker-infected citrus tissues (leaves and fruits) were\
    \ \ndetermined (Figure 6). All classification rates were high in all categories\
    \ (between healthy and non-\nhealthy leaves and fruit), with the exception of\
    \ the classification between healthy and asymptomatic \nimmature fruit (Figure\
    \ 6b). The RBF classifications have higher classifications than the KNN in all\
    \ \ncategories (Figure 6). The most critical classification determined was between\
    \ a healthy and an \nasymptomatic leaf, which achieved 96% and 94% accuracy with\
    \ the RBF and KNN methods, \nrespectively. These classification accuracies were\
    \ considered high enough to distinguish between \nhealthy and asymptomatic stages.\
    \  \n \n(a) \nFigure 5. The spectral signatures of the four categories—healthy,\
    \ asymptomatic, early, and late disease\ndevelopment stages—for leaves in laboratory\
    \ conditions. Spectral reﬂectance signatures produced by\nthe indoor leaf analysis\
    \ are shown with diﬀerent colored lines.\n \n \nFigure 5. The spectral signatures\
    \ of the four categories—healthy, asymptomatic, early, and late \ndisease development\
    \ stages—for leaves in laboratory conditions. Spectral reflectance signatures\
    \ \nproduced by the indoor leaf analysis are shown with different colored lines.\
    \ \nThe classification rates of healthy and canker-infected citrus tissues (leaves\
    \ and fruits) were \ndetermined (Figure 6). All classification rates were high\
    \ in all categories (between healthy and non-\nhealthy leaves and fruit), with\
    \ the exception of the classification between healthy and asymptomatic \nimmature\
    \ fruit (Figure 6b). The RBF classifications have higher classifications than\
    \ the KNN in all \ncategories (Figure 6). The most critical classification determined\
    \ was between a healthy and an \nasymptomatic leaf, which achieved 96% and 94%\
    \ accuracy with the RBF and KNN methods, \nrespectively. These classification\
    \ accuracies were considered high enough to distinguish between \nhealthy and\
    \ asymptomatic stages.  \n \n(a) \nFigure 6. Cont.\nRemote Sens. 2019, 11, 1373\n\
    11 of 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n12 of 23 \n \n \n(b) \nFigure\
    \ 6. The classification rates of healthy and canker-infected citrus tissues (leaves\
    \ and fruits) \ndetermined by the hyperspectral classification methods (radial\
    \ basis function, RBF; and K-nearest \nneighbor, KNN) between different canker\
    \ disease stages (healthy, asymptomatic, early, and late \nstage) for (a) leaves\
    \ (indoor and outdoor) and (b) immature (green) fruits (indoor). \nThe spectral\
    \ structure of immature (green) fruit in a healthy stage and in two disease \n\
    developmental stages (asymptomatic and late stage) are presented in Figure 7.\
    \ The late stage \nsignature showed higher reflectance than the other two stages\
    \ in the visible range and dropped down \nin 700 nm (Figure 7). The late disease\
    \ developmental stage showed a different reflectance slope in red \nand red edge\
    \ bands from all other stages. Healthy and asymptomatic signatures did not show\
    \ \nsignificant differences in the visible and NIR. The RBF and KNN recorded low\
    \ classification values \nwhen comparing healthy and asymptomatic fruit (47% and\
    \ 46%, respectively); so, it was not possible \nto distinguish between healthy\
    \ and asymptomatic immature fruit using the proposed technique \n(Figure 6b).\
    \ Nevertheless, it was possible to separate healthy fruit from those fruits with\
    \ late disease \ndevelopmental stage (Figure 6b).  \nFigure 7. The spectral structure\
    \ of immature (green) fruit in a healthy stage and in two disease \ndevelopmental\
    \ stages. The hyperspectral signature of Sugar Belle immature fruit in healthy,\
    \ \nasymptomatic, and late canker stages are shown with different colored lines.\
    \ \n3.2. UAV-Based Imaging Technique (Outdoor) \nFigure 6. The classiﬁcation rates\
    \ of healthy and canker-infected citrus tissues (leaves and fruits)\ndetermined\
    \ by the hyperspectral classiﬁcation methods (radial basis function, RBF; and\
    \ K-nearest\nneighbor, KNN) between diﬀerent canker disease stages (healthy, asymptomatic,\
    \ early, and late stage)\nfor (a) leaves (indoor and outdoor) and (b) immature\
    \ (green) fruits (indoor).\nThe spectral structure of immature (green) fruit in\
    \ a healthy stage and in two disease developmental\nstages (asymptomatic and late\
    \ stage) are presented in Figure 7. The late stage signature showed higher\nreﬂectance\
    \ than the other two stages in the visible range and dropped down in 700 nm (Figure\
    \ 7).\nThe late disease developmental stage showed a diﬀerent reﬂectance slope\
    \ in red and red edge bands\nfrom all other stages. Healthy and asymptomatic signatures\
    \ did not show signiﬁcant diﬀerences in the\nvisible and NIR. The RBF and KNN\
    \ recorded low classiﬁcation values when comparing healthy and\nasymptomatic fruit\
    \ (47% and 46%, respectively); so, it was not possible to distinguish between\
    \ healthy\nand asymptomatic immature fruit using the proposed technique (Figure\
    \ 6b). Nevertheless, it was\npossible to separate healthy fruit from those fruits\
    \ with late disease developmental stage (Figure 6b).\nRemote Sens. 2019, 11, x\
    \ FOR PEER REVIEW \n12 of 23 \n \n \n(b) \nFigure 6. The classification rates\
    \ of healthy and canker-infected citrus tissues (leaves and fruits) \ndetermined\
    \ by the hyperspectral classification methods (radial basis function, RBF; and\
    \ K-nearest \nneighbor, KNN) between different canker disease stages (healthy,\
    \ asymptomatic, early, and late \nstage) for (a) leaves (indoor and outdoor) and\
    \ (b) immature (green) fruits (indoor). \nThe spectral structure of immature (green)\
    \ fruit in a healthy stage and in two disease \ndevelopmental stages (asymptomatic\
    \ and late stage) are presented in Figure 7. The late stage \nsignature showed\
    \ higher reflectance than the other two stages in the visible range and dropped\
    \ down \nin 700 nm (Figure 7). The late disease developmental stage showed a different\
    \ reflectance slope in red \nand red edge bands from all other stages. Healthy\
    \ and asymptomatic signatures did not show \nsignificant differences in the visible\
    \ and NIR. The RBF and KNN recorded low classification values \nwhen comparing\
    \ healthy and asymptomatic fruit (47% and 46%, respectively); so, it was not possible\
    \ \nto distinguish between healthy and asymptomatic immature fruit using the proposed\
    \ technique \n(Figure 6b). Nevertheless, it was possible to separate healthy fruit\
    \ from those fruits with late disease \ndevelopmental stage (Figure 6b).  \nFigure\
    \ 7. The spectral structure of immature (green) fruit in a healthy stage and in\
    \ two disease \ndevelopmental stages. The hyperspectral signature of Sugar Belle\
    \ immature fruit in healthy, \nasymptomatic, and late canker stages are shown\
    \ with different colored lines. \n3.2. UAV-Based Imaging Technique (Outdoor) \n\
    Figure 7. The spectral structure of immature (green) fruit in a healthy stage\
    \ and in two disease\ndevelopmental stages.\nThe hyperspectral signature of Sugar\
    \ Belle immature fruit in healthy,\nasymptomatic, and late canker stages are shown\
    \ with diﬀerent colored lines.\nRemote Sens. 2019, 11, 1373\n12 of 22\n3.2. UAV-Based\
    \ Imaging Technique (Outdoor)\nWe were able to measure the spectral reﬂectance\
    \ of healthy citrus trees and trees infected with\ncanker disease with the UAV-based\
    \ imaging technique (Figure 8). The spectral signature of the healthy\ntrees was\
    \ similar to the signature of the healthy leaves produced by the indoor imaging\
    \ system. A peak\nwas observed in the green band (554 nm) with a reﬂectance value\
    \ of 31%. The classiﬁcation accuracy,\nbetween healthy and canker-infected citrus\
    \ trees, with the UAV-based imaging technique was 100%\nand 96% using the RBF\
    \ and KNN methods, respectively (Figure 6a). Regions of interest (RoIs) were\n\
    collected randomly from the canopy of the tree without targeting speciﬁc areas.\
    \ The severity of canker\ndisease and the reﬂectance values varied among the canopy\
    \ as both were dependent of the light\nconditions. Because leaf pigment content,\
    \ stoma damage, water content, and leaf cell structure aﬀects\nthe light reﬂectance,\
    \ the spectral reﬂectance of canker-infected trees shifted down in both visible\
    \ and\nnear-infrared bands. Thus, the chlorophyll content of the infected plants\
    \ was low in both indoor and\noutdoor analysis.\nRemote Sens. 2019, 11, x FOR\
    \ PEER REVIEW \n13 of 23 \n \nWe were able to measure the spectral reflectance\
    \ of healthy citrus trees and trees infected with \ncanker disease with the UAV-based\
    \ imaging technique (Figure 8). The spectral signature of the \nhealthy trees\
    \ was similar to the signature of the healthy leaves produced by the indoor imaging\
    \ \nsystem. A peak was observed in the green band (554 nm) with a reflectance\
    \ value of 31%. The \nclassification accuracy, between healthy and canker-infected\
    \ citrus trees, with the UAV-based \nimaging technique was 100% and 96% using\
    \ the RBF and KNN methods, respectively (Figure 6a). \nRegions of interest (RoIs)\
    \ were collected randomly from the canopy of the tree without targeting \nspecific\
    \ areas. The severity of canker disease and the reflectance values varied among\
    \ the canopy as \nboth were dependent of the light conditions. Because leaf pigment\
    \ content, stoma damage, water \ncontent, and leaf cell structure affects the\
    \ light reflectance, the spectral reflectance of canker-infected \ntrees shifted\
    \ down in both visible and near-infrared bands. Thus, the chlorophyll content\
    \ of the \ninfected plants was low in both indoor and outdoor analysis.  \n \n\
    Figure 8. Measurement of the spectral reflectance of healthy citrus trees and\
    \ trees infected with canker \ndisease. Hyperspectral signatures of healthy trees\
    \ and canker-infected trees were generated by the \nUAV-based imaging technique.\
    \ \n3.3. Vegetation Indices  \nThe maximum classification accuracy for indoor\
    \ and outdoor conditions using vegetation \nindices and the RBF and KNN methods\
    \ for detection of healthy and different canker disease stages \n(asymptomatic,\
    \ early, and late stage) were determined (Figure 9). Using the RBF method, the\
    \ highest \nclassifications were achieved by comparing healthy leaves with late-stage\
    \ canker-infected leaves in \nindoors (100% using the WI, PRI, GNDVI, and RARSb\
    \ indices), and healthy leaves with \nasymptomatic and early-stage canker-infected\
    \ leaves in indoors (94% with the WI index and 96% \nwith the PRI index, respectively)\
    \ (Figures 9 and 10). The KNN method achieved slightly lower \nclassification\
    \ accuracies than the RBF, except when comparing leaves in healthy and asymptomatic\
    \ \nstages in which similar accuracy with the RBF method (94%) was obtained (Figure\
    \ 9). Therefore, the \nRBF method was chosen to further analyze the classification\
    \ accuracy though by using only 12 of the \n31 proposed VIs that yielded higher\
    \ classification accuracies (Figure 10). The higher classifications \nthat were\
    \ obtained when comparing healthy with asymptomatic leaves were by the WI (100%),\
    \ NWI \n2 (98%), and SR761 (94%) (Figure 10a); when comparing healthy with early\
    \ disease stage leaves were \nby the PRI (100%), WI (98%), NWI2 (95%), and RVS1\
    \ (95%) (Figure 10b); and when comparing healthy \nwith late disease stage leaves\
    \ were by the WI (100%), PRI (100%), GNDVI (100%), RARSb (96%), and \nFigure 8.\
    \ Measurement of the spectral reﬂectance of healthy citrus trees and trees infected\
    \ with canker\ndisease. Hyperspectral signatures of healthy trees and canker-infected\
    \ trees were generated by the\nUAV-based imaging technique.\n3.3. Vegetation Indices\n\
    The maximum classification accuracy for indoor and outdoor conditions using vegetation\
    \ indices and\nthe RBF and KNN methods for detection of healthy and different\
    \ canker disease stages (asymptomatic,\nearly, and late stage) were determined\
    \ (Figure 9). Using the RBF method, the highest classifications were\nachieved\
    \ by comparing healthy leaves with late-stage canker-infected leaves in indoors\
    \ (100% using the WI,\nPRI, GNDVI, and RARSb indices), and healthy leaves with\
    \ asymptomatic and early-stage canker-infected\nleaves in indoors (94% with the\
    \ WI index and 96% with the PRI index, respectively) (Figures 9 and 10).\nThe\
    \ KNN method achieved slightly lower classification accuracies than the RBF, except\
    \ when comparing\nleaves in healthy and asymptomatic stages in which similar accuracy\
    \ with the RBF method (94%) was\nobtained (Figure 9). Therefore, the RBF method\
    \ was chosen to further analyze the classification accuracy\nthough by using only\
    \ 12 of the 31 proposed VIs that yielded higher classification accuracies (Figure\
    \ 10).\nThe higher classifications that were obtained when comparing healthy with\
    \ asymptomatic leaves were by\nthe WI (100%), NWI 2 (98%), and SR761 (94%) (Figure\
    \ 10a); when comparing healthy with early disease\nstage leaves were by the PRI\
    \ (100%), WI (98%), NWI2 (95%), and RVS1 (95%) (Figure 10b); and when\ncomparing\
    \ healthy with late disease stage leaves were by the WI (100%), PRI (100%), GNDVI\
    \ (100%),\nRemote Sens. 2019, 11, 1373\n13 of 22\nRARSb (96%), and SIPI (96%)\
    \ (Figure 10c). Thus, the WI index can be used to accurately identify healthy\n\
    leaves apart from all other categories and detect citrus canker. Figure 10d presents\
    \ the classification results\nof the UAV-based imaging technique, using the RBF\
    \ method, for each VI. Based on these results, the VIs\nthat could be used to\
    \ classify healthy vs. canker-infected citrus trees with around 100% classification\n\
    accuracy were the ARI (100%) and TCARI (96%) 1. Figure 11 compares the values\
    \ of some of the most\nsignificant VIs (as an example) in different plant conditions\
    \ (healthy, asymptomatic stage, and early\nand late disease development stage)\
    \ for citrus canker detection in both indoor and outdoor conditions\n(ρ > F <\
    \ 0.0001). The most significant VIs in indoor conditions were the WI and PRI (Figure\
    \ 11a,b),\nwhereas the ARI was in outdoor conditions (Figure 11c).\nRemote Sens.\
    \ 2019, 11, x FOR PEER REVIEW \n14 of 23 \n \nSIPI (96%) (Figure 10c). Thus, the\
    \ WI index can be used to accurately identify healthy leaves apart \nfrom all\
    \ other categories and detect citrus canker. Figure 10d presents the classification\
    \ results of the \nUAV-based imaging technique, using the RBF method, for each\
    \ VI. Based on these results, the VIs \nthat could be used to classify healthy\
    \ vs. canker-infected citrus trees with around 100% classification \naccuracy\
    \ were the ARI (100%) and TCARI (96%) 1. Figure 11 compares the values of some\
    \ of the most \nsignificant VIs (as an example) in different plant conditions\
    \ (healthy, asymptomatic stage, and early \nand late disease development stage)\
    \ for citrus canker detection in both indoor and outdoor conditions \n(ρ ˃ F ˂\
    \ 0.0001). The most significant VIs in indoor conditions were the WI and PRI (Figure\
    \ 11a-b), \nwhereas the ARI was in outdoor conditions (Figure 11c). \n \nFigure\
    \ 9. Determination of classification accuracies for indoor and outdoor conditions\
    \ using \nvegetation indices and the RBF and KNN methods for the detection of\
    \ citrus canker. The maximum \nclassification accuracies for indoor and outdoor\
    \ conditions for healthy vs. different canker disease \nstages (asymptomatic,\
    \ early and late stage) are indicated. \n \n(a) \nFigure 9.\nDetermination of\
    \ classiﬁcation accuracies for indoor and outdoor conditions using\nvegetation\
    \ indices and the RBF and KNN methods for the detection of citrus canker. The\
    \ maximum\nclassiﬁcation accuracies for indoor and outdoor conditions for healthy\
    \ vs. diﬀerent canker disease\nstages (asymptomatic, early and late stage) are\
    \ indicated.\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n14 of 23 \n \nSIPI (96%)\
    \ (Figure 10c). Thus, the WI index can be used to accurately identify healthy\
    \ leaves apart \nfrom all other categories and detect citrus canker. Figure 10d\
    \ presents the classification results of the \nUAV-based imaging technique, using\
    \ the RBF method, for each VI. Based on these results, the VIs \nthat could be\
    \ used to classify healthy vs. canker-infected citrus trees with around 100% classification\
    \ \naccuracy were the ARI (100%) and TCARI (96%) 1. Figure 11 compares the values\
    \ of some of the most \nsignificant VIs (as an example) in different plant conditions\
    \ (healthy, asymptomatic stage, and early \nand late disease development stage)\
    \ for citrus canker detection in both indoor and outdoor conditions \n(ρ ˃ F ˂\
    \ 0.0001). The most significant VIs in indoor conditions were the WI and PRI (Figure\
    \ 11a-b), \nwhereas the ARI was in outdoor conditions (Figure 11c). \n \nFigure\
    \ 9. Determination of classification accuracies for indoor and outdoor conditions\
    \ using \nvegetation indices and the RBF and KNN methods for the detection of\
    \ citrus canker. The maximum \nclassification accuracies for indoor and outdoor\
    \ conditions for healthy vs. different canker disease \nstages (asymptomatic,\
    \ early and late stage) are indicated. \n \n(a) \nFigure 10. Cont.\nRemote Sens.\
    \ 2019, 11, 1373\n14 of 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n15 of 23\
    \ \n \n \n(b) \n \n(c) \nFigure 10. Cont.\nRemote Sens. 2019, 11, 1373\n15 of\
    \ 22\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n16 of 23 \n \n \n(d) \nFigure\
    \ 10. Classification accuracy of the VIs using the RBF method to detect citrus\
    \ canker. VIs that \nwere able to detect the canker on leaves of (a) healthy vs.\
    \ asymptomatic stage (indoors), (b) healthy \nvs. early stage (indoors), (c) healthy\
    \ vs. late stage (indoors), and (d) healthy vs. canker-infected trees \nusing\
    \ the UAV-based imaging technique (outdoor) are shown. \n \n(a) \nFigure 10. Classiﬁcation\
    \ accuracy of the VIs using the RBF method to detect citrus canker. VIs that\n\
    were able to detect the canker on leaves of (a) healthy vs. asymptomatic stage\
    \ (indoors), (b) healthy vs.\nearly stage (indoors), (c) healthy vs. late stage\
    \ (indoors), and (d) healthy vs. canker-infected trees using\nthe UAV-based imaging\
    \ technique (outdoor) are shown.\nRemote Sens. 2019, 11, x FOR PEER REVIEW \n\
    16 of 23 \n \n \n(d) \nFigure 10. Classification accuracy of the VIs using the\
    \ RBF method to detect citrus canker. VIs that \nwere able to detect the canker\
    \ on leaves of (a) healthy vs. asymptomatic stage (indoors), (b) healthy \nvs.\
    \ early stage (indoors), (c) healthy vs. late stage (indoors), and (d) healthy\
    \ vs. canker-infected trees \nusing the UAV-based imaging technique (outdoor)\
    \ are shown. \n \n(a) \nFigure 11. Cont.\nRemote Sens. 2019, 11, 1373\n16 of 22\n\
    Remote Sens. 2019, 11, x FOR PEER REVIEW \n17 of 23 \n \n \n(b) \n \n(c) \nFigure\
    \ 11. Comparison of the VI values in different plant conditions (healthy, asymptomatic\
    \ stage, \nand early and late disease developmental stage) for citrus canker detection\
    \ in both indoor and outdoor \nconditions. Most significant vegetation index values\
    \ (for citrus canker disease detection) and their \nstandard deviation in different\
    \ plant conditions of (a) Water Index (WI) in laboratory conditions, (b) \nPhotochemical\
    \ Reflectance Index (PRI) in laboratory conditions, and (c) Anthocyanin Reflectance\
    \ \nIndex (ARI) in field conditions (UAV-based) are shown. \nFigure 11. Comparison\
    \ of the VI values in diﬀerent plant conditions (healthy, asymptomatic stage,\
    \ and\nearly and late disease developmental stage) for citrus canker detection\
    \ in both indoor and outdoor\nconditions. Most signiﬁcant vegetation index values\
    \ (for citrus canker disease detection) and their\nstandard deviation in diﬀerent\
    \ plant conditions of (a) Water Index (WI) in laboratory conditions,\n(b) Photochemical\
    \ Reﬂectance Index (PRI) in laboratory conditions, and (c) Anthocyanin Reﬂectance\n\
    Index (ARI) in ﬁeld conditions (UAV-based) are shown.\nRemote Sens. 2019, 11,\
    \ 1373\n17 of 22\n4. Discussion\nThe indoor imaging technique that we developed\
    \ in this study allowed us to determine the\nspectral signatures of leaves with\
    \ each disease development stage in laboratory conditions (Figure 5).\nIn a healthy\
    \ plant’s signature, the red bands usually showed high absorbance (Figure 5).\
    \ In this\nband, leaves in late disease development stages have had pigment reduction,\
    \ which aﬀected the\nspectral reﬂectance. Leaves in healthy and asymptomatic stages\
    \ did not show any visual diﬀerences\nbecause the chlorophyll concentration was\
    \ higher than in late disease development stage. However,\nthe asymptomatic stage\
    \ is a crucial stage for detecting a disease. Any delay on disease detection might\n\
    aﬀect a tree’s health or even kill them in some situations [62–64]. The canker\
    \ lesions on leaves in late\ndisease development stage turns brown, the edges\
    \ appear water-soaked, and it develops a yellow\nhalo. Therefore, the reduction\
    \ of water content and chlorophyll concentration might aﬀect the spectral\nreﬂectance\
    \ in the visible range [65,66]. Consistent with our results, Penuelas et al. [44]\
    \ conﬁrmed\nthat the red edge peak (680–780 nm) is an excellent indicator for\
    \ detecting chlorophyll content and\nwater stress.\nDetecting diseases at an early\
    \ stage (before visual symptoms appear) is very critical for successful\nmitigation\
    \ measures (e.g., chemical, or similar, applications) [67]. In our study, in near\
    \ infrared (NIR)\nrange, healthy leaves appeared diﬀerent from all other categories\
    \ (Figure 5), primarily in the red edge\n(700 nm). The NIR range of asymptomatic\
    \ and early disease stages was slightly shifted down. They\nwere very close to\
    \ each other in the red edge, while late disease stages showed a lower reﬂectance\n\
    in the NIR range at 700–950 nm. It is essential to distinguish the disease in\
    \ an asymptomatic stage\nbefore developing and spreading to the entire grove [68,69].\
    \ Several other studies conﬁrmed that\ndamages in leaves could cause changes in\
    \ the condition of leaf cells, which leads to the diﬀerences in\nspectral signature\
    \ in the NIR range (between healthy and diseased leaves). Detecting canker disease\n\
    in early stage would reduce the management costs by optimizing pesticide applications,\
    \ which in\nturn will reduce labor costs. In addition to this, optimization of\
    \ pesticide applications would reduce\nany negative eﬀect to the environment and\
    \ human health [70]. In our experiments and in laboratory\nconditions, the classiﬁcation\
    \ accuracy between healthy and asymptomatic leaves, which is the most\ncritical\
    \ moment in detecting a disease, was very high (96% with the RBF method; Figure\
    \ 6), making the\nproposed technique a valuable tool for early canker disease\
    \ detection.\nIt was not possible to distinguish between healthy and asymptomatic\
    \ immature (green) fruit (in\nthe laboratory); the classiﬁcation accuracy of the\
    \ proposed technique was low (47% with the RBF\nmethod, Figure 6b). However, the\
    \ proposed technique successfully distinguished healthy from late\ndisease development\
    \ stage on immature fruit (Figure 6b). In the past, several methods have been\n\
    developed to detect canker on mature citrus fruit [24,31], but to the best of\
    \ our knowledge, we are the\nﬁrst to develop a technique to detect canker on immature\
    \ (green) citrus fruit.\nIn the ﬁeld, the UAV-based imaging technique that we\
    \ have developed successfully distinguished\nhealthy and canker-infected citrus\
    \ trees with a 100% classiﬁcation accuracy using the RBF method\n(Figure 6a).\
    \ Several other studies also used a UAV-based imaging approach to detect other\
    \ diseases\nin early stages [71,72] and evaluate individual tree health status\
    \ [17]. For example, Garcia-Ruiz [71]\nutilized a multispectral camera to identify\
    \ Huanglongbing (HLB) disease in citrus and compared the\nresults between images\
    \ acquired by UAV and images captured by an aircraft; the UAV classiﬁcation\n\
    results were higher (67–85%) and had a better resolution than the aircraft classiﬁcation\
    \ (61–74%).\nHowever, and to the best of our knowledge, we are the ﬁrst to develop\
    \ an UAV-based technique to\ndetect citrus canker disease on citrus trees in the\
    \ grove.\nWhen bacteria attack a plant, some changes might occur to the leaves’\
    \ pigment content. This will\ncertainly aﬀect the photosynthesis process. As a\
    \ result, the leaves color would change or some lesion\nmight show up (as a result\
    \ of pathogen infection). These symptoms would aﬀect light reﬂectance\nthat can\
    \ be measured in a remote sensing technique. Therefore, vegetation indices (VIs)\
    \ have been\ndeveloped to enhance plant characteristic information, such as the\
    \ properties of an object, by calculating\nRemote Sens. 2019, 11, 1373\n18 of\
    \ 22\nthe ratio of two or more spectral bands. For example, the normalized diﬀerence\
    \ vegetation index\n(NDVI) uses the near-infrared (NIR) and red (R) channels as\
    \ follows: (NIR-R)/(NIR+R) [73,74].\nFinally, we found that it was possible to\
    \ use VIs to detect citrus canker and distinguish it from other\ndisorders in\
    \ both indoor and outdoor conditions. Here, the Water Index (WI), one of the 31\
    \ VIs used in\nthis study, allowed us to accurately distinguish healthy leaves\
    \ from all other categories and detect citrus\ncanker (Figure 10a–c). Additionally,\
    \ two other VIs used, the Anthocyanin Reﬂectance Index (ARI) and\nTransform Chlorophyll\
    \ Absorption in Reﬂectance Index (TCARI 1), with the RBF method, accurately\n\
    classiﬁed healthy from canker-infected citrus trees in the grove conditions (UAV-based\
    \ technique,\nFigure 10d). By analyzing the values of these most signiﬁcant VIs\
    \ in diﬀerent conditions, it was\nobserved that the values of the healthy trees\
    \ have had the signiﬁcantly highest values (ρ > F < 0.0001)\nthan those of diseased\
    \ trees (Figure 11). In previous studies, it was also found that the water status,\n\
    water stress, and canopy structure were aﬀected ﬁrst after a disease infection\
    \ [75,76], and thus, these\nindices can be used to identify phenotypical changes\
    \ and detect diseased trees.\n5. Conclusions\nThe indoor (laboratory conditions)\
    \ detection technique was able to detect canker-infected citrus\nleaves in asymptomatic,\
    \ early, and late disease developmental stages, and the outdoor (UAV-based)\n\
    detection technique was able to identify canker-infected citrus trees accurately.\
    \ Additionally, in\nthe laboratory, the proposed technique accurately identiﬁed\
    \ canker-infected immature (green) fruit\n(early and late disease development\
    \ stages) and distinguished them from healthy fruit. However,\nthe detection accuracy\
    \ of canker-infected immature fruit in the asymptomatic stage was low (47%),\n\
    and as a consequence, immature fruit was not a reliable tissue for early detection\
    \ of canker. Higher\nclassiﬁcations were achieved by the RBF method (about 96%\
    \ for healthy vs. asymptomatic and early\nstage, and 100% for healthy vs. late\
    \ stage, for both leaves and fruit). Furthermore, it was found\nthat several VIs\
    \ can be used to precisely detect canker-infected citrus plants. For the indoor\
    \ imaging\ntechnique, the WI index achieved high classiﬁcation accuracies when\
    \ comparing healthy leaves with\nall other categories, and for the UAV-based imaging\
    \ technique, the ARI and TCARI 1 were the most\ncommon indices that were able\
    \ to detect infected canker plants accurately. The developed imaging\ntechniques\
    \ would provide valuable tools for identifying citrus canker even in an asymptomatic\
    \ stage.\nThe UAV-based imaging technique can be utilized to detect canker-infected\
    \ trees and cover large areas\nin a short time and with a low-cost. Since both\
    \ ARI and TCARI 1 indices can be measured with a\nmultispectral camera, an inexpensive\
    \ UAV equipped with a multispectral imaging system can be used\nfor rapid citrus\
    \ canker detection in the ﬁeld.\nAuthor Contributions: Conceptualization, J.A.\
    \ and Y.A.; methodology, J.A., O.B., and Y.A.; validation, J.A., Y.A.\nand O.B.;\
    \ formal analysis, J.A.; investigation, J.A.; resources, Y.A. and O.B.; data curation,\
    \ J.A.; writing—original\ndraft preparation, J.A.; writing—review and editing,\
    \ J.A., O.B., and Y.A.; visualization, J.A.; supervision, Y.A. and\nO.B.; project\
    \ administration, Y.A.\nFunding: This research received no external funding.\n\
    Acknowledgments: The authors wish to thank everybody who has helped with completing\
    \ this work: Sri Charan\nKakarla for collecting images, the plant pathology lab\
    \ for conducting the PCR analysis, Timothy Willis and Ahmed\nOmer for leaves collection,\
    \ and John Landau for language editing.\nConﬂicts of Interest: The authors declare\
    \ no conﬂict of interest.\nReferences\n1.\nBock, C.H.; Parker, P.E.; Gottwald,\
    \ T.R. Eﬀect of stimulated wind-driven rain on duration and distance of\ndispersal\
    \ of Xanthomonas axonompodis pv. citri from canker-infected citrus trees. Plant\
    \ Dis. 2005, 89, 71–80.\n[CrossRef] [PubMed]\n2.\nHartung, J.S.; Daniel, J.F.;\
    \ Pruvost, O.P. Detection of anthomonas-campestris pv. citri by the polymerase\n\
    chain-reaction method. Appl. Environ. Microbiol. 1993, 59, 1143–1148. [PubMed]\n\
    Remote Sens. 2019, 11, 1373\n19 of 22\n3.\nDuan, S.; Jia, H.G.; Pang, Z.Q.; Teper,\
    \ D.; White, F.; Jones, J.; Zhou, C.Y.; Wang, N. Functional characterization\n\
    of the citrus canker susceptibility gene CsLOB1. Mol. Plant Pathol. 2018, 19,\
    \ 1908–1916. [CrossRef] [PubMed]\n4.\nBock, C.H.; Graham, J.H.; Gottwald, T.R.;\
    \ Cook, A.Z.; Parker, P.E. Wind speed and wind-associated leaf\ninjury aﬀect severity\
    \ of citrus canker on Swingle citrumelo. Eur. J. Plant Pathol. 2010, 128, 21–38.\
    \ [CrossRef]\n5.\nGottwald, R.T.; Graham, H.J.; Schubert, T.S. Citrus canker:\
    \ the pathogen and its impact. Online. Plant Health\nProgress. 2002. [CrossRef]\n\
    6.\nGraham, H.J.; Gottwald, R.T.; Cubero, J.; Achor, D. Xanthomonas axonopodis\
    \ pv. citri: Factors aﬀecting\nsuccessful eradication of citrus canker. Mol. Plant\
    \ Pathol. 2004, 5, 1–5. [CrossRef]\n7.\nPark, D.S.; Hyun, J.W.; Park, Y.J.; Kim,\
    \ J.S.; Kang, H.W.; Hahn, J.H.; Go, S.J. Sensitive and speciﬁc detection of\n\
    Xanthomonas axonopodis pv. citri by PCR using pathovar speciﬁc primers based on\
    \ hrpW gene sequences.\nMicrobiol. Res. 2006, 161, 145–149. [CrossRef]\n8.\nGraham,\
    \ J.H.; Leite, R.P. Lack of control of citrus canker by induced systemic resistance\
    \ compounds. Plant Dis.\n2004, 88, 745–750. [CrossRef]\n9.\nPartel, V.; Kakarla,\
    \ C.; Ampatzidis, Y. Development and evaluation of a low-cost and smart technology\n\
    for precision weed management utilizing artificial intelligence. Comput. Electron.\
    \ Agric. 2019, 157, 339–350.\n[CrossRef]\n10.\nPartel, V.; Nunes, L.; Stansley,\
    \ P.; Ampatzidis, Y. Automated vision-based system for monitoring Asian citrus\n\
    psyllid in orchards utilizing artiﬁcial intelligence. Comput. Electron. Agric.\
    \ 2019, 162, 328–336. [CrossRef]\n11.\nAshourloo, D.; Aghighi, H.; Matkan, A.A.;\
    \ Mobasheri, M.R.; Rad, A.M. An investigation into machine\nlearning regression\
    \ techniques for the leaf rust disease detection using hyperspectral measurement.\
    \ IEEE J.\nSel. Top. Appl. Earth Obs. Remote Sens. 2016, 9, 4344–4351. [CrossRef]\n\
    12.\nZhang, Y.; Lee, W.S.; Li, M.Z.; Zheng, L.H.; Ritenour, M.A. Non-destructive\
    \ recognition and classiﬁcation of\ncitrus fruit blemishes based on ant colony\
    \ optimized spectral information. Postharvest Biol. Technol. 2018,\n143, 119–128.\
    \ [CrossRef]\n13.\nBehmann, J.; Acebron, K.; Emin, D.; Bennertz, S.; Matsubara,\
    \ S.; Thomas, S.; Bohnenkamp, D.; Kuska, M.T.;\nJussila, J.; Salo, H.; et al.\
    \ Specim IQ: Evaluation of a new, miniaturized handheld hyperspectral camera and\n\
    its application for plant phenotyping and disease detection. Sensors 2018, 18,\
    \ 441. [CrossRef]\n14.\nAbdulridha, J.; Ampatzidis, Y.; Ehsani, R.; de Castro,\
    \ A.I. Evaluating the performance of spectral features and\nmultivariate analysis\
    \ tools to detect laurel wilt disease and nutritional deﬁciency in avocado. Comput.\
    \ Electron.\nAgric. 2018, 155, 203–211. [CrossRef]\n15.\nLu, J.Z.; Ehsani, R.;\
    \ Shi, Y.Y.; Abdulridha, J.; de Castro, A.I.; Xu, Y.J. Field detection of anthracnose\
    \ crown rot\nin strawberry using spectroscopy technology. Comput. Electron. Agric.\
    \ 2017, 135, 289–299. [CrossRef]\n16.\nMaes, W.H.; Steppe, K. Perspectives for\
    \ remote sensing with unmanned aerial vehicles in precision agriculture.\nTrends\
    \ Plant Sci. 2019, 24, 152–164. [CrossRef]\n17.\nAmpatzidis, Y.; Partel, V. UAV-based\
    \ high throughput phenotyping in citrus utilizing multispectral imaging\nand artiﬁcial\
    \ intelligence. Remote Sens. 2019, 11, 410. [CrossRef]\n18.\nZhang, J.; Virk,\
    \ S.; Porter, W.; Kenworthy, K.; Sullivan, D.; Schwartz, B. Applications of unmanned\
    \ aerial\nvehicle based imagery in turfgrass ﬁeld trials. Front. Plant Sci. 2019,\
    \ 10, 279. [CrossRef]\n19.\nAlbetis, J.; Jacquin, A.; Goulard, M.; Poilve, H.;\
    \ Rousseau, J.; Clenet, H.; Dedieu, G.; Duthoit, S. On the\npotentiality of UAV\
    \ multispectral imagery to detect ﬂavescence doree and grapevine trunk diseases.\n\
    Remote Sens. 2019, 11, 23. [CrossRef]\n20.\nAlbetis, J.; Duthoit, S.; Guttler,\
    \ F.; Jacquin, A.; Goulard, M.; Poilve, H.; Feret, J.B.; Dedieu, G. Detection\
    \ of\nﬂavescence doree grapevine disease using unmanned aerial vehicle (UAV) multispectral\
    \ imagery. Remote Sens.\n2017, 9, 308. [CrossRef]\n21.\nKerkech, M.; Haﬁane, A.;\
    \ Canals, R. Deep leaning approach with colorimetric spaces and vegetation indices\n\
    for vine diseases detection in UAV images. Comput. Electron. Agric. 2018, 155,\
    \ 237–243. [CrossRef]\n22.\nDash, J.P.; Pearse, G.D.; Watt, M.S. UAV multispectral\
    \ imagery can complement satellite data for monitoring\nforest health. Remote\
    \ Sens. 2018, 10, 1216. [CrossRef]\n23.\nLiu, Q.; Song, H.; Liu, G.; Huang, C.;\
    \ Li, H. Evaluating the potential of multi-seasonal CBERS-04 imagery for\nmapping\
    \ the quasi-circular vegetation patches in the Yellow River delta using random\
    \ forest. Remote Sens.\n2019, 11, 1216. [CrossRef]\n24.\nQin, J.; Burks, T.F.;\
    \ Ritenour, M.A.; Bonn, W.G. Detection of citrus canker using hyperspectral reﬂectance\n\
    imaging with spectral information divergence. J. Food Eng. 2009, 93, 183–191.\
    \ [CrossRef]\nRemote Sens. 2019, 11, 1373\n20 of 22\n25.\nWeng, H.Y.; Lv, J.W.;\
    \ Cen, H.Y.; He, M.B.; Zeng, Y.B.; Hua, S.J.; Li, H.Y.; Meng, Y.Q.; Fang, H.;\
    \ He, Y.\nHyperspectral reﬂectance imaging combined with carbohydrate metabolism\
    \ analysis for diagnosis of citrus\nHuanglongbing in diﬀerent seasons and cultivars.\
    \ Sens. Actuators B Chem. 2018, 275, 50–60. [CrossRef]\n26.\nSharif, M.; Khan,\
    \ M.A.; Iqbal, Z.; Azam, M.F.; Lali, M.I.U.; Javed, M.Y. Detection and classiﬁcation\
    \ of citrus\ndiseases in agriculture based on optimized weighted segmentation\
    \ and feature selection. Comput. Electron.\nAgric. 2018, 150, 220–234. [CrossRef]\n\
    27.\nZhang, B.H.; Liu, L.S.; Gu, B.X.; Zhou, J.; Huang, J.C.; Tian, G.Z. From\
    \ hyperspectral imaging to multispectral\nimaging: Portability and stability of\
    \ HIS-MIS algorithms for common defect detection. Postharvest Biol.\nTechnol.\
    \ 2018, 137, 95–105. [CrossRef]\n28.\nSankaran, S.; Mishra, A.; Maja, J.M.; Ehsani,\
    \ R. Visible-near infrared spectroscopy for detection of\nHuanglongbing in citrus\
    \ orchards. Comput. Electron. Agric. 2011, 77, 127–134. [CrossRef]\n29.\nMishra,\
    \ A.R.; Karimi, D.; Ehsani, R.; Lee, W.S. Identiﬁcation of citrus greening (HLB)\
    \ using a VIS-NIR\nspectroscopy technique. Trans. ASABE 2012, 55, 711–720. [CrossRef]\n\
    30.\nSankaran, S.; Ehsani, R. Comparison of visible near infrared and mid-infrared\
    \ spectroscopy for classiﬁcation\nof Huanglongbing and citrus canker infected\
    \ leaves. Agric. Eng. Int. CIGR J. 2013, 15, 75.\n31.\nPourreza, A.; Lee, W.S.;\
    \ Ritenour, M.A.; Roberts, P. Spectral characteristics of citrus black spot disease.\n\
    Horttechnology 2016, 26, 254–260.\n32.\nTanner, M.A.; Everett, C.L.; Youvan, D.C.\
    \ Molecular phylogenetic evidence for noninvasive zoonotic transmission\nof Staphylococcus\
    \ intermedius from a canine pet to a human. J. Clin. Microbiol. 2000, 38, 1628–1631.\
    \ [PubMed]\n33.\nPenuelas, J.; Pinol, J.; Ogaya, R.; Filella, I. Estimation of\
    \ plant water concentration by the reﬂectance water\nindex WI (R900/R970). Int.\
    \ J. Remote Sens. 1997, 18, 2869–2875. [CrossRef]\n34.\nHaboudane, D.; Miller,\
    \ J.R.; Pattey, E.; Zarco-Tejada, P.J.; Strachan, I.B. Hyperspectral vegetation\
    \ indices\nand novel algorithms for predicting green LAI of crop canopies: Modeling\
    \ and validation in the context of\nprecision agriculture. Remote Sens. Environ.\
    \ 2004, 90, 337–352. [CrossRef]\n35.\nJordan, C.F. Derivation of leaf area index\
    \ from quality of light on the forest ﬂoor. Ecology 1969, 50, 663–666.\n[CrossRef]\n\
    36.\nGitelson, A.A.; Merzlyak, M.N. Signature analysis of leaf reﬂectance spectra:\
    \ Algorithm development for\nremote sensing of chlorophyll. J. Plant Physiol.\
    \ 1996, 148, 494–500. [CrossRef]\n37.\nGamon, J.A.; Penuelas, J.; Field, C.B.\
    \ A narrow-waveband spectral index that tracks diurnal changes in\nphotosynthetic\
    \ eﬃciency. Remote Sens. Environ. 1992, 41, 35–44. [CrossRef]\n38.\nChappelle,\
    \ E.W.; Kim, M.S.; McMurtrey, J.E. Ration analysis of reﬂectance spectra (RARS)—An\
    \ algorithm for\nthe remote estimation concentration of chlorophyll-a, chlorophyll-b,\
    \ and carotenoid soybean leaves. Remote\nSens. Environ. 1992, 39, 239–247. [CrossRef]\n\
    39.\nBlackburn, G.A. Spectral indices for estimating photosynthetic pigment concentrations:\
    \ A test using senescent\ntree leaves. Int. J. Remote Sens. 1998, 19, 657–675.\
    \ [CrossRef]\n40.\nRaun, W.R.; Solie, J.B.; Johnson, G.V.; Stone, M.L.; Lukina,\
    \ E.V.; Thomason, W.E.; Schepers, J.S. In-season\nprediction of potential grain\
    \ yield in winter wheat using canopy reﬂectance. Agron. J. 2001, 93, 131–138.\n\
    [CrossRef]\n41.\nPenuelas, J.; Baret, F.; Filella, I. Semiempirical indexes to\
    \ assess carotenoids chlorophyll-a ratio from leaf\nspectral reﬂectance. Photosynthetica\
    \ 1995, 31, 221–230.\n42.\nPenuelas, J.; Filella, I.; Biel, C.; Serrano, L.; Save,\
    \ R. The reﬂectance at the 950–970 nm region as an indicator\nof plant water status.\
    \ Int. J. Remote Sens. 1993, 14, 1887–1905. [CrossRef]\n43.\nBarnes, J.D.; Balaguer,\
    \ L.; Manrique, E.; Elvira, S.; Davison, A.W. A reappraisal of the use of DMSO\
    \ for the\nextraction and determination of chlorophylls-A and chlorophylls-B in\
    \ lichens and higher-plants. Environ. Exp.\nBot. 1992, 32, 85–100. [CrossRef]\n\
    44.\nPenuelas, J.; Gamon, J.A.; Fredeen, A.L.; Merino, J.; Field, C.B. Reﬂectance\
    \ indexes associated with\nphysiological-changes in nitrogen-limited and water-limited\
    \ sunﬂower leaves. Remote Sens. Environ. 1994,\n48, 135–146. [CrossRef]\n45.\n\
    Hunt, E.R., Jr.; Rock, B.N. Detection of changes in leaf water content using near-\
    \ and middle-infrared\nreﬂectances. Remote Sens. Environ. 1989, 30, 43–54.\n46.\n\
    Haboudane, D.; Miller, J.R.; Tremblay, N.; Zarco-Tejada, P.J.; Dextraze, L. Integrated\
    \ narrow-band vegetation\nindices for prediction of crop chlorophyll content for\
    \ application to precision agriculture. Remote Sens.\nEnviron. 2002, 81, 416–426.\
    \ [CrossRef]\nRemote Sens. 2019, 11, 1373\n21 of 22\n47.\nGitelson, A.A.; Kaufman,\
    \ Y.J.; Stark, R.; Rundquist, D. Novel algorithms for remote estimation of vegetation\n\
    fraction. Remote Sens. Environ. 2002, 80, 76–87. [CrossRef]\n48.\nMerton, R. Monitoring\
    \ Community Hysteresis Using Spectral Shift Analysis and the Red-Edge Vegetation\n\
    Stress Index. In JPL Airborne Earth Science Workshop; NASA, Jet Propulsion Laboratory:\
    \ Pasadena, CA, USA,\n1998.\n49.\nBroge, N.H.; Leblanc, E. Comparing prediction\
    \ power and stability of broadband and hyperspectral vegetation\nindices for estimation\
    \ of green leaf area index and canopy chlorophyll density. Remote Sens. Environ.\
    \ 2001,\n76, 156–172. [CrossRef]\n50.\nRoujean, J.L.; Breon, F.M. Estimating par\
    \ absorbed by vegetation from bidirectional reﬂectance measurements.\nRemote Sens.\
    \ Environ. 1995, 51, 375–384. [CrossRef]\n51.\nMetternicht, G. Vegetation indices\
    \ derived from high-resolution airborne videography for precision crop\nmanagement.\
    \ Int. J. Remote Sens. 2003, 24, 2855–2877. [CrossRef]\n52.\nBabar, M.A.; Reynolds,\
    \ M.P.; Van Ginkel, M.; Klatt, A.R.; Raun, W.R.; Stone, M.L. Spectral reﬂectance\
    \ to\nestimate genetic variation for in-season biomass, leaf chlorophyll, and\
    \ canopy temperature in wheat. Crop\nSci. 2006, 46, 1046–1057. [CrossRef]\n53.\n\
    Bausch, W.C.; Duke, H.R. Remote sensing of plant nitrogen status in corn. Trans.\
    \ ASAE 1996, 39, 1869–1875.\n[CrossRef]\n54.\nGitelson, A.A.; Merzlyak, M.N.;\
    \ Chivkunova, O.B. Optical properties and nondestructive estimation of\nanthocyanin\
    \ content in plant leaves. Photochem. Photobiol. 2001, 74, 38–45. [CrossRef]\n\
    55.\nOmrani, E.; Khoshnevisan, B.; Shamshirband, S.; Saboohi, H.; Anuar, N.B.;\
    \ Nasir, M. Potential of radial\nbasis function-based support vector regression\
    \ for apple disease detection. Measurement 2014, 55, 512–519.\n[CrossRef]\n56.\n\
    Singh, V.; Rao, S.M. Application of image processing and radial basis neural network\
    \ techniques for ore\nsorting and ore classiﬁcation. Miner. Eng. 2005, 18, 1412–1420.\
    \ [CrossRef]\n57.\nPalmer, S.E. Parallel distributed-processing—Explorations in\
    \ the microstructure of cognition. Contemp. Psychol.\n1987, 32, 925–928. [CrossRef]\n\
    58.\nBarros, A.C.A.; Cavalcanti, G.D.C. Combining global optimization algorithms\
    \ with a simple adaptive\ndistance for feature selection and weighting. In Proceedings\
    \ of the IEEE International Joint Conference on\nNeural Networks (IJCNN 2008),\
    \ Hong Kong, China, 1–8 June 2008.\n59.\nAbdullah, M.Z.; Guan, L.C.; Azemi, B.\
    \ Stepwise discriminant analysis for colour grading of oil palm using\nmachine\
    \ vision system. Food Bioprod. Process. 2001, 79, 223–231. [CrossRef]\n60.\nSwartzla,\
    \ E.E.; Fukunaga, K. Introduction to statistical pattern recognition. IEEE Trans.\
    \ Syst. Man Cybern.\n1974, MC 4, 238.\n61.\nWeinberger, K.Q.; Saul, L.K. Distance\
    \ metric learning for large margin nearest neighbor classiﬁcation. J. Mach.\n\
    Learn. Res. 2009, 10, 207–244.\n62.\nAbdulridha, J.; Ehsani, R.; de Castro, A.\
    \ Detection and differentiation between laurel wilt disease, phytophthora\ndisease,\
    \ and salinity damage using a hyperspectral sensing technique. Agriculture 2016,\
    \ 6, 56. [CrossRef]\n63.\nAmpatzidis, Y.; De Bellis, L.; Luvisi, A. iPathology:\
    \ Robotic applications and management of plants and\nplant diseases. Sustainability\
    \ 2017, 9, 1010. [CrossRef]\n64.\nLuvisi, A.; Ampatzidis, Y.G.; De Bellis, L.\
    \ Plant pathology and information technology: Opportunity for\nmanagement of disease\
    \ outbreak and applications in regulation frameworks. Sustainability 2016, 8,\
    \ 831.\n[CrossRef]\n65.\nSims, D.A.; Gamon, J.A. Relationships between leaf pigment\
    \ content and spectral reflectance across a wide\nrange of species, leaf structures\
    \ and developmental stages. Remote Sens. Environ. 2002, 81, 337–354. [CrossRef]\n\
    66.\nDaughtry, C.S.T.; Walthall, C.L.; Kim, M.S.; de Colstoun, E.B.; McMurtrey,\
    \ J.E. Estimating corn leaf chlorophyll\nconcentration from leaf and canopy reﬂectance.\
    \ Remote Sens. Environ. 2000, 74, 229–239. [CrossRef]\n67.\nLowe, A.; Harrison,\
    \ N.; French, A.P. Hyperspectral image analysis techniques for the detection and\n\
    classiﬁcation of the early onset of plant disease and stress. Plant Methods 2017,\
    \ 13, 80. [CrossRef]\n68.\nSonobe, R.; Sano, T.; Horie, H. Using spectral reﬂectance\
    \ to estimate leaf chlorophyll content of tea with\nshading treatments. Biosyst.\
    \ Eng. 2018, 175, 168–182. [CrossRef]\n69.\nFrels, K.; Guttieri, M.; Joyce, B.;\
    \ Leavitt, B.; Baenziger, P.S. Evaluating canopy spectral reﬂectance vegetation\n\
    indices to estimate nitrogen use traits in hard winter wheat. Field Crop. Res.\
    \ 2018, 217, 82–92. [CrossRef]\nRemote Sens. 2019, 11, 1373\n22 of 22\n70.\nPatil,\
    \ J.K.; Kumer, R. Advances in image processing for detection of plant diseases.\
    \ Adv. Bioinf. Appl. Res.\n2011, 12, 135–141.\n71.\nGarcia-Ruiz, F.; Sankaran,\
    \ S.; Maja, J.M.; Lee, W.S.; Rasmussen, J.; Ehsani, R. Comparison of two aerial\n\
    imaging platforms for identiﬁcation of Huanglongbing-infected citrus trees. Comput.\
    \ Electron. Agric. 2013,\n91, 106–115. [CrossRef]\n72.\nAbdulridha, J.; Ehsani,\
    \ R.; Abd-Elrahma, A.; Ampatzidis, Y. A remote sensing technique for detecting\
    \ laurel\nwilt disease in avocado in presence of other biotic and abiotic stresses.\
    \ Comput. Electron. Agric. 2019, 156,\n549–557. [CrossRef]\n73.\nHansen, P.M.;\
    \ Schjoerring, J.K. Reﬂectance measurement of canopy biomass and nitrogen status\
    \ in wheat\ncrops using normalized diﬀerence vegetation indices and partial least\
    \ squares regression. Remote Sens.\nEnviron. 2003, 86, 542–553. [CrossRef]\n74.\n\
    Rodriguez-Caballero, E.; Knerr, T.; Weber, B. Importance of biocrusts in dryland\
    \ monitoring using spectral\nindices. Remote Sens. Environ. 2015, 170, 32–39.\
    \ [CrossRef]\n75.\nAsaari, M.S.M.; Mishra, P.; Mertens, S.; Dhondt, S.; Inze,\
    \ D.; Wuyts, N.; Scheunders, P. Close-range\nhyperspectral image analysis for\
    \ the early detection of stress responses in individual plants in a\nhigh-throughput\
    \ phenotyping platform. ISPRS J. Photogramm. Remote Sens. 2018, 138, 121–138.\
    \ [CrossRef]\n76.\nSandmann, M.; Grosch, R.; Graefe, J. The use of features from\
    \ ﬂuorescence, thermography, and NDVI\nimaging to detect biotic stress in lettuce.\
    \ Plant. Dis. 2018, 102, 1101–1107. [CrossRef]\n© 2019 by the authors. Licensee\
    \ MDPI, Basel, Switzerland. This article is an open access\narticle distributed\
    \ under the terms and conditions of the Creative Commons Attribution\n(CC BY)\
    \ license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  pdf_link: https://www.mdpi.com/2072-4292/11/11/1373/pdf?version=1559982677
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: UAV-Based Remote Sensing Technique to Detect Citrus Canker Disease Utilizing
    Hyperspectral Imaging and Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.agwat.2019.03.034
  analysis: '>'
  authors:
  - G.I. Ezenne
  - Louise Jupp
  - S. Mantel
  - Jane Tanner
  citation_count: 51
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. UAS in precision agriculture
    3. Commonly used UAS imaging cameras for water status 4. Real-time irrigation
    scheduling with thermal UAS 5. Conclusion Acknowledgment References Show full
    outline Cited by (56) Tables (1) Table 1 Agricultural Water Management Volume
    218, 1 June 2019, Pages 158-164 Review Current and potential capabilities of UAS
    for crop water productivity in precision agriculture Author links open overlay
    panel G.I. Ezenne a, Louise Jupp b, S.K. Mantel a, J.L. Tanner a Show more Add
    to Mendeley Share Cite https://doi.org/10.1016/j.agwat.2019.03.034 Get rights
    and content Highlights • Potential capabilities of UAS for crop water productivity
    in precision agriculture. • Technology that can improve crop water productivity
    (CWP). • UAS thermal remote sensing with certain indices is most suitable for
    crop water assessment. • Possibility for real-time irrigation scheduling with
    thermal UAS. Abstract In order to feed growing populations under scare water resources,
    a suitable technology that improves crop water productivity (CWP) is crucial.
    Precision agriculture that utilizes digital techniques such as unmanned aerial
    systems (UAS) can play a significant role in improving CWP. CWP is an important
    indicator that quantifies the effect of agricultural water management. To improve
    CWP, implementation of suitable methods for early detection of crop water stress
    before irreversible damage on crops occurs, is vital. Conventionally, farmers
    have relied on in situ measurements of soil moisture and weather variables for
    detecting crop water status for irrigation scheduling. This method is time consuming
    and does not account for spatial and temporal variability associated with crop
    water status. Hence, the aim of this study is to give an overview of the current
    and potential capabilities of UAS for crop water productivity in precision agriculture.
    Identified in this study are the factors as well as the technology that can improve
    CWP. UAS thermal remote sensing is found to be the most suitable technology for
    monitoring and assessing crop water status using certain indices. Determining
    a crop water stress index (CWSI) from thermal imagery has the potential to detect
    instantaneous variations of water status. CWSI obtained from UAS thermal imaging
    camera can be adapted for real-time irrigation scheduling for maximum crop water
    productivity. Previous article in issue Next article in issue Keywords Agricultural
    water useThermal imagingReal-time irrigation scheduling 1. Introduction By 2050
    agricultural production must double to meet the food demand of the growing population
    (Martínez et al., 2017). The agricultural industry must therefore, maximize the
    use of available resources in order to cope with the present trajectory of population
    growth and, precision agriculture can play a significant role in reaching this
    goal. Precision agriculture is a farming management concept where a practice is
    performed at the right time, right place and appropriate intensity (Maes and Steppe,
    2019). It often utilizes digital techniques (satellite, UAS, sensors, etc.) to
    optimize agricultural production processes while minimizing adverse environmental
    impacts (Schrijver, 2016; Abdullahi et al., 2015). Sensors commonly used in precision
    irrigation systems such as drip irrigation reduce crop water use but provide only
    point estimates. When digital techniques such as UAS are utilized in precision
    agriculture, the spatial and temporal variability of the crop conditions are accounted
    for in greater detail than commonly used sensor technologies. Precision agriculture
    generally aims to increase the quality and quantity of agricultural output while
    using less input (e.g. water, pesticides, fertilizer, energy, herbicides etc.).
    Agricultural water use is one of the main factors behind increasing water scarcity,
    with irrigated agriculture accounting for about 70% of the freshwater withdrawals
    in the world (Tshwene and Oladele, 2016). With water being a significant limiting
    factor in agricultural management, this high agricultural water use can be better
    managed using the crop water productivity concept. CWP also referred to as water
    use efficiency, can be defined as the relationship between crop output and the
    amount of water used in crop production. It is mathematically expressed as the
    quantity of yield (kg/hectare) per unit amount of water (mm/day) consumed (i.e.
    crop evapotranspiration) (Jin et al., 2016; Araya et al., 2018; Kukal and Irmak,
    2017; Brauman et al., 2013; Dogaru, 2016; Humphreys et al., 2006; Nhamo et al.,
    2016; Wichelns, 2014). CWP is an indicator that quantifies the effect of agricultural
    water management (Sakthivadivel et al., 1999; Sun et al., 2017). The use of CWP
    helps to meet the rising demand of water by irrigation amidst other sectors and
    also provide food for the growing population. According to Igbadun et al. (2006),
    there are many expressions and definitions for quantifying CWP, such as water
    use (technical) efficiency, which is defined as quantity of agricultural output
    per unit amount of water used in the production; water use (economic) efficiency,
    which is the value of the output produced per quantity of water consumed, and;
    water use (hydraulic) efficiency, which is defined as the ratio of water consumed
    by irrigated agriculture to the volume of water supplied. These three CWP definitions
    are centred on minimizing water input while maximizing the crop yield/output.
    Deficit irrigation techniques have been found to improve CWP by more than 200%
    (Zwart and Bastiaanssen, 2004; Hirich et al., 2014; Adeboye et al., 2015) if irrigation
    is timeously managed. However, deficit irrigation must be carefully managed as
    Igbadun et al. (2006) described an attempt to maximize CWP by withholding irrigation
    in multiple crop growth stages which resulted in significant reduction in crop
    yield. Zwart and Bastiaanssen (2004), however, found that plants are more efficient
    with water when they are stressed and agricultural production can be sustained
    with 20–40% less water resources as long as new water management techniques are
    adopted. The agricultural sector is currently under pressure to produce high quantity
    of food while using less water per unit of output thereby increasing CWP (Tshwene
    and Oladele, 2016). This is achieved by either producing the same quantity of
    crop with less water resources, or a higher quantity of crop with the same quantity
    of water resources (Zwart and Bastiaanssen, 2004). Either way, CWP is generally
    derived from the relationship between the water consumption and yield of a particular
    crop, both of which are highly variable in space and time (Kukal and Irmak, 2017).
    CWP can exhibit high spatial and temporal variability due to climatic and agricultural
    factors (Mdemu et al., 2009). Among the climatic factors, CWP is more sensitive
    to wind speed and relative humidity compared to sunshine hours while for agricultural
    factors, it is more sensitive to irrigation efficiency than the quantity of fertilizer
    used (Sun et al., 2017). Therefore, to improve CWP emphasis should focus on the
    climatic factors and on irrigation efficiency. Existing techniques for improving
    CWP such as partial root zone drying (Barrios-Masias and Jackson, 2016), optimized
    regulated deficit irrigation (Léllis et al., 2017; Gendron et al., 2018; Adeboye
    et al., 2015) etc. do not currently account for the integrated effects of climatic
    and irrigation efficiency factors as well as spatial variability of CWP. Unmanned
    aerial systems (UAS) technology offers high spatial resolution and rapid collection
    of data over large areas, potentially enabling farmers to increase the precision
    of irrigation both spatially and temporally. Therefore, the aim of this study
    is to give a general overview of the current and potential capabilities of UAS
    to improve crop water productivity under precision agriculture. 2. UAS in precision
    agriculture UAS are platforms for remote sensing (RS) used in precision agriculture
    and examples of their use are found in (Hunt et al., 2018; Adão et al., 2017;
    Bendig et al., 2012; Santesteban et al., 2017; Poblete et al., 2017; Muchiri and
    Kimathi, 2016; Abdullahi et al., 2015; Simelli and Apostolos, 2015; Whitehead
    et al., 2014; Whitehead and Hugenholtz, 2014; Manfreda et al., 2018). These platforms
    are evolving rapidly technically and with regard to regulations (Ballesteros et
    al., 2014). UASs recently have become a common remote sensing technology comprising
    aerial platforms suitable for carrying small and lightweight sensors. There are
    different classes of UAS for instance according to Whitehead and Hugenholtz (2014),
    in United States it ranges from micro (< 0.9 kg), mini (0.9–13.6 kg), tactical
    (13.6–454.5 kg), medium altitude long endurance (454.5 – 13,636.4 kg), and high
    altitude long endurance (> 13,636.4 kg). They noted that commercial and remote
    sensing applications prefer UAS weighing less than 5 kg because of cost advantages
    and reduced risk associated with blunt force impact. Mini-UAS weighing less than
    5 kg can carry 0.2 kg to 1.5 kg of sensor payload (Bendig et al., 2012). According
    to Gago et al. (2015), four component tasks to be considered before selecting
    the appropriate UAS for precision agriculture are: design of experiment, data
    acquisition, data processing and results. For agricultural management, fixed-wing
    airplanes and unmanned helicopters UASs are mainly used. These two aerial platforms
    have some limitations and advantages. To mention a few limitations and advantages,
    unmanned helicopters are able to travel in any direction while fixed wing airplanes
    travel in a more linear and restricted fashion. Fixed-wing aircrafts have simple
    flight systems and can fly for longer duration compared to unmanned helicopters.
    Unmanned helicopters have more complex flight systems and can offer lower flight
    altitudes with low-speed. Fixed-wing aircrafts therefore, have capacity to cover
    wider areas but their flight altitude is higher which reduces the image resolution.
    Also, fixed-wing UAS aerial platforms require specific runways or at least sufficient
    open space for landing and take-off. Because of these limitations, recently a
    new aircraft platform emerged, multi-copter (multi-rotor), which does not require
    special take-off or landing runway. The multi-copter is more user-friendly which
    makes it possible to be operated by farmers in order to obtain real-time data
    and apply adapted water management. Generally, the type of UAS required depends
    on the aim of the study as well as the quality of the desired output/result. In
    addition to UAS aerial platforms, the camera sensors are key to the quality of
    images produced. Selection of camera type also depends on the aim of the project.
    The most commonly used UAS cameras in agriculture include: thermal, multispectral,
    hyperspectral and red-green-blue (RGB). Gago et al. (2015) recommended different
    camera types depending on the type of crop trait/status of interest. For biotic
    and abiotic stress, thermal and hyperspectral cameras are recommended; while multispectral
    and red-green-blue (RGB) cameras are recommended for growth/biomass assessment.
    RGB cameras are one of the traditional imaging systems used in agroforestry applications,
    however they lack the precision and spectral range to profile materials that hyperspectral
    and multispectral cameras can provide (Adão et al., 2017). 3. Commonly used UAS
    imaging cameras for water status 3.1. Hyperspectral Hyperspectral imaging techniques
    measure the reflectance from the vegetation canopy. The recorded reflectance cannot
    be applied directly as a metric of leaf water content therefore reflectance indices
    are used. Indices for hyperspectral data include Simple Ratio Index (SRI), Normalized
    Difference (e.g. Photochemical Reflectance Index (PRI)) etc. (Elvanidi et al.,
    2018; Pôças et al., 2017; Rapaport et al., 2015). Hyperspectral technology integrates
    spectroscopy with the benefits of digital imagery (Loggenberg et al., 2018). The
    capability of hyperspectral imaging ranging from 400 to 2500 nm in assessing the
    water deficiency in tomato plants was demonstrated by Susič et al. (2018). Zarco-Tejada
    et al. (2012) demonstrated the ability of UAV based hyperspectral and thermal
    cameras to assess water stress levels in a citrus crop and confirmed that there
    is link between PRI and canopy temperature. Recently, Loggenberg et al. (2018)
    combined hyperspectral RS technologies with machine learning to discriminate between
    stressed and non-stressed grape vines from hyperspectral imaging. The results
    obtained showed that machine (ensemble) learners effectively analysed the hyperspectral
    data. Generally, hyperspectral imaging cameras capture more detailed data in both
    spatial and spectral ranges compared to other cameras. It can measure hundreds
    of bands which raises complexity when considering the quantity of data acquired.
    Loggenberg et al. (2018) noted that the major limiting factor in applying hyperspectral
    data is the inherent ‘curse of dimensionality’ which results in reduced classification
    accuracies. In addition, hyperspectral cameras are very expensive and complex
    (Elvanidi et al., 2018), which limit their wide-spread application especially
    in commercial agriculture. 3.2. Multispectral Multispectral imaging techniques
    are based on the theory that each surface reflects part of the light received.
    Therefore, multispectral cameras measure reflectance and light absorption from
    the vegetation canopy (Matese and Di Gennaro, 2018). Vegetation reflectance characteristics
    are used to derive vegetation indices (VIs). Healthy vegetation absorbs all the
    red light and reflects back much of the near infrared light while stressed vegetation
    reflects more red light and less near infrared light. Multispectral VIs used for
    detecting water status include: structural indices (e.g. Normalized Difference
    Vegetation Index (NDVI), Renormalized Difference Vegetation Index (RDVI), Optimized
    Soil-Adjusted Vegetation Index (OSAVI), SRI etc.); xanthophyll indices (e.g. Photochemical
    Reflectance Index (PRI570, PRI515)); chlorophyll indices (e.g. Transformed Chlorophyll
    Absorption in Reflectance Index (TCARI), TCARI/OSAVI etc.); Blue/green/red ratio
    indices (e.g. Green Index (GI), Red Green Ratio Index (RGRI) etc.) (Baluja et
    al., 2012; Rallo et al., 2014; Zarco-Tejada et al., 2013, 2012). A new index,
    PRInorm was proposed by Zarco-Tejada et al. (2013) in which PRI index is normalized
    to be sensitive to canopy structure and chlorophyll content. PRInorm performed
    better than standard PRI index when tested against leaf water potential and stomatal
    conductance. The NDVI which is the index of plant greenness is popularly used
    among other VIs (Matese and Di Gennaro, 2018; Zhao et al., 2017). It is expressed
    as the normalized difference between the red and NIR (Near-Infrared) bands hence,
    it manages the differences in illumination within an image due to aspect and slope
    as well as differences due to time images were obtained. Baluja et al. (2012)
    used different spectral indices to assess water status of a vineyard and noted
    that NDVI and TCARI/OSAVI gave the highest coefficient of determination with stem
    water potential (Ѱstem) and leaf stomatal conductance (gs). Therefore, the two
    indices reflect the cumulative water deficit hence long-term response to water
    status unlike thermal that gives short-term response. Zhao et al. (2017) established
    a relationship between canopy NDVI and Ѱstem at different growing stages. Multispectral
    RS imaging technology uses bands such as Green (500–600 nm spectral band), Red
    (600–700 nm spectral band), Red-Edge (700–730 nm) and NIR (700 nm to 1.3 μm) wavebands
    to capture visible and invisible images of vegetation. According to Poblete et
    al. (2017), plant water status is not accurately predicted with multispectral
    indices between 500–800 nm spectral band because of their non-sensitivity to water
    content however, wavelengths greater than 800 nm may be better. Rallo et al. (2014)
    noted that satisfactory estimation of leaf water potentials at leaf/canopy levels
    can be derived using VIs based on the NIR-shortwave infrared domain with specific
    optimization of the “centre-bands”. Recently, some researchers have developed
    artificial neural network (ANN) models derived from multispectral images to forecast
    spatial variability of Ѱstem of crops. Romero et al. (2018) classified the water
    status level of the grapevine based on ten vegetation indices using Red, Green,
    Red-Edge and NIR wavebands and found that there were no significant relationships
    between VIs individually and Ѱstem however with lower correlation values of less
    than 0.3 for almost all the indices studied. They went further to develop an ANN
    model (model 1) using all VIs calculated as inputs and compared the output with
    actual measured Ѱstem and got higher correlation values. Similarly, Poblete et
    al. (2017) compared ANN model outputs with ground-truth measurements of Ѱstem
    and found the best performance of the model was with simulations that included
    the spectral bands of 550, 570, 670, 700 and 800 nm. 3.3. Thermal imaging Thermal
    RS measures the radiation emitted from an object’s surface and converts it into
    temperature (Khanal et al., 2017). The theory behind thermal RS is that all objects
    on the earth’s surface with a temperature above - 273 °C emit radiation and the
    quantity of radiation is a function of the emissivity of the surface temperature
    (Khanal et al., 2017). Crop canopy surface temperature in general is a function
    of transpiration rate, available soil water status and atmospheric evaporative
    demand (Khanal et al., 2017). Therefore, thermal RS determines the crop canopy
    surface temperature as a way of quantifying crop water stress which is useful
    in irrigation scheduling. Additionally, thermal RS can also quantify soil moisture
    and evapotranspiration status. Many studies have used thermal RS to assess crop
    water stress for several crop species including citrus orchards (Zarco-Tejada
    et al., 2012), pinto beans (Zhou et al., 2018), sugar beet plants (Quebrajo et
    al., 2018), almond trees (García-Tejero et al., 2018a & 2018b), and grapevines
    (Bellvert et al., 2014; Bellvert et al., 2016; Espinoza et al., 2017; Matese et
    al., 2018; García-Tejero et al., 2016). Notwithstanding that thermal RS is capable
    of providing temporal and spatial information of crop water stress, certain issues
    must be considered while using thermal RS images. The issues include (1) temporal
    and spatial resolutions of images acquired, (2) atmospheric conditions, (3) altitude
    and viewing angle of thermal sensors, and (4) variation in crop species and crop
    growth stages (Khanal et al., 2017). Among these camera types, thermal imaging
    cameras are popularly used for water stress detection (Khanal et al., 2017). Some
    studies have used a combination of multispectral and thermal cameras in assessing
    crop water stress using some indicators (Zhou et al., 2018; Espinoza et al., 2017).
    They noted that thermal imaging cameras are useful tool for rapid detection of
    crop water stress. Considering cost, availability and popularity therefore, thermal
    imaging cameras seem to be the most suitable for detecting crop water stress (Table
    1). Table 1 shows research conducted using UASs with different RS technologies
    for crop water stress assessment, resolution of the images obtained, altitude
    of the flight and area covered by UAS. Table 1. Studies conducted using UASs with
    different RS technologies for assessment of crop water stress. Type of UAS Camera
    Altitude (m) Resolution (cm/px) Area (Ha) Type of Crop Purpose Some Indices used
    Correlation (R2) Study location Reference Ѱstem gs Multi-rotor Thermal 5, 10,
    20, 30, and 40 8 12 Sugar beet To predict variations in crop water use CWSI Cadiz,
    Spain Quebrajo et al. (2018) Multi-rotor Thermal 120 – 10 Olive trees Assessing
    crop water stress using thermal drift correction models Córdoba, Spain Mesas-Carrascosa
    et al. (2018) Multi-rotor Thermal 100 13 1.7 Grape Optimizing irrigation scheduling
    CWSI Sardinia, Italy Matese et al. (2018) Multi-rotor Thermal 70 < 9 7.5 Grape
    Estimation of plant water stress CWSI 0.69 0.71 Traibuenas, Navarra, Spain Santesteban
    et al. (2017) Multi- copter Thermal – – – Corn To determine and analyze CWSI CWSI
    Maryland, USA Ford et al. (2017) Multi-rotor Thermal 90 – 0.97 Nectarine and peach
    Water stress detection CWSI 0.72 0.82 Tatura, Australia Park et al. (2017) Multi-copter
    Thermal 60 36 0.04 Grape Assess vine water stress conditions CWSI 0.73 0.82 Maule
    region, Chile Sepúlveda-Reyes et al. (2016) Fixed-wing Thermal and RGB 90 10 32
    Barley Water stress detection CWSI Western Denmark Hoffmann et al. (2016) Type
    of UAS Camera Altitude (m) Resolution (cm/px) Area (Ha) Type of Crop Objective
    Some Indices used Correlation (R2) Study location Reference Ѱstem gs Fixed-wing
    Thermal and hyperspectral 250 35 0.6 Mandarin and Orange Testing the hypothesis
    that the CWSI is a reliable indicator CWSI 0.59-0.66 Seville, Spain Gonzalez-Dugo
    et al. (2014) 550 Fixed-wing Thermal and multispectral 250 20 7-10 Olive orchards
    Water stress detection caused by Verticillium wilt (VW) infection and severity
    CWSI Southern Spain Calderón et al. (2013) Fixed-wing Thermal 370 49 42 Apricot
    To characterize the spatial variations in water stress of five fruit tree species
    CWSI Murcia, Spain Gonzalez-Dugo et al. (2013) Orange Peach Almond Lemon Fixed-wing
    Thermal and multispectral 150 20 1.4 Grape Water stress detection CWSI −0.4 California,
    USA Zarco-Tejada et al. (2013) PRI 0.68 PRInorm Fixed-wing Thermal and hyperspectral
    575 40 0.6 Orange and Mandarin Water stress detection Tc–Ta 0.34 0.78 Seville,
    Spain Zarco-Tejada et al. (2012) PRI515 NDVI 0.38 0.59 0.24 0.32 Unmanned helicopter
    Thermal 150 12 – Almond Water stress detection Tc–Ta 0.74 0.66 California, USA
    Gonzalez-Dugo et al. (2012) – Thermal and multispectral 200 30 5 Grapevine Assess
    water stress variability CWSI 0.52 0.70 Logrono, Spain Baluja et al. (2012) NDVI
    0.68 0.75 PRI 0.25 0.46 TCARI/OSAVI 0.58 0.84 Unmanned helicopter Multispectral
    150 15 – Olive, peach and orange Detect plant water stress Southern Spain Zarco-Tejada
    et al. (2009) Unmanned helicopter Thermal 150 40 4 Olive Map CWSI and canopy conductance
    CWSI 0.82 0.91 Southern Spain Berni et al. (2009) 3.3.1. Indices for water stress
    in thermal imaging Several indices exist for monitoring and quantifying water
    stress based on the crop canopy temperature. These indices include Crop Water
    Stress Index (CWSI), Degrees Above Canopy Threshold (DACT) and Degrees Above Non-Stressed
    (DANS) etc., and ‘canopy temperature’ is used as the main driver for evaluating
    water status (DeJonge et al., 2015). Kullberg et al. (2017) evaluated thermal
    RS indices and found that crop canopy temperature based water stress indices with
    less data requirements (DANS and DACT), are sensitive to crop water stress and
    this is comparable to more data intensive methods, such as CWSI. However, DANS
    and DACT have not been widely tested like CWSI. A new indicator also based on
    canopy temperature, CTSD (standard deviation of canopy temperature) was developed
    by Han et al. (2016). This index was tested using maize crop and found to be non-sensitive
    to small changes in water stress. Among the indices, CWSI is used as the standard
    index for quantifying water stress however; it requires additional data, such
    as vapour pressure deficit (VPD), as well as ideal weather conditions and prior
    computation (baselines) (Ihuoma and Madramootoo, 2017; Gerhards et al., 2016).
    In estimating CWSI, emissivity from crop canopy acquired with a thermal imaging
    camera is used in the computation of water stress related to canopy temperature
    (Santesteban et al., 2017). According to Quebrajo et al. (2018), CWSI is computed
    as: (1) where is the maximum and is difference between canopy temperature (Tc)
    and air temperature (Ta) at the ith measurement moment. DeJonge et al. (2015)
    noted that accumulation of daily midday difference between (Ta - Tc) throughout
    a season is linearly proportional to the crop final yield. is a value representing
    the non-water-stressed baselines (NWSB). NWSB is a linear function that combines
    the change in difference between air temperature and canopy temperature when crop
    transpiration is at potential rate and vapour pressure deficit value simultaneously
    measures canopy temperature. NWSB denoted as ci is expressed mathematically as:
    (2) where a and b are empirical parameters obtained for each species in the study
    environment; VPDi is the vapour pressure deficit (kPa) at the moment of flight
    on the ith measurement day. According to García-Tejero et al. (2018a), thermal
    imagery information is better interpreted using NWSB. They evaluated the potential
    of NWSB obtained from thermal imagery in order to establish a practical protocol
    for decisions related to irrigation management in almond plantations. Some studies
    have evaluated CWSI indicator in some crops, sugar beet plants (Quebrajo et al.,
    2018), grape (Matese et al., 2018; Pou et al., 2014; García-Tejero et al., 2016;
    Santesteban et al., 2017), olive orchard (Egea et al., 2017), Euonymus japonica
    (euonymus) plants (Gómez-Bellot et al., 2015), citrus (Gonzalez-Dugo et al., 2014),
    maize (DeJonge et al., 2015) and found that it is a valuable indicator for assessing
    spatial variability of crop water status. CWSI derived from thermal imagery has
    also been deployed for high spatial and temporal monitoring of water stress of
    the corn canopy in greenhouse (Mangus et al., 2016). Results obtained indicate
    that CWSI estimated from thermal canopy temperature can be used to quantify spatial
    and temporal soil moisture variability for irrigation scheduling. In addition
    to water stress detection using crop temperature, thermal imaging cameras can
    also detect water stress using soil moisture index and evapotranspiration algorithms
    (Khanal et al., 2017). Evapotranspiration algorithms have been successfully applied
    to estimate daily evapotranspiration from soybean and corn (Khanal et al., 2017).
    4. Real-time irrigation scheduling with thermal UAS Recent advances in irrigation
    technology have platforms for continuous transmitting of data among irrigation
    controllers, installed field sensors as well as equipment for variable irrigation
    rate (Quebrajo et al., 2018). Thermally-based techniques currently employed in
    evaluating water stress for automatic irrigation scheduling include crop canopy
    temperature, CWSI (empirical or theoretical), and time temperature threshold (O’Shaughnessy
    and Evett, 2010). The CWSI algorithm is mostly used for automatic irrigation control
    systems in order to improve crop water productivity. O’Shaughnessy et al. (2012)
    developed a method that integrated CWSI and time threshold for automatic irrigation
    scheduling. The effectiveness of the method was investigated on short and long
    season grain sorghum hybrids. They noted that the method can effectively trigger
    the irrigation system for automatic irrigation scheduling. Also, Osroosh et al.
    (2015) developed an adaptive scheduling algorithm that relied on theoretical CWSI
    to automatically irrigate apple trees. Most of these studies utilized the mobility
    of linear or center pivot irrigation systems to mount thermal imaging cameras
    thereby getting a dynamic scan of the effects of canopy temperature (DeJonge et
    al., 2015). CWSI obtained from UAS thermal imaging cameras can be adapted for
    real-time irrigation scheduling for maximum crop water productivity. Real-time
    canopy temperature, the main driver of CWSI, is better estimated automatically
    from thermal images. Jiménez-Bello et al. (2011) developed and validated an automatic
    UAS thermal imaging process for assessing plant water status that requires no
    operator participation. With this, the UAS thermal imaging techniques, coupled
    with software platforms, will not only estimate the spatial and temporal crop
    water stress variation, but drastically reduce the time needed between image processing,
    analysis and taking action. With the water stress information from thermal imaging
    either from canopy temperature, soil moisture index or evapotranspiration algorithms,
    irrigation controllers can be communicated for real time irrigation scheduling
    to improve water use productivity. CWSI from thermal imagery has great potential
    to detect instantaneous variations of water status (Santesteban et al., 2017;
    Osroosh et al., 2016). Bellvert et al. (2016) used a UAS with a thermal sensor
    to fly over grape vineyards and obtained water stress information for real-time
    irrigation scheduling. Leaf water potential obtained from the CWSI was successfully
    used as an irrigation trigger and there was no negative effect on yield and composition
    of the crop. Torres-Rua (2017) also used UAS carrying a thermal camera sensor
    to estimate evapotranspiration. The evapotranspiration information obtained was
    aggregated to irrigation valve zones that the irrigation technology supports.
    Reliable data and information acquired rapidly and easily are therefore essential
    for assessing water use productivity. 5. Conclusion CWP exhibit high spatial and
    temporal variability due to climatic and agricultural factors. To improve CWP,
    emphasis should be more on the climatic factors as well as on irrigation efficiency.
    UAS technologies with these commonly used cameras: hyperspectral, multispectral
    and thermal can improve CWP. Among these cameras, UAS thermal RS is identified
    as a valuable tool for monitoring and assessing crop water status. UAS thermal
    imaging cameras are used to estimate climatic factors that can improve CWP. Thermal
    RS determines the crop canopy temperature as a way of quantifying crop water stress.
    Some indices have been identified and used for quantifying crop water status using
    thermal RS. Among these indices, CWSI is widely used in quantifying and assessing
    crop water stress despite its data demand compared to other indices. CWSI is known
    to vary in space and time therefore, UAS thermal imaging cameras have the potential
    to account for spatial and temporal variability associated with CWSI. In addition
    to water stress detection using crop temperature, thermal imaging cameras can
    also detect water stress using soil moisture index and evapotranspiration algorithms.
    Irrigation technologies that improve CWP should be adopted in order to meet the
    global food demand associated with increasing global populations. Conventional
    irrigation systems alone are not enough to improve CWP to feed the projected human
    population increase, especially in the face of climate change. Recently, researchers
    have tried to improve CWP by developing an adaptive irrigation scheduling algorithm
    that relied on theoretical CWSI obtained from thermal imaging cameras mounted
    on the linear or centre pivot irrigation system. This method failed to account
    properly for the spatial and temporal variation associated with crop water stress.
    Similarly, UAS have also been used for quasi-time irrigation scheduling where
    the crop water stress information obtained is interpreted and used much later
    after the measurement. For maximum CWP while accounting for spatial and temporal
    variability of crop water status, there is need for real-time irrigation scheduling
    using UAS carrying thermal imaging cameras. Real-time canopy temperature, the
    main driver of CWSI is estimated automatically from thermal images. With the water
    stress information from thermal imaging irrigation controllers can be communicated
    for real time irrigation scheduling thereby producing maximum crop water productivity.
    Acknowledgment We want to acknowledge Rhodes University, South Africa for funding
    and providing a favourable environment for study. References Abdullahi et al.,
    2015 H.S. Abdullahi, F. Mahieddine, R.E. Sheriff Technology impact on agricultural
    productivity: a review of precision agriculture using unmanned aerial vehicles
    In International Conference on Wireless and Satellite Systems, Springer, Cham
    (2015), pp. 388-400, 10.1007/978-3-319-25479-1 View in ScopusGoogle Scholar Adão
    et al., 2017 T. Adão, J. Hruška, L. Pádua, J. Bessa, E. Peres, R. Morais, J.J.
    Sousa Hyperspectral imaging: a review on UAV-based sensors, data processing and
    applications for agriculture and forestry Remote Sens. (Basel), 9 (11) (2017),
    pp. 1-31, 10.3390/rs9111110 View in ScopusGoogle Scholar Adeboye et al., 2015
    O.B. Adeboye, B. Schultz, K.O. Adekalu, K. Prasad Crop water productivity and
    economic evaluation of drip-irrigated soybeans (Glyxine max L. Merr.) Agric. Food
    Secur., 4 (10) (2015), pp. 1-13, 10.1186/s40066-015-0030-8 View in ScopusGoogle
    Scholar Araya et al., 2018 A. Araya, I. Kisekka, P.H. Gowda, P.V.V. Prasad Grain
    sorghum production functions under different irrigation capacities Agric. Water
    Manag., 203 (2018), pp. 261-271, 10.1016/j.agwat.2018.03.010 (March) View PDFView
    articleView in ScopusGoogle Scholar Ballesteros et al., 2014 R. Ballesteros, J.F.
    Ortega, D. Hernández, M.A. Moreno Applications of georeferenced high-resolution
    images obtained with unmanned aerial vehicles. Part I: description of image acquisition
    and processing Precis. Agric., 15 (6) (2014), pp. 579-592, 10.1007/s11119-014-9355-8
    View in ScopusGoogle Scholar Baluja et al., 2012 J. Baluja, M.P. Diago, P. Balda,
    R. Zorer, F. Meggio, F. Morales, J. Tardaguila Assessment of vineyard water status
    variability by thermal and multispectral imagery using an unmanned aerial vehicle
    (UAV) Irrig. Sci., 30 (6) (2012), pp. 511-522, 10.1007/s00271-012-0382-9 View
    in ScopusGoogle Scholar Barrios-Masias and Jackson, 2016 F.H. Barrios-Masias,
    L.E. Jackson Increasing the effective use of water in processing tomatoes through
    alternate furrow irrigation without a yield decrease Agric. Water Manag., 177
    (2016), pp. 107-117, 10.1016/j.agwat.2016.07.006 View PDFView articleView in ScopusGoogle
    Scholar Bellvert et al., 2014 J. Bellvert, P.J. Zarco-Tejada, J. Girona, E. Fereres
    Mapping crop water stress index in a ‘Pinot-noir’ vineyard: comparing ground measurements
    with thermal remote sensing imagery from an unmanned aerial vehicle Precis. Agric.,
    15 (4) (2014), pp. 361-376, 10.1007/s11119-013-9334-5 View in ScopusGoogle Scholar
    Bellvert et al., 2016 J. Bellvert, P.J. Zarco-Tejada, J. Marsal, J. Girona, V.
    González-Dugo, E. Fereres Vineyard irrigation scheduling based on airborne thermal
    imagery and water potential thresholds Aust. J. Grape Wine Res., 22 (2) (2016),
    pp. 307-315, 10.1111/ajgw.12173 View in ScopusGoogle Scholar Bendig et al., 2012
    J. Bendig, A. Bolten, G. Bareth Introducing a Low-Cost Mini-Uav for Thermal- and
    Multispectral-Imaging Isprs - Int. Arch. Photogramm. Remote. Sens. Spat. Inf.
    Sci., XXXIX-B1 (September) (2012), pp. 345-349, 10.5194/isprsarchives-XXXIX-B1-345-2012
    View in ScopusGoogle Scholar Berni et al., 2009 J.A.J. Berni, P.J. Zarco-Tejada,
    G. Sepulcre-Cantó, E. Fereres, F. Villalobos Mapping canopy conductance and CWSI
    in olive orchards using high resolution thermal remote sensing imagery Remote
    Sensing of Environment, 113 (11) (2009), pp. 2380-2388, 10.1016/j.rse.2009.06.018
    View PDFView articleView in ScopusGoogle Scholar Brauman et al., 2013 K.A. Brauman,
    S. Siebert, J.A. Foley Improvements in crop water productivity increase water
    sustainability and food security - a global analysis Environ. Res. Lett., 8 (2)
    (2013), 10.1088/1748-9326/8/2/024030 Google Scholar Calderón et al., 2013 R. Calderón,
    J.A. Navas-Cortés, C. Lucena, P.J. Zarco-Tejada High-resolution airborne hyperspectral
    and thermal imagery for early detection of Verticillium wilt of olive using fluorescence,
    temperature and narrow-band spectral indices Remote Sensing of Environment, 139
    (2013), pp. 231-245, 10.1016/j.rse.2013.07.031 View PDFView articleView in ScopusGoogle
    Scholar DeJonge et al., 2015 K.C. DeJonge, S. Taghvaeian, T.J. Trout, L.H. Comas
    Comparison of canopy temperature-based water stress indices for maize Agric. Water
    Manag., 156 (2015), pp. 51-62, 10.1016/j.agwat.2015.03.023 View PDFView articleView
    in ScopusGoogle Scholar Dogaru, 2016 D. Dogaru Crop water productivity under increasing
    irrigation capacities in Romania. A spatially-explicit assessment of winter wheat
    and maize cropping systems in the southern lowlands of the country EGU General
    Assembly 2016, vol. 18 (2016) Google Scholar Egea et al., 2017 G. Egea, C.M. Padilla-Díaz,
    J. Martinez-Guanter, J.E. Fernández, M. Pérez-Ruiz Assessing a crop water stress
    index derived from aerial thermal imaging and infrared thermometry in super-high
    density olive orchards Agric. Water Manag., 187 (2017), pp. 210-221, 10.1016/j.agwat.2017.03.030
    View PDFView articleView in ScopusGoogle Scholar Elvanidi et al., 2018 A. Elvanidi,
    N. Katsoulas, K.P. Ferentinos, T. Bartzanas, C. Kittas Hyperspectral machine vision
    as a tool for water stress severity assessment in soilless tomato crop Biosyst.
    Eng., 165 (2018), pp. 25-35, 10.1016/j.biosystemseng.2017.11.002 View PDFView
    articleView in ScopusGoogle Scholar Espinoza et al., 2017 C.Z. Espinoza, L.R.
    Khot, S. Sankaran, P.W. Jacoby High resolution multispectral and thermal remote
    sensing-based water stress assessment in subsurface irrigated grapevines Remote
    Sens. (Basel), 9 (9) (2017), 10.3390/rs9090961 Google Scholar Ford et al., 2017
    T. Ford, C. Hartman, A. Nagchaudhuri, M. Mitra, L. Marsh Analysis of yield response
    with deficit drip irrigation strategies, remote sensing with UAVS, and thermal
    image processing In ASABE Annual International Conference (2017), pp. 1-13, 10.1101/210732
    Google Scholar Gago et al., 2015 J. Gago, C. Douthe, R.E. Coopman, P.P. Gallego,
    M. Ribas-Carbo, J. Flexas, J. Escalona, H. Medrano UAVs challenge to assess water
    stress for sustainable agriculture Agric. Water Manag., 153 (2015), pp. 9-19,
    10.1016/j.agwat.2015.01.020 View PDFView articleView in ScopusGoogle Scholar García-Tejero
    et al., 2016 I.F. García-Tejero, J.M. Costa, R. Egipto, V.H. Durán-Zuazo, R.S.N.
    Lima, C.M. Lopes, M.M. Chaves Thermal data to monitor crop-water status in irrigated
    Mediterranean viticulture Agric. Water Manag., 176 (2016), pp. 80-90, 10.1016/j.agwat.2016.05.008
    View PDFView articleView in ScopusGoogle Scholar García-Tejero et al., 2018a I.F.
    García-Tejero, S. Gutiérrez-Gordillo, C. Ortega-Arévalo, M. Iglesias-Contreras,
    J.M. Moreno, L. Souza-Ferreira, V.H. Durán-Zuazo Thermal imaging to monitor the
    crop-water status in almonds by using the non-water stress baselines Sci. Hortic.,
    238 (February) (2018), pp. 91-97, 10.1016/j.scienta.2018.04.045 View PDFView articleView
    in ScopusGoogle Scholar García-Tejero et al., 2018b I.F. García-Tejero, C.J. Ortega-Arévalo,
    M. Iglesias-Contreras, J.M. Moreno, L. Souza, S.C. Tavira, V.H. Durán-Zuazo Assessing
    the crop-water status in almond (Prunus dulcis mill.) trees via thermal imaging
    camera connected to smartphone Sensors (Switzerland), 18 (4) (2018), pp. 1-13,
    10.3390/s18041050 View in ScopusGoogle Scholar Gendron et al., 2018 L. Gendron,
    G. Létourneau, L. Anderson, G. Sauvageau, C. Depardieu, E. Paddock, A. Hout, R.
    Levallois, O. Daugovish, S.S. Solis, J. Caron Real-time irrigation: Cost-effectiveness
    and benefits for water use and productivity of strawberries Sci. Hortic., 240
    (March) (2018), pp. 468-477, 10.1016/j.scienta.2018.06.013 View PDFView articleView
    in ScopusGoogle Scholar Gerhards et al., 2016 M. Gerhards, G. Rock, M. Schlerf,
    T. Udelhoven Water stress detection in potato plants using leaf temperature, emissivity,
    and reflectance Int. J. Appl. Earth Obs. Geoinf., 53 (2016), pp. 27-39, 10.1016/j.jag.2016.08.004
    View PDFView articleView in ScopusGoogle Scholar Gómez-Bellot et al., 2015 M.J.
    Gómez-Bellot, P.A. Nortes, M.J. Sánchez-Blanco, M.F. Ortuño Sensitivity of thermal
    imaging and infrared thermometry to detect water status changes in Euonymus japonica
    plants irrigated with saline reclaimed water Biosyst. Eng., 133 (2015), pp. 21-32,
    10.1016/j.biosystemseng.2015.02.014 View PDFView articleView in ScopusGoogle Scholar
    Gonzalez-Dugo et al., 2012 V. Gonzalez-Dugo, P. Zarco-Tejada, J.A.J. Berni, L.
    Suárez, D. Goldhamer, E. Fereres Almond tree canopy temperature reveals intra-crown
    variability that is water stress-dependent Agricultural and Forest Meteorology,
    154–155 (2012), pp. 156-165, 10.1016/j.agrformet.2011.11.004 View PDFView articleView
    in ScopusGoogle Scholar Gonzalez-Dugo et al., 2013 V. Gonzalez-Dugo, P. Zarco-Tejada,
    E. Nicolás, P.A. Nortes, J.J. Alarcón, D.S. Intrigliolo, E. Fereres Using high
    resolution UAV thermal imagery to assess the variability in the water status of
    five fruit tree species within a commercial orchard Precision Agriculture, 14
    (6) (2013), pp. 660-678, 10.1007/s11119-013-9322-9 View in ScopusGoogle Scholar
    Gonzalez-Dugo et al., 2014 V. Gonzalez-Dugo, P.J. Zarco-Tejada, E. Fereres Applicability
    and limitations of using the crop water stress index as an indicator of water
    deficits in citrus orchards Agric. For. Meteorol., 198–199 (2014), pp. 94-104,
    10.1016/j.agrformet.2014.08.003 View PDFView articleView in ScopusGoogle Scholar
    Han et al., 2016 M. Han, H. Zhang, K.C. DeJonge, L.H. Comas, T.J. Trout Estimating
    maize water stress by standard deviation of canopy temperature in thermal imagery
    Agric. Water Manag., 177 (2016), pp. 400-409, 10.1016/j.agwat.2016.08.031 View
    PDFView articleView in ScopusGoogle Scholar Hirich et al., 2014 A. Hirich, H.
    Fahmi, A. Rami, K. Laajaj, S. Jacobsen, H.E.L. Omari Using deficit irrigation
    to improve crop water productivity of sweet corn, chickpea, faba bean and quinoa
    : a synthesis of several field trials Rev. Mar. Sci. Agron. Vét., 2 (1) (2014),
    pp. 15-22 Google Scholar Hoffmann et al., 2016 H. Hoffmann, R. Jensen, A. Thomsen,
    H. Nieto, J. Rasmussen, T. Friborg Crop water stress maps for an entire growing
    season from visible and thermal UAV imagery Biogeosciences, 13 (24) (2016), pp.
    6545-6563, 10.5194/bg-13-6545-2016 View in ScopusGoogle Scholar Humphreys et al.,
    2006 E. Humphreys, L.G. Lewin, S. Khan, H.G. Beecher, J.M. Lacy, J.A. Thompson,
    G.D. Batten, A. Brown, C.A. Russell, E.W. Christen, B.W. Dunn Integration of approaches
    to increasing water use efficiency in rice-based systems in southeast Australia
    Field Crops Res., 97 (1 SPEC. ISS) (2006), pp. 19-33, 10.1016/j.fcr.2005.08.02
    View PDFView articleView in ScopusGoogle Scholar Hunt et al., 2018 E.R. Hunt,
    D.A. Horneck, C.B. Spinelli, R.W. Turner, A.E. Bruce, D.J. Gadler, J.J. Brungardt,
    P.B. Hamm Monitoring nitrogen status of potatoes using small unmanned aerial vehicles
    Precis. Agric., 19 (2) (2018), pp. 314-333, 10.1007/s11119-017-9518-5 View in
    ScopusGoogle Scholar Igbadun et al., 2006 H.E. Igbadun, H.F. Mahoo, A.K.P.R. Tarimo,
    B.A. Salim Crop water productivity of an irrigated maize crop in Mkoji sub-catchment
    of the great Ruaha River Basin, Tanzania Agric. Water Manag., 85 (1–2) (2006),
    pp. 141-150, 10.1016/j.agwat.2006.04.003 View PDFView articleView in ScopusGoogle
    Scholar Ihuoma and Madramootoo, 2017 S.O. Ihuoma, C.A. Madramootoo Recent advances
    in crop water stress detection Comput. Electron. Agric., 141 (2017), pp. 267-275,
    10.1016/j.compag.2017.07.026 View PDFView articleView in ScopusGoogle Scholar
    Jiménez-Bello et al., 2011 M.A. Jiménez-Bello, C. Ballester, J.R. Castel, D.S.
    Intrigliolo Development and validation of an automatic thermal imaging process
    for assessing plant water status Agric. Water Manag., 98 (10) (2011), pp. 1497-1504,
    10.1016/j.agwat.2011.05.002 View PDFView articleView in ScopusGoogle Scholar Jin
    et al., 2016 X. Jin, G. Yang, Z. Li, X. Xu, J. Wang, Y. Lan Estimation of water
    productivity in winter wheat using the AquaCrop model with field hyperspectral
    data Precis. Agric., 19 (1) (2016), pp. 1-17, 10.1007/s11119-016-9469-2 Google
    Scholar Khanal et al., 2017 S. Khanal, J. Fulton, S. Shearer An overview of current
    and potential applications of thermal remote sensing in precision agriculture
    Comput. Electron. Agric., 139 (2017), pp. 22-32, 10.1016/j.compag.2017.05.001
    View PDFView articleView in ScopusGoogle Scholar Kukal and Irmak, 2017 M.S. Kukal,
    S. Irmak Spatial and temporal changes in maize and soybean grain yield, precipitation
    use efficiency, and crop water productivity in the U.S. Great Plains Trans. Asabe,
    60 (4) (2017), pp. 1189-1208, 10.13031/trans.12072 View in ScopusGoogle Scholar
    Kullberg et al., 2017 E.G. Kullberg, K.C. DeJonge, J.L. Chávez Evaluation of thermal
    remote sensing indices to estimate crop evapotranspiration coefficients Agric.
    Water Manag., 179 (2017), pp. 64-73, 10.1016/j.agwat.2016.07.007 View PDFView
    articleView in ScopusGoogle Scholar Léllis et al., 2017 B.C. Léllis, D.F. Carvalho,
    A. Martínez-Romero, J.M. Tarjuelo, A. Domínguez Effective management of irrigation
    water for carrot under constant and optimized regulated deficit irrigation in
    Brazil Agric. Water Manag., 192 (2017), pp. 294-305, 10.1016/j.agwat.2017.07.018
    View PDFView articleView in ScopusGoogle Scholar Loggenberg et al., 2018 K. Loggenberg,
    A. Strever, B. Greyling, N. Poona Modelling water stress in a Shiraz vineyard
    using hyperspectral imaging and machine learning Remote Sens. (Basel), 10 (2)
    (2018), pp. 1-14, 10.3390/rs10020202 Google Scholar Maes and Steppe, 2019 W.H.
    Maes, K. Steppe Perspectives for remote sensing with unmanned aerial vehicles
    in precision agriculture Trends Plant Sci., 24 (2) (2019), pp. 152-164, 10.1016/j.tplants.2018.11.007
    View PDFView articleView in ScopusGoogle Scholar Manfreda et al., 2018 S. Manfreda,
    M.F. McCabe, P.E. Miller, R. Lucas, V.P. Madrigal, G. Mallinis, E.B. Dor, D. Helman,
    L. Estes, G. Ciraolo, J. Müllerová, F. Tauro, M.I. Lima, J.L.M.P. Lima, A. Maltese,
    F. Frances, K. Caylor, M. Kohv, M. Perks, G. Ruiz-Pérez, Z. Su, G. Vico, B. Toth
    On the use of unmanned aerial systems for environmental monitoring Remote Sensing,
    10 (4) (2018), pp. 2-20, 10.3390/rs10040641 Google Scholar Mangus et al., 2016
    D.L. Mangus, A. Sharda, N. Zhang Development and evaluation of thermal infrared
    imaging system for high spatial and temporal resolution crop water stress monitoring
    of corn within a greenhouse Comput. Electron. Agric., 121 (2016), pp. 149-159,
    10.1016/j.compag.2015.12.007 View PDFView articleView in ScopusGoogle Scholar
    Martínez et al., 2017 J. Martínez, G. Egea, J. Agüera, M. Pérez-Ruiz A cost-effective
    canopy temperature measurement system for precision agriculture: a case study
    on sugar beet Precis. Agric., 18 (1) (2017), pp. 95-110, 10.1007/s11119-016-9470-9
    View in ScopusGoogle Scholar Matese and Di Gennaro, 2018 A. Matese, S. Di Gennaro
    Practical applications of a multisensor UAV platform based on multispectral, thermal
    and RGB high resolution images in precision viticulture Agriculture, 8 (7) (2018),
    p. 116, 10.3390/agriculture8070116 View in ScopusGoogle Scholar Matese et al.,
    2018 A. Matese, R. Baraldi, A. Berton, C. Cesaraccio, S.F. Di Gennaro, P. Duce,
    O. Facini, M.G. Mameli, A. Piga, A. Zaldei Estimation of Water Stress in grapevines
    using proximal and remote sensing methods Remote Sensing, 10 (1) (2018), pp. 1-16,
    10.3390/rs10010114 Google Scholar Mdemu et al., 2009 M.V. Mdemu, C. Rodgers, P.L.G.
    Vlek, J.J. Borgadi Water productivity (WP) in reservoir irrigated schemes in the
    upper east region (UER) of Ghana Phys. Chem. Earth, 34 (4–5) (2009), pp. 324-328,
    10.1016/j.pce.2008.08.006 View PDFView articleView in ScopusGoogle Scholar Mesas-Carrascosa
    et al., 2018 F.J. Mesas-Carrascosa, F. Pérez-Porras, J.E.M. de Larriva, C.M. Frau,
    F. Agüera-Vega, F. Carvajal-Ramírez, A. García-Ferrer Drift correction of lightweight
    microbolometer thermal sensors on-board unmanned aerial vehicles Remote Sensing,
    10 (4) (2018), pp. 1-17, 10.3390/rs10040615 Google Scholar Muchiri and Kimathi,
    2016 N. Muchiri, S. Kimathi A review of applications and potential applications
    of UAV 2016 Annual Conference on Sustainable Research and Innovation (2016), pp.
    280-283 Google Scholar Nhamo et al., 2016 L. Nhamo, T. Mabhaudhi, M. Magombeyi
    Improving water sustainability and food security through increased crop water
    productivity in Malawi Water, 8 (9) (2016), p. 411, 10.3390/w8090411 View in ScopusGoogle
    Scholar O’Shaughnessy and Evett, 2010 S.A. O’Shaughnessy, S.R. Evett Canopy temperature
    based system effectively schedules and controls center pivot irrigation of cotton
    Agric. Water Manag., 97 (9) (2010), pp. 1310-1316, 10.1016/j.agwat.2010.03.012
    View PDFView articleView in ScopusGoogle Scholar O’Shaughnessy et al., 2012 S.A.
    O’Shaughnessy, S.R. Evett, P.D. Colaizzi, T.A. Howell A crop water stress index
    and time threshold for automatic irrigation scheduling of grain sorghum Agric.
    Water Manag., 107 (2012), pp. 122-132, 10.1016/j.agwat.2012.01.018 View PDFView
    articleView in ScopusGoogle Scholar Osroosh et al., 2015 Y. Osroosh, R. Troy Peters,
    C.S. Campbell, Q. Zhang Automatic irrigation scheduling of apple trees using theoretical
    crop water stress index with an innovative dynamic threshold Comput. Electron.
    Agric., 118 (2015), pp. 193-203, 10.1016/j.compag.2015.09.006 View PDFView articleView
    in ScopusGoogle Scholar Osroosh et al., 2016 Y. Osroosh, R.T. Peters, C.S. Campbell,
    Q. Zhang Comparison of irrigation automation algorithms for drip-irrigated apple
    trees Comput. Electron. Agric., 128 (2016), pp. 87-99, 10.1016/j.compag.2016.08.013
    View PDFView articleView in ScopusGoogle Scholar Park et al., 2017 S. Park, D.
    Ryu, S. Fuentes, H. Chung, E. Hernández-Montes, M. O’Connell Adaptive estimation
    of crop water stress in nectarine and peach orchards using high-resolution imagery
    from an unmanned aerial vehicle (UAV) Remote Sensing, 9 (8) (2017), 10.3390/rs9080828
    Google Scholar Poblete et al., 2017 T. Poblete, S. Ortega-Farías, M. Moreno, M.
    Bardeen Artificial neural network to predict vine water status spatial variability
    using multispectral information obtained from an unmanned aerial vehicle (UAV)
    Sensors, 17 (11) (2017), p. 2488, 10.3390/s17112488 View in ScopusGoogle Scholar
    Pôças et al., 2017 I. Pôças, J. Gonçalves, P.M. Costa, I. Gonçalves, L.S. Pereira,
    M. Cunha Hyperspectral-based predictive modelling of grapevine water status in
    the Portuguese Douro wine region Int. J. Appl. Earth Obs. Geoinf., 58 (2017),
    pp. 177-190, 10.1016/j.jag.2017.02.013 View PDFView articleView in ScopusGoogle
    Scholar Pou et al., 2014 A. Pou, M.P. Diago, H. Medrano, J. Baluja, J. Tardaguila
    Validation of thermal indices for water status identification in grapevine Agric.
    Water Manag., 134 (2014), pp. 60-72, 10.1016/j.agwat.2013.11.010 View PDFView
    articleView in ScopusGoogle Scholar Quebrajo et al., 2018 L. Quebrajo, M. Perez-Ruiz,
    L. Perez-Urrestarazu, G. Martı´nez, G. Egea Linking thermal imaging and soil remote
    sensing to enhance irrigation management of sugar beet Biosyst. Eng., 165 (2018),
    pp. 77-87, 10.1016/j.biosystemseng.2017.08.013 View PDFView articleView in ScopusGoogle
    Scholar Rallo et al., 2014 G. Rallo, M. Minacapilli, G. Ciraolo, G. Provenzano
    Detecting crop water status in mature olive groves using vegetation spectral measurements
    Biosyst. Eng., 128 (2014), pp. 52-68, 10.1016/j.biosystemseng.2014.08.012 View
    PDFView articleView in ScopusGoogle Scholar Rapaport et al., 2015 T. Rapaport,
    U. Hochberg, M. Shoshany, A. Karnieli, S. Rachmilevitch Combining leaf physiology,
    hyperspectral imaging and partial least squares-regression (PLS-R) for grapevine
    water status assessment Isprs J. Photogramm. Remote. Sens., 109 (2015), pp. 88-97,
    10.1016/j.isprsjprs.2015.09.003 View PDFView articleView in ScopusGoogle Scholar
    Romero et al., 2018 M. Romero, Y. Luo, B. Su, S. Fuentes Vineyard water status
    estimation using multispectral imagery from an UAV platform and machine learning
    algorithms for irrigation scheduling management Comput. Electron. Agric., 147
    (February) (2018), pp. 109-117, 10.1016/j.compag.2018.02.013 View PDFView articleView
    in ScopusGoogle Scholar Sakthivadivel et al., 1999 R. Sakthivadivel, C. De Fraiture,
    D.J. Molden, C. Perry, W. Kloezen Indicators of land and water productivity in
    irrigated agriculture Int. J. Water Resour. Dev., 15 (1–2) (1999), pp. 161-179,
    10.1080/07900629948998 View in ScopusGoogle Scholar Santesteban et al., 2017 L.G.
    Santesteban, S.F. Di Gennaro, A. Herrero-Langreo, C. Miranda, J.B. Royo, A. Matese
    High-resolution UAV-based thermal imaging to estimate the instantaneous and seasonal
    variability of plant water status within a vineyard Agric. Water Manag., 183 (2017),
    pp. 49-59, 10.1016/j.agwat.2016.08.026 View PDFView articleView in ScopusGoogle
    Scholar Schrijver, 2016 R. Schrijver Precision Agriculture and the Future of Farming
    in Europe Science and Technology Options Assessment, Brussels (2016), 10.2861/020809
    Google Scholar Sepúlveda-Reyes et al., 2016 D. Sepúlveda-Reyes, B. Ingram, M.
    Bardeen, M. Zúñiga, S. Ortega-Farías, C. Poblete-Echeverría Selecting canopy zones
    and thresholding approaches to assess grapevine water status by using aerial and
    ground-based thermal imaging Remote Sensing, 8 (10) (2016), 10.3390/rs8100822
    Google Scholar Simelli and Apostolos, 2015 I. Simelli, T. Apostolos The use of
    unmanned aerial systems (UAS) in agriculture Proceedings of the 7th International
    Conference on Infromation and Communication Technologies in Agriculture, Food
    and Evironment (HAICTA 2015) (2015), pp. 17-20, 10.1214/07-EJS057 Google Scholar
    Sun et al., 2017 S. Sun, C.F. Zhang, X. Li, T. Zhou, Y. Wang, P. Wu, H. Cai Sensitivity
    of crop water productivity to the variation of agricultural and climatic factors:
    a study of Hetao irrigation district, China J. Clean. Prod., 142 (2017), pp. 2562-2569,
    10.1016/j.jclepro.2016.11.020 View PDFView articleView in ScopusGoogle Scholar
    Susič et al., 2018 N. Susič, U. Žibrat, S. Širca, P. Strajnar, J. Razinger, M.
    Knapič, A. Vončina, G. Urek, B. Gerič Stare Discrimination between abiotic and
    biotic drought stress in tomatoes using hyperspectral imaging Sens. Actuators
    B: Chem., 273 (2018), pp. 842-852, 10.1016/j.snb.2018.06.121 View PDFView articleView
    in ScopusGoogle Scholar Torres-Rua, 2017 A. Torres-Rua Drones in agriculture:
    an overview of current capabilities and future directions Paper Prepared for the
    2017 Utah Water Users Workshop (2017) Saint George, UT, 1–9. Retrieved from https://conference.usu.edu/uwuw/includes/AlfonsoTorres_DronesinAgriculture.pdf?v=1.22
    Google Scholar Tshwene and Oladele, 2016 C. Tshwene, I. Oladele Water use productivity
    and food security among smallholder homestead food gardening and irrigation crop
    farmers in North West province, South Africa Journal of Agriculture and Environment
    for International Development, 110 (1) (2016), pp. 73-86, 10.12895/jaeid.20161.399
    View in ScopusGoogle Scholar Whitehead and Hugenholtz, 2014 K. Whitehead, C. Hugenholtz
    Remote Sensing of the Environment with Small Unmanned Aircraft Systems (UASs),
    Part 1: a review of progress and challenges J. Unmanned Veh. Syst., 2 (3) (2014),
    pp. 69-85, 10.1139/juvs-2014-0006 View in ScopusGoogle Scholar Whitehead et al.,
    2014 K. Whitehead, C.H. Hugenholtz, S. Myshak, O. Brown, A. LeClair, A. Tamminga,
    T.E. Barchyn, M. Brian, B. Eaton Remote sensing of the environment with small
    unmanned aircraft systems (UASs), part 2: scientific and commercial applications
    Journal of Unmanned Vehicle Systems, 02 (03) (2014), pp. 86-102, 10.1139/juvs-2014-0006
    View in ScopusGoogle Scholar Wichelns, 2014 D. Wichelns Do estimates of water
    productivity enhance understanding of farm-level water management? Water (Switzerland),
    6 (4) (2014), pp. 778-795, 10.3390/w6040778 View in ScopusGoogle Scholar Zarco-Tejada
    et al., 2009 P.J. Zarco-Tejada, J.A.J. Berni, L. Suárez, G. Sepulcre-Cantó, F.
    Morales, J.R. Miller Imaging chlorophyll fluorescence with an airborne narrow-band
    multispectral camera for vegetation stress detection Remote Sensing of Environment,
    113 (6) (2009), pp. 1262-1275, 10.1016/j.rse.2009.02.016 View PDFView articleView
    in ScopusGoogle Scholar Zarco-Tejada et al., 2012 P.J. Zarco-Tejada, V. González-Dugo,
    J.A.J. Berni Fluorescence, temperature and narrow-band indices acquired from a
    UAV platform for water stress detection using a micro-hyperspectral imager and
    a thermal camera Remote Sens. Environ., 117 (2012), pp. 322-337, 10.1016/j.rse.2011.10.007
    View PDFView articleView in ScopusGoogle Scholar Zarco-Tejada et al., 2013 P.J.
    Zarco-Tejada, V. González-Dugo, L.E. Williams, L. Suárez, J.A.J. Berni, D. Goldhamer,
    E. Fereres A PRI-based water stress index combining structural and chlorophyll
    effects: assessment using diurnal narrow-band airborne imagery and the CWSI thermal
    index Remote Sens. Environ., 138 (2013), pp. 38-50, 10.1016/j.rse.2013.07.024
    View PDFView articleView in ScopusGoogle Scholar Zhao et al., 2017 T. Zhao, B.
    Stark, Y.Q. Chen, A.L. Ray, D. Doll Challenges in water stress quantification
    using small unmanned aerial system (sUAS): lessons from a growing season of almond
    Journal of Intelligent and Robotic Systems: Theory and Applications, 88 (2–4)
    (2017), pp. 721-735, 10.1007/s10846-017-0513-x View in ScopusGoogle Scholar Zhou
    et al., 2018 J. Zhou, L.R. Khot, R.A. Boydston, P.N. Miklas, L. Porter Low altitude
    remote sensing technologies for crop stress monitoring: a case study on spatial
    and temporal monitoring of irrigated pinto bean Precis. Agric., 19 (3) (2018),
    pp. 1-15, 10.1007/s11119-017-9539-0 Google Scholar Zwart and Bastiaanssen, 2004
    S.J. Zwart, W.G.M. Bastiaanssen Review of measured crop water productivity values
    for irrigated wheat, rice, cotton and maize Agric. Water Manag., 69 (2) (2004),
    pp. 115-133, 10.1016/j.agwat.2004.04.007 View PDFView articleView in ScopusGoogle
    Scholar Cited by (56) Response of net water productivity to climate and edaphic
    moisture in wheat-maize rotation system 2024, Soil and Tillage Research Show abstract
    A robust model for diagnosing water stress of winter wheat by combining UAV multispectral
    and thermal remote sensing 2024, Agricultural Water Management Show abstract Diagnosis
    of winter-wheat water stress based on UAV-borne multispectral image texture and
    vegetation indices 2021, Agricultural Water Management Citation Excerpt : Stomatal
    conductance (Gs) is an important index to characterize crop water stress. Inversion
    of crop Gs by UAV remote sensing can avoid such problems as low efficiency, high
    cost and difficult field operation (Ezenne et al., 2019). This method has been
    proved to be effective in the detection of crop water stress. Show abstract Evaluation
    of canopy fraction-based vegetation indices, derived from multispectral UAV imagery,
    to map water status variability in a commercial vineyard 2024, Irrigation Science
    Latest Trends and Challenges in Digital Agriculture for Crop Production 2023,
    SSRN Optimization of Chickpea Irrigation in a Semi-Arid Climate Based on Morpho-Physiological
    Parameters 2023, SSRN View all citing articles on Scopus View Abstract © 2019
    Elsevier B.V. All rights reserved. Recommended articles Application of deficit
    irrigation in Phillyrea angustifolia for landscaping purposes Agricultural Water
    Management, Volume 218, 2019, pp. 193-202 S. Álvarez, …, M.J. Sánchez-Blanco View
    PDF Variation of microorganisms in drip irrigation systems using high-sand surface
    water Agricultural Water Management, Volume 218, 2019, pp. 37-47 Bo Zhou, …, Ji
    Feng View PDF Simulation of cotton growth and soil water content under film-mulched
    drip irrigation using modified CSM-CROPGRO-cotton model Agricultural Water Management,
    Volume 218, 2019, pp. 124-138 Meng Li, …, Shaoming Chen View PDF Show 3 more articles
    Article Metrics Citations Citation Indexes: 53 Captures Readers: 162 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Agricultural Water Management
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Current and potential capabilities of UAS for crop water productivity in
    precision agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agriengineering2030029
  analysis: '>'
  authors:
  - Zongmei Gao
  - Zhongwei Luo
  - Wen Zhang
  - Zhenzhen Lv
  - Yanlei Xu
  citation_count: 51
  full_citation: '>'
  full_text: ">\nAgriEngineering\nReview\nDeep Learning Application in Plant Stress\
    \ Imaging:\nA Review\nZongmei Gao 1\n, Zhongwei Luo 2, Wen Zhang 2,*\n, Zhenzhen\
    \ Lv 2,* and Yanlei Xu 3\n1\nDepartment of Biological Systems Engineering, Center\
    \ for Precision and Automated Agricultural Systems,\nWashington State University,\
    \ Prosser, WA 99350, USA; zongmei.gao@wsu.edu\n2\nSchool of Life Science and Engineering,\
    \ Southwest University of Science and Technology,\nMianyang 621010, China; zhongweiluo593@gmail.com\n\
    3\nCollege of Information and Technology, JiLin Agricultural University, Changchun\
    \ 130118, China;\nyanleixu@jlau.edu.cn\n*\nCorrespondence: wenzhang@swust.edu.cn\
    \ (W.Z.); zhenzhenlv139@gmail.com (Z.L.);\nTel.: +86-816-6089521 (W.Z.)\nReceived:\
    \ 19 May 2020; Accepted: 7 July 2020; Published: 14 July 2020\n\x01\x02\x03\x01\
    \x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\nAbstract: Plant stress is one\
    \ of major issues that cause signiﬁcant economic loss for growers.\nThe labor-intensive\
    \ conventional methods for identifying the stressed plants constrain their\napplications.\
    \ To address this issue, rapid methods are in urgent needs. Developments of advanced\n\
    sensing and machine learning techniques trigger revolutions for precision agriculture\
    \ based on deep\nlearning and big data. In this paper, we reviewed the latest\
    \ deep learning approaches pertinent\nto the image analysis of crop stress diagnosis.\
    \ We compiled the current sensor tools and deep\nlearning principles involved\
    \ in plant stress phenotyping. In addition, we reviewed a variety of deep\nlearning\
    \ applications/functions with plant stress imaging, including classiﬁcation, object\
    \ detection,\nand segmentation, of which are closely intertwined. Furthermore,\
    \ we summarized and discussed the\ncurrent challenges and future development avenues\
    \ in plant phenotyping.\nKeywords: deep learning; convolutional neural network;\
    \ crop stress; precision phenotyping\n1. Plant Stress and Sensors\nPlant stress\
    \ is one of the major threats to crops causing signiﬁcant reduction of crop yield\n\
    and quality [1]. The detection and diagnosis of the plant stress is urgently needed\
    \ for rapid and\nrobust application of precision agriculture in crop measurement.\
    \ Presently, intensive studies focus\non developing optical imaging methods for\
    \ plant disease detection. Diﬀerent from the conventional\nmethods using the visual\
    \ scoring, optical imaging is advanced to measure changes caused by abiotic\n\
    or biotic stressors in the plant physiology rapidly and without contact. In general,\
    \ the common\nimaging technologies have been employed for detecting the crop stress,\
    \ including digital, ﬂuorescence,\nthermography, LIDAR, multispectral, and hyperspectral\
    \ imaging techniques [2]. The common optical\nsensors used for plant stress detection\
    \ are shown in Figure 1.\nAgriEngineering 2020, 2, 430–446; doi:10.3390/agriengineering2030029\n\
    www.mdpi.com/journal/agriengineering\nAgriEngineering 2020, 2\n431\nFigure 1.\
    \ Typical optical sensors used for plant stress detection. (a) Digital sensor\
    \ for maize heat\nstress [3]; (b) multispectral imaging sensor for maize water\
    \ stress [4]; (c) ﬂuorescence imaging sensor\nfor chilling injury of tomato seedlings\
    \ [5]; (d) thermal imaging sensor for potato water stress [6], and (e)\nhyperspectral\
    \ imaging sensor for apple water stress [7].\nDigital imaging sensors acquire\
    \ the visible range of wavelengths, i.e., RGB colored images with red,\nblue,\
    \ and green channels to detect plant diseases. Such images provide physical attributes\
    \ of the plants,\nsuch as canopy vigor, leaf color, leaf texture, size, and shape\
    \ information [8]. Color and texture features\nare important for identifying the\
    \ characteristic diﬀerence between healthy and symptomatic plants.\nFrequently\
    \ used color features are RGB, LAB, YCBCR, and HSV spaces [9]. Additionally, contrast,\n\
    homogeneity, dissimilarity, energy, and entropy features of images are descriptive\
    \ facets of texture [10].\nIn other words, quantitative diagnosis features for\
    \ identifying the symptomatic and healthy plants\nhave been collected in these\
    \ images.\nThermal imaging sensors obtain infrared radiating images ranging from\
    \ 8 to 12 µm, which are\noften applied for predicting plant temperatures. Under\
    \ the infection, the temperature of infected plant\ntissues varies and related\
    \ to the impacts caused by pathogens. The temperature variance, other the\nhand,\
    \ appears with a counter-eﬀect on transpiration rate [11]. In other words, stress\
    \ from the infection\ntrigger both transpiration rate decrease and leaf temperature\
    \ increase, resulting in stomatal closure\nin plants. In turn, based on these\
    \ alterations, thermal imaging sensors could identify the infection\ndiseases.\
    \ Each pixel of the thermal image represents the temperature value of the object,\
    \ which is\nexpressed in manners of false color. In plant disease detection, the\
    \ thermal sensor could be mounted to\nground automated vehicles (GAV) and unmanned\
    \ aerial vehicles (UAV).\nFluorescence imaging sensors are often utilized to identify\
    \ variations of plant photosynthetic\nactivity [12]. The diﬀerences of stressed\
    \ and healthy leaves will be expressed in the diﬀerences of\nphotosynthetic activities,\
    \ which will be assessed by the photosynthetic electron transform using the\n\
    ﬂuorescence imaging sensor with an LED or laser illumination. For normal cases,\
    \ 685 nm is the\nAgriEngineering 2020, 2\n432\nwavelength at which chlorophyll\
    \ ﬂuorescence is emitted from photo-system II (PSII). The stressed\nplants could\
    \ change the patterns of chlorophyll ﬂuorescence emission, which could be reﬂected\
    \ and\nobserved in the ﬂuorescence imaging [13].\nBased on the number of spectral\
    \ bands in the optical sensing technologies, sensors contain 3–10 spectral\nbands\
    \ are named multispectral imaging sensors. The multispectral imaging sensors normally\
    \ extract a few\nor a stack of images from the visible to near-infrared spectrum\
    \ [14]. Plant stress often causes an increase in\nvisible reflectance, with a\
    \ decrease in chlorophyll and absorption of visible light. Additionally, reduced\n\
    near infrared (NIR) reflectance will happen due to changes of the leaf tissue.\
    \ Thus, the most used band\nchannels are green, red, red-edge and NIR. Multispectral\
    \ imaging sensor combined with drones have been\napplied broadly in remote sensing\
    \ for plant disease detection [15], while this type of sensors is limited to a\n\
    few spectral bands and sometimes cannot quantify the diseased plants severity.\n\
    Despite many successful studies having been applied to crop stress detection using\
    \ cheap\npassive imagery sensors, i.e., digital and near infrared (NIR), most\
    \ of the applications require fast\nimage processing and computational algorithms\
    \ for image analysis. Among the image analysis\ntechniques, supervised methods\
    \ have been popular with training data being used to develop a system.\nSuch methods\
    \ include shape segmentation, feature extraction, and classiﬁers for stress diagnosis.\n\
    In addition, machine learning algorithms search for the optimal decision boundary\
    \ in the feature space\nwith high dimensionality, which provides the basis for\
    \ many available image analysis systems [16].\nFor improving the image analysis\
    \ systems, deep learning has played a key role. Deep neural networks\nhave many\
    \ layers which transform input images to outputs (i.e., healthy or stressed) with\
    \ learning deep\nfeatures. The most applied networks are convolutional neural\
    \ networks (CNNs) in crop image analysis.\nCNNs consist of dozens or hundreds\
    \ of layers that process the images with convolution filters with a\nrespective\
    \ small size of batches [17]. Despite such initial successes, CNNs cannot collect\
    \ momentum\nwithout the advances in core computing systems and deep convolutional\
    \ networks become the current\nfocus. In agriculture, deep learning shows accepted\
    \ performance considering accuracy and efficiency based\non large datasets. To\
    \ build precise classifiers for improving plant disease diagnosis, the PlantVillage\
    \ project\n(https://plantvillage.psu.edu/posts/6948-plantvillage-dataset-download)\
    \ has obtained a large number of\nimages of healthy and diseased crops for free\
    \ [18]. Combined with the big data, deep learning has been put\nforwarded as the\
    \ future promising method in plant phenotyping [19]. For example, CNNs can effectively\n\
    detect and diagnose plant diseases [20] and classify plant fruits in the field\
    \ [21]. The promising results\npromote studies carrying out other phenotyping\
    \ tasks using deep learning, such as leaf morphological\nclassification [22].\
    \ Thus, we read many references about the utilization of deep learning in image-based\n\
    crop stress detection. Summarizing, with this paper we aim to:\n1.\nState the\
    \ principle of deep learning in the application for crop stress diagnosis based\
    \ on images.\n2.\nSearch for the challenges of deep learning in crop stress imaging.\n\
    3.\nHighlight the future directions that could be helpful for circumventing the\
    \ challenges in plant\nphenotyping tasks.\n2. Deep Learning Principle\n2.1. Machine\
    \ Learning\nMachine learning is a subset of artiﬁcial intelligence which is used\
    \ to operate speciﬁc tasks by\ncomputer systems [23]. In general, it is split\
    \ into supervised and unsupervised learning methods.\nSupervised learning methods\
    \ are expressed with an input matrix of independent x and dependent\ny variables.\
    \ This dependent variable of y has few formats, varying based on solving problems.\n\
    For classiﬁcation issues, y is usually a scalar for representing the category\
    \ labels, and it is a vector\ncontaining continuous values under regression [24].\
    \ Under segmented learning conditions, y is\nsometimes the ground truth label\
    \ image [25]. Supervised learning methods often aim to ﬁnd optimal\nmodel parameters,\
    \ which could predict the data to the greatest extent based on the loss function.\n\
    AgriEngineering 2020, 2\n433\nUnsupervised learning methods operate data processing\
    \ without dependent labels and aim to\nsearch for patterns (e.g., latent variables).\
    \ Common unsupervised learning methods include principal\ncomponent analysis (PCA),\
    \ k-nearest neighbors clustering, and T-distributed stochastic neighbor\nembedding\
    \ clustering [26]. Unsupervised training usually uses many diﬀerent loss functions\
    \ to process,\nsuch as reconstructing the loss function. The model must learn\
    \ to reconstruct the loss function in a\nsmaller dimension to reconstruct the\
    \ input data [27].\n2.2. Neural Network\nA neural network is built to recognize\
    \ patterns and provides the basis for most deep learning\nalgorithms [28]. A neural\
    \ network contains nodes that integrate input data with a set of coeﬃcients\n\
    and weights with amplify or dampen the input for learning the assigned tasks,\
    \ e.g., the common\nactivation function α and parameters Θ = \bw, β\t, here, w\
    \ represents the weights and β represents the\nbiases. An activation function\
    \ is normally followed by an elemental nonlinear factor/coeﬃcient σ, as a\ntransfer\
    \ function, as shown in Equation (1) [28]:\nα = σ\n\x10\nWTx + b\n\x11\n(1)\n\
    Sigmoidal and hyperbolic tangent functions are the common transfer functions for\
    \ neural networks.\nThe multilayer perceptron (MLP) is the most popular one in\
    \ traditional neural networks, with few\nconversion layers [28]:\nf(x; Θ) = σ\n\
    \x10\nWLσ\n\x10\nWL−1 . . . σ\n\x10\nW0x + b0\x11\n+ bL−1\x11\n+ bL\x11\n(2)\n\
    where WL is a matrix containing rows wk that is related with activation k in the\
    \ output, and L is the ﬁnal\nlayer. The so-called hidden layers are the layers\
    \ between input and output layers. A neural network\nwith many layers is often\
    \ called deep neural network (DNN), thence deep learning. The activation of\n\
    the last layer is mapped to distribution on the class P (y|x; Θ) through a softmax\
    \ function [28]:\nP\n\x10\ny\n\f\f\fx; Θ\n\x11\n= softmax (x; Θ) =\ne(WL\ni )Tx+bL\n\
    i\nPK\nk=1 e(WL\nk )Tx+bL\nk\n(3)\nwhere WL\ni is the weight vector associated\
    \ with class i to the output node. The typical diagram of deep\nneural network\
    \ MLP is shown in Figure 2.\nFigure 2. Typical architectures of deep neural network\
    \ used in imaging analysis. (a) autoencoder and\n(b) convolutional neural network.\n\
    AgriEngineering 2020, 2\n434\nCurrently, stochastic gradient descent (SGD) is\
    \ the famous method for ﬁtting the parameter Θ\nto process a small population\
    \ dataset. With SGD, a small batch is employed in each gradient and\nmaximum likelihood\
    \ optimization is used to minimize the negative impact of the log-likelihood.\n\
    It tracks the log loss for a binary classiﬁcation task and the softmax loss for\
    \ multiclass classiﬁcation.\nA disadvantage of this method is that it usually\
    \ does not directly optimize the quantity of interest [28].\nDNN became popular\
    \ in 2016, when it performed layer-by-layer training (pre-training) in an\nunsupervised\
    \ manner, and then supervised and ﬁne-tuned the stacked network to obtain good\n\
    performance. Such a DNN architecture includes a stacked autoencoder (SAE) and\
    \ a deep summary\nnetwork (DBN). However, such methods are often complex, which\
    \ need a great deal of engineering\nto obtain acceptable results [28,29]. Recently,\
    \ end-to-end training has been conducted on popular\narchitectures in a supervised\
    \ manner by streamlining the training procedure. The common architectures\nare\
    \ CNN and recurrent neural network (RNN) [30,31]. CNN has been widely used for\
    \ image analysis,\nand RNN is becoming more and more popular.\n2.3. Convolutional\
    \ Neural Network\nThe main diﬀerence between MLP and CNN is reﬂected in two aspects.\
    \ First, weights of the\nCNN architecture are shared with a network when the architecture\
    \ operates convolutions on the input\nimage [32]. In this way, separate detector\
    \ learning is not required for the same object appearing at\ndiﬀerent locations\
    \ in the image. As a result, the network is equally variable in the translation\
    \ of input\nimages. In addition, the number of parameters to be learned is reduced.\n\
    During CNN training, the input images are convolved with a set of K kernels W\
    \ =\n\bW1, W2, W3, . . . WK\n\t and biases β =\nn\nb1, . . . , bK\no\nin the convolution\
    \ layer, yielding a new feature\nmap Xk. Such features are exposed to a nonlinear\
    \ transformation parameter σ and such process would\nrepeat for each respective\
    \ convolutional layer l [32]:\nXl\nk = σ\n\x10\nWl−1\nk\n× Xl−1 + bl−1\nk\n\x11\
    \n(4)\nSecond, the main diﬀerence between MLP and CNN is the pooling layer. In\
    \ such layers, the pixels\nof the neighborhood are added based on the permutation\
    \ invariant function in CNN. This may prompt\na certain amount of rendering invariance\
    \ [33]. Then, the fully connected layers are usually added\nwith constant weights\
    \ after convolutional processing. Then, the softmax function is used to provide\n\
    activation information in the last layer, resulting in a category assignment.\
    \ A typical CNN architecture\nis shown in Figure 3 for identifying the ripeness\
    \ of strawberry based on hyperspectral imagery [34].\nFigure 3. One typical CNN\
    \ architecture for estimating the ripeness of strawberry based on hyperspectral\n\
    imagery [34]. Note: Conv represents convolutional layer; FC: fully connected layer.\n\
    AgriEngineering 2020, 2\n435\n2.4. CNN Architecture\nCNN normally uses a 2D image\
    \ as input, with a format of m × n × 3 (m × n × 1 for greyscale\nimages), where\
    \ m and n are the respective image height and width, and 3 is the number of image\n\
    channels. The CNN architecture often contains a few diﬀerent layers, including\
    \ convolutional layers,\npooling layers, and fully connected layers. The convolutional\
    \ and pooling layers are initial layers. A set\nof convolutional kernels (also\
    \ called ﬁlters) is used for each layer performing multiple transformations.\n\
    The convolution operations extract the associated features from small slices divided\
    \ from the full image.\nEach kernel is applied to the input slice and the output\
    \ of each kernel is applied to non-linear processing\nunits, making it capable\
    \ of learning abstraction and embedding non-linearity in the feature space [35].\n\
    The non-linear processing provides diﬀerent patterns of activations corresponding\
    \ to diﬀerent responses,\nwhich helps learn the semantic diﬀerences over the full\
    \ image. Then, the subsampling is applied to the\noutput of non-linear processing,\
    \ with summarizing the results and making the input insensitive to\nthe geometric\
    \ deformation [36]. The CNN architecture has been applied to many aspects, including\n\
    classiﬁcation, segmentation, and object detection, etc.\n2.4.1. Classiﬁcation\
    \ Architectures\nAmong the pre-trained networks, AlexNet is commonly used for\
    \ images classiﬁcation, which is\nrelatively simple with ﬁve convolutional layers.\
    \ The activation function of AlexNet is the hyperbolic\ntangent, which is the\
    \ most common choice in CNNs [37]. Then, the deep pre-trained networks appeared,\n\
    such as the VGG19 with 19 deep layers, winning the ImageNet challenge of 2014\
    \ [38]. These deeper\nnetworks use smaller stacked kernels and have lower memory\
    \ during inference, which improves\nthe performance of mobile computing devices,\
    \ such as smartphones [39]. Later, in 2015, the ResNet\narchitecture won the ImageNet\
    \ challenge and was made up of the ResNet blocks. The residual blocks\nlearn the\
    \ residuals and pre-processes the learning mapping for each layer, thereby providing\
    \ eﬀective\ntraining performance for deeper architectures. Szegedy et al. (2016)\
    \ developed a 22-layer neural\nnetwork referred as GoogLeNet, which employed the\
    \ inception blocks [40]. The advantage of using the\ninception blocks is that\
    \ it could increase the training process eﬃciency while decreasing the number\n\
    of parameters. The performance on ImageNet reached saturation after 2014 and crediting\
    \ the better\nperformance to the more complex architectures is biased. On the\
    \ other hand, it is not necessary\nto perform plant stress detection with the\
    \ deeper networks, providing a lower memory footprint.\nTherefore, AlexNet or\
    \ other relatively simple methods, such as VGG16, are still practical for crop\n\
    stress images.\n2.4.2. Segmentation Architectures\nSegmentation is important in\
    \ crop stress image analysis. The pixel in the image could be classiﬁed\nby the\
    \ CNN and the classiﬁed pixel could be presented with patches that extracted from\
    \ neighboring\npixels [41]. The disadvantage of this method is that the input\
    \ patches overlap, and the same convolution\nis repeatedly calculated. Fortunately,\
    \ the linear operators (convolution and dot product) can be written\nas convolutions\
    \ [42]. With a fully connected layer, a CNN can have a larger input image than\
    \ the\ntrained image and can generate a likelihood map instead of the output of\
    \ a pixel. Then, such a full\nconvolutional network can be eﬀectively applied\
    \ to the full input image.\n2.5. Hardware and Software\nThe dramatic increase\
    \ of deep learning applications could be due to the widespread development\nof\
    \ GPUs [43]. GPU computing started when NVIDIA launched CUDA (Computing Uniﬁed\
    \ Device\nArchitecture) and AMD launched Stream. The GPU is a highly parallel\
    \ computing engine which\noﬀers a great advantage compared with a central processing\
    \ unit (CPU). The Open Computing\nLanguage (OpenCL) uniﬁes diﬀerent GPU general\
    \ computing application programming interface\n(API) implementations and provides\
    \ a framework that can be used to write programs that execute on\nAgriEngineering\
    \ 2020, 2\n436\nheterogeneous platforms composed of a CPU and GPU. With the hardware,\
    \ deep learning on the GPU\nis much faster than on the CPU [44].\nOpen source\
    \ software packages also promote the development and application of deep learning.\n\
    These software packages allow users to operate the computing at a high level without\
    \ having to worry\nabout eﬃcient implementation. By far the most popular packages\
    \ include:\nCaﬀé, which oﬀers C++ and python interfaces, developed by graduate\
    \ students at UC Berkeley\nAI Research.\nTensorFlow, which provides C++ and python\
    \ interfaces, developed by Google Brain team.\nTheano, which provides a python\
    \ interface, developed by MILA lab in Montreal.\nPyTorch, which provides C++ and\
    \ python interface, developed by Facebook’s AI Research lab.\n3. Applications\
    \ of Deep Learning in Plant Stress Imaging\n3.1. Classiﬁcation\nDeep learning\
    \ has been applied successfully in plant phenotyping combined with various sensors\n\
    and speciﬁc tasks, including harvesting crop counting, weed control, and crop\
    \ stress detection [17,45–47].\nRegarding crop stress detection, with various\
    \ speciﬁc tasks, the image analysis methods are often\nvarying among classiﬁcation,\
    \ segmentation, and object detection in crop stress detection combined\nwith various\
    \ sensors (Figure 4). Image classiﬁcation is one of the earliest areas where deep\
    \ learning\ncontributed signiﬁcantly to the analysis of plant stress images. In\
    \ crop stress image classiﬁcation, one or\nmore images are usually used as input\
    \ data, and a diagnostic decision is used as output (e.g., healthy\nor diseased).\
    \ In this case, each diagnosis is a sample, and the size of the dataset is usually\
    \ smaller\ncompared to computer vision (thousands or millions of samples). Therefore,\
    \ for such applications,\nthe transfer learning should be popular for researchers.\
    \ Transfer learning essentially uses pre-trained\nnetworks to try to meet the\
    \ needs of deep network training on large datasets. At present, two transfer\n\
    learning methods are commonly applied: (1) the speciﬁc pre-trained network is\
    \ directly applied in\nimages processing, and (2) ﬁne-tuning the speciﬁed pre-trained\
    \ network for the aiming objective\nimages. Another beneﬁt of the former strategy\
    \ is that training a deep network is not necessary, making\nit easy to insert\
    \ the extracted features into existing image analysis pipelines. However, it is\
    \ still a\nchallenge to ﬁnd the best strategy. Barbedo (2019) used a CNN to classify\
    \ individual lesions and spots\non plant leaves instead of considering the entire\
    \ leaf [45]. This identiﬁed multiple diseases that aﬀect\nthe same leaf. The accuracy\
    \ obtained using this method was, on average, 12% higher than that obtained\n\
    using the original image. While proper symptom segmentation is still required\
    \ manually, preventing\nfull automation. Also, in this paper, the authors applied\
    \ deep learning to detect the individual lesions\nand spots for 14 plant species.\
    \ Speciﬁcally, this study used a pre-trained GoogLeNet CNN for training\nthe models.\
    \ The images were split into two groups for addressing diﬀerent objectives. The\
    \ ﬁrst group\nwas aimed to image classiﬁcation, to identify the origin of the\
    \ observed symptom, while the second one\nwas for object detection, which was\
    \ to identify disease areas amidst healthy tissue and to determine if\nsubsequent\
    \ classiﬁcation was conducted or not. The results showed that accuracies obtained\
    \ using this\napproach were, in average, 12% higher than those achieved using\
    \ the original images. The accuracies\nwere higher than 75% for all the considered\
    \ conditions or number of detected diseases, while the author\nalso claimed that\
    \ the resized input images for pre-trained neural network were not as advantageous\
    \ as\nthe original images under certain conditions. Other studies that applied\
    \ the deep learning into the\ncrop stress image classiﬁcation are shown in Table\
    \ 1.\nAgriEngineering 2020, 2\n437\nFigure 4. Applications of deep learning for\
    \ crop stress detection based on diﬀerent image analysis.\n(a) Classiﬁcation (images\
    \ from [47]), (b) segmentation (images from [48]), and (c) object detection\n\
    (images from [49]).\nTable 1. Applications of deep learning for crop stress classiﬁcation.\n\
    Reference\nSensor\nStress Type\nMethod\nApplication\n[50]\nRGB sensor\nBiotic\n\
    CNN pre-trained with\nAlexNet\nApple leaf Diseases\n[51]\nRGB sensor\nBiotic\n\
    CNN pre-trained with\nGoogLeNet\nCassava leaf Diseases\n[52]\nRGB sensor\nBiotic\n\
    FCN pre-trained with VGG,\nCNN pre-trained with VGG\nWheat leaf diseases\n[53]\n\
    RGB sensor\nBiotic\nCNN\nMaize leaf disease\n[54]\nRGB sensor\nAbiotic\nDNN\n\
    Tomato water stress\n[55]\nRGB sensor\nAbiotic and\nbiotic\nFaster R-CNN, R-FCN,\
    \ SSD\npre-trained with VGG,\nResNet\nNine tomato diseases\nand pests\n[56]\n\
    RGB sensor\nBiotic\nCNN pretrained with\nVGG16 and MSVM\nFive major diseases of\n\
    eggplant\n[57]\nRGB sensor\nAbiotic and\nbiotic\nCNN\nEight diﬀerent soybean\n\
    stresses\n[58]\nHyperspectral\nimaging\nBiotic\nCNN and RNN\nWheat Fusarium head\n\
    blight disease\nAgriEngineering 2020, 2\n438\nTable 1. Cont.\nReference\nSensor\n\
    Stress Type\nMethod\nApplication\n[59]\nRGB sensor\n(datasets from\nplantVillage)\n\
    Biotic\nVGG 16, Inception V4,\nResNet, DenseNets\n38 diﬀerent classes\nincluding\
    \ diseased and\nhealthy images of leaves\nof 14 plants\n[60]\nRGB sensor\nBiotic\n\
    SIFT encoding and CNN\npretrained with MobileNet\nGrapevine esca disease\n[61]\n\
    RGB sensor\nAbiotic\nDCNN pretrained with\nResNet\nMaize drought stress\n[62]\n\
    RGB sensor\n(datasets from\nplantVillage)\nBiotic\nCNN pretrained with\nAlexNet,\
    \ GoogLeNet,\nInception v3, ResNet-50,\nResNet-101 and SqueezeNet.\nGrapevine\
    \ yellows\ndisease\n[63]\nRGB sensor\nBiotic\nCNN\nRice blast disease\n[64]\n\
    RGB sensor\n(datasets from AI\nChallenger Global\nAI Contest)\nBiotic\nPD2SE-Net\
    \ based on CNN\nand ResNet\nApple, cherry, corn,\ngrape, peach, pepper,\npotato,\
    \ strawberry,\ntomato diseases\n[65]\nSmartphones\nBiotic\nCNN AlexNet, GoogLeNet,\n\
    ResNet, VGG16,\nMobileNetV2\nCoﬀee leaves with rust,\nbrown leaf spot and\ncercospora\
    \ leaf spot\n[66]\nHyperspectral\nimaging\nBiotic\nCNN\nYellow rust in winter\n\
    wheat\n[67]\nRGB sensor\nBiotic\nCNN\n14 crop species with\n38 classes of diseases.\n\
    [68]\nHyperspectral\nimaging\nBiotic\nGAN\nTomato spotted wilt\nvirus\n[69]\n\
    RGB sensor\nBiotic\nGAN, VGG16\nTea red scab, tea red leaf\nspot and tea leaf\
    \ blight\nNote: SIFT: Scale-invariant feature transform.\n3.2. Segmentation\n\
    Segmentation is used to identify the set of pixels or contours that make up the\
    \ target object [70].\nSegmentation is a common topic in papers applying deep\
    \ learning to plant disease imaging.\nVarious methods have been applied to segmentation,\
    \ such as developing unique segmentation\narchitectures based on CNNs and application\
    \ of RNNs. The popular segmentation CNN architectures\ninclude U-Net and Mask\
    \ R-CNN [71]. U-Net was investigated in biomedical image segmentation\nﬁrstly\
    \ [72], which was built upon a fully convolutional network (FCN). FCN is to provide\
    \ one\ncontracting network by continuous layers in which pooling layers are substituted\
    \ by up-sampling\noperators. The continuous layer would learn to gather a more\
    \ precise output, with an increase of\nthe resolution of the output. U-Net is\
    \ symmetric, that is, it has the same number of up-sampling\nand down-sampling\
    \ layers. The skip connections in U-Net use a concatenation operator between the\n\
    up-sampling and down-sampling layers [73]. This method connects the features in\
    \ the contact path\nand the extension path. This means that the entire image is\
    \ enabled to be processed forward through\nU-Net to directly generate a segmentation\
    \ mapping. In this way, U-Net could consider the entire image,\nwhich make it\
    \ more advanced than the patch-based CNN. Furthermore, Çiçek et al. (2016) built\
    \ one\n3D U-Net segmentation by replacing all 2D operations with their 3D counterparts\
    \ [74]. Lin et al. (2019)\napplied a U-Net CNN to segment and detect cucumber\
    \ powdery mildew-infected cucumber leaves\nobtained by an RGB sensor [46]. In\
    \ this study, since the powdery mildew-infected pixels were less than\nthat of\
    \ non-infected pixels, the authors proposed binary cross entropy loss function\
    \ to magnify the\nAgriEngineering 2020, 2\n439\nloss value of the powdery mildew-infected\
    \ pixels by 10 times. The results showed that the semantic\nsegmentation CNN model\
    \ achieved an average pixel accuracy of 96.08% for segmenting the diseased\npowdery\
    \ mildew on cucumber leaf images. It was still challenging to apply such deep\
    \ neural network\nin ﬁeld conditions. Diﬀerent applications of deep learning into\
    \ the crop stress image segmentation are\nsummarized in Table 2.\nTable 2. Applications\
    \ of deep learning for crop stress segmentation.\nReference\nSensor\nStress Type\n\
    Method\nApplication\n[75]\nSmart phone\nBiotic\nCNN\nCucumber diseases\n[76]\n\
    RGB (from Plant\nVillage, datasets)\nBiotic\nFractal Texture Analysis\n(SFTA)\
    \ and local binary\npatterns (LBP) combined with\nVGG16 and Caﬀe-AlexNet\nFruit\
    \ crops diseases\n[77]\nRGB sensor\nBiotic\nMask R-CNN\nRice leaf diseases\n[78]\n\
    RGB sensor (from AI\nChallenger 2019)\nBiotic\nCNN pre-trained with U-Net\nNineteen\
    \ plant\ndiseases\n[79]\nRGB sensor\nBiotic\nGlobal pooling dilated\nconvolutional\
    \ neural network\n(GPDCNN)\nCucumber leaf\ndisease\nR-CNN combines rectangular\
    \ region proposals with CNN features. Generally, R-CNN includes\ntwo-stage detection\
    \ procedures. Firstly, the algorithm detects subset regions of an image which\
    \ may\ncontain an object and extracts CNN features from the region proposals.\
    \ Then the object in each region\nis classiﬁed. R-CNN takes a large amount of\
    \ training of the deep neural network when there are\n2000 or more region proposals\
    \ per image that need to be classiﬁed. Meanwhile, there is no learning\nprocedure\
    \ at the ﬁrst searching stage as the selective search algorithm is ﬁxed. As a\
    \ result, it may lead\nto tricky candidate region proposals being generated [80,81].\
    \ During R-CNN processing, the region\nproposals need to be cropped and resized,\
    \ while the Faster R-CNN detector processes the entire\nimage. Thus, Faster R-CNN\
    \ can be applied for real-time object detection. Additionally, Faster R-CNN\n\
    is the backbone of Mask R-CNN. Faster R-CNN includes two outputs, that is, a class\
    \ label and a\nbounding-box oﬀset. A third branch is added to mask R-CNN upon\
    \ faster R-CNN architecture, which\noutputs the object mask [71]. In addition,\
    \ Mask R-CNN is one of the instance segmentation algorithms\nwhich produce a mask\
    \ that uses color or grayscale values to identify pixels belonging to the same\n\
    object. Except to feed the feature map to the region proposal network and the\
    \ classiﬁer, Mask R-CNN\nuses a feature map to predict a binary mask for the object\
    \ inside the bounding box.\n3.3. Object Detection\nObject detection is a key part\
    \ in imaging diagnosis and one of the most laborious tasks.\nTypically, the task\
    \ involves locating and identifying objects throughout the image [82]. For a long\n\
    time, the research goal of computer vision was to automatically detect objects,\
    \ for improving detection\naccuracy, and reducing labor. The object detection\
    \ based on deep learning uses CNN for pixel\nclassiﬁcation and then applies some\
    \ post-processing to obtain object candidates [81–83]. Since the\nimage classiﬁcation\
    \ is to classify each pixel in the image, which is basically equal to object classiﬁcation,\n\
    thereby the CNN architectures of segmentation are alike to those for the classiﬁcation\
    \ task, while the\nimage labels imbalance, hard negative detecting, and eﬃcient\
    \ processing image pixels etc., still remain\nas the challenging issues to be\
    \ addressed for object detection. Fuentes et al., (2017) applied Faster\nR-CNN\
    \ and a VGG-16 detector to recognize tomato plant diseases and pests [55]. Diseases\
    \ and pests\ncould be identiﬁed using the bounding-box and score for each class\
    \ being shown on each infected leaf.\nThat is, the detection method provides a\
    \ solution for detecting the class and location of diseases in\ntomato plants\
    \ practically. R-CNN and Faster R-CNN have been applied to object detection as\
    \ well,\nAgriEngineering 2020, 2\n440\nusing the regions in the image to locate\
    \ the object. Recently, the YOLO algorithm has often been\napplied for object\
    \ detection, which uses a single convolutional network to predict the bounding\
    \ boxes\nand classify such boxes [84]. The YOLO algorithm divides the image into\
    \ an M × M grid, then m\n(m<M) bounding boxes are taken within each of the grids.\
    \ The network yields a class probability\nfor each bounding box. When the bounding\
    \ boxes have higher class probability than a threshold\nvalue, they would be selected\
    \ and applied for locating the objects in the image. The limitation of the\nYOLO\
    \ network is that it sometimes cannot identify small objects in the images [84].\
    \ Singh et al. (2020)\napplied Faster R-CNN with an InceptionResnetV2 model and\
    \ a MobileNet model on PlantVillage\ndatasets to detect plant disease, which included\
    \ 2598 images from 13 plants and over 17 diseases [85].\nOther applications for\
    \ object detection are summarized in Table 3.\nTable 3. Application of deep learning\
    \ for crop stress object detection.\nReference\nSensor\nStress Type\nMethod\n\
    Application\n[55]\nRGB sensor\nAbiotic and\nbiotic\nFaster R-CNN, R-FCN\nNine\
    \ tomato diseases\nand pests\n[60]\nRGB sensor\nBiotic\nCNN pretrained with\n\
    RetinaNet\nGrapevine esca disease\n[82]\nRGB sensor\nBiotic\nYOLOv2 and YOLOv3\n\
    Mosquito bugs and red\nspider mites\n[83]\nRGB sensor\nBiotic\nMask R-CNN\nNorthern\
    \ leaf blight of\nmaize\n[86]\nSmartphone\nBiotic\nFaster R-CNN\nRice false smut\n\
    [87]\nRGB image from\nthe Internet\nBiotic\nFaster R-CNN and Mask\nR-CNN\nTen\
    \ tomato disease\n[88]\nSmartphones\nBiotic\nFaster R-CNN\nStrawberry\nverticillium\
    \ wilt\n[89]\nRGB image\nBiotic\nFaster R-CNN\nSweet Pepper Disease\nand Pest\n\
    4. Unique Challenges in Plant Stress Based on Imagery\nNoncontact plant stress\
    \ detection has been conducted on diﬀerent application scales, i.e., laboratory,\n\
    ground-based, and UAV. Additionally, the modality has been operated based on a\
    \ variety of sensors,\nsuch as digital, thermal, multispectral, and hyperspectral\
    \ imagery, with diﬀerent numbers of spectral\nchannels, from three to hundreds.\
    \ Such sensors could monitor the size, shape, and structural features\nor crops\
    \ based on the external views obtained from digital cameras. The digital sensors\
    \ could be\neasily operated under the natural light environment. Hyperspectral\
    \ imaging sensors could obtain the\ninside spectral signatures beyond the visible\
    \ wavelength range which could reﬂect the healthy crop\nconditions in a wide range\
    \ of spectra, while most of the commercial hyperspectral imaging sensors\ncould\
    \ only work in laboratory with controlled light conditions at present. On the\
    \ other hand, the wind\nwill make the crops move around. In general, for image\
    \ acquisition, it is still challenging for ﬁeld work.\nFurther, the crops are\
    \ not static:\nthe physiological properties change with their growth.\nEspecially\
    \ for biotic stress infected crops, the fungi or viruses in the crops have great\
    \ impacts\non the physiological changes. It will be diﬃcult to detect the stress\
    \ at an early stage without symptoms\nshowing based on image analysis. Further,\
    \ for the application of deep learning-assisted image analysis,\na lack of datasets\
    \ is a major obstacle as well. At present, the available open source images are\
    \ mainly\nfrom the PlantVillage dataset. On the other hand, one signiﬁcant challenge\
    \ is that of ground-truth\nlabelling, which is hugely laborious. The Amazon SageMaker\
    \ Ground Truth provides a service for\nmanaging the labelling, including two features.\
    \ One is annotation consolidation, which combines\nAgriEngineering 2020, 2\n441\n\
    diﬀerent people’s annotation task results into one high-ﬁdelity label. The second\
    \ one is automated\ndata labeling, which utilizes machine learning to label portions\
    \ of the provided data automatically.\nMoreover, to detect crop stress, the classiﬁcation\
    \ and segmentation are often used as binary\ntasks, i.e., healthy versus infected,\
    \ target infected area versus background. However, since these\ntwo categories\
    \ can be highly heterogeneous, this is usually a general simpliﬁcation. For instance,\n\
    the samples of the healthy class mainly consist of completely healthy objects\
    \ but also rarely few objects\nshowing early stresses. This could lead to classiﬁers\
    \ that are able to exclude the healthy samples but\ncannot identify the few rare\
    \ ones. The strategy for this case is to make a deep learning system with\nmulticlass\
    \ by giving it detailed annotations of all possible classes. Meanwhile, the within-class\
    \ variance\nfrom images may reduce the sensitivity of the deep learning system.\
    \ However, the between-classes\nvariance from a dataset that may not be generalized\
    \ to every image, such as the diﬀerent severity of\ndisease images, can obtain\
    \ a pseudo-deep learning training architecture in one certain experiment,\nbut\
    \ obstruct the usefulness of its broad application to practical decision-making\
    \ unless the nature of\nthis dataset is precisely understood. Parameter optimization\
    \ of the deep learning training models, i.e.,\nbatch size, learning rate, dropout\
    \ rate, etc., is a remaining challenge as well. There is currently no exact\n\
    method to achieve the best combinations of hyperparameters, which is often operated\
    \ empirically,\neven though Bayesian optimization has been put forwarded.\n5.\
    \ Outlook\nDeep learning has been applied successfully in plant stress (i.e.,\
    \ abiotic, and biotic stress) detection\neven though it still has many challenges.\
    \ Most of the papers we have reviewed are based on the 2D\nimages for symptomatic\
    \ stages, for example the digital and greyscale images. Such images could be\n\
    enabled to operate in the deep transfer learning architecture, such as Alexnet,\
    \ VGG, GoogleNet, while\nsuch pre-trained transfer networks could not be applied\
    \ to the 3D datasets, such as hyperspectral\nimages, which are more sensitive\
    \ to detecting the early-infected plants. In the future, deep neural\nnetworks\
    \ that can be used for 3D images should be the focus and early detections of plant\
    \ disease is\npivotal to the precision disease management, especially for diseases\
    \ without therapy using pesticide.\nOn the other hand, many tasks in plant stress\
    \ detection analysis could be granted, such as classiﬁcation,\nand such a strategy\
    \ may not be always optimal since it probably requires some post-processing, such\n\
    as segmentation. Further, semi-supervised and unsupervised deep learning are worthy\
    \ of being\nexploratory in the application of plant stress detection, though most\
    \ of studies are based on supervised\napproaches. The advantage of unsupervised\
    \ methods is that the networks training process could be\noperated without the\
    \ ground truth labels. The unsupervised approach for detecting the plant stress\n\
    are generative adversarial networks (GANs) [90], while another common unsupervised\
    \ approach,\ni.e., variational autoencoders (VAEs), is rarely applied for crop\
    \ disease diagnosis yet based on our\nknowledge [91]. Further, deep learning has\
    \ been applied for other objectives in agricultural imaging,\ne.g., crop load\
    \ estimation and harvesting, while image reconstruction remains unexplored, especially\n\
    for LiDAR point cloud data. In general, deep learning has provided promising results\
    \ in plant stress\ndetection, which could accelerate the development of precision\
    \ agriculture with the extension of\nﬁeld application.\nAuthor Contributions:\
    \ Conceptualization, Z.G., Y.X.; Supervision, W.Z.; Visualization, Z.L. (Zhongwei\
    \ Luo), Z.L.\n(Zhenzhen Lv); Writing—Original Draft Preparation, Z.G.; Writing—Review\
    \ & Editing, Z.G. and W.Z.; All authors\nhave read and agreed to the published\
    \ version of the manuscript.\nFunding: This research was funded by [National Natural\
    \ Science Foundation of China] grant number [31801625];\n[Applied Basic Research\
    \ Program of Science and Technology Department of Sichuan Province] grant number\n\
    [2019YJ0444]; [Program of Key Laboratory of Modern Agricultural Equipment and\
    \ Technology, Ministry of\nEducation] grant number [JNZ201919], and [Longshan\
    \ Academic Talent Research Supporting Program of SWUST]\ngrant number [17LZX546].\n\
    Conﬂicts of Interest: The authors declare no conﬂict of interest.\nAgriEngineering\
    \ 2020, 2\n442\nReferences\n1.\nCattivelli, L.; Rizza, F.; Badeck, F.W.; Mazzucotelli,\
    \ E.; Mastrangelo, A.M.; Francia, E.; Stanca, A.M.\nDrought tolerance improvement\
    \ in crop plants:\nAn integrated view from breeding to genomics.\nField Crop.\
    \ Res. 2008, 105, 1–14. [CrossRef]\n2.\nAraus, J.L.; Cairns, J.E. Field high-throughput\
    \ phenotyping: The new crop breeding frontier. Trends Plant Sci.\n2014, 19, 52–61.\
    \ [CrossRef] [PubMed]\n3.\nElazab, A.; Ordóñez, R.A.; Savin, R.; Slafer, G.A.;\
    \ Araus, J.L. Detecting interactive eﬀects of N fertilization and\nheat stress\
    \ on maize productivity by remote sensing techniques. Eur. J. Agron. 2016, 73,\
    \ 11–24. [CrossRef]\n4.\nZhang, L.; Zhang, H.; Niu, Y.; Han, W. Mapping maize\
    \ water stress based on UAV multispectral remote\nsensing. Remote Sens. 2019,\
    \ 11, 605. [CrossRef]\n5.\nDong, Z.; Men, Y.; Liu, Z.; Li, J.; Ji, J. Application\
    \ of chlorophyll ﬂuorescence imaging technique in analysis\nand detection of chilling\
    \ injury of tomato seedlings. Comput. Electron. Agric. 2020, 168, 105109. [CrossRef]\n\
    6.\nGerhards, M.; Rock, G.; Schlerf, M.; Udelhoven, T. Water stress detection\
    \ in potato plants using leaf\ntemperature, emissivity, and reﬂectance. Int. J.\
    \ Appl. Earth Obs. Geoinf. 2016, 53, 27–39. [CrossRef]\n7.\nKim, Y.; Glenn, D.M.;\
    \ Park, J.; Ngugi, H.K.; Lehman, B.L. Hyperspectral image analysis for water stress\n\
    detection of apple trees. Comput. Electron. Agric. 2011, 77, 155–160. [CrossRef]\n\
    8.\nMahlein, A.K. Plant disease detection by imaging sensors–parallels and speciﬁc\
    \ demands for precision\nagriculture and plant phenotyping. Plant Dis. 2016, 100,\
    \ 241–251. [CrossRef]\n9.\nBarbedo, J.G.A. Digital image processing techniques\
    \ for detecting, quantifying and classifying plant diseases.\nSpringerPlus 2013,\
    \ 2, 660. [CrossRef]\n10.\nGebejes, A.; Huertas, R. Texture characterization based\
    \ on grey-level co-occurrence matrix. Databases 2013,\n9, 10.\n11.\nLindenthal,\
    \ M.; Steiner, U.; Dehne, H.W.; Oerke, E.C. Eﬀect of downy mildew development\
    \ on transpiration\nof cucumber leaves visualized by digital infrared thermography.\
    \ Phytopathology 2005, 95, 233–240. [CrossRef]\n[PubMed]\n12.\nBuschmann, C.;\
    \ Langsdorf, G.; Lichtenthaler, H.K. Imaging of the blue, green, and red ﬂuorescence\
    \ emission\nof plants: An overview. Photosynthetica 2000, 38, 483–491. [CrossRef]\n\
    13.\nMutka, A.M.; Bart, R.S. Image-based phenotyping of plant disease symptoms.\
    \ Front. Plant Sci. 2015, 5, 734.\n[CrossRef] [PubMed]\n14.\nGao, Z.; Zhao, Y.;\
    \ Khot, L.R.; Hoheisel, G.A.; Zhang, Q. Optical sensing for early spring freeze\
    \ related\nblueberry bud damage detection: Hyperspectral imaging for salient spectral\
    \ wavelengths identiﬁcation.\nComput. Electron. Agric. 2019, 167, 105025. [CrossRef]\n\
    15.\nBoulent, J.; Foucher, S.; Théau, J.; St-Charles, P.L. Convolutional Neural\
    \ Networks for the Automatic\nIdentiﬁcation of Plant Diseases. Front. Plant Sci.\
    \ 2019, 10, 941. [CrossRef] [PubMed]\n16.\nWernick, M.N.; Yang, Y.; Brankov, J.G.;\
    \ Yourganov, G.; Strother, S.C. Machine learning in medical imaging.\nIEEE Signal\
    \ Process. Mag. 2010, 27, 25–38. [CrossRef]\n17.\nBauer, A.; Bostrom, A.G.; Ball,\
    \ J.; Applegate, C.; Cheng, T.; Laycock, S.; Zhou, J. Combining computer vision\n\
    and deep learning to enable ultra-scale aerial phenotyping and precision agriculture:\
    \ A case study of lettuce\nproduction. Hortic. Res. 2019, 6, 1–12. [CrossRef]\n\
    18.\nHughes, D.; Salathé, M. An open access repository of images on plant health\
    \ to enable the development of\nmobile disease diagnostics. arXiv 2015, arXiv:1511.08060.\n\
    19.\nSevetlidis, V.; Giuﬀrida, M.V.; Tsaftaris, S.A. Whole image synthesis using\
    \ a deep encoder-decoder network.\nIn International Workshop on Simulation and\
    \ Synthesis in Medical Imaging; Springer: Cham, Switzerland, 2016;\npp. 127–137.\n\
    20.\nMohanty, S.P.; Hughes, D.P.; Salathé, M. Using deep learning for image-based\
    \ plant disease detection.\nFront. Plant Sci. 2016, 7, 1419. [CrossRef]\n21.\n\
    Pawara, P.; Okafor, E.; Surinta, O.; Schomaker, L.; Wiering, M. Comparing Local\
    \ Descriptors and Bags of\nVisual Words to Deep Convolutional Neural Networks\
    \ for Plant Recognition. In ICPRAM; Science and\nTechnology Publications: Porto,\
    \ Portugal, 2017; pp. 479–486.\n22.\nNarvaez, F.Y.; Reina, G.; Torres-Torriti,\
    \ M.; Kantor, G.; Cheein, F.A. A survey of ranging and imaging\ntechniques for\
    \ precision agriculture phenotyping. Ieee ASME Trans. Mechatron. 2017, 22, 2428–2439.\n\
    [CrossRef]\nAgriEngineering 2020, 2\n443\n23.\nKoza, J.R.; Bennett, F.H.; Andre,\
    \ D.; Keane, M.A. Automated design of both the topology and sizing of analog\n\
    electrical circuits using genetic programming. In Artiﬁcial Intelligence in Design’96;\
    \ Springer: Dordrecht, The\nNetherlands, 1996; pp. 151–170.\n24.\nHarrington,\
    \ P. Machine Learning in Action; Manning Publications: Greenwich, CT, USA, 2012;\
    \ p. 384.\n25.\nCouprie, C.; Farabet, C.; Najman, L.; LeCun, Y. Indoor semantic\
    \ segmentation using depth information.\narXiv 2013, arXiv:1301.3572.\n26.\nAmruthnath,\
    \ N.; Gupta, T. A research study on unsupervised machine learning algorithms for\
    \ early fault\ndetection in predictive maintenance. In Proceedings of the 2018\
    \ 5th International Conference on Industrial\nEngineering and Applications (ICIEA),\
    \ Singapore, 26–28 April 2018; pp. 355–361.\n27.\nSrivastava, N.; Mansimov, E.;\
    \ Salakhudinov, R. Unsupervised learning of video representations using\nlstms.\
    \ In Proceedings of the International Conference on Machine Learning, Lille, France,\
    \ 6–11 July 2015;\npp. 843–852.\n28.\nGu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy,\
    \ A.; Shuai, B.; Chen, T. Recent advances in convolutional neural\nnetworks. Pattern\
    \ Recognit. 2018, 77, 354–377. [CrossRef]\n29.\nHasan, M.; Tanawala, B.; Patel,\
    \ K.J. Deep Learning Precision Farming: Tomato Leaf Disease Detection\nby Transfer\
    \ Learning. In Proceedings of the 2nd International Conference on Advanced Computing\
    \ and\nSoftware Engineering (ICACSE), Sultanpur, India, 8–9 February 2019; pp.\
    \ 843–852.\n30.\nZhang, J.; He, L.; Karkee, M.; Zhang, Q.; Zhang, X.; Gao, Z.\
    \ Branch detection for apple trees trained\nin fruiting wall architecture using\
    \ depth features and Regions-Convolutional Neural Network (R-CNN).\nComput. Electron.\
    \ Agric. 2018, 155, 386–393. [CrossRef]\n31.\nSingh, A.K.; Ganapathysubramanian,\
    \ B.; Sarkar, S.; Singh, A. Deep learning for plant stress phenotyping:\nTrends\
    \ and future perspectives. Trends Plant Sci. 2018, 23, 883–898. [CrossRef] [PubMed]\n\
    32.\nAkhtar, N.; Ragavendran, U. Interpretation of intelligence in CNN-pooling\
    \ processes: A methodological\nsurvey. Neural Comput. Appl. 2020, 32, 879–898.\
    \ [CrossRef]\n33.\nYonaba, H.; Anctil, F.; Fortin, V. Comparing sigmoid transfer\
    \ functions for neural network multistep ahead\nstreamﬂow forecasting. J. Hydrol.\
    \ Eng. 2010, 15, 275–283. [CrossRef]\n34.\nGao, Z.; Shao, Y.; Xuan, G.; Wang,\
    \ Y.; Liu, Y.; Han, X. Real-time hyperspectral imaging for the in-ﬁeld\nestimation\
    \ of strawberry ripeness with deep learning. Artif. Intell. Agric. 2020. [CrossRef]\n\
    35.\nUbbens, J.R.; Stavness, I. Deep plant phenomics: A deep learning platform\
    \ for complex plant phenotyping\ntasks. Front. Plant Sci. 2017, 8, 1190. [CrossRef]\
    \ [PubMed]\n36.\nKhan, A.; Sohail, A.; Zahoora, U.; Qureshi, A.S. A survey of\
    \ the recent architectures of deep convolutional\nneural networks. arXiv 2019,\
    \ arXiv:1901.06032.\n37.\nQayyum, A.; Malik, A.S.; Saad, N.M.; Iqbal, M.; Faris\
    \ Abdullah, M.; Rasheed, W.; Bin Jafaar, M.Y.\nScene classiﬁcation for aerial\
    \ images based on CNN using sparse coding technique. Int. J. Remote Sens. 2017,\n\
    38, 2662–2685. [CrossRef]\n38.\nSimonyan, K.; Zisserman, A. Very deep convolutional\
    \ networks for large-scale image recognition. arXiv\n2014, arXiv:1409.1556.\n\
    39.\nChen, C.F.; Lee, G.G.; Sritapan, V.; Lin, C.Y. Deep convolutional neural\
    \ network on iOS mobile devices.\nIn Proceedings of the 2016 IEEE International\
    \ Workshop on Signal Processing Systems (SiPS), Dallas, TX,\nUSA, 26–28 October\
    \ 2016; pp. 130–135.\n40.\nSzegedy, C.; Vanhoucke, V.; Ioﬀe, S.; Shlens, J.; Wojna,\
    \ Z. Rethinking the inception architecture for computer\nvision. In Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,\
    \ NV,\nUSA, 26 Jun–1 July 2016; pp. 2818–2826.\n41.\nLi, H.; Zhao, R.; Wang, X.\
    \ Highly eﬃcient forward and backward propagation of convolutional neural\nnetworks\
    \ for pixelwise classiﬁcation. arXiv 2014, arXiv:1412.4526.\n42.\nMallat, S. Understanding\
    \ deep convolutional networks. Philos. Trans. Royal Soc. A Math. Phys. Eng. Sci.\n\
    2016, 374, 20150203. [CrossRef] [PubMed]\n43.\nAwan, A.A.; Hamidouche, K.; Hashmi,\
    \ J.M.; Panda, D.K. S-caﬀe: Co-designing mpi runtimes and caﬀe\nfor scalable deep\
    \ learning on modern gpu clusters. In ACM Sigplan Notices; ACM: Austin, TX, USA,\
    \ 2017;\nVolume 52, pp. 193–205.\n44.\nSteinkraus, D.; Buck, I.; Simard, P.Y.\
    \ Using GPUs for machine learning algorithms. In Proceedings of\nthe Eighth International\
    \ Conference on Document Analysis and Recognition (ICDAR’05), Seoul, Korea,\n\
    31 August–1 September 2005; pp. 1115–1120.\nAgriEngineering 2020, 2\n444\n45.\n\
    Barbedo, J.G.A. Plant disease identiﬁcation from individual lesions and spots\
    \ using deep learning. Biosyst.\nEng. 2019, 180, 96–107. [CrossRef]\n46.\nLin,\
    \ K.; Gong, L.; Huang, Y.; Liu, C.; Pan, J. Deep learning-based segmentation and\
    \ quantiﬁcation of cucumber\nPowdery Mildew using convolutional neural network.\
    \ Front. Plant Sci. 2019, 10, 155. [CrossRef]\n47.\nFerentinos, K.P. Deep learning\
    \ models for plant disease detection and diagnosis. Comput. Electron. Agric.\n\
    2018, 145, 311–318. [CrossRef]\n48.\nHa, J.G.; Moon, H.; Kwak, J.T.; Hassan, S.I.;\
    \ Dang, M.; Lee, O.N.; Park, H.Y. Deep convolutional neural\nnetwork for classifying\
    \ Fusarium wilt of radish from unmanned aerial vehicles. J. Appl. Remote Sens.\
    \ 2017,\n11, 042621. [CrossRef]\n49.\nSchumann, A.; Waldo, L.; Holmes, W.; Test,\
    \ G.; Ebert, T. Artiﬁcial Intelligence for Detecting Citrus Pests,\nDiseases and\
    \ Disorders. In Citrus Industry News, Technology; AgNet Media, Inc.: Gainesville,\
    \ FL, USA, 2 July\n2018.\n50.\nLiu, B.; Zhang, Y.; He, D.; Li, Y.; Liu, B.; Zhang,\
    \ Y.; Li, Y. Identiﬁcation of Apple Leaf Diseases Based on Deep\nConvolutional\
    \ Neural Networks. Symmetry 2017, 10, 11. [CrossRef]\n51.\nRamcharan, A.; Baranowski,\
    \ K.; McCloskey, P.; Ahmed, B.; Legg, J.; Hughes, D.P. Deep Learning for\nImage-Based\
    \ Cassava Disease Detection. Front. Plant Sci. 2017, 8, 1852. [CrossRef]\n52.\n\
    Lu, J.; Hu, J.; Zhao, G.; Mei, F.; Zhang, C. An in-ﬁeld automatic wheat disease\
    \ diagnosis system. Comput.\nElectron. Agric. 2017, 142, 369–379. [CrossRef]\n\
    53.\nDeChant, C.;\nWiesner-Hanks, T.;\nChen, S.;\nStewart, E.L.;\nYosinski, J.;\n\
    Gore, M.A.;\nLipson, H.\nAutomated identiﬁcation of northern leaf blight-infected\
    \ maize plants from ﬁeld imagery using deep\nlearning. Phytopathology 2017, 107,\
    \ 1426–1432. [CrossRef]\n54.\nKaneda, Y.; Shibata, S.; Mineno, H. Multi-modal\
    \ sliding window-based support vector regression for\npredicting plant water stress.\
    \ Knowl. Based Syst. 2017, 134, 135–148. [CrossRef]\n55.\nFuentes, A.; Yoon, S.;\
    \ Kim, S.C.; Park, D.S. A robust deep-learning-based detector for real-time tomato\
    \ plant\ndiseases and pests recognition. Sensors 2017, 17, 2022. [CrossRef]\n\
    56.\nRangarajan, A.K.; Purushothaman, R. Disease Classiﬁcation in Eggplant Using\
    \ Pre-trained VGG16 and\nMSVM. Scientiﬁc Reports 2020, 10, 1–11.\n57.\nGhosal,\
    \ S.; Blystone, D.; Singh, A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar,\
    \ S. An explainable deep\nmachine vision framework for plant stress phenotyping.\
    \ Proc. Natl. Acad. Sci. USA 2018, 115, 4613–4618.\n[CrossRef]\n58.\nJin, X.;\
    \ Jie, L.; Wang, S.; Qi, H.; Li, S.; Jin, X.; Li, S.W. Classifying Wheat Hyperspectral\
    \ Pixels of Healthy\nHeads and Fusarium Head Blight Disease Using a Deep Neural\
    \ Network in the Wild Field. Remote Sens.\n2018, 10, 395. [CrossRef]\n59.\nToo,\
    \ E.C.; Yujian, L.; Njuki, S.; Yingchun, L. A comparative study of ﬁne-tuning\
    \ deep learning models for\nplant disease identiﬁcation. Comput. Electron. Agric.\
    \ 2019, 161, 272–279. [CrossRef]\n60.\nRançon, F.; Bombrun, L.; Keresztes, B.;\
    \ Germain, C. Comparison of SIFT Encoded and Deep Learning\nFeatures for the Classiﬁcation\
    \ and Detection of Esca Disease in Bordeaux Vineyards. Remote Sens. 2018, 11,\
    \ 1.\n[CrossRef]\n61.\nAn, J.; Li, W.; Li, M.; Cui, S.; Yue, H.; An, J.; Yue,\
    \ H. Identiﬁcation and Classiﬁcation of Maize Drought Stress\nUsing Deep Convolutional\
    \ Neural Network. Symmetry 2019, 11, 256. [CrossRef]\n62.\nCruz, A.; Ampatzidis,\
    \ Y.; Pierro, R.; Materazzi, A.; Panattoni, A.; De Bellis, L.; Luvisi, A. Detection\
    \ of\ngrapevine yellows symptoms in Vitis vinifera L. with artiﬁcial intelligence.\
    \ Comput. Electron. Agric. 2019,\n157, 63–76. [CrossRef]\n63.\nLiang, W.; Zhang,\
    \ H.; Zhang, G.; Cao, H. Rice Blast Disease Recognition Using a Deep Convolutional\
    \ Neural\nNetwork. Sci. Rep. 2019, 9, 2869. [CrossRef] [PubMed]\n64.\nLiang, Q.;\
    \ Xiang, S.; Hu, Y.; Coppola, G.; Zhang, D.; Sun, W. PD2SE-Net: Computer-assisted\
    \ plant disease\ndiagnosis and severity estimation network. Comput. Electron.\
    \ Agric. 2019, 157, 518–529. [CrossRef]\n65.\nEsgario, J.G.; Krohling, R.A.; Ventura,\
    \ J.A. Deep learning for classiﬁcation and severity estimation of coﬀee\nleaf\
    \ biotic stress. Comput. Electron. Agric. 2020, 169, 105162. [CrossRef]\n66.\n\
    Zhang, X.; Han, L.; Dong, Y.; Shi, Y.; Huang, W.; Han, L.; Sobeih, T. A Deep Learning-Based\
    \ Approach for\nAutomated Yellow Rust Disease Detection from High-Resolution Hyperspectral\
    \ UAV Images. Remote Sens.\n2019, 11, 1554. [CrossRef]\nAgriEngineering 2020,\
    \ 2\n445\n67.\nBrahimi, M.; Mahmoudi, S.; Boukhalfa, K.; Moussaoui, A. Deep interpretable\
    \ architecture for plant diseases\nclassiﬁcation. arXiv 2019, arXiv:1905.13523.\n\
    68.\nWang, D.; Vinson, R.; Holmes, M.; Seibel, G.; Bechar, A.; Nof, S.; Tao, Y.\
    \ Early Detection of Tomato Spotted\nWilt Virus by Hyperspectral Imaging and Outlier\
    \ Removal Auxiliary Classiﬁer Generative Adversarial Nets\n(OR-AC-GAN). Sci. Rep.\
    \ 2019, 9, 4377. [CrossRef]\n69.\nHu, G.; Wu, H.; Zhang, Y.; Wan, M. A low shot\
    \ learning method for tea leaf’s disease identiﬁcation. Comput.\nElectron. Agric.\
    \ 2019, 163, 104852. [CrossRef]\n70.\nGhosh, P.; Mitchell, M.; Tanyi, J.A.; Hung,\
    \ A.Y. Incorporating priors for medical image segmentation using a\ngenetic algorithm.\
    \ Neurocomputing 2016, 195, 181–194. [CrossRef]\n71.\nZhao, T.; Yang, Y.; Niu,\
    \ H.; Chen, Y.; Wang, D. Comparing U-Net convolutional networks with fully\nconvolutional\
    \ networks in the performances of pomegranate tree canopy segmentation. In Multispectral,\n\
    Hyperspectral, and Ultraspectral Remote Sensing Technology, Techniques and Applications\
    \ VII; Larar, A.M.,\nSuzuki, M., Wang, J., Eds.; SPIE: Bellingham, WA, USA, 2018;\
    \ Volume 10780, p. 64.\n72.\nBaumgartner, C.F.; Koch, L.M.; Pollefeys, M.; Konukoglu,\
    \ E. An exploration of 2D and 3D deep learning\ntechniques for cardiac MR image\
    \ segmentation. In International Workshop on Statistical Atlases and Computational\n\
    Models of the Heart; Springer: Cham, Switzerland, 2017; pp. 111–119.\n73.\nPeng,\
    \ C.; Li, Y.; Jiao, L.; Chen, Y.; Shang, R. Densely Based Multi-Scale and Multi-Modal\
    \ Fully Convolutional\nNetworks for High-Resolution Remote-Sensing Image Semantic\
    \ Segmentation. IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens. 2019, 12, 2612–2626.\
    \ [CrossRef]\n74.\nÇiçek, Ö.; Abdulkadir, A.; Lienkamp, S.S.; Brox, T.; Ronneberger,\
    \ O. 3D U-Net: Learning Dense Volumetric\nSegmentation from Sparse Annotation;\
    \ Springer: Cham, Switzerland, 2016; pp. 424–432.\n75.\nMa, J.; Du, K.; Zheng,\
    \ F.; Zhang, L.; Gong, Z.; Sun, Z. A recognition method for cucumber diseases\
    \ using leaf\nsymptom images based on deep convolutional neural network. Comput.\
    \ Electron. Agric. 2018, 154, 18–24.\n[CrossRef]\n76.\nKhan, M.A.; Akram, T.;\
    \ Sharif, M.; Awais, M.; Javed, K.; Ali, H.; Saba, T. CCDF: Automatic system for\n\
    segmentation and recognition of fruit crops diseases based on correlation coeﬃcient\
    \ and deep CNN features.\nComput. Electron. Agric. 2018, 155, 220–236. [CrossRef]\n\
    77.\nDas, S.; Roy, D.; Das, P. Disease Feature Extraction and Disease Detection\
    \ from Paddy Crops Using Image\nProcessing and Deep Learning Technique. In Computational\
    \ Intelligence in Pattern Recognition; Springer:\nSingapore, 2020; pp. 443–449.\n\
    78.\nHuang, S.; Liu, W.; Qi, F.; Yang, K. Development and Validation of a Deep\
    \ Learning Algorithm for the\nRecognition of Plant Disease. In Proceedings of\
    \ the 2019 IEEE 21st International Conference on High\nPerformance Computing and\
    \ Communications; IEEE 17th International Conference on Smart City; IEEE\n5th\
    \ International Conference on Data Science and Systems (HPCC/SmartCity/DSS), Zhangjiajie,\
    \ China,\n10–12 August 2019; pp. 1951–1957.\n79.\nZhang, S.; Zhang, S.; Zhang,\
    \ C.; Wang, X.; Shi, Y. Cucumber leaf disease identiﬁcation with global pooling\n\
    dilated convolutional neural network. Comput. Electron. Agric. 2019, 162, 422–430.\
    \ [CrossRef]\n80.\nKamilaris, A.; Prenafeta-Boldú, F.X. Deep learning in agriculture:\
    \ A survey. Comput. Electron. Agric. 2018,\n147, 70–90. [CrossRef]\n81.\nPatrício,\
    \ D.I.; Rieder, R. Computer vision and artiﬁcial intelligence in precision agriculture\
    \ for grain crops: A\nsystematic review. Comput. Electron. Agric. 2018, 153, 69–81.\
    \ [CrossRef]\n82.\nBhatt, P.; Sarangi, S.; Pappula, S. Detection of diseases and\
    \ pests on images captured in uncontrolled\nconditions from tea plantations. In\
    \ Autonomous Air and Ground Sensing Systems for Agricultural Optimization\nand\
    \ Phenotyping IV; Thomasson, J.A., McKee, M., Moorhead, R.J., Eds.; SPIE: Bellingham,\
    \ WA, USA, 2019;\np. 33.\n83.\nStewart, E.L.;\nWiesner-Hanks, T.;\nKaczmar, N.;\n\
    DeChant, C.;\nWu, H.;\nLipson, H.;\nGore, M.A.\nQuantitative Phenotyping of Northern\
    \ Leaf Blight in UAV Images Using Deep Learning. Remote Sens. 2019,\n11, 2209.\
    \ [CrossRef]\n84.\nGandhi, R. R-CNN, Fast R-CNN, Faster R-CNN, YOLO—Object Detection\
    \ Algorithms. Available online: https:\n//towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\n\
    (accessed on 19 June 2020).\nAgriEngineering 2020, 2\n446\n85.\nSingh, D.; Jain,\
    \ N.; Jain, P.; Kayal, P.; Kumawat, S.; Batra, N. PlantDoc: A dataset for visual\
    \ plant disease\ndetection. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD,\
    \ Hyderabad, India, 5–7 January\n2020; Association for Computing Machinery: New\
    \ York, NY, USA; pp. 249–253.\n86.\nSethy, P.K.; Barpanda, N.K.; Rath, A.K.; Behera,\
    \ S.K. Rice False Smut Detection based on Faster R-CNN.\nIndonesian J. Elect.\
    \ Eng. Comput. Sci., 2020, 19. [CrossRef]\n87.\nWang, Q.; Qi, F.; Sun, M.; Qu,\
    \ J.; Xue, J. Identiﬁcation of Tomato Disease Types and Detection of Infected\n\
    Areas Based on Deep Convolutional Neural Networks and Object Detection Techniques.\
    \ Computational\nIntelligence and Neuroscience 2019, 2019. [CrossRef]\n88.\nNie,\
    \ X.; Wang, L.; Ding, H.; Xu, M. Strawberry Verticillium Wilt Detection Network\
    \ Based on Multi-Task\nLearning and Attention. IEEE Access 2019, 7, 170003–170011.\
    \ [CrossRef]\n89.\nLin, T.L.; Chang, H.Y.; Chen, K.H. The pest and disease identiﬁcation\
    \ in the growth of sweet peppers using\nfaster R-CNN and mask R-CNN. J. Internet\
    \ Technol. 2020, 21, 605–614.\n90.\nForster, A.; Behley, J.; Behmann, J.; Roscher,\
    \ R. Hyperspectral Plant Disease Forecasting Using Generative\nAdversarial Networks.\
    \ In Proceedings of the 2019 IEEE International Geoscience and Remote Sensing\n\
    Symposium (IGARSS 2019), Yokohama, Japan, 28 July–2 August 2019; pp. 1793–1796.\n\
    91.\nPardede, H.F.; Suryawati, E.; Sustika, R.; Zilvan, V. Unsupervised convolutional\
    \ autoencoder-based feature\nlearning for automatic detection of plant diseases.\
    \ In Proceedings of the 2018 International Conference on\nComputer, Control, Informatics\
    \ and its Applications (IC3INA), Tangerang Indonesia, 1–2 November 2018;\npp.\
    \ 158–162.\n© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article\
    \ is an open access\narticle distributed under the terms and conditions of the\
    \ Creative Commons Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: AgriEngineering
  limitations: '>'
  pdf_link: https://www.mdpi.com/2624-7402/2/3/29/pdf?version=1594725436
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep Learning Application in Plant Stress Imaging: A Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.biosystemseng.2004.08.019
  analysis: '>'
  authors:
  - I.M. Scotford
  - P. C. H. Miller
  citation_count: 50
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline 1. Introduction 2. Current methods used to assess tiller density and leaf
    area index 3. Experimental details 4. Results and discussion 5. Conclusions Acknowledgements
    References Show full outline Cited by (56) Figures (10) Show 4 more figures Tables
    (4) Table 1 Table 2 Table 3 Table 4 Biosystems Engineering Volume 89, Issue 4,
    December 2004, Pages 395-408 Estimating Tiller Density and Leaf Area Index of
    Winter Wheat using Spectral Reflectance and Ultrasonic Sensing Techniques Author
    links open overlay panel I.M. Scotford, P.C.H. Miller Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.biosystemseng.2004.08.019 Get rights and
    content A 2-year study is reported in which tractor-mounted radiometer and ultrasonic
    sensing systems were evaluated to determine if the combined arrangement could
    be used to estimate tiller numbers (tillers m−2) and the leaf area index (LAI)
    of winter wheat. The system was tested over two growing seasons, on a range of
    winter wheat varieties planted at different seed rates on different soil types.
    By using a combination of the coefficient of variation (CV) of the normalised
    difference vegetation index (NDVI) and a new form of compound vegetation index
    derived from the radiometer and ultrasonic sensor height output measurements,
    the tiller numbers and leaf area index were estimated to an accuracy of ±125 tillers
    m−2 and ±0·47 LAI, respectively, without using any other direct field measurements.
    This study has shown that a tractor-mounted sensing system can be used to assess
    tiller numbers and leaf area index. These values offer the potential to be used
    directly with canopy management principles to aid the agronomic decision-making
    process. They can be used to help determine the optimum level of inputs such as
    fertilisers, fungicides and growth regulators at a given stage of crop development
    and to account for variations within a field. Previous article in issue Next article
    in issue 1. Introduction One of the most popular methods currently used to assess
    crop canopy characteristics is spectral reflectance, whether measured using radiometers,
    spectrometers or digital cameras, and subsequent calculation of a vegetation index
    (Godwin, 2000; Gitelson et al., 2001; Miller et al., 2003). However using this
    in isolation limits the range of crop canopy characteristics that can be sensed
    (Dampney et al., 1998). In addition when crop reaches growth stage (GS) 31 (first
    node detectable), as defined by Zadoks et al. (1974), spectral reflectance measurements
    tend to reach saturation and are less able to discriminate between differences
    in canopies (Dampney et al., 1998; Wiltshire et al., 2002). This limits the usefulness
    of this technique to aid input decisions since the majority of fungicide applications
    (Primrose McConnell''s, 1995) on winter wheat; and the second and third nitrogen
    applications (MAFF, 2000) are generally applied after GS 30 (stem extension).
    Nevertheless, some agronomic inputs have been based on this type of measurement
    with varying degrees of success (Secher, 1997; Bjerre, 1999; Miller et al., 2000;
    HGCA, 2002). It is hypothesised that improvements in decisions relating to agronomic
    inputs could be made if crop cover or green area and crop structure could be measured
    independently, especially if such measurements could be made throughout the growing
    season, and not limited to pre-GS 31 (first node detectable). A review by Dampney
    et al. (1998) and a paper by Miller (2000) both concluded that the combined output
    of multiple sensors would be required in the future to measure both crop cover
    and structure. In support of this, recent work by Scotford and Miller (2004) concluded
    that combining both spectral reflectance and ultrasonic measurements enabled the
    crop to be monitored throughout the growing season, beyond GS 31 (first node detectable)
    which could not be achieved using either sensing approach in isolation. However,
    to make practical use of these types of sensing systems, information about the
    crop canopy that can be linked directly to agronomic decisions is required. For
    example, when using canopy management principles (HGCA, 1998; Sylvester–Bradley
    et al., 2000; HGCA, (2000), HGCA, (2002)), both tiller density (tillers m−2) and
    leaf area index (LAI) defined as the ratio between the total leaf area (one side
    only) per unit area of ground are used to aid nitrogen input decisions. Similarly,
    Secher (1997) and Wiltshire et al. (2002) suggested there is scope to reduce fungicide
    inputs, without reducing yield or quality of winter wheat, by relating application
    to measured crop canopy characteristics. In support of this, both Christensen
    et al. (1997) and Miller et al. (2003) suggest that since many fungicides have
    a mode of action that is predominantly via surface contact, the quantity of spray
    liquid applied, at a given concentration, should be directly related to LAI. Based
    on this evidence, if both tiller numbers and LAI could be remotely sensed, such
    data could be used to produce treatment maps therefore aiding the implementation
    of precision agriculture. This study builds on work previously reported by Scotford
    and Miller (2004). The objective was to investigate techniques in which the normalised
    difference vegetation index (NDVI) measured using radiometers and ultrasonic measurements
    could be used in parallel to provide further information about winter wheat canopies.
    In particular, the ability of a tractor mounted combined radiometer and ultrasonic
    sensing system was evaluated to determine if it could provide estimates of tiller
    density and LAI of winter wheat throughout the growing season. 2. Current methods
    used to assess tiller density and leaf area index 2.1. Measuring tiller density
    The most common method used for measuring tiller numbers is by manual counting.
    A quadrant (square frame) is randomly thrown down onto the crop and the tillers
    within the frame are counted. Alternatively, the tillers along a measured row
    are counted and the tillers m−2 calculated assuming a fixed row width. However,
    both of these methods are extremely time consuming and subject to errors associated
    with selection of sampling position. 2.2. Measuring leaf area index The direct
    method for measuring the LAI of winter wheat and other crops is to destructively
    sample. In this method, all the crop is cut from a unit area of ground and the
    area of the leaves (one side only) are determined using an area meter (Jonckheere
    et al., 2004). However, as soon as the tillers are cut they suffer from water
    stress and become wrinkled, curved or bent making their specific area difficult
    to measure. Alternatively, a random selection of tillers can be cut from the crop
    and their leaf area measured; assuming the tiller density is known the LAI can
    be calculated. These types of measurements are time consuming and subject to errors
    associated with sampling position. Nevertheless, direct measurements are important
    since they are often used for validation of the indirect methods (Jonckheere et
    al., 2004). Leaf area index can be measured indirectly using commercial instruments
    that use models of light transmission through the canopy (Welles & Norman, 1991;
    Welles & Cohen, 1996; Weiss et al., 2004). The general principle is to measure
    photosynthetically active radiation (PAR) above and below the canopy which provides
    information about the penetration of PAR into the crop. The models used then take
    account of factors such as direct and diffuse incident light, leaf PAR absorption,
    solar zenith angle, location and canopy leaf angle distribution to calculate the
    LAI of the crop. Such systems are still relatively labour intensive; nevertheless,
    they do offer a much quicker method of LAI determination than direct methods.
    Where studies have been conducted comparing direct and indirect methods of LAI
    determination (Welles & Norman, 1991; Weiss et al., 2004) it has generally been
    found that the discrepancy between the two has been less than 15%. 2.3. Remote
    sensing methods used for tiller density and leaf area index estimation The most
    popular method used to remotely sense both tiller density and LAI are spectral
    reflectance techniques. Previous studies are summarised in Table 1. Generally,
    these studies have used measurements of NDVI or ratio vegetation index (RVI) and
    compared these measurements with manually counted tiller density or measurements
    of LAI from either direct or indirect measurements. Some good correlations have
    been shown, but generally it was identified that the relationship between tiller
    density or LAI and NDVI or RVI changes with time, requiring such measurement techniques
    to be calibrated for differing crop growth stages. Most of the studies used direct
    field measurements to calibrate their sensing system. Ideally, systems are required
    that can be used anytime during the growing season and which do not require direct
    ground calibration but as yet this goal has not been achieved. Table 1. A summary
    of previous studies using remote sensing methods to estimate tiller density and
    leaf area index Reference Crop Identified relationships Comments Taylor et al.
    (2000) Winter wheat Date 05/12/99, NT=100+2242 INDV, r2=0·95 Date 27/01/00, NT=578+5550
    INDV, r2=0·84 Date 07/03/00, NT=396+2148 INDV, r2=0·87 NDVI was derived from aerial
    digital photography (ADP) and tiller density obtained from manual counting of
    three 0·25 m2 quadrants at eight locations within the field targeted by visual
    interpretation of the ADP map. Standard errors of 44, 162 and 152 tillers m−2
    were obtained for the three respective sampling dates, earliest first. Dampney
    et al. (2001) Winter wheat Date 17/04/00, INDV=0·000385 NT+0·0382, r2=0·37 Data
    obtained on 17/04/00, 15/05/00 and 30/05/00 INDV=−1·56e(−0·6673 ILA)+0·931, r2=0·87
    NDVI derived from hand-held spectro-radiometer (Licor LI-1800). No details given
    on the methods used to measure tiller density or LAI Basso et al. (2001) Soyabean
    Dates 15/07/97, Linear regression between NDVI and LAI, r2=0·76 NDVI derived from
    airborne images and LAI measured using a non-destructive optical device (Licor
    LAI-2000) Boissard et al. (2001) Winter wheat Data obtained on 24/05/00, 08/06/00
    and 22/06/00 ILA=0·091 IRV+0·946, r2=0·53, where RVI is the ratio vegetation index=RNIR/Rred
    RVI derived from tractor mounted radiometers (Skye SKR 1800). No details given
    on the methods used to measure LAI Thomsen et al. (1997) Winter wheat Data obtained
    on 30/03/93, 03/05/93, 25/05/93, 21/06/93 and 27/07193 ILA=−0·0048 IRV2+0·4772
    IRV–0·6958, r2=0·95 RVI derived from airborne images and LAI measured using destructive
    sampling Bryson et al. (2003) Winter wheat Data obtained during 2001/2002 Imagery
    was ground truthed using established crop physiology techniques at each growth
    stage, good correlations were found between imaged and actual tiller density (r2=0·92)
    and LAI (r2=0·87) Airborne hyperspectral imaging techniques were used in crop
    growth models to produce maps of tiller density and LAI. No details given on the
    methods used to measure tiller density or LAI Gupta et al. (2001) Wheat ILA=1·33
    IRV+1·51, r2=0·51 RVI values derived from satellite spectral data. No details
    given on the methods used to measure LAI Wiltshire et al. (2002) Winter wheat
    Data obtained on eight days between 21/03/00 and 18/07/00. Linear regressions
    for each sampling date between LAI and NDVI gave r2 values ranging from 0·18 to
    0·66 NDVI derived from hand held spectro-radiometer (Licor LI-1800). LAI measured
    using a non-destructive optical device (Licor LAI-2000) NT, number of tillers
    in m−2; INDV, normalised difference vegetation index (NDVI); r2, coefficient of
    determination; ILA, leaf area index (LAI); IRV, ratio vegetation index (RVI);
    RNIR and Rred, reflectance in the near infrared and red, respectively. 3. Experimental
    details 3.1. Field experiments Field experiments were conducted over two consecutive
    growing seasons 2001/2002 and 2002/2003. In each season, a range of winter wheat
    crop canopies were obtained by manipulating wheat variety and seed rate. In the
    2001/2002 growing season, a plot trial was drilled on the 19th October in a field
    with heavy clay soil typical of that used for commercial winter wheat production
    in the UK. Three varieties of winter wheat (Claire, Consort and Riband) each sown
    at three different seed rates (50, 150 and 250 kg ha−1), making a total of nine
    treatments which were replicated three times across the plot area making a total
    of 27 plots (Fig. 1). In the 2002/2003 season, a similar plot trial was drilled
    on the 12–13th October. This trial involved two wheat varieties (Claire and Soissons)
    drilled at three seed rates (50, 150 and 250 kg ha−1) on two soil types (sandy
    and heavy clay). This gave a total of 12 treatments each replicated three times
    across the plot area making a total of 36 plots. For ease of identification, the
    plots were numbered 1–18 on the sandy soil and 20–37 on the clay soil (Fig. 2).
    Download : Download full-size image Fig. 1. Layout of plot experiment for growing
    season 2001/2002 for three varieties of winter wheat (Claire, Consort and Riband)
    planted at three seed rates (high, medium and low) Download : Download full-size
    image Fig. 2. Layout of plot experiment for growing season 2002/2003 for two varieties
    of winter wheat (Claire and Soissons), two soil types (clay and sandy) and three
    seed rates (high, medium and low) During both seasons each plot was 4 m wide (the
    drill width) by approximately 20 m to ensure the required seed rate was achieved
    for each plot. Due to the logistics of drill operation at field scale the plot
    experiment for each growing season was designed in blocks and strips which were
    randomised using a statistical software package (Genstat®). To minimise any variability
    associated with weeds, disease and fertiliser deficiencies, the whole plot areas
    were uniformly treated during both growing seasons in line with good agricultural
    practice in terms of weed control, fertiliser inputs, fungicides and growth regulators.
    3.2. Design of experimental measuring system A crop canopy measurement system,
    as used by Scotford and Miller (2004), which incorporated radiometers and an ultrasonic
    sensor, was used for all of the measurements conducted in this study. A brief
    description of the system is given below and a full specification together with
    information on operating and calibrating the system is also available (Scotford
    et al., 2002; Scotford & Miller, 2004). During the first year of experiments,
    the canopy measurement system used two, 2-channel radiometers (Skye, type SKR
    1800) measuring at narrow bandwidths centred at 660 and 730 nm. One radiometer
    measured incoming radiation (sunlight) while the other pointed downwards, measuring
    the reflected light from the crop canopy. The ultrasonic sensor used was a commercially
    available sensor (Pepperl+Fuchs, type UC 2000-30GM-IU-V1) with a sensing range
    of 0·2–2·0 m and was fitted with temperature compensation allowing operation in
    all temperatures likely to be encountered in the UK. The different measuring devices
    were mounted on a 3·75 m wide boom attached to the rear of a tractor so that they
    could be traversed over the crop canopy (Fig. 3). The output from each of the
    sensors was signal conditioned and transferred, via a universal serial bus (USB)
    link, to a laptop computer. Using purpose written software on the laptop computer,
    the radiometers and ultrasonic sensor were calibrated to give an output of NDVI
    and crop height, respectively (Scotford et al., 2002; Scotford & Miller, 2004).
    Download : Download full-size image Fig. 3. Radiometers and ultrasonic sensors
    mounted on a 3·75 m boom allowing them to be traversed over the crop canopy Following
    the first growing season and prior to the second, modifications were carried out
    to the canopy measurement system which included installing a dedicated power supply
    to the canopy measurement system, rather than using power supplied from the tractors
    electrical system; installing an additional downward pointing two-channel radiometer,
    similar to the radiometers already used on the canopy measurement system (Skye,
    type SKR 1800); and installing an additional ultrasonic sensor similar to that
    already used (Pepperl+Fuchs, type UC 2000-30GM-IU-V1). These modifications enabled
    greater versatility of the system. The dedicated power supply allowed the system
    to be used on any tractor, while the additional sensors increased the area of
    the crop which could be sampled hence reducing the errors associated with sampling
    position. The modified canopy measurement system was used for all measurements
    conducted during the 2002/2003 growing season. 3.3. Experimental procedure Using
    the canopy measurement system the plots were measured at approximately weekly
    intervals. During the first growing season (2001/2002) the downward pointing radiometer
    and ultrasonic sensor were mounted 1·0 m above the ground. However, towards the
    end of the growing season the crop was more than 0·8 m high and therefore the
    ultrasonic sensor was too close to the crop and was operating outside its sensing
    range. To alleviate this problem the sensor height above the ground was increased
    to 1·1 m in the second season (2002/2003). For all experiments during both growing
    seasons the sampling frequency was 2·0 Hz and the run time was 10 s, therefore
    approximately 20 values of NDVI and crop height were recorded for each plot. The
    forward speed of the tractor was set at 0·22 m s−1 therefore during a typical
    10 s scan, a linear distance just over 2·0 m was covered resulting in sensor readings
    being recorded every 0·11 m travelled. During the first growing season (2001/2002)
    the NDVI and crop height of the plots were measured at approximately weekly intervals
    between 25 March and 2 August 2002, representing GS 25 (mid tillering) to GS 91
    (grain ripening), respectively. Similarly during the second growing season (2002/2003)
    the plots were measured at approximately weekly intervals from 14 March to 11
    June 2003, representing growth stages between GS 25 (mid tillering) to GS 73 (early
    milk), respectively. During the first growing season (2001/2002) the tiller numbers
    were counted on each of the plots on two occasions 12 April 2002 and 10 May 2002
    representing GS 25 (mid tillering) and GS 31 (first node detectable) respectively.
    The LAI was measured on the 9 May 2002 representing GS 31 (first node detectable).
    In contrast the GS and manually measured crop height were measured at approximately
    weekly intervals. During the second growing season (2002/2003) the tiller numbers,
    LAI, GS and manually measured crop height were all measured at weekly intervals.
    Table 2 details the methods used to measure these parameters in each of the growing
    seasons. Table 2. Methods used to measure crop characteristics in each of the
    growing seasons Measured parameter Methods used to measure the parameter Empty
    Cell Growing season 2001/2002 Growing season 2002/2003 Tiller numbers, tillers
    m−2 The number of tillers in three 1 m rows were counted on each plot and tillers
    m−2 calculated in accordance to HGCA (1998) The number of tillers in five 0·5
    m rows were counted on each plot and tillers m−2 calculated in accordance to HGCA
    (1998) Growth stage (Zadoks et al., 1974) Assessed by visual inspection of the
    crop Assessed by visual inspection of the crop Crop height, m Assessed using a
    metre rule at three locations within each plot, the average of these being recorded
    as the crop height. The crop height was measured using a metre rule and recorded
    at five locations within each plot, the average of these measurements being taken
    as the crop height for each plot Leaf area index (LAI) LAI was assessed using
    destructive sampling. 20 tillers per plot were selected and their area measured
    using a Optimax image analysis system LAI assessments were based on light transmittance
    measurements through the canopy using a commercially available instrument (SunScan
    canopy analysis system—Delta T Devices, type ss1) 4. Results and discussion 4.1.
    Plot establishment Heavy rain in October 2001, at the start of the 2001/2002 growing
    season resulted in the plots being drilled into a less than ideal seedbed, resulting
    in lower than expected establishment. Consequently, the low seed rate plots, 50
    kg ha−1, for all three varieties were not monitored during the experiment due
    to insufficient plant numbers. The results reported in this paper for the 2001/2002
    growing season are therefore for all three varieties at the two higher seed rates,
    namely 150 and 250 kg ha−1. In contrast no problems were encountered with crop
    establishment during the 2002/2003 growing season and all 36 plots were monitored.
    However, early spring 2003 was particularly dry resulting in slower than normal
    development of the canopies. Nevertheless, the plots over the two growing seasons
    represented a wide range of varieties and crop densities grown on two soil types
    typical of those likely to be found in commercial wheat crops (Primrose McConnell''s,
    1995; HGCA, 1998). For the duration of the experiment the observable effects of
    variability associated with weeds, disease and fertiliser deficiencies were negligible
    in any of the plots. Only low levels of weeds and disease were observed on plots
    in either growing season and none of the plots indicated any signs of nutrient
    deficiencies. Hence, the uniform treatment was successful and it was considered
    that any variability between the plots was as designed and as a result of the
    variety and seed rates differences. This was important since differences in weed
    densities, disease levels or fertiliser levels can influence the NDVI values resulting
    in misinterpretation of the data. 4.2. Estimating crop density using the ultrasonic
    sensor To estimate crop density it was suggested by Scotford and Miller (2004)
    that the coefficient of variation (CV) of the ultrasonic signal could be used
    to estimate tiller numbers. This hypothesis is based on the premise that there
    would be less variation in the ultrasonic sensor signal (i.e. a lower CV value)
    as it traversed over a dense crop canopy (one with a large number of tillers per
    unit area) compared with one of lower density. To investigate this hypothesis,
    the CV of the ultrasonic sensor output was calculated for each plot on each of
    the measuring occasions. The CV values are given in Table 3 where the CV for each
    treatment is the average CV for the three replicate plots of the same treatment
    and the standard deviation is that between the three replicate plots. From this
    table it can be seen that the CV calculated for the 2001/2002 growing season was
    generally higher, as expected, for the medium seed rate plots compared with those
    plots of higher seed rate. A similar trend was also observed during the 2002/2003
    growing season. Although during this season there were only small differences
    between the high and medium seed rate plots, the CV does tend to increase for
    the majority of the low seed rate plots. These two seasons of results indicate
    the potential of this technique to distinguish between different canopy densities
    suggesting that the hypothesis is true and that tiller numbers can be linked directly
    to the CV of the ultrasonic sensor signal. Table 3. Mean values of coefficient
    of variation (CV) of the ultrasonic signal (standard deviation in parentheses)
    for four winter wheat varieties (Claire, Consort, Riband and Soissons), two soil
    types (clay and sandy) and three seed rates (high, medium and low); GS, growth
    stage Date and (GS) Mean values of coefficient of variation (CV) Empty Cell Claire
    Consort Riband Claire Consort Riband Empty Cell High High High Low Low Low 2001/2002
    growing season 11 April (22–29) 52 (19) 60 (57) 33 (12) 96 (47) 75 (19) 56 (32)
    02 May (30–31) 33 (4) 20 (2) 31 (18) 53 (22) 34 (0) 43 (25) 12 May (31–32) 32
    (5) 23 (6) 33 (15) 48 (22) 31 (5) 42 (8) 20 May (37-39) 19 (7) 9 (4) 11 (5) 21
    (6) 22 (4) 16 (7) 29 May (43–47) 8 (1) 9 (0) 9 (3) 14 (3) 14 (6) 9 (3) 11 June
    (55–59) 6 (1) 7 (1) 10 (2) 11 (6) 12 (7) 15 (6) 19 June (65–69) 6 (1) 8 (1) 10
    (8) 9 (1) 18 (10) 13 (4) 26 June (69–73) 6 (1) 9 (3) 8 (2) 8 (2) 13 (7) 12 (8)
    10 July (77–83) 9 (2) 10 (2) 10 (2) 9 (3) 19 (16) 12 (2) 24 July (83–87) 10 (2)
    15 (4) 13 (2) 12 (6) 22 (15) 26 (3) Clay Clay Clay Sandy Sandy Sandy High Medium
    Low High Medium Low 2002/2003 growing season (Claire) 14 March (22–29) 16 (2)
    16 (5) 18 (5) 19 (6) 17 (4) 16 (4) 20 March (22–29) 18 (3) 17 (4) 27 (6) 21 (10)
    28 (16) 31 (14) 27 March (22–29) 18 (1) 25 (5) 23 (8) 18 (9) 24 (17) 22 (11) 4
    April (22–29) 20 (8) 21 (8) 22 (6) 52 (32) 26 (10) 36 (11) 11 April (22–29) 21
    (1) 22 (7) 17 (5) 32 (13) 23 (8) 44 (21) 16 April (22–29) 22 (7) 24 (10) 28 (14)
    32 (17) 37 (13) 48 (12) 23 April (30) 19 (3) 25 (1) 22 (7) 24 (8) 25 (5) 26 (5)
    30 April (31–32) 19 (3) 21 (1) 26 (10) 26 (10) 24 (5) 33 (16) 8 May (32–37) 17
    (2) 20 (3) 24 (6) 13 (5) 20 (4) 35 (14) 14 May (37–39) 14 (3) 15 (4) 30 (9) 11
    (4) 18 (2) 35 (14) 21 May (39–43) 13 (4) 15 (6) 28 (16) 13 (8) 15 (6) 34 (16)
    29 May (51–59) 9 (3) 14 (3) 22 (8) 10 (5) 12 (4) 30 (22) 5 June (59–65) 6 (1)
    6 (1) 14 (6) 7 (5) 7 (1) 11 (6) 11 June (65–71) 6 (1) 9 (7) 14 (4) 6 (1) 6 (1)
    9 (3) 2002/2003 growing season (Soissons) 14 March (22–29) 22 (4) 17 (2) 22 (8)
    20 (6) 21 (5) 26 (5) 20 March (22–29) 20 (8) 18 (7) 28 (4) 27 (9) 25 (6) 34 (16)
    27 March (22–29) 18 (3) 17 (5) 27 (24) 22 (9) 23 (3) 56 (29) 4 April (22–29) 21
    (2) 20 (8) 26 (6) 30 (2) 62 (46) 57 (36) 11 April (22–30) 26 (1) 19 (1) 29 (11)
    28 (3) 30 (10) 42 (19) 16 April (30) 29 (6) 18 (3) 33 (13) 33 (5) 43 (12) 56 (34)
    23 April (31) 24 (6) 20 (5) 36 (13) 24 (1) 26 (7) 26 (11) 30 April (37) 21 (3)
    23 (1) 34 (4) 24 (2) 25 (7) 30 (9) 8 May (39–41) 17 (4) 22 (3) 38 (10) 14 (0)
    17 (5) 31 (7) 14 May (43–47) 9 (2) 10 (1) 33 (9) 9 (2) 15 (5) 24 (7) 21 May (45–59)
    7 (1) 13 (3) 26 (12) 8 (1) 10 (3) 20 (8) 29 May (59–65) 6 (1) 8 (4) 33 (22) 9
    (2) 15 (6) 15 (8) 5 June (65–71) 6 (0) 6 (1) 26 (13) 8 (2) 9 (1) 19 (8) 11 June
    (71–73) 6 (1) 7 (1) 22 (13) 9 (3) 9 (1) 18 (7) During the 2001/2002 growing season
    tillers were only counted on two occasions (12 April 2002 and 10 May 2002) whereas
    during the 2002/2003 growing season tillers were counted on a weekly basis. For
    the occasions when tillers were counted they were plotted against the CV of the
    ultrasonic height measurement (Fig. 4). It can be seen from this graph that, as
    expected the CV tends to increase with decreasing tiller numbers, however there
    is a lot of variability in this data (coefficient of variation r2 of 0·16). Download
    : Download full-size image Fig. 4. Relationship between number of tillers and
    the coefficient of variation (CV) of the ultrasonic signal for winter wheat grown
    during the 2001/2002 and 2002/2003 growing seasons. • 2001/2002 data; ■ Claire
    2002/2003—clay soil; □ Claire 2002/2003—sandy soil; ▴ Soissons 2002/2003—clay
    soil; Δ Soissons 2002/2003—sandy soil When considering the roll associated with
    the ultrasonic sensor being attached to a moving 3·75 m boom, it is not surprising
    that there is variability in these data. It is envisaged that by using active
    suspension systems for maintaining boom stability (Frost & O’Sullivan, 1988; Deprez
    et al., 2002) that this variability may be reduced. It is also known that the
    ultrasonic sensor has a divergence angle of 5° (Scotford & Miller, 2003) giving
    an approximate footprint area of 0·005 m2 when mounted 1 m above the ground and
    therefore is scanning a multiple number of tillers which may increase the variability
    of the signal. When used to assess density it may be the case that if this footprint
    area can be reduced by reducing the divergence angle or mounting the sensor nearer
    to the target that the level of variability in the signal is reduced. However,
    further work is required to test these scenarios, the evidence from this experiment
    suggests that for practical purposes the output from the ultrasonic sensors is
    unlikely to be used to determine tiller numbers. 4.3. Estimating crop density
    using normalised difference vegetation index Similar to the hypothesis used for
    measuring crop density using the CV of the ultrasonic signal, it was also hypothesised
    that the CV of the NDVI measurement would also vary as it traversed across the
    plot and that it too was expected to be less variable on more dense plots. Preliminary
    experiments were conducted where the radiometers were statically positioned above
    the crop canopy and their height varied from 0·3 to 1·3 m above the ground, for
    these heights there was little observed differences in the radiometer readings.
    This indicated that the radiometers were less susceptible to change in height
    above the target than the ultrasonic sensors and therefore the movement of the
    boom should not cause variability in the data. To test this hypothesis, the CV
    of the NDVI measurements were calculated for each plot on each of the measuring
    occasions (Table 4). The CV values are the average CV for the three plots of the
    same treatment and the standard deviation is that between them. On examination
    of these values it can be seen that in all cases the CV of the NDVI measurements
    increased with decreasing seed rates, once again suggesting that tiller numbers
    can be linked directly to the CV of the NDVI measurements. Figure 5 shows the
    comparison between the CV of the NDVI and tiller numbers for the two growing seasons.
    Table 4. Mean values of coefficient of variation (CV) for values of the normalised
    difference vegetation index NDVI (standard deviation in parentheses) for four
    winter wheat varieties (Claire, Consort, Riband and Soissons), two soil types
    (clay and sandy) and three seed rates (high, medium and low); GS, growth stage
    Date and (GS) Mean values of coefficient of variation (CV) Empty Cell Claire Consort
    Riband Claire Consort Riband Empty Cell High High High Low Low Low 2001/2002 growing
    season 11 April (22–29) 12 (3) 14 (6) 15 (4) 15 (4) 20 (6) 27 (9) 02 May (30–31)
    8 (4) 9 (4) 8 (7) 15 (6) 22 (12) 17 (7) 12 May (31–32) 5 (3) 11 (1) 6 (6) 10 (2)
    21 (14) 11 (5) 20 May (37–39) 4 (2) 4 (2) 6 (5) 10 (8) 13 (10) 10 (6) 29 May (43–47)
    4 (2) 3 (1) 4 (5) 7 (1) 14 (10) 9 (6) 11 June (55–59) 3 (1) 5 (4) 2 (1) 8 (3)
    17 (15) 10 (6) 19 June (65–69) 4 (1) 4 (1) 6 (4) 8 (4) 17 (14) 16 (5) 26 June
    (69–73) 8 (3) 9 (1) 9 (2) 12 (9) 21 (22) 9 (4) 10 July (77–83) 8 (1) 19 (2) 10
    (4) 7 (3) 13 (7) 14 (3) 24 July (83–87) 23 (7) 19 (4) 19 (1) 19 (5) 22 (6) 25
    (10) Clay Clay Clay Sandy Sandy Sandy High Medium Low High Medium Low 2002/2003
    growing season (Claire) 14 March (22–29) 5 (2) 6 (1) 15 (4) 3 (1) 6 (1) 14 (1)
    20 March (22–29) 6 (1) 6 (2) 16 (6) 6 (2) 10 (5) 18 (8) 27 March (22–29) 5 (0)
    5 (0) 17 (7) 3 (2) 11 (6) 16 (4) 4 April (22–29) 4 (1) 5 (1) 22 (5) 3 (1) 5 (1)
    14 (3) 11 April (22–29) 3 (0) 3 (0) 18 (5) 3 (1) 7 (3) 17 (8) 16 April (22–29)
    3 (1) 4 (1) 17 (5) 3 (1) 5 (2) 14 (1) 23 April (30) 2 (1) 4 (2) 22 (4) 3 (1) 4
    (2) 12 (8) 30 April (31–32) 2 (0) 3 (0) 12 (4) 2 (0) 4 (1) 11 (8) 8 May (32–37)
    2 (0) 3 (0) 14 (5) 2 (0) 5 (2) 12 (9) 14 May (37–39) 2 (0) 2 (0) 13 (5) 2 (2)
    3 (1) 36 (39) 21 May (39–43) 1 (0) 2 (1) 8 (2) 1 (0) 2 (0) 6 (6) 29 May (51–59)
    1 (0) 2 (1) 10 (6) 2 (0) 3 (0) 7 (6) 5 June (59–65) 2 (1) 2 (1) 9 (4) 3 (2) 3
    (1) 7 (6) 11 June (65–71) 2 (1) 2 (0) 6 (2) 2 (1) 2 (0) 7 (5) 2002/2003 growing
    season (Soissons) 14 March (22–29) 4 (2) 13 (3) 15 (6) 5 (1) 6 (2) 8 (2) 20 March
    (22–29) 3 (1) 8 (2) 23 (4) 3 (0) 6 (1) 19 (9) 27 March (22–29) 3 (1) 7 (2) 23
    (2) 4 (2) 7 (3) 14 (5) 4 April (22–29) 3 (0) 7 (2) 19 (6) 3 (1) 5 (0) 10 (2) 11
    April (22–30) 3 (0) 4 (1) 20 (4) 3 (1) 5 (2) 10 (8) 16 April (30) 2 (1) 3 (1)
    24 (3) 3 (1) 6 (2) 13 (7) 23 April (31) 2 (1) 5 (2) 16 (13) 2 (1) 4 (1) 8 (3)
    30 April (37) 2 (0) 3 (0) 18 (5) 2 (1) 4 (1) 6 (4) 8 May (39–41) 2 (1) 3 (1) 19
    (7) 2 (0) 4 (2) 9 (7) 14 May (43–47) 1 (0) 2 (0) 19 (11) 2 (0) 4 (1) 8 (9) 21
    May (45–59) 1 (0) 2 (0) 15 (6) 2 (1) 3 (1) 6 (3) 29 May (59–65) 2 (1) 2 (1) 13
    (6) 2 (0) 3 (1) 8 (5) 5 June (65–71) 2 (0) 3 (0) 12 (6) 3 (1) 3 (1) 7 (1) 11 June
    (71–73) 1 (0) 2 (1) 8 (4) 3 (1) 2 (1) 7 (5) Download : Download full-size image
    Fig. 5. Relationship between number of tillers and the coefficient of variation
    (CV) of the normalised difference vegetation index (NDVI) for wheat grown during
    the 2001/2002 and 2002/2003 growing seasons. • 2001/2002 data; ■ Claire 2002/2003—clay
    soil; □ Claire 2002/2003—sandy soil; ▴ Soissons 2002/2003—clay soil; Δ Soissons
    2002/2003—sandy soil On examination of this graph it can be seen that there is
    an improved relationship between the CV of the NDVI and the tiller numbers (r2=0·67)
    than observed between the CV of the ultrasonic signal, indicating that this may
    be a useful technique to estimate tiller numbers. The usefulness of this technique
    was further investigated by determining the relationship between CV of the NDVI
    CNDVI and tiller numbers NT during the 2001/2002 growing season and using this
    relationship to estimate the tiller numbers in the 2002/2003 growing season. It
    was found that the best-fit relationship for the 2001/2002 growing season was
    power-law fit of the form: (1) Using this relationship the tiller numbers in m−2
    were estimated for all the treatments used in the 2002/2003 growing season and
    compared against the measured values (Fig. 6). A value of the standard error Es
    per observation was calculated using the generalised equation: (2) where, in this
    example, Ve and Vm represent the estimated and measured values of tiller numbers
    respectively, and N is the number of observations. Although there is some degree
    of scatter in the data, the resulting standard error per observation was ±125
    tillers m−2 which compares favourably the standard error per observation quoted
    by Taylor et al. (2000), given in Table 1, which were calculated for specific
    growth stages. In contrast, the technique described in this paper holds true for
    the main part of the growing season and was not based on direct ground calibration.
    This technique offers potential to estimate a value of tiller density throughout
    the growing season that could be used to aid agronomic decisions. Download : Download
    full-size image Fig. 6. Relationship between estimated number of tillers and counted
    number of tillers for wheat grown during the 2002/2003 growing seasons. ■ Claire—clay
    soil; □ Claire—sandy soil; ▴ Soissons—clay soil; Δ Soissons—sandy soil; ---- ideal
    response estimated tillers=counted tillers It is recognised that this finding
    only applies to the NDVI sampling frequency and forward speeds used in this study,
    which in the case of the latter is probably unrealistic for commercial use. Nevertheless,
    the sampling frequency could be increased from 2·0 Hz used allowing higher forward
    speeds whilst retaining similar sampling distances between measurements, thus
    allowing these approaches to be used in the future. However, it may not be necessary
    to record NDVI values every 0·11 m travelled, greater distances between sampling
    points may still enable estimates of tiller numbers to be achieved. Further work
    is necessary to determine optimum NDVI sampling strategies for this application.
    Furthermore it is recognised that in this study the differences between the plots
    were only associated with planting density and variety. If there had been weed,
    disease or fertiliser differences between the areas of the field being measured
    these findings are unlikely to have held true. 4.4. Estimating leaf area index
    It has previously been suggested (Dampney et al., 1998; Danson & Rowland, 2000)
    that NDVI was only suitable for estimating the LAI of the crop until canopy closure
    or until the crop has a LAI of 3 or more. To illustrate this fact the LAI measured
    during this work is plotted against the measured NDVI values (Fig. 7). As predicted,
    NDVI is only useful up to about LAI 2–3 and then remains fairly constant after
    this point irrespective of the increasing LAI values. It should be remembered
    that the 2001/2002 LAI data were obtained using destructive sampling whereas the
    2002/2003 LAI data were obtained using the SunScan instrument (Delta T Devices,
    type ss1) that uses light interception at the base of the canopy. Nevertheless,
    both of these data sets follow this trend indicating that the two methods used
    for LAI measurement are comparable. Download : Download full-size image Fig. 7.
    Relationship between normalised difference vegetation index (NDVI) and leaf area
    index (LAI) for wheat grown during the 2001/2002 and 2002/2003 growing seasons.
    • 2001/2002 data; ■ Claire 2002/2003—clay soil; □ Claire 2002/2003—sandy soil;
    ▴ Soissons 2002/2003—clay soil; Δ Soissons 2002/2003—sandy soil The fact that
    NDVI cannot be used to directly estimate LAI beyond about 3 is not surprising,
    since the reflectance values that are used to calculate NDVI are reflected mainly
    off the top surface of the crop especially when LAI exceeds 3, whereas LAI is
    a function of both crop height and tiller density (tillers m2). A tall plant is
    likely to have a larger LAI than a shorter plant due to the fact that as it grows
    taller it produces more leaves; and also that the leaves tend to get bigger as
    the plant grows. It follows that having more plants or tillers per unit area will
    also increase the LAI. However, this is based on the premise that all tillers
    have the same LAI for a given height and the LAI to tiller height ratio is constant.
    To test this hypothesis a compound vegetation index was derived from the manually
    measured crop height and the manually counted numbers of tillers per unit area.
    It was assumed that the maximum crop height was 1·0 m and the ideal number of
    tillers per unit area was 600 (HGCA, 1998). Based on this assumption, the crop
    height and tiller numbers were normalised by dividing by 1 and 600, respectively.
    The two normalised values were multiplied together to achieve a dimensionless
    compound vegetation index that was compared directly with the LAI measurements
    (Fig. 8). It can be seen from this graph that there is a good linear relationship
    between the two (r2=0·88) illustrating this is a much better method of estimating
    LAI than using NDVI, especially when the LAI increases beyond three. However,
    crop height and tiller numbers used in this example were, respectively, measured
    and counted manually and therefore are time consuming to conduct and do not offer
    advantages over using destructive sampling or the SunScan instrument to measure
    the LAI. Download : Download full-size image Fig. 8. Relationship between a compound
    vegetation index (derived from the measured crop height and counted tiller numbers)
    and leaf area index (LAI) for wheat grown during the 2002/2003 growing seasons.
    ■ Claire—clay soil; □ Claire—sandy soil; ▴ Soissons—clay soil; Δ Soissons—sandy
    soil; ----- best-fit linear relationship, coefficient of determination r2=0·88
    It has previously been shown that ultrasonic sensors can be used to measure the
    height of the crop to and accuracy of ±0·1 m (Kataoka et al., 2002; Scotford &
    Miller, 2004). It has also been shown in this paper (Section 4.3) that the CV
    of the NDVI measurements can be used to estimate tiller numbers. Therefore it
    was hypothesised that substituting the manually measured values of crop height
    and tiller numbers with values estimated by the sensors used in this study it
    should be possibly to obtain a compound vegetation index that is linearly related
    to the LAI. Using the ultrasonic measurements from the 2002/2003 growing season
    the height of the crop was estimated using the 90 percentile technique developed
    by Scotford and Miller (2004) and plotted against the measured crop height (Fig.
    9). A standard error per observation of ±0·07 was calculated using Eqn (2), which
    compares favourably with those identified by Kataoka et al. (2002); and Scotford
    and Miller (2004). These errors in height measurement are generally associated
    with the roll of the boom caused by the tractor travelling over uneven ground.
    The tiller numbers used were those estimated in Section 4.3 for the 2002/2003
    growing season based on the relationship identified during the 2001/2002 growing
    season. These two estimated values were normalised and multiplied together resulting
    in a dimensionless compound vegetation index and compared with the LAI measurements
    (Fig. 10). A standard per observation of±0·47 LAI was calculated using Eqn (2).
    Although the scatter of the data does tend to increase as the LAI increases, the
    trend for the derived compound vegetation index is to increase with increasing
    values of LAI. In contrast, the trend for conventional LAI comparisons using NDVI
    (Fig. 7) is that NDVI does not tend to increase once LAI increases past about
    3. Hence, illustrating that using a combination of simple measurement techniques
    to derive a compound vegetation index, the LAI of a winter wheat crop can be estimated
    throughout the growing season. Download : Download full-size image Fig. 9. Estimated
    crop height (90% percentile values after Scotford & Miller, 2004) plotted against
    the measured crop height values for wheat grown during 2002/2003 growing season.
    ■ Claire—clay soil; □ Claire—sandy soil; ▴ Soissons—clay soil; Δ Soissons—sandy
    soil; ---- estimated crop height=measured crop height Download : Download full-size
    image Fig. 10. Relationship between a compound vegetation index (derived from
    the output of the combined radiometer and ultrasonic sensing system) and leaf
    area index (LAI) for wheat grown during the 2002/2003 growing seasons. ■ Claire—clay
    soil; □ Claire—sandy soil; ▴ Soissons—clay soil; Δ Soissons—sandy soil; ---- best-fit
    linear relationship, coefficient of determination r2=0·84 5. Conclusions (1) Analysis
    of the ultrasonic data indicated that the coefficient of variation (CV) of the
    height measurements gave an indication of the crop density, but there was a lot
    of variability in the data. As a result the CV of the height measurements cannot
    practically be used to estimate tiller numbers. (2) Analysis of the data indicated
    that the coefficient of variation (CV) of the normalised difference vegetation
    index (NDVI) data could be used throughout the growing season to estimate tiller
    numbers. Using a relationship identified in the 2001/2002 growing season the tiller
    numbers in the 2002/2003 growing season were estimated, without using direct ground
    calibration, to an accuracy of ±125 tiller m−2 when compared with manually counted
    tillers. (3) Leaf area index of winter wheat was found to correlate well with
    a compound vegetation index derived from manually measured values of crop height
    and manually counted values tillers. (4) Using a relationship identified in the
    2001/2002 growing season for estimating tillers numbers, and a crop height estimate
    from the ultrasonic sensors a compound vegetation index was derived which could
    be used to estimate the leaf area index, in the 2002/2003 growing season, to an
    accuracy of ±0·47 when compared to leaf area index measurements obtained using
    a commercially available light interception instrument. These findings hold true
    for winter wheat crops of the variety, seed rate and soil-type combinations used
    in this study over two growing seasons. (5) These combined sensing approaches
    enable winter wheat to be monitored throughout the growing season, beyond growth
    stage 31 (first node detectable) which has generally been the limit of traditional
    spectral reflectance techniques. Using a combination of the coefficient of variation
    of the normalised difference vegetation index combined with the ultrasonic measurements
    enables estimates of both tiller numbers and leaf area index to be made without
    the need for direct ground calibration. These values can be used directly to aid
    the agronomic decision-making process to help determine the optimum level of inputs.
    Acknowledgements The authors gratefully acknowledge the financial support for
    this work provided by the Home Grown Cereals Authority (HGCA) and the Biotechnology
    and Biological Sciences Research Council (BBSRC). The input of Pete Richards and
    Pete Inskip for the development and construction of the radiometer and ultrasonic
    sensing system. Robert Hale, the farm manager, for establishing and managing the
    plots; and Rodger White for his statistical advice. References Basso et al., 2001
    B. Basso, J.T. Ritchie, F.J. Pierce, R.P. Braga, J.W. Jones Spatial validation
    of crop models for precision agriculture Agricultural Systems, 68 (2001), pp.
    97-112 View PDFView articleView in ScopusGoogle Scholar Bjerre K D, (1999) Bjerre
    K D (1999). Disease maps and site-specific fungicide application in winter wheat.
    In: Precision agriculture 99, Part 1: (Stafford J V, ed.), pp 495–504. Second
    European Conference on Precision Agriculture, Odense Congress Centre, Denmark
    11–15 July 1999. SCI Sheffield Academic Press, Sheffield Google Scholar Boissard
    P; Boffety D; Devaux J F; Zwaenepoel P; Huet P; Gilliot J M; Heurtaux J; Troizier
    J, (2001) Boissard P; Boffety D; Devaux J F; Zwaenepoel P; Huet P; Gilliot J M;
    Heurtaux J; Troizier J (2001). Mapping of the wheat leaf area from multidate radiometric
    data provided by on-board sensors. In: Proceedings of the 3rd European Conference
    on Precision Agriculture ECPA (Blackmore S; Grenier G, eds), pp 157–162. Montpellier,
    France, 18–20 June 2001 Google Scholar Bryson R J; Reeves R; Poilve H, (2003)
    Bryson R J; Reeves R; Poilve H (2003). Economic evaluation of the application
    of hyperspectral imagery as an aid to whole farm crop management through precision
    agricultural techniques in the UK. In: Programme Book of the Joint Conference
    of ECPA – PLF (Werner A; Jarfe A, eds), pp 173. Wageningen Academic Publishers,
    The Netherlands Google Scholar Christensen S; Heisel T; Secher B J M; Jensen A;
    Haahr V, (1997) Christensen S; Heisel T; Secher B J M; Jensen A; Haahr V (1997).
    Spatial variation of pesticide doses adjusted to varying canopy density in cereals.
    In: Precision Agriculture 97, Vol. I: Spatial Variability in Soil and Crop (Stafford
    J V, ed.), pp 211–218. First European Conference on Precision Agriculture, Warwick,
    7–10 September 1997. SCI Bios Scientific Publishers, Oxford, UK Google Scholar
    Dampney P M R; Bryson R; Clark W; Strang M; Smith A, (1998) Dampney P M R; Bryson
    R; Clark W; Strang M; Smith A (1998). The use of sensor technologies in agricultural
    cropping systems. A scientific review and recommendations for cost effective developments.
    A review report to the Ministry of Agriculture, Fisheries and Food, London, UK
    Google Scholar Dampney P; Quegan S; Meadows P, (2001) Dampney P; Quegan S; Meadows
    P (2001). Advanced radar for measuring green area index (GAI), biomass and shoot
    numbers in wheat (RADWHEAT). HGCA Project Report No. 252. Home Grown Cereals Authority,
    Caledonia House, London, UK Google Scholar Danson F M; Rowland C S, (2000) Danson
    F M; Rowland C S (2000). Crop LAI from neural network inversion. In: Aspects of
    Applied Biology 60, Proceedings of Remote Sensing in Agriculture Conference (Bryson
    R J; Howard W; Riding A E; Simmonds L P; Stevens M D, eds), 26–28 June 2000, pp
    45–52. The Association of Applied Biologists, Wellesbourne, Warwick CV35, 9EF,
    UK Google Scholar Deprez et al., 2002 K. Deprez, J. Anthonis, H. Ramon, H. Van
    Brussel Development of a slow active suspension for stabilizing the roll of spray
    booms, Part 1: Hybrid modelling Biosystems Engineering, 81 (2) (2002), pp. 185-191,
    10.1006/bioe.2001.0023 View PDFView articleView in ScopusGoogle Scholar Frost
    and O’Sullivan, 1988 A.R. Frost, J.A. O’Sullivan Verification and use of a mathematical
    model of an active twin link boom suspension Journal of Agricultural Engineering
    Research, 40 (1988), pp. 259-274 View PDFView articleView in ScopusGoogle Scholar
    Gitelson A A; Merzlyak M N; Zur Y; Stark R; Gritz U, (2001) Gitelson A A; Merzlyak
    M N; Zur Y; Stark R; Gritz U (2001). Non-destructive and remote sensing techniques
    for estimation of vegetation status. In: Proceedings of the 3rd European Conference
    on Precision Agriculture ECPA (Blackmore S; Grenier G, eds), pp 205–210, Montpellier,
    France, 18–20 June 2001 Google Scholar Godwin, 2000 R.J. Godwin Precision farming—a
    multi disciplinary approach for cereal production Institution of Agricultural
    Engineers, Landwards, 55 (2) (2000), pp. 4-9 Google Scholar Gupta et al., 2001
    R.K. Gupta, D. Vijayand, T.S. Prasad Characterisation of red-near infrared transition
    for wheat and chickpea using 3 nm bandwidth data Advances in Space Research, 28
    (1) (2001), pp. 89-194 Google Scholar HGCA, (1998) HGCA (1998). The Wheat Growth
    Guide. Home Grown Cereals Authority, Caledonia House, London, UK Google Scholar
    HGCA, (2000) HGCA (2000). Canopy management in winter wheat. Topic Sheet No. 40,
    Autumn 2000. Home Grown Cereals Authority, Research and Development, Caledonia
    House, London, UK Google Scholar HGCA, (2002) HGCA (2002). Precision farming of
    cereals, practical guidelines and crop nutrition. Home Grown Cereals Authority,
    London, UK Google Scholar Jonckheere et al., 2004 I. Jonckheere, S. Fleck, K.
    Nackaerts, B. Muys, P. Coppin, M. Weiss, F. Baret Review of methods for in situ
    leaf area index (LAI) determination. Part I, theories, sensors and hemispherical
    photography Agricultural and Forest Meteorology, 121 (2004), pp. 16-35 Google
    Scholar Kataoka T; Okamoto H; Kaneko T; Hata S, (2002) Kataoka T; Okamoto H; Kaneko
    T; Hata S (2002). Performance of crop height sensing using ultra sonic sensor
    and laser beam sensor. ASAE Paper No. 02-1184. ASAE Annual International Meeting,
    Chicago, IL, USA, 28–31 July 2002 Google Scholar MAFF, (2000) MAFF (2000). Fertiliser
    Recommendations for Agricultural and Horticultural Crops (RB 209). Ministry of
    Agriculture, Fisheries and Food, The Stationary Office, London, UK Google Scholar
    Miller P C H, (2000) Miller P C H (2000). Crop sensing and management. In: Proceedings
    of the HGCA 2000 Crop Management into the Millennium Conference, Cambridge, 6–7
    January 2000, pp 6.1–6.12. Home Grown Cereals Authority, London, UK Google Scholar
    Miller P C H; Lane A G; Wheeler H C, (2000) Miller P C H; Lane A G; Wheeler H
    C (2000). Matching the application of fungicides to crop canopy characteristics.
    In: Proceedings of the BCPC Conference-Pests and Diseases 2000, Brighton, UK,
    13–16 November 2000, British Crop Protection Council, pp 629–636 Google Scholar
    Miller P C H; Scotford I M; Walklate P J, (2003) Miller P C H; Scotford I M; Walklate
    P J (2003). Characterizing crop canopies to provide a basis for improved pesticide
    application. American Society of Agricultural Engineers Paper 03-1094. ASAE Annual
    International Meeting, Las Vegas, NV, USA, 27–30 July 2003 Google Scholar Primrose
    McConnell''s, (1995) Primrose McConnell''s (1995). In: The Agricultural Notebook,
    19th edn, (Soffe R J, ed.). Blackwell Science, London, UK Google Scholar Scotford
    I M; Miller P C H, (2003) Scotford I M; Miller P C H (2003). Monitoring the growth
    of winter wheat using measurements of normalised difference vegetation index (NDVI)
    and crop height. European Conference on Precision Agriculture (ECPA) Conference
    Proceedings, Berlin, June 2003, pp 627–632. Wageningen Academic Publishers, The
    Netherlands Google Scholar Scotford and Miller, 2004 I.M. Scotford, P.C.H. Miller
    Combination of spectral reflectance and ultrasonic sensing to monitor the growth
    of winter wheat Biosystems Engineering, 87 (1) (2004), pp. 27-38, 10.1016/j.biosystemseng.2003.09.009
    View PDFView articleView in ScopusGoogle Scholar Scotford I M; Richards P A; Inskip
    P F, (2002) Scotford I M; Richards P A; Inskip P F (2002). Crop canopy measurement
    system (CMS): specification and user guide. Contract Report, CR/1307/02/2457,
    Silsoe Research Institute, Bedford, UK Google Scholar Secher B J M, (1997) Secher
    B J M (1997). Site specific control of diseases in winter wheat. In: Optimising
    Pesticide Application (Western N M; Cross J V; Lavers A; Miller P C H; Robinson
    T H, eds), Vol. 48, pp 57–64. The Association of Applied Biologists, Aspects of
    Applied Biology, Wellesbourne, Warwick, UK Google Scholar Sylvester-Bradley R;
    Spink J; Foulkes M J; Bryson R J; Scott R K; Stokes D T; King J A; Parish D; Paveley
    N D; Clare R W, (2000) Sylvester-Bradley R; Spink J; Foulkes M J; Bryson R J;
    Scott R K; Stokes D T; King J A; Parish D; Paveley N D; Clare R W (2000). Sector
    challenge project—canopy management in practice. In: Proceedings of the HGCA 2000
    Crop Management into the Millennium Conference, Cambridge, pp 11.1–11.14, 6–7
    January 2000. Home Grown Cereals Authority, London, UK Google Scholar Taylor J
    C; Wood G A; Welsh J P, (2000) Taylor J C; Wood G A; Welsh J P (2000). Exploring
    management strategies for precision farming of cereals assisted by remote sensing.
    In: Aspects of Applied Biology 60, Proceedings of ‘Remote Sensing in Agriculture’
    Conference (Bryson R J; Howard W; Riding A E; Simmonds L P; Stevens M D, eds),
    26–28, June 2000, pp 53–60. The Association of Applied Biologists, Wellesbourne,
    Warwick, UK Google Scholar Thomsen A; Schelde K; Heidmann T; Hougaard H, (1997)
    Thomsen A; Schelde K; Heidmann T; Hougaard H (1997). Mapping of field variability
    in crop development and water balance within a field with highly variable soil
    conditions. In: Precision Agriculture 97, Vol. I: Spatial Variability in Soil
    and Crop (Stafford J V, ed.), pp 189–196. First European Conference on Precision
    Agriculture, Warwick, 7–10 September 1997. SCI Bios Scientific Publishers, Oxford
    Google Scholar Welles and Cohen, 1996 J.M. Welles, S. Cohen Canopy structure measurement
    by gap fraction analysis using commercial instruments Journal of Experimental
    Botany, 47 (302) (1996), pp. 1335-1342 CrossRefView in ScopusGoogle Scholar Welles
    and Norman, 1991 J.M. Welles, J.M. Norman Instrument for indirect measurement
    of canopy architecture Agronomy Journal, 83 (5) (1991), pp. 818-825 CrossRefGoogle
    Scholar Weiss et al., 2004 M. Weiss, F. Baret, G.J. Smith, I. Jonckheere, P. Coppin
    Review of methods for in situ leaf area index (LAI) determination. Part II, estimation
    of LAI, errors and sampling Agricultural and Forest Meteorology, 121 (2004), pp.
    37-53 View PDFView articleView in ScopusGoogle Scholar Wiltshire J; Clark W S;
    Riding A; Steven M; Holmes G; Moore M, (2002) Wiltshire J; Clark W S; Riding A;
    Steven M; Holmes G; Moore M (2002). Spectral reflectance as a basis for in-field
    sensing of crop canopies for precision husbandry of winter wheat. HGCA Project
    Report No. 288. Home Grown Cereals Authority, Caledonia House, London, UK Google
    Scholar Zadoks et al., 1974 J.C. Zadoks, T.T. Chang, F.C. Konzak A decimal code
    for the growth stages of cereals Weed Research, 14 (1974), pp. 415-421 CrossRefGoogle
    Scholar Cited by (56) Ag-IoT for crop and environment monitoring: Past, present,
    and future 2022, Agricultural Systems Citation Excerpt : Fruit spectral signature
    was used to identify defects in the fruit that could not be revealed via visible
    light. NDVI and NDRE were used to quantitatively determine crop density as well
    as the nitrogen and water requirements (Scotford and Miller, 2004). We would like
    to highlight that more research is needed to demonstrate the optical measurand
    use in Ag-IoT as they can be used in a wide range of applications in agriculture.
    Show abstract Optoelectronic proximal sensing vehicle-mounted technologies in
    precision agriculture: A review 2019, Computers and Electronics in Agriculture
    Show abstract Application of thermal imaging of wheat crop canopy to estimate
    leaf area index under different moisture stress conditions 2018, Biosystems Engineering
    Citation Excerpt : On the other hand, thermal images can separate the leaves (crop)
    from the soil area by their different signatures, irrespective of illumination
    conditions, due to the differences in the temperature pattern (Jones et al., 2009).
    Scotford and Miller (2004) reported that reflectance-based indices (LAI from digital
    images) are suitable for canopy coverage or when the crop has LAI of 3 or more.
    The result may be otherwise poor. Show abstract A multi-sensor system for high
    throughput field phenotyping in soybean and wheat breeding 2016, Computers and
    Electronics in Agriculture Citation Excerpt : However, the development of field-based
    phenotyping systems has been slower than their greenhouse counterparts. There
    have been numerous reports in the literature on the vehicle based sensor systems
    for crop measurements in the fields; and most of them were developed in the context
    of precision agriculture (Scotford and Miller, 2004; Noh et al., 2006; Sui and
    Thomasson, 2006; Sui et al., 2008; Farooque et al., 2013; Wang et al., 2014).
    Montes et al. (2011) reported a field phenotyping system employing a light curtain
    and spectral reflectance sensors for the biomass determination of maize in early
    developmental stages. Show abstract A mobile sensor for leaf area index estimation
    from canopy light transmittance in wheat crops 2015, Biosystems Engineering Citation
    Excerpt : Regression analyses between mobile CropMeter measurements and SunScan
    SS1 LAI readings have yielded significant relationships and the CropMeter was
    found to be suitable for use in precision plant protection (Dammer et al., 2008).
    A sensor combination with a radiometer and an ultrasonic sensor was tested for
    estimating the LAI in winter wheat (Scotford & Miller, 2004). The capability of
    mobile ground-based laser rangefinder systems for estimating the LAI, biomass,
    and crop height has also been evaluated (Gebbers, Ehlert, Adamek, 2011). Show
    abstract Variations in crop variables within wheat canopies and responses of canopy
    spectral characteristics and derived vegetation indices to different vertical
    leaf layers and spikes 2015, Remote Sensing of Environment Show abstract View
    all citing articles on Scopus View Abstract Copyright © 2004 Silsoe Research Institute.
    Published by Elsevier Ltd. All rights reserved. Recommended articles An anti-noise
    modem for visible light communication systems using the improved M-ary position
    phase shift keying AEU - International Journal of Electronics and Communications,
    Volume 85, 2018, pp. 126-133 Pu Miao, …, Zhimin Chen View PDF Geometrical method
    for interpolating S-peaks from cow ECG using a microcontroller Biosystems Engineering,
    Volume 129, 2015, pp. 324-328 M. Johannes Tiusanen, …, Matti E. Pastell View PDF
    Comparison of methods to assess fear of humans in commercial breeding gilts and
    sows Applied Animal Behaviour Science, Volume 181, 2016, pp. 70-75 Candice Powell,
    …, Paul H. Hemsworth View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 54 Policy Citations: 2 Captures Readers: 72 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Biosystems Engineering
  limitations: '>'
  pdf_link: null
  publication_year: 2004
  relevance_score1: 0
  relevance_score2: 0
  title: Estimating Tiller Density and Leaf Area Index of Winter Wheat using Spectral
    Reflectance and Ultrasonic Sensing Techniques
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/jstars.2013.2248345
  analysis: '>'
  authors:
  - Jiancheng Luo
  - Wenjiang Huang
  - Jing Zhao
  - Jingcheng Zhang
  - Chunjiang Zhao
  - Ronghua Ma
  citation_count: 35
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Journal of Selected Topi...
    >Volume: 6 Issue: 2 Detecting Aphid Density of Winter Wheat Leaf Using Hyperspectral
    Measurements Publisher: IEEE Cite This PDF Juhua Luo; Wenjiang Huang; Jinling
    Zhao; Jingcheng Zhang; Chunjiang Zhao; Ronghua Ma All Authors 33 Cites in Papers
    728 Full Text Views Abstract Document Sections I. Introduction II. Materials and
    Methods III. Results IV. Discussion V. Conclusions Authors Figures References
    Citations Keywords Metrics Footnotes Abstract: Wheat aphid, Sitobion avenae F.
    is one of the most destructive pests that emerge in northwest China almost every
    year, impacting on the production of winter wheat. Hyperspectral remote sensing
    has been demonstrated to be superior to a traditional method in detecting diseases
    and pests. In this study, spectral features (SFs) were examined by four methods
    to detect aphid density of wheat leaf and model was established to estimate aphid
    density using partial least square regression (PLSR). A total of 60 wheat leaves
    with different aphid densities were selected. Aphid density of the leaves was
    first visually estimated, and then the reflectance of leaves was measured in the
    spectral range of 350-2500 nm using a spectroradiometer coupling with a leaf clip.
    A total of 48 spectral features were obtained and examined via correlation analysis,
    independent t-test by spectral derivative method, continuous removal method, continuous
    wavelet analysis (CWA) and commonly used vegetation indices for stress detection.
    Based on variable importance in projection (VIP), five spectral features (VIP
    ≥ 1) were selected from 17 spectral features due to their strong correlation with
    aphid density (R 2 ≥ 0.5) to establish the model for estimating aphid density
    by PLSR. The result showed that the model had a great potential in detecting aphid
    density with a relative root mean square error (RMSE) of 15 and a coefficient
    of determination (R 2 ) of 0.77. Published in: IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing ( Volume: 6, Issue: 2, April
    2013) Page(s): 690 - 698 Date of Publication: 11 April 2013 ISSN Information:
    DOI: 10.1109/JSTARS.2013.2248345 Publisher: IEEE SECTION I. Introduction Wheat
    aphid, Sitobion avenae (Fabricius), is one of the most destructive pests in agricultural
    systems, especially in the regions of temperate climate in the northern and southern
    hemispheres. It occurs annually in the wheat planting area in China. Aphid feeds
    on wheat plants and injects saliva that contains plant toxins, which results in
    significant yield loss [1]. An average of 5 per tiller is reported to be the threshold
    of wheat aphid infestation that requires management actions. A density of 10 aphids
    per tiller results in a 7% yield loss, and 40 aphids per tiller leads to an 11%
    yield loss [2]. Pesticide application is considered to be effective in controlling
    aphid and minimizing the yield loss of winter wheat [3], but the efficacy and
    the cost of pesticides become questionable for long-term use [4]. Because of irregular
    pattern of wheat aphid infestation, excessive amount of pesticide is normally
    applied using automatic spray systems, which inevitably increases the cost of
    wheat production and impacts on the environment [5]. Therefore, for site-specific
    management and judicious application of pesticides, it is important to monitor
    aphid infestation at critical junctures of crop growth and obtain spatial distribution
    information on aphid infestation within a particular field. The conventional way
    to obtain information of density infestation in the field is manual field scouting,
    but it has been proved to be expensive, time-consuming and difficult for large
    farming area. Fortunately, hyperspectral remote sensing technology may be an effective
    alternative for and obtaining spatial distribution information on aphid infestation
    density over a large area due to its relatively low cost and the possibility to
    be mounted on airborne and space-borne platforms. Many studies found that hyperspectral
    remote sensing has potential in detecting crop stress, such as disease [6], water
    stress [7], toxic industrial chemical stress [8], and so on. There has been also
    some progress in detecting aphid infestation density using hyperspectral remote
    sensing. Riedell and Blackmer [9] reported that leaf reflectances in the 625–635
    nm and 680–695 nm ranges, as well as the normalized total pigment to chlorophyll
    ‘a’ ratio index (NPCI) were good indicators of chlorophyll loss and leaf senescence
    caused by Russian wheat aphid (Diuraphis noxia Kurdjumov) and greenbug (Schizaphis
    graminum Rondani) feeding on wheat Yang [10] used a hand-held radiometer in greenhouse
    to characterize greenbug stress in wheat and found that the band centered at 694
    nm and the vegetation indices derived from bands centered at 800 nm and 694 nm
    were the most sensitive to greenbug-damaged wheat. Turk [11] studied the relationship
    between spectral indices and greenbug abundance and found that damage sensitive
    spectral index1 (DSSI1), damage sensitive spectral index2 (DSSI2), simple ratio
    (SR) and normalized difference vegetation index (NDVI) were related to the damage
    caused by greenbug. That laboratory further developed an aphid index (AI) and
    found that AI had the strongest relationship with greenbug density. They concluded
    that remote sensing data had the potential in distinguishing damage by Russian
    wheat aphid and quantifying its abundance in wheat, but success for Russian wheat
    aphid density estimation depended on the selection of the spectral vegetation
    indices [12]. Yang [13] suggested that ratio-based vegetation indices (based on
    800/450 nm and 950/450 nm) were useful in differentiating the stress caused by
    Russian wheat aphid and greenbug in wheat. These findings suggest that remote
    sensing using spectral reflectance and indices can be an effective for non-destructively
    detecting plant stress caused by Russian wheat aphid or greenbug. However, there
    is no systematical exploration of spectral responses induced by aphid infestation.
    In addition, what spectral features (SFs) can be used to detect aphid infestation
    has not been systematically summarized. Therefore, the objectives of this study
    are: 1) to understand the spectral responses induced by wheat aphid infestation
    at leaf level; 2) to establish sensitive SFs set of aphid infestation by examining
    the sensitivity to aphid infestation of a set of possible SFs derived from different
    methods; 3) to develop multivariate model by partial least square regression (PLSR)
    for estimating aphid infestation density at leaf level. SECTION II. Materials
    and Methods A. Field Site and Sampling Winter wheat (Triticum aestivum L.) was
    grown in an experimental field in Beijing Academy of Agriculture Forestry Sciences,
    China, located at 39 ∘ 56 ′ N, 116 ∘ 16 ′ E with an altitude of 56 m. The tested
    wheat cultivar of experimental field was ‘Jingdong 8’ which was widely grown in
    Beijing in 2010 with moderate susceptibility to aphid. During May and June 2011,
    wheat aphid (Sitobion avenae F.) naturally occurred in the whole experimental
    field. Aphid density developed rapidly and reached the maximum in the filling
    stage of winter wheat. The best time to prevent wheat aphid is the early period
    of the filling stage. Therefore, the experiment was conducted on May 13, 2011
    when winter wheat was in the early period of the filling stage. B. Data Measurement
    1. Leaf Sample Collection In order to minimize the influence of other factors,
    we choose the second (from the top) leaf of wheat plant to conduct the experiment.
    Based on different aphid density on the second (from the top) leaf of wheat plant,
    winter wheat plants were collected from the experimental field. Subsequently,
    the wheat plants were transported to a nearby indoor laboratory, and a total of
    60 leaves were chosen and removed from the winter wheat plants using scissors
    by visually estimating based on different aphid densities ranging from 0 to 120
    per leaf. Aphid density (aphid number per leaf) was recorded, and then aphids
    were removed from the sample leaves in order to leaf spectral measurement. 2.
    Leaf Spectral Measurement Reflectance measurements of these sample leaves were
    recorded between 350 and 2500 nm using a Fieldspec FR spectroradiometer (ASD Inc.,
    Boulder, CO, USA) coupling with a leaf clip. The instrument provided sampling
    intervals of 3 nm between 350 and 1050 nm, and 10 nm between 1050 and 2500 nm.
    Signatures collected from this device have 2151 spectral bands sampled at 1 nm
    over the range of 350–2500 nm with a spectral resolution ranging from 3–10 nm.
    [14]. Five reflectance spectra were taken per leaf using an ASD leaf clip covering
    a halogen bulb-illuminated area with a radius totalling 10 mm. A mean reflectance
    spectrum was calculated for each leaf. C. The Methods of Extracting SFs 1. Vegetation
    Indices Aphid damage reportedly includes changes in leaf pigment content, cellular
    structure and water content. Based on a physiological perspective, changes in
    these properties due to aphid are presumably responsible for the corresponding
    spectral changes. In most reviewed article, some vegetation indices (VIs) are
    related with physiological or biochemical parameters of plant. In this study,
    16 VIs which have potential in detecting plant stresses according to the literatures,
    such as crop diseases, drought and insufficient of nitrogen were selected to test
    their potential in estimating leaf aphid infestation density (Table I). Table
    I Various Vegetation Indices Compiled from Literature 2. Spectral Derivative Analysis
    Derivative spectroscopy uses changes in spectral reflectance with respect to wavelength
    to sharpen spectral features. In many cases, derivatives are more effective in
    solving problems of quantitative analysis than ratios and differences [26]. First
    derivative reflectance (FDR) of reflectance spectrum can be derived from the following
    equation: FD R λ(i) = ( R λ(j+1) − R λ(j) ) Δλ (1) View Source where FDR is the
    first derivative reflectance at a wavelength i midpoint between wavebands j and
    j+1 . R λ(j) is the reflectance at waveband j , R λ(j+1) is the reflectance at
    waveband j+1 , and Δλ is the difference in wavelengths between j and j+1 . According
    to the literature review, Table II provides 20 spectral features (SFs) derived
    from first derivative spectra, including spectral position features, spectral
    area features and spectral index features. Table II Spectral Features (SFS) Derived
    from First Derivative Spectra 3. Continuum Removal Transform Continuum removal
    is a numerical method to remove the absorptions of bands not of interest. The
    continuum initially defines a convex hull over the top of a spectrum, fitting
    straight-line segments that connect local spectra maxima. Subsequently, the original
    spectrum values are divided by the continuum line leading to continuum-removed
    spectrum [27]. In this study, a continuum removal process was applied in the spectral
    range from 500 to 780 nm of leaf samples. The definition of absorption features
    was described by Van der Meer [28]. For each leaf sample spectrum, 8 SFs derived
    from continuum removal were calculated: a) H: maximum band depth; b) W1: wavelength
    width at left half of the hall; c) W2: wavelength width at right half of the hall;
    d) A1: left half area of the hall; e) A2: right half area of the hall; f) λ1 :
    the starting wavelength of the hall; g) λ2 : wavelength position of maximum band
    depth in the hall; h) λ3 : the ending wavelength position of the hall. The definitions
    of these 8 SFs are given schematically in Fig. 1. Fig. 1. Definitions of 8 SFs
    derived from continuum removal in the region from 500 to 780 nm: a) H: maximum
    band depth; b) W1: wavelength width at left half of the hall; c) W2: wavelength
    width at right half of the hall; d) A1: left half area of the hall; e) A2: right
    half area of the hall; f) λ1 : the starting wavelength of the hall; g) λ2 : wavelength
    position of maximum band depth in the hall; h) λ3 : the ending wavelength position
    of the hall. Show All 4. Continuous Wavelet Transformation (CWT) Wavelet transform
    uses a mother wavelet basis function to convert the original reflectance spectrum
    into a set of coefficients. The main equation of wavelet transformation can be
    as follows: ψ a,b (λ)= 1 a − − √ ψ( λ−b a ) (2) View Source where a is the scaling
    factor determining the width of the wavelet, b is the shifting factor indicating
    the position, ψ a,b (λ) represents the wavelets that are transformed by scaling
    and shifting the mother wavelet ψ(λ) . In the transformation process, the output
    of CWT is given by Mallat [29]: W f (a,b)=(f, ψ a,b )= ∫ +∞ −∞ f(λ) ψ a,b (λ)dλ
    (3) View Source where f(λ) ( λ=1,2…n,n is the number of wavebands and herein n=2151
    ) is the reflectance spectrum and the coefficients ( W f ( a i , b j ),i=1,2…m,j=1,2,…,n)
    are able to constitute a 2-dimensional scalogram (i.e., an m×n matrix), in which
    one dimension is scale and the other is wavelength. The value of each element
    of the scalogram represents the wavelet power that indicates the correlation between
    the scaled and shifted mother wavelet and a segment of the reflectance spectrum.
    Spectral signals can vary in both amplitude (e.g. feature depth) and scale (e.g.,
    feature width). Each capturing spectral features of different widths by CWT were
    as scales. Narrow absorption features in the original spectrum will be captured
    by wavelets at a low scale (narrow width), whilst the shape of the continuum will
    be captured by wavelets at a higher scale. It has been reported that the shape
    of the absorption features of vegetation is similar to a Gaussian or quasi-Gaussian
    function. Therefore, the second derivative of Gaussian (DOG) also known as the
    Mexican Hat is used as the mother wavelet basis [30]. In this study, the leaf
    reflectance spectra ranged from 350 to 2500 nm and there were 2151 bands available
    (350–2500 nm). Any scale greater than 210=1024 was discarded because the decomposed
    components at high scales did not carry meaningful spectral information. All CWT
    operations were carried out by Matlab (Natick, Massachusetts, USA). D. The Methods
    of Extracting SFs 1. Correlation Analysis Correlation analysis was carried out
    to examine the sensitivity of each SF to aphid infestation density. The sensitivity
    of SFs could be described by the absolute coefficient of correlation (R) between
    SFs and aphid infestation density. The high absolute R indicates a strong sensitivity
    of the SF. 2. Partial Least Square Regression (PLSR) The PLS algorithm combines
    the features of principal components and multiple regression, so it has been shown
    to perform well in identifying important signal form predictors with high multicollinearity
    [31]. In consideration of SFs with multicollinearity, PLSR was used to establish
    the multivariable model for estimating aphid infestation density. PLS algorithm
    iteratively produces a series of model, to find a few PLS factors (also known
    as components or latent variables) which explain most of the variation in both
    the predictor and response variable. It produces variable importance in projection
    (VIP) which measures the contribution of independent variable to the contribution
    dependent variable, and is used to select the most influential predictors in the
    model according to the magnitude of their values. Therefore, VIP is a necessary
    part of PLSR procedure. Based on some studies [32], [33], predictors could be
    classified according to their relevance in explaining dependent variable as: VIP≥1.0
    (highly influential), 0.8≤VIP<1 (moderately influential) and VIP<0.8 (less influential).
    In the study, all SFs at which the VIP values were above a threshold of 1.0 were
    considered important and were used for PLSR model. This method was used to maximize
    model robustness using only these SFs that fitted with the variable to be explained.
    3. Calibration and Validation of Regression Model The entire dataset of 60 leaf
    samples, consisting of reflectance spectra and leaf aphid density, was divided
    into two groups. A total of 36 leaf samples were selected randomly and assigned
    to the calibration of regression model, while the other 24 leaf samples were used
    for validating the model. The predictive capabilities of the model were assessed
    based on the predictive R 2 value, the root mean square error (RMSE) and fitting
    equation between the measured and predicted aphid density. The formula of RMSE
    is: RMSE= ∑ i=1 n ( e i − o i ) 2 n − − − − − − − − − −  ⎷    (4) View Source
    where n is the sample number, and and are the predicted and measured values of
    aphid density, respectively. SECTION III. Results A. Response of Leaf Reflectance
    to Aphid Density of Wheat Fig. 2 shows the aphid-infested leaf spectrum (average
    spectrum of sample leaves with aphid density >80) and non-infested leaf spectrum
    (average spectrum of sample leaves with aphid density <5). It was obvious that
    the reflectance of the leaf infested by aphid was higher in the visible (VIS)
    region and short-wave infrared (SWIR) region, but lower in the near-infrared (NIR)
    region than the reflectance of the non-infested leaf. It has been reported that
    leaf optical property is a function of the plant internal and external structure,
    water content and biochemical concentration [34]. The leaf spectra were mainly
    influenced by pigment concentrations in the VIS region and the leaf internal structure
    in the SWIR region. In the SWIR region, the spectra were dominated by leaf water
    absorption and had the greatest sensitivity to water content [9]. Aphid feeding
    by piercing the leaf and sucking out leaf juice caused a reduction in pigment
    concentrations especially for chlorophylls and leaf water content in the infested
    leaf. Therefore, it exhibited a higher reflectance in the VIS and SWIR regions
    than the non-infested leaf. Also, infested leaf tissue was destroyed and multiple
    scattering became weaker [7], leading to a lower reflectance than the non-infested
    leaf in the NIR region. Fig. 2. Representative spectra of leaves non-infested
    and infested by aphid (The average spectrum of sample leaves with aphid density
    of more than 80 represents the infested leaf spectrum, and the average spectrum
    of sample leaves with aphid density of less than 5 represents the non-infested
    leaf spectrum). Show All The differences of first derivative spectra (Fig. 3)
    between non-infested and infested leave are noticeable in the regions from 510
    to 530 nm and from 680 to 760 nm, which locate in the green edge and red edge
    regions. Compared with the non-infested leaf, the red edge position of infested
    leaf shows a significant shift away toward shorter wavelengths Fig. 3. First derivative
    spectral curves of leaves non-infested and infested by aphid. Show All (a blue
    shifting). The blue shifting phenomenon confirms that extracting red edge optical
    parameters from hyperspectral data can be used to diagnose the plant healthy status
    [35]. B. Extraction of SFs 1. Correlation of Vegetation Indices with Leaf Aphid
    Density Table III shows the results of correlation analysis between each of the
    16 SFs based on spectral indices Table III Coefficients of Determination ( R 2
    ) for Correlations between Spectral Indices and Aphid Density Fig. 4. Correlation
    scalogram relating wavelet power with aphid density in the calibration dataset.
    Show All and aphid density of 36 samples, among which 9 SFs significantly correlated
    with aphid density (p−value<0.001) . The SFs AI, GNDVI and RVSI exhibited higher
    sensitivity to aphid density than all other spectral indices because they had
    R 2 of greater than 0.50 (p−value≤0.0001) . 2. Correlation of SFs Derived from
    Spectral Derivative Analysis with Leaf Aphid Density Correlation analysis was
    carried out between 20 SFs derived from derivative spectral and aphid density,
    respectively. The results showed that the SFs had a significant correlation with
    aphid density (p−value<0.001) except Dy, λb , λy , SDy and SDr (Table IV). Spectral
    index features generally showed higher R 2 than spectral position features and
    spectral area features, overall. There were 10 SFs with a R 2 of greater than
    0.50 and (SDr−SDg)/(SDr+SDg) had the highest correlation with aphid density (
    R 2 =0.62) . 3. Correlation of SFs Derived from Continuum Removal Transform with
    Leaf Aphid Density Table V summarizes the correlation between SFs derived from
    continuum removal transform and aphid density. A2, W2 and λ3 had a significant
    correlation with aphid density (p−value<0.001) and W2 had the highest R 2 value
    ( R 2 =0.43) . 4. Correlation of SFs Derived from Continuous Wavelet Fig. 4 shows
    a correlation scalogram between aphid density and spectral reflectance of leaf
    by continuous wavelet analysis. R 2 was reported for each correlation scalogram
    at each wavelength and scale. The R 2 values were obtained for the linear correlation
    established between wavelet power and aphid density. Therefore, each element of
    the correlation scalogram represented a feature characterized by the R 2 value,
    wavelength and scale. The R 2 values ranged from 0 to 0.65. The most informative
    features were secreted out by cut-off R 2 value over 0.48 which was obtained by
    ranking these features in descending order based on the R 2 values and to retain
    those encompassing the top 5%. The informative wavelet features whose R 2 values
    were more than 0.48 were in the ranges of 484–552 nm, 609–619 nm, 718–770 nm and
    1673–1713 nm in every scale (Fig. 4). The maximum R 2 value in each wavelet feature
    range was selected, and the wavelength and scale of the four distinct wavelet
    features most strongly correlated with aphid density are shown in Table VI. All
    the R 2 values were greater than 0.50. 5. Sensitive SF Set of Aphid A total of
    48 SFs were extracted by spectral indices, spectral derivation analysis, continuum
    removal transform and CWT. There were 34 SFs with p−value≤0.001 . SF with the
    higher R 2 value indicates the larger potential in estimating aphid density. According
    to some researches on crop diseases (Zhang et al. 2012), in this study, we ranked
    these SFs with p−value≤0.001 in descending order based on the R 2 values and to
    retain those encompassing the top 50%. Therefore, the cut-off R 2 value was determined
    as 0.50. There were 17 SFs with R 2 ≥0.5 defined as sensitive features to aphid
    density. The sensitive SFs have great potential in estimating aphid density. So,
    a sensitive SF set of aphid infestation composed of 17 SFs was obtained. The 17
    SFs were AI, GNDVI, RVSI, (SDr−SDg)/(SDr+SDg) , SDr/SDg, SDy/SDb, SDr/SDb, (SDr−SDb)/(SDr+SDb)
    , (SDy−SDb)/(SDy+SDb) , SDr/SDy, (SDr−SDy)/(SDr+SDy) , SDg, SDb, F1, F2, F3, F4.
    Table VII shows the correlation coefficient between the 17 SFs. It is concluded
    that the 17 SFs are high inter-correlated. The absolute value of correlation coefficient
    (R) ranges from 0.54 to 0.90. High correlations among some SFs are because the
    SFs derived from different extraction method might be similar responses to aphid
    infestation density. Table IV Coefficients of Determination ( r 2 ) for Correlations
    between SFs Derived from Derivative Spectra and Aphid Density Table V Coefficients
    of Determination ( R 2 ) for Correlations between SFs Derived from Continuum Removal
    Transform and Aphid Density Table VI Coefficients of Determination ( R 2 ) for
    Correlations between SFs Derived from CWT and Aphid Density C. Extraction of SFs
    Fig. 5 shows the VIP values of 17 SFs from sensitive spectral features set of
    aphid infestation. High VIP value indicated that the SF is of major importance
    for estimating aphid infestation density. The five SFs with highest VIP values
    over 1 were RVSI, B4, F1, F3 and F4, suggesting great potential of explanation
    to be included in the model for estimating aphid infestation density. Therefore,
    five SFs were used to develop multivariate model for estimating aphid infestation
    density by PLSR. Fig. 5. VIP (Variable Importance in Projection) values of each
    variable (D1: (SDr−SDb)/(SDr+SDb) ; D2: (SDr−SDy)/(SDr+SDy) ; D3: (SDr−SDg)/(SDr+SDg)
    ; D4: (SDy−SDb)/(SDy+SDb) ; B1: SDr/SDb; B2: SDr/SDy; B3: SDr/SDg; B4: SDy/SDb).
    Show All The model with the five SFs was further validated using the validation
    dataset, and the potential predictability of model for estimating aphid infestation
    density was assessed using the R 2 and RMSE values of the predicted and measured
    values (Fig. 6). The results showed that multivariable model had revealed great
    potential in estimating aphid infestation density in winter wheat at leaf level.
    Fig. 6. Plots of measured versus predicted aphid density using PLSR model. The
    predicted R 2 , RMSE values and fitting equation shown are obtained from validation
    dataset. Dashed line is 1:1 line. Show All SECTION IV. Discussion Monitoring crop
    disease and pest has been a main challenge in research on crop remote sensing
    and precision management. Most of studies have shown that some vegetation indices
    have the potential in detecting disease and pest for crop. For example, Huang
    et al. [36] found that the photochemical reflectance index (PRI) has a strong
    estimating power for the yellow rust disease in winter wheat at canopy level.
    Other studies reported that SFs derived from spectral derivative analysis and
    continuum removal transform could also effectively detect crop diseases. For example,
    Jiang et al. [37] found that the sum of first derivatives within the red edge
    (SDr) and the green edge (SDg) had a significant negative linear correlation with
    disease severity. CWT has been reported as an emerging and potential tool for
    inversion biochemical constituent concentrations from leaf reflectance spectra
    [30]. However, there were few reports about CWT for detecting disease and pest
    for crop. In this study, a total of 48 SFs were obtained by four methods mentioned
    above, and we examined responses of 48 SFs to aphid infestation in winter wheat
    at a leaf level. In general, CWT seemed to be more promising than the other methods
    for extracting aphid infestation information because 4 SFs derived from CWT were
    all highly significant correlated with aphid infestation density ( R 2 >0.50 ;
    p−value≤0.0001 ). The low scale features (F1: 491 nm, scale 3; F2: 617 nm, scale
    3; F3: 750 nm, scale 2) captured narrow absorption features that were primarily
    influenced by pigment concentration. The high-scale (ranging from 6 to 10) features
    (F4: 1690 nm, scale 6) captured broad changes in strong water absorption region.
    Moreover, F3 was located in the red-edge range and had the strongest correlation
    with aphid density ( R 2 =0.67) . We could conclude that the red edge might be
    the most sensitive to aphid density, which was consistent with the result of Fig.
    3. Some indices and SFs derived from spectral derivative analysis were also sensitive
    to aphid density. On the whole, we found that spectral index features from spectral
    derivative analysis had higher R 2 values than the spectral position features
    and spectral area features because spectral index features could minimize the
    noises from the environment and emphasize the response information from aphid
    infestation. In comparison, the continuous removal transform performed less effectively
    than the other methods as the R 2 values from continuous removal transform were
    all below 0.43. It was possible that the absorption features derived from continuous
    removal transform had low response to aphid infestation density. Sensitive SFs
    set of aphid infestation was composed by 17 SFs with p−value≤0.0001 and R 2 ≥0.5
    . The high correlations obtained between SFs (Table VII), so PLSR was used to
    build multivariable model with 5 SFs selected by VIP for estimating aphid density
    because PLSR can reduces a large number of collinear variables to a few non-correlated
    latent variables or factors. As a result the multivariable model was more effective
    and accurate in predicting aphid infestation density ( R 2 =0.77 ; RMSE=15 ).
    Moreover, 3 SFs in 5 SFs selected by VIP were wavelet features (F1, F3 and F4),
    indicating once again that CWT was a potential method for detecting aphid infestation.
    In conclusion, those sensitive SFs set of aphid infestation and multivariable
    model were significant to estimate aphid infestation in winter wheat at leaf level.
    Table VII Correlation Coefficient (R) between the SFs It should be noted that
    the spectrometric measurements were conducted only after aphids were removed from
    the sample leaves because aphid populations are highly mobile and tend to move
    (e. g. from leaf to leaf, plant to plant) after an infestation or when the surrounding
    environment is no longer suitable for their survival. Therefore, it is obvious
    that for cropping fields under aphid attack, most leaves would appear to be in
    the “post-attack” status (i.e. no aphids on most leaves). For remote sensing observations
    at canopy level or at a larger scale, the spectral contribution from infested
    plants or leaves is dominant, compared with the signal from aphids themselves
    which have little effect on spectral characteristics. It may be concluded that
    the spectral analysis of the post-attacked leaves, as conducted in the present
    study, is meaningful for monitoring aphid attacks over large areas. However, the
    spectra of leaves covered by aphid could be different from the spectra of leaves
    with aphids removed. It will be necessary, therefore, to compare this spectral
    difference using some hyperspectral imaging techniques in the future. SECTION
    V. Conclusions The study obtained 48 SFs derived from vegetation indices, derivation
    spectral transform, continuum removal transform and CWT, and tested their potential
    in estimating aphid infestation density in winter wheat at leaf level. The sensitive
    SFs set of aphid infestation was composed by 17 SFs with p−value≤0.0001 and R
    2 ≥0.5 . Meanwhile, in order to get robust model for estimating aphid infestation
    density, multivariable model with 5 SFs selected by VIP≥1 was developed using
    PLSR. The conclusions were as follows: 1) CWT was a more promising spectral analysis
    method to detect aphid infestation than other methods, because 4 wavelet features
    derived from CWT had extremely significant correlation with aphid density with
    R 2 over 0.5, while the continuums removal transform was less effective in detecting
    aphid infestation with R 2 values of the SFs being below 0.43. In spectral derivative
    analysis, spectral index features performed better than spectral position and
    area features in detecting aphid infestation. 2) The validation result confirmed
    that the model established using PLSR had good potential in estimating aphid infestation
    density in winter wheat at leaf level. Our findings suggested that aphid infestation
    density could be estimated by sensitive SFs and the model at the leaf level. However,
    laboratory analysis at the leaf level is only the first step toward the goal of
    using remote sensing technology to detect aphid infestation at canopy level in
    field condition. It remains challenging to examine and upscale the results to
    canopy level in field condition. In field condition, apart from aphid infestation,
    many factors might affect and change spectral reflectance, such as wheat canopy
    architecture, multiple scattering phenomena, atmospheric effects, and so on. Meanwhile,
    the other possible stresses such as drought and insufficient of nitrogen can also
    have a certain impact on canopy spectra. Therefore, on the one hand, we will test
    the response potential of the sensitive SFs for estimating aphid infestation at
    canopy level of winter wheat. On the other hand, we need to increase the strength
    of the linkage between leaf-level and the canopy-level SFs according to some physically-based
    models; more observations have to be investigated for detecting other stresses.
    Finally, our findings need to be applied and tested to more wheat varieties. ACKNOWLEDGMENT
    The authors are grateful to Mr. Weiguo Li, and Mrs. Hong Chang for data collection.
    We are acknowledged for Lake and Watershed Data Center, Scientific Data Sharing
    Platform for Lake and Watershed. Authors Figures References Citations Keywords
    Metrics Footnotes More Like This Camera-based Heart Rate measurement using continuous
    wavelet transform 2017 International Conference on System Science and Engineering
    (ICSSE) Published: 2017 Agriculture change detection model using remote sensing
    images and GIS: Study area Vellore 2012 International Conference on Radar, Communication
    and Computing (ICRCC) Published: 2012 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Journal of Selected Topics in Applied Earth Observations and Remote
    Sensing
  limitations: '>'
  pdf_link: null
  publication_year: 2013
  relevance_score1: 0
  relevance_score2: 0
  title: Detecting Aphid Density of Winter Wheat Leaf Using Hyperspectral Measurements
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.compag.2017.09.038
  analysis: '>'
  authors:
  - Mubarakat Shuaibu
  - Won Suk Lee
  - John K. Schueller
  - Paul Gader
  - Young Ki Hong
  - Sang-Cheol Kim
  citation_count: 36
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methods
    3. Results and discussion 4. Conclusion Acknowledgement References Show full outline
    Cited by (36) Figures (7) Show 1 more figure Tables (4) Table 1 Table 2 Table
    3 Table 4 Computers and Electronics in Agriculture Volume 148, May 2018, Pages
    45-53 Original papers Unsupervised hyperspectral band selection for apple Marssonina
    blotch detection Author links open overlay panel Mubarakat Shuaibu a, Won Suk
    Lee a, John Schueller b, Paul Gader c, Young Ki Hong d, Sangcheol Kim d Show more
    Add to Mendeley Share Cite https://doi.org/10.1016/j.compag.2017.09.038 Get rights
    and content Highlights • Hyperspectral images of different stages of apple Marssonina
    blotch were collected and analyzed. • Orthogonal subspace projection was used
    in performing both feature selection and redundancy reduction. • Selected bands
    were mostly found in the NIR region. • Selected features performed well in distinguishing
    between AMB severity stages. Abstract Apple Marssonina blotch (AMB) is a severe
    fungal disease that has been plaguing top apple producing countries in the world
    since it was first found in Japan in 1907. The disease causes premature defoliation
    and eventually leads to fruit shrinkage and reduction of starch content. AMB has
    a long latency period ranging from two to five weeks and at its early symptomatic
    stage, the disease develops symptoms similar to other apple blotch-like diseases,
    thus making it difficult to detect using only visible information. Hyperspectral
    imagery was investigated in this study for the detection of different stages of
    AMB. While hyperspectral images contain a wealth of information that can help
    distinguish between similar-looking objects, they also contain a large amount
    of redundancy. An unsupervised feature selection method called orthogonal subspace
    projection (OSP) was used to perform feature selection and redundancy reduction
    simultaneously. Ten optimal spectral bands were selected using the algorithm,
    with six out the selected bands within the same near-infrared spectral region.
    These bands served as input features for three classifiers—ensemble bagged, decision
    tree and weighted k-nearest neighbor. The selected bands and classifiers achieved
    overall accuracy ranging from 71.3% to 84.3%, thus indicating the feasibility
    of using the OSP feature selection method for reducing the size of hyperspectral
    data and designing a multispectral imaging system for detecting various AMB disease
    stages. Previous article in issue Next article in issue Keywords AppleBand selectionHyperspectral
    imagingMarssonina blotchOrthogonal subspace projection 1. Introduction Based on
    recent studies conducted by the Food and Agriculture Organization (FAO), the apple
    fruit ranked second worldwide, behind banana, in terms of production. Apples are
    highly nutritional, and they are produced in all continents of the world. The
    latest statistics from the FAO recorded over 80.8 million metric tons of apples
    produced in 2013, and Asia alone accounted for over 60% that quantity (FAO, 2013).
    For these reasons, the apple is regarded as an extremely important fruit crop.
    However, a severe fungal disease called apple Marssonina blotch (AMB) has been
    causing a decline in apple production in some of the top producing continents
    in the world including Asia, North America, and Europe. AMB is caused by a pathogen
    called Diplocarpon mali, and it has been spreading explosively since it was first
    discovered in Japan over a decade ago (Harada et al., 1974, Lee et al., 2011,
    Lee and Shin, 2000, Tamietti and Matta, 2003). The disease pathogen primarily
    infects the leaves of apple trees, and it ultimately leads to a reduction of fruit
    size and starch content. AMB is rain and wind dispersed, and it usually occurs
    in the summer after periods of extended rainfall. After the disease pathogen has
    infected a tree, it typically takes between two to five weeks before symptoms
    begin to develop. Early-stage symptoms first appear as dark green or brownish
    spots on the surface of the leaf, after which the disease progresses to a stage
    where the spots coalesce, and necrotic and chlorotic spots appear. Thiophyphanate-methyl
    and other fungicide treatments are being used by apple growers to control the
    spread of the disease. However, it has been reported that AMB is becoming resistant
    to some of these treatments especially at the symptomatic stage and as a result,
    the disease is better detected at the asymptomatic stage (Tanaka et al., 2000).
    Methods capable of detecting the disease at this stage would not only help better
    control the progression of the disease, but they will also minimize fungicide
    wastage and environmental pollution. A study conducted by Lee et al. (2012) showed
    the feasibility of using optical coherence tomography for diagnosing AMB at both
    the asymptotic and early-symptomatic stages. From two-dimensional (2D) and three-dimensional
    (3D) imaging scans created by the system, they found distinctive differences between
    the inner cross-sectional layers of healthy and diseased leaves. While promising
    results were reported, this technique is a destructive method for detecting AMB
    progression. Non-destructive methods for disease detection using remote sensing
    technologies are becoming more prevalent in agriculture. Remote sensors are capable
    of extracting information about an object without coming in physical contact with
    it. The primary type of remote sensors used for most agricultural applications
    detects natural radiation that is emitted or reflected by an object and makes
    use of surface reflectance between the visible and near-infrared (NIR) regions
    of the electromagnetic spectrum to provide information about the object. Spectroscopy,
    multispectral imaging, and hyperspectral imaging are just a few examples of remote
    sensing techniques that have been employed for quality and quantity assessment
    of crops, including disease detection. Infected plants have been shown to produce
    spectral characteristics different from healthy ones due to the difference in
    the way they absorb light in the visible and NIR spectral regions. Healthy plants
    generally have lower reflectance in the visible spectral range due to the absorption
    of pigments like chlorophyll and anthocyanin while they exhibit higher reflectance
    than infected plants in the NIR region due to an abundance of light scattering
    within the cell structure of the leaf; and in the short-wave infrared (SWIR) range
    of the spectra, healthy plants typically have lower reflectance resulting from
    the absorption of water and proteins (Jacquemoud and Ustin, 2001; Lee et al.,
    2010). Yuan et al. (2014) applied spectroscopy for the detection of a highly destructive
    disease in winter wheat called powdery mildew. They extracted 32 spectral features
    and examined them using independent t-test and correlation analysis. Furthermore,
    they applied both partial least squares regression (PLSR) and multivariate linear
    regression (MLR) for estimating disease severity. Based on their report, the PLSR
    model outperformed the MLR model and achieved a correlation of determination (R2)
    of 0.8 using seven regression components. Huanglongbing (HLB), arguably the most
    severe disease affecting the citrus industry in Florida and other regions of the
    world, has been analyzed be several researchers using spectrometric technology.
    One of such analyses, conducted by Sankaran et al. (2011), yielded an accuracy
    of 98% for HLB detection using a quadratic discriminant analysis classification
    algorithm. It was reported that the raw spectral data originally consisting of
    989 spectral bands was reduced to 24 features using a feature extraction algorithm
    known as principal component analysis (PCA). Jones et al. (2010) also investigated
    the use of reflectance spectroscopy in the quantitative analysis of a tomato disease
    called bacterial leaf spot. They found significant wavelengths from the absorbance
    spectra that distinguished between several degrees of disease infestation using
    both PLSR and stepwise multiple linear regression (SMLR). The predictive disease
    model built based on spectral data achieved an R2 of 0.82. In 2008, it was reported
    that a preliminary study of visible and near-infrared reflectance spectroscopy
    produced a model that appeared to be valuable in the early detection of Botrytis
    cinerea on asymptomatic eggplant leaves. The resulting backpropagation neural
    network (BP-NN) model was found to have an accuracy ranging between 70% and 85%
    in predicting fungal infections (Wu et al., 2008). Xu et al. (2007) used a combination
    of spectroscopy and Fourier transform to detect leaf-miner disease on tomato leaves.
    They found that infected leaves had lower reflectance than healthy ones in the
    NIR range (800–1100 nm) due to damage in the cell structure of the leaf. Whereas
    at the SWIR region, diseased leaves showed higher reflectance values especially
    at the water absorption bands of 1450 nm and 1900 nm—an indication of low water
    content in the leaves. The determinate model built by the authors associating
    severity of disease damage with reflectance at key wavelengths yielded a coefficient
    of determination of 0.92. While spectroscopic instruments such as spectrometers
    offer several advantages including ease of use and cost-effectiveness, they are
    only capable of extracting spectral information of a small region of an object.
    Another major disadvantage of using this technique is that it does not contain
    spatial information which can be used in analyzing and visualizing images for
    object identification. Multispectral and hyperspectral imaging technologies resolve
    the shortcomings of spectroscopy by combining two sensing methods for their operation—spectroscopy
    and imaging. They are both capable of capturing spatial and spectral information,
    and they contain spectral bands that extend beyond the visible range of the electromagnetic
    spectrum. Multispectral imaging systems differ from hyperspectral in that the
    former contains broader bands and typically comprises less than ten spectral bands.
    Hyperspectral systems, on the other hand, contain narrower bands and usually comprise
    hundreds of spectral bands. In other words, hyperspectral systems have finer spectral
    resolution than multispectral systems. Multispectral sensors are more portable
    than hyperspectral sensors and can be easily integrated into most remote sensing
    platforms. Due to their high spectral resolution, hyperspectral images can provide
    more accurate information about a material than multispectral images, but they
    also contain a great deal of redundant information. Plant diseases have been detected
    by several researchers by combining hyperspectral data and feature selection algorithms
    as a preliminary step in developing multispectral sensors for on-the-field plant
    disease detection. Qin et al. (2009) developed a hyperspectral imaging approach
    for detecting canker lesions on Ruby Red grapefruit with a resulting accuracy
    of 95.2%. It was concluded that hyperspectral imaging technique coupled with the
    spectral information divergence (SID) based image classification method was effective
    in discriminating citrus canker from other surface diseases that have threatened
    the marketability of citrus. Hyperspectral imaging was also found to be a valuable
    tool in detecting crop diseases in their early stage using a procedure based on
    both support vector machines and spectral vegetation indices. This method distinguished
    diseased from non-diseased sugar beet leaves as well as differentiated between
    leaves infected by three pathogens at the asymptomatic stage with accuracies between
    65% and 90% (Rumpf et al., 2010). Also, a study of sugarcane crops affected by
    orange rust disease revealed the potential of using hyperspectral imagery for
    detecting the disease (Apan et al., 2004). The combination of visible and near-infrared
    spectral bands with the moisture-sensitive band, 1660 nm, yielded increased ability
    to identify rust-affected areas. However, disease-water stress indices (R800/R1660;
    R1660/R550; (R800 + R550)/(R1660 + R680)) performed the best in targeting affected
    areas. A preliminary test conducted by Del Fiore et al. (2010) demonstrated the
    usefulness of hyperspectral imagery in the early detection of certain toxigenic
    fungal species in maize including A. flavus, A. parasiticus and F. verticillioides.
    They successfully used the visible and near-infrared spectral ranges to distinguish
    between healthy and infected maize kernels. Using a combination of discriminant
    analysis and principal component analysis, they discovered three wavelengths—410 nm,
    535 nm and 945 nm— that were able to detect at least one of the fungal species
    after three days of incolution. Shafri and Hamdan (2009) detected Ganoderma basal
    stem rot disease in oil palm trees with an accuracy ranging between 73% and 84%
    using a combination of aerial hyperspectral imagery and different red edge position
    (REP) algorithms. The results of their analysis showed that the Lagrangian interpolation
    REP technique was most effective in discriminating between healthy and diseased
    trees. Most feature selection methods that have been applied to hyperspectral
    data are computationally expensive because they find optimal features by performing
    an exhaustive search that involves comparing each pair of bands instead of evaluating
    them jointly; while others preserve the original information by producing transformed
    features (Bajcsy and Groves, 2004, Chang et al., 1999, Keshava, 2004, Shen and
    Bassett III, 2002). Another problem faced with using most feature selection algorithms
    is that they perform feature selection and redundancy reduction in isolation.
    Du and Yang (2008) proposed a fast and efficient unsupervised feature selection
    method based on using a similarity metric that is commonly employed for endmember
    extraction called orthogonal subspace projection (OSP). When used as a band selection
    method, the OSP algorithm finds the most distinctive and informative bands in
    a hyperspectral dataset. Due to the many advantages OSP offers, it was used in
    this work for performing joint feature selection and redundancy reduction. The
    primary objective of this study was to evaluate the potential of using hyperspectral
    images for the identification of different stages of AMB. The specific objectives
    were to: i. determine optimal spectral bands for AMB disease detection using the
    OSP algorithm, and ii. develop a classification algorithm capable of distinguishing
    between various degrees of AMB infestation. 2. Materials and methods 2.1. Hyperspectral
    data acquisition Hyperspectral images of Fuji apple leaves were obtained from
    an experimental apple orchard situated in Gunwi-City, Korea. The test region measured
    40 m × 60 m with a total of 260 trees set aside for the experiment. Datasets were
    acquired between September and November 2015 using a hyperspectral camera (Spectral
    camera PS-V10E, SPECIM Inc., Finland). The sensor was used to acquire images of
    tree leaves in an enclosed area, and it consisted of an imaging spectrograph covering
    the spectral range of 356–1000 nm and a sensitive high speed interlaced charge-coupled
    device (CCD) detector. The spatial resolution of the system was about 9 µm while
    its spectral resolution was adjusted to 2.8 nm. The sensor’s output was saved
    as digital 12-bit raw files. Each hyperspectral image measured approximately 975
    (lines) × 696 (samples) × 519 (bands). The indoor setup also consisted of a light
    source (Halogen reflector, JCR 15V 150W 5H 1CT, Philips, England), a linear travel
    translation stage used in moving samples horizontally as line scan images were
    acquired by the camera at a distance of about 0.2 m between them, and a laptop
    installed with a spectral imaging system software (SpectralDAQ-PFD) was used for
    controlling the parameters of the camera. The system, with the exclusion of the
    computer, was assembled in a dark chamber to reduce the effect of ambient light
    during image acquisition. Over 1000 leaves were taken off uninoculated trees,
    and they included healthy mature, healthy young, AMB early stage, AMB advanced
    stage and molybdenum nutrient deficient leaves shown in Fig. 1. A total of 2000
    pixels were extracted from hyperspectral images of each group within defined regions
    of interest on individual leaves. The five classes defined for the analysis were
    healthy, AMB asymptomatic (ambE_asymp), early stage AMB symptomatic (ambE_symp),
    advanced stage AMB (ambA), and nutrient deficient (nd). Samples for the ambE_asymp
    class were taken a few pixels away from amE_symp regions based on recommendations
    from plant pathologists who have a long experience with AMB at the research site
    where hyperspectral data for this analysis was acquired. Download : Download high-res
    image (174KB) Download : Download full-size image Fig. 1. Apple leaves used for
    indoor hyperspectral analysis: (a) healthy mature, (b) healthy young, (c) AMB
    early stage, (d) AMB advanced stage, and (e) molybdenum nutrient deficient. 2.2.
    Image preprocessing Flat field correction, using a 99% white reflectance standard
    and a dark current measurement automatically generated by the hyperspectral system,
    was used to calibrate each leaf image to reflectance and the calibration equation
    is given below: (1) where is the corrected reflectance image, is the original
    uncorrected image, and and are the mean radiance spectral values of regions of
    interest extracted from dark current and white reflectance images, respectively.
    After images were calibrated to reflectance, a Savitzky-Golay filter, with a second-degree
    polynomial and a filter width of 7, was used to smooth and minimize noisy signals
    in the images. All hyperspectral images were spectrally subsetted to remove noisy
    bands. A total of 437 spectral bands between wavelengths 400 nm and 957 nm were
    retained afterward. All hyperspectral image analyses, including the preprocessing
    steps, were performed using a combination of ENVI 5.2 (Exelis Visual solutions
    information Inc., Boulder, CO, USA) and MATLAB R2014a (Version 8.3, The MathWorks
    Inc., Natwick, MA, USA). 2.3. Unsupervised feature selection using orthogonal
    subspace projection (OSP) Unlike other similarity measures which take measurements
    from pairs of bands, OSP evaluates bands jointly. The algorithm is less computationally
    expensive than most other band selection methods because the others find optimal
    band combinations by performing an exhaustive search; whereas, OSP performs an
    unsupervised sequential forward search to find the best bands. OSP performs band
    selection as follows: assuming there are M bands in the original dataset, to find
    the first band, the algorithm randomly selects a band for band 1, A1 and projects
    all the other M-1 bands to its orthogonal subspace. It then finds a second band,
    A2, with the maximum projection in A1′s orthogonal subspace; this is considered
    as the band most dissimilar to A1. All other M-2 bands are now projected on A2′s
    orthogonal subspace, and band A3 is chosen as the band with maximum projection
    in A2′s orthogonal subspace. The algorithm continues until Ai+1 = Ai-1. When this
    occurs, Ai+1 is selected as the true band 1, B1 and Ai is selected as the true
    band 2, B2. To find the third band, B3 (and subsequent bands), the algorithm finds
    the band that is most dissimilar from B1 and B2 by using the orthogonal subspace,
    , of both bands defined below: (2) (3) where is an identity matrix, is the number
    of pixels per band, and is an × 2 matrix with the first column containing all
    pixels in band 1, B1, and the second column includes all pixels in band 2, B2.
    (The superscript, means transpose). After implementing (2), the projection is
    computed; includes all pixels in the original dataset, B and is the component
    of B in the orthogonal subspace of B1 and B2. The band that yields the maximum
    orthogonal component is considered the most dissimilar band to the first two bands
    and will be selected as band, B3. For finding subsequent bands, the size of in
    (2) changes to [B1 B2 B3] and then to [B1 B2 B3 B4] and so forth until the desired
    number of bands are selected. (Note: denotes the norm of ). In this work, OSP
    was used to select optimal spectral bands from regions that provide a measure
    of disease and nutrient stresses in apple leaves optimal spectral bands were used
    as input for three classifiers—ensemble bagged, decision tree and weighted k-nearest
    neighbor (KNN). 3. Results and discussion 3.1. Reflectance spectra and OSP feature
    selection The mean spectra of the five defined classes were created using all
    2000 samples for each class, and it is shown in Fig. 2. From the figure, we notice
    spectral signatures are similar for the classes comprising green leaves (healthy,
    nd and ambE_asymp). The green classes as a whole are noticeably different from
    ambE_symp and ambA especially at the nitrogen absorption band of 550 nm due to
    little or total absence of chlorophyll in the leaves. Another important thing
    to note is that the abrupt rise of the spectral signatures of all the classes,
    except ambA, started around the red edge while this point of inflection in ambA
    occurred earlier—a phenomenon known as blue shift in stressed plant and an indication
    of severe damage to the plant’s cell structure. Download : Download high-res image
    (154KB) Download : Download full-size image Fig. 2. Mean spectra of healthy, AMB
    diseased and nutrient deficient leaves. The reflectance data with 437 bands were
    used as features for the OSP algorithm. To determine the optimal number of bands
    required for the classification process, a stop criterion was introduced by the
    authors based on the difference between consecutive OSP norm values. The point
    at which the difference curve became approximately constant was chosen as the
    appropriate stop region (Fig. 3). This point was chosen because thereafter the
    norm remained almost the same regardless of the number of bands that were added,
    thus indicating the band at this point contained similar information as succeeding
    bands. The norm difference was calculated as: (4) where is the maximum OSP norm
    at the band and is the OSP norm difference between the and bands. Download : Download
    high-res image (82KB) Download : Download full-size image Fig. 3. Plot of OSP
    norm against band numbers. A total of 10 optimal wavelengths were selected by
    the algorithm based on the norm, and they are listed in Table 1. From the table,
    it can be seen six out of the ten spectral bands are between the 921 nm and 957 nm
    range. Spectral bands in this range are capable of distinguishing between leaves
    of similar color and different disease stages better than the visible range. Table
    1. Optimal spectral bands selected using OSP norm. Band number Spectral bands
    Wavelengths (nm) OSP norm 1 896.3 40.7 2 702.4 13.9 3 946.7 9.0 4 957.3 8.9 5
    751.3 8.1 6 937.3 8.1 7 929.4 6.8 8 952.0 6.1 9 921.4 5.6 10 680.7 5.4 3.2. Classification
    using OSP selected spectral bands Before the classification process, fivefold
    cross-validation was applied to the OSP selected spectral bands with a total of
    10,000 observations (2000 for each class) to avoid any bias and overfitting. After
    individual stages of the validation process had been conducted, the classification
    performance results were summed up to create one dataset. To decide the best classifier
    for AMB detection, various classifiers with different parameters were investigated
    using the 10 selected OSP bands as input features. The top three classifiers which
    provided the highest accuracies were then selected and they are: (1) decision
    tree with a maximum number of 100 leaves selected using a Gini diversity index
    split criterion, (2) weighted KNN with a Euclidean distance, 10 neighbors and
    a squared inverse distance weight and finally, (3) an ensemble bagged classifier
    with 30 learning cycles and a maximum number of 9229 splits. The accuracy results
    achieved using these classifiers are given in the confusion matrix in Table 2,
    Table 3, Table 4. The cells in the main diagonal of the tables contain the number
    and percentages of correctly classified samples while the other cells contain
    misclassified data. Before applying the selected bands to the classifiers, two
    preliminary classification tests were performed. The first entailed using a combination
    of the full wavelength range of the reflectance data and the top three classifiers.
    The average overall improvement in accuracy, when compared to using the 10 selected
    bands, was about 8% thus indicating huge redundancy in the hyperspectral data.
    The second test was conducted by taking out three spectral bands from Table 1
    (957.3 nm, 937.3 nm, and 921.4 nm) that were either close to another OSP selected
    band on the electromagnetic spectrum and/or had similar OSP norm as another band.
    In this situation, the average overall accuracy reduced by about 4%. Table 2.
    Decision tree classification results using 10 OSP selected bands with an overall
    accuracy of 79.8%. Table 3. Ensemble bagged classification results using 10 OSP
    selected bands with an overall accuracy of 84.3%. Table 4. Weighted KNN classification
    results using 10 OSP selected bands with an overall accuracy of 71.3%. From the
    classification Tables, it can be seen the ensemble classifier achieved the overall
    highest accuracy of 84.3%, followed by decision tree with an accuracy of 79.8%
    and finally weighted KNN with an accuracy of 71.3%. Ensemble also outperformed
    the other two classifiers by correctly mapping more observations to each of the
    five classes. It correctly mapped 1664 pixels to the healthy class while the lowest
    performer for this, KNN, correctly mapped 1330 pixels. Among the five classes,
    ambE_asymp had the lowest accuracy across all three classifiers. Decision tree
    correctly mapped 59.2% of its pixels, KNN mapped 55.7% while ensemble correctly
    classified 67.6%. The most advanced stage of the disease, with yellow leaves,
    had the highest accuracy among all the classes with the highest accuracy achieved
    by the ensemble classifier with a value of 97.3%. Overall classification results
    proved that the OSP feature selection algorithm could efficiently determine optimal
    features for AMB disease stages. The algorithm combined both feature selection
    and redundancy reduction in one single step by implementing a stop criterion based
    on the OSP norm curve. Nine of the ten spectral bands selected by the algorithm
    were spread across the NIR region. This was expected since the NIR region is more
    effective in detecting differences between the cell structure of healthy and seemingly
    healthy vegetation of the same color than the visible range. The only band selected
    from the visible region was from the red channel at the region close to the red
    edge band (680.7 nm). OSP showed the importance of the NIR region in differentiating
    between AMB stages. The ten optimal bands selected by the algorithm worked well
    with all three classifiers with overall accuracies ranging between 71.3% and 84.3%.
    The major reason accuracies above this range were not achieved in this analysis
    was mainly due to the similarities of the spectral signatures of classes comprising
    only green leaves whether healthy or stressed. The hyperspectral sensor used in
    this work had wavelengths that ranged between 400 nm and 1000 nm. An improvement
    in the results would be possible if a sensor capable of capturing the short-wave
    infrared (SWIR) region of the spectrum is added to the one used in this analysis
    to effectively separate healthy and AMB asymptomatic leaves. The SWIR region has
    multiple water absorption bands, and those bands might be more effective in distinguishing
    between the classes than some of the bands in the hyperspectral sensor used in
    this work. Unsurprisingly, all three classifiers correctly mapped more of the
    observations for the advanced stage of AMB correctly than they did for other classes
    due to its distinct color. Another thing to note is that a fraction of its samples
    was still misclassified as other classes. One major reason this occurred was due
    to human error while selecting yellow spots on the leaves versus green. Future
    work on this analysis will include supplementing the current hyperspectral system
    with one that can collect data in the SWIR region. 3.3. Classification of outdoor
    hyperspectral image using OSP selected bands Since the feature selection and classification
    methods were designed for use in an outdoor setting, it was important to also
    test their performance on a new outdoor image. The test was conducted using a
    hyperspectral image of an AMB diseased tree acquired in 2014. The original image
    contained leaves and background—branches, apple fruit, reflectance standards,
    etc. Before applying the classification model, the background was masked using
    a combination of support vector machine (SVM) and occurrence texture filters on
    the selected spectral bands. The first stage of the masking process involved using
    a supervised SVM classifier to mask the background and the results are shown in
    Fig. 4. From the figure, it can be seen not all the regions were successfully
    masked and as a result a second stage masking process had to be performed. Download
    : Download high-res image (173KB) Download : Download full-size image Fig. 4.
    Comparison of the original hyperspectral image and SVM-masked image. (a) Before
    mask was applied, and (b) after the mask was applied. Five occurrence filters
    were applied on the SVM masked image, and their respective results were compared.
    The applied filters—data range, mean, entropy, variance and skewness—were used
    in calculating the image texture in every 3 × 3 processing window. From Fig. 5,
    it can be seen the entropy image emphasized the leaf regions more than the other
    texture images. As a result, it was chosen for the final stage of the masking
    process and the resulting image is shown in Fig. 6. Download : Download high-res
    image (216KB) Download : Download full-size image Fig. 5. Texture images from
    the previously SVM-masked image (a) Data range, (b) entropy, (c) variance, (d)
    mean, and (e) skewness. Download : Download high-res image (160KB) Download :
    Download full-size image Fig. 6. Combined SVM-texture filter masking result. (a)
    Entropy mask, (b) hyperspectral composite image after applying the mask. The ensemble
    classifier had the overall best performance among the three used in this work,
    thus its trained model was used to predict the outdoor classes. The results of
    the classification process are given in Fig. 7. The classifier detected healthy,
    ambE_asymp, ambE_symp and ambA regions on the leaves with majority of the predicted
    pixels mapped to the healthy class. It can also be seen from the figure that some
    ambE_symp pixels were misclassified as healthy. As earlier mentioned, a SWIR sensor
    will be included in future analyses to improve on the classification results.
    Download : Download high-res image (104KB) Download : Download full-size image
    Fig. 7. Ensemble classification results using OSP selected bands. (a) Hyperspectral
    composite image, and (b) ensemble classified image. 4. Conclusion In this study,
    indoor hyperspectral images showing AMB disease progression were collected and
    analyzed. An efficient unsupervised feature selection algorithm called orthogonal
    subspace projection (OSP) was applied to the spectral domain of the images and
    the algorithm extracted ten optimal bands from reflectance data. The OSP algorithm
    performed both feature selection and redundancy reduction at the same time, and
    the number of optimal features was decided using a simple stop criterion based
    on the difference between consecutive OSP norm values. Most of the spectral bands
    selected by the OSP algorithm were from the NIR region, thus indicating the importance
    of this region for AMB detection. OSP not only showed potential for the detection
    of AMB, but it also worked well for detecting nutrient deficiency in apple leaves.
    Three classifiers, ensemble bagged, decision tree and k-nearest neighbor, were
    used in classifying the selected features. The ensemble bagged classifier had
    the overall best performance with an accuracy exceeding 84%. Based on the results
    obtained from analyzing the indoor hyperspectral data, a low-cost multispectral
    imaging system can be built and mounted on either a ground-based platform or an
    aerial platform and this will enable apple growers apply crop inputs such as fungicides
    more efficiently based on different AMB disease stages. Acknowledgement The authors
    would like to thank the Rural Development Administration (RDA), Korea, for supporting
    this research. We would also like to thank all the members of the Precision Agriculture
    Laboratory, University of Florida for their assistance in this study. References
    Apan et al., 2004 A. Apan, A. Held, S. Phinn, J. Markley Detecting sugarcane ‘orange
    rust’disease using EO-1 Hyperion hyperspectral imagery Int. J. Remote Sens., 25
    (2) (2004), pp. 489-498 View in ScopusGoogle Scholar Bajcsy and Groves, 2004 P.
    Bajcsy, P. Groves Methodology for hyperspectral band selection Photogramm. Eng.
    Rem. Sens., 70 (7) (2004), pp. 793-802 View in ScopusGoogle Scholar Chang et al.,
    1999 C.-I. Chang, Q. Du, T.-L. Sun, M.L. Althouse A joint band prioritization
    and band-decorrelation approach to band selection for hyperspectral image classification
    IEEE Trans. Geosci. Rem. Sens., 37 (6) (1999), pp. 2631-2641 View in ScopusGoogle
    Scholar Del Fiore et al., 2010 A. Del Fiore, M. Reverberi, A. Ricelli, F. Pinzari,
    S. Serranti, A. Fabbri, G. Bonifazi, C. Fanelli Early detection of toxigenic fungi
    on maize by hyperspectral imaging analysis Int. J. Food Microbiol., 144 (1) (2010),
    pp. 64-71 View PDFView articleView in ScopusGoogle Scholar Du and Yang, 2008 Q.
    Du, H. Yang Similarity-based unsupervised band selection for hyperspectral image
    analysis IEEE Geosci. Rem. Sens. Lett., 5 (4) (2008), pp. 564-568 View in ScopusGoogle
    Scholar FAO, 2013 FAO Global fruit production in 2013, by variety (in million
    metric tons) Retrieved from <http://www.statista.com/statistics/264001/worldwide-production-of-fruit-by-variety/>
    (2013) Google Scholar Harada et al., 1974 Harada, Y., Sawamura, K., Konno, K.,
    1974. Diplocarpon mali sp. nov., the perfect state of apple blotch fungus Marssonina
    coronaria. Ann. Phytopathol. Soc. Jpn. Google Scholar Jacquemoud and Ustin, 2001
    Jacquemoud, S., Ustin, S.L., 200). Leaf optical properties: a state of the art.
    Paper presented at the 8th International Symposium of Physical Measurements &
    Signatures in Remote Sensing. Google Scholar Jones et al., 2010 C. Jones, J. Jones,
    W. Lee Diagnosis of bacterial spot of tomato using spectral signatures Comput.
    Electron. Agric., 74 (2) (2010), pp. 329-335 View PDFView articleView in ScopusGoogle
    Scholar Keshava, 2004 N. Keshava Distance metrics and band selection in hyperspectral
    processing with applications to material identification and spectral libraries
    IEEE Trans. Geosci. Rem. Sens., 42 (7) (2004), pp. 1552-1565 View in ScopusGoogle
    Scholar Lee et al., 2012 C.-H. Lee, S.-Y. Lee, H.-Y. Jung, J.-H. Kim The application
    of optical coherence tomography in the diagnosis of Marssonina blotch in apple
    leaves J. Opt. Soc. Kor., 16 (2) (2012), pp. 133-140 View in ScopusGoogle Scholar
    Lee et al., 2011 D.-H. Lee, C.-G. Back, N.K.K. Win, K.-H. Choi, K.-M. Kim, I.-K.
    Kang, C. Choi, T.M. Yoon, J.Y. Uhm, H.-Y. Jung Biological characterization of
    Marssonina coronaria associated with apple blotch disease Mycobiology, 39 (3)
    (2011), pp. 200-205 View in ScopusGoogle Scholar Lee and Shin, 2000 H.-T. Lee,
    H.-D. Shin Taxonomic studies on the genus Marssonina in Korea Mycobiology, 28
    (1) (2000), pp. 39-46 CrossRefGoogle Scholar Lee et al., 2010 W. Lee, V. Alchanatis,
    C. Yang, M. Hirafuji, D. Moshou, C. Li Sensing technologies for precision specialty
    crop production Comput. Electron. Agric., 74 (1) (2010), pp. 18-19 Google Scholar
    Qin et al., 2009 J. Qin, T.F. Burks, M.A. Ritenour, W.G. Bonn Detection of citrus
    canker using hyperspectral reflectance imaging with spectral information divergence
    J. Food Eng., 93 (2) (2009), pp. 183-191 View PDFView articleView in ScopusGoogle
    Scholar Rumpf et al., 2010 T. Rumpf, A.-K. Mahlein, U. Steiner, E.-C. Oerke, H.-W.
    Dehne, L. Plümer Early detection and classification of plant diseases with support
    vector machines based on hyperspectral reflectance Comput. Electron. Agric., 74
    (1) (2010), pp. 91-99 View PDFView articleView in ScopusGoogle Scholar Sankaran
    et al., 2011 S. Sankaran, A. Mishra, J.M. Maja, R. Ehsani Visible-near infrared
    spectroscopy for detection of Huanglongbing in citrus orchards Comput. Electron.
    Agric., 77 (2) (2011), pp. 127-134 View PDFView articleView in ScopusGoogle Scholar
    Shafri and Hamdan, 2009 H.Z. Shafri, N. Hamdan Hyperspectral imagery for mapping
    disease infection in oil palm plantationusing vegetation indices and red edge
    techniques Am. J. Appl. Sci., 6 (6) (2009), p. 1031 View in ScopusGoogle Scholar
    Shen and Bassett III, 2002 Shen, S.S., Bassett III, E.M., 2002. Information-theory-based
    band selection and utility evaluation for reflective spectral systems. Paper presented
    at the AeroSense 2002. Google Scholar Tamietti and Matta, 2003 G. Tamietti, A.
    Matta First report of leaf blotch caused by Marssonina coronaria on apple in Italy
    Plant Dis., 87 (8) (2003) 1005-1005 Google Scholar Tanaka et al., 2000 Tanaka,
    S., Kamegawa, N., Ito, S., Kameya Iwaki, M., 2000. Detection of thiophanate-methyl-resistant
    strains in Diplocarpon mali, causal fungus of apple [Malus pumila] blotch. J.
    General Plant Pathol. (Japan). Google Scholar Wu et al., 2008 D. Wu, L. Feng,
    C. Zhang, Y. He Early detection of Botrytis cinerea on eggplant leaves based on
    visible and near-infrared spectroscopy Trans. ASABE, 51 (3) (2008), pp. 1133-1139
    View in ScopusGoogle Scholar Xu et al., 2007 H. Xu, Y. Ying, X. Fu, S. Zhu Near-infrared
    spectroscopy in detecting leaf miner damage on tomato leaf Biosys. Eng., 96 (4)
    (2007), pp. 447-454 View PDFView articleView in ScopusGoogle Scholar Yuan et al.,
    2014 L. Yuan, Y. Huang, R.W. Loraamm, C. Nie, J. Wang, J. Zhang Spectral analysis
    of winter wheat leaves for detection and differentiation of diseases and insects
    Field Crops Res., 156 (2014), pp. 199-207 View PDFView articleView in ScopusGoogle
    Scholar Cited by (36) Selecting hyperspectral bands and extracting features with
    a custom shallow convolutional neural network to classify citrus peel defects
    2023, Smart Agricultural Technology Show abstract Fingerprint Spectral Signatures
    Revealing the Spatiotemporal Dynamics of Bipolaris Spot Blotch Progression for
    Presymptomatic Diagnosis 2023, Engineering Show abstract Study on qualitative
    impact damage of yellow peaches using the combined hyperspectral and physicochemical
    indicators method 2022, Journal of Molecular Structure Citation Excerpt : The
    above studies used geometric parameters to evaluate the damage degree, but the
    damage degree is not always positively correlated with geometric parameters, especially
    for the fruits with insignificant external damage characteristics, while the damaged
    fruits can lead to the changes of their internal quality and their physicochemical
    indexes are also affected, so it is necessary to evaluate the damage degree of
    fruit from the perspective of changes in physicochemical indexes. In recent years,
    due to the advantages of hyperspectral technology over other inspection techniques,
    which are fast, non-destructive and accurate, the hyperspectral technique has
    been widely used in food inspection, for example, for quality monitoring of meat
    [6,7], fish [8,9], fruits [10,11], vegetables [12,13], cereals [14,15], and nuts
    [16,17]. In addition, hyperspectral technology is used to assess the mechanical
    damage of fruit; however, most of these studies distinguish whether the fruit
    is damaged, and some researchers use hyperspectral technology to discern the storage
    time of fruit after bruise. Show abstract Computer vision-based platform for apple
    leaves segmentation in field conditions to support digital phenotyping 2022, Computers
    and Electronics in Agriculture Citation Excerpt : One of the limitations of this
    work is that each leaf should be analyzed separately. In (Shadrin et al., 2020),
    the researchers pursued a similar to (Shuaibu et al., 2018) goal of selecting
    optimal wavebands in Near-Infrared and Mid-Infrared ranges to detect the signs
    of apple scab, moniliasis, and powdery mildew. They introduced a new discriminative
    coefficient that allows detection of a particular disease and distinguishes it
    from other diseases and healthy trees. Show abstract Detection of apple proliferation
    disease in Malus × domestica by near infrared reflectance analysis of leaves 2021,
    Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy Citation
    Excerpt : This coincides with the results of the “General-APP infected” model
    in which no spectral differences between asymptomatic and symptomatic leaves from
    APP infected trees were found (see Section 3.2). Spectroscopic techniques combined
    with multivariate data analysis are becoming increasingly important in phytopathology
    [65,67,68]. However, applications for phytoplasmoses are still rare and exist
    mainly for grapevine [69,70]. Show abstract Machine learning techniques for analysis
    of hyperspectral images to determine quality of food products: A review 2021,
    Current Research in Food Science Show abstract View all citing articles on Scopus
    View Abstract © 2017 Elsevier B.V. All rights reserved. Recommended articles Hyperspectral
    measurements of yellow rust and fusarium head blight in cereal crops: Part 2:
    On-line field measurement Biosystems Engineering, Volume 167, 2018, pp. 144-158
    Rebecca L. Whetton, …, Abdul M. Mouazen View PDF Early detection of fungal infection
    of stored apple fruit with optical sensors – Comparison of biospeckle, hyperspectral
    imaging and chlorophyll fluorescence Food Control, Volume 85, 2018, pp. 327-338
    P.M. Pieczywek, …, A. Kurenda View PDF Remote hyperspectral imaging of grapevine
    leafroll-associated virus 3 in cabernet sauvignon vineyards Computers and Electronics
    in Agriculture, Volume 130, 2016, pp. 109-117 Sarah L. MacDonald, …, Monica L.
    Cooper View PDF Show 3 more articles Article Metrics Citations Citation Indexes:
    35 Captures Readers: 61 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply.'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Unsupervised hyperspectral band selection for apple Marssonina blotch detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11119-023-10014-y
  analysis: '>'
  authors:
  - Ruben Chin
  - Çaǧatay Çatal
  - Ayalew Kassahun
  citation_count: 11
  full_citation: '>'
  full_text: ">\nREVIEW\nAccepted: 19 March 2023 / Published online: 28 March 2023\n\
    © The Author(s) 2023, corrected publication 2023\n \n Cagatay Catal\nccatal@qu.edu.qa\n\
    1 \nInformation Technology Group, Wageningen University and Research, Wageningen,\
    \ The \nNetherlands\n2 \nDepartment of Computer Science and Engineering, Qatar\
    \ University, Doha, Qatar\nPlant disease detection using drones in precision agriculture\n\
    Ruben Chin1 · Cagatay Catal2\n · Ayalew Kassahun1\nPrecision Agriculture (2023)\
    \ 24:1663–1682\nhttps://doi.org/10.1007/s11119-023-10014-y\nAbstract\nPlant diseases\
    \ affect the quality and quantity of agricultural products and have an impact\
    \ \non food safety. These effects result in a loss of income in the production\
    \ sectors which are \nparticularly critical for developing countries. Visual inspection\
    \ by subject matter experts \nis time-consuming, expensive and not scalable for\
    \ large farms. As such, the automation of \nplant disease detection is a feasible\
    \ solution to prevent losses in yield. Nowadays, one of \nthe most popular approaches\
    \ for this automation is to use drones. Though there are several \narticles published\
    \ on the use of drones for plant disease detection, a systematic overview \nof\
    \ these studies is lacking. To address this problem, a systematic literature review\
    \ (SLR) \non the use of drones for plant disease detection was undertaken and\
    \ 38 primary studies \nwere selected to answer research questions related to disease\
    \ types, drone categories, \nstakeholders, machine learning tasks, data, techniques\
    \ to support decision-making, ag-\nricultural product types and challenges. It\
    \ was shown that the most common disease is \nblight; fungus is the most important\
    \ pathogen and grape and watermelon are the most \nstudied crops. The most used\
    \ drone type is the quadcopter and the most applied machine \nlearning task is\
    \ classification. Color-infrared (CIR) images are the most preferred data \nused\
    \ and field images are the main focus. The machine learning algorithm applied\
    \ most \nis convolutional neural network (CNN). In addition, the challenges to\
    \ pave the way for \nfurther research were provided.\nKeywords Drone · Plant disease\
    \ detection · Machine learning\n1 3\nPrecision Agriculture (2023) 24:1663–1682\n\
    Introduction\nGlobal agricultural food production has to increase by at least\
    \ 70% to meet the needs of \nthe increasing world population (Ahirwar et al.,\
    \ 2019). This is a challenging goal because \nthe agricultural sector depends\
    \ largely on conditions that are not fully controlled such as \nthe weather, soil\
    \ condition and the quality and quantity of irrigation water. Therefore, it is\
    \ \ncrucial to adopt precision technologies such as drones to make optimum use\
    \ of resources \nand improve agricultural productivity.\nDrones have been used\
    \ for diverse application purposes in precision agriculture and new \nways of\
    \ using them are being explored. Many drone applications have been developed for\
    \ \ndifferent purposes such as pest detection, crop yield prediction, crop spraying,\
    \ yield estima-\ntion, water stress detection, land mapping, identifying nutrient\
    \ deficiency in plants, weed \ndetection, livestock control, protection of agricultural\
    \ products and soil analysis (Celen et \nal., 2020).\nPlant disease detection\
    \ is one of the application areas of drones and has been investigated \nextensively\
    \ (Veroustraete, 2015). One of the benefits of using drones is the early detection\
    \ of \ndiseases and the prevention of the spread of infection to mitigate crop\
    \ loss (Kitpo and Inoue \n2018). Previous review studies on the application of\
    \ drones in precision agriculture are \nlimited in scope. Mogili & Deepak (2018)\
    \ reviewed the literature on the types of drones and \ntheir use for spraying\
    \ pesticides. Likewise, Devi & Priya (2021) focused on the recognition \nof plant\
    \ diseases using images captured by drones. Decision-support systems using drones\
    \ \ncan lead to better decisions, increase production, improve the quality of\
    \ products and save \nlabor (Sinha, 2020).\nDrones are used for many different\
    \ crop types and diseases. Some diseases show visible \nsymptoms, while other\
    \ diseases can only be detected by measuring temperature. Although \ndrones have\
    \ been proven to be promising tools for disease detection, a detailed systematic\
    \ \noverview of the state-of-the-art on the adoption of drones for disease detection\
    \ is lack-\ning. Recently, a large number of studies have been published on the\
    \ application of drones. \nFor example, pesticide spraying and crop monitoring\
    \ studies have been reviewed recently \n(Hafeez et al., 2022), however, the review\
    \ was not conducted systematically. Similarly, a \ntraditional review study has\
    \ been published on the use of drone technology for sustainable \nweed management\
    \ focused only on weed management (Esposito et al., 2021). Some recent \nstudies\
    \ explain the design of drones for precision agriculture (de Oca and Flores, 2021;\
    \ \nHajare et al., 2021).\nThe objective of this study was to present a systematic\
    \ review of the literature on disease \ndetection using drones. The systematic\
    \ literature review (SLR) presented in this paper is \ndifferent from traditional\
    \ reviews and aims to identify all relevant scientific papers related \nto the\
    \ main theme of this study.\nBackground and related work\nDisease detection\n\
    Traditional farming related to disease detection relied on naked-eye observation,\
    \ which is \ntime-consuming and expensive and requires a lot of expertise (Sandhu\
    \ & Kaur, 2019). Cur-\n1 3\n1664\nPrecision Agriculture (2023) 24:1663–1682\n\
    rently, there are many methods to detect diseases in agricultural crops. Two main\
    \ categories \ncan be distinguished: direct and indirect detection methods. Direct\
    \ detection methods consist \nof polymerase chain reaction, fluorescence in-situ\
    \ hybridization, enzyme-linked immuno-\nsorbent assay, immunofluorescence and\
    \ flow cytometry. Indirect detection methods consist \nof thermography, fluorescence\
    \ imaging, hyperspectral techniques and gas chromatography. \nThe focus of this\
    \ study lies within the indirect detection methods, especially thermography \n\
    and hyperspectral techniques that are supported by drones. Thermography is based\
    \ on dif-\nferences in the surface temperature of plant leaves and canopies. Hyperspectral\
    \ imaging can \nmeasure the changes in reflectance resulting from the biophysical\
    \ and biochemical charac-\nteristics changes upon infection. Indirect methods\
    \ can be used to identify biotic, abiotic and \npathogenic diseases (Fang & Ramasamy,\
    \ 2015).\nRelated work\nA lot of research studies on disease detection using drones\
    \ have been undertaken. All of \nthese studies have had different focii, and different\
    \ diseases and detection methods have \nbeen investigated. In Table 1, an overview\
    \ of the current literature reviews and surveys on \ndisease detection by drones\
    \ is presented.\nMethodology\nTo perform this study, an SLR protocol was followed.\
    \ The SLR covers the last decade of \npapers on this research topic. The SLR answers\
    \ several research questions that were defined \nin this study. The selected primary\
    \ studies are used to extract data and which was analysed \nto respond to the\
    \ research questions.\nAs there is no overview of the published papers on disease\
    \ detection using drones, the \ngoal of this research was to present an overview\
    \ of the state-of-the-art. Wright et al. (2007) \ndesigned a guideline to conduct\
    \ an SLR, which was followed in this study.\nResearch questions\nThe motivations\
    \ for the SLR were to identify the.\n1. diseases detected by drones,\n2. drone\
    \ type used in disease detection,\n3. actors/stakeholders involved in disease\
    \ detection by drones,\n4. executed tasks in disease detection,\n5. main parameters\
    \ for stakeholders to work with,\n6. techniques used to support decision-making\
    \ in disease detection by drones,\n7. product types that drones are used for,\
    \ and.\n8. problems of using drones in disease detection.\nFollowing these motivations\
    \ the following research questions were formulated:\n1. What kind of diseases\
    \ are detected by using a drone?\n1 3\n1665\nPrecision Agriculture (2023) 24:1663–1682\n\
    ID\nReference \nand year\nPublication type \n& venue\nTitle\n1\nPanday et \nal.\
    \ (2020)\nJournal / Drones\nA Review on Drone-Based \nData Solutions for Cereal\
    \ \nCrops\n2\nMessina \n& Modica \n(2020)\nJournal / Remote \nSensing\nApplications\
    \ of UAV Ther-\nmal Imagery in Precision \nAgriculture: State of the Art \nand\
    \ Future Research Outlook\n3\nTsouros et \nal. (2019)\nJournal / \nInformation\n\
    A Review on UAV-based \nApplications for Precision \nAgriculture\n4\nGarcía-\n\
    Berná et al. \n(2020)\nJournal / Applied \nSciences\nSystematic Mapping Study\
    \ \non Remote Sensing in \nAgriculture\n5\nDaponte et \nal. (2019)\nConference\
    \ / \nIOP Conference \nSeries: Earth and \nEnvironmental \nScience\nA Review on\
    \ the Use \nof Drones for Precision \nAgriculture\n6\nMogili & \nDeepak \n(2018)\n\
    Conference / \nInternational \nConference on \nRobotics and \nSmart Manu-\nfacturing\
    \ \n(RoSMa2018)\nReview on Application of \nDrone Systems in Precision \nAgriculture\n\
    7\nKim et al. \n(2019)\nJournal / IEEE \nAccess\nUnmanned Aerial Vehicles \nin\
    \ Agriculture: A Review \nof Perspective of Platform, \nControl, and Applications\n\
    8\nBoursianis \net al. (2022)\nJournal / Internet \nof Things\nInternet of Things\
    \ (IoT) \nand Agricultural Unmanned \nAerial Vehicles (UAVs) in \nsmart farming:\
    \ A compre-\nhensive review\n9\nHassler & \nBaysal-\nGurel \n(2019)\nJournal /\
    \ \nAgronomy\nUnmanned Aircraft System \n(UAS) Technology and Ap-\nplications\
    \ in Agriculture\n10\nAbdullahi \net al. (2015)\nConference / \nInternational\
    \ \nConference on \nWireless and Sat-\nellite Systems\nTechnology Impact on \n\
    Agricultural Productivity: A \nReview of Precision Agricul-\nture using Unmanned\
    \ Aerial \nVehicles\n11\nZhang & \nKovacs \n(2012)\nJournal / Preci-\nsion Agriculture\n\
    The Application of Small \nUnmanned Aerial Systems \nfor Precision Agriculture:\
    \ A \nReview\n12\nKhanal et \nal. (2017)\nJournal / \nComputers and \nElectronics\
    \ in \nAgriculture\nAn Overview of Current and \nPotential Applications of \n\
    Thermal Remote Sensing in \nPrecision Agriculture\nTable 1 Related review works\
    \ on \nthe use of drones for precision \nagriculture\n \n1 3\n1666\nPrecision\
    \ Agriculture (2023) 24:1663–1682\n2. What is the drone type used in disease detection?\n\
    3. What are the actors/stakeholders involved in disease detection by drones?\n\
    4. Which tasks are executed to support decision-making in disease detection by\
    \ drones?\n5. Which data are generated by drones to support disease detection?\n\
    6. Which techniques are used to support decision-making in disease detection by\
    \ drones?\n7. For which agriculture product types are drones used for disease\
    \ detection?\n8. What are the challenges to the application of drones in disease\
    \ detection?\nSearch string\nTo determine the search string, a number of test\
    \ searches were done on Scopus and ACM \nwith the following terms:\n(drone OR\
    \ uav) AND disease AND detection.\n(drone OR uav) AND “disease detection”.\n(drone\
    \ OR uav) AND (“bacteria*” OR “fun*” OR “vir*” OR blight OR wilt OR rot) \nAND\
    \ detection.\nBased on the insights obtained, the following search string was\
    \ used to carry out the SLR:\n(Drone OR UAV) AND Disease AND Detection.\nThe databases\
    \ selected were ScienceDirect, IEEE, ACM Digital Library, Springer, Wiley \nand\
    \ Scopus.\nSelection criteria\nThe first step of selection was filtering by reading\
    \ the title, abstract and introduction in order \nto determine if the paper is\
    \ relevant. The second step was to exclude papers using the exclu-\nsion criteria\
    \ given in Table 2.\nQuality Assessment\nAll of the selected papers were scored\
    \ based on eight quality assessment questions given \nin Table 3 (Kitchenham et\
    \ al., 2009). Papers were assigned either 1 (good quality), 0.5 (fair \nquality),\
    \ or 0 (bad quality) scores. Papers with a total score lower than four were excluded\
    \ \nfrom the research.\nTable 4 presents the distribution of papers based on databases\
    \ where they were found \nat different selection stages. After the initial search,\
    \ 1852 papers were retrieved, of which \n58 remained after applying the selection\
    \ criteria. After quality assessment, 38 papers were \nNumber\nDescription\nEC\
    \ 1\nArticle published before 2010.\nEC 2\nArticle not published in English or\
    \ Dutch.\nEC 3\nFull text is not available.\nEC 4\nThe paper is not about the\
    \ use of drones \nin precision agriculture.\nEC 5\nAn experimental result is not\
    \ available.\nEC 6\nIt is a review article.\nEC 7\nIt is not a peer-reviewed journal\
    \ article.\nTable 2 Exclusion criteria\n \n1 3\n1667\nPrecision Agriculture (2023)\
    \ 24:1663–1682\nselected as primary studies. The 38 papers were carefully read\
    \ in full and the required data \nfor answering the research questions were extracted.\n\
    Data collection\nThe data extraction form is presented in Table 5. All the collected\
    \ articles are listed in \nTable 6.\nResults\nRQ 1: What kind of diseases are\
    \ detected by using a drone?\nThere was a wide variety of diseases that have been\
    \ studied (Fig. 1). Blight accounted for \n8 out of 44 identified diseases. The\
    \ number of counts of wilt is six. All of the other diseases \nare more or less\
    \ evenly distributed.\nThe identified diseases were classified into major categories;\
    \ the categorization scheme \nthat was selected is related to the disease-causing\
    \ pathogens, which are: fungus, bacteria, \nvirus, nematode and abiotic (Abdulkhadir\
    \ & Alghuthaymi, 2016). A major finding is that, \naccording to Fig. 2, fungus\
    \ alone accounted for 64% of the diseases investigted. Bacteria \nwas second place\
    \ with a percentage of 26%. The remainder of the disease-causing patho-\nSource\n\
    After search \nstring\nAfter selection \ncriteria\nAfter \nquality \nassessment\n\
    ScienceDirect\n27\n9\n8\nIEEE Xplore\n34\n1\n1\nACM Digital Library\n1\n0\n0\n\
    Springer\n1566\n7\n5\nWiley\n21\n1\n1\nScopus\n203\n40\n23\nTotal\n1852\n58\n\
    38\nTable 4 The process of selecting \nprimary studies\n \nNumber\nQuestion\n\
    Q1\nAre the aims of the study clearly stated?\nQ2\nAre the scope and context of\
    \ the study clearly \ndefined?\nQ3\nIs the proposed solution clearly explained\
    \ and \nvalidated by an empirical study?\nQ4\nAre the variables used in the study\
    \ likely to be \nvalid and reliable?\nQ5\nIs the research process documented adequately?\n\
    Q6\nAre all the study questions answered?\nQ7\nAre negative findings presented?\n\
    Q8\nAre the main findings stated clearly in terms of \ncreditability, validity\
    \ and reliability?\nTable 3 Quality assessment \nquestions\n \n1 3\n1668\nPrecision\
    \ Agriculture (2023) 24:1663–1682\ngens accounted for the rest of the diseases.\
    \ This means that viruses, nematodes and abiotics \nwere only responsible for\
    \ 10% of diseases.\nAccording to Fig. 3, there were a wide variety of plants for\
    \ which drones were used to \ndetect disease causing pathogens. The two crops\
    \ that stand out in this figure are grape and \nwatermelon. They both have a large\
    \ representation of fungus. Citrus seems to be vulnerable \nonly to bacterial\
    \ diseases and wheat to fungus diseases. Tomato has three disease-causing \npathogens,\
    \ namely fungus, bacteria and viruses.\nRQ 2: What is the drone type used in disease\
    \ detection?\nFive different drone types were distinguished. The fixed-wing has\
    \ a very different design in \ncomparison with the other drones; it has two wings\
    \ for an aerodynamic shape that makes it \nlook like an airplane. A single-rotor\
    \ helicopter has one big rotor on top of it and one small \nrotor at the end of\
    \ the tail, it looks like a helicopter. The quadcopter is a design that has four\
    \ \nrotors; two of them rotate in the clockwise direction and the other ones in\
    \ the counter-clock-\nwise direction. The hexacopter has six rotors and the octocopter\
    \ has eight rotors (Mogili & \nDeepak, 2018). Due to the fact that the single-rotor\
    \ helicopter is not mentioned in anyone \nof the extracted papers, this drone\
    \ type is left out of the categorization. The drone type most \nused, according\
    \ to Fig. 4, is the quadcopter, namely 14 times. The hexacopter is used in 9 of\
    \ \nthe identified papers. The remainder of the papers used the fixed-wing and\
    \ octocopter. This \nmeans that the quadcopter is the dominant drone type used\
    \ in disease detection.\nNumber\nElement Extracted\nContent\n1.\nTitle\n2.\nAuthor(s)\n\
    3.\nYear\n4.\nJournal\n5.\nSource\n6.\nDisease (RQ 1)\n7.\nDrone type (RQ 2)\n\
    Fixed wing, quadcopter, hexa-\ncopter or octocopter.\n8.\nActors/stakehold-\n\
    ers (RQ 3)\nFarmer, research community, \nconsumer, the environment and \ntourism\
    \ sector.\n9.\nTask (RQ 4)\nDetection, categorization, clas-\nsification, monitoring,\
    \ mapping, \ndiscrimination, quantification, \nidentification and predicting.\n\
    10.\nData (RQ 5)\nRGB image, CIR image, V-NIR \nimage, thermal image and MS \n\
    image.\n11.\nThe technique \n(RQ 6)\n12.\nProduct type \n(RQ 7)\n13.\nChallenges\
    \ (RQ 8)\nTable 5 Data extraction form\n \n1 3\n1669\nPrecision Agriculture (2023)\
    \ 24:1663–1682\nReference\nTitle\nSource\nDang et al., 2020a\nUAV based wilt detection\
    \ system via convolutional neural networks\nScience-\nDirect\nAbdulridha et al.,\
    \ \n2020a\nDetecting powdery mildew disease in squash at different stages using\
    \ \nUAV-based hyperspectral imaging and artificial intelligence\nScience-\nDirect\n\
    Kerkech et al., \n2020a\nVine disease detection in UAV multispectral images using\
    \ optimized \nimage registration and deep learning segmentation approach\nScience-\n\
    Direct\nKerkech et al., 2018\nDeep learning approach with colorimetric spaces\
    \ and vegetation indices \nfor vine diseases detection in UAV images\nScience-\n\
    Direct\nChen et al., 2020a\nEarly detection of bacterial wilt in peanut plants\
    \ through leaf-level hyper-\nspectral and unmanned aerial vehicle data\nScience-\n\
    Direct\nCastrignano et al., \n2021\n A geostatistical fusion approach using UAV\
    \ data for probabilistic estima-\ntion of Xylella fastidiosa subsp. pauca infection\
    \ in olive trees\nScience-\nDirect\nBagheri, 2020\nApplication of aerial remote\
    \ sensing technology for detection of fire \nblight infected pear trees\nScience-\n\
    Direct\nSelvaraj et al., 2020\nDetection of banana plants and their major diseases\
    \ through aerial images \nand machine learning methods: A case study in DR Congo\
    \ and Republic \nof Benin\nScience-\nDirect\nChen et al., 2020b\nAn AIoT based\
    \ smart agricultural system for pests detection\nIEEE\nJavan et al., 2019\nUAV-based\
    \ multispectral imagery for fast citrus greening detection\nSpringer\nAbdulridha\
    \ et al., \n2020b\nDetection of target spot and bacterial spot diseases in tomato\
    \ using UAV-\nbased and benchtop-based hyperspectral imaging techniques\nSpringer\n\
    Alberto et al., 2020\nExtraction of onion fields infected by anthracnose-twister\
    \ disease in \nselected municipalities of Nueva Ecija using UAV imageries\nSpringer\n\
    Calderon et al., \n2014\nDetection of downy mildew of opium poppy using high-resolution\
    \ multi-\nspectral and thermal imagery acquired with an unmanned aerial vehicle\n\
    Springer\nWiesner-Hanks et \nal., 2018\nImage set for deep learning: field images\
    \ of maize annotated with disease \nsymptoms\nSpringer\nWu et al., 2019\nAutonomous\
    \ detection of plant disease symptoms directly from aerial \nimagery\nWiley\n\
    Kerkech et al., \n2020b\nVddNet: Vine disease detection network based on multispectral\
    \ images \nand depth map\nScopus\nViera-Torres et al., \n2020\nGenerating the\
    \ baseline in the early detection of bud rot and red ring \ndisease in oil palms\
    \ by geospatial technologies\nScopus\nDang et al., 2020b\nFusarium wilt of radish\
    \ detection using rgb and near infrared images \nfrom unmanned aerial vehicles\n\
    Scopus\nDeng et al., 2020\nDetection of citrus huanglongbing based on multi-input\
    \ neural network \nmodel of UAV hyperspectral remote sensing\nScopus\nAbdulridha\
    \ et al., \n2020c\nLaboratory and UAV-based identification and classification\
    \ of tomato \nyellow leaf curl, bacterial spot, and target spot diseases in tomato\
    \ utiliz-\ning hyperspectral imaging and machine learning\nScopus\nDi Nisio et\
    \ al., 2020\nFast detection of olive trees affected by xylella fastidiosa from\
    \ UAVs \nusing multispectral imaging\nScopus\nWang et al., 2020a\nA plant-by-plant\
    \ method to identify and treat cotton root rot based on \nUAV remote sensing\n\
    Scopus\nSavian et al., 2020\nPrediction of the kiwifruit decline syndrome in diseased\
    \ orchards by \nremote sensing\nScopus\nWang et al., 2020b\nAutomatic classification\
    \ of cotton root rot disease based on UAV remote \nsensing\nScopus\nSiebring et\
    \ al., 2019\nObject-based image analysis applied to low altitude aerial imagery\
    \ for \npotato plant trait retrieval and pathogen detection\nScopus\nBohnenkamp\
    \ et al., \n2019\nIn-field detection of yellow rust in wheat on the ground canopy\
    \ and UAV \nscale\nScopus\nTable 6 Selected primary studies\n1 3\n1670\nPrecision\
    \ Agriculture (2023) 24:1663–1682\nRQ 3: What are the actors/stakeholders involved\
    \ in disease detection by drones?\nAccording to Table 7, several stakeholders\
    \ were involved in disease detection by drones. \nAt first, the farmer was always\
    \ involved. He or she needed to operate the drone, interpret \nthe data or take\
    \ action after the data is analyzed. The research community was identified as\
    \ \na stakeholder in the 29% of the papers because the studies were used for future\
    \ work and \nFig. 1 The number of articles \nper disease in the 38 studies that\
    \ \nused drones for disease detection\n \nReference\nTitle\nSource\nKalischuk\
    \ et al., \n2019\nAn improved crop scouting technique incorporating unmanned aerial\
    \ \nvehicle-assisted multispectral crop imaging into conventional scouting \n\
    practice for gummy stem blight in watermelon\nScopus\nZhang et al., 2019\n A deep\
    \ learning-based approach for automated yellow rust disease detec-\ntion from\
    \ high-resolution hyperspectral UAV images\nScopus\nAbdulridha et al., \n2019\n\
    UAV-based remote sensing technique to detect citrus canker disease \nutilizing\
    \ hyperspectral imaging and machine learning\nScopus\nXavier et al., 2019\nIdentification\
    \ of ramularia leaf blight cotton disease infection levels by \nmultispectral,\
    \ multiscale uav imagery\nScopus\nHeim et al., 2019\nMultispectral, aerial disease\
    \ detection for myrtle rust (Austropuccinia \npsidii) on a lemon myrtle plantation\n\
    Scopus\nHuang et al., 2019\nDetection of helminthosporium leaf blotch disease\
    \ based on UAV \nimagery\nScopus\nFranceschini et al., \n2019\nFeasibility of\
    \ unmanned aerial vehicle optical imagery for early detection \nand severity assessment\
    \ of late blight in potato\nScopus\nAlbetis et al., 2019\nOn the potentiality\
    \ of UAV multispectral imagery to detect Flavescence \ndorée and Grapevine Trunk\
    \ diseases\nScopus\nZhang et al., 2018\nDetection of rice sheath blight using\
    \ an unmanned aerial system with \nhigh-resolution color and multispectral imaging\n\
    Scopus\nAlbetis et al., 2017\nDetection of Flavescence dorée grapevine disease\
    \ using Unmanned \nAerial Vehicle (UAV) multispectral imagery\nScopus\nDi Gennaro\
    \ et al., \n2016\nUnmanned Aerial Vehicle (UAV)-based remote sensing to monitor\
    \ grape-\nvine leaf stripe disease within a vineyard affected by esca complex\n\
    Scopus\nCalderon et al., \n2013\nHigh-resolution airborne hyperspectral and thermal\
    \ imagery for early \ndetection of Verticillium wilt of olive using fluorescence,\
    \ temperature \nand narrow-band spectral indices\nScopus\nTable 6 (continued)\n\
    \ \n1 3\n1671\nPrecision Agriculture (2023) 24:1663–1682\nespecially because the\
    \ data is donated to researchers for research. This indicates that disease \n\
    detection with drones is still an active area of research. From the consumer perspective,\
    \ \nearly disease detection leads to better quality of products and safer products;\
    \ 24% of the \npapers explicitly mention this issue. 8% of the papers state that\
    \ drone use has a positive \nimpact on the environment because it leads to less\
    \ usage of fertilizers. One paper was related \nto the tourism sector due to the\
    \ relationship between plants, landscape and tourism.\nActor/Stakeholder\nTotal\
    \ Counts\nAs % of Total\nFarmer\n38\n100%\nResearch Community\n11\n29%\nConsumer\n\
    9\n24%\nEnvironment\n3\n8%\nTourism Sector\n1\n3%\nTable 7 Actors/stakeholders\
    \ \ninvolved or influenced by disease \ndetection\n \nFig. 4 Drone type used for\
    \ \ndisease detection\n \nFig. 3 Disease-causing patho-\ngens related to the product\
    \ type\n \nFig. 2 Proportion of disease-\ncausing pathogens related to the \n\
    identified diseases\n \n1 3\n1672\nPrecision Agriculture (2023) 24:1663–1682\n\
    RQ 4: Which task is executed to support decision-making in disease detection by\
    \ \ndrones?\nFigure 5 shows that the main task of supporting decision-making in\
    \ disease detection is a \nclassification task, which was done in 27 selected\
    \ papers. There are 15 papers that applied \na detection task for data processing.\
    \ A total of 8 studies applied a mapping technique to \nanalyze data. Other tasks\
    \ executed were categorization, monitoring, discrimination, quanti-\nfication,\
    \ identification and prediction.\nRQ 5: Which data are generated by drones to\
    \ support disease detection?\nThe results show that mainly CIR images were generated\
    \ by drones to support decision-\nmaking, as shown in Fig. 6. CIR images were\
    \ the most frequently used with the occurrence \nof 20 out of the 38 papers. RGB\
    \ images were the second most used in disease detection \n(14 times). Other types\
    \ of images used for disease detection were visible and near-infrared \n(V-NIR)\
    \ image, thermal image and multispectral (MS) image and together account for 9\
    \ of \nthe 38 selected papers.\nThere are 3 different kinds of images distinguished\
    \ based on the subject of the image, \nnamely leaf, plant or field-based. Field-based\
    \ images are the dominant type as shown in \nFig. 6 Categorization of image \n\
    type used for disease detection\n \nFig. 5 The most performed \ntasks among the\
    \ 38 studies \nsummarized\n \n1 3\n1673\nPrecision Agriculture (2023) 24:1663–1682\n\
    Fig. 7. Most, 58%, of the papers made use of field-based images. The rest of the\
    \ papers \nmade use of plant and leaf-based images, which accounted for 28% and\
    \ 14%, respectively.\nRQ 6: Which techniques are used to support decision-making\
    \ in disease detection \nby drones?\nThe results show that a wide variety of techniques\
    \ were used to support decision-making \nin disease detection using drones. Largely\
    \ the CNN-based models were applied in disease \ndetection using drones (Fig.\
    \ 8). CNN-based models were applied in 10 of the 38 selected \npapers. The CNN-based\
    \ model category consists of the following models: GoogleNet, \nVGG16, RetinaNet,\
    \ YOLO and VGG-Net.\nSupport vector machines (SVM) is another algorithm that stood\
    \ out in Fig. 9 with a \ncount of 6. Both RBF and RF have 3 counts each. Other\
    \ algorithms identified were K-means \nclustering, AKAZE, Segnet, MLP, SDA, LSC,\
    \ QSVM, LDA, unsupervised clustering, \nKMSVM, KMSEG and KNN.\nRQ 7: For which\
    \ agriculture product types are drones used for disease detection?\nAs shown in\
    \ Fig. 9, a wide variety of agricultural crops were analyzed for disease detec-\n\
    tion using drones. Grape occurred 6 times in the 38 selected papers. Olive, citrus,\
    \ cotton \nand wheat were all counted 3 times. The rest of the product types were\
    \ more or less evenly \ndistributed.\nFig. 8 The techniques most used \nin the\
    \ 38 summarized studies\n \nFig. 7 Overview of image (Leaf, \nPlant, Field)\n\
    \ \n1 3\n1674\nPrecision Agriculture (2023) 24:1663–1682\nRQ 8: What are the challenges\
    \ of the application of drones in disease detection?\nThe results show that there\
    \ are several challenges encountered in the application of drones \nfor disease\
    \ detection (Table 8). The challenges can be categorized into two core categories,\
    \ \nnamely dataset and model building. Challenges related to the dataset were\
    \ deformations on \nthe image dataset, the limited number of expert-labeled data,\
    \ strong randomness in data and \nthe lack of classes in the dataset. Challenges\
    \ related to model building were the small size \nof the training sample, long\
    \ training time and long processing time. As seen in Table 8, only \n2 papers\
    \ proposed possible solutions for the encountered challenges.\nCategory\nChallenge\n\
    Possible \nSolutions\nReference\nDataset\nDeformations in the \nimage dataset\n\
    Kerkech et \nal. 2020a\nThe limited number \nof expert-labeled \ndata\nEnriching\
    \ the \nUAV multispec-\ntral images data-\nbase, with plots \nand new diseases\
    \ \nsamples\nKerkech et \nal. 2018\nStrong randomness \nin data\nDeng et al. \n\
    2020\nLack of classes in \nthe dataset\nIncluding extra \nparameters\nHeim et\
    \ al. \n2019\nModel \nBuilding\nThe small size of \nthe training sample\nKerkech\
    \ et \nal. 2020b\nLong training time\nSelvaraj et \nal. 2020\nLong processing\
    \ \ntime\nChen et al. \n2020a, Wang \net al. 2020a, \nand Wang et \nal. 2020b\n\
    Table 8 The encountered chal-\nlenges in the application of \ndrones in disease\
    \ detection\n \nFig. 9 The most investigated \ncrop types in the 38 studies that\
    \ \napply drones in disease detection\n \n1 3\n1675\nPrecision Agriculture (2023)\
    \ 24:1663–1682\nDiscussion\nGeneral discussion\nBlight and wilt were the two major\
    \ diseases studied using drone data because both of these \ntwo disease categories\
    \ exhibit very visible symptoms in the picture. In addition, the major \ndisease-causing\
    \ pathogen that was identified using drones was fungus. This is also in line \n\
    with the fact that fungus diseases show visible symptoms. It shows that drones\
    \ are mainly \nused for detecting diseases that show visible symptoms.\nThe dominant\
    \ drone type used was the quadcopter. According to the reviewed papers, \nthis\
    \ is mostly due to financial motives. When a large area must be covered, either\
    \ multiple \ndrones are flown at the same time as a swarm or a drone with a larger\
    \ range is used. There-\nfore, the relation between drone type and field size\
    \ must be carefully analyzed.\nAccording to the reviewed papers, drones have been\
    \ used more often to detect disease in \ngrapes and, to a lesser extent, in olive,\
    \ citrus, cotton and wheat production. Grapes, water-\nmelon and tomatoes were\
    \ mentioned often in relation to disease-causing pathogens. This \nindicates that\
    \ drones are probably used for multiple purposes. The diversity of techniques\
    \ \nidentified indicates that either different techniques are used for different\
    \ types of decisions \nor plants, or the researchers are still exploring diverse\
    \ techniques. The results clearly show \nthat classification is the dominant task\
    \ performed in disease detection by drones. This means \nthat a plant or part\
    \ of the field is assessed as healthy or not healthy in relation to the investi-\n\
    gated disease but not necessarily detecting the disease.\nFarmers seem to be involved\
    \ in all cases because they need to take action after the data \ngathered by the\
    \ drones is analyzed. A significant finding is that 29% of the identified papers\
    \ \nstated that their data is available for the research community for future\
    \ work.\nThe data gathered in disease detection is diverse. This could be because\
    \ of the fact that \nthis is an active area of research. Researchers are experimenting\
    \ with cameras that need \nto be mounted on drones flown at various heights and\
    \ conditions. That is probably an \nadditional reason why many different algorithms\
    \ are applied. The challenges encountered \nwhere papers did not come up with\
    \ possible recommendations indicate the need for further \ninvestigations.\nWhile\
    \ traditional machine learning algorithms such as SVM may provide satisfac-\n\
    tory results in many precision agriculture studies, researchers should also consider\
    \ using \nadvanced deep learning algorithms that can utilize the increasing amount\
    \ of data avail-\nable and have better performance (Oikonomidis et al., 2022a,\
    \ b; Kaya et al., 2019). For \ndisease detection using drones deep learning algorithms,\
    \ such as transformers, long short \nterm memory (LSTM) and autoencoders, can\
    \ be investigated for various problem scenarios. \nMore research can also be done\
    \ in detecting diseases that do not have visible symptoms.\nThis study has also\
    \ identified some challenges and potential solutions for the challenges. \nFor\
    \ example, the processing time and training time were mentioned in some studies\
    \ as \npotential challenges. Advanced data infrastructures and techniques such\
    \ as distributed \nmachine learning and hierarchical federated learning should\
    \ be considered in future studies.\n1 3\n1676\nPrecision Agriculture (2023) 24:1663–1682\n\
    Threats to validity\nThere are a number of threats to validity in relation to\
    \ the conducted SLR, which include \nconstruct, internal, external and conclusion\
    \ validity threats.\nConstruct validity refers to measuring to what degree the\
    \ test measures what it claims, \nor purports, to be measuring. In other words,\
    \ the SLR should be the right method for the \ngoals of the study. Databases are\
    \ a very effective source for literature searching, but they \nare highly susceptible\
    \ to query phrasing. Minimal differences in a search query can result \nin major\
    \ differences in the outcome of relevant literature. The databases also have different\
    \ \nformats, therefore, the search method is slightly different. Some papers might\
    \ have been \nmissed due to the search criteria used, but the 38 primary studies\
    \ helped to respond to all \nthe research questions. Six widely used electronic\
    \ databases have been covered, but there \nmight be some papers that are not indexed\
    \ by these databases. Many papers can be found \nusing Google Scholar, but many\
    \ of them are not peer-reviewed. As such, Google Scholar \nwas excluded as a literature\
    \ database in this research because the targets were only peer-\nreviewed and\
    \ high-quality studies.\nThe quality assessment could be vulnerable to subjective\
    \ decisions. This threat has been \nminimized by following the standards for this\
    \ procedure. The main objective of the qual-\nity assessment is to identify low-quality\
    \ papers instead of assigning a precise quality score \nper paper. As such, while\
    \ the assessment can be considered subjective, the overall strategy \nhas generally\
    \ been adopted in SLR studies. In addition to this, the extraction of data could\
    \ \nbe incomplete because of the fact that data is not available or missed in\
    \ the papers. This is \nreduced by formulating a clear, unambiguous data extraction\
    \ form. Since categories were \nwidely used, there might be some risk of incorrect\
    \ categorization. However, the impact \nof such misinterpretation should be minimal\
    \ because of the large number of items in each \ncategory.\nInternal validity\
    \ is the extent to which a study establishes a trustworthy cause-and-effect \n\
    relationship between a treatment and an outcome. In this SLR, all of the research\
    \ ques-\ntions are formulated to investigate the necessary elements for disease\
    \ detection by drones \nin precision agriculture. Because of the fact that all\
    \ of these elements are well-defined, the \nrelationship between the questions\
    \ and the research goal is satisfactory.\nExternal validity is the extent to which\
    \ the outcome of a study can be expected to apply \nto other settings. In other\
    \ words, this validity refers to how generalizable the findings are. \nBecause\
    \ of the fact that algorithms can be applied to other areas without major modifica-\n\
    tions, it will be possible to use these for other (new) disease detection methods\
    \ by drones. \nSince this research field is very active and many articles are\
    \ published, results might be \ndifferent in a new SLR study that includes recent\
    \ papers. During this research, the aim has \nbeen to cover all the papers published\
    \ so far. However, due to the formal review processes \nthat took substantial\
    \ time, new papers may have been published and high-quality primary \nstudies\
    \ might not have been included by the time the study is published.\nThe reproducibility\
    \ of the SLR is measured by reliability. The procedure of Wright et al. \n(2007)\
    \ was followed in this study. The processes of question design, search process,\
    \ screen-\ning criteria and quality evaluation all comply to the standards. The\
    \ results of the collected \ndata were analyzed with tables and graphs to formulate\
    \ objective conclusions.\nIn addition to these potential threats, emphasis should\
    \ be given to two particular exclu-\nsion criteria. One of them excludes review\
    \ articles. Review articles were excluded because, \n1 3\n1677\nPrecision Agriculture\
    \ (2023) 24:1663–1682\nin SLR studies, only primary studies are included. The\
    \ other excluded papers were those \nthat did not present experimental results.\
    \ That is because the answers to research questions \nrequire studies that have\
    \ experimental results. If there is valuable information in papers that \nare\
    \ either review papers or are not experimental studies, they might have been missed\
    \ in this \nresearch. However, the main objective was to respond to the research\
    \ questions defined in \nthis research instead of presenting all the information\
    \ discussed in the literature.\nConclusion\nResults show that blight and wilt\
    \ are the most widely studied disease types. More than 10 \ndisease types were\
    \ covered by a single study. To have a better understanding of the use \nof drones\
    \ for disease detection, the diseases were categorized into five categories and\
    \ the \nresults show that fungus accounts for 64% of the diseases for which drones\
    \ were used. \nVirus, nematode and abiotic were studied only in 10% of the studies.\
    \ This observations \nindicate that while researchers can perform new research\
    \ for the less studied disease cat-\negories, practitioners can apply drones to\
    \ detect fungus-related diseases because there is \nalready substantial scientific\
    \ evidence. Grape and watermelon have been widely studied in \ndifferent studies.\
    \ There are few studies on kiwi, squash, pear, lemon, onion and rice, which \n\
    shows the potential of the utility of drones but further and in-depth research\
    \ is needed. Most \nresearchers apply drones for classification tasks. Most of\
    \ the studies (58%) utilized field \nimages and very few papers used leaf images\
    \ (14%) or plant images (28%). Research on \nsmall-scale objects such as leaves\
    \ and plants requires higher-resolution visual inspections \nand this might not\
    \ be possible in some cases where the available equipment and sensors do \nnot\
    \ support very precise inspections. The most used algorithm is CNN probably because\
    \ \nthis algorithm has been the basis for complex deep learning-based models such\
    \ as VGG16, \nGoogleNet and VGG-Net and because many researchers represented the\
    \ underlying prob-\nlem as a classification task. However, if the problem is represented\
    \ in a different way rather \nthan a classification task, the corresponding appropriate\
    \ algorithm could be a different one.\nFunding Open Access funding provided by\
    \ the Qatar National Library.\nDeclarations\nConflicts of Interest The authors\
    \ have no relevant financial or non-financial interests to \ndisclose.The authors\
    \ have no conflicts of interest to declare that are relevant to the content \n\
    of this article.All authors certify that they have no affiliations with or involvement\
    \ in any \norganization or entity with any financial interest or non-financial\
    \ interest in the subject mat-\nter or materials discussed in this manuscript.The\
    \ authors have no financial or proprietary \ninterests in any material discussed\
    \ in this article.\nOpen Access This article is licensed under a Creative Commons\
    \ Attribution 4.0 International License, which \npermits use, sharing, adaptation,\
    \ distribution and reproduction in any medium or format, as long as you give \n\
    appropriate credit to the original author(s) and the source, provide a link to\
    \ the Creative Commons licence, \nand indicate if changes were made. The images\
    \ or other third party material in this article are included in the \narticle’s\
    \ Creative Commons licence, unless indicated otherwise in a credit line to the\
    \ material. If material is \nnot included in the article’s Creative Commons licence\
    \ and your intended use is not permitted by statutory \nregulation or exceeds\
    \ the permitted use, you will need to obtain permission directly from the copyright\
    \ holder. \nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\
    \ The Creative Commons \n1 3\n1678\nPrecision Agriculture (2023) 24:1663–1682\n\
    Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)\
    \ applies to the data \nmade available in this article, unless otherwise stated\
    \ in a credit line to the data.\nReferences\nAbdulkhadir, W. M., & Alghuthaymi,\
    \ M. A. (2016). Plant Pathogens. Plant Growth (pp. 49–59).\nAbdullahi, H. S.,\
    \ Mahieddine, F., & Sheriff, R. E. (2015, July). Technology impact on agricultural\
    \ productiv-\nity: A review of precision agriculture using unmanned aerial vehicles.\
    \ In International conference on \nwireless and satellite systems (pp. 388–400).\
    \ Cham, Switzerland: Springer.\nAbdulridha, J., Ampatzidis, Y., Roberts, P., &\
    \ Kakarla, S. C. (2020a). Detecting powdery mildew disease in \nsquash at different\
    \ stages using UAV-based hyperspectral imaging and artificial intelligence. Biosystems\
    \ \nEngineering, 197, 135–148.\nAbdulridha, J., Ampatzidis, Y., Kakarla, S. C.,\
    \ & Roberts, P. (2020b). Detection of target spot and bacterial \nspot diseases\
    \ in tomato using UAV-based and benchtop-based hyperspectral imaging techniques.\
    \ Preci-\nsion Agriculture, 21(5), 955–978.\nAbdulridha, J., Ampatzidis, Y., Qureshi,\
    \ J., & Roberts, P. (2020c). Laboratory and UAV-based identification \nand classification\
    \ of tomato yellow leaf curl, bacterial spot, and target spot diseases in tomato\
    \ utilizing \nhyperspectral imaging and machine learning. Remote Sensing, 12(17),\
    \ 2732.\nAbdulridha, J., Batuman, O., & Ampatzidis, Y. (2019). UAV-based remote\
    \ sensing technique to detect citrus \ncanker disease utilizing hyperspectral\
    \ imaging and machine learning. Remote Sensing, 11(11), 1373.\nAhirwar, S., Swarnkar,\
    \ R., Bhukya, S., & Namwade, G. (2019). Application of Drone in Agriculture. Interna-\n\
    tional Journal of Current Microbiology and Applied Sciences, 8(1), 2500–2505.\n\
    Alberto, R. T., Rivera, J. E., Biagtan, A. R., & Isip, M. F. (2020). Extraction\
    \ of onion fields infected by \nanthracnose-twister disease in selected municipalities\
    \ of Nueva Ecija using UAV imageries. Spatial \nInformation Research, 28(3), 383–389.\n\
    Albetis, J., Duthoit, S., Guttler, F., Jacquin, A., Goulard, M., Poilve, H., et\
    \ al. (2017). Detection of Flaves-\ncence dorée grapevine disease using unmanned\
    \ aerial vehicle (UAV) multispectral imagery. Remote \nSensing, 9(4), 308.\nAlbetis,\
    \ J., Jacquin, A., Goulard, M., Poilve, H., Rousseau, J., Clenet, H., et al. (2019).\
    \ On the potentiality of \nUAV multispectral imagery to detect Flavescence dorée\
    \ and grapevine trunk Diseases. Remote Sensing, \n11(1), 23.\nBagheri, N. (2020).\
    \ Application of aerial remote sensing technology for detection of fire blight\
    \ infected pear \ntrees. Computers and Electronics in Agriculture, 168, 105147.\n\
    Bohnenkamp, D., Behmann, J., & Mahlein, A. (2019). In-field detection of yellow\
    \ rust in wheat on the \nGround Canopy and UAV Scale. Remote Sensing, 11(21),\
    \ 2495.\nBoursianis, A. D., Papadopoulou, M. S., Diamantoulakis, P., Liopa-Tsakalidi,\
    \ A., Barouchas, P., Salahas, G., \net al. (2022). Internet of things (IoT) and\
    \ agricultural unmanned aerial vehicles (UAVs) in smart farm-\ning: A comprehensive\
    \ review. Internet of Things, 18, 100187.\nCalderon, R., Montes-Borrego, M., Landa,\
    \ B. B., Navas-Cortes, J. A., & Zarco-Tejada, P. J. (2014). Detection \nof downy\
    \ mildew of opium poppy using high-resolution multi-spectral and thermal imagery\
    \ acquired \nwith an unmanned aerial vehicle. Precision Agriculture, 15(6), 639–661.\n\
    Calderon, R., Navas-Cortes, J. A., Lucena, C., & Zarco-Tejada, P. J. (2013). High-resolution\
    \ airborne hyper-\nspectral and thermal imagery for early detection of Verticillium\
    \ wilt of olive using fluorescence, tem-\nperature and narrow-band spectral indices.\
    \ Remote Sensing of Environment, 139(December), 231–245.\nCastrignano, A., Belmonte,\
    \ A., Antelmi, I., Quarto, R., Quarto, F., Shaddad, S., et al. (2021). A geostatistical\
    \ \nfusion approach using UAV data for probabilistic estimation of Xylella fastidiosa\
    \ subsp. pauca infection \nin olive trees. Science of The Total Environment, 752,\
    \ 141814.\nCelen, I. H., Önler, E., & de Lyon, L. B. (2020). Drone Technology\
    \ in Precision Agriculture. Chapter in Aca-\ndemic Studies in Engineering Sciences,\
    \ Kurt, H. I., ed., Livre de Lyon, 2020, 121–140.\nChen, C., Huang, Y., Li, Y.,\
    \ Chen, Y., Chang, C., & Huang, Y. (2020a). An AIoT based Smart Agricultural \n\
    System for Pests Detection. Ieee Access: Practical Innovations, Open Solutions,\
    \ 8, 180750–180761.\nChen, T., Yang, W., Zhang, H., Zhu, B., Wang, X., Wang, S.,\
    \ et al. (2020b). Early detection of bacterial wilt \nin peanut plants through\
    \ leaf-level hyperspectral and unmanned aerial vehicle data. Computers and \n\
    Electronics in Agriculture, 177, 105708.\nDang, L. M., Hassan, S. I., Suhyeon,\
    \ I., Sangaiah, A. K., Mehmood, I., Rho, S., et al. (2020a). UAV based wilt \n\
    detection system via convolutional neural networks. Sustainable Computing: Informatics\
    \ and Systems, \n28, 100250.\n1 3\n1679\nPrecision Agriculture (2023) 24:1663–1682\n\
    Dang, L. M., Wang, H., Li, Y., Min, K., Kwak, J. T., Lee, O. N., et al. (2020b).\
    \ Fusarium wilt of radish detec-\ntion using rgb and near infrared images from\
    \ unmanned aerial vehicles. Remote Sensing, 12(17), 2863.\nDaponte, P., De Vito,\
    \ L., Glielmo, L., Iannelli, L., Liuzza, D., Picariello, F. (2019, May). A review\
    \ on the \nuse of drones for precision agriculture. In IOP Conference Series:\
    \ Earth and Environmental Science, \n275(1), 012022.\nDe Oca, A. M., & Flores,\
    \ G. (2021). The AgriQ: A low-cost unmanned aerial system for precision agriculture.\
    \ \nExpert Systems with Applications, 182, 115163.\nDeng, X., Zhu, Z., Yang, J.,\
    \ Zheng, Z., Huang, Z., Yin, X., et al. (2020). Detection of citrus huanglongbing\
    \ \nbased on multi-input neural network model of UAV hyperspectral remote sensing.\
    \ Remote Sensing, \n12(17), 2678.\nDevi, K. A., & Priya, R. (2021). Plant Disease\
    \ Identification using the unmanned aerial vehicle images. Turk-\nish Journal\
    \ of Computer and Mathematics Education, 12(10), 2396–2399.\nDi Gennaro, S. F.,\
    \ Battiston, E., Di Marco, S., & Facini, O. (2016). Unmanned aerial vehicle (UAV)-based\
    \ \nremote sensing to monitor grapevine leaf stripe disease within a vineyard\
    \ affected by esca complex. \nPhytopathologia Mediterranea, 55(2), 262–275.\n\
    Di Nisio, A., Adamo, F., Acciani, G., & Attivissimo, F. (2020). Fast detection\
    \ of olive trees affected by xylella \nfastidiosa from uavs using multispectral\
    \ imaging. Sensors (Basel, Switzerland), 20(17), 4915.\nEsposito, M., Crimaldi,\
    \ M., Cirillo, V., Sarghini, F., & Maggio, A. (2021). Drone and sensor technology\
    \ \nfor sustainable weed management: A review (8 vol., p. 18). Chemical and Biological\
    \ Technologies in \nAgriculture.\nFang, Y., & Ramasamy, R. P. (2015). Current\
    \ and prospective methods for Plant Disease Detection. Biosen-\nsors, 5(3), 537–561.\n\
    Franceschini, M. H., Bartholomeus, H., Apeldoorn van, D. F., Suomalainen, J.,\
    \ & Kooistra, L. (2019). Fea-\nsibility of unmanned aerial vehicle optical imagery\
    \ for early detection and severity assessment of late \nblight in Potato. Remote\
    \ Sensing, 11(3), 224.\nGarcía-Berná, J. A., Ouhbi, S., Benmouna, B., García-Mateos,\
    \ G., Fernández-Alemán, J. L., & Molina-\nMartínez, J. M. (2020). Systematic mapping\
    \ study on remote sensing in agriculture. Applied Sciences, \n10(10), 3456.\n\
    Hafeez, A., Husain, M. A., Singh, S. P., Chauhan, A., Khan, M. T., Kumar, N.,\
    \ et al. (2022). Implementation \nof drone technology for farm monitoring & pesticide\
    \ spraying: A review. Information Processing in \nAgriculture. https://doi.org/10.1016/j.inpa.2022.02.002(Article\
    \ in press).\nHajare, R., Mallikarjuna Gowda, C. P., & Sanjaya, M. V. (2021).\
    \ Design and implementation of Agricultural \nDrone for Areca Nut Farms. Advances\
    \ in VLSI, Signal Processing, Power Electronics, IoT, communica-\ntion and embedded\
    \ Systems (pp. 251–262). Singapore: Springer.\nHassler, S. C., & Baysal-Gurel,\
    \ F. (2019). Unmanned aircraft system (UAS) technology and applications in \n\
    agriculture. Agronomy, 9(10), 618.\nHeim, R. H., Wright, I. J., Scarth, P., Carnegie,\
    \ A. J., Taylor, D., & Oldeland, J. (2019). Multispectral, aerial \ndisease detection\
    \ for myrtle rust (Austropuccinia psidii) on a lemon myrtle plantation. Drones,\
    \ 3(1), 25.\nHuang, H., Deng, J., Lan, Y., Yang, A., Zhang, L., Wen, S., et al.\
    \ (2019). Detection of helminthosporium leaf \nblotch disease based on UAV imagery.\
    \ Applied Sciences, 9(3), 558.\nJavan, F. D., Samadzadegan, F., Pourazar, S. S.,\
    \ & Fazeli, H. (2019). UAV-based multispectral imagery for \nfast Citrus Greening\
    \ detection. Journal of Plant Diseases and Protection, 126(4), 307–318.\nKalischuk,\
    \ M., Paret, M. L., Freeman, J. H., Raj, D., Da Silva, S., Eubanks, S., et al.\
    \ (2019). An improved crop \nscouting technique incorporating unmanned aerial\
    \ vehicle-assisted multispectral crop imaging into con-\nventional scouting practice\
    \ for gummy stem blight in Watermelon. Plant Disease, 103(7), 1642–1650.\nKaya,\
    \ A., Keceli, A. S., Catal, C., Yalic, H. Y., Temucin, H., & Tekinerdogan, B.\
    \ (2019). Analysis of trans-\nfer learning for deep neural network based plant\
    \ classification models. Computers and Electronics in \nAgriculture, 158, 20–29.\n\
    Kerkech, M., Hafiane, A., & Canals, R. (2018). Deep learning approach with colorimetric\
    \ spaces and veg-\netation indices for vine diseases detection in UAV images.\
    \ Computers and Electronics in Agriculture, \n155(December), 237–243.\nKerkech,\
    \ M., Hafiane, A., & Canals, R. (2020a). Vine disease detection in UAV multispectral\
    \ images using \noptimized image registration and deep learning segmentation approach.\
    \ Computers and Electronics in \nAgriculture, 174, 105446.\nKerkech, M., Hafiane,\
    \ A., & Canals, R. (2020b). VddNet: Vine disease detection network based on multi-\n\
    spectral images and depth map. Remote Sensing, 12(20), 3305.\nKhanal, S., Fulton,\
    \ J., & Shearer, S. (2017). An overview of current and potential applications\
    \ of thermal \nremote sensing in precision agriculture. Computers and Electronics\
    \ in Agriculture, 139, 22–32.\nKim, J., Kim, S., Ju, C., & Son, H. I. (2019).\
    \ Unmanned aerial vehicles in agriculture: A review of per-\nspective of platform,\
    \ control, and applications. Ieee Access: Practical Innovations, Open Solutions,\
    \ 7, \n105100–105115.\n1 3\n1680\nPrecision Agriculture (2023) 24:1663–1682\n\
    Kitchenham, B., Brereton, P., Budgen, O., Turner, D., Bailey, M., J., & Linkman,\
    \ S. (2009). Systematic \nliterature reviews in Software Engineering - A systematic\
    \ literature review. Information and Software \nTechnology, 51(1), 7–15.\nKitpo,\
    \ N., & Inoue, M. (2018, March). Early rice disease detection and position mapping\
    \ system using drone \nand IoT architecture. In 2018 12th South East Asian Technical\
    \ University Consortium (SEATUC), 1, \n1–5. doi:https://doi.org/10.1109/SEATUC.2018.8788863.\n\
    Messina, G., & Modica, G. (2020). Applications of UAV thermal imagery in precision\
    \ agriculture: State of \nthe art and future research outlook. Remote Sensing,\
    \ 12(9), 1491.\nMogili, U. R., & Deepak, B. B. V. L. (2018). Review on application\
    \ of drone systems in precision agriculture. \nProcedia Computer Science, 133,\
    \ 502–509.\nOikonomidis, A., Catal, C., & Kassahun, A. (2022a). Deep learning\
    \ for crop yield prediction: A systematic \nliterature review. New Zealand Journal\
    \ of Crop and Horticultural Science, 1–26. https://doi.org/10.10\n80/01140671.2022.2032213.\n\
    Oikonomidis, A., Catal, C., & Kassahun, A. (2022b). Hybrid deep learning-based\
    \ Models for Crop Yield \nPrediction. Applied Artificial Intelligence, 36(1),\
    \ 2031822.\nPanday, U. S., Pratihast, A. K., Aryal, J., & Kayastha, R. B. (2020).\
    \ A review on drone-based data solutions \nfor cereal crops. Drones, 4(3), 41.\n\
    Sandhu, G. K., & Kaur, R. (2019). Plant disease detection techniques: a review.\
    \ In 2019 International Confer-\nence on Automation, Computational and Technology\
    \ Management (ICACTM), 34–38. IEEE.\nSavian, F., Martini, M., Ermacora, P., Paulus,\
    \ S., & Mahlein, A. (2020). Prediction of the kiwifruit decline \nsyndrome in\
    \ diseased orchards by remote sensing. Remote Sensing, 12(14), 2194.\nSelvaraj,\
    \ M. G., Vergara, A., Montenegro, F., Ruiz, H. A., Safari, N., Raymaekers, D.,\
    \ et al. (2020). Detection \nof banana plants and their major diseases through\
    \ aerial images and machine learning methods: A case \nstudy in DR Congo and Republic\
    \ of Benin. ISPRS Journal of Photogrammetry and Remote Sensing, \n169, 110–124.\n\
    Siebring, J., Valente, J., Franceschini, M. H., Kamp, J., & Kooistra, L. (2019).\
    \ Object-based image analysis \napplied to low altitude aerial imagery for potato\
    \ plant trait retrieval and pathogen detection. Sensors \n(Basel, Switzerland),\
    \ 19(24), 5477.\nSinha, J. P. (2020). Aerial robot for smart farming and enhancing\
    \ farmers’ net benefit. Indian Journal of \nAgricultural Sciences, 90(2), 258–267.\n\
    Tsouros, D. C., Bibi, S., & Sarigiannidis, P. G. (2019). A review on UAV-based\
    \ applications for precision \nagriculture. Information, 10(11), 349.\nVeroustraete,\
    \ F. (2015). The rise of the Drones in Agriculture. EC Agriculture, 2(2), 325–327.\n\
    Viera-Torres, M., Sinde-Gonzalez, I., Gil-Docampo, M., Bravo-Yandun, V., & Toulkeridis,\
    \ T. (2020). Gen-\nerating the baseline in the early detection of bud rot and\
    \ red ring disease in oil palms by geospatial \ntechnologies. Remote Sensing,\
    \ 12(19), 3229.\nWang, T., Thomasson, J. A., Isakeit, T., Yang, C., & Nichols,\
    \ R. L. (2020a). A plant-by-plant method to iden-\ntify and treat cotton root\
    \ rot based on UAV remote sensing. Remote Sensing, 12(15), 2453.\nWang, T., Thomasson,\
    \ J. A., Yang, C., Isakeit, T., & Nichols, R. L. (2020b). Automatic classification\
    \ of cotton \nroot rot disease based on UAV remote sensing. Remote Sensing, 12(8),\
    \ 1310.\nWiesner-Hanks, T., Stewart, E. L., Kaczmar, N., DeChant, C., Wu, H.,\
    \ Nelson, R. J., et al. (2018). Image set \nfor deep learning: Field images of\
    \ maize annotated with disease symptoms. BMC Research Notes, 11, \n440.\nWright,\
    \ R. W., Brand, R. A., Dunn, W., & Spindler, K. W. (2007). How to write a systematic\
    \ review. Clinical \nOrthopaedics and Related Research, 455, 23–29.\nWu, H., Wiesner-Hanks,\
    \ T., Stewart, E. L., DeChant, C., Kaczmar, N., Gore, M. A., et al. (2019). Autono-\n\
    mous detection of plant disease symptoms directly from Aerial Imagery. The Plant\
    \ Phenome Journal, \n2(1), 1–9.\nXavier, T. W., Souto, R. N., Statella, T., Galbieri,\
    \ R., Santos, E. S., Suli, G. S., et al. (2019). Identification of \nramularia\
    \ leaf blight cotton disease infection levels by multispectral, multiscale uav\
    \ imagery. Drones, \n3(2), 33.\nZhang, C., & Kovacs, J. M. (2012). The application\
    \ of small unmanned aerial systems for precision agricul-\nture: A review. Precision\
    \ agriculture, 13(6), 693–712.\nZhang, D., Zhou, X., Zhang, J., Lan, Y., Xu, C.,\
    \ & Liang, D. (2018). Detection of rice sheath blight using \nan unmanned aerial\
    \ system with high-resolution color and multispectral imaging.PLoS ONE, 13(5),\
    \ \ne0187470.\nZhang, X., Han, L., Dong, Y., Shi, Y., Huang, W., Han, L., et al.\
    \ (2019). A deep learning-based approach \nfor automated yellow rust disease detection\
    \ from high-resolution hyperspectral UAV images. Remote \nSensing, 11(13), 1554.\n\
    1 3\n1681\nPrecision Agriculture (2023) 24:1663–1682\nPublisher’s Note Springer\
    \ Nature remains neutral with regard to jurisdictional claims in published maps\
    \ and \ninstitutional affiliations.\nSpringer Nature or its licensor (e.g. a society\
    \ or other partner) holds exclusive rights to this article under a \npublishing\
    \ agreement with the author(s) or other rightsholder(s); author self-archiving\
    \ of the accepted manu-\nscript version of this article is solely governed by\
    \ the terms of such publishing agreement and applicable law. \n1 3\n1682\n"
  inline_citation: '>'
  journal: Precision Agriculture
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/s11119-023-10014-y.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Plant disease detection using drones in precision agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3389/fpls.2022.791018
  analysis: '>'
  authors:
  - Jaafar Abdulridha
  - Yiannis Ampatzidis
  - Jawwad A. Qureshi
  - Pamela D. Roberts
  citation_count: 15
  full_citation: '>'
  full_text: ">\nORIGINAL RESEARCH\npublished: 20 May 2022\ndoi: 10.3389/fpls.2022.791018\n\
    Edited by:\nGlen C. Rains,\nUniversity of Georgia, United States\nReviewed by:\n\
    Xiangming Xu,\nNational Institute of Agricultural\nBotany (NIAB), United Kingdom\n\
    Saeed Hamood Alsamhi,\nIbb University, Yemen\n*Correspondence:\nJaafar Abdulridha\n\
    ftash@uﬂ.edu\nYiannis Ampatzidis\ni.ampatzidis@uﬂ.edu\nSpecialty section:\nThis\
    \ article was submitted to\nSustainable and Intelligent\nPhytoprotection,\na section\
    \ of the journal\nFrontiers in Plant Science\nReceived: 07 October 2021\nAccepted:\
    \ 25 April 2022\nPublished: 20 May 2022\nCitation:\nAbdulridha J, Ampatzidis Y,\n\
    Qureshi J and Roberts P (2022)\nIdentiﬁcation and Classiﬁcation\nof Downy Mildew\
    \ Severity Stages\nin Watermelon Utilizing Aerial\nand Ground Remote Sensing\n\
    and Machine Learning.\nFront. Plant Sci. 13:791018.\ndoi: 10.3389/fpls.2022.791018\n\
    Identiﬁcation and Classiﬁcation of\nDowny Mildew Severity Stages in\nWatermelon\
    \ Utilizing Aerial and\nGround Remote Sensing and\nMachine Learning\nJaafar Abdulridha1*,\
    \ Yiannis Ampatzidis1*, Jawwad Qureshi2 and Pamela Roberts3\n1 Department of Agricultural\
    \ and Biological Engineering, Southwest Florida Research and Education Center,\
    \ University\nof Florida, Immokalee, FL, United States, 2 Department of Entomology\
    \ and Nematology, Southwest Florida Research\nand Education Center, University\
    \ of Florida, Immokalee, FL, United States, 3 Department of Plant Pathology, Southwest\n\
    Florida Research and Education Center, University of Florida, Immokalee, FL, United\
    \ States\nRemote sensing and machine learning (ML) could assist and support growers,\n\
    stakeholders, and plant pathologists determine plant diseases resulting from viral,\n\
    bacterial, and fungal infections. Spectral vegetation indices (VIs) have shown\
    \ to be\nhelpful for the indirect detection of plant diseases. The purpose of\
    \ this study was to\nutilize ML models and identify VIs for the detection of downy\
    \ mildew (DM) disease in\nwatermelon in several disease severity (DS) stages,\
    \ including low, medium (levels 1 and\n2), high, and very high. Hyperspectral\
    \ images of leaves were collected in the laboratory\nby a benchtop system (380–1,000\
    \ nm) and in the ﬁeld by a UAV-based imaging system\n(380–1,000 nm). Two classiﬁcation\
    \ methods, multilayer perceptron (MLP) and decision\ntree (DT), were implemented\
    \ to distinguish between healthy and DM-affected plants.\nThe best classiﬁcation\
    \ rates were recorded by the MLP method; however, only 62.3%\naccuracy was observed\
    \ at low disease severity. The classiﬁcation accuracy increased\nwhen the disease\
    \ severity increased (e.g., 86–90% for the laboratory analysis and 69–\n91% for\
    \ the ﬁeld analysis). The best wavelengths to differentiate between the DS stages\n\
    were selected in the band of 531 nm, and 700–900 nm. The most signiﬁcant VIs for\
    \ DS\ndetection were the chlorophyll green (Cl green), photochemical reﬂectance\
    \ index (PRI),\nnormalized phaeophytinization index (NPQI) for laboratory analysis,\
    \ and the ratio analysis\nof reﬂectance spectral chlorophyll-a, b, and c (RARSa,\
    \ RASRb, and RARSc) and the Cl\ngreen in the ﬁeld analysis. Spectral VIs and ML\
    \ could enhance disease detection and\nmonitoring for precision agriculture applications.\n\
    Keywords: artiﬁcial intelligence, hyperspectral imaging, plant disease, remote\
    \ sensing, UAV\nINTRODUCTION\nFlorida is the second-largest watermelon producer\
    \ in the United States behind Texas. Based on\nthe information from USDA Economic\
    \ Research Service in 2017, more than 113,000 acres were\ncultivated throughout\
    \ the United States, producing 40 million pounds of watermelon. The USA’s\nannual\
    \ total watermelon budget was $578.8 million in 2016. Several diseases adversely\
    \ impact\nFrontiers in Plant Science | www.frontiersin.org\n1\nMay 2022 | Volume\
    \ 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection in Watermelon\n\
    watermelon\nproduction\nin\nFlorida,\nand\naccurate\ndisease\nidentiﬁcation is\
    \ critical to implementing timely and eﬀective\nmanagement\ntactics.\nOne\nof\n\
    the\nsigniﬁcant\ndiseases\nof\nwatermelon is downy mildew (DM) caused by the fungal-like\n\
    oomycete (Pseudoperonospora cubensis). Symptoms of DM are\nfound on the leaves\
    \ where the lesions begin as chlorotic (yellow)\nareas that become necrotic (brown/black)\
    \ areas surrounded by\na chlorotic halo. Under humid conditions, dense sporulation\
    \ of\nthe pathogen on the underside of the leaves within the lesions\nappears\
    \ fuzzy. Severely aﬀected leaves become crumpled and\nbrown and may appear scorched.\
    \ Downy mildew infection\nspreads quickly and, if left unchecked, can destroy\
    \ an entire\nplanting within days, hence the nickname “wildﬁre” for both\nthe\
    \ rapid disease development and scorched leaf appearance.\nDowny mildew does not\
    \ aﬀect stems or fruit directly; however,\ndefoliation due to DM leaves fruit\
    \ exposed to sunburning,\nmaking the fruit non-marketable. Therefore, successful\
    \ ﬁeld\nscouting for diseases would improve the yield production by\nimplementing\
    \ timely and eﬀective management actions.\nBecause of the rapid and destructive\
    \ occurrence of the disease,\nearly detection and preventative fungicide applications\
    \ are\ncritical to its management. Non-destructive methods have been\nutilized\
    \ as remote sensing tools for identifying and evaluating\ndiseases occurring over\
    \ a season (Immerzeel et al., 2008). Bagheri\n(2020) applied aerial multispectral\
    \ imagery for the detection\nof ﬁre blight infected pear trees by utilizing unmanned\
    \ aerial\nvehicle (UAV), and several vegetation indices (VIs) (IPI, RDVI,\nMCARI1,\
    \ MCARI2, TVI, MTVI1, MTVI2, TCARI, PSRI, and\nARI) were evaluated for disease\
    \ detection. The support vector\nmachine (SVM) method was used to detect diseased\
    \ trees\nwith an accuracy of 95%. In another example, cotton root rot\ndisease\
    \ was detected by using UAV remote sensing and three\nautomatic classiﬁcation\
    \ methods to delineate cotton root rot-\ncontaminated areas (Wang et al., 2020).\
    \ Ye et al. (2020) developed\na technique for detecting and monitoring Fusarium\
    \ wilt disease\nby utilizing UAV multispectral imagery. Eight VIs associated\n\
    with pigment concentration and plant development changes were\nselected to determine\
    \ the plants’ biophysical and biochemical\ncharacteristics during the disease\
    \ progress development stages\n(Ye et al., 2020). Unmanned aerial vehicles can\
    \ cover large crop\nareas by employing aerial photography to monitor the progress\
    \ of\na disease over time. Dense period sequence analysis can provide\nadditional\
    \ information on the timing of plant ﬁeld changes and\nenhance the quality and\
    \ accuracy of information derived from\nremote sensing (Woodcock et al., 2020).\n\
    One of the beneﬁts of aerial imaging using UAVs is providing\ninformation on disease\
    \ hot spots. Remote sensing (e.g., UAV-\nbased hyperspectral imagery) can detect\
    \ plants with diseases in\nasymptomatic and early disease development stages,\
    \ which are\ncritical for timely disease management (Hariharan et al., 2019).\n\
    Abdulridha et al. (2019, 2020a) successfully detected diﬀerent\ndisease development\
    \ stages of laurel wilt in avocado and bacterial\nand target spots in tomatoes\
    \ with high classiﬁcation accuracies\nutilizing remote sensing and machine learning.\
    \ Lu et al. (2018)\nutilized spectral reﬂectance data to detect three diﬀerent\
    \ diseases\nin tomatoes, late blight, target spot, and bacterial spot, and\nseveral\
    \ VIs were extracted to distinguish between healthy\nand diseased plants. Only\
    \ a few studies utilized hyperspectral\ndata for watermelon disease detection,\
    \ which were conducted\nmainly in laboratories. Blazquez and Edwards (1986) utilized\
    \ a\nspectroradiometer technique to measure the spectral reﬂectance\nof healthy\
    \ watermelon and distinguish it from two diseases\n(Fusarium wilt and downy mildew).\
    \ They found signiﬁcant\ndiﬀerences between the disease categories, especially\
    \ in the NIR\nregion (700–900 nm). Kalischuk et al. (2019) used UAV-based\nmultispectral\
    \ imaging and several VIs to detect watermelons\ninfected with gummy stem blight,\
    \ anthracnose, Fusarium wilt,\nPhytophthora fruit rot, Alternaria leaf spot, and\
    \ cucurbit leaf\ncrumple virus in the ﬁeld.\nHowever, all the beneﬁts of remote\
    \ sensing for disease\ndetection are wasted (or squandered) if not timed correctly\n\
    with early control management. If early disease detection\nis achieved and management\
    \ practices are applied in time,\nlimiting the disease spread throughout the ﬁeld\
    \ and minimizing\neconomic losses are possible. One of the main goals of precision\n\
    agriculture is to optimize fungicide and pesticide usage by\ndetecting diseased\
    \ areas (hotspots) and performing site-speciﬁc\nspraying. A sensor-based detection\
    \ and mapping of stress\nsymptoms in crops are required to accomplish spatially\
    \ precise\napplications. Some recent studies have focused on sensor-based\ndetection\
    \ of pathogen infections in crops to implement site-\nspeciﬁc fungicide applications\
    \ (Abdulridha et al., 2020b,c). High-\nthroughput phenotyping tools, the internet\
    \ of things, and a smart\nenvironment can be used to observe the heterogeneity\
    \ of crop\nvigor and could be helpful to optimize agricultural input usage\nthrough\
    \ improved decisions on the spot and accurate timing and\ndose of chemical applications\
    \ (West et al., 2003; Almalki et al.,\n2021; Saif et al., 2021; Alsamhi et al.,\
    \ 2022).\nEarly disease detection is key to limiting the spread, reducing\nthe\
    \ severity, and minimizing crop damage. Accurate disease\nidentiﬁcation at the\
    \ beginning of an outbreak is essential for\nimplementing eﬀective management\
    \ tactics. To the best of\nour knowledge, a high-throughput technique for detecting\
    \ and\nmonitoring DM severity stages in watermelon ﬁelds has not\nyet been developed.\
    \ In this study, hyperspectral images were\ncollected in the ﬁeld (via UAVs) and\
    \ in the laboratory (via a\nbenchtop system) to (i) train ML models for the detection\
    \ and\nmonitoring of the DM for several DS stages in watermelon,\nand (ii) select\
    \ the best bands and VIs to distinguish between a\nhealthy and a DM-aﬀected watermelon\
    \ plant. To the best of our\nknowledge, we are the ﬁrst to develop a UAV-based\
    \ hyperspectral\nimaging technique to detect DM-aﬀected watermelon plants in\n\
    several disease severity (DS) stages.\nMATERIALS AND METHODS\nExperimental Plot\
    \ Design\nThe experiments were conducted at the University of Florida’s\nSouthwest\
    \ Florida Research and Education Center in Immokalee,\nFL, United States. Guidelines\
    \ established by the University of\nFlorida were followed for land preparation,\
    \ fertility, irrigation,\nweed management, and insect control. The beds were 0.81\
    \ m\nwide with 3.66 m centers covered with black polyethylene ﬁlm.\nFrontiers\
    \ in Plant Science | www.frontiersin.org\n2\nMay 2022 | Volume 13 | Article 791018\n\
    Abdulridha et al.\nDowny Mildew Detection in Watermelon\nFour-week-old watermelon\
    \ transplants “Crimson Sweet” were\nplanted on 9 March 2019 into the soil (Immokalee\
    \ ﬁne sand)\nin a complete randomized block treatment design with four\nreplicates.\
    \ Each plot consisted of 10 plants spaced 0.91 m apart\nwithin an 8.23 m row with\
    \ 3.05 m between each plot. The plants\nwere infected naturally by DM.\nData Collection\n\
    Healthy watermelon leaves were collected on April 10, 2019. After\nthe ﬁrst detection\
    \ of DM, leaves were collected from DM-aﬀected\nplants in ﬁve disease severities\
    \ (DS; percentage of leaf tissue\naﬀected) stages. For the laboratory analysis,\
    \ leaves were collected\nin the low DS stage (few lesions); then, from the medium\
    \ 1 and 2,\nhigh and very high DS stages. The grading system of DM severity\n\
    is as follows: the low DS had 5–10% severity, and as the disease\nprogressed,\
    \ the percentage of infection gradually increased in low\nto medium DS stages\
    \ 1 and 2 (11–20% and 21–30% severity,\nrespectively), and then the high and very\
    \ high DS stages increased\ndramatically from 31 to 50% and 51 to 75% severity,\
    \ respectively\n(Figure 1). In this study, spectral data were not collected when\n\
    the DS was more than 75%, because the leaves were in very bad\nshape and desiccated.\
    \ In the ﬁeld (UAV-based) study, DS was\ncategorized in two stages: low and high\
    \ (Figure 2). The diﬀerence\nbetween the number of DS stages in the laboratory\
    \ and ﬁeld was\ndue to the fact that, in the laboratory, each leaf was analyzed\
    \ as\na sample, and in the ﬁeld, an entire plant was used as a sample.\nHence,\
    \ it was easier to quantify the DS stage in a single leaf than\nin an entire plant,\
    \ which can include leaves in diﬀerent DS stages.\nData Collection and Analysis\
    \ in the\nLaboratory\nSpectral data were collected using a benchtop hyperspectral\n\
    imaging system, Pika L 2.4 (Resonon Inc., Bozeman MT,\nUnited States) (Figure\
    \ 1E). The Pika L 2.4 was equipped with a\n23-mm lens, which has a spectral range\
    \ of 380–1,030 nm, 15.3◦\nﬁeld of view, and a spectral resolution of 2.1 nm. The\
    \ same\nhyperspectral camera was utilized in the laboratory (Figure 1E)\nand ﬁeld\
    \ (Figure 2C) after changing lenses, which covered the\nsame spectral range. Resonon’s\
    \ hyperspectral imagers (RHI),\nknown as push-broom imagers, are line-scan imagers,\
    \ which have\n281 spectral channels. The system is made up of a linear stage\n\
    assembly, which is shifted by a stage motor. In the laboratory,\ncontrolled broadband\
    \ halogen lighting sources were set up above\nthe linear stage to produce ideal\
    \ situations for conducting spectral\nscans. The hyperspectral imaging system\
    \ was arranged in a way\nthat the lens’ distance from the linear stage was 0.5\
    \ m. The lights\nwere positioned at the same level as the lens on a parallel plane.\n\
    All scans were performed using the Spectronon Pro (Resonon\nInc., Bozeman, MT,\
    \ United States) software, which was connected\nto the camera system using a USB\
    \ cable. Before performing\nthe scans of the leaves, dark current noise was removed\
    \ using\nthe software. Then, the camera was calibrated by using a white\ntile\
    \ (reﬂectance reference), provided by the manufacturer, and\nplaced under the\
    \ same conditions as used for performing scans.\nThe selection of the regions\
    \ of interest (RoIs) (e.g., Figure 1B)\nwas done manually by picking six spectral\
    \ scan regions per leaf\nFIGURE 1 | (A) Healthy watermelon leaves and downy mildew\
    \ infected leaves\nin different severity stages (as examples): (B) low (this image\
    \ includes\nexamples of regions of interest, RoIs); (C) medium; and (D) high.\n\
    (E) Hyperspectral data collection in the laboratory by a Pika L2 (Resonon Inc.,\n\
    Bozeman MT, United States) hyperspectral camera.\n(10 leaves per DS stage) to\
    \ prevent the occurrence of any bias.\nThe total spectral scans (RoIs) selected\
    \ for each DS stage was\n60. Regions of interest were selected in such a way that\
    \ they\nincluded both the aﬀected and unaﬀected areas of leaf tissue. The\npixel\
    \ number of each spectral scan selected was between 800 and\n900 pixels. The average\
    \ of 60 spectral scans was used to form\nan overall spectral scan signature curve\
    \ for each DS stage. The\nSpectronon Pro software, which is a post-processing\
    \ data analysis\nsoftware, was used to analyze the spectral data of each leaf\
    \ scan.\nSeveral areas containing the symptomatic and non-symptomatic\nregions\
    \ on the leaves were selected using the selection tool and the\nspectrum was generated.\
    \ For the healthy and DM-aﬀected plans\n(ﬁve DS stages), several random spots\
    \ on leaves were selected and\nthe average spectral reﬂectance was calculated\
    \ and used to form\nthe spectral signature curves.\nData Collection and Analysis\
    \ in the Field\nSpectral data were collected in the ﬁeld by using a UAV\n(Matrice\
    \ 600 Pro, Hexacopter, DJI Inc., Shenzhen, China)\nFrontiers in Plant Science\
    \ | www.frontiersin.org\n3\nMay 2022 | Volume 13 | Article 791018\nAbdulridha\
    \ et al.\nDowny Mildew Detection in Watermelon\nFIGURE 2 | Downy mildew severity\
    \ stages in the ﬁeld: (A) low; (B) high; (C) UAV-based hyperspectral imaging system;\
    \ and (D) a calibration tarp.\nand the same hyperspectral camera (Pika L 2.4).\
    \ The UAV-\nbased imaging system included (Figure 2C): (i) a Resonon\nPika L 2.4\
    \ hyperspectral camera; (ii) a visible-NIR (V-NIR)\nobjective lens for the Pika\
    \ L camera with a focal length\nof 17 mm, ﬁeld of view (FOV) of 17.6 degrees,\
    \ and an\ninstantaneous ﬁeld of view (IFOV) of 0.71 mrad; (iii) a Global\nNavigation\
    \ Satellite System (GNSS) (Tallysman 33-2710NM-\n00-3000, Tallysman Wireless Inc.,\
    \ Ontario, Canada)/Inertial\nMeasurement Unit (IMU) (Ellipse N, SGB Systems S.A.S.,\n\
    France) ﬂight control system for multirotor aircraft to record\nsensor position\
    \ and orientation, and (iv) a Resonon hyperspectral\ndata analysis software (Spectronon\
    \ Pro, Resonon, Bozeman, MT,\nUnited States), which is capable of rectifying the\
    \ GPS/IMU\ndata using a georectiﬁcation plugin. The data were collected\nat 30\
    \ m above the ground and a speed rate of 1.5 m s−1. In\nthe produced map, the\
    \ pixel size is a function of the working\ndistance (distance between the camera\
    \ lens and the scanning\nstage/ﬁeld) and FOV. This value varies according to the\
    \ ﬂight\nparameters. In this study, it was around 35 mm per pixel.\nGray tarp\
    \ (Group VIII Technologies) was utilized to correct the\ndata reﬂectance from\
    \ radiance; the reﬂectance tarp was 36%.\nRadiometric calibration was performed\
    \ by using a calibrated\nintegrating sphere. The manufacturer took 100 lines of\
    \ spectral\ndata and built a radiometric calibration ﬁle that contains a\nlookup\
    \ table with all combinations of integration times and frame\nrates. These data\
    \ were used to convert raw camera data (digital\nnumbers) to physical units of\
    \ radiance in micro ﬂicks. The Pika\nL 2.4 camera is a “pushbroom” or line-scan\
    \ type imager that\nproduces a 2-D image, where every pixel in the image contains\n\
    a continuous reﬂectance spectrum. A calibration tarp was used to\ncalibrate the\
    \ data for various illumination conditions in the ﬁeld\n(Figure 2D). The RoIs\
    \ were randomly handpicked for each plant,\nand several spectral scans were done\
    \ to cover the entire canopy.\nEach RoI contained four pixels, and four RoIs were\
    \ selected for\neach plant. The total sample size for each DS stage was 20 plants.\n\
    The RoIs were then transferred as a text ﬁle and processed using\nthe SPSS software\
    \ (SPSS 13.0, Inc., Chicago; Microsoft Corp.,\nRedmond, WA, United States).\n\
    Vegetation Indices\nVegetation indices could serve as indicators to identify DS\
    \ stages\nbased on any defectiveness in the functioning of plants such\nas physiochemical\
    \ defects that aﬀect photosynthesis, metabolic,\nand nutritional processes. The\
    \ factors that are most aﬀected by\ndiseases and that could be measured are chlorophyll\
    \ content, cell\nstructure, cell sap, presence, and relative abundance of pigments\n\
    concentration, water content, and carbon as expressed in the\nsolar-reﬂected optical\
    \ spectrum (400–2,500 nm) (Xiao et al.,\n2014). A neural network multilayer perceptron\
    \ was performed\nto select the best VIs that could identify DM disease and its\
    \ DS.\nFor data analysis and evaluation of all VIs evaluated in this study\n(Table\
    \ 1), the SPSS software was used. The correlation coeﬃcient\nwas another parameter\
    \ that was utilized in this study to evaluate\nits VI’s performance in detecting\
    \ DM severity stages.\nClassiﬁcation Methods\nDecision Tree\nA decision tree (DT)\
    \ is a non-parametric managed learning\nprocess used for organization and regression.\
    \ The objective of DT\nis to produce a model that calculates the value of a goal\
    \ variable by\nlearning simple choice instructions deducted from data features.\n\
    A decision tree has the capability of handling data measured on\ndiﬀerent rulers\
    \ in the absence of any models for the proportion\ndistributions of the data individually\
    \ from the modules, elasticity,\nFrontiers in Plant Science | www.frontiersin.org\n\
    4\nMay 2022 | Volume 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection\
    \ in Watermelon\nTABLE 1 | Spectral vegetation indices evaluated for downy mildew\
    \ disease detection.\nRatio analysis of reﬂectance spectral chlorophyll-a (RARSa)\n\
    RARSa = R675\nR700\nChappelle et al., 1992\nRatio analysis of reﬂectance spectral\
    \ chlorophyll b (RARSb)\nRARSb =\nR675\n(R700×R650)\nChappelle et al., 1992\n\
    Ratio analysis of reﬂectance spectra (RARSc)\nRARSc = R760\nR500\nChappelle et\
    \ al., 1992\nPigment speciﬁc simple ratio (PSSRa)\nPSSRa = R800\nR680\nBlackburn,\
    \ 1998\nNormalized difference vegetation index 761 (NDVI 761)\nNDVI 761 = (R761−R651)\n\
    (R761+R651)\nRaun et al., 2001\nGreen NDVI (GNDVI)\nGNDVI = (NIR850−G580)\n(NIR850+G580)\n\
    Gitelson and Merzlyak, 1996\nPhotochemical reﬂectance index (PRI)\nPRI = (R531−R570)\n\
    (R531+R570)\nGamon et al., 1992\nSimple ratio index (SR900)\nSR900 = R900\nR650\n\
    Jordan, 1969\nWater stress and canopy temperature (NWI 2)\nNWI2 = R970−R850\n\
    R970+R850\nBabar et al., 2006\nStructure insensitive pigment index (SIPI)\nSIPI\
    \ = (R840−R450)\n(R840−R670)\nPenuelas et al., 1995\nNormalized phaeophytinization\
    \ index (NPQI)\nNPQI = (R415−R435)\n(R415−R435)\nBarnes et al., 1992\nNormalized\
    \ difference vegetation index 761 (NDVI 761)\nNDVI 761 = (R761−R651)\n(R761+R651)\n\
    Raun et al., 2001\nNormalized difference vegetation index 850 (NDVI 850)\nNDVI\
    \ 850 = (R850−R651)\n(R850+R651)\nRaun et al., 2001\nSimple ratio index (SR850)\n\
    SR850 = R850\nR650\nJordan, 1969\nModiﬁed triangular vegetation index1 (MTVI 1)\n\
    MTVI 1 = 1.2[1.2(1.2(R760-R580)—2.5(R650-R580)]\nHaboudane et al., 2004\nModiﬁed\
    \ triangular vegetation index2 (MTVI 2)\nMTVI 2 =\n1.5[1.2(R760−R580)−2.5(R650−R580)]\n\
    SQ[(2 ∗ R760 + 1)∧2 − (6 ∗ R760 − 5 ∗ SQ(R650) − 0.5]\nHaboudane et al., 2004\n\
    Renormalized difference vegetation Index (RDVI)\nRDVI =\n(R761−R651)\nSQ(R761+R651)\n\
    Roujean and Breon, 1995\nTriangle vegetation index (TVI)\nTVI = 0.5[120∗(R761-R581)-200(R651-R581)]\n\
    Broge and Leblanc, 2001\nRed-edge vegetation stress index 1 (RVS1)\nRVS1 = [ (R651+Red\
    \ Edge 750)\n2\n] − Red Edge 733\nMerton, 1998\nGreen vegetation (VI green)\n\
    VI Green = (R760−R651)\n(R760+R651)\nGitelson et al., 2002\nTransform chlorophyll\
    \ absorption in reﬂectance index (TCARI)\nTCARI = 3[(R740-R651)-0.2(R740-R581)\
    \ (R740/R651)]\nHaboudane et al., 2002\nWater index (WI)\nWI = R900\nR970\nPenuelas\
    \ et al., 1997\nModiﬁed chlorophyll absorption in reﬂectance index (mCARI 1)\n\
    mCARI 1 = 1.2[(2.5∗R761-R651)-1.3(R761-R581)]\nHaboudane et al., 2004\nAnthocyanin\
    \ reﬂectance index (ARI)\nARI = (\n1\nR550 ) − (\n1\nR700 )\nGitelson et al.,\
    \ 2001\nChlorophyll green (Chl green)\nChl green = (R760/R800)\n(R540/R560)\n\
    Gitelson et al., 2003\nChlorophyll index green (Cl green NIR/Green)\nCl green\
    \ NIR/Green =\n\x10\nNIR\nGreen\n\x11\n− 1\nGitelson et al., 2003\nChlorophyll\
    \ index red edge (Cl rededge)\nCl rededge =\n\x10\nR780\nR705\n\x11\n− 1\nGitelson\
    \ et al., 2003\nand capability to handle non-linear relationships among features\n\
    and modules (Friedl and Brodley, 1997). The decision tree can\nbe qualiﬁed rapidly\
    \ and are quick in execution. It is a widely used\ntechnique in image processing\
    \ for detecting several plant diseases.\nIn this study, the classiﬁcation was\
    \ accrued between healthy and\nDM-aﬀected watermelon plans in several DS stages.\
    \ The dataset\nwas split into 70% training and 30% testing.\nMultilayer Perceptron\n\
    Multilayer perceptron (MLP), which is a deep artiﬁcial neural\nnetwork, was applied\
    \ to identify the diﬀerence between healthy\nand several DM disease severity stages.\
    \ Similar to a neural\nnetwork, MLP is a function of predictors, also called inputs,\n\
    or independent variables that minimize the prediction error of\ntarget variables,\
    \ also called outputs. These models can learn by\nexample. Thus, when using a\
    \ neural network, there is no need\nto program how the output is obtained for\
    \ the given certain\ninput; rather, a learning algorithm is used by the neural\
    \ network\nto calculate the relationship between input and output, which\nis then\
    \ utilized to predict output with the entered input values.\nThe neural network\
    \ creates a ﬁtted model in an analytical form,\nwhere the parameters are weight,\
    \ bias, and network topology.\nThe multilayer perceptron is a fully connected\
    \ multilayer feed-\nforward supervised learning network trained by the back-\n\
    propagation algorithm to minimize a quadratic error criterion;\nno values are\
    \ fed back to earlier layers. The multilayer perceptron\nis composed of an input\
    \ layer, a hidden layer, and an output layer.\nThe input and output layers are\
    \ not weighted, and the transfer\nfunctions on the hidden layer nodes are radially\
    \ symmetric. The\nfull dataset was randomly split into two datasets by partitioning\n\
    the active dataset into training (70%) and testing (30%) samples.\nAfter learning,\
    \ the MPL model was run on the test set that\nprovided an unbiased estimate of\
    \ the generalization error.\nRESULTS\nLaboratory-Based Analysis\nDuring the spectral\
    \ data collection under optimal light and\ntemperature conditions in the laboratory,\
    \ the spectral reﬂectance\nof ﬁve DM disease severity stages was taken. The spectral\n\
    signatures\nand\ncorrelation\ncoeﬃcient\nat\ndiﬀerent\ndisease\nseverities were\
    \ measured and compared (Figure 3). The spectral\nreﬂectance of the very high\
    \ DS stage showed a signiﬁcant increase\nFrontiers in Plant Science | www.frontiersin.org\n\
    5\nMay 2022 | Volume 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection\
    \ in Watermelon\nFIGURE 3 | (A) Spectral reﬂectance signatures (collected in the\
    \ laboratory) of downy mildew affected watermelon leaves in ﬁve disease severity\
    \ (DS) stages; and (B)\ncorrelation coefﬁcient for watermelon leaves in healthy\
    \ (H) and ﬁve DS stages.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nH vs. Low\
    \ severity\nstage\nH vs. Medium\nseverity stage 1\nH vs. Medium\nseverity stage\
    \ 2\nH vs. High\nseverity stage\nH vs. Very high\nseverity stage\nClassiﬁca\x02\
    on rate (%)\nRBF (%)\nTD (%)\nFIGURE 4 | The classiﬁcation results of the MLP\
    \ and DT methods to\ndistinguish healthy (H) against several disease severity\
    \ stages of downy\nmildew disease in watermelon in the laboratory. The vertical\
    \ lines on the\ncolumns are error bars.\nin the green and red range (450–650 nm),\
    \ while other stages\nshowed lower spectral reﬂectance in the visible range (380–\n\
    700 nm). The spectral reﬂectance values of the very high DS stage\nwere decreased\
    \ in the NIR range, and the spectral reﬂectance in\nthe red edge diverged from\
    \ other stages. The leaves of watermelon\nin the low and medium DS stages had\
    \ very few symptoms;\ntherefore, there were only slight diﬀerences between the\
    \ spectral\nreﬂectance for these stages. The spectral reﬂectance signature is\n\
    strongly related to the severity of the expected DM pathogen\nsymptom (Figure\
    \ 3A). In the visible range (380–700 nm), there\nTABLE 2 | Best wavebands and\
    \ vegetation indices measured in the laboratory\nand ﬁeld for detecting different\
    \ disease severity stages of downy mildew.\nDisease severity (DS)\nstages\nThe\
    \ weight of best bands\n(95–100%)\nBest vegetation\nindices\nLaboratory\nLow\n\
    722 (100%), 711 (99%), 716\n(98%), 709 (96%)\nCI green\nMedium 1\n722 (100%),\
    \ 720 (99%), 718\n(99%), 716 (98%)\nPRI\nMedium 2\n1,020 (100%), 1,014 (99%),\n\
    1,019 (98%), 1,016 (96%)\nNPQI\nHigh\n1,010 (100%), 1,020 (99%),\n1,014 (99%),\
    \ 1,007 (97%)\nNPQI\nVery high\n761 (100%), 759 (99%), 757\n(99%), 763 (99%)\n\
    NPQI\nField (UAV based)\nLow\n952 (100%), 956 (99%), 947\n(98%), 965 (97%)\nRARSc,\
    \ RARSa\nHigh\n755 (100%), 771 (99%), 766\n(99%), 780 (98%)\nRARSb, CI green\n\
    were not many diﬀerences in the correlation coeﬃcient signature\nof all DS stages\
    \ (Figure 3B). The signatures were identical\nuntil the red-edge range (700 nm),\
    \ where it gradually showed\ndiﬀerences between DS stages (especially the high\
    \ and very\nhigh severity stages). It is obvious from Figure 3B that the\ncorrelation\
    \ coeﬃcient showed wide diﬀerences in the NIR range\nas the DS increased.\nThe\
    \ results of the classiﬁcation varied based on disease\nseverity, as in the low\
    \ DS stage, the classiﬁcation was lower\nFrontiers in Plant Science | www.frontiersin.org\n\
    6\nMay 2022 | Volume 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection\
    \ in Watermelon\n0\n10\n20\n30\n40\n50\n60\n70\n80\n380\n580\n780\n980\nReﬂectance\
    \ (%)\nWavelength (nm)\nH\nLow severity stage\nHigh severity stage\n-1\n-0.5\n\
    0\n0.5\n1\n380\n580\n780\n980\nCorrela\x02on Coeﬃcient \nWavelength (nm)\nH\n\
    Low severity stage\nHigh severity stage\nA\nB\nFIGURE 5 | (A) Spectral reﬂectance\
    \ signatures developed from hyperspectral imaging collected in the ﬁeld; and (B)\
    \ correlation coefﬁcient for watermelon plant in\nhealthy (H), low, and high downy\
    \ mildew severity stages.\nthan the other stages (e.g., 62.3 and 52.6% for MLP\
    \ and DT,\nrespectively). The classiﬁcation rate increased as the severity of\n\
    disease symptoms increased (Figure 4). The highest classiﬁcation\nvalue of MLP\
    \ was in the very high DS stage at 90%. The DT\nclassiﬁcation method had lower\
    \ detection accuracies than the\nMLP classiﬁcation method for all DS stages (Figure\
    \ 4). Hence,\nthe MLP method was selected as the best classiﬁcation method\nfor\
    \ DM detection and DS stages classiﬁcation. Therefore, it was\nused for the selection\
    \ of the best wavebands and VIs for disease\ndetection. The best wavebands for\
    \ detecting the low and medium\n1 DS stages were found on the red edge (711–722\
    \ nm), medium\n2 and high DS stages were found at 1,007–1,020 nm, and the\nvery\
    \ high DS stage was found in red edge (759–761 nm). The\nbest VIs, selected by\
    \ using the MLP classiﬁcation method, for low\nand medium DS stages 1 were the\
    \ Cl green and PRI, respectively,\nwhile for medium 2, high, and very high DS\
    \ stages were the\nNPQI (Table 2).\nField-Based Analysis\nFigure 5A shows the\
    \ UAV-based spectral signatures of healthy\nand DM-aﬀected plants. In the visible\
    \ range, the spectral\nreﬂectance values of healthy plants and DM-aﬀected plans\
    \ in a\nlow DS stage were lower than in the high DS stage, which had\na peak value\
    \ of 20% at the green band. The spectral reﬂectance\nvalues of healthy plants\
    \ were lower than the low and high DS\nstages in the NIR range. The spectral reﬂectance\
    \ of the low DS\nstage was higher than the high DS stage, especially in the range\
    \ of\n750–915. Major diﬀerences cannot be seen in the red edge, where\nboth DS\
    \ stages had almost identical signatures. In the NIR range\n(700–1,000 nm), the\
    \ spectral reﬂectance values of the low and\nhigh DS stages were higher than the\
    \ spectral reﬂectance values\nof healthy plants.\nThe correlation coeﬃcients for\
    \ healthy, low, and high DS\nstages were almost identical in the visible range\
    \ (Figure 5B). In\nthe range of 750–1,000 nm, there were some diﬀerences recorded\n\
    0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nH vs. Low severity stage\nH vs. High\
    \ severity stage\nClassiﬁca\x02on rate (%)\nRBF (%)\nTD (%)\nFIGURE 6 | The classiﬁcation\
    \ results of the MLP and DT methodologies for\ndetecting several disease severity\
    \ stages of downy mildew in watermelon\nplants in the ﬁeld against healthy plants\
    \ (H). The vertical lines on the columns\nare error bars.\nbetween the correlation\
    \ coeﬃcient values of the two DS stages\n(Figure 5B). Figure 5B shows diﬀerences\
    \ between the low and\nhigh DS stages at 750 and 1,000 nm.\nThe MLP method gave\
    \ a higher classiﬁcation accuracy than\nthe DT method; in the low DS stage, the\
    \ classiﬁcation accuracy of\nthe MLP was 69%, while the classiﬁcation accuracy\
    \ of the DT was\n60% (Figure 6). The highest classiﬁcation accuracy was achieved\n\
    in the high DS stage at 91% for MLP, while it was 69% for DT. The\nbest wavebands\
    \ for low DS stage classiﬁcation were between 952\nand 965 nm, while in the high\
    \ DS stage the best wavebands were\nin red edge (755–780 nm). The best VIs for\
    \ disease detection were\nthe RARSc and RARSa for the low DS stage, and the RARSb\
    \ and\nCI green for the high DS stage (Table 2).\nFrontiers in Plant Science |\
    \ www.frontiersin.org\n7\nMay 2022 | Volume 13 | Article 791018\nAbdulridha et\
    \ al.\nDowny Mildew Detection in Watermelon\nDISCUSSION\nWatermelon plants are\
    \ very susceptible to diseases; therefore,\nseveral studies developed non-destructive\
    \ and high-throughput\ntechniques to detect diseases like anthracnose, leaf blight,\
    \ and\nleaf spot (He et al., 2021). Blazquez and Edwards (1986)\ndeveloped a laboratory-based\
    \ technique to detect watermelon\ninfected with Fusarium wilt, downy mildew, and\
    \ watermelon\nmosaic. All three diseases had signiﬁcant diﬀerences in the\nnear\
    \ infra-red range (700–900 nm). Kalischuk et al. (2019)\napplied UAV multispectral\
    \ imaging to identify, using NDVI,\ngummy stem blight, anthracnose, Fusarium wilt,\
    \ Phytophthora\nfruit rot, Alternaria leaf spot, and cucurbit leaf crumple disease.\n\
    Disease incidence and severity ratings were signiﬁcantly diﬀerent\nbetween conventional\
    \ scouting and UAV-assisted scouting. There\nis no other UAV-based system available\
    \ for the detection and\nmonitoring of DM in the ﬁeld.\nThe diﬀerence between\
    \ our results (aka, DS detection accuracy\nby the models, and best wavelengths\
    \ and VIs for DS detection)\nin the laboratory and ﬁeld can be explained by the\
    \ diﬀerent\nenvironmental conditions and data collection procedures. For\nexample,\
    \ in the laboratory, data are collected from single leaves\nat a close distance\
    \ and with artiﬁcial light, in contrast to the ﬁeld\nwhere data are collected\
    \ by a UAV (30 m above ground) from\nentire plants. Hence, the results from the\
    \ laboratory and ﬁeld\nanalysis cannot be directly compared.\nSpectral Reﬂectance\
    \ Signatures\nThe DM-aﬀected watermelon leaves showed varied spectral\nreﬂectance\
    \ signatures for each DS stage (both in the laboratory\nand ﬁeld). However, in\
    \ the visible range, there were no signiﬁcant\ndiﬀerences between the healthy\
    \ and the low DS stage (5–10%\ninfection), which indicates that it is very diﬃcult\
    \ to distinguish\namong these stages with visual observation. For that reason,\n\
    hyperspectral imaging and machine learning can be used for a\nmore eﬃcient and\
    \ rapid early plant disease and severity detection\n(compared to visual detection\
    \ methods).\nLaboratory measurements showed that the spectral reﬂectance\nof healthy\
    \ plants was higher than the other DS stages in the\nblue range. The main diﬀerences\
    \ between healthy and infected\nplants in the spectral signatures, both in the\
    \ laboratory and ﬁeld\nmeasurements, were found in the high severity stage in\
    \ the green,\nred edge, and NIR range (700–1,000), while in NIR, the spectral\n\
    reﬂectance signature of healthy plants was higher than the other\nDS stages. The\
    \ high DS stage had a unique signature that can be\nused to distinguish this stage\
    \ from others (and healthy plants).\nAlthough we were able to visually observe\
    \ the change of the colors\nof the leaves for some of the DS stages, not all had\
    \ the same\nlevel of change.\nIn the ﬁeld, the results did not show signiﬁcant\
    \ diﬀerences\nbetween the low and high DS stages in the visible range; only\n\
    slight diﬀerences were observed. However, that minor diﬀerences\nin the spectral\
    \ reﬂectance in the visible range can be still\nconsidered as an indication of\
    \ color change in the leaves. The\nleaves and the plant canopy in the low DS stage\
    \ showed very\nfew visual symptoms, and the classiﬁcation accuracy was low,\n\
    both in the laboratory and ﬁeld. As the chlorophyll content\nand water content\
    \ decrease, the leaf cell damage increases\n(Barnawal et al., 2017), and that\
    \ helps the classiﬁcation methods\nto better classify the DS stages, especially\
    \ in the medium and\nvery high DS stages.\nVegetation Indices\nThe common practice\
    \ for selecting signiﬁcant wavelengths\nfor the DS detection of new VIs is by\
    \ the correlation to a\nbiochemical or biophysical trait, for example, chlorophyll\n\
    a + b content leaf structure parameter, the water content,\nand so on (Gitelson\
    \ and Merzlyak, 1996; Hatﬁeld et al.,\n2008).\nRegularly,\nthe\nﬂuctuation\nof\n\
    spectral\nreﬂectance\nmight guide to the detection of plants under stress without\n\
    specifying or providing a description of what type of stress\ncause the damage\
    \ to the plant (Carter and Miller, 1994;\nGitelson and Merzlyak, 1994). For example,\
    \ as was mentioned\nearlier, several factors could reduce the chlorophyll content,\n\
    of\nwhich\nsome\nare\nphysiological\nor\nbiochemical\ncaused\nby DM severity and\
    \ this reduction might inﬂuence the\nphotosynthesis activity.\nIn both experimental\
    \ conditions, the best vegetation indices\n(e.g., the PRI, this index is more\
    \ related to the green range;\nand the CI green) were able to discriminate between\
    \ healthy\nand DM-aﬀected plants in the low and the high DS stages.\nThe PRI,\
    \ which is produced by normalizing 531 and 570 nm,\nbasically relays on the green\
    \ range. Any deﬁciency or disorder in\nchlorophyll will inﬂuence the PRI value;\
    \ the PRI has increasingly\nbeen used as an indicator of photosynthetic eﬃciency\
    \ (Garbulsky\net al., 2011) and as an indicator of water stress (Suarez et al.,\n\
    2008). One of the ﬁrst changes in a DM-aﬀected plant is the\nreduction of the\
    \ chlorophyll concentration that aﬀects the process\nof photosynthesis in the\
    \ infected leaf, and some VIs associated\nwith chlorophyll content could be used\
    \ to detect these changes\n(Mandal et al., 2009; Bellow et al., 2013; Barnawal\
    \ et al., 2017).\nOur ﬁndings suggest the same.\nCONCLUSION\nThe selected best\
    \ spectral VIs resulted in high speciﬁcity and\nsensitivity for the detection\
    \ and identiﬁcation of downy mildew\ndisease in diﬀerent stages of severity. Lower\
    \ classiﬁcation results\nwere achieved in the low DS stage, because of the minor\n\
    changes in leaf composition (compared to a healthy plant).\nThe highest classiﬁcation\
    \ results were obtained from the MLP\nmethod in high and very high DS stages (87–90%),\
    \ while the\nDT method recorded lower classiﬁcation results (compared to\nMLP)\
    \ for all DS stages. Some VIs can be used for disease\ndetection and classiﬁcation\
    \ of the DS stages. The use of\nhyperspectral imaging for identifying the most\
    \ signiﬁcant VIs\nto detect and identify several DS stages will further enhance\
    \ the\nunderstanding and speciﬁcity of disease detection. Future work\nincludes\
    \ the development of a simple and inexpensive UAV-based\nsensor, based on previous\
    \ research and developments, that only\nmeasures spectral reﬂectance at narrow\
    \ bands (e.g., customized\nmultispectral camera), centered at speciﬁc wavelengths\
    \ for early\nDM detection in the ﬁeld.\nFrontiers in Plant Science | www.frontiersin.org\n\
    8\nMay 2022 | Volume 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection\
    \ in Watermelon\nDATA AVAILABILITY STATEMENT\nThe raw data supporting the conclusions\
    \ of this article will be\nmade available by the authors, without undue reservation.\n\
    AUTHOR CONTRIBUTIONS\nJA, YA, JQ, and PR conceived, designed, processed, analyzed,\n\
    and interpreted the experiments. JA, YA, and PR acquired\nthe data. JA and YA\
    \ analyzed the data and prepared the\nmanuscript. JQ and PR edited the manuscript.\
    \ All authors\ncontributed\nto\nthe\narticle\nand\napproved\nthe\nsubmitted\n\
    version.\nFUNDING\nThis\nmaterial\nwas\nmade\npossible,\nin\npart,\nby\na\nCooperative\n\
    Agreement\nfrom\nthe\nU.S.\nDepartment\nof\nAgriculture’s Agricultural Marketing\
    \ Service through grant\nAM190100XXXXG036.\nREFERENCES\nAbdulridha, J., Ehsani,\
    \ R., Abd-Elrahma, A., and Ampatzidis, Y. (2019). A remote\nsensing technique\
    \ for detecting laurel wilt disease in avocado in presence of\nother biotic and\
    \ abiotic stresses. Comput. Electron. Agric. 156, 549–557. doi:\n10.1016/j.compag.2018.12.018\n\
    Abdulridha, J., Ampatzidis, Y., Kakarla, S. C., and Roberts, P. (2020a). Detection\n\
    of target spot and bacterial spot disease in tomato using UAV-based and\nbenchtop-based\
    \ hyperspectral imaging techniques. Precis. Agric. 21, 955–978.\ndoi: 10.1007/s11119-019-09703-4\n\
    Abdulridha, J., Ampatzidis, Y., Qureshi, J., and Roberts, P. (2020b). Laboratory\n\
    and UAV-based identiﬁcation and classiﬁcation of tomato yellow leaf curl,\nbacterial\
    \ spot, and target spot diseases in tomato utilizing hyperspectral\nimaging and\
    \ machine learning. Remote Sens. 12:2732. doi: 10.3390/rs1217\n2732\nAbdulridha,\
    \ J., Ampatzidis, Y., Roberts, P., and Kakarla, S. C. (2020c). Detecting\npowdery\
    \ mildew disease in squash at diﬀerent stages using UAV-based\nhyperspectral imaging\
    \ and artiﬁcial intelligence. Biosyst. Eng. 197, 135–148.\ndoi: 10.1016/j.biosystemseng.2020.07.001\n\
    Almalki, F. A., Souﬁene, B., Alsamhi, S. H., and Sakli, H. (2021). A Low-Cost\n\
    Platform for Environmental Smart Farming Monitoring System Based on IoT\nand UAVs.\
    \ Sustainability 13:5908. doi: 10.3390/su13115908\nAlsamhi, S. H., Almalki, F.\
    \ A., Afghah, F., Hawbani, A., Shvetsov, A. V., Lee,\nB., et al. (2022). Drones’\
    \ Edge Intelligence Over Smart Environments in B5G:\nblockchain and Federated\
    \ Learning Synergy. IEEE Trans. Green Commun.\nNetw. 6, 295–312. doi: 10.1109/tgcn.2021.3132561\n\
    Babar, M. A., Reynolds, M. P., Van Ginkel, M., Klatt, A. R., Raun, W. R., and\n\
    Stone, M. L. (2006). Spectral reﬂectance to estimate genetic variation for in-\n\
    season biomass, leaf chlorophyll, and canopy temperature in wheat. Crop Sci.\n\
    46, 1046–1057. doi: 10.2135/cropsci2005.0211\nBagheri, N. (2020). Application\
    \ of aerial remote sensing technology for detection\nof ﬁre blight infected pear\
    \ trees. Comput. Electron. Agric. 168:105147. doi:\n10.1016/j.compag.2019.105147\n\
    Barnawal, D., Pandey, S. S., Bharti, N., Pandey, A., Ray, T., Singh, S., et al.\
    \ (2017).\nACC deaminase-containing plant growth-promoting rhizobacteria protect\n\
    Papaver somniferum from downy mildew. J. Appl. Microbiol. 122, 1286–1298.\ndoi:\
    \ 10.1111/jam.13417\nBarnes, J. D., Balaguer, L., Manrique, E., Elvira, S., and\
    \ Davison, A. W. (1992).\nA Reappraisal of the Use of Dmso for the Extraction\
    \ and Determination of\nChlorophylls-A and Chlorophylls-B in Lichens and Higher-Plants.\
    \ Environ.\nExp. Bot. 32, 85–100. doi: 10.1016/0098-8472(92)90034-y\nBellow, S.,\
    \ Latouche, G., Brown, S. C., Poutaraud, A., and Cerovic, Z. G. (2013).\nOptical\
    \ detection of downy mildew in grapevine leaves: daily kinetics of\nautoﬂuorescence\
    \ upon infection. J. Exp. Bot. 64, 333–341. doi: 10.1093/jxb/\ners338\nBlackburn,\
    \ G. A. (1998). Spectral indices for estimating photosynthetic pigment\nconcentrations:\
    \ a test using senescent tree leaves. Int. J. Remote Sens. 19,\n657–675. doi:\
    \ 10.1080/014311698215919\nBlazquez, C. H., and Edwards, G. J. (1986). Spectral\
    \ reﬂectance of healthy and\ndiseased watermelon leaves. Ann. Appl. Biol. 108,\
    \ 243–249. doi: 10.1111/j.1744-\n7348.1986.tb07646.x\nBroge, N. H., and Leblanc,\
    \ E. (2001). Comparing prediction power and stability\nof broadband and hyperspectral\
    \ vegetation indices for estimation of green leaf\narea index and canopy chlorophyll\
    \ density. Remote Sens. Environ. 76, 156–172.\ndoi: 10.1016/s0034-4257(00)00197-8\n\
    Carter, G. A., and Miller, R. L. (1994). Early detection of plant stress by digital\n\
    imaging within narrow stress-sensitive wavebands. Remote Sens. Environ. 50,\n\
    295–302. doi: 10.1016/0034-4257(94)90079-5\nChappelle, E. W., Kim, M. S., and\
    \ McMurtrey, J. E. (1992). Ration analysis\nof reﬂectance spectra (RARS)-An algorithm\
    \ for the remote estimation\nconcentration of chlorophyll-a, chlorophyll-b, and\
    \ carotenoid soybean leaves.\nRemote Sens. Environ. 39, 239–247. doi: 10.1016/0034-4257(92)90089-3\n\
    Friedl, M. A., and Brodley, C. E. (1997). Decision tree classiﬁcation of land\
    \ cover\nfrom remotely sensed data. Remote Sens. Environ. 61, 399–409. doi: 10.1016/\n\
    s0034-4257(97)00049-7\nGamon, J. A., Penuelas, J., and Field, C. B. (1992). A\
    \ narrow-waveband spectral\nindex that tracks diurnal changes in photosynthetic\
    \ eﬃciency. Remote Sens.\nEnviron. 41, 35–44. doi: 10.1016/0034-4257(92)90059-S\n\
    Garbulsky, M. F., Penuelas, J., Gamon, J., Inoue, Y., and Filella, I. (2011).\n\
    The photochemical reﬂectance index (PRI) and the remote sensing of\nleaf, canopy\
    \ and ecosystem radiation use eﬃciencies A review and meta-\nanalysis. Remote\
    \ Sens. Environ. 115, 281–297. doi: 10.1016/j.rse.2010.08.\n023\nGitelson, A.,\
    \ and Merzlyak, M. N. (1994). Quantitative estimation of chlorophyll-a\nusing\
    \ reﬂectance spectra - experiments with autumn chestnut and maple leaves.\nJ.\
    \ Photochem. Photobiol. B Biol. 22, 247–252. doi: 10.1016/1011-1344(93)06963-\n\
    4\nGitelson, A. A., Gritz, Y., and Merzlyak, M. N. (2003). Relationships between\
    \ leaf\nchlorophyll content and spectral reﬂectance and algorithms for non-destructive\n\
    chlorophyll assessment in higher plant leaves. J. Plant Physiol. 160, 271–282.\n\
    doi: 10.1078/0176-1617-00887\nGitelson, A. A., Kaufman, Y. J., Stark, R., and\
    \ Rundquist, D. (2002). Novel\nalgorithms for remote estimation of vegetation\
    \ fraction. Remote Sens. Environ.\n80, 76–87. doi: 10.1016/s0034-4257(01)00289-9\n\
    Gitelson, A. A., and Merzlyak, M. N. (1996). Signature analysis of leaf\nreﬂectance\n\
    spectra:\nalgorithm\ndevelopment\nfor\nremote\nsensing\nof\nchlorophyll. J. Plant\
    \ Physiol. 148, 494–500. doi: 10.1016/s0176-1617(96)\n80284-7\nGitelson, A. A.,\
    \ Merzlyak, M. N., and Chivkunova, O. B. (2001). Optical properties\nand nondestructive\
    \ estimation of anthocyanin content in plant leaves.\nPhotochem. Photobiol. 74,\
    \ 38–45. doi: 10.1562/0031-8655(2001)074<0038:\nopaneo>2.0.co;2\nHaboudane, D.,\
    \ Miller, J. R., Pattey, E., Zarco-Tejada, P. J., and Strachan, I. B.\n(2004).\
    \ Hyperspectral vegetation indices and novel algorithms for predicting\ngreen\
    \ LAI of crop canopies: modeling and validation in the context of precision\n\
    agriculture. Remote Sens. Environ. 90, 337–352. doi: 10.1016/j.rse.2003.12.013\n\
    Haboudane, D., Miller, J. R., Tremblay, N., Zarco-Tejada, P. J., and Dextraze,\n\
    L. (2002). Integrated narrow-band vegetation indices for prediction of crop\n\
    chlorophyll content for application to precision agriculture. Remote Sens.\nEnviron.\
    \ 81, 416–426. doi: 10.1016/s0034-4257(02)00018-4\nHariharan, J., Fuller, J.,\
    \ Ampatzidis, Y., Abdulridha, J., and Lerwill, A. (2019). Finite\ndiﬀerence analysis\
    \ and bivariate correlation of hyperspectral data for detecting\nlaurel wilt disease\
    \ and nutritional deﬁciency in avocado. Remote Sens. 11:1748.\ndoi: 10.3390/rs11151748\n\
    Hatﬁeld, J. L., Gitelson, A. A., Schepers, J. S., and Walthall, C. L. (2008).\
    \ Application\nof spectral remote sensing for agronomic decisions. Agron. J. 100,\
    \ S117–S131.\nFrontiers in Plant Science | www.frontiersin.org\n9\nMay 2022 |\
    \ Volume 13 | Article 791018\nAbdulridha et al.\nDowny Mildew Detection in Watermelon\n\
    He, X., Fang, K., Qiao, B., Zhu, X. H., and Chen, Y. N. (2021). Watermelon Disease\n\
    Detection Based on Deep Learning. Int. J. Pattern Recogn. Artiﬁcial Intelligence\n\
    35:2152004. doi: 10.1142/s0218001421520042\nImmerzeel, W. W., Gaur, A., and Zwart,\
    \ S. J. (2008). Integrating remote sensing and\na process-based hydrological model\
    \ to evaluate water use and productivity in a\nsouth Indian catchment. Agric.\
    \ Water Manag. 95, 11–24. doi: 10.1016/j.agwat.\n2007.08.006\nJordan, C. F. (1969).\
    \ Derivation of leaf area index from quality of light on the forest\nﬂoor. Ecology\
    \ 50, 663–666. doi: 10.2307/1936256\nKalischuk,\nM.,\nParet,\nM.\nL.,\nFreeman,\n\
    J.\nH.,\nRaj,\nD.,\nDa\nSilva,\nS.,\nEubanks,\nS.,\net\nal.\n(2019).\nAn\nImproved\n\
    Crop\nScouting\nTechnique\nIncorporating\nUnmanned\nAerial\nVehicle-Assisted\n\
    Multispectral\nCrop\nImaging into Conventional Scouting Practice for Gummy Stem\
    \ Blight\nin Watermelon. Plant Dis. 103, 1642–1650. doi: 10.1094/PDIS-08-18-\n\
    1373-RE\nLu, J. Z., Ehsani, R., Shi, Y. Y., de Castro, A. I., and Wang, S. (2018).\
    \ Detection\nof multi-tomato leaf diseases (late blight, target and bacterial\
    \ spots) in diﬀerent\nstages by using a spectral-based sensor. Sci. Rep. 8:2793.\
    \ doi: 10.1038/s41598-\n018-21191-6\nMandal, K., Saravanan, R., Maiti, S., and\
    \ Kothari, I. L. (2009). Eﬀect of\ndowny mildew disease on photosynthesis and\
    \ chlorophyll ﬂuorescence\nin Plantago ovata Forsk. J. Plant Dis. Prot. 116, 164–168.\
    \ doi: 10.1007/bf03356\n305\nMerton, R. (1998). “Monitoring Community Hysteresis\
    \ Using Spectral Shift\nAnalysis and the Red-Edge Vegetation Stress Index,” in\
    \ JPL Airborne Earth\nScience Workshop, (Pasadena, CA: NASA, Jet Propulsion Laboratory).\n\
    Penuelas, J., Baret, F., and Filella, I. (1995). Semiempirical indexes to assess\n\
    carotenoids chlorophyll-a ratio from leaf spectral reﬂectance. Photosynthetica\n\
    31, 221–230.\nPenuelas, J., Pinol, J., Ogaya, R., and Filella, I. (1997). Estimation\
    \ of plant water\nconcentration by the reﬂectance water index WI (R900/R970).\
    \ Int. J. Remote\nSens. 18, 2869–2875. doi: 10.1080/014311697217396\nRaun, W.\
    \ R., Solie, J. B., Johnson, G. V., Stone, M. L., Lukina, E. V., Thomason,\nW.\
    \ E., et al. (2001). In-season prediction of potential grain yield in winter\n\
    wheat using canopy reﬂectance. Agron. J. 93, 131–138. doi: 10.2134/agronj2001.\n\
    931131x\nRoujean, J. L., and Breon, F. M. (1995). Estimating Par Absorbed by Vegetation\n\
    from Bidirectional Reﬂectance Measurements. Remote Sens. Environ. 51, 375–\n384.\
    \ doi: 10.1016/0034-4257(94)00114-3\nSaif, A. K., Dimyati, K., Noordin, A. N.\
    \ S., Mohd Shah, S., Alsamhi, H., and\nAbdullah, Q. (2021). “Energy Eﬃcient Tethered\
    \ UAV Development in B5G for\nSmart Environment and Disaster Recovery,” in 1st\
    \ International conference on\nEmerging smart technology, IEEE, (Piscataway: IEEE).\n\
    Suarez, L., Zarco-Tejada, P. J., Sepulcre-Canto, G., Perez-Priego, O., Miller,\
    \ J. R.,\nJimenez-Munoz, J. C., et al. (2008). Assessing canopy PRI for water\
    \ stress\ndetection with diurnal airborne imagery. Remote Sens. Environ. 112,\
    \ 560–575.\ndoi: 10.1016/j.rse.2007.05.009\nWang, T. Y., Thomasson, J. A., Yang,\
    \ C. H., Isakeit, T., and Nichols, R. L. (2020).\nAutomatic Classiﬁcation of Cotton\
    \ Root Rot Disease Based on UAV Remote\nSensing. Remote Sens. 12:21.\nWest, J.\
    \ S., Bravo, C., Oberti, R., Lemaire, D., Moshou, D., and McCartney, H. A.\n(2003).\
    \ The potential of optical canopy measurement for targeted control of\nﬁeld crop\
    \ diseases. Annu. Rev. Phytopathol. 41, 593–614. doi: 10.1146/annurev.\nphyto.41.121702.103726\n\
    Woodcock, C. E., Loveland, T. R., Herold, M., and Bauer, M. E. (2020).\nTransitioning\
    \ from change detection to monitoring with remote sensing: A\nparadigm shift.\
    \ Remote Sens. Environ. 238:111558. doi: 10.1016/j.rse.2019.\n111558\nXiao, Y.\
    \ F., Zhao, W. J., Zhou, D. M., and Gong, H. L. (2014). Sensitivity Analysis\n\
    of Vegetation Reﬂectance to Biochemical and Biophysical Variables at Leaf,\nCanopy,\
    \ and Regional Scales. IEEE Trans. Geosci. Remote Sens. 52, 4014–4024.\ndoi: 10.1109/tgrs.2013.2278838\n\
    Ye, H. C., Huang, W. J., Huang, S. Y., Cui, B., Dong, Y. Y., Guo, A. T., et al.\
    \ (2020).\nRecognition of Banana Fusarium Wilt Based on UAV Remote Sensing. Remote\n\
    Sens. 12:938. doi: 10.3390/rs12060938\nAuthor Disclaimer: The contents are solely\
    \ the responsibility of the authors and\ndo not necessarily represent the oﬃcial\
    \ views of the USDA.\nConﬂict of Interest: The authors declare that the research\
    \ was conducted in the\nabsence of any commercial or ﬁnancial relationships that\
    \ could be construed as a\npotential conﬂict of interest.\nPublisher’s Note: All\
    \ claims expressed in this article are solely those of the authors\nand do not\
    \ necessarily represent those of their aﬃliated organizations, or those of\nthe\
    \ publisher, the editors and the reviewers. Any product that may be evaluated\
    \ in\nthis article, or claim that may be made by its manufacturer, is not guaranteed\
    \ or\nendorsed by the publisher.\nCopyright © 2022 Abdulridha, Ampatzidis, Qureshi\
    \ and Roberts. This is an open-\naccess article distributed under the terms of\
    \ the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\
    \ reproduction in other forums is permitted,\nprovided the original author(s)\
    \ and the copyright owner(s) are credited and that the\noriginal publication in\
    \ this journal is cited, in accordance with accepted academic\npractice. No use,\
    \ distribution or reproduction is permitted which does not comply\nwith these\
    \ terms.\nFrontiers in Plant Science | www.frontiersin.org\n10\nMay 2022 | Volume\
    \ 13 | Article 791018\n"
  inline_citation: '>'
  journal: Frontiers in Plant Science
  limitations: '>'
  pdf_link: https://www.frontiersin.org/articles/10.3389/fpls.2022.791018/pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Identification and Classification of Downy Mildew Severity Stages in Watermelon
    Utilizing Aerial and Ground Remote Sensing and Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/app6120412
  analysis: '>'
  authors:
  - Miguel Ángel Lara
  - Belén Diezma
  - Lourdes Lleó
  - Jean‐Michel Roger
  - Yolanda Garrido
  - María I. Gil
  - Margarita Ruiz-Altisent
  citation_count: 17
  full_citation: '>'
  full_text: ">\napplied  \nsciences\nArticle\nHyperspectral Imaging to Evaluate the\
    \ Effect of\nIrrigation Water Salinity in Lettuce\nMiguel Ángel Lara 1,*, Belén\
    \ Diezma 1, Lourdes Lleó 1, Jean Michel Roger 2, Yolanda Garrido 3,\nMaría Isabel\
    \ Gil 3 and Margarita Ruiz-Altisent 1\n1\nLPF-TAGRALIA, Departamento de Ingeniería\
    \ Agroforestal, E.T.S.I. Agronómica, Alimentaria y de\nBiosistemas, Universidad\
    \ Politécnica de Madrid, CEI-Moncloa. Avda. Complutense s/n, Madrid 28040,\nSpain;\
    \ belen.diezma@upm.es (B.D.); lourdes.lleo@upm.es (L.L.); margarita.ruiz.altisent@upm.es\
    \ (M.R.-A.)\n2\nIRSTEA, 361 Rue Jean-François Breton BP 5095, Montpellier 34196,\
    \ France; jean-michel.roger@irstea.fr\n3\nCEBAS-CSIC, Research Group on Quality,\
    \ Safety and Bioactivity of Plant Foods,\nDepartment of Food Science and Technology,\
    \ P.O. Box 164, Espinardo 30100, Spain;\nygarrido@cebas.csic.es (Y.G.); migil@cebas.csic.es\
    \ (M.I.G.)\n*\nCorrespondence: miguelangel.larablas@gmail.com; Tel.: +34-650-268-849\n\
    Academic Editor: Kuanglin Kevin Chao\nReceived: 23 October 2016; Accepted: 30\
    \ November 2016; Published: 7 December 2016\nAbstract: Salinity is one of the\
    \ most important stress factors in crop production, particularly in\narid regions.\
    \ This research focuses on the effect of salinity on the growth of lettuce plants;\
    \ three\nsolutions with different levels of salinity were considered and compared\
    \ (S1 = 50, S2 = 100 and\nS3 = 150 mM NaCl) with a control solution (Ct = 0 mM\
    \ NaCl). The osmotic potential and water\ncontent of the leaves were measured,\
    \ and hyperspectral images of the surfaces of 40 leaves (10 leaves\nper treatment)\
    \ were taken after two weeks of growth. The mean spectra of the leaves (n = 32,000)\
    \ were\npre-processed by means of a Savitzky–Golay algorithm and standard normal\
    \ variate normalization.\nPrincipal component analysis was then performed on a\
    \ calibration set of 28 mean spectra, yielding an\ninitial model for salinity\
    \ effect detection. A second model was subsequently proposed based on an\nindex\
    \ computing an approximation to the second derivative at the red edge region.\
    \ Both models were\napplied to all the hyperspectral images to obtain the corresponding\
    \ artiﬁcial images, distinguishing\nbetween the 28 that were used to extract the\
    \ calibration mean spectra and the rest that constituted an\nexternal validation.\
    \ Those virtual images were studied using analysis of variance in order to compare\n\
    their ability for detecting salinity effects on the leaves. Both models showed\
    \ signiﬁcant differences\nbetween each salinity level, and the hyperspectral images\
    \ allowed observations of the distribution of\nthe salinity effects on the leaf\
    \ surfaces, which were more intense in the areas distant from the veins.\nHowever,\
    \ the index-based model is simpler and easier to apply because it is based solely\
    \ on the\nreﬂectance at three different wavelengths, thus allowing for the implementation\
    \ of less expensive\nmultispectral devices.\nKeywords: hyperspectral imaging;\
    \ lettuce; salinity; non-destructive assessment; salinity index\n1. Introduction\n\
    Soil and irrigation salinity are key factors in the growth of most vegetables\
    \ and can cause many\nproblems in agriculture, particularly in arid and semi-arid\
    \ regions [1,2]. Industrial development and\npopulation growth have increased\
    \ the contamination and salinization of surface and underground\nwater and thus\
    \ of agricultural soils. Saline soils cover a signiﬁcant area in the southeast\
    \ of Spain,\na major region of horticultural crops, and in many other areas in\
    \ the world. Lettuce (Lactuca sativa L.)\nis one of the most important crops in\
    \ this region, the major producer area in the European Union. This\nproduction\
    \ is primarily employed for the fresh consumption of a ready-to-eat product. Acosta\
    \ et al. [3]\nAppl. Sci. 2016, 6, 412; doi:10.3390/app6120412\nwww.mdpi.com/journal/applsci\n\
    Appl. Sci. 2016, 6, 412\n2 of 18\nreported some peaks value of 6.4 decisiemens\
    \ per meter (dS/m) electrical conductivity (EC), which\ncorresponded with 50 mM\
    \ NaCl. This high electrical conductivity observed in the soils in the\nsoutheastern\
    \ area of Spain is mainly the result of poor quality irrigation water used in\
    \ agriculture.\nLettuce is moderately sensitive to salinity. When the EC is greater\
    \ than 1.3 dS/m (nearly 10 mM\nof NaCl), growth is affected, and its yield decreases\
    \ by 13% for each unit of EC above that level [4].\nSalinity primarily affects\
    \ crop growth in two ways: (a) by increasing the osmotic potential of the\nsoil,\
    \ making water less available to plants; and (b) by creating excessive concentrations\
    \ of speciﬁc\nelements in the plant [5]. Herbaceous crops affected by salinity\
    \ at moderate levels do not show visible\ndamage in their leaves and appear to\
    \ be normal, although their growth and yield decrease. They may\nhave deep green\
    \ and more succulent leaves with more density and thickness [6]. However, when\n\
    excess salinity induces imbalances in concentrations of certain mineral elements,\
    \ such as Cl, Na,\nB or Ca, necrosis, chlorosis and tip burn can appear in the\
    \ leaves [7–9]. Marginal necrosis is a typical\nfoliar symptom in plants suffering\
    \ from salinity stress [10,11]. In lettuce, tip burn is a physiological\ndisorder\
    \ displayed as necrosis in the margins of young developing leaves and is commonly\
    \ observed\nunder saline conditions [9,12].\nHyperspectral imaging has been widely\
    \ employed to detect salinity levels in soils and\ncanopies through remote sensing,\
    \ with the development of numerous indexes for estimating saline\nconcentrations\
    \ in relation to reﬂectance at different wavelengths [13,14], giving information\
    \ about\nspatial distribution with spatial resolutions higher than meters. This\
    \ remote sensing technique has been\napplied to many different crops and plants,\
    \ including sugarcane [14], cotton, corn, cogon grass, reeds,\nsaltcedar, suaeda,\
    \ and aeluropus [15]. In addition, there are numerous remote sensing and at laboratory\n\
    level studies that have observed the effects of water stress [16–19] and nutrient\
    \ deﬁciencies [20–22].\nHowever, there are not many studies at laboratory level\
    \ that have been devoted to study changes\nin the optical behavior of leaf tissues\
    \ due to alterations in their structure and/or composition when\nsaline concentration\
    \ increases. This work applies proximal hyperspectral imaging techniques as a\n\
    non-destructive procedure for early identiﬁcation of the inﬂuence of saline stress\
    \ on one by one, newly\nharvested “baby” lettuces, exploring simultaneously the\
    \ presence of any distribution of the effects on\nthe surface of the leaves. The\
    \ current work constitutes a proof of concept to the development of a rapid,\n\
    non-destructive and cheap procedure for the analysis of the symptoms of saline\
    \ stress in the crops.\nThis procedure could be of application in ﬁelds such as\
    \ precision agriculture, robotic monitoring or\nphysiological and plant breeding\
    \ research instrumentation.\nThe ﬁrst part of this paper explains the materials\
    \ and methods employed to obtain the\nhyperspectral images and other analytical\
    \ measurements; it also outlines the data and processed\nhyperspectral images\
    \ used to obtain and evaluate the models and to identify the effects of different\n\
    saline concentrations on the leaves. The second part of the paper describes the\
    \ obtained results and\nthe applied models while validating the different leaf\
    \ samples with an extensive discussion.\n2. Materials and Methods\n2.1. Materials\
    \ and Analytical Measurements\nThe lettuce plants (Lactuca sativa L. var capitata\
    \ variety “Tempo neutro”) were grown from seeds.\nThe seeds were inserted into\
    \ polystyrene cylinders that were positioned through holes in the plastic lid\n\
    of containers with Hoagland´s nutrient solution inside, thus enabling the roots\
    \ to be in contact with the\nnutrient solution. The composition of the nutrient\
    \ solution was 7 mM K+, 4 mM Ca2+, 14 mM NO3−,\n1 mM Mg2+, 1 mM PO42−, 1 mM SO42−,\
    \ 20 µM Fe2+, 2.5 µM B3+, 2 µM Mn2+, 2 µM Zn2+, 0.5 µM Cu2+\nand 0.5 µM Mo6+;\
    \ it was maintained between pH levels of 5.5 and 6.5 through routine replacement\
    \ of\nthe hydroponic solution. After 14 days, plants with similar development\
    \ were selected and placed\nin containers (ﬁve plants per container) using 15\
    \ plants per treatment. Four salt treatments were\nthen applied—a control treatment\
    \ without NaCl (Ct) and three levels of salt through the addition of\ndifferent\
    \ concentrations of NaCl to the nutrient solution: S1 (50 mM NaCl), S2 (100 mM\
    \ NaCl) and\nAppl. Sci. 2016, 6, 412\n3 of 18\nS3 (150 mM NaCl). The application\
    \ of salt treatments was conducted for a total of 10 days. The average\nEC of\
    \ the nutrient solution during the experiments were 1.23 dS/m, 6.09 dS/m, 10.77\
    \ dS/m and\n14.95 dS/m, for Ct, S1, S2 and S3, respectively. The experiments were\
    \ performed in a controlled growth\nchamber with a temperature (T) of 23 ◦C/18\
    \ ◦C (day/night), a photoperiod of 16 h/8 h (light/darkness)\nand a photosynthetically\
    \ active radiation of 400 µmol·m2/s. The leaves were harvested at the end\nof\
    \ the experiments and a sample of 10 leaves per treatment, with similar size,\
    \ was selected for the\nacquisition of hyperspectral images. Figure 1 shows, as\
    \ an example, Red-Green-Blue (RGB) images of\none leaf per treatment.\nAppl. Sci. 2016, 6, 412 \n\
    3 of 19 \ntotal of 10 days. The average EC of the nutrient solution during the experiments were 1.23 dS/m, 6.09 \n\
    dS/m,  10.77  dS/m  and  14.95  dS/m,  for  Ct,  S1,  S2  and  S3,  respectively. \
    \ The  experiments  were \nperformed  in  a  controlled  growth  chamber  with \
    \ a  temperature  (T)  of  23  °C/18  °C  (day/night),  a \nphotoperiod of 16 h/8 h (light/darkness) and a photosynthetically active radiation of 400 μmol∙m2/s. \n\
    The leaves were harvested at the end of the experiments and a sample of 10 leaves per treatment, \n\
    with similar size, was selected for the acquisition of hyperspectral images. Figure 1 shows, as an \n\
    example, Red‐Green‐Blue (RGB) images of one leaf per treatment.  \n \nFigure \
    \ 1.  Red‐Green‐Blue  (RGB)  image  of  one  leaf  per  saline  treatment.  Apparently, \
    \ no  visible \nsymptoms of stress can be observed. \nAfter  image  acquisition, \
    \ the  water  content  and  osmotic  potential  were  determined  for  the \n\
    harvested leaves. The leaf water contents were estimated following the method described by [23] \n\
    with  modifications.  Twenty  grams  of  leaf  pieces  per  treatment  were  taken, \
    \ homogenized  with  a \ncommercial grinder, and divided into three samples of five grams per treatment. The samples were \n\
    dried  in  an  oven  (Thermocenter  T  C40/100,  Salvis  Lab,  Rotkreuz,  Switzerland), \
    \ at  65  °C  until \nachieving a constant weight. The water content (WC) was calculated as: \n\
    Fresh weight\nDry weight\n% \n100\nFresh weight\nWC\n-\n=\n´\n \n(1) \nFor the calculation of the osmotic potential (Ψs), three 5‐g samples of leaf pieces per treatment \n\
    were collected and frozen at −20 °C. The samples were centrifuged at 2800× g (9.8 m/s2) for 15 min \n\
    (Centronic  centrifuge,  J.P.  Selecta,  Barcelona,  Spain).  The  supernatant \
    \ was  analysed  with  a \nmicro‐osmometer (Roebling 13DR, Löser Messtechnik, Berlin, Germany) to determine osmolarity. \n\
    The osmotic potential was calculated with the Van’t Hoff equation [24]: \nΨs = −R∙x∙T∙x∙cs \n\
    (2) \nwhere R is the ideal gas constant (m3∙Pa∙mol−1∙K−1), T is the temperature (K), and cs is the osmolarity \n\
    (Osm∙m−3). \n2.2. Hyperspectral Images \nHyperspectral  images  from  the  adaxial \
    \ surfaces  of  40  selected  leaves  were  taken  with  a \nhyperspectral vision system, consisting of an EMCCD Luca‐R camera (Andor™ Technology, Belfast, \n\
    Northern  Ireland)  coupled  to  a  VIS‐NIR  spectrometer  (Headwall  Photonics \
    \ Hyperspec‐VNIR™, \nFigure 1. Red-Green-Blue (RGB) image of one leaf per saline\
    \ treatment. Apparently, no visible\nsymptoms of stress can be observed.\nAfter\
    \ image acquisition, the water content and osmotic potential were determined for\
    \ the\nharvested leaves. The leaf water contents were estimated following the\
    \ method described by [23] with\nmodiﬁcations. Twenty grams of leaf pieces per\
    \ treatment were taken, homogenized with a commercial\ngrinder, and divided into\
    \ three samples of ﬁve grams per treatment. The samples were dried in an\noven\
    \ (Thermocenter T C40/100, Salvis Lab, Rotkreuz, Switzerland), at 65 ◦C until\
    \ achieving a constant\nweight. The water content (WC) was calculated as:\n% WC\
    \ = Fresh weight − Dry weight\nFresh weight\n× 100\n(1)\nFor the calculation of\
    \ the osmotic potential (Ψs), three 5-g samples of leaf pieces per treatment\n\
    were collected and frozen at −20 ◦C. The samples were centrifuged at 2800× g (9.8\
    \ m/s2) for\n15 min (Centronic centrifuge, J.P. Selecta, Barcelona, Spain). The\
    \ supernatant was analysed with\na micro-osmometer (Roebling 13DR, Löser Messtechnik,\
    \ Berlin, Germany) to determine osmolarity.\nThe osmotic potential was calculated\
    \ with the Van’t Hoff equation [24]:\nΨs = −R·x·T·x·cs\n(2)\nwhere R is the ideal\
    \ gas constant (m3·Pa·mol−1·K−1), T is the temperature (K), and cs is the osmolarity\n\
    (Osm·m−3).\n2.2. Hyperspectral Images\nHyperspectral images from the adaxial surfaces\
    \ of 40 selected leaves were taken with\na hyperspectral vision system, consisting\
    \ of an EMCCD Luca-R camera (Andor™ Technology, Belfast,\nAppl. Sci. 2016, 6,\
    \ 412\n4 of 18\nNorthern Ireland) coupled to a VIS-NIR spectrometer (Headwall\
    \ Photonics Hyperspec-VNIR™,\nFitchburg, MA, USA) working in the range of 400–1000\
    \ nm. A total of 189 wavelengths were considered\nalong this range, thus obtaining\
    \ a spectral resolution of 3.2 nm. The spectrometer of the camera was\nequipped\
    \ with a progressive line-by-line scan spectrograph with a slit of 25 µm. The\
    \ spatial resolutions\nin the line direction and in the scan direction were 0.26\
    \ mm/pixel and 0.1 mm/pixel, respectively.\nA halogen lamp was used for illumination.\
    \ Headwall Hyperspec™ software (Hyperspec III, Headwall\nPhotonicsTM, Fitchburg,\
    \ MA, USA) was used to control the equipment. Samples were scanned by\nacquiring\
    \ the entire surface of the leaf (scan length = 100 mm) creating a hypercube dataset.\
    \ Relative\nreﬂectance hyperspectral images were simultaneously computed by the\
    \ software of the camera. White\nreference (barium sulfate) and dark current signal\
    \ (acquired with the objective of the camera covered\nby a black tap) were acquired\
    \ before each batch of images. Each line of the image was then corrected\npixel\
    \ by pixel by subtracting the dark current and dividing the result by the white\
    \ reference minus the\ndark current.\nData Preprocessing and Processing. Computation\
    \ of Models\nThe 40 hyperspectral images of lettuce leaves were randomly divided\
    \ in two different groups:\na calibration set with seven leaves per treatment\
    \ (28 leaves in total) for generating the models, and\na validation set with the\
    \ three remaining leaves of each treatment (12 leaves in total) for testing the\n\
    models. Figure 2 shows schematically the procedures carries out for the computation\
    \ and validation of\nthe models developed.\nAppl. Sci. 2016, 6, 412 \n4 of 19 \n\
    Fitchburg,  MA,  USA)  working  in  the  range  of  400–1000  nm.  A  total  of \
    \ 189  wavelengths  were \nconsidered along this range, thus obtaining a spectral resolution of 3.2 nm. The spectrometer of the \n\
    camera was equipped with a progressive line‐by‐line scan spectrograph with a slit of 25 μm. The \n\
    spatial  resolutions  in  the  line  direction  and  in  the  scan  direction \
    \ were  0.26  mm/pixel  and  0.1 \nmm/pixel, respectively. A halogen lamp was used for illumination. Headwall Hyperspec™ software \n\
    (Hyperspec III, Headwall PhotonicsTM, Fitchburg, MA, USA) was used to control the equipment. \n\
    Samples were scanned by acquiring the entire surface of the leaf (scan length = 100 mm) creating a \n\
    hypercube dataset. Relative reflectance hyperspectral images were simultaneously computed by the \n\
    software of the camera. White reference (barium sulfate) and dark current signal (acquired with the \n\
    objective of the camera covered by a black tap) were acquired before each batch of images. Each line \n\
    of the image was then corrected pixel by pixel by subtracting the dark current and dividing the \n\
    result by the white reference minus the dark current. \nData Preprocessing and Processing. Computation of Models \n\
    The 40 hyperspectral images of lettuce leaves were randomly divided in two different groups: a \n\
    calibration set with seven leaves per treatment (28 leaves in total) for generating the models, and a \n\
    validation set with the three remaining leaves of each treatment (12 leaves in total) for testing the \n\
    models. Figure 2 shows schematically the procedures carries out for the computation and validation \n\
    of the models developed. \n \nFigure 2. Scheme of the methodology followed for the computation and validation of both models \n\
    developed. \nFor each hyperspectral image, the pixels from the green surface of the leaves, eliminating the \n\
    mid‐rib pixels, were considered. The corresponding spectra (a minimum of 32,000 in the smaller \n\
    leaf) were used to calculate the mean spectrum of each leaf.  \nThe differences on the spectral signature of the spectra was faced analyzing the raw reflectance \n\
    spectra, the normalized spectra by Standard Normal Variate scaling (SNV) and their corresponding \n\
    first derivatives, which allow to identify better the displacement left‐right of certain bands of the \n\
    spectrum,  such  as  the  red  edge  area  [25,26].  The  first  derivative  of \
    \ the  leaves  mean  spectra  was \ncomputed by means of the Savitzky–Golay (SG) algorithm [27]. In the present work, a polynomial of \n\
    order three was fitted to each spectrum considering 21 wavelengths width to smooth spectra, and \n\
    then the first derivative function was applied.  \nFor  the  calibration  set, \
    \ containing  the  28  mean  spectra,  two  different  preprocessing  were \n\
    applied in order to remove the additive and the multiplicative effect such as it has been previously \n\
    reported on spinach leaves by [28]. Savitzky–Golay smoothing and differentiation algorithm (SG) \n\
    Figure 2.\nScheme of the methodology followed for the computation and validation\
    \ of both\nmodels developed.\nFor each hyperspectral image, the pixels from the\
    \ green surface of the leaves, eliminating the\nmid-rib pixels, were considered.\
    \ The corresponding spectra (a minimum of 32,000 in the smaller leaf)\nwere used\
    \ to calculate the mean spectrum of each leaf.\nThe differences on the spectral\
    \ signature of the spectra was faced analyzing the raw reﬂectance\nspectra, the\
    \ normalized spectra by Standard Normal Variate scaling (SNV) and their corresponding\
    \ ﬁrst\nderivatives, which allow to identify better the displacement left-right\
    \ of certain bands of the spectrum,\nsuch as the red edge area [25,26]. The ﬁrst\
    \ derivative of the leaves mean spectra was computed by\nmeans of the Savitzky–Golay\
    \ (SG) algorithm [27]. In the present work, a polynomial of order three\nwas ﬁtted\
    \ to each spectrum considering 21 wavelengths width to smooth spectra, and then\
    \ the ﬁrst\nderivative function was applied.\nAppl. Sci. 2016, 6, 412\n5 of 18\n\
    For the calibration set, containing the 28 mean spectra, two different preprocessing\
    \ were applied\nin order to remove the additive and the multiplicative effect\
    \ such as it has been previously reported on\nspinach leaves by [28]. Savitzky–Golay\
    \ smoothing and differentiation algorithm (SG) was applied to\nthe spectra: a\
    \ polynomial of order three was ﬁtted considering 21 wavelengths width and the\
    \ second\nderivative function was applied to the smoothed spectra (D2). After\
    \ SG algorithm, the multiplicative\neffect was corrected by SNV scaling [29].\
    \ SNV subtracts to each wavelength of the spectrum (λi) the\nmean value of this\
    \ spectrum (λm), and it divides this result by the standard deviation of this\
    \ spectrum\n(STD). Therefore resulting spectra have a standard deviation equal\
    \ to one and a mean equal to zero.\nFor the set of 28 preprocessed (SG D2 SNV)\
    \ spectra, a principal component analysis (PCA) was\nperformed in order to obtain\
    \ an initial model. The loading(s) of the principal component(s) (PC)\nthat showed\
    \ to be the most sensitive to the effect of the salinity was retained such as\
    \ the vector of\nprojection (or model) that was applied to the totality of hyperspectral\
    \ images: those used to compute\nthe average spectra (n = 28 hyperspectral images)\
    \ of the calibration set, and those that have not been\nimplied at all in the\
    \ deﬁnition of the model and that allow to carry out its external validation (n\
    \ = 12\nhyperspectral images). The hyperspectral images (SG D2 SNV preprocessed)\
    \ were projected onto the\nchosen principal component(s) producing the corresponding\
    \ artiﬁcial images of scores.\nAs a result of these analyses and taken into account\
    \ the pattern of the loadings obtained in the\nPCA, an index that computes an\
    \ approximation to the second derivative at the red edge region was\nproposed.\
    \ It was named Level Salinity Index (LSI). This index was also applied to all\
    \ the hyperspectral\nimages of the leaves (n = 40 hyperspectral images), thus\
    \ obtaining the corresponding virtual images of\nthe index.\nIn order to evaluate\
    \ the ability of the proposed index in comparison to other previously applied,\n\
    a collection of indexes described in the literature were computed on a representative\
    \ group of spectra\nof the calibration set. Only 7000 spectra per treatment (28,000\
    \ spectra in total) were considered in order\nto evaluate the ability of the indexes\
    \ in a worst scenario, in systems with poorer spatial resolution and\nconsequently\
    \ lower number of pixels per image. Some of them are speciﬁcally related to salinity\
    \ and\nothers related to water, pigments and chlorophyll content under stress\
    \ conditions. One-way Analysis\nof Variance (ANOVA) and Tukey’s least signiﬁcant\
    \ difference procedures were carried out for each\nindex comparing the salinity\
    \ treatments. In addition, scores resulting from PCA of preprocessed\nspectra\
    \ was included in such comparison.\nSimilarly to the procedure followed in the\
    \ calibration set, in the validation set of virtual images\nof scores of PCA and\
    \ of the index proposed in this work, 3000 pixels per treatment were randomly\n\
    selected and one-way ANOVA’s and Tukey’s least signiﬁcant difference procedures\
    \ were applied.\nIn addition to ANOVA’s, the virtual images (PCA scores and LSI)\
    \ of the calibration and validation\nset were also used for comparison, evaluating\
    \ visually the differences between salinity classes and the\nspatial distribution\
    \ of the effects of the salinity on the leaves.\nIn order to improve the visualization\
    \ of these virtual images, the pixels of the nerves were not\nconsidered. For\
    \ that a segmentation procedure was applied to the images. Firstly, artiﬁcial\
    \ images\nwere computed based on the combination of planes 730, 680 and 550 nm,\
    \ where the main differences\nbetween nerve and green area were observed. Then\
    \ the Otsu method was applied to these images to\nobtain the mask that separated\
    \ the green areas and the nerves. Finally, these masks were applied onto\nthe\
    \ scores and index images.\n3. Results and Discussion\n3.1. Effect of Salinity\
    \ on Water Content and Osmotic Potential\nThe water content and osmotic potential\
    \ data are shown in Table 1. The water content of the\nleaves decreased when saline\
    \ concentration increases from 50 mM NaCl to 150 mM NaCl, while similar\nvalues\
    \ were obtained in the control and in the samples subjected to 50 mM NaCl. The\
    \ same trend\ncan be observed with the osmotic potential of the leaves, but in\
    \ this parameter distinguishing also\nAppl. Sci. 2016, 6, 412\n6 of 18\nbetween\
    \ control and 50 mM NaCl, being signiﬁcantly more negative than control. Salinity\
    \ increase can\ninduce the accumulation of ions, mainly Na+ and Cl−, in the leaves.\
    \ This fact would be in accordance\nwith the results shown in Table 1, since an\
    \ accumulation of ions implies a water content reduction and\na more negative\
    \ osmotic potential in the leaves. These results conﬁrm that the salinity levels\
    \ studied\nare actually having an effect on the leaves.\nTable 1. Water content\
    \ and osmotic potential of the lettuce leaves treated without NaCl (Ct) and treated\n\
    with 50, 100 and 150 mM NaCl (S1, S2 and S3, respectively). Letters indicate the\
    \ different mean groups\naccording to Tukey’s test for a p level of 0.05.\nSaline\
    \ Treatment\nWater Content (%)\nOsmotic Potential (MPa)\nMean ± STD\nMean ± STD\n\
    Ct\n94.53 ± 0.09a\n−0.65 ± 0.04a\nS1\n94.54 ± 0.12a\n−1.08 ± 0.09b\nS2\n93.15\
    \ ± 0.18b\n−1.49 ± 0.11c\nS3\n92.70 ± 0.07c\n−2.06 ± 0.09d\n3.2. Effect of Salinity\
    \ on Spectral Features\n3.2.1. Non Preprocessed and SNV Reﬂectance Spectra\nThe\
    \ mean spectrum of each saline treatment was computed (calibration set, n = 7\
    \ leaves\nper treatment). The raw spectra and the preprocessed SNV spectra were\
    \ considered for comparison.\nFigure 3A shows the mean spectra of each saline\
    \ treatment. It can be seen that the Ct global reﬂectance\nis lower than the others,\
    \ primarily in the infrared range. This fact can be related to certain physical\n\
    properties inducing lower scattering in the leaves of the Ct leaf samples.\nAppl. Sci. 2016, 6, 412 \n\
    6 of 19 \nreduction and a more negative osmotic potential in the leaves. These results confirm that the salinity \n\
    levels studied are actually having an effect on the leaves. \nTable 1. Water content and osmotic potential of the lettuce leaves treated without NaCl (Ct) and \n\
    treated with 50, 100 and 150 mM NaCl (S1, S2 and S3, respectively). Letters indicate the different \n\
    mean groups according to Tukey’s test for a p level of 0.05. \nSaline Treatment \n\
    Water Content (%)\nOsmotic Potential (MPa) \nMean ± STD \nMean ± STD \nCt \n94.53 ± 0.09a \n\
    −0.65 ± 0.04a \nS1 \n94.54 ± 0.12a \n−1.08 ± 0.09b \nS2 \n93.15 ± 0.18b \n−1.49 ± 0.11c \n\
    S3 \n92.70 ± 0.07c \n−2.06 ± 0.09d \n3.2. Effect of Salinity on Spectral Features \n\
    3.2.1. Non Preprocessed and SNV Reflectance Spectra \nThe mean spectrum of each saline treatment was computed (calibration set, n = 7 leaves per \n\
    treatment).  The  raw spectra and  the  preprocessed SNV spectra were  considered for \
    \ comparison. \nFigure  3A  shows  the  mean  spectra  of  each  saline  treatment. \
    \ It  can  be  seen  that  the  Ct  global \nreflectance is lower than the others, primarily in the infrared range. This fact can be related to certain \n\
    physical properties inducing lower scattering in the leaves of the Ct leaf samples.  \n\
    400\n500\n600\n700\n800\n900\n1000\n1100\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\
    Wavelength (nm)\nRelative reflectance\n \n \nCt\nS1\nS2\nS3\n \n(a)\n400\n500\n\
    600\n700\n800\n900\n1000\n1100\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nWavelength (nm)\n\
    S N V  c o r r e c t e d  s p e c t r a  v a lu e s\n \n \nCt\nS1\nS2\nS3\n480\n\
    500\n520\n540\n560\n580\n600\n620\n640\n660\n680\n-1.1\n-1\n-0.9\n-0.8\n-0.7\n\
    -0.6\n-0.5\nWavelength (nm)\nS N V  c o r r e c t e d  s p e c t r a  v a lu e\
    \ s\n \n \nCt\nS1\nS2\nS3\n(b) \n(c)\nFigure 3. (a) Mean relative reflectance spectra of the leaves of each saline treatment. Calibration set, n \n\
    = 28 leaves (4 saline treatments × 7 leaves). (b) Same spectra corrected with Standard Normal Variate \n\
    (SNV) normalization to eliminate multiplicative effect. (c) Zoom in the visible range of SNV corrected \n\
    spectra. See Table 1 for abbreviations. \nFigure 3. (a) Mean relative reﬂectance\
    \ spectra of the leaves of each saline treatment. Calibration set,\nn = 28 leaves\
    \ (4 saline treatments × 7 leaves). (b) Same spectra corrected with Standard Normal\
    \ Variate\n(SNV) normalization to eliminate multiplicative effect. (c) Zoom in\
    \ the visible range of SNV corrected\nspectra. See Table 1 for abbreviations.\n\
    Appl. Sci. 2016, 6, 412\n7 of 18\nCrops often have a higher NIR reﬂectance level\
    \ under water stress conditions [16–18] due to air\nreplacing water between the\
    \ cells and producing optical discontinuities and scattering. Blum [30]\nnoted\
    \ that crops under salt stress conditions typically show the same symptoms as\
    \ under water stress\nconditions. Salinity affects the osmotic potential of the\
    \ plants and the water availability of the soil;\nas such, the water absorption\
    \ capacity is affected [5]. As shown in Table 1, the osmotic potential and\nwater\
    \ content in the lettuce leaves decreases when the salinity increases, mainly\
    \ in S2 and S3 treatments.\nThis fact could induce water stress in the plants\
    \ that could justify the higher NIR reﬂectance observed\nin the leaves of those\
    \ treatments.\nOther effects observed in the crops under saline stress include\
    \ the loss of growth and turgor,\nas well as the destruction of cellular membranes\
    \ [31]. Moreover, an accumulation of salt in the leaves\ncan alter the refractive\
    \ index of the water. Gitelson et al. [32], the increase in NIR reﬂectance could\
    \ be\ncaused by an increase in the thickness and density of the leaves. This fact\
    \ has been observed in leaves\nunder moderate salinity levels [6]. All of these\
    \ described effects can affect the scattering process, and\nthis scattering can\
    \ induce a multiplicative effect on the spectra in the NIR region. This multiplicative\n\
    effect has been removed applying SNV.\nFigure 3B shows the mean spectra of the\
    \ leaves in each saline treatment corrected with SNV\nnormalization in order to\
    \ remove the multiplicative effect. Differences among each spectrum can be\nobserved\
    \ in two areas (Figure 3C): (1) the peak near 550 nm and (2) the valleys near\
    \ 480 and 680 nm.\nThe visible region of the spectra depends on the absorbance\
    \ of the different pigments of the leaves,\nmainly chlorophyll but also carotenoids\
    \ and ﬂavonoids. The reﬂectance peak near 550 nm is due to the\nsigniﬁcant absorption\
    \ of chlorophyll in the blue and red regions and the reﬂection in the green region.\n\
    The height of the peak of each spectrum can be compared by computing the difference\
    \ between the\nreﬂectance values at 550 and 510 nm. A progressive and systematic\
    \ decrease in the height of the peak\nwas observed from the Ct to the S3 treatments\
    \ as the salinity increased. Many authors have related the\nchanges in the visible\
    \ region of the spectra to the decrease of pigment content in the leaves, primarily\n\
    of chlorophyll [20,32,33]. When the chlorophyll content decreases, there is an\
    \ increase of reﬂectance\nmainly in the blue and red regions of the spectra. This\
    \ effect causes the spectra to ﬂatten, resulting in\na decrease in the height\
    \ of the peak near 550 nm. Some research indicates that the total chlorophyll\n\
    and carotenoid content decreases in plant leaves under salt stress [34]. Gitelson\
    \ et al. [35] introduced\na carotenoid reﬂectance index (CRI550) (Equation (3)),\
    \ based on the reﬂectance values at 510 (R510) and\n550 (R550) nm, to follow the\
    \ evolution of the carotenoid content in the leaves, and that decreases with\n\
    the decreasing of carotenoids.\nCRI550 = (1/R510) − (1/R550)\n(3)\nApplying the\
    \ CRI550 index to our spectra, a decrease in the carotenoid content could be observed\n\
    as the salinity increased. The values of CRI were 0.78, 0.74, 0.65 and 0.55, respectively,\
    \ for our salinity\ntreatments from Ct to S3.\nChlorophyll absorption bands at\
    \ 480 and 680 nm bands, suffered changes in the reﬂectance level\nshowing a progressive\
    \ increase with increasing salt concentrations (Figure 3C). This could indicate\n\
    a slight loss of chlorophyll in the leaves when the salinity increased.\nDifferent\
    \ views have been reported regarding the observed changes in the visible range\
    \ with\nincreasing salinity levels. Some studies related to vegetative growth\
    \ under salinity conditions showed\na decrease in the peak at 550 nm and in the\
    \ visible region with increased salinity in the plants or\nsoil [14,15], which\
    \ is consistent with our ﬁndings. However, other studies showed either an increase\
    \ in\nreﬂectance in the visible region, primarily near 550 nm [26,36], or no differences\
    \ in the visible range\nunder salt stress conditions [37].\n3.2.2. First Derivative\
    \ of the Spectra\nFigure 4A shows the ﬁrst derivative mean spectra of each saline\
    \ treatment, where the maximum\nvalue corresponds with the red edge. A magniﬁcation\
    \ of the red edge region can be seen in Figure 4B.\nAppl. Sci. 2016, 6, 412\n\
    8 of 18\nAppl. Sci. 2016, 6, 412 \n8 of 19 \n400\n500\n600\n700\n800\n900\n1000\n\
    -0.02\n0\n0.02\n0.04\n0.06\nWavelength (nm)\nF ir s t  d e r iv a t iv e  v a\
    \ lu e\n \n \nCt\nS1\nS2\nS3\n670\n680\n690\n700\n710\n720\n730\n740\n750\n0.06\n\
    0.062\n0.064\n0.066\n0.068\n0.07\nWavelength (nm)\nF ir s t  d e r iv a t iv e\
    \  v a lu e\n \n \nCt\nS1\nS2\nS3\n(a) \n(b)\nFigure 4. (a) First derivative mean spectra of the leaves of each saline treatment. Calibration set, n = \n\
    28 leaves (4 saline treatments × 7 leaves); (b) zoom on the red edge. See Table 1 for abbreviations. \n\
    A displacement to the right in the peak, though slight and progressive, can be observed in the \n\
    S3 treatment (Figure 4B). However, numerous authors found that a displacement to the left of the \n\
    red edge typically occurs when plants suffer environmental conditions of stress (water, nutritive, or \n\
    salt stress) [21,32,38]. This displacement to the left is typically related to the loss of activity and the \n\
    degradation of chlorophyll [39]. Lara et al. [28] confirmed this fact in spinach leaves during shelf life \n\
    and attributed it to aging processes and the loss of cell structure and chlorophyll. However, the \n\
    effect of aging in harvested leaves over time is not similar to the effect of saline stress in freshly \n\
    harvested leaves. \nLi et al. [26] performed a test with castor bean seedlings under salinity conditions of 0, 100, 200, \n\
    and 300 mM of NaCl. They observed a displacement to the right of the red edge at 100 mM NaCl and \n\
    displacements towards the left edge for higher concentrations. This salt concentration (100 mM) is in \n\
    the range of the highest concentrations of the present research. The salinity may initially cause a \n\
    displacement  of  the  red  edge  to  the  right  until  reaching  a  certain \
    \ level  of  salt  concentration, \nsubsequently shifting to the left when the salinity stress increases; this concurs with [26].  \n\
    Although it has been reported that an increment in the soil salinity generally tends to induce a \n\
    decrease in leaf reflectance in the NIR spectral region [40], in this study in that region (800–1000 nm), \n\
    it  can  be  observed  a  minimal  effect  of  the  salinity.  As  high  near \
    \ infrared  reflectance  values  are \nassociated with proper development of the plants, it could be interpreted that the levels of salinity \n\
    applied in the present study are not enough to induce change in the NIR, i.e. are not enough to cause \n\
    the displacement of the red edge towards lower wavelengths.  \n3.3. Models and Data Processing \n\
    3.3.1. Second Derivative and SNV Normalization: Principal Component Analysis \n\
    Figure  5A  shows  the  mean  preprocessed  spectra  (SG  D2  SNV)  corresponding \
    \ to  the  seven \nleaves from the calibration set for each salt treatment. This preprocessing procedure (SG D2 SNV) \n\
    showed the best ability for sensing the effect of salinity once PCA was applied, and therefore these \n\
    spectra were considered for the computation of the PCA model. The second derivative is a measure \n\
    of the change in the slope of the spectral curve, and therefore it presents a value different from zero \n\
    when there is a curvature on the spectra. It changes its value if there is a change in the curvature \n\
    (typically  when  there  is  a  lateral  displacement  of  the  spectra  at  red‐edge \
    \ area).  Just  at  red  edge \n(maximum slope, inflection point) the curvature is zero, and it becomes different from zero if the \n\
    spectra move towards the right or towards the left. Peaks at 550 nm and 510 nm in the preprocessed \n\
    spectra decrease with the salinity level, so the slope of the spectra decrease with increased salinity. \n\
    This  concurs  with  the  previously  observed  flattening  process  of  the \
    \ spectra  as  the  salinity  level \nincreases. \nFigure 4. (a) First derivative\
    \ mean spectra of the leaves of each saline treatment. Calibration set,\nn = 28\
    \ leaves (4 saline treatments × 7 leaves); (b) zoom on the red edge. See Table\
    \ 1 for abbreviations.\nA displacement to the right in the peak, though slight\
    \ and progressive, can be observed in the\nS3 treatment (Figure 4B). However,\
    \ numerous authors found that a displacement to the left of the\nred edge typically\
    \ occurs when plants suffer environmental conditions of stress (water, nutritive,\n\
    or salt stress) [21,32,38]. This displacement to the left is typically related\
    \ to the loss of activity and\nthe degradation of chlorophyll [39]. Lara et al.\
    \ [28] conﬁrmed this fact in spinach leaves during\nshelf life and attributed\
    \ it to aging processes and the loss of cell structure and chlorophyll. However,\n\
    the effect of aging in harvested leaves over time is not similar to the effect\
    \ of saline stress in freshly\nharvested leaves.\nLi et al. [26] performed a test\
    \ with castor bean seedlings under salinity conditions of 0, 100,\n200, and 300\
    \ mM of NaCl. They observed a displacement to the right of the red edge at 100\
    \ mM\nNaCl and displacements towards the left edge for higher concentrations.\
    \ This salt concentration\n(100 mM) is in the range of the highest concentrations\
    \ of the present research. The salinity may initially\ncause a displacement of\
    \ the red edge to the right until reaching a certain level of salt concentration,\n\
    subsequently shifting to the left when the salinity stress increases; this concurs\
    \ with [26].\nAlthough it has been reported that an increment in the soil salinity\
    \ generally tends to induce\na decrease in leaf reﬂectance in the NIR spectral\
    \ region [40], in this study in that region (800–1000 nm),\nit can be observed\
    \ a minimal effect of the salinity. As high near infrared reﬂectance values are\
    \ associated\nwith proper development of the plants, it could be interpreted that\
    \ the levels of salinity applied in the\npresent study are not enough to induce\
    \ change in the NIR, i.e. are not enough to cause the displacement\nof the red\
    \ edge towards lower wavelengths.\n3.3. Models and Data Processing\n3.3.1. Second\
    \ Derivative and SNV Normalization: Principal Component Analysis\nFigure 5A shows\
    \ the mean preprocessed spectra (SG D2 SNV) corresponding to the seven leaves\n\
    from the calibration set for each salt treatment. This preprocessing procedure\
    \ (SG D2 SNV) showed\nthe best ability for sensing the effect of salinity once\
    \ PCA was applied, and therefore these spectra\nwere considered for the computation\
    \ of the PCA model. The second derivative is a measure of the\nchange in the slope\
    \ of the spectral curve, and therefore it presents a value different from zero\
    \ when\nthere is a curvature on the spectra. It changes its value if there is\
    \ a change in the curvature (typically\nwhen there is a lateral displacement of\
    \ the spectra at red-edge area). Just at red edge (maximum slope,\ninﬂection point)\
    \ the curvature is zero, and it becomes different from zero if the spectra move\
    \ towards\nthe right or towards the left. Peaks at 550 nm and 510 nm in the preprocessed\
    \ spectra decrease with\nAppl. Sci. 2016, 6, 412\n9 of 18\nthe salinity level,\
    \ so the slope of the spectra decrease with increased salinity. This concurs with\
    \ the\npreviously observed ﬂattening process of the spectra as the salinity level\
    \ increases.\nAppl. Sci. 2016, 6, 412 \n9 of 19 \n400\n500\n600\n700\n800\n900\n\
    100\n-3\n-2\n-1\n0\n1\n2\n3\n4\nWavelength (nm)\np\n \n \nCt\nS1\nS2\nS3\n \n\
    (a)\n400\n500\n600\n700\n800\n900\n1000\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n\
    Wavelenght (nm)\nL o a d in g s  o n  P C 1\n \n \nld1\nCt\nS1\nS2\nS3\n-2\n-1.5\n\
    -1\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\nSaline treatment\nP C 1  s c o re s  (9 4 ´4\
    \ 6 %  e x p la in e d  v a ria n c e )\n(b) \n(c)\nFigure  5.  (a)  Mean  Savitzky‐Golay \
    \ algorithm  (SG),  second  derivative  (D2)  and  SNV  pre‐treated \nspectra of the leaves of calibration set for each saline treatment; (b) The first principal component \n\
    (PC1)  loadings  from  principal  component  analysis  (PCA)  applied  on  second derivative \
    \ and  SNV \nprocessed spectra; and (c) PC1 scores of mean spectra of the leaves of calibration set for each saline \n\
    treatment (Ct, S1, S2 and S3 treatments, x‐axis), with the mean values of each group are shown in red. \n\
    The first principal component (PC1) from the PCA applied on the SG D2 SNV preprocessed \n\
    spectra, explaining a 94.46% of variance, was retained as the most related to the changes caused by \n\
    the salt concentration in the leaves. The PC1 loadings are shown in Figure 5B. The projections of the \n\
    mean spectra for the PC1 (scores) for each of the four salinity groups are presented in Figure 5C. \n\
    Two regions of high loading values can be observed (Figure 5B), corresponding to the major \n\
    regions with differences in the preprocessed spectra (Figure 5A): one is located at approximately \n\
    500–600 nm and the other occurs at the red edge region. The remaining wavelengths have loading \n\
    values close to zero reflecting no changes in the slope at NIR region, above approximately 780 nm. \n\
    However,  there  is  an  increase  of  the  level  of  reflectance  at  this \
    \ NIR  region  observed  in  the  raw \n(without preprocessing) reflectance spectra, Figure 3A. This fact is deeply discussed in Section 3.2.1 \n\
    and at the end of Section 3.2.2.  \nThe loadings of PC1 around 500–600 nm presents an approximation to the slope between 550 \n\
    and 510 nm (Figure 5B). Looking to the values (in the loadings of PC1 in the Figure 5B) of “y‐axis” at \n\
    550 and at 510 nm, it is shown that “y‐axis” at 550 is higher and more positive than “y‐axis” at 510 \n\
    (that is negative). Thus, the inner product of PC1 by SG D2 SNV spectra at this region produces an \n\
    approximation to the slope at this range. As the salinity level increases, the value of the slope (which \n\
    is negative in this range) increases (because it becomes less negative).  \nIn addition, the SG D2 SNV spectra showed a displacement to the right at the red edge region. \n\
    PC1 also senses this displacement of the preprocessed spectra at 710 nm. Looking to the PC1 in the \n\
    region comprises between 675 and 745 nm, the y‐axis value at 675 nm and at 745 nm is lower and \n\
    Figure 5. (a) Mean Savitzky-Golay algorithm (SG), second derivative (D2) and SNV\
    \ pre-treated spectra\nof the leaves of calibration set for each saline treatment;\
    \ (b) The ﬁrst principal component (PC1)\nloadings from principal component analysis\
    \ (PCA) applied on second derivative and SNV processed\nspectra; and (c) PC1 scores\
    \ of mean spectra of the leaves of calibration set for each saline treatment\n\
    (Ct, S1, S2 and S3 treatments, x-axis), with the mean values of each group are\
    \ shown in red.\nThe ﬁrst principal component (PC1) from the PCA applied on the\
    \ SG D2 SNV preprocessed\nspectra, explaining a 94.46% of variance, was retained\
    \ as the most related to the changes caused by the\nsalt concentration in the\
    \ leaves. The PC1 loadings are shown in Figure 5B. The projections of the mean\n\
    spectra for the PC1 (scores) for each of the four salinity groups are presented\
    \ in Figure 5C.\nTwo regions of high loading values can be observed (Figure 5B),\
    \ corresponding to the major\nregions with differences in the preprocessed spectra\
    \ (Figure 5A): one is located at approximately\n500–600 nm and the other occurs\
    \ at the red edge region. The remaining wavelengths have loading\nvalues close\
    \ to zero reﬂecting no changes in the slope at NIR region, above approximately\
    \ 780 nm.\nHowever, there is an increase of the level of reﬂectance at this NIR\
    \ region observed in the raw (without\npreprocessing) reﬂectance spectra, Figure\
    \ 3A. This fact is deeply discussed in Section 3.2.1 and at the\nend of Section\
    \ 3.2.2.\nThe loadings of PC1 around 500–600 nm presents an approximation to the\
    \ slope between\n550 and 510 nm (Figure 5B). Looking to the values (in the loadings\
    \ of PC1 in the Figure 5B) of “y-axis”\nat 550 and at 510 nm, it is shown that\
    \ “y-axis” at 550 is higher and more positive than “y-axis” at\n510 (that is negative).\
    \ Thus, the inner product of PC1 by SG D2 SNV spectra at this region produces\n\
    an approximation to the slope at this range. As the salinity level increases,\
    \ the value of the slope\n(which is negative in this range) increases (because\
    \ it becomes less negative).\nAppl. Sci. 2016, 6, 412\n10 of 18\nIn addition,\
    \ the SG D2 SNV spectra showed a displacement to the right at the red edge region.\n\
    PC1 also senses this displacement of the preprocessed spectra at 710 nm. Looking\
    \ to the PC1 in the\nregion comprises between 675 and 745 nm, the y-axis value\
    \ at 675 nm and at 745 nm is lower and\nnegative in comparison with y-axis value\
    \ at 710 nm, which is positive. Therefore, inner product of PC1\nby SG D2 SNV\
    \ spectra at this region produces an approximation to the minus second derivative\
    \ at\n710 (−SD710) [28]:\n−SD710 = 2 × R710 − R675 − R745\n(4)\nwhere −SD710 is\
    \ related to the curvature at 710 nm: if this wavelength corresponds to the inﬂection\n\
    point, the –SD710 is equal to zero; on the contrary, −SD710 will be positive if\
    \ the preprocessed\nspectra move towards the right. In the present research the\
    \ preprocessed spectra at red-edge region\nmoved towards the right, with the increasing\
    \ levels of salinity, so −SD710 showed increasing values.\nAs a consequence of\
    \ the above-mentioned facts, the scores of PC1 (i.e., the projection of each spectral\n\
    signal, in our case the SG D2 SNV spectra, in the PC, obtained multiplying the\
    \ spectra by the loading)\nevolved towards higher values showing the behavior\
    \ described around 500–600 nm and 710 nm.\nScores progressively evolve towards\
    \ higher values with increasing levels of salinity. Therefore,\nboth previously\
    \ mentioned regions (500–600 nm and 710 nm) could be related to the salt concentration\n\
    of the leaves. The analysis of the results of the PCA on the SG D2 SNV spectra\
    \ allowed identifying\nsimilar spectral regions more susceptible to the effect\
    \ of salinity than the analysis of the SNV spectra\nand ﬁrst derivative spectra.\n\
    Figure 6 shows the images of scores of the leaves from the calibration set. These\
    \ images are\nobtained projecting the hyperspectral images, corrected with SG\
    \ D2 SNV, on the PC1. The differences\nbetween Ct and S3 leaves were clear, evolving\
    \ from blue pixels (low score value) to orange and red\npixels (high score value)\
    \ when the salinity concentration increased.\nAppl. Sci. 2016, 6, x FOR PEER REVIEW\
    \  \n11 of 20 \n \nCt treatment \n \nS1 treatment \n \nS2 treatment \n \nS3 treatment\
    \ \nFigure 6. Cont.\nAppl. Sci. 2016, 6, 412\n11 of 18\n \nS3 treatment \n \n\
    Figure 6. Virtual images of scores obtained by projection of the hyperspectral\
    \ images, corrected with \nsecond derivative Savitzky–Golay algorithm and SNV\
    \ normalization, on PC1. Leaves from \ncalibration set. Pixels colored from blue\
    \ to red are showing increasing levels of salinity affectation. \n3.3.2. A New\
    \ Index on the Red Edge Region: Level Salinity Index LSI \nA new spectral index\
    \ (combination of only few wavelengths) was considered for multispectral \nproposes;\
    \ they could be cheaper and faster than hyperspectral vision systems. As previously\
    \ \nexplained, a slight displacement to the right of the red edge region was observed\
    \ on the first \nderivative of the spectra of the S3 treatment, which could be\
    \ related to the moderate increase in the \nsalt concentration of the leaves.\
    \ On the other hand, the PC1 loading showed the red edge region as \nFigure 6.\
    \ Virtual images of scores obtained by projection of the hyperspectral images,\
    \ corrected with\nsecond derivative Savitzky–Golay algorithm and SNV normalization,\
    \ on PC1. Leaves from calibration\nset. Pixels colored from blue to red are showing\
    \ increasing levels of salinity affectation.\n3.3.2. A New Index on the Red Edge\
    \ Region: Level Salinity Index LSI\nA new spectral index (combination of only\
    \ few wavelengths) was considered for multispectral\nproposes; they could be cheaper\
    \ and faster than hyperspectral vision systems. As previously explained,\na slight\
    \ displacement to the right of the red edge region was observed on the ﬁrst derivative\
    \ of the\nspectra of the S3 treatment, which could be related to the moderate\
    \ increase in the salt concentration\nof the leaves. On the other hand, the PC1\
    \ loading showed the red edge region as the most sensitive\narea to changes in\
    \ the spectra induced by salinity. Consequently, an index, Level Salinity Index\
    \ (LSI),\nbased on the most relevant wavelengths of PC1 loadings (Figure 5B) was\
    \ proposed.\nThis LSI index is an approximation to the second derivative at 710\
    \ nm (see explanation at\nSection 3.3.1) and it is a value related to the red\
    \ edge displacements observed in the SG D2 SNV\npreprocessed spectra and also\
    \ in the loadings of PC1. It is also very similar, but not identical, to the\n\
    proposed by [28]:\nLSI = [(R675 + R745)/2] − R710\n(5)\nThe LSI was applied to\
    \ each pixel of the hyperspectral images of the calibration set leaves in order\n\
    to generate corresponding virtual images and predict the salinity effects on the\
    \ leaves (Figure 7).\nAppl. Sci. 2016, 6, 412 \n12 of 19 \n \nCt treatment\nS1 treatment\n\
    S2 treatment\nFigure 7. Cont.\nAppl. Sci. 2016, 6, 412\n12 of 18\nS2 treatment\n\
    S3 treatment\nFigure 7. Virtual images obtained by applying the Level Salinity Index (LSI) on each pixel of the \n\
    hyperspectral  images  of  lettuce  leaves,  from  calibration  set.  Pixels \
    \ colored  from  blue  to  red  are \nshowing increasing levels of salinity affectation. \n\
    Clear differences between the LSI values from the pixels of the leaves corresponding to the \n\
    control treatment (Ct) and the pixels of the leaves corresponding to the highest salinity treatment \n\
    (S3)  were  observed  (Figure 7).  The  increase in  the  LSI  values  noted in \
    \ the  images  responds  to a \nFigure 7. Virtual images obtained by applying\
    \ the Level Salinity Index (LSI) on each pixel of the\nhyperspectral images of\
    \ lettuce leaves, from calibration set. Pixels colored from blue to red are showing\n\
    increasing levels of salinity affectation.\nClear differences between the LSI\
    \ values from the pixels of the leaves corresponding to the\ncontrol treatment\
    \ (Ct) and the pixels of the leaves corresponding to the highest salinity treatment\
    \ (S3)\nwere observed (Figure 7). The increase in the LSI values noted in the\
    \ images responds to a lateral\ndisplacement of the red edge to the right [28,41],\
    \ which concurs with the ﬁndings obtained from the\nﬁrst derivative spectra analysis.\n\
    3.3.3. Comparison between Spectral Indexes\nTable 2 presents eleven narrow band\
    \ indexes that are related to pigment, water content and\nsalinity effects, all\
    \ of them extracted from a literature review on this topic.\nTable 2. Indexes\
    \ used for comparison with Level Salinity Index (LSI) index.\nIndex\nFormula\n\
    NDVI, related to chlorophyll content [42]\nNDVI = (R800 − R670)/(R800 + R670)\n\
    NDVI705, related to chlorophyll content [43]\nNDVI705 = (R750 − R705)/(R750 +\
    \ R705)\nRed Edge Position Index, related to chlorophyll content [44]\nRPI = (R750/R700)\n\
    Related to chlorophyll content in anthocyanin free leaves [45]\nChlgreen = (R760/R550)\
    \ − 1\nChlred edge = (R760/R705) − 1\nPlant Senescence Reﬂectance Index [46]\n\
    PSRI = (R680 − R500)/(R750)\nWater Index, related to water content [40]\nWI =\
    \ (R900/R970)\nGreen region NDVI, related to salinity [13]\nNDVIgreen = (R550\
    \ − R670)/(R550 + R670)\nFar red region NDVI, related to salinity [13]\nNDVIfar\
    \ red = (R710 − R670)/(R710 + R670)\nSimple Ratio Vegetation Index, related to\
    \ salinity [37]\nSRVI = (R830/R660)\nGreen and Indigo Ratio, related to salinity\
    \ [47]\nGIR = (R436/R554)\nThese eleven indexes, along with LSI index and PCA\
    \ scores, are applied on the spectra of the\nleaves from the calibration set (n\
    \ = 1000 spectra per leaf, 7000 spectra per salinity class). Table 3 presents\n\
    the results of the one-way ANOVA’s and Tukey–Kramer tests performed on these indexes,\
    \ calculated\nconsidering as grouping variable the salinity treatment. LSI (F\
    \ = 3440.2), Green and Indigo Ratio (GIR)\nAppl. Sci. 2016, 6, 412\n13 of 18\n\
    (F = 1340.2), Green region Normalized Difference Vegetation Index (NDVIgreen)\
    \ (F = 1160.7), Chlred edge\n(F = 1024.1), NDVI705 (F = 874.2), Chlgreen (F =\
    \ 737.2), NDVIfar red (F = 702.8) and RPI (F = 686.4) showed\nconsistent trends\
    \ and all of them were able to distinguish the four groups of salinity treatments.\
    \ Among\nthese indexes, the three with the highest segregation ability had been\
    \ speciﬁcally deﬁned for sensing\nsalinity effects (LSI, GIR and NDVIgreen). However,\
    \ Simple Ratio Vegetation Index (SRVI) (F = 119.7),\nalso for salinity detection,\
    \ together with NDVI (F = 137.3), Plant Senescence Reﬂectance Index (PSRI)\n(F\
    \ = 119.8) and Water Index (WI) (F = 53.4), did not give consistent trend or clear\
    \ distinction between\nsalinity classes. The scores of PC1 showed similar segregating\
    \ ability as LSI (F = 2845.88).\nTable 3. Analysis of variance (F-Fisher and p-value)\
    \ comparing the values of indexes for the four\nsalinity classes (n = 7000 spectra\
    \ per class belonging to the calibration set of leaves). Mean values and\nstandard\
    \ error of the different indexes per salinity class. Means followed by the same\
    \ letter are not\nsigniﬁcantly different by Tukey–Kramer test; different letters\
    \ implies there is signiﬁcant difference\nbetween means.\nIndex\nF-Fisher\np-Value\n\
    Mean\nStandard Error\nLSI\n3440.2\n0\nCt\n0.01765a\n0.00045\nS1\n0.03130b\n0.00045\n\
    S2\n0.05175c\n0.00045\nS3\n0.07829d\n0.00045\nNDVI\n137.3\n2.45 × 10−88\nCt\n\
    0.82469a\n0.00080\nS1\n0.80969b\n0.00080\nS2\n0.80345c\n0.00080\nS3\n0.80714d\n\
    0.00080\nNDVI705\n874.2\n0\nCt\n0.45693a\n0.00076\nS1\n0.46466b\n0.00076\nS2\n\
    0.47856c\n0.00076\nS3\n0.50774d\n0.00076\nRPI\n686.4\n0\nCt\n3.48135a\n0.00864\n\
    S1\n3.54853b\n0.00864\nS2\n3.64758c\n0.00864\nS3\n3.99075d\n0.00864\nChlgreen\n\
    737.2\n0\nCt\n2.26632a\n0.00731\nS1\n2.28408a\n0.00731\nS2\n2.39855b\n0.00731\n\
    S3\n2.69576c\n0.00731\nChlred edge\n1024.1\n0\nCt\n1.78446a\n0.00583\nS1\n1.84552b\n\
    0.00583\nS2\n1.94359c\n0.00583\nS3\n2.20687d\n0.00583\nPSRI\n119.8\n4.38 × 10−77\n\
    Ct\n−0.00536a\n0.00013\nS1\n−0.00706b\n0.00013\nS2\n−0.00552a\n0.00013\nS3\n−0.00366c\n\
    0.00013\nWI\n53.4\n2.03 × 10−34\nCt\n1.28100a\n0.00036\nS1\n1.27616b\n0.00036\n\
    S2\n1.27872c\n0.00036\nS3\n1.28220d\n0.00036\nNDVIgreen\n1160.7\n0\nCt\n0.53842a\n\
    0.00108\nS1\n0.50228b\n0.00108\nS2\n0.47366c\n0.00108\nS3\n0.45415d\n0.00108\n\
    Appl. Sci. 2016, 6, 412\n14 of 18\nTable 3. Cont.\nIndex\nF-Fisher\np-Value\n\
    Mean\nStandard Error\nNDVIfar red\n702.8\n0\nCt\n0.68830a\n0.00102\nS1\n0.65912b\n\
    0.00102\nS2\n0.63788c\n0.00102\nS3\n0.62661d\n0.00102\nSRVI\n119.7\n5.35 × 10−77\n\
    Ct\n10.53830a\n0.04345\nS1\n9.64646b\n0.04345\nS2\n9.46616c\n0.04345\nS3\n10.04068d\n\
    0.04345\nGIR\n1340.2\n0\nCt\n0.35211a\n0.00100\nS1\n0.38783b\n0.00100\nS2\n0.41381c\n\
    0.00100\nS3\n0.43755d\n0.00100\nScores PC1\n2845.88\n0\nCt\n−1.0902a\n0.000227\n\
    S1\n−0.6474b\n0.000228\nS2\n0.0851c\n0.000199\nS3\n1.1235d\n0.000212\nAlthough\
    \ seven of the eleven indexes reviewed from literature showed good performance\
    \ for\nsalinity effect detection, the LSI index, speciﬁcally deﬁned for images\
    \ of lettuces acquired under\nlaboratory conditions, exhibited the best behavior,\
    \ with the highest F-value (Table 3).\n3.4. Validation of the Models to Evaluate\
    \ Salinity Effects\nIn order to test the performance of the prediction models\
    \ generated on the calibration set, both\nmodels (PC1 on SG D2 SNV spectra and\
    \ LSI index) were applied to the hyperspectral images of the\nleaves of the validation\
    \ set, thus obtaining corresponding virtual images for each case of the two sets.\n\
    Table 4 presents the results of the one-way ANOVA’s and Tukey–Kramer tests performed\
    \ on the LSI\nindexes and PCA scores of n = 1000 spectra from each leaf of the\
    \ validation set, considering as grouping\nvariable the salinity treatment. Differences\
    \ between the leaves corresponding to the Ct treatment and\nthe S3 treatment (Figure\
    \ 8) were observed with both models. It can be seen that the virtual images\n\
    of both models for the same leaf showed a similar distribution of pixel values.\
    \ Both types of virtual\nimages were able to sense the effect of salinity in a\
    \ similar way. However, the LSI-based model was\nsimpler and faster to apply because\
    \ it is based solely on the reﬂectance at three different wavelengths.\nTable\
    \ 4. Analysis of variance (F-Fisher and p-value) comparing the values of LSI index\
    \ and the ﬁrst\nprincipal component (PC1) scores for the four salinity classes\
    \ (n = 3000 spectra per class belonging to\nthe validation set of leaves). Mean\
    \ values and standard error of the different indexes per salinity class.\nMeans\
    \ followed by the same letter are not signiﬁcantly different by Tukey–Kramer test;\
    \ different letters\nimplies there is signiﬁcant difference between means.\nIndex\n\
    F-Fisher\np-Value\nMean\nStandard Error\nLSI\n1491.57\n0\nCt\n0.01845\n0.00131a\n\
    S1\n0.03149\n0.00127b\nS2\n0.05250\n0.00127c\nS3\n0.08018\n0.00127d\nScores PC1\n\
    1197.02\n0\nCt\n−1.05383\n0.05122a\nS1\n−0.63252\n0.05122b\nS2\n0.12681\n0.05122c\n\
    S3\n1.16911\n0.05122d\nAppl. Sci. 2016, 6, 412\n15 of 18\nAppl. Sci. 2016, 6,\
    \ x FOR PEER REVIEW  \n17 of 20 \n \nCt treatment \n \n \nS1 treatment \n \n \n\
    S2 treatment \n \n \nS3 treatment \n \n \nFigure 8. Virtual images obtained by\
    \ applying both models developed on the hyperspectral images \nof lettuce leaves,\
    \ from the validation set. Pixels colored from blue to red are showing increasing\
    \ \nlevels of salinity affectation. Left: PC1 model. Right: LSI model. \nAs it\
    \ was introduced, salinity stress can induce marginal necrosis and tip burn in\
    \ the leaves of \nlettuce. These symptoms have recently been associated with local\
    \ oxidative stress in leaf margins \nthat appear before symptom expression [9].\
    \ This pattern is coincident with the distribution of the \nmost affected pixels\
    \ by salinity in our images. Because of this, hyperspectral images could be used\
    \ to \nAppl. Sci. 2016, 6, x FOR PEER REVIEW  \n17 of 20 \n \nCt treatment \n\
    \ \n \nS1 treatment \n \n \nS2 treatment \n \n \nS3 treatment \n \n \nFigure 8.\
    \ Virtual images obtained by applying both models developed on the hyperspectral\
    \ images \nof lettuce leaves, from the validation set. Pixels colored from blue\
    \ to red are showing increasing \nlevels of salinity affectation. Left: PC1 model.\
    \ Right: LSI model. \nAs it was introduced, salinity stress can induce marginal\
    \ necrosis and tip burn in the leaves of \nlettuce. These symptoms have recently\
    \ been associated with local oxidative stress in leaf margins \nthat appear before\
    \ symptom expression [9]. This pattern is coincident with the distribution of\
    \ the \nmost affected pixels by salinity in our images. Because of this, hyperspectral\
    \ images could be used to \nFigure 8. Virtual images obtained by applying both\
    \ models developed on the hyperspectral images of\nlettuce leaves, from the validation\
    \ set. Pixels colored from blue to red are showing increasing levels of\nsalinity\
    \ affectation. Left: PC1 model. Right: LSI model.\nIt can be observed in the images\
    \ that the pixels from the external parts of the leaves are the most\naffected\
    \ by the salinity. The salinity effect is more intense in the areas furthest away\
    \ from the veins\nof the leaves. In an intact plant, water is transported to the\
    \ leaves through the xylem of vascular\nbundles that branch into a ﬁne network\
    \ of nerves through the leaf [48]. Salinity restricts water uptake\nand transpiration\
    \ and thus reduces water and nutrient uptake and transport to young leaves [49].\n\
    This limitation in the xylem transport due to the effect of salinity could explain\
    \ why the most affected\nareas of the leaves correspond to areas with thin venation\
    \ (leaf margin).\nAs it was introduced, salinity stress can induce marginal necrosis\
    \ and tip burn in the leaves of\nlettuce. These symptoms have recently been associated\
    \ with local oxidative stress in leaf margins that\nappear before symptom expression\
    \ [9]. This pattern is coincident with the distribution of the most\nAppl. Sci.\
    \ 2016, 6, 412\n16 of 18\naffected pixels by salinity in our images. Because of\
    \ this, hyperspectral images could be used to detect\nsymptoms of salt stress\
    \ in leaves before visible damage expression and thus avoid signiﬁcant economic\n\
    losses in crop production.\n4. Conclusions\nHyperspectral images were applied\
    \ in a non-destructive manner to evaluate the effect of salinity\non lettuce leaves\
    \ at harvest. These images were studied in order to determine the best combination\
    \ of\nwavelengths for diagnosing the problem of salinity in lettuce leaves.\n\
    Based on the spectral features, two models were proposed. The ﬁrst model was based\
    \ on\na principal component analysis of the second derivative mean spectra of\
    \ the leaves and normalized with\nSNV. PC1 was most related to the changes in\
    \ the leaves induced by salt concentration. By projecting\nthe conveniently processed\
    \ hyperspectral images on the PC1, the corresponding virtual images of\nscores\
    \ showed clear differences among the leaves for each saline treatment.\nThe second\
    \ model was based on an LSI index focused on the red edge region. Several indexes\n\
    from literature exhibited ability to sense the salinity effect on the leaves.\
    \ However, LSI was the most\ndiscriminant index compared to them. This fact could\
    \ indicate that the detailed interpretation of the\nspectra behavior could enhance\
    \ the performance of indexes in each particular cultivar and condition.\nOnce\
    \ the index is proposed, a less expensive multispectral system could be employed\
    \ to compute the\nrespective virtual images (only three wavelengths were actually\
    \ required).\nAn analysis of variance using only about 1% of pixels extracted\
    \ at random from the total\npopulation of pixels of each leaf showed signiﬁcant\
    \ differences between the leaves for each salinity\nlevel; in an industrial application,\
    \ this allows for the implementation of a hyperspectral or multispectral\ndevice\
    \ with smaller spatial resolution. This could simplify the technology, managed\
    \ data volume and\nhardware requirements of the system, making it more economical\
    \ and faster.\nHyperspectral images allow for observations of the distribution\
    \ of the salinity effects on the\nsurface of the leaves, which are more intense\
    \ in the areas furthest from the veins. Marginal necrosis\nappearing in leaves\
    \ of plants under salinity stress can be detected by the artiﬁcial images before\
    \ the\ndamage is visible.\nFurther research will be necessary to validate these\
    \ models with larger data sets and salinity\nranges and different varieties and\
    \ growing conditions than those examined in this study.\nAcknowledgments: The\
    \ funding of this work was supported by the MICINN through projects Multihort\n\
    (AGL2008-05666-C02-01) and QualityLeaf (AGL2013-48529-R). LPF-TAGRALIA is part\
    \ of the CEI Moncloa Campus.\nAuthor Contributions: Miguel Ángel Lara, Belén Diezma\
    \ and Lourdes Lleó conceived and designed the\nexperiments, and wrote the article.\
    \ Miguel Ángel Lara, Belén Diezma, Lourdes Lleó, Yolanda Garrido and\nMaría Isabel\
    \ Gil performed the experiments. Miguel Ángel Lara analyzed the data. Yolanda\
    \ Garrido and\nMaría Isabel Gil contributed reagents and materials, provided knowledge\
    \ about plant physiology and reviewed\nthe article. Jean Michel Roger contributed\
    \ chemometrics tools, provided knowledge in chemometrics and data\nanalysis, and\
    \ reviewed the article. Margarita Ruiz-Altisent provided scientiﬁc knowledge and\
    \ reviewed the article.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nLamsal, K.; Paudyal, G.N.; Saeed, M. Model for\
    \ assessing impact of salinity on soil water availability and\ncrop yield. Agric.\
    \ Water Manag. 1999, 41, 57–70. [CrossRef]\n2.\nShannon, M.C.; Grieve, C.M.; Francois,\
    \ L.E. Whole-plant response to salinity.\nIn Plant-Environment\nInteractions;\
    \ Wilkinson, R.E., Ed.; Marcel Dekker: New York, NY, USA, 1994; pp. 199–244.\n\
    3.\nAcosta, J.A.; Faz, A.; Jansen, B.; Kalbitz, K.; Martínez-Martínez, S. Assessment\
    \ of salinity status in intensively\ncultivated soils under semiarid climate,\
    \ Murcia, SE Spain. J. Arid Environ. 2011, 75, 1056–1066. [CrossRef]\n4.\nÜnlükara,\
    \ A.; Cemek, B.; Karaman, S.; Er¸sahin, S. Response of lettuce (Lactuca sativa\
    \ var. crispa) to salinity of\nirrigation water. N. Z. J. Crop Horticult. Sci.\
    \ 2008, 36, 265–273.\nAppl. Sci. 2016, 6, 412\n17 of 18\n5.\nYamaguchi, T.; Blumwald,\
    \ E. Developing salt-tolerant crop plants:\nChallenges and opportunities.\nTrends\
    \ Plant Sci. 2005, 10, 615–620. [CrossRef] [PubMed]\n6.\nMaas, E.V.; Hoffman,\
    \ G.J. Crop salt tolerance ± current assessment. J. Irrig. Drain. Eng. 1977, 103,\
    \ 115–134.\n7.\nBie, Z.; Ito, T.; Shinohara, Y. Effects of sodium sulfate and\
    \ sodium bicarbonate on the growth, gas exchange\nand mineral composition of lettuce.\
    \ Sci. Horticult. 2004, 99, 215–224. [CrossRef]\n8.\nEraslan, F.; Inal, A.; Savasturk,\
    \ O.; Gunes, A. Changes in antioxidative system and membrane damage of\nlettuce\
    \ in response to salinity and boron toxicity. Sci. Horticult. 2007, 114, 5–10.\
    \ [CrossRef]\n9.\nCarassay, L.R.; Bustos, D.A.; Golberg, A.D.; Taleisnik, E. Tipburn\
    \ in salt-affected lettuce (Lactuca sativa L.)\nplants results from local oxidative\
    \ stress. J. Plant Physiol. 2012, 169, 285–293. [CrossRef] [PubMed]\n10.\nWinsor,\
    \ G.; Adams, P. Diagnosis of mineral disorders in plants. In Glasshouse Crops;\
    \ Robinson, J.B.D., Ed.;\nMinistry of Agriculture, Fisheries and Food/Agriculture\
    \ and Fisheries Research Council, HMSO: London,\nUK, 1987.\n11.\nPoss, J.A.; Grattan,\
    \ S.R.; Grieve, C.M.; Shannon, M.C. Characterization of leaf boron injury in salt-stressed\n\
    Eucalyptus by image analysis. Plant Soil 1999, 206, 237–245. [CrossRef]\n12.\n\
    Saure, M.C. Causes of the tipburn disorder in leaves of vegetables. Sci. Horticult.\
    \ 1998, 76, 131–147. [CrossRef]\n13.\nPoss, J.A.; Russell, W.B.; Grieve, C.M.\
    \ Estimating yields of salt- and water-stressed forages with remote\nsensing in\
    \ the visible and near infrared. J. Environ. Qual. 2006, 35, 1060–1071. [CrossRef]\
    \ [PubMed]\n14.\nHamzeh, S.; Naseri, A.A.; AlaviPanah, S.K.; Mojaradi, B.; Bartholomeus,\
    \ H.M.; Clevers, J.G.P.W.; Behzad, M.\nEstimating salinity stress in sugarcane\
    \ ﬁelds with spaceborne hyperspectral vegetation indices. Int. J. Appl.\nEarth\
    \ Obs. Geoinf. 2013, 21, 282–290. [CrossRef]\n15.\nZhang, T.; Zeng, S.; Gao, Y.;\
    \ Ouyang, Z.; Li, B.; Fang, C.; Zhao, B. Using hyperspectral vegetation indices\
    \ as\na proxy to monitor soil salinity. Ecol. Indic. 2011, 11, 1552–1562. [CrossRef]\n\
    16.\nTucker, C.J. Remote sensing of leaf water content in the near infrared. Remote\
    \ Sens. Environ. 1980, 10, 23–32.\n[CrossRef]\n17.\nHarris, A.; Bryant, R.G.;\
    \ Baird, A.J. Mapping the effects of water stress on Sphagnum: Preliminary\nobservations\
    \ using airborne remote sensing. Remote Sens. Environ. 2006, 100, 363–378. [CrossRef]\n\
    18.\nClevers, J.G.P.W.; Kooistra, L.; Schaepman, M.E. Estimating canopy water\
    \ content using hyperspectral\nremote sensing data. Int. J. Appl. Earth Obs. Geoinf.\
    \ 2010, 12, 119–125. [CrossRef]\n19.\nSuárez, L.; Zarco-Tejada, P.J.; González-Dugo,\
    \ V.; Berni, J.A.J.; Sagardoy, R.; Morales, F.; Fereres, E. Detecting\nwater stress\
    \ effects on fruit quality in orchards with time-series PRI airborne imagery.\
    \ Remote Sens. Environ.\n2010, 114, 286–298. [CrossRef]\n20.\nAl-Abbas, A.H.;\
    \ Barr, R.; Hall, J.D.; Crance, F.L.; Baumgardner, M.F. Spectra of normal and\
    \ nutrient-deﬁcient\nmaize leaves. Agron. J. 1974, 66, 16–20. [CrossRef]\n21.\n\
    Masoni, A.; Ercoli, L.; Mariotti, M. Spectral properties of leaves deﬁcient in\
    \ iron, sulfur, magnesium, and\nmanganese. Agron. J. 1996, 88, 937–943. [CrossRef]\n\
    22.\nPacumbaba, R.O., Jr.; Beyl, C.A. Changes in hyperspectral reﬂectance signatures\
    \ of lettuce leaves in response\nto macronutrient deﬁciencies. Adv. Space Res.\
    \ 2011, 48, 32–42. [CrossRef]\n23.\nAgüero, M.V.; Ponce, A.G.; Moreira, M.R.;\
    \ Roura, S.I. Lettuce quality loss under conditions that favor the\nwilting phenomenon.\
    \ Postharvest Biol. Technol. 2011, 59, 124–131. [CrossRef]\n24.\nVan’t Hoff, J.H.\
    \ The role of osmotic pressure in the analogy between solution and gases. Z. Phys.\
    \ Chem. 1887,\n1, 481–508. [CrossRef]\n25.\nRichardson, A.D.; Duigan, S.P.; Berlyn,\
    \ G.P. An evaluation of noninvasive methods to estimate foliar\nchlorophyll content.\
    \ New Phytol. 2002, 153, 185–194. [CrossRef]\n26.\nLi, G.; Wan, S.; Zhou, J.;\
    \ Yang, Z.; Qin, P. Leaf chlorophyll ﬂuorescence, hyperspectral reﬂectance, pigments\n\
    content, malondialdehyde and proline accumulation responses of castor bean (Ricinus\
    \ communis L.) seedlings\nto salt stress levels. Ind. Crops Prod. 2010, 31, 13–19.\
    \ [CrossRef]\n27.\nSavitzky, A.; Golay, M.J.E. Smoothing and differentiation of\
    \ data by simpliﬁed least squares procedures.\nAnal. Chem. 1964, 36, 1627–1639.\
    \ [CrossRef]\n28.\nLara, M.A.; Lleó, L.; Diezma-Iglesias, B.; Roger, J.M.; Ruiz-Altisent,\
    \ M. Monitoring spinach shelf-life with\nhyperspectral image through packaging\
    \ ﬁlms. J. Food Eng. 2013, 119, 353–361. [CrossRef]\n29.\nBarnes, R.J.; Dhanoa,\
    \ M.S.; Lister, S.J. Standard normal variate transformation and de-trending of\n\
    near-infrared diffuse reﬂectance spectra. Appl. Spectrosc. 1989, 43, 772–777.\
    \ [CrossRef]\n30.\nBlum, A. Plant Breeding for Stress Environment; CRC Press Inc.:\
    \ Boca Raton, FL, USA, 1988.\nAppl. Sci. 2016, 6, 412\n18 of 18\n31.\nLeidi, E.O.;\
    \ Pardo, J.M. Tolerancia de los cultivos al estrés salino: Qué hay de nuevo. Revista\
    \ de Investigaciones\nde la Facultad de Ciencias Agrarias 2002, UNR 2, 70–91.\n\
    32.\nGitelson, A.; Viña, A.; Arkebauer, T.J.; Rundquist, D.C.; Keydan, G.; Leavitt,\
    \ B. Remote estimation of leaf\narea index and green leaf biomass in maize canopies.\
    \ Geophys. Res. Lett. 2003, 30, 1248. [CrossRef]\n33.\nKnipling, E.B. Physical\
    \ and physiological basis for the reﬂectance of visible and near-infrared radiation\
    \ from\nvegetation. Remote Sens. Environ. 1970, 1, 155–159. [CrossRef]\n34.\n\
    Parida, A.K.; Das, A.B. Salt tolerance and salinity effects on plants: A review.\
    \ Ecotoxicol. Environ. Saf. 2005,\n60, 324–349. [CrossRef] [PubMed]\n35.\nGitelson,\
    \ A.; Zur, Y.; Chivkunova, O.B.; Merzlyak, M.N. Assessing carotenoid content in\
    \ plant leaves with\nreﬂectance spectroscopy. Photochem. Photobiol. 2002, 75,\
    \ 272–281. [CrossRef]\n36.\nPeñuelas, P.; Isla, R.; Filella, I.; Araus, J.L. Visible\
    \ and near infrared reﬂectance assessment of salinity effects\non barley. Crop\
    \ Sci. 1997, 37, 198–202. [CrossRef]\n37.\nWang, D.; Wilson, C.; Shannon, M.C.\
    \ Interpretation of salinity and irrigation effects on soybean canopy\nreﬂectance\
    \ in visible and near-infrared spectrum domain. Int. J. Remote Sens. 2002, 23,\
    \ 811–824. [CrossRef]\n38.\nMariotti, M.; Ercoli, L.; Masoni, A. Spectral properties\
    \ of iron deﬁcient corn and sunﬂower leaves. Remote\nSens. Environ. 1996, 58,\
    \ 282–288. [CrossRef]\n39.\nUstin, S.L.; Gitelson, A.A.; Jacquemoud, S.; Schaepman,\
    \ M.; Asner, G.P.; Gamon, J.A.; Zarco-Tejada, P.\nRetrieval of foliar information\
    \ about plant pigment systems from high resolution spectroscopy. Remote Sens.\n\
    Environ. 2009, 113, S67–S77. [CrossRef]\n40.\nPeñuelas, J.; Pinol, J.; Ogaya,\
    \ R.; Filella, I. Estimation of plant water concentration by the reﬂectance water\n\
    index wi (r900/r970). Int. J. Remote Sens. 1997, 18, 2869–2875. [CrossRef]\n41.\n\
    Diezma, B.; Lleó, L.; Roger, J.M.; Herrero-Langreo, A.; Lunadei, L.; Ruiz-Altisent,\
    \ M. Examination of the\nquality of spinach leaves using hyperspectral imaging.\
    \ Postharvest Biol. Technol. 2013, 85, 8–17. [CrossRef]\n42.\nTucker, C.J. Red\
    \ and photographic infrared linear combinations for monitoring vegetation. Remote\
    \ Sens.\nEnviron. 1979, 8, 127–150. [CrossRef]\n43.\nGitelson, A.; Merzlyak, M.N.\
    \ Spectral reﬂectance changes associated with autumn senescence of\nAesculus hippocastanum\
    \ L. and Acer platanoides. Leaves spectal features and relation to chlorophyll\
    \ estimation.\nJ. Plant Physiol. 1994, 143, 286–292. [CrossRef]\n44.\nGitelson,\
    \ A.; Merzlyak, M.N.; Lichtenthaler, H.K. Detection of red edge position and chlorophyll\
    \ content by\nreﬂectance measurements near 700 nm. J. Plant Physiol. 1996, 148,\
    \ 501–508. [CrossRef]\n45.\nGitelson, A.A.; Keydan, G.P.; Merzlyak, M.N. Three-Band\
    \ Model for Non-Invasive Estimation of Chlorophyll\nCarotenoids and Anthocyanin\
    \ Contents in Higher Plant Leaves. Papers in Natural Resources. 2006. Paper 258.\n\
    Available online: http://digitalcommons.unl.edu/natrespapers/258 (accessed on\
    \ 1 November 2016).\n46.\nMerzlyak, M.N.; Gitelson, A.A.; Chivkunova, O.B.; Rakitin,\
    \ V.Y. Non-destructive optical detection of pigment\nchanges during leaf senescence\
    \ and fruit ripening. Physiol. Plant. 1999, 106, 135–141. [CrossRef]\n47.\nRud,\
    \ R.; Shoshany, M.; Alchanatis, V. Spectral indicators for salinity effects in\
    \ crops: A comparison of a new\ngreen indigo ratio with existing indices. Remote\
    \ Sens. Lett. 2011, 2, 289–298. [CrossRef]\n48.\nHolbrook, N.M. El balance hídrico\
    \ de las plantas.\nIn Fisología Vegetal; Taiz, L., Zeiger, E., Eds.;\nUniversitat\
    \ Jaume I: Castelló de la Plana, Spain, 2006; pp. 79–115.\n49.\nLazof, D.B.; Bernstein,\
    \ N. Effects of salinization on nutrient transport to lettuce leaves: Consideration\
    \ of leaf\ndevelopmental stage. New Phytol. 1999, 144, 85–94. [CrossRef]\n© 2016\
    \ by the authors; licensee MDPI, Basel, Switzerland. This article is an open access\n\
    article distributed under the terms and conditions of the Creative Commons Attribution\n\
    (CC-BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Applied sciences
  limitations: '>'
  pdf_link: https://www.mdpi.com/2076-3417/6/12/412/pdf?version=1481096379
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Hyperspectral Imaging to Evaluate the Effect of IrrigationWater Salinity
    in Lettuce
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/plants12102061
  analysis: '>'
  authors:
  - Alebel Mekuriaw Abebe
  - Y.G. Kim
  - Jae‐Young Kim
  - Song Lim Kim
  - Jeongho Baek
  citation_count: 5
  full_citation: '>'
  full_text: ">\nCitation: Abebe, A.M.; Kim, Y.; Kim,\nJ.; Kim, S.L.; Baek, J. Image-Based\n\
    High-Throughput Phenotyping in\nHorticultural Crops. Plants 2023, 12,\n2061. https://doi.org/10.3390/\n\
    plants12102061\nAcademic Editor: Georgios\nKoubouris\nReceived: 12 April 2023\n\
    Revised: 12 May 2023\nAccepted: 18 May 2023\nPublished: 22 May 2023\nCopyright:\n\
    © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an\
    \ open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the\
    \ Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nplants\nReview\nImage-Based High-Throughput Phenotyping in\nHorticultural\
    \ Crops\nAlebel Mekuriaw Abebe, Younguk Kim, Jaeyoung Kim, Song Lim Kim and Jeongho\
    \ Baek *\nDepartment of Agricultural Biotechnology, National Institute of Agricultural\
    \ Science, Rural Development\nAdministration, Jeonju 54874, Republic of Korea\n\
    * Correspondence: ﬁrstleon@korea.kr; Tel.: +82-63-238-4670\nAbstract: Plant phenotyping\
    \ is the primary task of any plant breeding program, and accurate\nmeasurement\
    \ of plant traits is essential to select genotypes with better quality, high yield,\
    \ and\nclimate resilience. The majority of currently used phenotyping techniques\
    \ are destructive and time-\nconsuming. Recently, the development of various sensors\
    \ and imaging platforms for rapid and\nefﬁcient quantitative measurement of plant\
    \ traits has become the mainstream approach in plant\nphenotyping studies. Here,\
    \ we reviewed the trends of image-based high-throughput phenotyping\nmethods applied\
    \ to horticultural crops. High-throughput phenotyping is carried out using various\n\
    types of imaging platforms developed for indoor or ﬁeld conditions. We highlighted\
    \ the applications\nof different imaging platforms in the horticulture sector\
    \ with their advantages and limitations.\nFurthermore, the principles and applications\
    \ of commonly used imaging techniques, visible light\n(RGB) imaging, thermal imaging,\
    \ chlorophyll ﬂuorescence, hyperspectral imaging, and tomographic\nimaging for\
    \ high-throughput plant phenotyping, are discussed. High-throughput phenotyping\n\
    has been widely used for phenotyping various horticultural traits, which can be\
    \ morphological,\nphysiological, biochemical, yield, biotic, and abiotic stress\
    \ responses. Moreover, the ability of high-\nthroughput phenotyping with the help\
    \ of various optical sensors will lead to the discovery of new\nphenotypic traits\
    \ which need to be explored in the future. We summarized the applications of image\n\
    analysis for the quantitative evaluation of various traits with several examples\
    \ of horticultural crops\nin the literature. Finally, we summarized the current\
    \ trend of high-throughput phenotyping in\nhorticultural crops and highlighted\
    \ future perspectives.\nKeywords: phenotyping; image analysis; phenomics; sensor;\
    \ horticultural crop\n1. Introduction\nThe world population keeps increasing and\
    \ is expected to reach ten billion by 2050,\nso as the demand for food and energy.\
    \ This alarms the need to maximize the yield and\nquality of food crops as well\
    \ as reduce postharvest losses. Breeding for high yield, better\nquality, and\
    \ resistance to biotic (disease, pest, weed) and abiotic (drought, salt, heat,\
    \ cold)\nstresses should be the priority to meet the projected food demand. Plant\
    \ phenotyping\nis the core of any plant breeding program, and accurate measurement\
    \ of plant traits is\nessential for the selection of the best genotypes. Phenotype\
    \ is the result of the interactions\nbetween genotype and all the surrounding\
    \ environmental conditions during the plant\ngrowth cycle, whereas phenotyping\
    \ refers to the measurement of any aspect of plant\ntraits, including growth,\
    \ development, and physiology [1]. Plant phenomics is the high-\nthroughput collection\
    \ and analysis of multidimensional phenotypes of the whole plant\nthrough its\
    \ life span [2,3]. The advancement of next-generation sequencing and marker\n\
    technology has accelerated genomic study, allowing the mapping and identiﬁcation\
    \ of\ngenes controlling complex traits [4]. However, phenomic information is not\
    \ adequately\navailable due to the effect of environmental factors and lack of\
    \ accurate measurements\nlimiting the phenotypic characterization of crop traits.\n\
    Plants 2023, 12, 2061. https://doi.org/10.3390/plants12102061\nhttps://www.mdpi.com/journal/plants\n\
    Plants 2023, 12, 2061\n2 of 23\nConventional phenotyping has been the bottleneck\
    \ for breeding for a long time as\nit is labor-intensive, time-consuming, and\
    \ does not have high throughput [5]. The recent\nintroduction of high-throughput\
    \ phenotyping methods is accelerating plant phenotyping,\nenabling high-throughput\
    \ measurement of several phenotypic data nondestructively and\nobjectively [3,6].\
    \ Furthermore, high-throughput phenotyping with the help of optical\nsensors,\
    \ computer vision, and robotics will bring new traits under consideration which\n\
    were difﬁcult to measure via the conventional method.\nHigh-throughput phenotyping\
    \ platforms can image hundreds of plants daily using\nvarious types of optical\
    \ sensors, allowing the measurement of morphological, physiological,\nbiochemical,\
    \ and performance traits in a non-destructive way [7–10]. The principle of image-\n\
    based phenotyping is based on the interaction of electromagnetic radiation and\
    \ the plant\nsurface, including absorption, reﬂection, emission, transmission,\
    \ and ﬂuorescence, which\ndiffer between normal and stressed plants or among genotypes\
    \ [9]. These interactions\nwill help to estimate various types of phenotypic traits\
    \ of the plant with the help of\noptical sensors. Image-based high-throughput\
    \ phenotyping aims to quantify numerous\ntraits, which requires the use of various\
    \ types of optical sensors. Some of the currently\navailable sensors for plant\
    \ phenotyping include visible light (red–green–blue), thermal,\nﬂuorescence, hyperspectral,\
    \ multispectral, light detection and ranging (LiDAR), magnetic\nresonance (MRI)\
    \ imaging, X-ray computed tomography (X-ray CT), and positron emission\ntomography\
    \ (PET) [3,11,12]. The applications of various types of sensors may differ\ndepending\
    \ on imaging platforms, accessibility, cost, and the target trait under consideration.\n\
    The different sensors can be used separately or in combination for fast and accurate\
    \ plant\nphenotyping, each of which comes with its own advantages and limitations.\n\
    High-throughput phenotyping is used in breeding, crop cultivation, and even posthar-\n\
    vest, depending on the purpose of phenotyping. In plant breeding, phenotyping\
    \ a large\nnumber of samples (population) aims to increase the selection intensity\
    \ and accuracy and\ncharacterize various crop traits to select the best genotypes,\
    \ while phenotyping in crop\ncultivation is used to monitor the occurrence of\
    \ any plant stresses such as disease, pests,\nnutrient stress, weeds, or abiotic\
    \ stress at early stages [1,8]. Real-time phenotypic data\nacquisition and analysis\
    \ will help to make immediate management decisions for the crop.\nHence, image-based\
    \ phenotyping will play a pivotal role in the precision cultivation of\nhorticultural\
    \ crops. Horticultural crops are mostly utilized in the fresh state and are highly\n\
    perishable due to their high water content, such as vegetables and fruits. The\
    \ market value\nof horticultural products is highly dependent on external appearance\
    \ (color, shape, size,\ntexture) and internal (soluble solid content and ﬁrmness)\
    \ quality attributes. The status\nof these quality attributes changes over time\
    \ during maturity, ripening, and postharvest\nstorage and should be routinely\
    \ monitored [13,14]. Currently, external quality attributes\nare mostly evaluated\
    \ using visual inspection in the horticulture chain, which is slow and\nsubjective.\
    \ On the other hand, internal quality attributes are quantiﬁed using destructive\n\
    laboratory analysis or handheld/portable instruments, which are also limited in\
    \ speed and\nsample size. Due to the highly perishable nature and the dynamics\
    \ of quality attributes\nover time in horticultural products, image-based phenotyping\
    \ will greatly improve the\nspeed, volume, and accuracy of postharvest phenotyping.\n\
    In this paper, we reviewed the applications of image-based phenotyping for the\
    \ as-\nsessment of various traits in horticultural crops. Commonly used imaging\
    \ platforms and\nsensors for high-throughput phenotyping are described. The application\
    \ of these technolo-\ngies for phenotyping various traits of horticultural crops\
    \ is discussed with several examples\nin the literature. Finally, the current\
    \ trends and future perspectives of high-throughput\nphenotyping in horticultural\
    \ crops are highlighted. Using multiscale imaging platforms\nequipped with state-of-the-art\
    \ imaging technologies will enable rapid and accurate quan-\ntitative measurement\
    \ of diverse plant phenotypic traits to accelerate crop improvement,\nprecision\
    \ agriculture, and objective postharvest phenotyping (Figure 1).\nPlants 2023,\
    \ 12, 2061\n3 of 23\nREVIEW \n3 of 24 \n \nFigure 1. Schematic overview of image-based\
    \ high-throughput phenotyping in horticultural crops. \nUAP, unmanned aerial platform;\
    \ MAP, manned aerial platform; RGB, red–green–blue; LiDAR, light \ndetection and\
    \ ranging; X-ray CT, X-ray computed tomography; MRI, magnetic resonance imaging.\
    \ \n2. High-Throughput Phenotyping Platforms \nHigh-throughput phenotyping is\
    \ carried out using various types of imaging plat-\nforms. The suitability of\
    \ high-throughput phenotyping platforms may vary depending on \nh\nl b\nh h\n\
    b\nh\nfi ld I\nFigure 1. Schematic overview of image-based high-throughput phenotyping\
    \ in horticultural crops.\nUAP, unmanned aerial platform; MAP, manned aerial platform;\
    \ RGB, red–green–blue; LiDAR, light\ndetection and ranging; X-ray CT, X-ray computed\
    \ tomography; MRI, magnetic resonance imaging.\nPlants 2023, 12, 2061\n4 of 23\n\
    2. High-Throughput Phenotyping Platforms\nHigh-throughput phenotyping is carried\
    \ out using various types of imaging platforms.\nThe suitability of high-throughput\
    \ phenotyping platforms may vary depending on the\nimaging environment, e.g.,\
    \ laboratory, growth chamber, greenhouse, or ﬁeld. In most\ncontrolled environment\
    \ conditions, imaging is carried out by moving the sensor towards\nthe plant (sensor\
    \ movement type) or the plant is transported to the ﬁxed imaging setup\nusing\
    \ a conveyer belt or other transporting methods (plant movement type) for routine\n\
    phenotypic measurement. For example, a greenhouse-based sensor-to-plant platform\
    \ was\nused to measure static and dynamic traits such as geometric, structural,\
    \ color, and textural\nphenotypes of lettuce [15]. Image-based phenotyping in\
    \ controlled environment conditions\nhas the advantage of high precision, high\
    \ repeatability, continuous automated operation,\nand absence of interference\
    \ from external environmental conditions. However, they are\ngenerally expensive\
    \ and can monitor a very limited number of samples. The conveyer\ntype and benchtop-type\
    \ platforms are mostly used and have well-established systems for\ncontrolled\
    \ environment conditions [6].\nImaging platforms for field-based conditions can\
    \ be ground or aerial-based, tar-\ngeting phenotyping of plant characteristics\
    \ at individual plant or area levels. They\nare grouped into ground-based or aerial-based\
    \ on the structures where the sensor is\nmounted. Ground-based imaging platforms\
    \ such as pole/tower-based, mobile platform\n(vehicle), gantry-based, and cable\
    \ suspended are flexible for deployment and have a\ngood spatial resolution. However,\
    \ they are subject to varying environmental conditions\ndue to the slow speed\
    \ of covering a large field. Aerial-based imaging platforms include\nunmanned\
    \ aerial platforms, manned aerial platforms, and satellites. These imaging\nplatforms\
    \ can cover a wide area in a short period of time and are able to overcome\nvarying\
    \ environmental conditions. The disadvantage of the platforms is that they have\n\
    a limited payload, and the spatial resolution of the image is affected by the\
    \ speed and\naltitude of the aerial structure [6,9]. Unmanned aerial vehicle (UAV)\
    \ platforms were used\nfor the measurement of various traits in horticultural\
    \ crops. For example, UAV-based\nremote sensing coupled with different machine\
    \ learning approaches was used for dis-\nease detection and classification in\
    \ potato, tomato, banana, pear, and apple [16–22], for\ntree detection in orchards\
    \ such as banana and citrus [23–25], for aboveground biomass\nestimation in onion,\
    \ potato, tomato, and strawberry [26–29], and other traits of fruits\nand vegetables\
    \ [23,30,31].\n3. Commonly Used Imaging Techniques for High-Throughput Plant Phenotyping\n\
    3.1. Visible Light Imaging\nVisible light sensors detect light in a wavelength\
    \ spectrum of 400–700 nm and provide\nreﬂected values of red, green, and blue\
    \ (RGB). Visible light imaging is widely used for\nhigh-throughput phenotyping\
    \ because of its accessibility, simplicity, and low cost [32].\nHigh-resolution\
    \ RGB images can be used to accurately measure plant biomass [28,33–37],\nroot\
    \ architecture [38,39], plant growth rate [40–43], germination rate [44], yield\
    \ [45–47],\ndisease detection and quantiﬁcation [17,48–50], and abiotic stresses\
    \ [51]. Their application\nin the ﬁeld can be affected by minimal color variation\
    \ between the leaf and the background\nand the inﬂuence of light for automatic\
    \ image processing [32].\n3.2. Thermal Imaging\nThermal infrared imaging allows\
    \ the visualization and distribution of infrared ra-\ndiation over a leaf or plant\
    \ surface. A thermal camera converts infrared radiation (heat)\nemitted from the\
    \ object into visible images showing the spatial distribution of surface\ntemperature.\
    \ The thermal sensor records the emitted light from the object in the thermal\n\
    range of 3–5 µm or 7–14 µm with an image showing the temperature values per pixel.\n\
    Thermal imaging can be used to detect the physiological status of the plant in\
    \ response to\nbiotic and abiotic stress, such as canopy or leaf temperature [52],\
    \ transpiration and stomatal\nconductance [53], and plant water status [9,11].\
    \ Under water deﬁcit conditions, plants\nPlants 2023, 12, 2061\n5 of 23\nclose\
    \ their stomata and reduce water loss through transpiration which is also highly\
    \ linked\nwith the soil moisture content. The reduction in transpirational cooling\
    \ results in increased\nleaf temperature. Hence, thermal imaging can be used to\
    \ manage water and irrigation in\nprecision agriculture [54].\n3.3. Hyperspectral\
    \ Imaging\nHyperspectral imaging captures electromagnetic spectra (λ) and spatial\
    \ (x, y) data at\nevery pixel in an image to reconstruct the 3D data matrix called\
    \ hypercube, containing\nthousands of images in the spectral range of 250–2500\
    \ nm encompassing UV, VIS, NIR,\nand SWIR [55]. It offers a large amount of information,\
    \ allowing the extraction of a wide\nrange of phenotypic traits, while the storage\
    \ and analysis of the vast amount of hyperspec-\ntral data is challenging. Some\
    \ of its applications include estimation of nutrient content,\ndisease detection\
    \ [16,56–58], fruit maturity and ripening [59,60], and other physiological\nand\
    \ biochemical traits which are used to infer plant growth and development as well\
    \ as\nyield [55].\n3.4. Fluorescence Imaging\nThe light energy absorbed by chlorophyll\
    \ can be used for photosynthesis, dissipated\nas heat, or re-emitted. Fluorescence\
    \ is the light emitted when the plant absorbs radiation\nof shorter wavelengths,\
    \ mainly via the chlorophyll complex, and is very small (<3%)\ncompared to the\
    \ total amount of radiation emitted to the object from the light sources. The\n\
    amount of re-emitted light (ﬂuorescence) is a good indicator of the plant’s ability\
    \ to utilize\nthe absorbed light and is used to estimate the overall plant health\
    \ status [61]. Fluorescence\nimaging is used to estimate photosynthetic efﬁciency\
    \ and other associated metabolic\nprocesses of the plant affected by biotic and\
    \ abiotic stresses [62–65]. The ﬂuorescence\npattern of plants under stress conditions\
    \ will show an altered pattern compared to no\nstressed plants. Sensors sensitive\
    \ to ﬂuorescence are used to capture ﬂuorescence signals\nafter illumination of\
    \ the plant or tissue with visible light, infrared, or UV light. Maximum\nquantum\
    \ efﬁciency (Fv/Fm), non-photochemical quenching (energy dissipated as heat from\n\
    photosynthetic reaction center), the effective quantum yield of PSII (ΦPSII or\
    \ Fq′/Fm′),\nand relative electron transport rate are some of the parameters derived\
    \ from chlorophyll\nﬂuorescence which are used to assess the physiological status\
    \ of the plant in relation to\ndifferent stress conditions, where Fm is maximum\
    \ ﬂuorescence of a dark-adapted leaf and\nFv is the difference between Fm and\
    \ minimum ﬂuorescence from dark-adapted leaf (F0) [10].\nThe problem with ﬂuorescence\
    \ imaging in the ﬁeld condition is that it does not specify\nthe cause of signal\
    \ changes in the plant, e.g., light, temperature, or other environmental\nfactors\
    \ [11,61].\n3.5. Tomographic Imaging\nOther imaging techniques such as magnetic\
    \ resonance imaging (MRI), X-ray com-\nputed tomography (X-ray CT), and positron\
    \ emission tomography (PET) provide high-\nresolution 3D images of a single plant\
    \ or plant parts [66]. MRI captures the 3D images\nof the internal structures\
    \ of the sample enabling non-invasive quantification of both\nstatic and dynamic\
    \ traits such as structural, biochemical, and temporal changes inside\nthe plant.\
    \ MRI can be used to monitor changes in growth and development (seed and\nbulb\
    \ germination, seed development, fruit growth, and root growth), water dynamics\n\
    within the plant, drought stress responses (drought stress, salt stress, cold\
    \ stress, and\nheat stress), and the plant–pathogen interaction [67]. X-ray CT\
    \ is used to visualize\nthe 3D structures of internal and external features of\
    \ the plant at the micro or macro\nlevel. When the X-ray beam passes through the\
    \ sample, part of the beam is absorbed,\nand the remaining radiograph is recorded\
    \ by the detector. Multiple 2D projections\nare recorded by moving the sample\
    \ or the sensor, which are then used to reconstruct\nthe 3D images [68]. For example,\
    \ it has been used for the characterization of size and\nshape-related morphological\
    \ traits of seed and fruit [69,70]. These imaging techniques\nPlants 2023, 12,\
    \ 2061\n6 of 23\nare time-consuming and are not suitable when a large number of\
    \ samples are under\nconsideration. In addition, due to the larger size and heavy\
    \ weight of the equipment it is\nnot usable on aerial imaging platforms [55].\
    \ Examples of images from commonly used\nsensors in high-throughput phenotyping\
    \ are shown in Figure 2.\nPlants 2023, 12, x FOR PEER REVIEW \n6  of  24 \n \n\
     \nFigure 2. Examples of images from commonly used sensors in high-throughput phenotyping and \n\
    their spectral range. (a) RGB; (b) NIR [71]; (c) SWIR [72]; (d) thermal (Qubit phenomics, Canada); \n\
    (e) X-ray [73]; (f) MRI [66]; (g) ﬂuorescence; (h) LiDAR and photogrammetry point cloud (Pix4D \n\
    S.A., Prilly, Switzerland). RGB and ﬂuorescence images were captured in our lab. \n\
    Imaging technology is the primary component of high-throughput plant phenotyp-\n\
    ing as acquired phenotypic traits are determined by the type of sensor (imaging tech-\n\
    nique).  Visible  light  (RGB)  imaging  and  multi/hyperspectral  imaging  techniques \
    \ are \nwidely  used  to  acquire  morphological,  physiological,  biochemical, \
    \ biotic,  and  abiotic \nstress-related traits. Fluorescence imaging and thermal imaging are used to capture the \n\
    photosynthetic and surface temperature of the plant, respectively, which are physiological \n\
    traits. LiDAR, X-ray CT, and MRI are mainly used to acquire morphological traits [3]. Dif-\n\
    ferent imaging techniques come with their speciﬁc advantages and disadvantages to cap-\n\
    ture certain plant traits. The summary of imaging techniques and measurable phenotypic \n\
    traits with potential applications in high-throughput plant phenotyping is presented in \n\
    Table 1. Among the variety of commercially available sensors for diﬀerent imaging tech-\n\
    niques, the choice depends on the cost, robustness, and other speciﬁcations of the sensor \n\
    to capture the target trait [74]. Examples of diﬀerent sensors used for high-throughput \n\
    phenotyping of some horticultural crops are presented in Table 2. High-throughput phe-\n\
    notyping will beneﬁt from the increasing capabilities and advancements of sensor tech-\n\
    nologies. \n \n \nFigure 2. Examples of images from commonly used sensors in high-throughput\
    \ phenotyping and\ntheir spectral range. (a) RGB; (b) NIR [71]; (c) SWIR [72];\
    \ (d) thermal (Qubit phenomics, Canada);\n(e) X-ray [73]; (f) MRI [66]; (g) ﬂuorescence;\
    \ (h) LiDAR and photogrammetry point cloud (Pix4D S.A.,\nPrilly, Switzerland).\
    \ RGB and ﬂuorescence images were captured in our lab.\nImaging technology is\
    \ the primary component of high-throughput plant phenotyping\nas acquired phenotypic\
    \ traits are determined by the type of sensor (imaging technique).\nVisible light\
    \ (RGB) imaging and multi/hyperspectral imaging techniques are widely used\nto\
    \ acquire morphological, physiological, biochemical, biotic, and abiotic stress-related\n\
    traits. Fluorescence imaging and thermal imaging are used to capture the photosynthetic\n\
    and surface temperature of the plant, respectively, which are physiological traits.\
    \ LiDAR,\nX-ray CT, and MRI are mainly used to acquire morphological traits [3].\
    \ Different imaging\ntechniques come with their speciﬁc advantages and disadvantages\
    \ to capture certain plant\ntraits. The summary of imaging techniques and measurable\
    \ phenotypic traits with potential\napplications in high-throughput plant phenotyping\
    \ is presented in Table 1. Among the\nvariety of commercially available sensors\
    \ for different imaging techniques, the choice\ndepends on the cost, robustness,\
    \ and other speciﬁcations of the sensor to capture the target\ntrait [74]. Examples\
    \ of different sensors used for high-throughput phenotyping of some\nhorticultural\
    \ crops are presented in Table 2. High-throughput phenotyping will beneﬁt\nfrom\
    \ the increasing capabilities and advancements of sensor technologies.\nPlants\
    \ 2023, 12, 2061\n7 of 23\nTable 1. Summary of most common imaging techniques\
    \ used in high-throughput plant phenotyp-\ning [11,75].\nImaging Technique a\n\
    Phenotypic Traits\nAdvantages\nLimitations\nPotential Applications\nVisible light\
    \ imaging\nShape, color, size,\nbiomass, pigment\ncontent, disease and\npest,\
    \ stress responses,\nnutrient stress,\nvegetation indices\nCheap, easy operation\n\
    and maintenance,\nprovide color\ninformation, high\nresolution, fast data\nacquisition\n\
    Limited to three spectral\nbands (RGB), affected by\nlight, only provide\nrelative\
    \ measurement\nGrowth monitoring,\nplant stress detection,\nfruit maturity and\n\
    ripening estimation,\ngrading and sorting,\nquality evaluation,\nyield prediction,\
    \ 3D\nmodeling, crop\nmanagement, robotic\nharvesting\nThermal imaging\nLeaf greenness,\
    \ leaf\ncolor, leaf chlorophyll\ncontent, leaf/canopy\ntemperature, disease\n\
    and pest, phenology,\nphotosynthetic status\nWide measurement\nrange, background\n\
    interference can be\nremoved\nRequire sensor calibration\nand atmospheric\ncorrection,\
    \ difficulty of\nthrough time comparison\ndue to changes in\nambient condition\n\
    affecting canopy\ntemperature, need\nreference for comparison,\ndifficult to separate\
    \ soil\nand plant temperature in\nsparse canopies (limiting\nthe automation of\
    \ image\nprocessing)\nPlant stress detection,\nirrigation scheduling\nHyperspectral\n\
    imaging\nLeaf/canopy water\nstatus, canopy coverage\nand volume, leaf\ngreenness,\
    \ disease and\npest, photosynthetic\nrate, nutrient stress,\nmetabolites\nHigh\
    \ spectral\nresolution, background\ninterference can be\nremoved\nExpensive, low\
    \ spatial\nresolution, too large\nimage data challenging\nfor storage and analysis,\n\
    affected by ambient\nlight condition\nGrowth monitoring,\nbiotic and abiotic stress\n\
    detection, fruit\nmaturity and ripening\nestimation, quality\nevaluation, biomass\n\
    estimation, metabolite\nprediction\nFluorescence imaging\nChlorophyll content,\n\
    canopy coverage,\ndisease and pest,\nphotosynthetic status\nSensitive to\nﬂuorescence\
    \ and water\nstress\nLimited in ﬁeld\napplication, difﬁcult to\nmeasure at the\
    \ canopy\nscale due to the small\nsignal-to-noise ratio\nGrowth monitoring,\n\
    early detection of biotic\nand abiotic stress\nMultispectral imaging\nCanopy coverage\
    \ and\nvolume, chlorophyll\ncontent, leaf greenness,\nplant diseases and\npests,\
    \ photosynthetic\nstatus, water content\nEasy in image\nprocessing; mature\ntechnology\n\
    Limited to several\nspectral bands; spectral\ndata should be\nfrequently calibrated\n\
    using referenced objects;\neffects of camera\ngeometrics, illumination\ncondition,\
    \ and sun angle\non the data signal\nGrowth monitoring,\nbiotic and abiotic stress\n\
    detection\nLiDAR\nPlant height, canopy\nvolume, shoot biomass\nProvide\nthree-dimensional\n\
    shape\nExpensive, sensitive to\nthe small difference in\npath length; speciﬁc\n\
    illumination required\nfor some laser scanning\ninstruments, data\nprocessing\
    \ is\ntime-consuming\nGrowth monitoring,\nstructure capture\nPlants 2023, 12,\
    \ 2061\n8 of 23\nTable 1. Cont.\nImaging Technique a\nPhenotypic Traits\nAdvantages\n\
    Limitations\nPotential Applications\n3D laser scanner\nGeometrical plant traits\n\
    such as shape, length,\nheight, canopy\nstructure and volume\nLong measurement\n\
    distance; high precision;\ngood penetration\nExpensive, affected by\nexternal\
    \ factors such as\nwind and fog\nGrowth monitoring,\norgan morphogenesis\nMRI\n\
    Internal structures,\nmetabolites,\ndevelopment of root\nsystems, water\npresence\n\
    Available for screening\n3D structural\ninformation\nExpensive, low\nthroughput,\
    \ slow data\nacquisition\nAcquire 3D structures\nof the whole plant or\nplant\
    \ parts\nX-ray CT\nSize and shape\nLarge penetration\ndepth, scalable ﬁeld of\n\
    view, minimal sample\npreparation\nExpensive, low\nthroughput\nGrowth monitoring,\n\
    seed and fruit\ndevelopment, organ\nmorphogenesis, 3D\nvisualization of plant\n\
    organs and tissues\na Imaging technique: LiDAR—light detection and ranging; MRI—magnetic\
    \ resonance imaging; X-ray CT—X-ray\ncomputed tomography.\nTable 2. Examples of\
    \ sensors used for different imaging techniques in high-throughput plant\nphenotyping.\n\
    Imaging Technique\nSensor (Manufacturer)\nResolution\nCrop\nReference\nVisible\
    \ light\nimaging\nDJI Phantom 4 Pro (DJI Technology Co.,\nShenzhen, China)\n3000\
    \ × 4000 px\nStrawberry\n[76]\nSony Cyber-shot DSC-H3 camera (Sony\nCorporation,\
    \ Tokyo, Japan)\n3264 × 2448 px\nTomato\n[38]\nFujiﬁlm X20 (Fujiﬁlm Corporation,\n\
    Tokyo, Japan)\n3000 × 4000 px\nApple\n[77]\nThermal\nimaging\n3DR Solo quadcopter\
    \ (3D Robotics,\nBerkeley, CA, USA)\n1280 × 960 px\nBanana\n[23]\nVario CAM hr\
    \ inspect 575 (Jenoptic,\nJena, Germany)\n768 × 576 px\nMango\n[78]\nHyperspectral\
    \ imaging\nPika L 2.4 (Resonon Inc., Bozeman,\nMT, USA)\nunknown\nTomato\n[58]\n\
    HySpex VNIR 1800, HySpex SWIR 384\n(Norsk Elektro Optikk A/S,\nSkedsmokorset,\
    \ Norway)\nunknown\nGrape\n[79]\nFluorescence imaging\nPlantScreenTM Transect\
    \ XZ system\n(Photon Systems Instruments, Drásov,\nCzech Republic)\n1360 × 1024\
    \ px\nLettuce\n[64]\nMultispectral\nParrot Sequoia camera (Parrot Drone SAS,\n\
    Paris, France)\n1280 × 960 px\nBanana\n[80]\nRedEdge-M, (MicaSense, Seattle, WA,\
    \ USA)\n1280 × 960 px\nCitrus\n[81]\nLiDAR\nPlantEye F400 (Phenospex, Heerlen,\n\
    The Netherlands)\nunknown\nPotato\n[82]\n3D laser scanner\nFARO Focus 3D 120 terrestrial\
    \ laser scanner\n(Faro Technologies Inc., Lake Mary,\nFL, USA)\n1/5\nCassava\n\
    [83]\nX-ray CT\nX-ray imaging system (Xeye-5100F, SEC,\nSuwon, Republic of Korea)\n\
    2304 × 1300 px\nWatermelon\n[70]\nPlants 2023, 12, 2061\n9 of 23\n4. Applications\
    \ of Image-Based High-Throughput Phenotyping in Horticultural Crops\n4.1. Measurement\
    \ of Morphological Traits\nThe morphological traits, including color, size, shape,\
    \ and surface texture, determine\nthe appearance of the produce and are used as\
    \ quality parameters for visual inspection of\nhorticultural crops. Although visual\
    \ evaluation is a widely used nondestructive method\nfor grading and sorting in\
    \ horticultural crops, utilization of high-throughput phenotyping\nplatforms is\
    \ essential to obtain robust, faster, and objective results [14]. Nowadays, quan-\n\
    titative measurement of these traits (color, size, shape, and surface texture)\
    \ using image\nanalysis is increasingly used in different horticultural crops\
    \ [13,84–87].\nFor example, grape berry color is a very important trait in grape\
    \ breeding, which is\nqualitatively classiﬁed into six classes (green, yellow,\
    \ rose, red, grey, dark red violet, or blue–\nblack) according to the International\
    \ Organization of Vine and Wine [88] or simply as noir\n(red, blue, or black)\
    \ and non-noir (green or white). However, a qualitative assessment is\nvery difﬁcult\
    \ to differentiate between noir and non-noir groups. Image-based phenotyping\n\
    using different color spaces, RGB (red–green–blue), L*a*b (lightness, red–green,\
    \ blue–\nyellow), and HSI (hue, saturation, intensity), allows for the easy discrimination\
    \ of grape\nberry genotypes with different colors. RGB and HSI are able to separate\
    \ within the noir\nand non-noir groups and enable the identiﬁcation of minor QTLs\
    \ controlling grape berry\ncolor, which were not identiﬁed previously using qualitative\
    \ evaluation [89]. Quantitative\nmeasurement of strawberry fruit shape based on\
    \ elliptic Fourier descriptors (EFDs) [90]\nand image analysis allowed the identiﬁcation\
    \ of two QTLs for shape via a genome-wide\nassociation study [84]. The fruit shape\
    \ was highly correlated with the fruit length-to-width\nratio.\nThe application\
    \ of computer vision for shape quantiﬁcation using images of sweet\npotatoes has\
    \ shown that shape features, length-to-width ratio, curvature, cross-section\n\
    roundness, and cross-sectional diameters, are highly predictive of shape classes.\
    \ A neural\nnetwork-based shape classiﬁer was able to predict marketable (high\
    \ market value) and\nnon-marketable sweet potato classes with 84.59% accuracy\
    \ [13]. In most food industries,\nquality is mainly assessed by experts based\
    \ on subjective evaluation, which is very slow\nand inconsistent. The application\
    \ of image-based phenotyping in the food industry is\nvery important for fast,\
    \ reliable, and objective evaluation. The browning of apple slices\nwas quantiﬁed\
    \ using color space, L*a*b, and textural features (entropy, contrast, and\nhomogeneity)\
    \ from the RGB images taken over time and showed that cv. Golden Delicious\nhas\
    \ less browning compared to Honey Crispy and Granny Smith [87].\nIn most horticultural\
    \ crops, color, size, and texture are used as indicators of maturity\nand ripening.\
    \ The maturity and ripening of plum and banana fruits were estimated based\non\
    \ these features using image analysis in which color was the dominant feature\
    \ for the\nclassiﬁcation of maturity and ripening levels [91,92]. In general,\
    \ color, size, shape, and\ntexture are used to evaluate the external qualities\
    \ of horticultural crops that greatly affect the\nmarket value of the produce.\
    \ The applications of these quality attributes for the assessment\nof external\
    \ qualities of horticultural crops based on hyperspectral imaging were previously\n\
    reviewed [14].\n4.2. Measurement of Physiological Traits\nPhysiological traits\
    \ indicate the processes that occur within the plant, such as photo-\nsynthesis,\
    \ transpiration, and canopy temperature. These traits determine how the plant\n\
    is functioning under certain environmental conditions and are used to characterize\
    \ the\nplant response to biotic and abiotic stresses, plant growth, and plant\
    \ development [3].\nPhysiological traits can be quantiﬁed using various types\
    \ of sensors, including RGB, ChlF,\nmultispectral/hyperspectral, and thermal.\n\
    In horticultural crops, the physiological processes continue after harvesting\
    \ (posthar-\nvest physiology). Postharvest physiology deals with the response\
    \ of horticultural produce\nduring postharvest storage and handling conditions\
    \ along the processing or marketing\nchain. It determines the ripening, shelf\
    \ life, and the quality of the produce. Due to their\nPlants 2023, 12, 2061\n\
    10 of 23\nhighly perishable nature, the quality and shelf life of horticultural\
    \ crops is dependent on\npre/postharvest handling and storage conditions [14].\
    \ Hence, high-throughput postharvest\nphenotyping is necessary for rapid, robust,\
    \ and accurate measurement of ripening, shelf\nlife, quality, food safety, and\
    \ biochemical contents of the horticultural produce [86]. This\nwill help to track\
    \ the physiological status of the produce and make immediate decisions\nto avoid\
    \ economic losses. For example, visual inspection and analytical methods such\
    \ as\nspectroscopy and HPLC (high-performance liquid chromatography) analysis\
    \ are widely\nused for fruit quality assessment, which is labor-intensive, destructive,\
    \ time-consuming,\nand not robust. Therefore, using high-throughput methods which\
    \ can accurately and\nefﬁciently measure fruit and vegetable quality attributes\
    \ is essential. Chilling injury, one\nof the most common postharvest physiological\
    \ disorder in horticultural products was\ndetected using hyperspectral imaging\
    \ and achieved more than 91% detection accuracy in\napple, peach, and kiwi fruit\
    \ [93–96].\n4.3. Biochemical Component Analysis\nHorticultural crops are rich\
    \ sources of pigments such as anthocyanin, carotenoid, and\nchlorophyll, which\
    \ serve as strong antioxidants and promote human health [31]. Quantiﬁ-\ncation\
    \ of these pigments has been mainly based on laboratory extraction, which is laborious\n\
    and time-consuming. Handheld nondestructive devices such as chlorophyll meters\
    \ and\nchroma meters were developed as an option to overcome destructive measurement,\
    \ but\nthey are still limited to be used in large-scale production or breeding\
    \ programs. Hence,\nimage-based phenotyping of pigment content is receiving increasing\
    \ attention because it is\nnondestructive, robust, and fast. Anthocyanin, carotenoid,\
    \ and chlorophyll content of red\nlettuce genotypes showed a high correlation\
    \ with the vegetation indices calculated from\nimages taken by remotely piloted\
    \ aircraft [31]. The total carotenoid content in cassava root\nwas estimated from\
    \ the colorimetric indices extracted from the RGB images of root pulp.\nThe total\
    \ carotenoid content of cassava root showed a high correlation with color indices\
    \ b*\nand chroma [97]. In addition, optical sensors can be used to nondestructively\
    \ measure the\namount of nutrients in the plant, such as nitrogen, phosphorus,\
    \ and potassium, enabling\naccurate monitoring of plant growth and precise management\
    \ of crop production [98].\n4.4. Disease Detection and Quantiﬁcation\nPlant diseases\
    \ are one of the challenges of crop production worldwide, causing sig-\nniﬁcant\
    \ yield loss every year. Early detection and accurate measurement of disease is\
    \ a\nvital part of phytopathology and breeding [10]. Assessment of disease using\
    \ conventional\nvisual scores and laboratory-based analysis is time-consuming,\
    \ laborious, and subjective.\nIn recent years, rapid and high-throughput methods\
    \ for the measurement of disease ex-\ntent and severity have been widely used\
    \ based on image analysis captured by various\ntypes of sensors [22,48,79,99,100].\
    \ High-throughput detection and quantiﬁcation of dis-\nease is especially essential\
    \ in horticultural crops which are prone to diverse pathogens\nduring pre-harvest\
    \ and post-harvest handling stages. Image analysis has been widely\nused for the\
    \ detection and quantiﬁcation of horticultural crop diseases such as apple\nscab\
    \ [101,102], ﬁre blight [20,21,56,103], powdery mildew [48,104–106], Fusarium\
    \ wilt [22],\nbacterial blight [107], bacterial wilt [108], early blight, and\
    \ late blight [19,99,109–111]. Image\nanalysis can be used to closely monitor\
    \ the plant health status as the disease infection can\nbe detected at early stages\
    \ before the development of typical symptoms. This enables us to\ntake appropriate\
    \ management measures to reduce the yield or quality loss.\n4.5. Abiotic Stress\
    \ Responses\nAbiotic stresses are any kind of environmental conditions that affect\
    \ normal plant\ngrowth and yield, such as drought, salinity, heat stress, and\
    \ cold stress. Rapid and accurate\nphenotyping of plant responses to various abiotic\
    \ stresses is essential to accelerate plant\nbreeding programs dealing with the\
    \ development of climate-resilient genotypes. Image-\nbased high-throughput phenotyping\
    \ is especially important when screening a large number\nPlants 2023, 12, 2061\n\
    11 of 23\nof accessions. Various imaging techniques have been used to measure\
    \ the response of\nhorticultural plants to different abiotic stresses [82,112–114].\
    \ Hyperspectral images were\nused to detect heat stress tolerance in ginseng [115].\
    \ Cadmium stress in kale and basil\nwas detected using high-throughput hyperspectral\
    \ images. Among the vegetation indices\nanalyzed, only the anthocyanin reﬂectance\
    \ index was able to detect all levels of cadmium\nstress in both kale and basil.\
    \ The anthocyanin reﬂectance index was signiﬁcantly higher\nin cadmium-stressed\
    \ plants than in the respective controls [116]. The applications of\nhigh-throughput\
    \ phenotyping using image analysis to assess various traits in selected\nhorticultural\
    \ crops are summarized in Table 3.\nTable 3. Applications of image-based high-throughput\
    \ phenotyping in horticultural crops.\nCrop\nTrait\nSensor a\nEnvironment\nReference\n\
    Apple\nSeed morphology\nRGB\nLaboratory\n[117]\nPlant growth\nRGB\nField\n[40,118,119]\n\
    Fruit detection and counting\nRGB\nField\n[77,120,121]\nYield prediction\nMS,\
    \ RGB\nField\n[45,122]\nFruit ripening\nAerial video\nField\n[123]\nWinter dormancy\n\
    ChlF\nField\n[124]\nLow oxygen stress\nChlF\nLaboratory\n[125]\nApple scab\nThermal,\
    \ MS\nControlled\n[101,102]\nFire blight\nMS, HS\nField\n[21,56,103]\nPowdery\
    \ mildew\nRGB, MS\nField\n[48]\nDrought stress\nThermal, MS\nField\n[112]\nBanana\n\
    Plant growth\nLaser scanning\nField\n[80,126]\nPlant counting\nMS, laser scanning\n\
    Field\n[23,126]\nFruit maturity\nRGB\nLaboratory\n[92]\nYellow Sigatoka\nRGB\n\
    Field\n[17]\nMultiple diseases\nRGB\nField\n[49]\nFusarium wilt\nMS\nField\n[22]\n\
    Cabbage\nSeed morphology\nRGB\nLaboratory\n[127]\nPlant growth\nRGB\nField\n[41]\n\
    Shoot biomass\nRGB\nField\n[33]\nHeat stress\nMS\nField\n[41]\nCarrot\nRoot morphology\n\
    RGB\nLaboratory\n[39,128,129]\nCassava\nRoot bulking rate\nGPR\nField\n[130]\n\
    Root morphology\nRGB\nField and Laboratory\n[131,132]\nShoot biomass\nLiDAR\n\
    Field\n[83]\nRoot biomass\nGPR\nField\n[133]\nCarotenoid content\nRGB\nLaboratory\n\
    [97]\nStarch content\nThermal\nField\n[134]\nPlant growth\nRGB, MS\nControlled\
    \ and ﬁeld\n[42,135]\nBacterial blight\nRGB\nLaboratory\n[107]\nCitrus\nPlant\
    \ counting\nRGB\nField\n[24,81,136,137]\nPlant water status\nThermal\nControlled\n\
    [54]\nCitrus canker\nHS\nField\n[57]\nHuanglongbing (HLB)\nGPR, MS, ChlF\nField\n\
    [62,138,139]\nGrape\nBunch architecture\n3D scanner\nField\n[140–142]\nBerry counting\n\
    RGB\nField\n[143,144]\nBerry maturity\nRGB\nLaboratory\n[145]\nYield prediction\n\
    RGB, HS\nField\n[47,146–148]\nGrape yellows\nHS\nField\n[79]\nGrape leafroll\n\
    HS\nLaboratory\n[149]\nPowdery mildew\nRGB\nLaboratory\n[104]\nDrought stress\n\
    RGB, Thermal\nField\n[113]\nPlants 2023, 12, 2061\n12 of 23\nTable 3. Cont.\n\
    Crop\nTrait\nSensor a\nEnvironment\nReference\nLettuce\nSeed morphology\nRGB\n\
    Laboratory\n[150]\nLeaf semantic components\nRGB\nLaboratory\n[151]\nPlant growth\n\
    RGB\nControlled\n[43,152,153]\nShoot biomass\nChlF\nControlled\n[154]\nAnthocyanin\
    \ content\nRGB\nField\n[31,155]\nCarotenoid content\nRGB\nField\n[31,156]\nChlorophyll\
    \ content\nRGB\nField\n[31]\nDrought stress\nHS. ChlF\nControlled\n[63,157,158]\n\
    Salt stress\nChlF\nControlled\n[64]\nMango\nFruit maturity\nHS, LiDAR\nField\n\
    [159]\nFruit ripening\nHS\nField\n[60]\nFruit detection\nRGB\nField\n[30]\nDrought\
    \ stress\nThermal\nField\n[78]\nPear\nPlant growth\nRGB\nField\n[160]\nFire blight\n\
    HS\nField\n[20]\nRusset\nRGB\nLaboratory\n[50]\nPepper\nSeed quality\nX-ray CT\n\
    Laboratory\n[161]\nLeaf area\nRGB\nControlled\n[162]\nPotato\nCrop emergence\n\
    RGB\nField\n[44]\nPlant growth\nRGB\nControlled\n[163]\nTuber growth and development\n\
    X-ray CT\nControlled\n[164]\nTuber skin color\nRGB\nLaboratory\n[165]\nTuber size\n\
    RGB\nLaboratory\n[166]\nShoot biomass\nRGB, HS\nField\n[35,167,168]\nNitrogen\
    \ content\nRGB\nField\n[169]\nTuber moisture content\nHS\nLaboratory\n[170]\n\
    Stomatal conductance\nThermal\nField\n[53]\nYield prediction\nRGB, Thermal, HS\n\
    Field\n[46,171]\nEarly blight\nHS\nField\n[172]\nLate blight\nRGB, MS\nField\n\
    [19,99,173,174]\nBacterial soft rot\nRGB\nLaboratory\n[51]\nVerticillium wilt\n\
    MS\nField\n[18]\nDrought stress\nLiDAR\nControlled\n[82]\nStrawberry\nPlant growth\n\
    LiDAR\nField\n[175]\nFruit morphology\nRGB\nLaboratory\n[84,176–178]\nShoot biomass\n\
    RGB, Thermal\nField\n[28,179,180]\nYield prediction\nRGB\nField\n[76]\nLeaf gray\
    \ mold\nHS\nLaboratory\n[100]\nVerticillium wilt\nRGB, MS\nField\n[181]\nHeat\
    \ and drought stress\nHS\nControlled\n[182]\nTomato\nRoot architecture\nRGB\n\
    Controlled\n[38]\nShoot biomass\nRGB\nControlled\n[29,37]\nFruit morphology\n\
    RGB\nLaboratory\n[183]\nNitrogen, phosphorus,\npotassium content\nMS\nControlled\n\
    [98]\nChlorophyll content\nRGB, MS\nControlled\n[184]\nYield prediction\nRGB,\
    \ MS\nField\n[185,186]\nBacterial wilt\nChlF\nControlled\n[108,187]\nTYLC, early\
    \ blight, bacterial spot\nHS\nField\n[16,58]\nDrought stress\nRGB, Thermal, HS\n\
    Controlled, ﬁeld\n[188,189]\nChilling injury (seedling)\nChlF\nControlled\n[65]\n\
    Salt stress\nRGB, MS, Thermal\nField\n[36]\na Sensor: RGB—red, green, blue; IR—infrared;\
    \ HS—hyperspectral; MS—multispectral; NIR—near infrared;\nChlF—chlorophyll ﬂuorescence;\
    \ LiDAR—light detection and ranging; GPR—ground penetrating radar.\nPlants 2023,\
    \ 12, 2061\n13 of 23\n5. Current Status and Future Perspectives\nThe statistics\
    \ of publications related to high-throughput phenotyping in horticultural\ncrops\
    \ for the last two decades were surveyed and summarized in Figure 3. The number\
    \ of\npublications dealing with high-throughput phenotyping using image analysis\
    \ is increasing\nevery year and is mostly used in agriculture and biological sciences\
    \ (Figure 3a,b). Among\nthe search keywords, ‘fruit’ was the most mentioned word\
    \ in these publications and\nshowed an exponential increase in the last ﬁve years\
    \ (Figure 3c), indicating the increasing\ninterest of the researches to automate\
    \ the measurement of various fruit traits during growth,\nmaturity, ripening,\
    \ and postharvest stages. Most of these documents are research articles\n(71%),\
    \ followed by review papers (16%) (Figure 3d). Image-based phenotyping studies\n\
    are actively conducted in many countries, with USA and China taking the lead with\n\
    more number publications (Figure 3e). The applications of various types of sensors\
    \ may\ndepend on imaging platforms, accessibility, cost, and the target trait\
    \ under consideration.\nHyperspectral sensors are most popular for the phenotyping\
    \ of horticultural crops, followed\nby thermal sensors (Figure 3f).\nOne of the\
    \ upcoming challenges in phenomics is the handling of massive amounts of\ndata\
    \ generated from image-based phenotyping and the ability to extract important\
    \ knowl-\nedge from big data [3]. Here, the application of computer science is\
    \ inevitable when dealing\nwith digital phenotyping. Specialized in handling multidimensional\
    \ and multivariate\ndata automatically, it is suitable for application to high-throughput\
    \ phenotyping [190]. In\nmachine learning (ML), humans are interpretable to the\
    \ mathematical algorithm models\nthat solve given problems such as classiﬁcation,\
    \ regression, and cluster. ML provides\nprompt results for classifying and identifying\
    \ the plants or their phenotypes, predicting\nand estimating the yield or inﬂuences\
    \ of external factors [191].\nML models start by training the dataset. Various\
    \ algorithms/methods, such as support\nvector machine (SVM), decision tree, random\
    \ forest, k-nearest neighbors (KNNs), logistics,\nregressions, clustering, dimensionality\
    \ reduction, and artiﬁcial neural network (ANN),\nempower the training. All models\
    \ require the accumulation of data for their accurate and\nefﬁcient outputs [192].\
    \ Lack of sufﬁcient data learning results in common errors in the\noutput. This\
    \ issue can be easily fulﬁlled by massive data processing from image-based\nhigh-throughput\
    \ phenotyping.\nHowever, when the amount of data to be processed is extremely\
    \ large, deep learning\nwill be more compatible than the other ML algorithms [193,194].\
    \ The machine learning\napplied ANNs is well known as ‘deep learning’. Deep learning\
    \ has similar features but is\nslightly different from conventional ML types such\
    \ as supervised learning, unsupervised\nlearning, and reinforcement learning [195].\
    \ They differ depending on the absence or\npresence of human interventions in\
    \ feature extraction after their training process. Deep\nlearning requires no\
    \ human intervention if the training data well annotates the targeted\nfeature,\
    \ while ML requires feature extraction before classiﬁcation [196]. The most common\n\
    algorithms used in deep learning are convolutional neural networks (CNN), long\
    \ short-\nterm memory networks (LSTM network), recurrent neural networks (RNN),\
    \ multi-layer\nperceptron (MLP), and radial basis function networks (RBFN).\n\
    Despite their differences, both conventional MLs and deep learning provide powerful\n\
    results. Therefore, appropriate selection based on the purpose and limitations\
    \ would\nprovide more advantages during the process. For instance, ML is generally\
    \ used in the eval-\nuation and prediction of stress, biomasses, and yields [16,28,171,173,186].\
    \ Deep learning is\nmore proper in the detection, recognition, and counting of\
    \ objects in large and complex\ndatasets such as biotic/abiotic stresses and individual\
    \ plants [197,198]. Recently, ML and\nDL methods have been increasingly used in\
    \ high-throughput plant phenotyping of various\ntraits (Table 4).\nPlants 2023,\
    \ 12, 2061\n14 of 23\nPlants 2023, 12, x FOR PEER REVIEW \n14  of  24 \n \n \n\
    Figure 3. Statistics of image-based high-throughput phenotyping studies in horticultural crops dur-\n\
    ing the past two decades. (a) The number of annual publications related to image-based phenotyp-\n\
    ing of horticultural crops. (b) Major areas of research using image-based high-throughput pheno-\n\
    typing. (c) Annual number of publications with diﬀerent search keywords. (d) The type of publica-\n\
    tions related to image-based high-throughput phenotyping of horticultural crops. (e) Number of \n\
    image-based high-throughput phenotyping studies by country (top 20). (f) Type of imaging tech-\n\
    niques for high-throughput phenotyping of horticultural crops. Note: the data were obtained from \n\
    the Scopus (https://www.scopus.com) database (Elsevier, The Netherlands) accessed on 21 February \n\
    2023. The publications were searched using keywords: horticultural crop, fruit, vegetable, ornamen-\n\
    tal, ﬂower, and sensor types (RGB, thermal, hyperspectral, multispectral, X-ray CT, MRI, LiDAR, \n\
    ToF, Raman, ChlF) within the search results of high-throughput phenotyping using image analysis. \n\
    One of the upcoming challenges in phenomics is the handling of massive amounts of \n\
    data  generated  from  image-based  phenotyping  and  the  ability  to  extract \
    \ important \nknowledge from big data [3]. Here, the application of computer science is inevitable when \n\
    dealing with digital phenotyping. Specialized in handling multidimensional and multi-\n\
    variate data automatically, it is suitable for application to high-throughput\
    \ phenotyping\nFigure 3. Statistics of image-based high-throughput phenotyping\
    \ studies in horticultural crops during\nthe past two decades. (a) The number\
    \ of annual publications related to image-based phenotyping of\nhorticultural\
    \ crops. (b) Major areas of research using image-based high-throughput phenotyping.\n\
    (c) Annual number of publications with different search keywords. (d) The type\
    \ of publications\nrelated to image-based high-throughput phenotyping of horticultural\
    \ crops. (e) Number of image-\nbased high-throughput phenotyping studies by country\
    \ (top 20). (f) Type of imaging techniques for\nhigh-throughput phenotyping of\
    \ horticultural crops. Note: the data were obtained from the Scopus\n(https://www.scopus.com)\
    \ database (Elsevier, The Netherlands) accessed on 21 February 2023. The\npublications\
    \ were searched using keywords: horticultural crop, fruit, vegetable, ornamental,\
    \ ﬂower,\nand sensor types (RGB, thermal, hyperspectral, multispectral, X-ray\
    \ CT, MRI, LiDAR, ToF, Raman,\nChlF) within the search results of high-throughput\
    \ phenotyping using image analysis.\nPlants 2023, 12, 2061\n15 of 23\nTable 4.\
    \ Examples of machine learning and deep learning applications in high-throughput\
    \ plant\nphenotyping.\nAlgorithm\nApplication\nAlgorithm Type\nImaging\nTechnique\n\
    Plant Species\nPhenotypic\nTrait\nReference\nClassiﬁcation\nFaster R-CNN\nRGB\n\
    Strawberry\nYield\nprediction\n[76]\nClassiﬁcation\nConvolution Network\nX-ray\
    \ CT\nWatermelon\nSeed quality\n[70]\nClassiﬁcation\nLinear Discriminance Model,\n\
    Partially Least Square,\nMulti-Layer Perceptron,\nRadial-Basis Function Network\n\
    Hyperspectral\nGrape\nGrape yellows\n[79]\nClassiﬁcation\nYOLOv3\nMultispectral\n\
    Hamlin citrus\nPlant counting\n[81]\nDetection\n3D Point clouds\nVisible light\n\
    Apple\nFruit detection\nand counting\n[77]\nDetection\nConvolutional Neural Network,\n\
    Template Matching, Local\nMaximum Filter\nThermal\nBanana\nPlant counting\n[23]\n\
    Identiﬁcation\nNeural Network Radial Basis\nFunction, K-Nearest Neighbor\nHyperspectral\n\
    Tomato\nbacterial spot\n[58]\nIdentiﬁcation\nPCA\nFluorescence\nLetucce\nSalt\
    \ stress\n[64]\nIdentiﬁcation\nEVI2 Threshold\nMultispectral\nBanana\nPlant growth\n\
    [80]\nIdentiﬁcation\nLogistic Regression\nLiDAR\nPotato\nDrought stress\n[82]\n\
    Available aerial high-throughput phenotyping platforms target the measurement\
    \ of\nabove-ground parts, while ﬁeld-scale root phenotyping using images remains\
    \ a bottleneck.\nNovel technologies enabling root phenotyping at the ﬁeld level\
    \ will be a breakthrough,\nespecially for root and tuber crops, to capture root\
    \ system architecture and to predict\nthe yield of these underground parts at\
    \ the ﬁeld level. With the development of novel\ntechnologies with respect to\
    \ sensing and data analysis methods, image-based phenotyping\ncan discover new\
    \ traits which were difﬁcult to measure or detect using conventional\nphenotyping\
    \ [3]. The newly discovered traits, in combination with the available omics\n\
    data, need to be explored for the new frontier of crop improvement. The storage\
    \ and\nsharing of the large amount of phenomic data obtained from image-based\
    \ phenotyping\nare still challenging and need to be resolved. The data should\
    \ be standardized and easily\naccessible among research communities, academia,\
    \ industry, and farmers. Minimizing the\ncost of sensors and phenotyping platforms,\
    \ along with the automation of big data analysis\nmethods, will greatly increase\
    \ the signiﬁcance of phenomics in crop improvement to meet\nthe projected global\
    \ food demand.\n6. Conclusions\nImage-based phenotyping methods have become an\
    \ integral part of plant breeding,\ncultivation, and quality assessment of economically\
    \ important crops. This review highlights\nthe progress and applications of image-based\
    \ phenotyping as applied to horticultural\ncrops. We explored commonly used imaging\
    \ techniques: RGB, thermal, hyperspectral,\nﬂuorescence, and tomographic imaging\
    \ with their advantages and drawbacks in relation\nto high-throughput phenotyping\
    \ of horticultural crop traits. They are used to measure\nmorphological, physiological,\
    \ biochemical, disease and pest, and abiotic stresses.\nIn addition to accelerating\
    \ the breeding cycle by enabling rapid and accurate mea-\nsurement of phenotypic\
    \ traits in a large population, image-based phenotyping will help\nto monitor\
    \ the plant condition and make immediate decisions such as pesticide spray,\n\
    fertilization, or harvesting, which will greatly improve yield and the quality\
    \ of the produce.\nMoreover, the physiological processes of horticultural crops\
    \ continue even after harvest,\nand their quality is highly dependent on postharvest\
    \ storage and handling conditions.\nHence, image-based phenotyping is especially\
    \ important for postharvest phenotyping of\nhorticultural crops. This will allow\
    \ real-time monitoring of internal and external qualities of\nPlants 2023, 12,\
    \ 2061\n16 of 23\nthe horticultural product and will continue to play a signiﬁcant\
    \ role along the horticultural\nchain. Machine learning and deep learning technologies\
    \ should be well integrated into\nimage-based phenotyping to mine knowledge from\
    \ the massive amount of data generated.\nAuthor Contributions: A.M.A. wrote the\
    \ original draft, Y.K. and J.K. contributed to writing the\nmachine learning and\
    \ deep learning section, S.L.K. advised on the contents of the manuscript, and\n\
    J.B. reviewed and edited the manuscript. All authors have read and agreed to the\
    \ published version\nof the manuscript.\nFunding: This work was carried out with\
    \ the support of the “Cooperative Research Program for\nAgriculture Science and\
    \ Technology Development (Project No. PJ01673501)”, Rural Development\nAdministration,\
    \ Republic of Korea.\nInstitutional Review Board Statement: Not applicable.\n\
    Informed Consent Statement: Not applicable.\nData Availability Statement: Not\
    \ applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n\
    References\n1.\nHickey, L.T.; Hafeez, A.N.; Robinson, H.; Jackson, S.A.; Leal-Bertioli,\
    \ S.C.M.; Tester, M.; Gao, C.; Godwin, I.D.; Hayes, B.J.; Wulff,\nB.B.H. Breeding\
    \ crops to feed 10 billion. Nat. Biotechnol. 2019, 37, 744–754. [CrossRef]\n2.\n\
    Yang, W.; Feng, H.; Zhang, X.; Zhang, J.; Doonan, J.H.; Batchelor, W.D.; Xiong,\
    \ L.; Yan, J. Crop Phenomics and High-Throughput\nPhenotyping: Past Decades, Current\
    \ Challenges, and Future Perspectives. Mol. Plant 2020, 13, 187–214. [CrossRef]\n\
    3.\nSun, D.; Robbins, K.; Morales, N.; Shu, Q.; Cen, H. Advances in optical phenotyping\
    \ of cereal crops. Trends Plant Sci. 2022, 27,\n191–208. [CrossRef]\n4.\nWerner,\
    \ T. Next generation sequencing in functional genomics. Brief. Bioinform. 2010,\
    \ 11, 499–511. [CrossRef]\n5.\nFasoula, D.A.; Ioannides, I.M.; Omirou, M. Phenotyping\
    \ and Plant Breeding: Overcoming the Barriers. Front. Plant Sci. 2020, 10,\n1713.\
    \ [CrossRef]\n6.\nLi, D.; Quan, C.; Song, Z.; Li, X.; Yu, G.; Li, C.; Muhammad,\
    \ A. High-Throughput Plant Phenotyping Platform (HT3P) as a Novel\nTool for Estimating\
    \ Agronomic Traits From the Lab to the Field. Front. Bioeng. Biotechnol. 2021,\
    \ 8, 623705. [CrossRef]\n7.\nDas Choudhury, S.; Samal, A.; Awada, T. Leveraging\
    \ Image Analysis for High-Throughput Plant Phenotyping. Front. Plant Sci.\n2019,\
    \ 10, 508. [CrossRef]\n8.\nChawade, A.; van Ham, J.; Blomquist, H.; Bagge, O.;\
    \ Alexandersson, E.; Ortiz, R. High-Throughput Field-Phenotyping Tools for\nPlant\
    \ Breeding and Precision Agriculture. Agronomy 2019, 9, 258. [CrossRef]\n9.\n\
    Jangra, S.; Chaudhary, V.; Yadav, R.C.; Yadav, N.R. High-Throughput Phenotyping:\
    \ A Platform to Accelerate Crop Improvement.\nPhenomics 2021, 1, 31–53. [CrossRef]\n\
    10.\nMutka, A.M.; Bart, R.S. Image-based phenotyping of plant disease symptoms.\
    \ Front. Plant Sci. 2015, 5, 734. [CrossRef]\n11.\nLi, L.; Zhang, Q.; Huang, D.\
    \ A review of imaging techniques for plant phenotyping. Sensors 2014, 14, 20078–20111.\
    \ [CrossRef]\n12.\nHe, F.; Meng, Q.; Tang, L.; Huang, X.; Lu, X.; Wang, R.; Zhang,\
    \ K.; Li, Y. Research progress in hyperspectral imaging technology\nfor fruit\
    \ quality detection. J. Fruit Sci. 2021, 38, 1590–1599.\n13.\nHaque, S.; Lobaton,\
    \ E.; Nelson, N.; Yencho, G.C.; Pecota, K.V.; Mierop, R.; Kudenov, M.W.; Boyette,\
    \ M.; Williams, C.M. Computer\nvision approach to characterize size and shape\
    \ phenotypes of horticultural crops using high-throughput imagery. Comput.\nElectron.\
    \ Agric. 2021, 182, 106011. [CrossRef]\n14.\nLu, Y.; Saeys, W.; Kim, M.; Peng,\
    \ Y.; Lu, R. Hyperspectral imaging technology for quality and safety evaluation\
    \ of horticultural\nproducts: A review and celebration of the past 20-year progress.\
    \ Postharvest. Biol. Technol. 2020, 170, 111318. [CrossRef]\n15.\nDu, J.; Fan,\
    \ J.; Wang, C.; Lu, X.; Zhang, Y.; Wen, W.; Liao, S.; Yang, X.; Guo, X.; Zhao,\
    \ C. Greenhouse-based vegetable high-\nthroughput phenotyping platform and trait\
    \ evaluation for large-scale lettuces. Comput. Electron. Agric. 2021, 186, 106193.\n\
    [CrossRef]\n16.\nAbdulridha, J.; Ampatzidis, Y.; Qureshi, J.; Roberts, P. Laboratory\
    \ and UAV-based identiﬁcation and classiﬁcation of tomato\nyellow leaf curl, bacterial\
    \ spot, and target spot diseases in tomato utilizing hyperspectral imaging and\
    \ machine learning. Remote\nSens. 2020, 12, 2732. [CrossRef]\n17.\nCalou, V.B.C.;\
    \ Teixeira, A.D.S.; Moreira, L.C.J.; Lima, C.S.; de Oliveira, J.B.; de Oliveira,\
    \ M.R.R. The use of UAVs in monitoring\nyellow sigatoka in banana. Biosyst. Eng.\
    \ 2020, 193, 115–125. [CrossRef]\n18.\nLizarazo, I.; Rodriguez, J.L.; Cristancho,\
    \ O.; Olaya, F.; Duarte, M.; Prieto, F. Identiﬁcation of symptoms related to potato\n\
    Verticillium wilt from UAV-based multispectral imagery using an ensemble of gradient\
    \ boosting machines. Smart Agric. Technol.\n2023, 3, 100138. [CrossRef]\n19.\n\
    Rodríguez, J.; Lizarazo, I.; Prieto, F.; Angulo-Morales, V. Assessment of potato\
    \ late blight from UAV-based multispectral imagery.\nComput. Electron. Agric.\
    \ 2021, 184, 106061. [CrossRef]\nPlants 2023, 12, 2061\n17 of 23\n20.\nSchoofs,\
    \ H.; Delalieux, S.; Deckers, T.; Bylemans, D. Fire blight monitoring in pear\
    \ orchards by unmanned airborne vehicles\n(UAV) systems carrying spectral sensors.\
    \ Agronomy 2020, 10, 615. [CrossRef]\n21.\nXiao, D.; Pan, Y.; Feng, J.; Yin, J.;\
    \ Liu, Y.; He, L. Remote sensing detection algorithm for apple ﬁre blight based\
    \ on UAV multispectral\nimage. Comput. Electron. Agric. 2022, 199, 107137. [CrossRef]\n\
    22.\nZhang, S.; Li, X.; Ba, Y.; Lyu, X.; Zhang, M.; Li, M. Banana Fusarium Wilt\
    \ Disease Detection by Supervised and Unsupervised\nMethods from UAV-Based Multispectral\
    \ Imagery. Remote Sens. 2022, 14, 1231. [CrossRef]\n23.\nAeberli, A.; Johansen,\
    \ K.; Robson, A.; Lamb, D.W.; Phinn, S. Detection of banana plants using multi-temporal\
    \ multispectral UAV\nimagery. Remote Sens. 2021, 13, 2123. [CrossRef]\n24.\nDonmez,\
    \ C.; Villi, O.; Berberoglu, S.; Cilek, A. Computer vision-based citrus tree detection\
    \ in a cultivated environment using\nUAV imagery. Comput. Electron. Agric. 2021,\
    \ 187, 106273. [CrossRef]\n25.\nOsco, L.P.; Nogueira, K.; Marques Ramos, A.P.;\
    \ Faita Pinheiro, M.M.; Furuya, D.E.G.; Gonçalves, W.N.; de Castro Jorge, L.A.;\n\
    Marcato Junior, J.; dos Santos, J.A. Semantic segmentation of citrus-orchard using\
    \ deep neural networks and multispectral\nUAV-based imagery. Precis. Agric. 2021,\
    \ 22, 1171–1188. [CrossRef]\n26.\nBallesteros, R.; Ortega, J.F.; Hernandez, D.;\
    \ Moreno, M.A. Onion biomass monitoring using UAV-based RGB imaging. Precis.\n\
    Agric. 2018, 19, 840–857. [CrossRef]\n27.\nLiu, Y.; Feng, H.; Yue, J.; Fan, Y.;\
    \ Jin, X.; Zhao, Y.; Song, X.; Long, H.; Yang, G. Estimation of Potato Above-Ground\
    \ Biomass Using\nUAV-Based Hyperspectral images and Machine-Learning Regression.\
    \ Remote Sens. 2022, 14, 5449. [CrossRef]\n28.\nZheng, C.; Abd-Elrahman, A.; Whitaker,\
    \ V.; Dalid, C. Prediction of Strawberry Dry Biomass from UAV Multispectral Imagery\n\
    Using Multiple Machine Learning Methods. Remote Sens. 2022, 14, 4511. [CrossRef]\n\
    29.\nJohansen, K.; Morton, M.J.L.; Malbeteau, Y.; Aragon, B.; Al-Mashharawi, S.;\
    \ Ziliani, M.G.; Angel, Y.; Fiene, G.; Negrão, S.; Mousa,\nM.A.A.; et al. Predicting\
    \ Biomass and Yield in a Tomato Phenotyping Experiment Using UAV Imagery and Random\
    \ Forest. Front.\nArtif. Intell. 2020, 3, 28. [CrossRef]\n30.\nXiong, J.; Liu,\
    \ Z.; Chen, S.; Liu, B.; Zheng, Z.; Zhong, Z.; Yang, Z.; Peng, H. Visual detection\
    \ of green mangoes by an unmanned\naerial vehicle in orchards based on a deep\
    \ learning method. Biosyst. Eng. 2020, 194, 261–272. [CrossRef]\n31.\nClemente,\
    \ A.A.; Maciel, G.M.; Siquieroli, A.C.S.; Gallis, R.B.D.A.; Pereira, L.M.; Duarte,\
    \ J.G. High-throughput phenotyping to\ndetect anthocyanins, chlorophylls, and\
    \ carotenoids in red lettuce germplasm. Int. J. Appl. Earth Obs. Geoinf. 2021,\
    \ 103, 102533.\n[CrossRef]\n32.\nKim, J.; Chung, Y.S. A short review of RGB sensor\
    \ applications for accessible high-throughput phenotyping. J. Crop Sci. Biotechnol.\n\
    2021, 24, 495–499. [CrossRef]\n33.\nSinde-González, I.; Gómez-López, J.P.; Tapia-Navarro,\
    \ S.A.; Murgueitio, E.; Falconí, C.; Benítez, F.L.; Toulkeridis, T. Determining\n\
    the Effects of Nanonutrient Application in Cabbage (Brassica oleracea var. capitate\
    \ L.) Using Spectrometry and Biomass Estimation\nwith UAV. Agronomy 2022, 12,\
    \ 81. [CrossRef]\n34.\nLi, B.; Xu, X.; Zhang, L.; Han, J.; Bian, C.; Li, G.; Liu,\
    \ J.; Jin, L. Above-ground biomass estimation and yield prediction in potato by\n\
    using UAV-based RGB and hyperspectral imaging. ISPRS J. Photogramm. Remote Sens.\
    \ 2020, 162, 161–172. [CrossRef]\n35.\nLiu, Y.; Feng, H.; Yue, J.; Jin, X.; Li,\
    \ Z.; Yang, G. Estimation of potato above-ground biomass based on unmanned aerial\
    \ vehicle\nred-green-blue images with different texture features and crop height.\
    \ Front. Plant Sci. 2022, 13, 938216. [CrossRef]\n36.\nJohansen, K.; Morton, M.J.L.;\
    \ Malbeteau, Y.M.; Aragon, B.; Al-Mashharawi, S.K.; Ziliani, M.G.; Angel, Y.;\
    \ Fiene, G.M.; Negrão,\nS.S.C.; Mousa, M.A.A.; et al. Unmanned aerial vehicle-based\
    \ phenotyping using morphometric and spectral analysis can quantify\nresponses\
    \ of wild tomato plants to salinity stress. Front. Plant Sci. 2019, 10, 370. [CrossRef]\n\
    37.\nLaxman, R.H.; Hemamalini, P.; Bhatt, R.M.; Sadashiva, A.T. Non-invasive quantiﬁcation\
    \ of tomato (Solanum lycopersicum L.) plant\nbiomass through digital imaging using\
    \ phenomics platform. Indian J. Plant Physiol. 2018, 23, 369–375. [CrossRef]\n\
    38.\nAlaguero-Cordovilla, A.; Gran-Gómez, F.J.; Tormos-Moltó, S.; Pérez-Pérez,\
    \ J.M. Morphological characterization of root system\narchitecture in diverse\
    \ tomato genotypes during early growth. Int. J. Mol. Sci. 2018, 19, 3888. [CrossRef]\n\
    39.\nBrainard, S.H.; Bustamante, J.A.; Dawson, J.C.; Spalding, E.P.; Goldman,\
    \ I.L. A Digital Image-Based Phenotyping Platform for\nAnalyzing Root Shape Attributes\
    \ in Carrot. Front. Plant Sci. 2021, 12, 690031. [CrossRef]\n40.\nHobart, M.;\
    \ Pﬂanz, M.; Weltzien, C.; Schirrmann, M. Growth height determination of tree\
    \ walls for precise monitoring in apple\nfruit production using UAV photogrammetry.\
    \ Remote Sens. 2020, 12, 1656. [CrossRef]\n41.\nKim, D.W.; Yun, H.S.; Jeong, S.J.;\
    \ Kwon, Y.S.; Kim, S.G.; Lee, W.S.; Kim, H.J. Modeling and testing of growth status\
    \ for Chinese\ncabbage and white radish with UAV-based RGB imagery. Remote Sens.\
    \ 2018, 10, 563. [CrossRef]\n42.\nWasonga, D.O.; Yaw, A.; Kleemola, J.; Alakukku,\
    \ L.; Mäkelä, P.S.A. Red-green-blue and multispectral imaging as potential tools\n\
    for estimating growth and nutritional performance of cassava under deﬁcit irrigation\
    \ and potassium fertigation. Remote Sens.\n2021, 13, 598. [CrossRef]\n43.\nGang,\
    \ M.S.; Kim, H.J.; Kim, D.W. Estimation of Greenhouse Lettuce Growth Indices Based\
    \ on a Two-Stage CNN Using RGB-D\nImages. Sensors 2022, 22, 5499. [CrossRef] [PubMed]\n\
    44.\nLi, B.; Xu, X.; Han, J.; Zhang, L.; Bian, C.; Jin, L.; Liu, J. The estimation\
    \ of crop emergence in potatoes by UAV RGB imagery. Plant\nMethods 2019, 15, 15.\
    \ [CrossRef] [PubMed]\n45.\nChen, R.; Zhang, C.; Xu, B.; Zhu, Y.; Zhao, F.; Han,\
    \ S.; Yang, G.; Yang, H. Predicting individual apple tree yield using UAV\nmulti-source\
    \ remote sensing data and ensemble learning. Comput. Electron. Agric. 2022, 201,\
    \ 107275. [CrossRef]\nPlants 2023, 12, 2061\n18 of 23\n46.\nElsayed, S.; El-Hendawy,\
    \ S.; Khadr, M.; Elsherbiny, O.; Al-Suhaibani, N.; Alotaibi, M.; Tahir, M.U.;\
    \ Darwish, W. Combining\nthermal and RGB imaging indices with multivariate and\
    \ data-driven modeling to estimate the growth, water status, and yield of\npotato\
    \ under different drip irrigation regimes. Remote Sens. 2021, 13, 1679. [CrossRef]\n\
    47.\nKurtser, P.; Ringdahl, O.; Rotstein, N.; Berenstein, R.; Edan, Y. In-ﬁeld\
    \ grape cluster size assessment for vine yield estimation\nusing a mobile robot\
    \ and a consumer level RGB-D Camera. IEEE Robot. Autom. Lett. 2020, 5, 2031–2038.\
    \ [CrossRef]\n48.\nChandel, A.K.; Khot, L.R.; Sallato, B. Apple powdery mildew\
    \ infestation detection and mapping using high-resolution visible\nand multispectral\
    \ aerial imaging technique. Sci. Hortic. 2021, 287, 110228. [CrossRef]\n49.\n\
    Gomez Selvaraj, M.; Vergara, A.; Montenegro, F.; Alonso Ruiz, H.; Safari, N.;\
    \ Raymaekers, D.; Ocimati, W.; Ntamwira, J.; Tits, L.;\nOmondi, A.B.; et al. Detection\
    \ of banana plants and their major diseases through aerial images and machine\
    \ learning methods: A\ncase study in DR Congo and Republic of Benin. ISPRS J.\
    \ Photogramm. Remote Sens. 2020, 169, 110–124. [CrossRef]\n50.\nKim, Y.; Oh, S.;\
    \ Kim, K.; Jeong, H.W.; Kim, D. Bi-dimensional Image Analysis for the Phenotypic\
    \ Evaluation of Russet in Asian\nPear (Pyrus spp.). Hortic. Sci. Technol. 2022,\
    \ 40, 192–198.\n51.\nLee, U.; Silva, R.R.; Kim, C.; Kim, H.; Heo, S.; Park, I.S.;\
    \ Kim, W.; Jansky, S.; Chung, Y.S. Image Analysis for Measuring Disease\nSymptom\
    \ to Bacterial Soft Rot in Potato. Am. J. Potato Res. 2019, 90, 303–313. [CrossRef]\n\
    52.\nAhmadi, S.H.; Agharezaee, M.; Kamgar-Haghighi, A.A.; Sepaskhah, A.R. Comparing\
    \ canopy temperature and leaf water potential\nas irrigation scheduling criteria\
    \ of potato in water-saving irrigation strategies. Int. J. Plant Prod. 2017, 11,\
    \ 333–348.\n53.\nPrashar, A.; Yildiz, J.; McNicol, J.W.; Bryan, G.J.; Jones, H.G.\
    \ Infra-red Thermography for High Throughput Field Phenotyping in\nSolanum tuberosum.\
    \ PLoS ONE 2013, 8, e65816. [CrossRef] [PubMed]\n54.\nVieira, G.H.S.; Ferrarezi,\
    \ R.S. Use of thermal imaging to assess water status in citrus plants in greenhouses.\
    \ Horticulturae 2021, 7,\n249. [CrossRef]\n55.\nSari´c, R.; Nguyen, V.D.; Burge,\
    \ T.; Berkowitz, O.; Trtílek, M.; Whelan, J.; Lewsey, M.G.; ˇCustovi´c, E. Applications\
    \ of hyperspectral\nimaging in plant phenotyping. Trends Plant Sci. 2022, 27,\
    \ 301–315. [CrossRef]\n56.\nSkoneczny, H.; Kubiak, K.; Spiralski, M.; Kotlarz,\
    \ J. Fire blight disease detection for apple trees: Hyperspectral analysis of\
    \ healthy,\ninfected and dry leaves. Remote Sens. 2020, 12, 2101. [CrossRef]\n\
    57.\nAbdulridha, J.; Batuman, O.; Ampatzidis, Y. UAV-based remote sensing technique\
    \ to detect citrus canker disease utilizing\nhyperspectral imaging and machine\
    \ learning. Remote Sens. 2019, 11, 1373. [CrossRef]\n58.\nAbdulridha, J.; Ampatzidis,\
    \ Y.; Kakarla, S.C.; Roberts, P. Detection of target spot and bacterial spot diseases\
    \ in tomato using\nUAV-based and benchtop-based hyperspectral imaging techniques.\
    \ Precis. Agric. 2020, 21, 955–978. [CrossRef]\n59.\nShao, Y.; Wang, Y.; Xuan,\
    \ G.; Gao, Z.; Hu, Z.; Gao, C.; Wang, K. Assessment of Strawberry Ripeness Using\
    \ Hyperspectral Imaging.\nAnal. Lett. 2020, 54, 1547–1560. [CrossRef]\n60.\nGutiérrez,\
    \ S.; Wendel, A.; Underwood, J. Spectral ﬁlter design based on in-ﬁeld hyperspectral\
    \ imaging and machine learning for\nmango ripeness estimation. Comput. Electron.\
    \ Agric. 2019, 164, 104890. [CrossRef]\n61.\nMaxwell, K.; Johnson, G.N. Chlorophyll\
    \ ﬂuorescence—A practical guide. J. Exp. Bot. 2000, 51, 659–668. [CrossRef] [PubMed]\n\
    62.\nWeng, H.Y.; Zeng, Y.B.; Cen, H.Y.; He, M.B.; Meng, Y.Q.; Liu, Y.; Wan, L.;\
    \ Xu, H.X.; Li, H.Y.; Fang, H.; et al. Characterization and\ndetection of leaf\
    \ photosynthetic response to citrus huanglongbing from cool to hot seasons in\
    \ two orchards. Trans. ASABE 2020,\n63, 501–512. [CrossRef]\n63.\nKumar, P.; Eriksen,\
    \ R.L.; Simko, I.; Mou, B. Molecular Mapping of Water-Stress Responsive Genomic\
    \ Loci in Lettuce (Lactuca\nspp.) Using Kinetics Chlorophyll Fluorescence, Hyperspectral\
    \ Imaging and Machine Learning. Front. Genet. 2021, 12, 634554.\n[CrossRef] [PubMed]\n\
    64.\nAdhikari, N.D.; Simko, I.; Mou, B. Phenomic and physiological analysis of\
    \ salinity effects on lettuce. Sensors 2019, 19, 4814.\n[CrossRef] [PubMed]\n\
    65.\nDong, Z.; Men, Y.; Li, Z.; Zou, Q.; Ji, J. Chlorophyll ﬂuorescence imaging\
    \ as a tool for analyzing the effects of chilling injury on\ntomato seedlings.\
    \ Sci. Hortic. 2019, 246, 490–497. [CrossRef]\n66.\nMetzner, R.; Eggert, A.; van\
    \ Dusschoten, D.; Pﬂugfelder, D.; Gerth, S.; Schurr, U.; Uhlmann, N.; Jahnke,\
    \ S. Direct comparison of\nMRI and X-ray CT technologies for 3D imaging of root\
    \ systems in soil: Potential and challenges for root trait quantiﬁcation. Plant\n\
    Methods 2015, 11, 17. [CrossRef]\n67.\nBorisjuk, L.; Rolletschek, H.; Neuberger,\
    \ T. Surveying the plant’s world by magnetic resonance imaging. Plant J. 2012,\
    \ 70, 129–146.\n[CrossRef]\n68.\nPiovesan, A.; Vancauwenberghe, V.; Van De Looverbosch,\
    \ T.; Verboven, P.; Nicolaï, B. X-ray computed tomography for 3D plant\nimaging.\
    \ Trends Plant Sci. 2021, 26, 1171–1185. [CrossRef]\n69.\nLiu, W.; Liu, C.; Jin,\
    \ J.; Li, D.; Fu, Y.; Yuan, X. High-Throughput Phenotyping of Morphological Seed\
    \ and Fruit Characteristics\nUsing X-Ray Computed Tomography. Front. Plant Sci.\
    \ 2020, 11, 601475. [CrossRef]\n70.\nAhmed, M.R.; Yasmin, J.; Park, E.; Kim, G.;\
    \ Kim, M.S.; Wakholi, C.; Mo, C.; Cho, B.K. Classiﬁcation of watermelon seeds\
    \ using\nmorphological patterns of x-ray imaging: A comparison of conventional\
    \ machine learning and deep learning. Sensors 2020, 20,\n6753. [CrossRef]\n71.\n\
    Agostini, A.; Alenyà, G.; Fischbach, A.; Scharr, H.; Wörgötter, F.; Torras, C.\
    \ A cognitive architecture for automatic gardening.\nComput. Electron. Agric.\
    \ 2017, 138, 69–79. [CrossRef]\n72.\nKim, D.M.; Zhang, H.; Zhou, H.; Du, T.; Wu,\
    \ Q.; Mockler, T.C.; Berezin, M.Y. Highly sensitive image-derived indices of\n\
    water-stressed plants using hyperspectral imaging in SWIR and histogram analysis.\
    \ Sci. Rep. 2015, 5, 15919. [CrossRef] [PubMed]\nPlants 2023, 12, 2061\n19 of\
    \ 23\n73.\nBlonder, B.; De Carlo, F.; Moore, J.; Rivers, M.; Enquist, B.J. X-ray\
    \ imaging of leaf venation networks. N. Phytol. 2012, 196,\n1274–1282. [CrossRef]\
    \ [PubMed]\n74.\nKim, J.Y. Roadmap to High Throughput Phenotyping for Plant Breeding.\
    \ J. Biosyst. Eng. 2020, 45, 43–55. [CrossRef]\n75.\nBian, L.; Zhang, H.; Ge,\
    \ Y.; ˇCepl, J.; Stejskal, J.; El-Kassaby, Y.A. Closing the gap between phenotyping\
    \ and genotyping: Review of\nadvanced, image-based phenotyping technologies in\
    \ forestry. Ann. For. Sci. 2022, 79, 22. [CrossRef]\n76.\nChen, Y.; Lee, W.S.;\
    \ Gan, H.; Peres, N.; Fraisse, C.; Zhang, Y.; He, Y. Strawberry yield prediction\
    \ based on a deep neural network\nusing high-resolution aerial orthoimages. Remote\
    \ Sens. 2019, 11, 1584. [CrossRef]\n77.\nZine-El-Abidine, M.; Dutagaci, H.; Galopin,\
    \ G.; Rousseau, D. Assigning apples to individual trees in dense orchards using\
    \ 3D\ncolour point clouds. Biosyst. Eng. 2021, 209, 30–52. [CrossRef]\n78.\nTaria,\
    \ S.; Alam, B.; Rane, J.; Kumar, M.; Babar, R.; Singh, N.P. Deciphering endurance\
    \ capacity of mango tree (Mangifera indica L.)\nto desiccation stress using modern\
    \ physiological tools. Sci. Hortic. 2022, 303, 111247. [CrossRef]\n79.\nBendel,\
    \ N.; Backhaus, A.; Kicherer, A.; Köckerling, J.; Maixner, M.; Jarausch, B.; Biancu,\
    \ S.; Klück, H.C.; Seiffert, U.; Voegele,\nR.T.; et al. Detection of two different\
    \ grapevine yellows in Vitis vinifera using hyperspectral imaging. Remote Sens.\
    \ 2020, 12, 4151.\n[CrossRef]\n80.\nAeberli, A.; Phinn, S.; Johansen, K.; Robson,\
    \ A.; Lamb, D.W. Characterisation of Banana Plant Growth Using High-Spatiotemporal-\n\
    Resolution Multispectral UAV Imagery. Remote Sens. 2023, 15, 679. [CrossRef]\n\
    81.\nAmpatzidis, Y.; Partel, V. UAV-based high throughput phenotyping in citrus\
    \ utilizing multispectral imaging and artiﬁcial\nintelligence. Remote Sens. 2019,\
    \ 11, 410. [CrossRef]\n82.\nMulugeta Aneley, G.; Haas, M.; Köhl, K. LIDAR-Based\
    \ Phenotyping for Drought Response and Drought Tolerance in Potato.\nPotato Res.\
    \ 2022. [CrossRef]\n83.\nAdams, T.; Bruton, R.; Ruiz, H.; Barrios-Perez, I.; Selvaraj,\
    \ M.G.; Hays, D.B. Prediction of aboveground biomass of three cassava\n(Manihot\
    \ esculenta) genotypes using a terrestrial laser scanner. Remote Sens. 2021, 13,\
    \ 1272. [CrossRef]\n84.\nNagamatsu, S.; Tsubone, M.; Wada, T.; Oku, K.; Mori,\
    \ M.; Hirata, C.; Hayashi, A.; Tanabata, T.; Isobe, S.; Takata, K.; et al.\nStrawberry\
    \ fruit shape: Quantiﬁcation by image analysis and qtl detection by genome-wide\
    \ association analysis. Breed Sci. 2021,\n71, 167–175. [CrossRef]\n85.\nAlfatni,\
    \ M.S.M.; Shariff, A.R.M.; Shafri, H.Z.M.; Saaed, O.M.B.; Eshanta, O.M. Oil palm\
    \ fruit bunch grading system using red,\ngreen and blue digital number. J. Appl.\
    \ Sci. 2008, 8, 1444–1452. [CrossRef]\n86.\nRifna, E.J.; Dwivedi, M. Emerging\
    \ nondestructive technologies for quality assessment of fruits, vegetables, and\
    \ cereals. In Food\nLosses, Sustainable Postharvest and Food Technologies, 1st\
    \ ed.; Galanakis, C.M., Ed.; Elsevier: Amsterdam, The Netherlands, 2021; pp.\n\
    219–253.\n87.\nSubhashree, S.N.; Sunoj, S.; Xue, J.; Bora, G.C. Quantiﬁcation\
    \ of browning in apples using colour and textural features by image\nanalysis.\
    \ Food Qual. Saf. 2017, 1, 221–226. [CrossRef]\n88.\nOIV. OIV Descriptor List\
    \ for Grape Varieties and Vitis Species, 2nd ed.; International Organisation of\
    \ Vine and Wine: Paris, France,\n2009.\n89.\nUnderhill, A.N.; Hirsch, C.D.; Clark,\
    \ M.D. Evaluating and Mapping Grape Color Using Image-Based Phenotyping. Plant\n\
    Phenomics 2020, 2020, 8086309. [CrossRef]\n90.\nKuhl, F.P.; Giardina, C.R. Elliptic\
    \ Fourier features of a closed contour. Comput. Electron. Agric. 1982, 18, 236–258.\
    \ [CrossRef]\n91.\nKaur, H.; Sawhney, B.K.; Jawandha, S.K. Evaluation of plum\
    \ fruit maturity by image processing techniques. J. Food Sci. Technol.\n2018,\
    \ 55, 3008–3015. [CrossRef]\n92.\nSurya Prabha, D.; Satheesh Kumar, J. Assessment\
    \ of banana fruit maturity by image processing technique. J. Food Sci. Technol.\n\
    2015, 52, 1316–1327. [CrossRef]\n93.\nElMasry, G.; Wang, N.; Vigneault, C. Detecting\
    \ chilling injury in Red Delicious apple using hyperspectral imaging and neural\n\
    networks. Postharvest Biol. Technol. 2009, 52, 1–8. [CrossRef]\n94.\nPan, L.;\
    \ Zhang, Q.; Zhang, W.; Sun, Y.; Hu, P.; Tu, K. Detection of cold injury in peaches\
    \ by hyperspectral reﬂectance imaging and\nartiﬁcial neural network. Food Chem.\
    \ 2016, 192, 134–141. [CrossRef] [PubMed]\n95.\nGe, Y.; Tu, S. Identiﬁcation of\
    \ Chilling Injury in Kiwifruit Using Hyperspectral Structured-Illumination Reﬂectance\
    \ Imaging\nSystem (SIRI) with Support Vector Machine (SVM) Modelling. Anal. Lett.\
    \ 2022, 56, 2040–2052. [CrossRef]\n96.\nLu, Y.; Lu, R. Detection of chilling injury\
    \ in pickling cucumbers using dual-band chlorophyll ﬂuorescence imaging. Foods\
    \ 2021, 10,\n1094. [CrossRef] [PubMed]\n97.\nDe Carvalho, R.R.B.; Cortes, D.F.M.;\
    \ e Sousa, M.B.; de Oliveira, L.A.; de Oliveira, E.J. Image-based phenotyping\
    \ of cassava roots\nfor diversity studies and carotenoids prediction. PLoS ONE\
    \ 2022, 17, e0263326. [CrossRef] [PubMed]\n98.\nSun, G.; Ding, Y.; Wang, X.; Lu,\
    \ W.; Sun, Y.; Yu, H. Nondestructive determination of nitrogen, phosphorus and\
    \ potassium contents\nin greenhouse tomato plants based on multispectral three-dimensional\
    \ imaging. Sensors 2019, 19, 5295. [CrossRef]\n99.\nSugiura, R.; Tsuda, S.; Tamiya,\
    \ S.; Itoh, A.; Nishiwaki, K.; Murakami, N.; Shibuya, Y.; Hirafuji, M.; Nuske,\
    \ S. Field phenotyping\nsystem for the assessment of potato late blight resistance\
    \ using RGB imagery from an unmanned aerial vehicle. Biosyst. Eng. 2016,\n148,\
    \ 1–10. [CrossRef]\n100. Wu, G.; Fang, Y.; Jiang, Q.; Cui, M.; Li, N.; Ou, Y.;\
    \ Diao, Z.; Zhang, B. Early identiﬁcation of strawberry leaves disease utilizing\n\
    hyperspectral imaging combing with spectral features, multiple vegetation indices\
    \ and textural features. Comput. Electron. Agric.\n2023, 204, 107553. [CrossRef]\n\
    Plants 2023, 12, 2061\n20 of 23\n101. Belin, É.; Rousseau, D.; Boureau, T.; Cafﬁer,\
    \ V. Thermography versus chlorophyll ﬂuorescence imaging for detection and\nquantiﬁcation\
    \ of apple scab. Comput. Electron. Agric. 2013, 90, 159–163. [CrossRef]\n102.\
    \ Bleasdale, A.J.; Blackburn, G.A.; Whyatt, J.D. Feasibility of detecting apple\
    \ scab infections using low-cost sensors and interpreting\nradiation interactions\
    \ with scab lesions. Int. J. Remote Sens. 2022, 43, 4984–5005. [CrossRef]\n103.\
    \ Jarolmasjed, S.; Sankaran, S.; Marzougui, A.; Kostick, S.; Si, Y.; Quirós Vargas,\
    \ J.J.; Evans, K. High-throughput phenotyping of ﬁre\nblight disease symptoms\
    \ using sensing techniques in apple. Front. Plant Sci. 2019, 10, 576. [CrossRef]\
    \ [PubMed]\n104. Qiu, T.; Underhill, A.; Sapkota, S.; Cadle-Davidson, L.; Jiang,\
    \ Y. High throughput saliency-based quantiﬁcation of grape powdery\nmildew at\
    \ the microscopic level for disease resistance breeding. Hortic. Res. 2022, 9,\
    \ uhac187. [CrossRef] [PubMed]\n105. Sultan Mahmud, M.; Zaman, Q.U.; Esau, T.J.;\
    \ Price, G.W.; Prithiviraj, B. Development of an artiﬁcial cloud lighting condition\n\
    system using machine vision for strawberry powdery mildew disease detection. Comput.\
    \ Electron. Agric. 2019, 158, 219–225.\n[CrossRef]\n106. Tapia, R.; Abd-Elrahman,\
    \ A.; Osorio, L.; Lee, S.; Whitaker, V.M. Combining canopy reﬂectance spectrometry\
    \ and genome-wide\nprediction to increase response to selection for powdery mildew\
    \ resistance in cultivated strawberry. J. Exp. Bot. 2022, 73,\n5322–5335. [CrossRef]\
    \ [PubMed]\n107. Elliott, K.; Berry, J.C.; Kim, H.; Bart, R.S. A comparison of\
    \ ImageJ and machine learning based image analysis methods to measure\ncassava\
    \ bacterial blight disease severity. Plant Methods 2022, 18, 86. [CrossRef]\n\
    108. Kim, J.H.; Bhandari, S.R.; Chae, S.Y.; Cho, M.C.; Lee, J.G. Application of\
    \ maximum quantum yield, a parameter of chlorophyll\nﬂuorescence, for early determination\
    \ of bacterial wilt in tomato seedlings. Hortic. Environ. Biotechnol. 2019, 60,\
    \ 821–829. [CrossRef]\n109. Kundu, R.; Dutta, D.; MK, N.; Chakrabarty, A. Near\
    \ Real Time Monitoring of Potato Late Blight Disease Severity using Field\nBased\
    \ Hyperspectral Observation. Smart Agric. Technol. 2021, 1, 100019. [CrossRef]\n\
    110. Hou, C.; Zhuang, J.; Tang, Y.; He, Y.; Miao, A.; Huang, H.; Luo, S. Recognition\
    \ of early blight and late blight diseases on potato\nleaves based on graph cut\
    \ segmentation. J. Agric. Food Res. 2021, 5, 100154. [CrossRef]\n111. Franceschini,\
    \ M.H.D.; Bartholomeus, H.; van Apeldoorn, D.F.; Suomalainen, J.; Kooistra, L.\
    \ Feasibility of unmanned aerial vehicle\noptical imagery for early detection\
    \ and severity assessment of late blight in Potato. Remote Sens. 2019, 11, 224.\
    \ [CrossRef]\n112. Virlet, N.; Costes, E.; Martinez, S.; Kelner, J.J.; Regnard,\
    \ J.L. Multispectral airborne imagery in the ﬁeld reveals genetic deter-\nminisms\
    \ of morphological and transpiration traits of an apple tree hybrid population\
    \ in response to water deﬁcit. J. Exp. Bot.\n2015, 66, 5453–5465. [CrossRef]\n\
    113. Briglia, N.; Montanaro, G.; Petrozza, A.; Summerer, S.; Cellini, F.; Nuzzo,\
    \ V. Drought phenotyping in Vitis vinifera using RGB and\nNIR imaging. Sci. Hortic.\
    \ 2019, 256, 108555. [CrossRef]\n114. Chen, S.; Guo, Y.; Sirault, X.; Stefanova,\
    \ K.; Saradadevi, R.; Turner, N.C.; Nelson, M.N.; Furbank, R.T.; Siddique, K.H.M.;\
    \ Cowling,\nW.A. Nondestructive phenomic tools for the prediction of heat and\
    \ drought tolerance at anthesis in Brassica species. Plant Phenom.\n2019, 2019,\
    \ 3264872. [CrossRef] [PubMed]\n115. Faqeerzada, M.A.; Park, E.; Kim, T.; Kim,\
    \ M.S.; Baek, I.; Joshi, R.; Kim, J.; Cho, B.K. Fluorescence Hyperspectral Imaging\
    \ for Early\nDiagnosis of Heat-Stressed Ginseng Plants. Appl Sci. 2023, 13, 31.\n\
    116. Zea, M.; Souza, A.; Yang, Y.; Lee, L.; Nemali, K.; Hoagland, L. Leveraging\
    \ high-throughput hyperspectral imaging technology\nto detect cadmium stress in\
    \ two leafy green crops and accelerate soil remediation efforts. Environ. Pollut.\
    \ 2022, 292, 118405.\n[CrossRef] [PubMed]\n117. Ropelewska, E.; Rutkowski, K.P.\
    \ Cultivar discrimination of stored apple seeds based on geometric features determined\
    \ using\nimage analysis. J. Stored Prod. Res. 2021, 92, 101804. [CrossRef]\n118.\
    \ Wu, J.; Yang, G.; Yang, H.; Zhu, Y.; Li, Z.; Lei, L.; Zhao, C. Extracting apple\
    \ tree crown information from remote imagery using\ndeep learning. Comput. Electron.\
    \ Agric. 2020, 174, 105504. [CrossRef]\n119. Sun, X.; Fang, W.; Gao, C.; Fu, L.;\
    \ Majeed, Y.; Liu, X.; Gao, F.; Yang, R.; Li, R. Remote estimation of grafted\
    \ apple tree trunk\ndiameter in modern orchard with RGB and point cloud based\
    \ on SOLOv. Comput. Electron. Agric. 2022, 199, 107209. [CrossRef]\n120. Fu, L.;\
    \ Majeed, Y.; Zhang, X.; Karkee, M.; Zhang, Q. Faster R–CNN–based apple detection\
    \ in dense-foliage fruiting-wall trees\nusing RGB and depth features for robotic\
    \ harvesting. Biosyst. Eng. 2020, 197, 245–256. [CrossRef]\n121. Chen, W.; Zhang,\
    \ J.; Guo, B.; Wei, Q.; Zhu, Z. An Apple Detection Method Based on Des-YOLO v4\
    \ Algorithm for Harvesting\nRobots in Complex Environment. Math. Probl. Eng. 2021,\
    \ 2021, 7351470. [CrossRef]\n122. Ge, L.; Zou, K.; Zhou, H.; Yu, X.; Tan, Y.;\
    \ Zhang, C.; Li, W. Three dimensional apple tree organs classiﬁcation and yield\
    \ estimation\nalgorithm based on multi-features fusion and support vector machine.\
    \ Inf. Process. Agric. 2022, 9, 431–442. [CrossRef]\n123. Sabzi, S.; Abbaspour-Gilandeh,\
    \ Y.; García-Mateos, G.; Ruiz-Canales, A.; Molina-Martínez, J.M.; Arribas, J.I.\
    \ An automatic\nnon-destructive method for the classiﬁcation of the ripeness stage\
    \ of red delicious apples in orchards using aerial video. Agronomy\n2019, 9, 84.\
    \ [CrossRef]\n124. Shurygin, B.; Konyukhov, I.; Khruschev, S.; Solovchenko, A.\
    \ Non-Invasive Probing of Winter Dormancy via Time-Frequency\nAnalysis of Induced\
    \ Chlorophyll Fluorescence in Deciduous Plants as Exempliﬁed by Apple (Malus ×\
    \ domestica Borkh.). Plants\n2022, 11, 2811. [CrossRef] [PubMed]\n125. Schlie,\
    \ T.P.; Dierend, W.; Köpcke, D.; Rath, T. Detecting low-oxygen stress of stored\
    \ apples using chlorophyll ﬂuorescence imaging\nand histogram division. Postharvest.\
    \ Biol. Technol. 2022, 189, 111901. [CrossRef]\n126. Miao, Y.; Wang, L.; Peng,\
    \ C.; Li, H.; Li, X.; Zhang, M. Banana plant counting and morphological parameters\
    \ measurement based\non terrestrial laser scanning. Plant Methods 2022, 18, 66.\
    \ [CrossRef]\nPlants 2023, 12, 2061\n21 of 23\n127. Huang, K.Y.; Cheng, J.F. A\
    \ novel auto-sorting system for Chinese cabbage seeds. Sensors 2017, 17, 886.\
    \ [CrossRef]\n128. Turner, S.D.; Ellison, S.L.; Senalik, D.A.; Simon, P.W.; Spalding,\
    \ E.P.; Miller, N.D. An automated image analysis pipeline enables\ngenetic studies\
    \ of shoot and root morphology in carrot (Daucus carota L.). Front. Plant Sci.\
    \ 2018, 871, 1703. [CrossRef]\n129. Brainard, S.H.; Ellison, S.L.; Simon, P.W.;\
    \ Dawson, J.C.; Goldman, I.L. Genetic characterization of carrot root shape and\
    \ size using\ngenome-wide association analysis and genomic-estimated breeding\
    \ values. Theor. Appl. Genet. 2022, 135, 605–622. [CrossRef]\n130. Delgado, A.;\
    \ Hays, D.B.; Bruton, R.K.; Ceballos, H.; Novo, A.; Boi, E.; Selvaraj, M.G. Ground\
    \ penetrating radar: A case study for\nestimating root bulking rate in cassava\
    \ (Manihot esculenta Crantz). Plant Methods 2017, 13, 65. [CrossRef]\n131. Yonis,\
    \ B.O.; Pino del Carpio, D.; Wolfe, M.; Jannink, J.L.; Kulakow, P.; Rabbi, I.\
    \ Improving root characterisation for genomic\nprediction in cassava. Sci. Rep.\
    \ 2020, 10, 8003. [CrossRef]\n132. Atanbori, J.; Montoya, P.M.E.; Selvaraj, M.G.;\
    \ French, A.P.; Pridmore, T.P. Convolutional Neural Net-Based Cassava Storage\
    \ Root\nCounting Using Real and Synthetic Images. Front. Plant Sci. 2019, 10,\
    \ 1516. [CrossRef]\n133. Agbona, A.; Teare, B.; Ruiz-Guzman, H.; Dobreva, I.D.;\
    \ Everett, M.E.; Adams, T.; Montesinos-Lopez, O.A.; Kulakow, P.A.; Hays,\nD.B.\
    \ Prediction of root biomass in cassava based on ground penetrating radar phenomics.\
    \ Remote Sens. 2021, 13, 4908. [CrossRef]\n134. Nkouaya Mbanjo, E.G.; Hershberger,\
    \ J.; Peteti, P.; Agbona, A.; Ikpan, A.; Ogunpaimo, K.; Kayondo, S.I.; Abioye,\
    \ R.S.; Naﬁu, K.;\nAlamu, E.O.; et al. Predicting starch content in cassava fresh\
    \ roots using near-infrared spectroscopy. Front. Plant Sci. 2022, 13,\n990250.\
    \ [CrossRef] [PubMed]\n135. Selvaraj, M.G.; Valderrama, M.; Guzman, D.; Valencia,\
    \ M.; Ruiz, H.; Acharjee, A. Machine learning for high-throughput ﬁeld\nphenotyping\
    \ and image processing provides insight into the association of above and below-ground\
    \ traits in cassava (Manihot\nesculenta Crantz). Plant Methods 2020, 16, 87. [CrossRef]\
    \ [PubMed]\n136. Csillik, O.; Cherbini, J.; Johnson, R.; Lyons, A.; Kelly, M.\
    \ Identiﬁcation of citrus trees from unmanned aerial vehicle imagery using\nconvolutional\
    \ neural networks. Drones 2018, 2, 39. [CrossRef]\n137. Osco, L.P.; de Arruda,\
    \ M.D.S.; Marcato Junior, J.; da Silva, N.B.; Ramos, A.P.M.; Moryia, É.A.S.; Imai,\
    \ N.N.; Pereira, D.R.;\nCreste, J.E.; Matsubara, E.T.; et al. A convolutional\
    \ neural network approach for counting and geolocating citrus-trees in UAV\nmultispectral\
    \ imagery. ISPRS J. Photogramm. Remote Sens. 2020, 160, 97–106. [CrossRef]\n138.\
    \ Zhang, X.; Derival, M.; Albrecht, U.; Ampatzidis, Y. Evaluation of a ground\
    \ penetrating radar to map the root architecture of\nHLB-infected citrus trees.\
    \ Agronomy 2019, 9, 354. [CrossRef]\n139. Chang, A.; Yeom, J.; Jung, J.; Landivar,\
    \ J. Comparison of canopy shape and vegetation indices of citrus trees derived\
    \ from UAV\nmultispectral images for characterization of citrus greening disease.\
    \ Remote Sens. 2020, 12, 4122. [CrossRef]\n140. Rist, F.; Herzog, K.; Mack, J.;\
    \ Richter, R.; Steinhage, V.; Töpfer, R. High-precision phenotyping of grape bunch\
    \ architecture using\nfast 3d sensor and automation. Sensors 2018, 18, 763. [CrossRef]\
    \ [PubMed]\n141. Rist, F.; Gabriel, D.; Mack, J.; Steinhage, V.; Töpfer, R.; Herzog,\
    \ K. Combination of an automated 3D ﬁeld phenotyping workﬂow\nand predictive modelling\
    \ for high-throughput and non-invasive phenotyping of grape bunches. Remote Sens.\
    \ 2019, 11, 2953.\n[CrossRef]\n142. Luo, L.; Liu, W.; Lu, Q.; Wang, J.; Wen, W.;\
    \ Yan, D.; Tang, Y. Grape berry detection and size measurement based on edge image\n\
    processing and geometric morphology. Machines 2021, 9, 233. [CrossRef]\n143. Liu,\
    \ S.; Zeng, X.; Whitty, M. A vision-based robust grape berry counting algorithm\
    \ for fast calibration-free bunch weight\nestimation in the ﬁeld. Comput. Electron.\
    \ Agric. 2020, 173, 105360. [CrossRef]\n144. Buayai, P.; Saikaew, K.R.; Mao, X.\
    \ End-to-End Automatic Berry Counting for Table Grape Thinning. IEEE Access 2021,\
    \ 9, 4829–4842.\n[CrossRef]\n145. Ramos, R.P.; Gomes, J.S.; Prates, R.M.; Simas\
    \ Filho, E.F.; Teruel, B.J.; dos Santos Costa, D. Non-invasive setup for grape\
    \ maturation\nclassiﬁcation using deep learning. J. Sci. Food Agric. 2021, 101,\
    \ 2042–2051. [CrossRef] [PubMed]\n146. Anastasiou, E.; Balafoutis, A.; Darra,\
    \ N.; Psiroukis, V.; Biniari, A.; Xanthopoulos, G.; Fountas, S. Satellite and\
    \ proximal sensing to\nestimate the yield and quality of table grapes. Agriculture\
    \ 2018, 8, 94. [CrossRef]\n147. Torres-Sánchez, J.; Mesas-Carrascosa, F.J.; Santesteban,\
    \ L.G.; Jiménez-Brenes, F.M.; Oneka, O.; Villa-Llop, A.; Loidi, M.; López-\nGranados,\
    \ F. Grape cluster detection using UAV photogrammetric point clouds as a low-cost\
    \ tool for yield forecasting in vineyards.\nSensors 2021, 21, 3083. [CrossRef]\n\
    148. Olenskyj, A.G.; Sams, B.S.; Fei, Z.; Singh, V.; Raja, P.V.; Bornhorst, G.M.;\
    \ Earles, J.M. End-to-end deep learning for directly\nestimating grape yield from\
    \ ground-based imagery. Comput. Electron. Agric. 2022, 198, 107081. [CrossRef]\n\
    149. Gao, Z.; Khot, L.R.; Naidu, R.A.; Zhang, Q. Early detection of grapevine\
    \ leafroll disease in a red-berried wine grape cultivar using\nhyperspectral imaging.\
    \ Comput. Electron. Agric. 2020, 179, 105807. [CrossRef]\n150. Seki, K.; Toda,\
    \ Y. QTL mapping for seed morphology using the instance segmentation neural network\
    \ in Lactuca spp. Front. Plant\nSci. 2022, 13, 949470. [CrossRef]\n151. Du, J.;\
    \ Li, B.; Lu, X.; Yang, X.; Guo, X.; Zhao, C. Quantitative phenotyping and evaluation\
    \ for lettuce leaves of multiple semantic\ncomponents. Plant Methods 2022, 18,\
    \ 54. [CrossRef]\n152. Du, J.; Lu, X.; Fan, J.; Qin, Y.; Yang, X.; Guo, X. Image-Based\
    \ High-Throughput Detection and Phenotype Evaluation Method for\nMultiple Lettuce\
    \ Varieties. Front. Plant Sci. 2020, 11, 563386. [CrossRef]\n153. Zhang, L.; Xu,\
    \ Z.; Xu, D.; Ma, J.; Chen, Y.; Fu, Z. Growth monitoring of greenhouse lettuce\
    \ based on a convolutional neural\nnetwork. Hortic. Res. 2020, 7, 124.\nPlants\
    \ 2023, 12, 2061\n22 of 23\n154. Kim, C.; van Iersel, M.W. Morphological and Physiological\
    \ Screening to Predict Lettuce Biomass Production in Controlled\nEnvironment Agriculture.\
    \ Remote Sens. 2022, 14, 316. [CrossRef]\n155. Zhang, Y.; Li, M.; Li, G.; Li,\
    \ J.; Zheng, L.; Zhang, M.; Wang, M. Multi-phenotypic parameters extraction and\
    \ biomass estimation\nfor lettuce based on point clouds. Measurement 2022, 204,\
    \ 112094. [CrossRef]\n156. Maciel, G.M.; Gallis, R.B.A.; Barbosa, R.L.; Pereira,\
    \ L.M.; Siquieroli, A.C.S.; Peixoto, J.V.M. Image phenotyping of lettuce\ngermplasm\
    \ with genetically diverse carotenoid levels. Bragantia 2020, 79, 224–235. [CrossRef]\n\
    157. Osco, L.P.; Ramos, A.P.M.; Moriya, É.A.S.; Bavaresco, L.G.; de Lima, B.C.;\
    \ Estrabis, N.; Pereira, D.R.; Creste, J.E.; Júnior, J.M.;\nGonçalves, W.N.; et\
    \ al. Modeling hyperspectral response of water-stress induced lettuce plants using\
    \ artiﬁcial neural networks.\nRemote Sens. 2019, 11, 2797. [CrossRef]\n158. Sorrentino,\
    \ M.; Colla, G.; Rouphael, Y.; Panzarová, K.; Trtílek, M. Lettuce reaction to\
    \ drought stress: Automated high-throughput\nphenotyping of plant growth and photosynthetic\
    \ performance. Acta Hortic. 2020, 1268, 133–141. [CrossRef]\n159. Wendel, A.;\
    \ Underwood, J.; Walsh, K. Maturity estimation of mangoes using hyperspectral\
    \ imaging from a ground based mobile\nplatform. Comput. Electron. Agric. 2018,\
    \ 155, 298–313. [CrossRef]\n160. Guo, Y.; Chen, S.; Wu, Z.; Wang, S.; Bryant,\
    \ C.R.; Senthilnath, J.; Cunha, M.; Fu, Y.H. Integrating spectral and textural\
    \ information\nfor monitoring the growth of pear trees using optical images from\
    \ the UAV platform. Remote Sens. 2021, 13, 1795. [CrossRef]\n161. Raju Ahmed,\
    \ M.; Yasmin, J.; Wakholi, C.; Mukasa, P.; Cho, B.K. Classiﬁcation of pepper seed\
    \ quality based on internal structure\nusing X-ray CT imaging. Comput. Electron.\
    \ Agric. 2020, 179, 105839. [CrossRef]\n162. Horgan, G.W.; Song, Y.; Glasbey,\
    \ C.A.; Van Der Heijden, G.W.A.M.; Polder, G.; Dieleman, J.A.; Bink, M.C.A.M.;\
    \ Van Eeuwijk, F.A.\nAutomated estimation of leaf area development in sweet pepper\
    \ plants from image analysis. Funct. Plant Biol. 2015, 42, 486–492.\n[CrossRef]\n\
    163. Musse, M.; Hajjar, G.; Ali, N.; Billiot, B.; Joly, G.; Pépin, J.; Quellec,\
    \ S.; Challois, S.; Mariette, F.; Cambert, M.; et al. A global\nnon-invasive methodology\
    \ for the phenotyping of potato under water deﬁcit conditions using imaging, physiological\
    \ and\nmolecular tools. Plant Methods 2021, 17, 81. [CrossRef] [PubMed]\n164.\
    \ Van Harsselaar, J.K.; Claußen, J.; Lübeck, J.; Wörlein, N.; Uhlmann, N.; Sonnewald,\
    \ U.; Gerth, S. X-ray CT Phenotyping Reveals\nBi-Phasic Growth Phases of Potato\
    \ Tubers Exposed to Combined Abiotic Stress. Front. Plant Sci. 2021, 12, 613108.\
    \ [CrossRef]\n[PubMed]\n165. Caraza-Harter, M.V.; Endelman, J.B. Image-based phenotyping\
    \ and genetic analysis of potato skin set and color. Crop Sci. 2020,\n60, 202–210.\
    \ [CrossRef]\n166. Si, Y.; Sankaran, S.; Knowles, N.R.; Pavek, M.J. Potato Tuber\
    \ Length-Width Ratio Assessment Using Image Analysis. Am. J. Potato\nRes. 2017,\
    \ 94, 88–93. [CrossRef]\n167. Yang, H.; Li, F.; Wang, W.; Yu, K. Estimating above-ground\
    \ biomass of potato using random forest and optimized hyperspectral\nindices.\
    \ Remote Sens. 2021, 13, 2339. [CrossRef]\n168. Liu, Y.; Feng, H.; Yue, J.; Fan,\
    \ Y.; Jin, X.; Song, X.; Yang, H.; Yang, G. Estimation of Potato Above-Ground\
    \ Biomass Based on\nVegetation Indices and Green-Edge Parameters Obtained from\
    \ UAVs. Remote Sens. 2022, 14, 5323. [CrossRef]\n169. Fan, Y.; Feng, H.; Jin,\
    \ X.; Yue, J.; Liu, Y.; Li, Z.; Feng, Z.; Song, X.; Yang, G. Estimation of the\
    \ nitrogen content of potato plants\nbased on morphological parameters and visible\
    \ light vegetation indices. Front. Plant Sci. 2022, 13, 1012070. [CrossRef]\n\
    170. Muruganantham, P.; Samrat, N.H.; Islam, N.; Johnson, J.; Wibowo, S.; Grandhi,\
    \ S. Rapid Estimation of Moisture Content in\nUnpeeled Potato Tubers Using Hyperspectral\
    \ Imaging. Appl. Sci. 2023, 13, 53. [CrossRef]\n171. Sun, C.; Feng, L.; Zhang,\
    \ Z.; Ma, Y.; Crosby, T.; Naber, M.; Wang, Y. Prediction of end-of-season tuber\
    \ yield and tuber set in\npotatoes using in-season UAV-based hyperspectral imagery\
    \ and machine learning. Sensors 2020, 20, 5293. [CrossRef]\n172. Van De Vijver,\
    \ R.; Mertens, K.; Heungens, K.; Somers, B.; Nuyttens, D.; Borra-Serrano, I.;\
    \ Lootens, P.; Roldán-Ruiz, I.; Vangeyte, J.;\nSaeys, W. In-ﬁeld detection of\
    \ Alternaria solani in potato crops using hyperspectral imaging. Comput. Electron.\
    \ Agric. 2020, 168,\n105106. [CrossRef]\n173. Duarte-Carvajalino, J.M.; Alzate,\
    \ D.F.; Ramirez, A.A.; Santa-Sepulveda, J.D.; Fajardo-Rojas, A.E.; Soto-Suárez,\
    \ M. Evaluating late\nblight severity in potato crops using unmanned aerial vehicles\
    \ and machine learning algorithms. Remote Sens. 2018, 10, 1513.\n[CrossRef]\n\
    174. Qi, C.; Sandroni, M.; Cairo Westergaard, J.; Høegh Riis Sundmark, E.; Bagge,\
    \ M.; Alexandersson, E.; Gao, J. In-ﬁeld classiﬁcation\nof the asymptomatic biotrophic\
    \ phase of potato late blight based on deep learning and proximal hyperspectral\
    \ imaging. Comput.\nElectron. Agric. 2023, 205, 107585. [CrossRef]\n175. Saha,\
    \ K.K.; Tsoulias, N.; Weltzien, C.; Zude-Sasse, M. Estimation of Vegetative Growth\
    \ in Strawberry Plants Using Mobile LiDAR\nLaser Scanner. Horticulturae 2022,\
    \ 8, 90. [CrossRef]\n176. Feldmann, M.J.; Hardigan, M.A.; Famula, R.A.; López,\
    \ C.M.; Tabb, A.; Cole, G.S.; Knapp, S.J. Multi-dimensional machine learning\n\
    approaches for fruit shape phenotyping in strawberry. GigaScience 2020, 9, giaa030.\
    \ [CrossRef] [PubMed]\n177. Zingaretti, L.M.; Monfort, A.; Pérez-Enciso, M. Automatic\
    \ fruit morphology phenome and genetic analysis: An application in the\noctoploid\
    \ strawberry. Plant Phenom. 2021, 2021, 981291. [CrossRef]\n178. Li, B.; Cockerton,\
    \ H.M.; Johnson, A.W.; Karlström, A.; Stavridou, E.; Deakin, G.; Harrison, R.J.\
    \ Deﬁning strawberry shape\nuniformity using 3D imaging and genetic mapping. Hortic.\
    \ Res. 2020, 7, 115. [CrossRef]\n179. Zheng, C.; Abd-Elrahman, A.; Whitaker, V.M.;\
    \ Dalid, C. Deep Learning for Strawberry Canopy Delineation and Biomass\nPrediction\
    \ from High-Resolution Images. Plant Phenom. 2022, 2022, 9850486. [CrossRef]\n\
    Plants 2023, 12, 2061\n23 of 23\n180. Guan, Z.; Abd-Elrahman, A.; Fan, Z.; Whitaker,\
    \ V.M.; Wilkinson, B. Modeling strawberry biomass and leaf area using object-based\n\
    analysis of high-resolution images. ISPRS J. Photogramm. Remote Sens. 2020, 163,\
    \ 171–186. [CrossRef]\n181. Cockerton, H.M.; Li, B.; Vickerstaff, R.J.; Eyre,\
    \ C.A.; Sargent, D.J.; Armitage, A.D.; Marina-Montes, C.; Garcia-Cruz, A.; Passey,\
    \ A.J.;\nSimpson, D.W.; et al. Identifying Verticillium dahliae Resistance in\
    \ Strawberry Through Disease Screening of Multiple Populations\nand Image Based\
    \ Phenotyping. Front. Plant Sci. 2019, 10, 924. [CrossRef]\n182. Poobalasubramanian,\
    \ M.; Park, E.S.; Faqeerzada, M.A.; Kim, T.; Kim, M.S.; Baek, I.; Cho, B.K. Identiﬁcation\
    \ of Early Heat and\nWater Stress in Strawberry Plants Using Chlorophyll-Fluorescence\
    \ Indices Extracted via Hyperspectral Images. Sensors 2022, 22,\n8706. [CrossRef]\n\
    183. Zhu, Y.; Gu, Q.; Zhao, Y.; Wan, H.; Wang, R.; Zhang, X.; Cheng, Y. Quantitative\
    \ Extraction and Evaluation of Tomato Fruit\nPhenotypes Based on Image Recognition.\
    \ Front. Plant Sci. 2022, 13, 859290. [CrossRef] [PubMed]\n184. Sun, G.; Wang,\
    \ X.; Sun, Y.; Ding, Y.; Lu, W. Measurement method based on multispectral three-dimensional\
    \ imaging for the\nchlorophyll contents of greenhouse tomato plants. Sensors 2019,\
    \ 19, 3345. [CrossRef] [PubMed]\n185. Chang, A.; Jung, J.; Yeom, J.; Maeda, M.M.;\
    \ Landivar, J.A.; Enciso, J.M.; Avila, C.A.; Anciso, J.R. Unmanned Aircraft System-\n\
    (UAS-) Based High-Throughput Phenotyping (HTP) for Tomato Yield Estimation. J.\
    \ Sens. 2021, 2021, 8875606. [CrossRef]\n186. Tatsumi, K.; Igarashi, N.; Mengxue,\
    \ X. Prediction of plant-level tomato biomass and yield using machine learning\
    \ with unmanned\naerial vehicle imagery. Plant Methods 2021, 17, 77. [CrossRef]\n\
    187. Méline, V.; Caldwell, D.L.; Kim, B.S.; Khangura, R.S.; Baireddy, S.; Yang,\
    \ C.; Sparks, E.E.; Dilkes, B.; Delp, E.J.; Iyer-Pascuzzi,\nA.S. Image-based assessment\
    \ of plant disease progression identiﬁes new genetic loci for resistance to Ralstonia\
    \ solanacearum in\ntomato. Plant J. 2023, 113, 887–903. [CrossRef] [PubMed]\n\
    188. Žibrat, U.; Susiˇc, N.; Knapiˇc, M.; Širca, S.; Strajnar, P.; Razinger, J.;\
    \ Vonˇcina, A.; Urek, G.; Geriˇc Stare, B. Pipeline for imaging,\nextraction,\
    \ pre-processing, and processing of time-series hyperspectral data for discriminating\
    \ drought stress origin in tomatoes.\nMethodsX 2019, 6, 399–408. [CrossRef] [PubMed]\n\
    189. Fullana-Pericàs, M.; Conesa, M.À.; Gago, J.; Ribas-Carbó, M.; Galmés, J.\
    \ High-throughput phenotyping of a large tomato collection\nunder water deﬁcit:\
    \ Combining UAVs’ remote sensing with conventional leaf-level physiologic and\
    \ agronomic measurements.\nAgric. Water Manag. 2022, 260, 107283. [CrossRef]\n\
    190. Tsaftaris, S.A.; Minervini, M.; Scharr, H. Machine Learning for Plant Phenotyping\
    \ Needs Image Processing. Trends Plant Sci. 2016,\n21, 989–991. [CrossRef]\n191.\
    \ Mochida, K.; Koda, S.; Inoue, K.; Hirayama, T.; Tanaka, S.; Nishii, R.; Melgani,\
    \ F. Computer vision-based phenotyping for\nimprovement of plant productivity:\
    \ A machine learning perspective. GigaScience 2019, 8, giy153. [CrossRef]\n192.\
    \ Yoosefzadeh-Najafabadi, M.; Earl, H.J.; Tulpan, D.; Sulik, J.; Eskandari, M.\
    \ Application of Machine Learning Algorithms in Plant\nBreeding: Predicting Yield\
    \ from Hyperspectral Reﬂectance in Soybean. Front. Plant Sci. 2021, 11, 624273.\
    \ [CrossRef]\n193. Khan, M.; Jan, B.; Farman, H. Deep Learning: Convergence to\
    \ Big Data Analytics, 1st ed.; SpringerBriefs in Computer Science;\nSpringer:\
    \ Singapore, 2019.\n194. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature\
    \ 2015, 521, 436–444. [CrossRef] [PubMed]\n195. Kim, Y.; Kwak, G.-H.; Lee, K.-D.;\
    \ Na, S.-I.; Park, C.-W.; Park, N.-W. Performance Evaluation of Machine Learning\
    \ and Deep\nLearning Algorithms in Crop Classiﬁcation: Impact of Hyper-parameters\
    \ and Training Sample Size. Korean J. Remote Sens. 2018,\n34, 811–827.\n196. Toda,\
    \ Y.; Okura, F.; Ito, J.; Okada, S.; Kinoshita, T.; Tsuji, H.; Saisho, D. Training\
    \ instance segmentation neural network with\nsynthetic datasets for crop seed\
    \ phenotyping. Commun. Biol. 2020, 3, 173. [CrossRef] [PubMed]\n197. Singh, A.K.;\
    \ Ganapathysubramanian, B.; Sarkar, S.; Singh, A. Deep Learning for Plant Stress\
    \ Phenotyping: Trends and Future\nPerspectives. Trends Plant Sci. 2018, 23, 883–898.\
    \ [CrossRef] [PubMed]\n198. Ubbens, J.R.; Stavness, I. Deep Plant Phenomics: A\
    \ Deep Learning Platform for Complex Plant Phenotyping Tasks. Front. Plant\nSci.\
    \ 2017, 8, 1190. [CrossRef]\nDisclaimer/Publisher’s Note: The statements, opinions\
    \ and data contained in all publications are solely those of the individual\n\
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or\
    \ the editor(s) disclaim responsibility for any injury to\npeople or property\
    \ resulting from any ideas, methods, instructions or products referred to in the\
    \ content.\n"
  inline_citation: '>'
  journal: Plants
  limitations: '>'
  pdf_link: https://www.mdpi.com/2223-7747/12/10/2061/pdf?version=1684819157
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Image-Based High-Throughput Phenotyping in Horticultural Crops
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/978-3-030-90673-3_27
  analysis: '>'
  authors:
  - José Luís Araus
  - Ma. Luisa Buchaillot
  - Shawn C. Kefauver
  citation_count: 3
  full_citation: '>'
  full_text: ">\n495\n© The Author(s) 2022\nM. P. Reynolds, H.-J. Braun (eds.), Wheat\
    \ Improvement, \nhttps://doi.org/10.1007/978-3-030-90673-3_27\nChapter 27\nHigh\
    \ Throughput Field Phenotyping\nJose Luis Araus, Maria Luisa Buchaillot, and Shawn C. Kefauver\n\
    Abstract The chapter aims to provide guidance on how phenotyping may contrib-\n\
    ute to the genetic advance of wheat in terms of yield potential and resilience\
    \ to \nadverse conditions. Emphasis will be given to field high throughput phenotyping,\
    \ \nincluding affordable solutions, together with the need for environmental and\
    \ spatial \ncharacterization. Different remote sensing techniques and platforms\
    \ are presented, \nwhile concerning lab techniques only a well proven trait, such\
    \ as carbon isotope \ncomposition, is included. Finally, data integration and\
    \ its implementation in prac-\ntice is discussed. In that sense and considering\
    \ the physiological determinants of \nwheat yield that are amenable for indirect\
    \ selection, we highlight stomatal conduc-\ntance and stay green as key observations.\
    \ This choice of traits and phenotyping \ntechniques is based on results from\
    \ a large set of retrospective and other physiologi-\ncal studies that have proven\
    \ the value of these traits together with the highlighted \nphenotypical approaches.\n\
    Keywords Genetic advance · High throughput · Modelling · Field phenotyping \n\
    platforms · Remote sensing\n27.1  Learning Objectives\n• Understanding how phenotyping\
    \ may contribute to wheat genetic advance and \npotential techniques to apply.\n\
    J. L. Araus (*) · M. L. Buchaillot · S. C. Kefauver \nIntegrative Crop Ecophysiology\
    \ Group, Faculty of Biology, University of Barcelona, \nBarcelona, Spain \nAGROTECNIO\
    \ (Center for Research in Agrotechnology), Lleida, Spain\ne-mail: jaraus@ub.edu;\
    \ sckefauver@ub.edu\n496\n27.2  Introduction\nPhenotyping is nowadays considered\
    \ a major bottleneck limiting the breeding \nefforts [1]. In fact, high throughput\
    \ precision phenotyping is becoming more \naccepted as viable way to capitalize\
    \ on recent developments in crop genomics (see \nChaps. 28 and 29) and prediction\
    \ models (see Chap. 31). However, for many breed-\ners, the adoption of new phenotyping\
    \ traits and methodologies only makes sense if \nthey provide added value relative\
    \ to current phenotyping practices. In that sense, a \nbasic concern for many\
    \ breeders is still the controlled nature of many of the pheno-\ntyping platforms\
    \ developed in recent years and the perception that most of these \nplatforms\
    \ are unable to fully replicate environmental variables influencing complex \n\
    traits at the scale of climate variability nor handle the elevated numbers of\
    \ pheno-\ntypes required by breeding programs [2]. This does not exclude for example\
    \ the \ninterest of indoors (i.e., fully controlled) platforms for specific studies\
    \ or traits to be \nevaluated, or even the need to developing special outdoor\
    \ (i.e., near field) but still \ncontrolled facilities. This is the case of phenotyping\
    \ arrangements aimed to evaluate \nresilience to particular stressors (e.g., diseases,\
    \ pests, waterlogging…) or the perfor-\nmance of hidden plant parts (i.e., roots)\
    \ or non-laminar photosynthetic organs (e.g., \nears, culms). While this chapter\
    \ will focus on the general aspects concerning wheat \nphenotyping, specific information\
    \ about special setups is very abundant.\nPhenotyping of simple traits (e.g.,\
    \ plant height) can be achieved even by untrained \npersonnel within a manageable\
    \ time frame. However, manual phenotyping of com-\nplex traits, which is often\
    \ the case when focusing on drought or heat tolerance, \nrequires experienced\
    \ professionals and is time intensive. Another important point to \nconsider is\
    \ that phenotyping of the large genotype sets is generally only feasible if \n\
    conducted by several persons. Moreover, in case phenotyping is conducted visually\
    \ \nthis results in an inflation of measuring error, which might be further increased\
    \ by \nfatigue setting, and is prone to subjective appreciation of each person.\
    \ A recent \npaper [3] has defined the high throughput phenotyping as “relatively\
    \ new for most \nbreeders and requiring significantly greater investment with\
    \ technical hurdles for \nimplementation and a steeper learning curve than the\
    \ minimum data set,” where \nvisual assessments are often the preferred choice.\n\
    In what follows, this chapter will address crop phenotyping within the context\
    \ of \nits implementation under real growing (i.e., field) conditions. Literature\
    \ and exam-\nples included will refer as much as possible to wheat or other small\
    \ grain cereals \nunder field conditions. In that sense we will introduce the\
    \ term high throughput field \nphenotyping (HTFP).\nThe aim of an efficient phenotyping\
    \ method is to enhance genetic gain (Fig. 27.1), \nwhich is defined as the amount\
    \ of increase in performance achieved per unit time \nthrough artificial selection\
    \ (see Chap. 7), usually referred to the increase after one \ngeneration (or cycle)\
    \ has passed. Continuing on, the potential contribution of phe-\nnotyping to wheat\
    \ breeding is placed in context by taking the genetic-advance deter-\nminants\
    \ as a framework of reference. Alternative ways to dissect the role of \nphenotypic\
    \ on genetic gain have been assessed elsewhere [4].\nJ. L. Araus et al.\n497\n\
    Accelerating genetic gain can be achieved by increasing selection intensity, \n\
    accuracy and genetic variation, and/or reducing cycle time (see also Chap. 30).\
    \ \nPhenotyping contributes both directly and indirectly to these variables [5].\
    \ Direct \neffects include increasing selection intensity by the development and\
    \ deployment of \nmore high throughput phenotyping techniques, evaluating larger\
    \ populations even-\ntually across different environments, which is actually the\
    \ main purpose of this \nchapter, improved selection accuracy, which involves\
    \ the repeatability and preci-\nsion of the phenotyping techniques deployed, and\
    \ identifying new genetic variabil-\nity for the targeted traits, which, provided\
    \ that it exists [6], may be secured through \npreselection, using very high throughput\
    \ affordable approaches, even if they are not \nas accurate [4].\nIndirect positive\
    \ effects are diverse but also relevant. Low-cost phenotyping pro-\ntocols allow\
    \ breeders to increase selection intensity and identify new genetic vari-\nability,\
    \ for example through the evaluation of larger populations. Phenotyping \nmeans\
    \ more than just selecting the right traits and choosing the appropriate tools\
    \ for \nevaluation, together with efficient data management. It also requires\
    \ appropriate \ntrial management and spatial variability handling [1, 5]. Improved\
    \ trial management \nand field variation control will increase the selection accuracy\
    \ -of phenotyping and \nthus the heritability of the trait being selected (see\
    \ Chaps. 5, 6, 7 and 12). Therefore, \nselection accuracy is also improved through\
    \ the deploying of phenotyping tech-\nniques to account for the growing conditions\
    \ where plants are phenotyped (spatial \nvariability in environmental factors,\
    \ which also may involve the use of phenotyping \ntechniques). While phenotyping\
    \ does not directly contribute towards decrease cycle \ntime, it is likely to\
    \ play a more important role indirectly. For example, targeted \nHTFP will permit\
    \ the reliable phenotyping of greater numbers of genetic resources \nderived from\
    \ breeding lines by using smaller plot sizes and assessments obtained at \nearlier\
    \ stages of population development. This allows breeders to reduce the \nFig.\
    \ 27.1 Direct and indirect ways how high throughput precision field phenotyping\
    \ may contrib-\nute to genetic gain in wheat\n27 High Throughput Field Phenotyping\n\
    498\nduration of breeding cycles and the loss of potentially important alleles\
    \ with linkage \ndrag [4], therefore contributing to increasing the genetic gain\
    \ (see Chap. 7). \nMoreover, while most efforts are considered toward direct selection\
    \ for yield, indi-\nrect selection for physiological, morphological or biochemical\
    \ yield-component \ntraits can provide the opportunity to introduce new alleles\
    \ from which genetic prog-\nress can be made [4].\nPhenotypic expression is the\
    \ response of genotypes to varied environmental con-\nditions (GxE) or even to\
    \ the agronomical management practices (GxExM), and \ntherefore the full disentangling\
    \ of the link between plant phenotype and its genetic \nbackground cannot be achieved\
    \ (see Chap. 15) without considering the full and \naccurate quantification of\
    \ the environmental and agronomical conditions experi-\nenced during growth [5].\
    \ Therefore, appropriate documentation of the environmen-\ntal growth conditions\
    \ is essential for any crop phenomics strategy. This implies a \nsystematic collection\
    \ and integration of meteorological data at different spatio- \ntemporal scales,\
    \ frequently using low-cost sensors [7]. Finally, new avenues for data \nmanagement\
    \ and exploitation are required in order to optimally capitalize on recent \n\
    improvements in data capture and computation capacity.\nSummarizing, the objective\
    \ of practical phenotyping innovation is the imple-\nmentation of high throughput\
    \ precision phenotyping under real (i.e., field in most \ncases) conditions and\
    \ preferably at an affordable cost. On the other hand, proper \nHTFP requires\
    \ some basic uniform characteristics such as similar phenology of the \nwhole\
    \ set of varieties selected as well as the identification of the right growth\
    \ stage \n(or stages) when phenotyping has to be conducted. In other words, a\
    \ phenotypic \ntrait may have a positive, negative or no relationship with grain\
    \ yield or another \ntarget parameter depending on the growth stage at measurement.\
    \ Such differential \nperformance of a phenotypic trait may depend on different\
    \ factors such as the phe-\nnological stage when it is measured or the growing\
    \ conditions. The phenotypical \nperformance may even be biased if the targeted\
    \ germplasm is too diverse in terms \nof phenology (e.g., heading, anthesis or\
    \ maturity dates). Therefore, in addition to \nchoosing the optimal phenotypic\
    \ traits, the time at which they are assessed, while \navoiding too wide of a\
    \ genotypic range in phenology, is also crucial. This applies for \nremote sensing\
    \ traits such as vegetation indices as well as for lab traits such as the \ncarbon\
    \ isotope composition [6, 8].\n27.3  Platforms: From Ground to the Sky\nThe drawbacks\
    \ of most time-consuming phenotyping methods in terms of through-\nput and standardization\
    \ can be overcome using image-based data collection. Remote \nsensing technologies,\
    \ with the respective controllers and data loggers that comple-\nment the imaging\
    \ systems, are usually assembled into what are termed as phenotyp-\ning platforms\
    \ [5, 7, 9, 10]. The use of these platforms allows for a more efficient and \n\
    accurate phenotyping with stable error across all genotypes, whether as single\
    \ plants \nor in micro-plots. However, currently many of these platforms are costly\
    \ and/or not \nJ. L. Araus et al.\n499\napplicable on a wide scale. Therefore,\
    \ there is a strongly expressed need by the crop \nbreeding community to develop\
    \ both state-of-the-art and cost-effective, easy to use, \nand nonstationary HTFP\
    \ platforms. These platforms may also represent tailored \nsolutions to specific\
    \ cases or a feasible formula on how to apply standard phenotyp-\ning tools in\
    \ breeding programs with limited resources [11].\nThe concept of the phenotypic\
    \ platform is wide and embraces a varied range of \noptions in terms of placement:\
    \ ground, aerial or even eventually (in the coming \nyears) at the space level\
    \ (Fig. 27.2; [10]). Within the category of ground phenotyp-\ning, platforms have\
    \ quickly diversified, and the range of options is very wide: from \na simple\
    \ hand-held sensor, including for example monopods and tripods carrying \nany\
    \ sensors from a simple yet effective RGB color camera, to complex unmanned \n\
    ground vehicles of diverse nature, which are generically termed as “phenomobiles,”\
    \ \nand include tractor-mounted sensors, other tailored solutions (e.g., carts,\
    \ buggies) or \nmobile cranes. Within the ground category one may also include\
    \ highly complex \nstationary facilities. Cable-based robotics systems are becoming\
    \ also an alternative \nfor outdoor (i.e. field) phenotyping, which allow imaging\
    \ platforms to move about \na defined area [12]. Within the hand-held category\
    \ of platforms, smartphones are \nbecoming an alternative giving they may carry\
    \ out different imagers (e.g. RGB and \nthermal), data management activities and\
    \ geo-referencing functions [5, 7, 9, 10].\nFig. 27.2 Different Categories of\
    \ Ground and Aerial Phenotyping Platforms. Ground level: these \ninclude from\
    \ Handheld sensors (in this case just a person holding a mobile), to Phenopoles,\
    \ \nPhenomobiles, Stationary Platforms. From 10 to 100 m: Unmanned Aerial Vehicles,\
    \ as drones of \ndifferent sizes and more or less compactness, fixed-wind drone.\
    \ From 100 to 4000 m Manned \nAerial Vehicles as airplanes or helicopters. In\
    \ the near future different categories of satellites \n(Nanosatellite, Microsatellite\
    \ and Satellites) from 50 to 700 km\n27 High Throughput Field Phenotyping\n500\n\
    Aerial platforms of different nature are being widely used, particularly more\
    \ and \nmore involving unmanned aerial vehicles (UAV), popularly known as drones.\
    \ \nProximal and remote sensing sensors are now able to be mounted on low flying\
    \ \nmultirotor UAVs, with image acquisition capabilities at spatial scales in\
    \ centime-\nters, relevant to crop breeding [13]. The use of drones has popularized\
    \ in the recent \nyears [10] and even book manuals (even if mostly focused on\
    \ crop management) \nhave been produced. The remote sensing tools most frequently\
    \ deployed in pheno-\ntyping platforms are RGB cameras, alongside multispectral\
    \ and thermal sensors or \nimagers [14]. The increasing availability of compact\
    \ drones which don’t need to be \nassembled, bring the sensors embedded, and are\
    \ affordable, reliable and easy to \ncontrol, is popularizing more and more this\
    \ option (Fig. 27.3). Nevertheless, other \nunmanned options offer appealing alternatives\
    \ with contrasting capabilities, partic-\nularly fixed wing UAVs, where for example,\
    \ the crop area to monitor area larger \nthan a few hectares, or in related precision\
    \ agriculture activities [15]. Other alterna-\ntives such as manned aircrafts\
    \ are less used by crop breeders given the cost of this \nalternative, while the\
    \ use of satellites on phenotyping are not yet a reality in practi-\ncal terms\
    \ due to the lack of free sub-meter resolution data, but they will surely be of\
    \ \nincreasing interest in the near future as these technologies advance [5, 10].\n\
    Fig. 27.3 Example of different types of affordable aerial platforms and sensors\
    \ (less than 5000 \nUSD). (A) Aerial Platforms: (1) Phantom 4 Multispectral (https://www.dji.com/es/p4-\
    \ \nmultispectral); (2) Mavic 2 Pro (https://www.dji.com/es/mavic- 2); (3) & (4)\
    \ are for a company \nnamed Sentera which add a multispectral camera to Phantom\
    \ and Mavic (https://sentera.com); (5) \nAgroCam Mapper QC; (6) AgroCam Mapper\
    \ FW the last one has integrated NDVI cameras \n(https://www.agrocam.eu/uav- system).\
    \ (B) Affordable sensors that can be used on a phenopole or \non a drone: (7)\
    \ Sony Qx1 RGB (https://www.sony.es); (8) Olympus OM-D E-M10 MKII RGB \n(https://www.olympus.es);\
    \ (9) GoPro, an RGB camera that can be modified to calculate the NDVI; \n(10)\
    \ Parrot Sequoia multispectral Camera (https://www.parrot.com); (11) AgroCam NDVI\
    \ cam-\nera, (https://www.agrocam.eu); (12) Smartphone CatS60 with RGB and Thermal\
    \ camera (https://\nwww.catphones.com)\nJ. L. Araus et al.\n501\nHere we outline\
    \ standards for the deploying simple stationary, cable-based robot-\nics and ultimately\
    \ UAVs as progressively more mobile and high throughput pheno-\ntyping platforms\
    \ for the transport of the various proximal and remote sensing \nsensors/imagers.\
    \ The primary selection criterion concerning the equipment to carry \nout the\
    \ HTFP platforms concerns the choice of the most adequate sensors for the \nestimation\
    \ of the specific biophysical traits of interest at an appropriate technology\
    \ \nreadiness level. Many of the simultaneous major technological advancements\
    \ in \nHTFP platforms come from the impressive miniaturization of imaging and\
    \ mea-\nsurement technologies and on-board processing capacities, but perhaps\
    \ even more \nimportantly massive leaps and bounds in communications, compact\
    \ and lightweight \nbatteries, inertial sensors, electronic compasses, data storage\
    \ and intelligent auto-\nmated control algorithms. These have come together to\
    \ enable the development of \nmore compact and light weight scientific imaging\
    \ sensors and at the same time \nimproved indoor robotics systems and UAVs with\
    \ increasing autonomy, carrying \ncapacity, stability, and security. The result\
    \ is that scientific quality of remote sensing \nplatforms and sensors that only\
    \ 5 years ago were nearly exclusively limited to very \nexpensive indoor installations\
    \ and manned airborne platforms (only able to provide \nground spatial resolution\
    \ a.k.a. pixel sizes on the order of 5–20 m and not amenable \nto phenotyping)\
    \ are now available in more cost-effective unmanned systems. In fact, \nUAVs are\
    \ nowadays the most popular mobile platform for phenotyping pur-\nposes [16].\n\
    27.4  Phenotyping Is More than Just Monitoring Techniques\nCrop phenotyping is\
    \ about collecting useful and meaningful data for integration \ninto crop breeding\
    \ programs. As such, a complete HTFP platform research protocol \nshould include\
    \ considerations for every part of the full process in order to ensure \nthat\
    \ no bottlenecks impede the throughput of the phenotyping activities. This \n\
    includes but is not limited to (1) the equipment (sensors, platforms and software);\
    \ \n(2) use operation (e.g. pilot permits and training, flight plans, and image\
    \ acquisition \nin case of UAV); (3) proper storing and managing of experimental\
    \ datasets for long \nterm use; (4) image processing (pre-processing, calibration,\
    \ mosaicking); (5) data \ngeneration (extraction from processed image to plot\
    \ level data); (6) data analysis \n(index calculations, stats scripts) and database\
    \ structure (storage, linkages, inven-\ntory indexing, ontologies, etc.); (7)\
    \ specific case studies of bottlenecks to through-\nput, training requirements,\
    \ costs, and optimization for specific crops (scalable/\ntransferable traits)\
    \ [7]. All the major components from sensors to platforms to soft-\nware for each\
    \ key processing step are intricately intertwined and need to be consid-\nered\
    \ together such that pre-integrated systems or close attention to integration\
    \ \ndetails will improve both data quality and data throughput. Some examples\
    \ have \nbeen provided for the deployment of RGB images.\n27 High Throughput Field\
    \ Phenotyping\n502\n27.5  Data Integration: From Ideotype to Modelling \nand More\n\
    Connecting genomic and phenomic datasets remains challenging. Is in this context\
    \ \nwhere plant phenotyping is creating new needs for data standardization, analysis\
    \ \nand storage [17]. In addition, the value of phenotypic data is moving from\
    \ empiri-\ncal/descriptive context, where ideotype, understood as the fixed combination\
    \ of \ntraits that confers advantage to a given wheat genotype was the target,\
    \ to the use of \nphenotypic data in a more mechanistic way, through simulation\
    \ models aiming to \npredict genotype performance (see Chaps. 31 and 32). In that\
    \ sense integration of \nphenotypic data into simulation models to predict trait\
    \ value is of increasing impor-\ntance [18]. Besides that, large amounts of phenotypic\
    \ data are used in a statistically \noriented manner, for marker-assisted and\
    \ even more for genomic selection (see \nChaps. 28 and 29). The future of crop\
    \ breeding lies in the standardization of data \ncollection across phenotyping\
    \ platforms.\nOn the other hand, the development of specific software tools that\
    \ meet the needs \nof the crop phenotyping community in terms of remote sensing\
    \ data processing, \nextraction and analysis have been identified as potentially\
    \ the greatest bottleneck for \ngenerating high quality phenotypic data [19].\
    \ This includes for example the devel-\nopment of intuitive, easy-to-use semiautomatic\
    \ programs for microplot extraction \nencompassing also appropriate flight planning\
    \ to capture images with sufficient \nquality, which implies relevant concepts\
    \ such as view, sharpness and exposure cal-\nculations, in addition to consider\
    \ ground control points (GCPs), viewing geometry \nand way-point flights [20].\
    \ These new software tools will need be integrated to \ninclude not only the assessment\
    \ of crop growth performance (including for example \ncrop establishment, stay\
    \ green) and grain yield, but also the detection and quantifi-\ncation of phenological\
    \ stages (heading or maturity times and even anthesis), agro-\nnomical yield components\
    \ (ear density), total biomass, or identifying specific pests \nand diseases and\
    \ further quantifying its impact.\nOverall, there is a great need for new analytical\
    \ approaches that can integrate \nmultiple types of data or provide proper experimental\
    \ design in observational \ncontexts. This need will only grow with the development\
    \ of imaging, sequenc-\ning, and sensing technologies. A recent push in this direction\
    \ has been an empha-\nsis on machine learning and artificial intelligence in phenotyping\
    \ [21]. \nConcerning trait measurements, implementing machine learning methods\
    \ on \nUAV data enhances the capability of data processing and prediction in various\
    \ \napplications [16], such as wheat ear counting [22]. High spatial resolution\
    \ UAV-\nbased remote sensing imagery with a resolution between 0 and 10 cm is\
    \ the most \nfrequently employed data source amongst those utilized for machine\
    \ learning \napproaches [16]. Classification and regression are two main prediction\
    \ problems \nthat are commonly used in UAV- based applications. Taking RGB images\
    \ as a \nproximal remote sensing approach may increase the resolution of images\
    \ and \ntherefore the usefulness of these images when analyzed with machine learning\
    \ \nJ. L. Araus et al.\n503\nmethods. Thus, for example, using an RGB camera placed\
    \ on a pole at 1.2 m \nfrom the ground provided a ground spatial resolution better\
    \ than 0.2 mm, able to \nassess the thickness of the residual stems standing straight\
    \ after the cutting by \nthe combine machine during harvest. In that case, a faster\
    \ Regional Convolutional \nNeural Network (Faster-RCNN) deep- learning model was\
    \ first trained to iden-\ntify the stems cross section [23]. Machine learning\
    \ algorithms can be imple-\nmented using either open source or commercial software.\
    \ Open source coding \nenvironments such as Python and R are freely available\
    \ and may be redistributed \nand modified.\n27.6  Affordable Phenotyping Approaches\n\
    Many of the desired phenotypic traits can be acquired using cost-effective and\
    \ read-\nily available RGB cameras, which are characterized as very high spatial\
    \ resolution \nimaging sensors, with quality color calibration and PAR spectral\
    \ coverage \n(Fig. 27.3). These are extensively addressed elsewhere (e.g. [7]).\
    \ In short, several \nRGB vegetation indexes use the spectral concept for the\
    \ estimation of biomass and \ncanopy chlorophyll, while others are based on alternate\
    \ color space transforms such \nas Hue Saturation Intensity (HSI), CIE-LAB and\
    \ CIE-LUV [24]. Practical solutions \nexist for the calculation of these RGB vegetation\
    \ indexes using free, open- source \nsoftware. Thus, for example, our team at\
    \ the University of Barcelona has developed \nopen-source software tools for analyzing\
    \ high resolution RGB digital images, with \nspecial consideration to cost-effectiveness,\
    \ technology availability and computing \ncapacity using digital cameras or smartphones\
    \ for data acquisition. Besides the for-\nmulation of vegetation indices amenable\
    \ to monitor crop growth, stay green, or \nquantify the impact of a given pest\
    \ or disease which affect the green biomass, exam-\nples exist on the use of RGB\
    \ images to specific purposes such as for example assess-\ning ear density [22].\
    \ Recently methods have been proposed to phenotype early \ndevelopment of wheat,\
    \ specifically to assess the rate of plant emergence, the num-\nber of tillers,\
    \ and the beginning of stem elongation using drone-based RGB imag-\nery. Moreover,\
    \ the characteristics of the digital RGB images, together with the \nsupport of\
    \ machine learning approaches, make feasible the automatic identification \nof\
    \ plant deficiencies and biotic stressed based in the shape and pattern of leaf\
    \ symp-\ntoms such as chlorosis, necrosis spots etc.\nBesides the RGB sensors,\
    \ in the last years a wide range of affordable multispec-\ntral imagers, and even\
    \ thermal imagers, and dual multispectral/RGB or thermal/\nRGM imagers are available,\
    \ making HTFP more feasible to, for example, small \nseed companies and national\
    \ agricultural research organizations.\nThe main traits that can be measured in\
    \ the field using affordable HTP-approaches \nis included, with the sensors/indices,\
    \ as well as a qualitative assessment of their \nprecision, in Table 27.1.\n27\
    \ High Throughput Field Phenotyping\n504\nTrait \nExamples of  \nSpectral Indexes\
    \ \nSensors \n Qualitative assessment of their \nprecision \nReference \nMultispectral/\
    \ \nHyperspectral \nRGB \nThermal \n Growth                          \nEarly Vigor\
    \                       \nStay Green                  \nQuantification \npest/disease\
    \             \nGreen \nBiomass   \nSenescence \nNormalized Difference \nVegetation\
    \ Index (NDVI) \n \n \n \nSome indexes formulated using RGB \nand multispectral\
    \ images may \nbecome saturated at medium to high \nlevels of biomass, which implies\
    \ a \nloss in accuracy. Saturating canopies \nare common between end of tillering\
    \ \nto grain filling. Captured with a field \nsensor (points) or with cameras\
    \ \n(images) at different heights such us \nin UAVs, phenopoles, etc. (Fig\n27.2).\
    \ The platform and the sensor \ndetermine the spectral resolution. \nBiomass estimation\
    \ using thermal \nsensors is only related to fraction of \nvegetation cover, so\
    \ accuracy is low.               \n[24, 25] \n \nOptimized Soil-adjusted \nVegetation\
    \ index (OSARI) \n \n \n \nGreener Green Area (GGA) \n \n \n \na* \n \n \n \n\
    u*v*A \n \n  \n \nCrop Senescence Index \n(CSI) \n \n \n \nNormalized Green-Red\
    \ \nDifference Index (NGRDI) \n \n \n \nThermal bands \n \n \n \n \nNitrogen \n\
    content \nLeaf \nPigments:  \nCarotenoids,  \nanthocyanins \nChlorophyll Content\
    \ Index \n(CCI) \n \n \n \nSome indices, like multispectral and\nRGB, estimate\
    \ the different pigment \ncontent. Could be taking with a \nsensor (only points)\
    \ or with cameras \n(images) at different height such us in \ndrones, phenopoles\
    \ (Fig\n \n27.2). \nThe platform and the cost of the \nsensor will determinate\
    \ the spectral \nresolution. \n[24, 25] \n \nTransformed Chlorophyll \nAbsorption\
    \ Ratio Index \n(TCARI) or \n(TCARI/OSAVI) \n \n \n \nAnthocyanin Reflectance\
    \ \nIndex (ARI2) \n \n \n \nCarotenoid Reflectance \nIndex 2 (CRI2) \n \n \n \n\
    Triangular Green Index \n(TGI) \n \n \n \nPhenology \n(e.g heading, \nanthesis,\
    \ \nmaturity \ntimes) \nSpecific algorithms are\nrequired for specific stages\n\
    such as anthesis. For another\nphenological stages, such as\nheading or maturity,\
    \ even\nchanges in the vegetation\nindices presented above, or an\nincrease in\
    \ canopy\ntemperature may suffice \n \n \n \nDepending of the phenological stage\
    \ \nto assess resolution is key and \ntherefore imager and distance of \nacquisition\
    \ must be considered. This \nis the case for example of anthesis \ntime which\
    \ is determined based in the \nappearance of extruded anthers. In \nsuch a case\
    \ resolution in the range of \nmm are needed \n[24] \nDetection \npest/disease,\n\
    Agronomical \nyield \ncomponents, \nNumber of \nseedlings and \nspikes\nSpecific\
    \ algorithms for \ndetecting plant health \nsymptoms, counting \nseedlings during\
    \ crop \nemergence or the number \nof spikes during crop\nreproductive stage \n\
    Image resolution is key and varies \ndepending on the imager used \n(RGG>multispectral>thermal)\
    \ and \nthe distance from where the image is \nacquired. For some traits, such\
    \ as ear \ncounting, only the ears above the \ncanopy not overlapped by other\
    \ \ntillers, will be accounted \n[24]\nHeight/ Plant \nArchitecture\nRGB or multispectral\
    \ 3D \nmodel\nIn the best scenarios, accuracy in the \nrange of cm is achieved.\
    \ It provides\nexternal canopy assessments, which \nmeans for additional assessment\
    \ of \ncrop yield or canopy architecture \nadditional measurements using other\
    \ \napproaches (or harvesting) should to \nbe considered.\n[26]\nLight Use \n\
    Efficiency\nPhotochemical Reflectance \nIndex (PRI)\nPowerful index at both, the\
    \ single\nleaf level and for canopy level,\neven when its combination with\ngas\
    \ exchange or chlorophyll\nfluorescence is recomended.\n[27]\nCrop water \nstatus,\
    \ \ntranspiration, \nWater content\nCrop Water stress Index \n(CWSI)\nUse NIR,\
    \ SWIR or TIR bands to \nestimate canopy water content. \nAccuracy depends on\
    \ the infrared \nwater absorption bands selected. \n[14, 25]\nNormalized Difference\
    \ \nWater index (NDWI)\nThermal bands\nEnvironmental factors such as wind, \n\
    clouds or the presence of bare soil \nwithin the canopy may strongly affect \n\
    thermal measurement accuracy. \nMoreover, using aerial platforms is \nrecommended\
    \ to avoid short-term \ntime-related dynamics. \nTable 27.1 Example indexes per\
    \ trait and sensor type with a qualitative assessment of precision\nJ. L. Araus\
    \ et al.\n505\n27.7  Hyperspectral Imaging for Crop Phenotyping: Pros \nand Cons\n\
    Hyperspectral sensors and cameras are among the most promising for the phenotyp-\n\
    ing of advanced traits. The application of hyperspectral reflectance to proximal\
    \ (i.e. \nground level) plant phenotyping at high resolution range makes it possible\
    \ to infer, \nin the case of wheat under field conditions, not only grain yield\
    \ but for example the \ncontent of metabolites in leaves and ears [28], or photosynthetic\
    \ capacities and \nquenching. Hyperspectral imaging techniques have been expanding\
    \ considerably in \nrecent years. The cost of current solutions is decreasing,\
    \ but these high-end tech-\nnologies are not yet available for moderate to low-cost\
    \ outdoor or indoor applica-\ntions. However new methodological developments,\
    \ such as a single-pixel imaging \nsetup [29], which do not require (as much of\
    \ an investment) high computational \ncapacity, may offer a more approachable\
    \ alternative.\nIn spite of the recent availability of hyperspectral UAV sensors,\
    \ both the sensor \ndesign and the resulting data result in several complications\
    \ at the time of capture, \npre-processing, calibration, and analysis [13]. Firstly,\
    \ “hyper” literally means “too \nmuch” so hyperspectral sensors are and openly\
    \ acknowledged as frequently captur-\ning more data than is necessary for any\
    \ specific given purpose. For that reason, they \nare and will continue to be\
    \ considered as more exploratory and experimental rather \nthan operational sensors.\
    \ It is on the scientific community to take on the challenge \nof first acquiring\
    \ what may be considered as excess data in order to later distil the \n“big data”\
    \ down to the essential and prescribe the more specific and required mea-\nsurements\
    \ for any particular measurement goal, in this case the phenotyping of \nphotosynthesis\
    \ and biophysical traits relevant to the disentangling of genetic \nsequencing\
    \ data and maximizing yield to feed the future [30].\nMoreover, the use of hyperspectral\
    \ images from moving platforms, such as those \ncarried out from UAV, has additional\
    \ challenges [13]. Unlike sensors that capture \nwhole images in one instant like\
    \ RGB (which captures three separate spectral \nregions in one image with its\
    \ integrated Bayer filter) and the more common multi-\nspectral cameras (which\
    \ capture each spectral region with a different sensor and are \nlater corrected\
    \ for parallax) most hyperspectral cameras are not “area array” type. \nMost hyperspectral\
    \ imagers are of the “line scanning” type, which require a moving \nmirror and\
    \ spectral prism to iteratively measure each wavelength over a single line \n\
    of pixels as the UAV moves forward. This requires carefully programmed and timed\
    \ \ninternal sensor movements with the external robotics platform or UAV flights\
    \ at \nspecific forward movement velocities relative to the distance between the\
    \ sensor \nand crop. The data is also thus more likely to be adversely affected\
    \ by environmen-\ntal conditions and gimbal instability. In turn, the carrying\
    \ platform and hyperspec-\ntral camera system must be fully integrated as the\
    \ inertial measurement unit (IMU) \naccelerometer (yaw, pitch and roll) and positioning,\
    \ whether in local or GPS (geo-\ngraphical location) data in order to create a\
    \ correct hyperspectral image. Ground \ntopographical variability, if present,\
    \ should also be optimally corrected for using a \nseparately produced digital\
    \ elevation model (DEM).\n27 High Throughput Field Phenotyping\n506\nStill, despite\
    \ these complications, adequately integrated hyperspectral sensors \nand platforms\
    \ from stationary solutions to UAVs are available and may provide \nexcellent\
    \ data with in-depth knowledge and expertise in data interpretation and pro-\n\
    cessing. More common UAV multispectral sensors are based precisely on the exten-\n\
    sive data analysis from field spectroscopy and airborne hyperspectral imaging\
    \ \nconducted by research laboratories over the past 40 years [31]. The best bands\
    \ for \nmeasuring specific plant spectral properties that are associated with\
    \ physiological \ntraits of interest have been selected with regards to both their\
    \ specific central wave-\nlength and their bandwidth (range of wavelengths where\
    \ radiation is measured) and \ndesigned accordingly. However, no full VNIR+SWIR\
    \ hyperspectral sensors have \nbeen available for application as HTFPs with the\
    \ specific purpose of crop breeding \nuntil very recently, due to the many technological\
    \ barriers that impeded their deploy-\nment on UAVs, and as such the linkage between\
    \ spectral wavelengths and breeding \ntraits has not been completed.\n27.8  Implementing\
    \ Phenotyping in Practice\nSome approaches for practical wheat phenotyping will\
    \ be briefly presented taking \ngrain yield as the breeding target (Fig. 27.4).\
    \ A thorough set of examples of traits \nand conditions where phenotyping may\
    \ be applied in practice at different levels \n(handy, high throughput, and precision\
    \ phenotyping) may be accessed elsewhere [3].\nIdentifying the key traits for\
    \ phenotyping may result in convergent approaches. \nOn one hand, grain yield\
    \ may be dissected into three main physiological compo-\nnents: the amount of\
    \ resources (radiation, water, nitrogen…) captured by the crop, \nthe efficient\
    \ use of these resources and the dry matter partitioning (so called harvest \n\
    index). The kind of resource considered depend in each case on what is the limiting\
    \ \nfactor (e.g. under drought conditions or low nitrogen fertility, water and\
    \ nitrogen \nwill be the relevant resources, respectively). On the other hand,\
    \ retrospective stud-\nies, comparing cultivars developed through the last decades,\
    \ also provide clues on \nthe most successful, to date, physiological traits,\
    \ involved in the genetic advance \nafter Green Revolution. For this approach\
    \ it is important to avoid confounding \neffects associated with the inclusion\
    \ in the comparison, genotypes developed prior \nGreen Revolution or even transitional\
    \ ones. In that sense genetic advance in wheat \nfor a wide range of environmental\
    \ conditions has been associated with a higher \nstomatal conductance [32]. Remote\
    \ sensing techniques such as infrared thermome-\ntry or thermography may be deployed\
    \ as proxies for higher transpiration [1, 4, 5, 9]. \nAn alternative is to use\
    \ the stable carbon isotope, one of the few lab-phenotyping \ntraits widely accepted.\
    \ Usually a lower (i.e. more negative) carbon isotope composi-\ntion (δ13C) or,\
    \ alternatively, a higher carbon isotope discrimination (Δ13C), particu-\nlarly\
    \ when analyzed in mature kernels and confounding effects are avoided (such as\
    \ \ndifferences in phenology [6]) is pursued, since it indicates a better water\
    \ status and \neventually more water captured by the crop, in spite the fact water\
    \ use efficiency \ndecreases. Another trait to consider is stay green, which may\
    \ be relevant particularly \nunder good agronomical conditions [33]. This trait\
    \ may be assessed through \nJ. L. Araus et al.\n507\nmultispectral of RGB-derived\
    \ vegetation indices assessed during grain filling [24, \n34]. The same category\
    \ of indices may be used to assess early vigor and ground \ncovering. The three\
    \ categories of main remote sensing approaches (RGB, thermal or \nmultispectral/hyperspectral)\
    \ may be used to assess differences in phenology, par-\nticularly heading and\
    \ anthesis nature.\nDigital RGB imaging may allow to 3D surface reconstruction\
    \ to provide estima-\ntions of plant height, and incidence of lodging, while image-pattern\
    \ recognition \nmay help to identify the presence of a pest or disease, which\
    \ may be further quanti-\nfied on their impact by RGB or multispectral vegetation\
    \ indices [26].\nGreater biomass is also considered as a key target trait for\
    \ selection, particularly \nsince harvest index is reaching theoretical maximum,\
    \ while the increasing in bio-\nmass have been minor during the more the half\
    \ century elapsed from the beginning \nof the Green Revolution to the present.\
    \ HTFP, particularly when deployed from an \naerial platform allows the assessment\
    \ of biomass through different techniques in the \nfull plot rather than in subsamples.\
    \ Moreover, there is the capacity to undertake \nrepetitive measurements which\
    \ may improve the estimation [4]. A priory the most \ncanonical way to assess\
    \ biomass is using LiDAR (Light Detection and Ranging) \nmounted in an aerial\
    \ platform or in a “phenomobile” [26]. However still today the \nmost common way\
    \ to assess green biomass is through vegetation indices, either \nFig. 27.4 Different\
    \ examples from the University of Barcelona using different platforms and sen-\n\
    sors; mostly of affordable nature: (1) Thermal Camera: FLIR Tau 640 with Thermal\
    \ Capture; (2) \nModified GoPro taking the NDVI, that was installed in a Mavic\
    \ pro 2; (3) Mavic 2 Pro with an \nRGB camera; (4) Sony Qx1 used from ground to\
    \ count spikes; (5) MultiSPEC 4C camera, which \nhas 4 channels, from AIRINOV\
    \ company, on a phenopole of 5 m; this multispectral camera is quite \nsimilar\
    \ to Parrot Sequoia in capacities and cost; (6) Cat s60 mobile phone, which takes\
    \ thermal and \nRGB pictures\n27 High Throughput Field Phenotyping\n508\nmultispectral\
    \ or RGB-derived, given the common perception these approaches \nbeing more affordable\
    \ and easier to use than the LiDAR [26]. However, an inherent \nlimitation of\
    \ the vegetation indices is that they saturate, which makes its use less \neffective\
    \ during the central part of the crop cycle, even when still is of value to \n\
    assess early stages of growth or stay green. Moreover, vegetation indices do not\
    \ \ninform about canopy height. Nevertheless, a more accurate determination of\
    \ green \nbiomass than that associated to vegetation indices, together with plant\
    \ height may \nbe also achieved using RGB images; this time through three-dimensional\
    \ recon-\nstruction of the crop canopy. This evaluation may improve further if\
    \ canopy height \nis combined with the number and thickness of the stems, evaluated\
    \ through high- \nresolution RGB images [23].\nAnother potential target for current\
    \ phenotyping, which has been traditionally \nneglected, is the photosynthetic\
    \ contribution of the ear to grain filling. While a \nrecent study has confirmed\
    \ that genotypic variability exists for this trait and more-\nover showing the\
    \ first examples of HTFP for this trait [35], the advent on remote \nsensing techniques\
    \ based on the combination of RGB imaging for in situ organ \ndetection, together\
    \ with thermal and/or multispectral imaging may allow in the near \nfuture the\
    \ evaluation of this trait from aerial platforms [36].\nHowever, there still exist\
    \ several areas not fully explored in terms of HTFP pro-\ntocols, such as root\
    \ phenotyping, just one among many hidden yet very important \nattributes to consider\
    \ in new potential phenotyping target traits.\n27.9  Key Concepts\nHTFP may contribute\
    \ to speed genetic advances in different ways. Nevertheless, \nphenotyping under\
    \ controlled conditions may still have applicability in some cases. \nUsually,\
    \ there is not a single technological solution, but rather different options in\
    \ \nterms of throughput and even cost are available. In this sense, affordable\
    \ phenotyp-\ning techniques, including various sensors and platforms, are more\
    \ approachable \nthan ever before. Remote sensing techniques are the most commonly\
    \ used for phe-\nnotyping but other approaches, like the lab-based traits may\
    \ be also useful. \nEventually, hyperspectral techniques may even replace many\
    \ lab-based approaches. \nBesides that, image processing and even more data analysis,\
    \ including prediction \nmodels are the actual components of the phenotyping pipeline\
    \ that will allow full \nexploitation of new technological developments, in terms\
    \ of traits and platforms, \nfor HTFP.\n27.10  Conclusions\nAs a take-home message,\
    \ phenotyping is evolving very fast in terms of throughput, \nthe range of traits\
    \ that can be assessed, and the adaptation of the costs of sensors and \nplatforms\
    \ to a growing market for these technologies. However, the computing and \nJ.\
    \ L. Araus et al.\n509\nstatistical components still remain as the most commonly\
    \ perceived bottleneck that \ncurrently limits HTFP from reaching full operability.\
    \ This includes a wide range of \nareas: from automation of data capturing and\
    \ further data processing, to the use of \nthe data produced to drive prediction\
    \ models or even its integration and application \nin genomic selection.\nIn this\
    \ sense remote sensing techniques will become more accessible to breeders \nif\
    \ image analysis services were to become more widely available, affordable and\
    \ \nautomatized (i.e., customer friendly), providing curated phenotypic data in\
    \ near real \ntime. As examples, on board data pre-processing and 5G in-flight\
    \ data transmission \nare two of the main paths forward for simplified processing\
    \ and improved usability \nof remote sensing sensors. Both go hand-in-hand with\
    \ improvements in sensor- \nplatform integration, in which the sensor and platform\
    \ have become more and more \ninterconnected and thus are able to share GPS, altimetry,\
    \ IMU, power sources and \ntransmission capacities for improved efficiency and\
    \ operability. Manual UAV flights \nand separate manual programming of sensor\
    \ data capture are already in the past. In \nmany modern commercial UAVs, smartphone\
    \ connectivity already converts the \nUAV controller to an all-in-one command\
    \ station for programming flight paths, \nviewing UAV and sensor details in-flight,\
    \ and even limited data viewing and down-\nloading in real-time. Also, in smart\
    \ sensors, such as the Tetracam MCAW system \n(https://www.tetracam.com/Products-\
    \ Macaw.htm), images are calibrated to reflec-\ntance, corrected for parallax\
    \ and combined into multiband TIFFs or even processed \ninto programmable vegetation\
    \ indexes in flight by the on-board micro-processor \nand fast solid state disk\
    \ drives; these also include Wi-Fi to smartphone connectivity. \nEven though the\
    \ current wireless connectivity of these can’t keep pace with the \nonboard data\
    \ capture and automated pre-processing, both of these, including even \nUAV hyperspectral\
    \ data, should be both processable and transmissible in real-time \nwith 5G Wi-Fi,\
    \ enabling the automation of the rest of the pre-processing, from \nStructure-to-Motion\
    \ orthomosaicking and on to micro-plot extraction (given the \nproper GIS metadata),\
    \ either in PC or cloud-based services inter-connected to UAV \nfunctionality\
    \ or specific sensors or as a third party solution, such as DroneMapper, \nPix4Dcloud,\
    \ AgisoftCloud, Micasense AtlasCloud, DroneDeploy, and many more \n(http://dronemapper.com,\
    \ \nhttps://www.pix4d.com/product/pix4dcloud, \nhttps://\ncloud.agisoft.com, https://atlas.micasense.com,\
    \ see also https://micasense.com/\nsoftware- solutions). Given that there are\
    \ already precision agriculture crop pest/\ndisease UAVs that can detect specific\
    \ pest or disease presence or absence and spray \nwith onboard imaging and artificial\
    \ intelligence decision support, the next step for \nplant phenotyping must be\
    \ close behind.\nOn the other a routinely assessment under field conditions of\
    \ particular traits, \nrelevant for grain and fodder quality (e.g., contents of\
    \ amino acids, micronutrients, \nprovitamins), or for HTFP in frontier areas such\
    \ as the breeding for higher and more \nefficient photosynthesis. This will be\
    \ feasible through hyperspectral techniques, \nproviding not only computing capabilities\
    \ are optimized, but also cost of hyperspec-\ntral sensors and imagers decrease.\n\
    27 High Throughput Field Phenotyping\n510\nReferences\n 1. Araus JL, Cairns JE\
    \ (2014) Field high-throughput phenotyping: the new crop breeding fron-\ntier.\
    \ Trends Plant Sci 19:52–61\n 2. Pauli D, Chapman SC, Bart R, Topp CN, Lawrence-Dill\
    \ CJ, Poland J, Gore MA (2016) The \nquest for understanding phenotypic variation\
    \ via integrated approaches in the field environ-\nment. Plant Physiol 172:00592.2016.\
    \ https://doi.org/10.1104/pp.16.00592\n 3. Reynolds M, Chapman S, Crespo-Herrera\
    \ L, Molero G, Mondal S, Pequeno DNL, Pinto F, \nPinera-Chavez FJ, Poland J, Rivera-Amado\
    \ C, Saint Pierre C, Sukumaran S (2020) Breeder \nfriendly phenotyping. Plant\
    \ Sci 295:110396. https://doi.org/10.1016/j.plantsci.2019.110396\n 4. Rebetzke\
    \ GJ, Jimenez-Berni J, Fischer RA, Deery DM, Smith DJ (2019) High-throughput \n\
    phenotyping to enhance the use of crop genetic resources. Plant Sci 282:40–48.\
    \ https://doi.\norg/10.1016/j.plantsci.2018.06.017\n 5. Araus JL, Kefauver SC,\
    \ Zaman-Allah M, Olsen MS, Cairns JE (2018) Translating high- \nthroughput phenotyping\
    \ into genetic gain. Trends Plant Sci 23:451–466. https://doi.\norg/10.1016/j.tplants.2018.02.001\n\
    \ 6. Araus JL, Slafer GA, Royo C, Serret MD (2008) Breeding for yield potential\
    \ and stress adapta-\ntion in cereals. Crit Rev Plant Sci 27:377–412. https://doi.org/10.1080/07352680802467736\n\
    \ 7. Araus JL, Kefauver SC (2018) Breeding to adapt agriculture to climate change:\
    \ affordable \nphenotyping solutions (Review). Curr Opin Plant Biol 45:237–247.\
    \ https://doi.org/10.1016/j.\npbi.2018.05.003\n 8. Sanchez-Bragado R, Newcomb\
    \ M, Chairi F, Condorelli GE, Ward R, White JW, Maccaferri M, \nTuberosa R, Araus\
    \ JL, Serret MD (2020) Carbon isotope composition and the NDVI as pheno-\ntyping\
    \ approaches for drought adaptation in durum wheat: beyond trait selection. Agronomy\
    \ \n10:1679. https://doi.org/10.3390/agronomy10111679\n 9. Araus JL, Kefauver\
    \ SC, Zaman-Allah M, Olsen MS, Cairns JE (2018) Phenotyping: new crop \nbreeding\
    \ frontier. In: Meyers R (ed) Encyclopedia of sustainability science and technology.\
    \ \nSpringer, New York\n 10. Jin X, Zarco-Tejada P, Schmidhalter U, Reynolds MP,\
    \ Hawkesford MJ, Varshney RK, Yang T, \nNie C, Li Z, Ming B, Xiao Y, Xie Y, Li\
    \ S (2020) High-throughput estimation of crop traits: a \nreview of ground and\
    \ aerial phenotyping platforms. IEEE Geosci Remote Sens:1–33. https://\ndoi.org/10.1109/MGRS.2020.2998816\n\
    \ 11. Reynolds D, Baret F, Welcker C, Bostrom A, Ball J, Cellini F, Lorence A,\
    \ Chawade A, Khafif \nM, Noshita K, Mueller-Linow M, Zhou J, Tardieu F (2019)\
    \ What is cost-efficient phenotyp-\ning? Optimizing costs for different scenarios.\
    \ Plant Sci 282:14–22. https://doi.org/10.1016/j.\nplantsci.2018.06.015\n 12.\
    \ Andrade-Sanchez P, Gore MA, Heun JT, Thorp KR, Carmo-Silva AE, French AN, Salvucci\
    \ \nME, White JW (2014) Development and evaluation of a field-based high-throughput\
    \ pheno-\ntyping platform. Funct Plant Biol 41. https://doi.org/10.1071/FP13126\n\
    \ 13. Aasen H, Honkavaara E, Lucieer A, Zarco-Tejada PJ (2018) Quantitative remote\
    \ sensing at \nultra-high resolution with UAV spectroscopy: a review of sensor\
    \ technology, measurement \nprocedures, and data correction workflows. Remote\
    \ Sens 10:1091. https://doi.org/10.3390/\nrs10071091\n 14. Gracia-Romero A, Kefauver\
    \ SC, Fernandez-Gallego JA, Vergara-Díaz O, Nieto-Taladriz MT, \nAraus JL (2019)\
    \ UAV and ground image-based phenotyping: a proof of concept with durum \nwheat.\
    \ Remote Sens 11:1244. https://doi.org/10.3390/rs11101244\n 15. Maes WH, Steppe\
    \ K (2019) Perspectives for remote sensing with unmanned aerial vehi-\ncles in\
    \ precision agriculture. Trends Plant Sci 24:152–164. https://doi.org/10.1016/j.\n\
    tplants.2018.11.007\n 16. Eskandari R, Mahdianpari M, Mohammadimanesh F, Salehi\
    \ B, Brisco B, Homayouni S (2020) \nMeta-analysis of unmanned aerial vehicle (UAV)\
    \ imagery for agro-environmental monitoring \nusing machine learning and statistical\
    \ models. Remote Sens 12:3511. https://doi.org/10.3390/\nrs12213511\nJ. L. Araus\
    \ et al.\n511\n 17. Bolger M, Schwacke R, Gundlach H, Schmutzer T, Chen J, Arend\
    \ D, Opperman M, Weise S, \nLange M, Fiorani F, Spannagl M, Scholz U, Mayer K,\
    \ Usadel B (2017) From plant genomes \nto phenotypes. J Biotechnol 261:46–52.\
    \ https://doi.org/10.1016/j.jbiotec.2017.06.003\n 18. Brown TB, Cheng R, Siriault\
    \ XRR, Rungrat T, Murray KD, Trtilek M, Furbank RT, Badger \nM, Pogson BJ, Borevitz\
    \ JO (2014) TraitCapture: genomic and environment modelling of plant \nphenomic\
    \ data. Curr Opin Plant Biol 18:73–79. https://doi.org/10.1016/j.pbi.2014.02.002\n\
    \ 19. Furbank RT, Tester M (2011) Phenomics–technologies to relieve the phenotyping\
    \ bottleneck. \nTrends Plant Sci 16:635–644. https://doi.org/10.1016/j.tplants.2011.09.005\n\
    \ 20. Roth L, Hund A, Aasen H (2018) PhenoFly Planning Tool: flight planning for\
    \ high- resolution \noptical remote sensing with unmanned aerial systems. Plant\
    \ Methods 14:116. https://doi.\norg/10.1186/s13007- 018- 0376- 6\n 21. Singh A,\
    \ Ganapathysubramanian B, Singh AK, Sarkar S (2016) Machine learning for \nhigh-throughput\
    \ stress phenotyping in plants. Trends Plant Sci 21:110–124. https://doi.\norg/10.1016/j.tplants.2015.10.015\n\
    \ 22. Fernandez-Gallego JA, Lootens P, Borra-Serrano I, Derycke V, Haesaert G,\
    \ Roldán-Ruiz I, \nAraus JL, Kefauver SC (2020) Automatic wheat ear counting using\
    \ machine learning based on \nRGB UAV imagery. Plant J 103:1603–1613. https://doi.org/10.1111/tpj.14799\n\
    \ 23. Jin X, Madec S, Dutartre D, de Solan B, Comar A, Baret F (2019) High-throughput\
    \ mea-\nsurements of stem characteristics to estimate ear density and above-ground\
    \ biomass. Plant \nPhenomics 2019:1–10. https://doi.org/10.34133/2019/4820305\n\
    \ 24. Fernandez-Gallego JA, Kefauver SC, Vatter T, Aparicio Gutierrez N, Nieto-Taladriz\
    \ MT, \nAraus JL (2019) Low-cost assessment of grain yield in durum wheat using\
    \ RGB images. Eur \nJ Agron 105:146–156. https://doi.org/10.1016/j.eja.2019.02.007\n\
    \ 25. Gracia-Romero A, Verdara-Diaz O, Thierfelder C, Cairns JE, Kefauver SC,\
    \ Araus JL (2018) \nPhenotyping conservation agriculture management effects on\
    \ ground and aerial remote sens-\ning assessments of maize hybrids performance\
    \ in Zimbabwe. Remote Sens 10:349. https://doi.\norg/10.3390/rs10020349\n 26.\
    \ Madec S, Baret F, de Solan B, Thomas S, Dutartre D, Jezequel S, Hemmerlé M,\
    \ Colombeau \nG, Comar A (2017) High-throughput phenotyping of plant height: comparing\
    \ unmanned aer-\nial vehicles and ground LiDAR estimates. Front Plant Sci 8:1–14.\
    \ https://doi.org/10.3389/\nfpls.2017.02002\n 27. Garbulsky MF, Peñuelas J, Gamon\
    \ J, Inoue Y, Filella I (2011) The photochemical reflectance \nindex (PRI) and\
    \ the remote sensing of leaf, canopy and ecosystem radiation use efficiencies.\
    \ \nA review and meta-analysis. Remote Sens Environ 115:281–297. https://doi.org/10.1016/j.\n\
    rse.2010.08.023\n 28. Vergara-Diaz O, Vatter T, Kefauver SC, Obata T, Fernie AR,\
    \ Araus JL (2020) Assessing durum \nwheat ear and leaf metabolomes in the field\
    \ through hyperspectral data. Plant J 102:615–630. \nhttps://doi.org/10.1111/tpj.14636\n\
    \ 29. Ribes M, Russias G, Tregoat D, Fournier A (2020) Towards low-cost hyperspectral\
    \ single- \npixel imaging for plant phenotyping. Sensors 20:1132. https://doi.org/10.3390/s20041132\n\
    \ 30. Verrelst J, Malenovský Z, Van der Tol C, Camps-Valls G, Gastellu-Etchegorry\
    \ JP, Lewis \nP, North P, Moreno J (2019) Quantifying vegetation biophysical variables\
    \ from imaging \n spectroscopy data: a review on retrieval methods. Surv Geophys\
    \ 40:589–629. https://doi.\norg/10.1007/s10712- 018- 9478- y\n 31. Schaepman ME,\
    \ Ustin SL, Plaza AJ, Painter TH, Verrelst J, Liang S (2009) Earth system sci-\n\
    ence related imaging spectroscopy—an assessment. Remote Sens Environ 113:S123–S137.\
    \ \nhttps://doi.org/10.1016/j.rse.2009.03.001\n 32. Roche D (2015) Stomatal conductance\
    \ is essential for higher yield potential of C3 crops. Plant \nSci 34:429–453.\
    \ https://doi.org/10.1080/07352689.2015.1023677\n 33. Carmo-Silva E, Andralojc\
    \ PJ, Scales JC, Driever SM, Mead A, Lawson T, Raines CA, Parry \nMAJ (2017) Phenotyping\
    \ of field-grown wheat in the UK highlights contribution of light \nresponse of\
    \ photosynthesis and flag leaf longevity to grain yield. J Exp Bot 68:3473–3486.\
    \ \nhttps://doi.org/10.1093/jxb/erx169\n27 High Throughput Field Phenotyping\n\
    512\n 34. Lopes MS, Reynolds MP (2012) Stay-green in spring wheat can be determined\
    \ by spectral \nreflectance measurements (normalized difference vegetation index)\
    \ independently from phe-\nnology. J Exp Bot 63:3789–3798. https://doi.org/10.1093/jxb/ers071\n\
    \ 35. Molero G, Reynolds MP (2020) Spike photosynthesis measured at high throughput\
    \ indicates \ngenetic variation independent of flag leaf photosynthesis. Field\
    \ Crop Res 255:107866. https://\ndoi.org/10.1016/j.fcr.2020.107866\n 36. Sanchez-Bragado\
    \ R, Vicente R, Molero G, Serret MD, Maydup ML, Araus JL (2020) New \navenues\
    \ for increasing yield and stability in C3 cereals: exploring ear photosynthesis.\
    \ Curr \nOpin Plant Biol 56:223–234. https://doi.org/10.1016/j.pbi.2020.01.001\n\
    Open Access  This chapter is licensed under the terms of the Creative Commons\
    \ Attribution 4.0 \nInternational License (http://creativecommons.org/licenses/by/4.0/),\
    \ which permits use, sharing, \nadaptation, distribution and reproduction in any\
    \ medium or format, as long as you give appropriate \ncredit to the original author(s)\
    \ and the source, provide a link to the Creative Commons license and \nindicate\
    \ if changes were made.\nThe images or other third party material in this chapter\
    \ are included in the chapter's Creative \nCommons license, unless indicated otherwise\
    \ in a credit line to the material. If material is not \nincluded in the chapter's\
    \ Creative Commons license and your intended use is not permitted by \nstatutory\
    \ regulation or exceeds the permitted use, you will need to obtain permission\
    \ directly from \nthe copyright holder.\nJ. L. Araus et al.\n"
  inline_citation: '>'
  journal: Springer eBooks
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/978-3-030-90673-3_27.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: High Throughput Field Phenotyping
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.25165/j.ijabe.20201302.4280
  analysis: '>'
  authors:
  - Wenjing Zhu
  - Jinyang Li
  - Lin Li
  - Aichen Wang
  - Xinhua Wei
  - Hanping Mao
  citation_count: 2
  full_citation: '>'
  full_text: ">\nMarch, 2020                        Int J Agric & Biol Eng      Open\
    \ Access at https://www.ijabe.org                          Vol. 13 No. 2   189\
    \ \n \nNondestructive diagnostics of soluble sugar, total nitrogen and their ratio\
    \ of \ntomato leaves in greenhouse by polarized spectra–hyperspectral data fusion\
    \ \n \nWenjing Zhu1,2*, Jinyang Li1,2, Lin Li1,2, Aichen Wang1,2, Xinhua Wei1,2,\
    \ Hanping Mao1,2  \n(1. School of Agricultural Equipment Engineering, Jiangsu\
    \ University, Zhenjiang 212013, China; \n2. Key Laboratory of Modern Agricultural\
    \ Equipment and Technology, Ministry of Education, Jiangsu University, Zhenjiang\
    \ 212013, China) \n \nAbstract: Polarized spectra–hyperspectral data fusion technique\
    \ was used to estimate the soluble sugar (SS), total nitrogen (N), \nand their\
    \ ratio (SS/N), of greenhouse tomato leaves.  Fresh tomato leaves of five different\
    \ growth stages (seedling, flowering, \ninitial fruiting, mid-fruiting and picking\
    \ stage) and five different nitrogen treatments (severe stress 25%, moderate stress\
    \ 50%, \nmild stress 75%, normal 100%, and excess 150%) at every stage were collected\
    \ for spectra acquisition and SS and N \ndetermination.  Polarized reflectance\
    \ spectra were acquired with a polarization reflectance spectrum spectro-goniophotometer\
    \ \nsystem and four polarization degree features were extracted.  Hyperspectral\
    \ data were collected with a hyperspectral imaging \nsystem and four reflectance\
    \ spectrum features and eight image features were extracted.  Initially, models\
    \ were built with \npolarization degree features, image features, and spectral\
    \ features respectively.  Linear and nonlinear fusion methods were \ncomparatively\
    \ used for modeling based on normalized data of the three sources.  The results\
    \ suggest that the performances of \nSS/N models are better than those of N and\
    \ SS models, and the prediction capability of the Support Vector Machine (SVM)\
    \ \nmodels of N and SS/N are superior to those obtained with single kind feature.\
    \  This work indicates that the polarized \nspectrum-hyperspectral multidimensional\
    \ information detecting method can feasibly judge the tomato nutrient stress conditions.\
    \  \nMulti-features data fusion analysis technique can enhance the prediction\
    \ accuracy of spectral diagnostics technology in \nprecision agriculture. \nKeywords:\
    \ polarized spectra, hyperspectral, soluble sugar (SS), total nitrogen (N), data\
    \ fusion, tomato leaf \nDOI: 10.25165/j.ijabe.20201302.4280 \n \nCitation: Zhu\
    \ W J, Li J Y, Li L, Wang A C, Wei X H, Mao H P.  Nondestructive diagnostics of\
    \ soluble sugar, total nitrogen \nand their ratio of tomato leaves in greenhouse\
    \ by polarized spectra–hyperspectral data fusion.  Int J Agric & Biol Eng, 2020;\
    \ \n13(2): 189–197. \n \n1  Introduction \nCarbon (C) and nitrogen (N) are the\
    \ basic constituent elements \nof organic matter; thus, quantitative monitoring\
    \ of carbonaceous \nand nitrogenous substances in plants is an important research\
    \ \ndirection[1, 2].  N is the main component of proteins, chlorophyll \nand nucleic\
    \ acids, and is thus known as “the element of crop life”[3].  \nDifferent nutrient\
    \ physiologic conditions, especially N, have \ndifferent effects on the growth\
    \ rate of plants[4], dry matter \ndistribution[5], nutrient uptake state[6], soil\
    \ physical and chemical \nproperties[7], \nphotosynthesis[8], \nand \ncarbon \n\
    and \nnitrogen \nmetabolism[2].  Among these, the carbon-nitrogen ratio (C/N)\
    \ \nreflects the physiological status and growth vigor of the plant[9].  \nThe\
    \ C/N is expressed in a variety of forms, including total \ncarbon/total nitrogen[10],\
    \ (cellulose + lignin)/total nitrogen[11], and \nsoluble sugar/total nitrogen\
    \ (SS/N)[12].  In this study, the SS/N \nform was selected for study because tomato\
    \ soluble sugar (SS) \ncontent is one of the most crucial internal quality factors,\
    \ which can \n                                                 \nReceived date:\
    \ 2018-03-28    Accepted date: 2020-01-16 \nBiographies: Jinyang Li, PhD, Associate\
    \ Professor, research interest: biological \ninformation sensing, Email: by0817136@163.com;\
    \ Lin Li, Assistant Researcher, \nresearch interests: agricultural engineering,\
    \ Email: lilin@ujs.edu.cn; Aichen \nWang, Assistant researcher, research interests:\
    \ agricultural engineering, Email: \nwinterwac@163.com; Xinhua Wei, PhD, Professor,\
    \ research interest: intelligent \nagricultural equipment, Email: wei_xh@126.com;\
    \ Hanping Mao, PhD, \nProfessor, research interest: intelligent agricultural equipment,\
    \ maohp@ujs.edu.cn.  \n*Corresponding author: Wenjing Zhu, PhD, Assistant Researcher,\
    \ research \ninterests: agricultural information technology. Jiangsu University,\
    \ Zhenjiang \n212013, China.  Tel: +86-511-88797338, Email: zwj0410@foxmail.com.\
    \ \ndetermine the fruit sweetness and flavor quality, in addition to \nbeing sensitive\
    \ to nutritional status, and thus better reflects the \nplant growth status[13].\
    \  Plant physiology studies have already \nshown that the route of tomato fruit\
    \ SS accumulation is mainly \nthrough two cytological pathways: apoplastic unloading\
    \ and plastid \npathway unloading[14].  Both methods require long-distance \n\
    transport from the blade through the sieve tube, then unloading \nfrom the sieve\
    \ element/companion cell complexes to the fruit[15].  \nHence, the non-destructive\
    \ testing of leaves is important for the \ndiagnosis and management of N and SS/N\
    \ in tomato growth. \nIn recent years, numerous studies involving rapid estimation\
    \ \nof plant leaf nutrient requirements have been carried out with \nhyperspectral\
    \ imaging to replace time-consuming and costly \ntraditional chemical analysis.\
    \  Researchers attempted to apply \nhyperspectral imaging technique to diagnosis\
    \ and estimate plant \nproperties, such as nitrogen concentration in flue-cured\
    \ tobacco \nleaves[3], nitrogen deficiency effects on plant growth, leaf \nphotosynthesis,\
    \ and hyperspectral reflectance properties of \nsorghum[16], and chlorophyll concentration\
    \ to display the nitrogen \ndeficiency distribution map of cucumber leaves[17].\
    \  Analytical \nmodels were successfully developed to optimize remotely-sensed\
    \ \nvegetation indices for retrieving leaf biochemical constituents[18,19], \n\
    to investigate changes at the leaf scale[20,21] or canopy level[22,23].  \nIn\
    \ addition to detecting the major elements, an SS content model of \noilseed rape\
    \ leaves was built via a back-propagation neural \nnetwork[24].  Typical indices\
    \ aimed at N estimation and \nchlorophyll evaluation were tested to assess leaf\
    \ C/N in winter \nwheat and spring barley[25].  Leaf C/N ratios were estimated\
    \ in \nthree forest sites, with R2 of the models being more than 0.8 with \n190\
    \   March, 2020                        Int J Agric & Biol Eng      Open Access\
    \ at https://www.ijabe.org                         Vol. 13 No. 2 \nnot more than\
    \ 4 wavelengths being selected[26].  These studies \nshow that using hyperspectral\
    \ imaging technique to estimate plant \nN, SS, and C/N status is feasible.  However,\
    \ these studies used \nsensors to collect leaf spectral information at a fixed\
    \ angle, and \nfocused on wavelength information or strength of the light source,\
    \ \nsuch as reflectivity or reflection intensity.  In reality, light \nintensity,\
    \ wavelength, phase, and polarization contain large \namounts of information,\
    \ but polarization examination is rarely \nperformed.  The mechanisms of polarization\
    \ reflection spectra can \neffectively improve the modeling, and be useful to\
    \ the condition of \nisotropy diffuse reflection inversion.  Since the 1970s,\
    \ \ntechnologies for polarization reflection spectroscopy have rapidly \ndeveloped,\
    \ with applications in many research areas[27].  Polarized \nreflectance spectroscopy\
    \ was applied in a variety of plants both at \nthe single-leaf and canopy level,\
    \ such as laurel (Laurus nobilis), \nmullein (Verbascum thapsus L.)[28], wheat\
    \ (Triticum aestivum)[29], \nchrysina \ngloriosa \n(Gloriosa \nsuperba \nL.)[30],\
    \ \narabidopsis \n(arabidopsis thaliana Linn. Heynh.)[31] and citrus (Citrus \n\
    reticulate)[32].  Shibayama et al.[29] discussed the volume, specular, \nand hot-spot\
    \ scattering features of heading-stage wheat canopies. \nWu et al.[33] found the\
    \ five reflectance parameters of sedum \nspectabile boreau (Hylotelephium erythrostictum\
    \ Miq. H. Ohba) \nwere consistent for all spectral and spatial aspects by polarized\
    \ \nhyperspectral imaging, and concluded that polarization-measured \nreflectance\
    \ images can replace traditional remote sensing intensity \nreflectance images.\
    \  In addition, Tan and Khan[34] conducted \npolarimetric measurements of the\
    \ backscattered light from lilac \nleaves and found that depolarization ratio\
    \ was a good indicator of \nwater stress.  \nFew investigations of tomato carbon\
    \ and nitrogen metabolism \nby feature layer data fusion of polarized spectra–hyperspectral\
    \ \nmeasurements have been reported, although tomato is cultivated in \nseveral\
    \ Chinese provinces as a significant alimentary crop with \nlarge yield and planting\
    \ area.  Song et al.[35] proposed that \npolarized reflectance features of leaves\
    \ were mainly affected by \nviewing zenith angle, incident zenith angle, azimuth,\
    \ and polarizer \nangle after they observed single maize (Zea mays L.) leaves.\
    \  Liao \net al.[36] investigated the effects of observation angle on the \nestimation\
    \ of leaf area index of two types of summer maize, and \nestablished partial least\
    \ square regression models; the results show \nthat the simple ratio index model\
    \ has obtained the highest \nestimation accuracy, with R2 of 0.47 and RMSE of\
    \ 0.30, by the \ncombination of six observation angles.  Lü[37] measured the \n\
    polarization of a corn canopy in different growth periods, at the \nnadir before\
    \ heading, and found that polarized light accounts for up \nto 10% of the total\
    \ reflection.  Jay et al.[38] evaluated the potential \nof \nnadir \nand \noff-nadir\
    \ \nground-based \nspectro-radiometric \nmeasurements to remotely sense five plant\
    \ traits relevant for field \nphenotyping, namely, the leaf area index, leaf chlorophyll\
    \ and \nnitrogen contents, and canopy chlorophyll and nitrogen contents \nover\
    \ fourteen sugar beet (Beta vulgaris L.) cultivars.  They \nshowed great potential\
    \ to retrieve canopy nitrogen content, with \nRMSE = 10%, while the estimation\
    \ of leaf-level quantities was less \naccurate, with the best accuracy being RMSE\
    \ = 17%.  Our \nresearch team assessed the feasibility of determining the nitrogen\
    \ \nand potassium content of fresh, greenhouse-grown tomato leaves \nby using\
    \ a self-developed polarization reflectance spectrum \nspectro-goniophotometer\
    \ system, and found that it was more \naccurate to measure the nitrogen and potassium\
    \ content with \npolarized models[39,40]. \nIn this study, we extract data from\
    \ both the hyperspectral \nimaging \nsystem \nand \npolarized \nspectro-goniophotometer\
    \ \nmeasurements to obtain complex information.  We take into \nconsideration\
    \ both the characteristics of the continuous correlation \nof spectral data and\
    \ the advantages of specific spatial angles, \nmaking the predictive model more\
    \ robust.  We evaluate the \naccuracy and feasibility of establishing predictive\
    \ models of N, SS, \nand SS/N.  Rapid detection combined with data fusion provides\
    \ \nthe theoretical basis and technical support for predicting the carbon \nand\
    \ nitrogen metabolism parameters of crops. \n2  Materials and methods \n2.1  Sample\
    \ preparation  \nTomato (Lycopersicon esculentum Mill) seedlings with five \n\
    true leaves from the Vegetable Research Institute, Academy of \nAgricultural Sciences,\
    \ Liaoning Province, China, were transplanted \nindividually into pots filled\
    \ with perlite in a Venlo-type greenhouse \nat Jiangsu University in China (32.11ºN,\
    \ 119.27ºE).  Plants were \nspaced in double rows at a density of 30 cm2 per plant.\
    \  In contrast \nto conventional agriculture, the application and availability\
    \ of \nnutrients can be monitored easily in greenhouse systems because \neach\
    \ group of tomato roots had a fixed nutrient solution content \ndelivered by a\
    \ self-developed timed irrigation and collection \nsystem.  The average temperature\
    \ of the greenhouse was 23.26ºC, \nwhile the humidity was 55.18%. \nWe induced\
    \ five levels of N nutrition stress (25%, 50%, 75%, \n100%, and 150%) to the tomato\
    \ plants (Table 1).  Yamazaki \nnutrient solution was used for irrigation; the\
    \ nutrient solution was \nformulated to eliminate NO3 and NH4 without changing\
    \ the \nconcentrations of the other essential elements.  The same trace \nelement\
    \ nutrient solution was supplied to all plants, and consisted \nof: EDTA-Na2Fe\
    \ 20 mg/L, H3BO3 2.86 mg/L, MnSO4.4H2O  \n2.13 mg/L, ZnSO4.7H2O 0.22 mg/L, CuSO4.5H2O\
    \ 0.08 mg/L, and \n(NO4)6Mo7O24.4H2O 0.02 mg/L.  The solutions were supplied at\
    \ \na rate of 500 mL/plant.day for 1 month after transplanting, and \n1000 mL/d·plant\
    \ for an additional month until harvesting.  \nRecommended pesticides (carbendazim,\
    \ colloidal sulfur suspension \nagent) were used as needed to control greenhouse\
    \ insects. \n \nTable 1  Tomato nitrogen-stress macro-element solutions      \
    \                      mg·L-1 \nN content \nCa(NO3)2 \nKNO3 \n(NH4)H2PO4 \nMgSO4\
    \ \nComplement NaH2PO4 \nComplement (NH2)2CO3 \nComplement KCl \n25% \n89 \n101\
    \ \n19 \n246 \n60 \n0 \n224 \n50% \n177 \n202 \n39 \n246 \n40 \n0 \n149 \n75%\
    \ \n266 \n303 \n58 \n246 \n20 \n0 \n75 \n100% \n354 \n404 \n77 \n246 \n0 \n0 \n\
    0 \n150% \n354 \n404 \n77 \n246 \n0 \n4 \n0 \n \n2.2  Measurement of polarized\
    \ spectra data \nThe polarization reflectance spectra were collected using a \n\
    device that was specifically designed for leaf bidirectional \nreflectance (BRDF)\
    \ measurements.  This device was developed \nby the Key Laboratory of Modern Agricultural\
    \ Equipment and \nTechnology[39], Jiangsu University.  The instrument was preheated\
    \ \nMarch, 2020   Zhu W J, et al.  Nondestructive diagnostics of soluble sugar,\
    \ total nitrogen and their ratio of tomato leaves in greenhouse    Vol. 13 No.2\
    \   191 \nfor more than 1 hour to ensure that the instrument can work \nsteadily.\
    \  The operating mode of this device, and the BRDF \nmeasurement principle and\
    \ angle range, were as described in our \nprevious articles[39].  Light from a\
    \ halogen light source was \ntransmitted to an illumination manipulator through\
    \ an optical fiber \nand produced unpolarized radiation in the VIS-NIR spectrum.\
    \  \nThe spectral range spans 300-1000 nm; spectra were measured in \n0.443 nm\
    \ intervals, resulting in 1580 measurements.  For each \nsample, the spectral\
    \ collection was repeated at three points: the leaf \ntip, middle, and bottom,\
    \ and the average curves were taken as the \nfinal scan result, to minimize the\
    \ impact of random errors. \nDuring the growing period, a total of five spectra\
    \ were \ncollected, corresponding to the five stages of tomato growth: \nseedling\
    \ stage, flowering stage, initial fruiting stage, mid-fruiting \nstage, and picking\
    \ stage.  Developing a database of information on \nleaf analysis and fertility\
    \ practices over time facilitates the \ndiagnosis of problems as they occur. \
    \ In total, 96 fresh leaves were \ncollected for the determination of N and SS\
    \ content.  To avoid \nunwanted scattering, the laboratory must take dark measures\
    \ to \nensure a reflectance of less than 3% in the VIS-NIR spectrum.  \nThe temperature\
    \ in the laboratory was maintained at 25ºC and \nrelative humidity at 40%.  A\
    \ white reference panel (Spectralon, \nBaSO4, 99% reflectance) was used under\
    \ the same conditions to \nconvert the spectral reflectance before each test to\
    \ minimize \nenvironmental influences, and the scan time was 0.2 s.  The raw \n\
    polarization spectra of N stress samples are presented in Figure 1. \n \nFigure\
    \ 1  0º Polarization spectra of 96 N nutrition stress samples \n \n2.3  Measurement\
    \ of hyperspectral data \nHyperspectral data of tomato leaves were collected by\
    \ a \nhyperspectral imaging system.  The system was mainly composed \nof a hyperspectral\
    \ camera (V10E-QE, Spectral Imaging Ltd., \nFinland), a 150 W halogen lamp DC\
    \ tunable light source (2900-ER \n+9596-E, Illumination Technologies, USA), two-fiber\
    \ line light \nsource (PIN 9145+9530, Illumination Technologies, USA), moving\
    \ \nstage (MTS120, Beijing Optical Instrument Factory, China), \ncontroller (SC100,\
    \ Beijing Optical Instrument Factory, China), and \ncomputer components.  Before\
    \ the hyperspectral image data \nacquisition, the exposure time of the visible-near\
    \ infrared camera \nwas determined in advance to ensure that the image was clear\
    \ and \nthe speed of the displacement table was determined to avoid the \ndistortion\
    \ of the image size and spatial resolution.  The exposure \ntime was determined\
    \ after analysis and comparison to be 20 ms, \nand the displacement speed of the\
    \ displacement table was     \n1.25 mm/s.  The spectral range was 300-1000 nm\
    \ while the \nspectral resolution was 2.8 nm.  The sampling interval was 1.2 nm;\
    \ \nthus, a single acquisition can obtain 512 independent hyperspectral \nimages.\
    \  The black and white fields must be calibrated to set the \nreflectivity range\
    \ before the data acquisition, and then the \nsecond-order Butterworth filter\
    \ is used for digital filtering to \nremove noise interference.  \n2.4  Chemical\
    \ measurement \nThe chemical content determinations were performed in \nparallel\
    \ with the spectral test. The leaf samples were picked \nbetween 9:00 and 12:00\
    \ am Beijing local time into numbered \nself-styled bags, then sorted into the\
    \ professional plant preservation \nbox, immediately transported to the laboratory,\
    \ and then polarized \nreflection spectrum measurement experiments and hyperspectral\
    \ \nimage acquisition were performed.  After collection, the leaves \nwere placed\
    \ in an oven and dried at 80°C until a constant weight \nwas achieved.  The total\
    \ nitrogen content of the samples was \ndetermined by the Kjeldahl method.  Concentrated\
    \ sulfuric acid  \n(5 mL) was added to each sample, and samples were heated to\
    \ \n380ºC for 4 h according to the Kjeldahl protocol.  The N content \nof the\
    \ concentrates was then determined using a continuous flow \nAutoAnalyzer 3 (SEAL\
    \ Analytical Instruments, USA).  All \nchemical reagents used in the analyses\
    \ were of analytical grade; \nresults were expressed on a fresh-weight basis (mg/g).\
    \ \nThe anthrone colorimetric method was used to determine the \nSS content in\
    \ the leaves.  0.1 g of fresh plant leaves were placed \nin test tubes, 5 ml of\
    \ distilled water was added, a plastic film used \nfor sealing, and a boiling\
    \ water bath extraction for 30 min (repeated \nextraction 2 times) was performed.\
    \  The extract was filtered into a \n25 ml volumetric flask.  1 ml of the extract\
    \ was transferred from \nthe volumetric flask to the inside of the tube, 0.5 ml\
    \ of anthrone \nethyl acetate and 5 ml of concentrated sulfuric acid were added\
    \ in \nsequence, and the mixed liquid was sufficiently shaken and placed \nin\
    \ a boiling water bath for 30 min.  After cooling to room \ntemperature for 10\
    \ min, the absorbance was measured at a \nwavelength of 630 nm.  The SS content\
    \ was calculated based on \nthe standard curve that has been drawn using glucose\
    \ as a standard \nsample. \nThe SS/N is the sugar and nitrogen ratio of the mass\
    \ fraction \nand is a dimensionless index obtained by dividing the mass fraction\
    \ \nof soluble sugar SS by the mass fraction of total nitrogen N, and \nmultiplying\
    \ by the conversion factor F according to  \n/\n100%\nSS\nSS\nN\nF\n N\n\n\n\
    \              (1) \n2.5  Software and statistical analysis \nAll data processing\
    \ and analysis were performed using \nMATLAB Version 2010a (Mathworks, USA) for\
    \ Windows 7.  \nBRDF analyzer software (Isuzu Optics, Taiwan) was used for the\
    \ \nacquisition and analysis of raw BRDF spectral data.  ENVIV4.5 \n(Research\
    \ System, Inc, USA) was used for the acquisition and \nanalysis of raw hyperspectral\
    \ data.  The precisions of the model \nand validated model were evaluated with\
    \ the root mean square \nerror of cross-validation (RMSECV), the root mean square\
    \ error of \nprediction (RMSEP), and the correlation coefﬁcient (R).  Higher \n\
    R, lower RMSECV, and lower RMSEP indicate the higher \nprecision and accuracy\
    \ of a model. \n3  Results and discussion \n3.1  Extraction of optimum polarization\
    \ degree features \n3.1.1  Plant physiological analysis of polarization reflections\
    \ \nbased on scanning electron microscopy \nIn order to show that there is a link\
    \ between the nutritional \n192   March, 2020                        Int J Agric\
    \ & Biol Eng      Open Access at https://www.ijabe.org                       \
    \  Vol. 13 No. 2 \nstress and the microstructure of the tomato leaves, the surface\
    \ \nmicrostructures of the N-deficient and N-excess leaves were \nanalyzed by\
    \ scanning electron microscopy compared to the normal \nleaves.  Figures 2a-2c\
    \ show the surface microstructures of normal, \nN-deficient, and N-excess tomato\
    \ leaves at ×200 magnification, \nrespectively. \n \n \n \n \na. Control group,\
    \ ×200 \nb. N deficiency, ×200 \nc. N excess, ×200 \n \nFigure 2  Scanning electron\
    \ micrographs of N-stressed leaves \n \nIn Figure 2a, the pores can be seen clearly,\
    \ the blade surface is \nmainly distributed with tapered villi and a small number\
    \ of \nmushroom-shaped villi, as well as protruding vascular bundles.  \nDefensive\
    \ cells were half-moon shaped, and the cilia stretch well.  \nAlthough the epidermal\
    \ cells are irregular in shape, they are tightly \nfitted with each other.  Due\
    \ to the good growth of the chloroplast, \nthe corpus cavernous bodies, and the\
    \ fence tissue, the blade surface \nhas a uniform water-like structure, and the\
    \ mesophyll morphology \nis plump.  Most of the cilia lodged, and the stomas have\
    \ closed \ndue to lack of N (Figure 2b).  Significantly more folds and cracks\
    \ \non the surface of the leaf compare to Figure 2a, the epidermal cells \nare\
    \ dry and contraction because of undernourishment.  The leaf \nlooks more withered\
    \ and scorched.  Figure 2c shows that the blade \ntissue is loose and the folds\
    \ are obvious, due to the application of \nexcess N, resulting in excessive development\
    \ of the stratum \ncorneum and greater thickness than normal leaves.  The incident\
    \ \nlight is reflected by the surface of the object, resulting in \npolarization\
    \ degree features, which are mainly controlled by two \nfactors: the refractive\
    \ index of the object, and the incident angle.  \nThe change of polarization parameters\
    \ caused by the severity of \nnutrient stress of tomato leaves was investigated,\
    \ and the \npolarization parameters were judged and selected according to the\
    \ \nfollowing rule: Orthogonal experiments and range analyses were \nperformed\
    \ to verify the optimum angle combination from the \npolarized reflectance parameters.\
    \  Optimum angle combination \nexperiments were then conducted to fine-tune the\
    \ optimal \nparameters, which resulted in the following conditions: incident \n\
    zenith angle, 60º; viewing zenith angle, 45°; light source polarizer, \n0º; detector\
    \ polarizer, 45º; and azimuth, 180º.  The specific \nselection method was as described\
    \ in our previous articles[39,40]. \n3.1.2  Extraction of polarization degree\
    \ features \nSince there is significant noise in the hyperspectral spectrum \n\
    below 440 nm and above 950 nm, polarization data from 440-950 nm \nwere selected\
    \ for analysis.  The average polarization spectrum of \neach tomato sample in\
    \ the 0°, 90°, +45°, -45° direction was \nextracted.  Hence, the polarization\
    \ degree of each sample was \ncalculated at different wavelengths according to\
    \ the Stokes \nformula[41]: \n       \n0\n90\n90\n0\n45\n45\nr\nl\nI\nI\nI\nQ\n\
    I\nI\nS\nU\nI\nI\nV\nI\nI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\
    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n\
    \ (2) \nFigure 3 shows the polarization degree for different nutritional \nstresses,\
    \ in which it can be seen that the polarization degree \nincreases with the increase\
    \ of nitrogen supply level.  The \npolarization degree is maintained at a relatively\
    \ high level between \n350-680 nm, then the curve suddenly drops at 680-720 nm;\
    \ the \nnear-infrared region shows a lower degree of polarization and the \ncurve\
    \ tends to become flat beyond 720 nm.  The mean values of \npolarization in the\
    \ five growth stages were correlated with the \nchemical reference value of N,\
    \ as shown in Figure 4, where a \ndistinguishing line of significance level α\
    \ = 0.01 is shown to \nestimate the probability that the overall parameter may\
    \ fall within a \ncertain interval.  The smaller the value of α, the higher the\
    \ \nconfidence value or confidence level, and interval estimates are \nmore reliable.\
    \  \n \nFigure 3  Polarization degree for different nutritional stresses \n \n\
    \ \nFigure 4  Correlation analysis between polarization degree and \nN-content\
    \ of tomato leaves \nMarch, 2020   Zhu W J, et al.  Nondestructive diagnostics\
    \ of soluble sugar, total nitrogen and their ratio of tomato leaves in greenhouse\
    \    Vol. 13 No.2   193 \nThe specific steps of wavelength extraction are as follows:\
    \ \nfirstly, the sensitive band obtained by correlation analysis was \ndivided\
    \ into several wavelength groups, with each group containing \n10 wavelengths;\
    \ secondly, each group was sorted to form a subset \nof Ui according to the value\
    \ of the relevant coefficient; thirdly, the \nband with the largest correlation\
    \ coefficient was set to Us; finally, \nthe bands in Ui were compared with the\
    \ Us individually, and if the \ncorrelation coefficients with all the bands in\
    \ Us were less than 0.8, \nthey were selected.  The final results for sensitive\
    \ wavelengths \nobtained by the method were 380.49 nm, 655.41 nm, 744.48 nm, \n\
    and 850.58 nm. \n3.2  Feature extraction of hyperspectral data \n3.2.1  Extraction\
    \ of texture features under sensitive wavelengths \nPrincipal component analysis\
    \ (PCA) was used to reduce the \nspectral dimension after a 5×5 window median\
    \ filter was applied, \nand image segmentation at 700 nm, because the difference\
    \ between \nthe background and the leaf at 700 nm was the most pronounced.  \n\
    Figure 5 shows images of the first five principal components (PCs) \nof tomato\
    \ leaves.  The total accumulative contribution rate of variance \nfrom the top\
    \ five PCs (PC1, PC2, PC3, PC4, and PC5) was 99.00%, \nwhich means that the top\
    \ five scores for the 120 samples can \nexplain 99.00% of the raw spectral information\
    \ from all samples. \n \n \nPC1 \nPC2 \nPC3 \nPC4 \nPC5 \n \nFigure 5  PC images\
    \ of the first five PCs of the mildly-stressed (N = 75%) tomato leaf \n \nThe\
    \ weight coefficients of the first five principal components \nwere calculated\
    \ and plotted using PCA analysis.  The weight \ncoefficients can represent the\
    \ importance of an indicator item in the \noverall system.  Here, by analyzing\
    \ the weight coefficient of the \nfirst five principal components of the N-stressed\
    \ leaves, the \nmaximum absolute value of the weight coefficient can be shown,\
    \ \nwhich indicates that these wavelengths contribute most to the \nreaction N-stress.\
    \  Figure 6 shows the weight coefficients of the \nN-content of the first five\
    \ PCs after PCA; the final results for \nsensitive wavelengths were 464.91 nm,\
    \ 566.29 nm, 696.28 nm, and \n724.66 nm. \n \nFigure 6  Weight coefficient of\
    \ the N-content of the first five PCs \nafter PCA \n \nEight \ntexture \nfeatures\
    \ \naccording \nto \nthe \ngray \nlevel \nco-occurrence matrices (GLCM) were extracted\
    \ based on the above \nfour sensitive wavelengths: Mean (MEA), Variance (VAR),\
    \ \nHomogeneity (HOM), Entropy (ENT), Dissimilarity (DIS), \nAngular Second Moment\
    \ (ASM), Contrast (CON), and Correlation \n(COR).  These texture features are\
    \ the most representative and \npersuasive in the study of plant nutrient stress[42].\
    \ \n1\n1\n0\n0\n( , ) ( , )\nL\nL\ni\nj\nMEA\ni j p i j\n\n\n\n\n\n   \
    \           \n(3) \n1\n1\n0\n0\n(\n)(\n) ( , )\nL\nL\nx\ny\ni\nj\nVAR\ni\nu\n\
    j\nu\np i j\n\n\n\n\n\n\n\n\n \nwhere, \n1\n1\n0\n0\n( , )\nL\nL\nx\n\
    i\nj\nu\ni\np i j\n\n\n\n\n \n; \n1\n1\n0\n0\n( , )\nL\nL\ny\ni\nj\nu\n\
    j\np i j\n\n\n\n\n \n         (4) \n1\n1\n0\n0\n( , )\n1\nL\nL\ni\nj\n\
    p i j\nHOM\ni\nj\n\n\n\n\n\n\n\n\n               (5) \n1\n1\n0\n0\n(\
    \ , )ln ( , )\nL\nL\ni\nj\nENT\np i j\np i j\n\n\n\n\n \n            (6)\
    \ \n1\n1\n2\n0\n0\n( , )\n1\nL\nL\ni\nj\np i j\nDIS\ni\nj\n\n\n\n\n\n\n\
    \n\n               (7) \n1\n1\n2\n0\n0\n( ( , ))\nL\nL\ni\nj\nASM\np i j\n\
    \n\n\n\n\n               (8) \n1\n1\n2\n0\n0\n(\n)\n( , )\nL\nL\ni\nj\n\
    CON\ni\nj\np i j\n\n\n\n\n\n\n\n              (9) \n1\n1\n0\n0\n( , )\n\
    L\nL\nx\ny\ni\nj\nx\ny\nijp i j\nu u\nCOR\n \n\n\n\n\n\n\n\n \n\n\
    \n\n\n\n\n\n\n \nwhere, \n1\n1\n0\n0\n( , )\nL\nL\nx\ni\nj\nu\ni\np i\
    \ j\n\n\n\n\n \n;  \n1\n1\n0\n0\n( , )\nL\nL\ny\ni\nj\nu\nj\np i j\n\n\
    \n\n\n \n;  \n1\n1\n2\n0\n0\n(\n(\n)\n( , )\nL\nL\nx\nx\ni\nj\ni\nu\np i\
    \ j\n\n\n\n\n\n\n \n\n; \n1\n1\n2\n0\n0\n(\n(\n)\n( , )\nL\nL\ny\ny\n\
    i\nj\nj\nu\np i j\n\n\n\n\n\n\n \n\n  (10) \n \nTable 2  Correlation\
    \ coefficients between texture features and \nN-content at the sensitive wavelengths\
    \ \nSensitive \nwavelength MEA \nVAR \nHOM \nCON \nDIS \nENT \nASM \nCOR \n464.91\
    \ nm –0.4558 –0.2483 0.4703 –0.5850 –0.5038 –0.5795 0.6892 0.6798 \n566.29 nm\
    \ –0.3941 0.4420 –0.6772 0.6711 0.4235 0.1034 –0.6028 0.3546 \n696.28 nm 0.0971\
    \ 0.6867 –0.2792 0.4900 0.6578 0.1184 –0.1010 0.2316 \n724.66 nm 0.4593 0.5553\
    \ –0.5814 0.3862 0.4290 0.5804 –0.5854 0.6840 \n \n194   March, 2020         \
    \               Int J Agric & Biol Eng      Open Access at https://www.ijabe.org\
    \                         Vol. 13 No. 2 \nCorrelation analysis of the eight features\
    \ and the chemical \nreference value of N content were implemented, as shown in\
    \ Table \n2.  The texture features with the highest correlation values were \n\
    chosen for final modelling: VAR696.28, HOM566.29, CON566.29, \nDIS696.28, ASM566.29,\
    \ and COR724.66.  The correlation values of \nMEA and ENT under four sensitive\
    \ wavelengths are all below ±0.6, \nnon-adoption.  Texture feature is often used\
    \ as a feature for image \nclassification and information extraction when the\
    \ spectral features \nof ground objects are similar.  It is formed by the repeated\
    \ \noccurrence of gray distribution in the spatial position.  Therefore, \nthere\
    \ is a certain gray relationship between two pixels separated by \na certain distance\
    \ in the image space, that is, the spatial correlation \ncharacteristics of gray\
    \ in the image.  Texture features are not \nbased on pixel features it needs to\
    \ be calculated in the area \ncontaining multiple pixel points.  In pattern matching,\
    \ this kind of \nregional feature has great advantages and cannot be matched \n\
    successfully because of local deviation.  The correlation values of \nMEA and\
    \ ENT under four sensitive wavelengths are all below ±0.6 \nwhich illustrate that\
    \ these two methods have fewer advantages in \npattern matching and lower matching\
    \ success rate. \n3.2.2  Optimal reflection spectrum features selected by iPLS-GA\
    \ \nFigure 7a presents the raw spectra profile of all the samples.  \nRaw spectra\
    \ contained background information and noise in \naddition to the sample information.\
    \  Spectral pretreatment is an \neffective mean to overcome spectroscopic instability.\
    \  Traditional \nmethods of spectral pre-processing include smoothing, derivative,\
    \ \nstandard normal variate transformation (SNV), multiplicative \nscatter correction,\
    \ Fourier transform, and min/max normalization[43].  \nThe best performance was\
    \ shown by SNV, which was thus selected \nfor this study.  SNV transformation\
    \ was performed for each \nspectrum, individually, by subtracting the mean of\
    \ the spectrum \nand scaling with the standard deviation of the spectrum, as \n\
    illustrated by: \n,SNV\n2\n1(\n) / (\n1)\ni\ni\nn\ni\ni\nx\nx\nx\nx\nx\nn\n\n\
    \n\n\n\n\n         (11) \nwhere, xi,SNV is the SNV transformed spectral value\
    \ for the ith \nvariable; xi is the ith variable in the raw spectrum, and x  is\
    \ the \nmean of the raw spectrum[44].  The spectra after SNV \npreprocessing are\
    \ presented in Figure 7b. \n \n   a.                                         \
    \        b. \nFigure 7  Raw spectra (a) and SNV preprocessed spectra (b) of tomato\
    \ leaf samples \n \nThe spectral preprocessing method cannot eliminate all the\
    \ \ninvalid information such as instrument noise and sample \nbackground noise\
    \ contained in the original spectrum; therefore, it is \nnecessary to optimize\
    \ the independent variables and extract the \neffective information from the redundant\
    \ data.  Hence, the \nreduced model predictive ability and robustness will be\
    \ enhanced.  \nIn this study, we used an interval partial least squares-genetic\
    \ \nalgorithm (iPLS-GA) to optimize the variables.  The most \nprominent advantage\
    \ of the iPLS approach is to find out the most \nrelevant intervals for the quality\
    \ of the test[45].  The GA simulates \nthe natural genetic mechanism and natural\
    \ selection of the \nbiological world and is more suitable for solving complex,\
    \ \nnonlinear optimization problems[46].  The sensitive wavelengths of \nthe N\
    \ were found to be 741.48 nm, 755.74 nm, 767.44 nm, and \n784.37 nm using iPLS-GA.\
    \ \n3.3  Model calibrations \nAll 96 samples for each nutrient element were randomly\
    \ \ndivided into two subsets: a calibration set, used to build the model, \nand\
    \ a prediction set, used to test the robustness of the model.  To \navoid bias\
    \ in the division of the subsets, the division was \nperformed as follows: all\
    \ samples were sorted according to their \nrespective y-value (the measured reference\
    \ value of N and SS \ncontent).  Two abnormal samples were removed according to\
    \ the \nMahalanobis \ndistance. \n \nTo \nreach \na \n2/1 \ndivision \nof \ncalibration/prediction\
    \ spectra, one spectrum from every three \nsamples was added to the prediction\
    \ set, so that the final calibration \nset contained 80 spectra and the remaining\
    \ 40 spectra constituted \nthe prediction set.  The range of y-values in the calibration\
    \ set \ncovered the range in the prediction set; therefore, the distribution of\
    \ \nthe samples was appropriate (Table 3). \n \nTable 3  Reference measurement\
    \ results for samples \nNutrients \nUnits \nSet \nS.N.a \nRange \nMean \nS.D.b\
    \ \nN \n% \nCalibration set \n70 \n3.40-8.08 \n5.37 \n2.35 \n% \nPrediction set\
    \ \n35 \n3.85-7.92 \n5.29 \n2.21 \nSS \n% \nCalibration set \n70 \n0.81-7.59 \n\
    4.02 \n2.69 \n% \nPrediction set \n35 \n0.92-7.08 \n3.93 \n2.47 \nNote: a S.N.:\
    \ sample number.  b S.D.: standard deviation. \n \nA total of 14 feature variables\
    \ were used for modeling, \nincluding 4 polarization degree features at 380.49\
    \ nm, 655.41 nm, \n744.48 nm, and 850.58 nm; 6 texture features (VAR696.28, HOM566.29,\
    \ \nCON566.29, DIS696.28, ASM566.29, and COR724.66), and 4 reflection \nspectrum\
    \ features at 741.48 nm, 755.74 nm, 767.44 nm, and  \n784.37 nm.  \nLinear models\
    \ (Multiple Linear Regression (MLR)[47] and \nPartial Least Squares (PLS)), and\
    \ nonlinear models (Support \nVector Machine (SVM)[48] and Back propagation Artificial\
    \ Neural \nNetwork (BPANN)[49]) were both applied comparatively in model \ncalibration.\
    \  From the results for predicting N, the best data fusion \nmodel was achieved\
    \ by BPANN with Rc = 0.9845, RMSEC = \n0.1031, and Rp = 0.9400, RMSEP = 0.1995.\
    \  Figure 8a illustrates \nthe predicted and measured values of N for the calibration\
    \ and \nprediction sets using BPANN.  Among the SS results, the best \ndata fusion\
    \ model for prediction was also achieved by BAPNN \nwith Rc = 0.9845, RMSEC =\
    \ 0.2893, and Rp = 0.9315, and \nRMSEP = 0.5559.  Figure 8b illustrates the predicted\
    \ and \nMarch, 2020   Zhu W J, et al.  Nondestructive diagnostics of soluble sugar,\
    \ total nitrogen and their ratio of tomato leaves in greenhouse    Vol. 13 No.2\
    \   195 \nmeasured values of SS for the calibration and prediction sets using\
    \ \nBPANN.  After normalization and optimization, the best data \nfusion model\
    \ for SS/N prediction was achieved by SVM with Rc = \n0.9679, RMSEC = 0.0077,\
    \ and Rp = 0.9466, RMSEP = 0.0259, \nand corresponding to C = 16 and g = 0.5.\
    \  Figure 8c shows the \npredicted and measured values of SS/N for the calibration\
    \ and \nprediction sets using SVM.  As investigated from the results, the \nsix\
    \ models performed well in the prediction of N, SS, and SS/N \nwhile using the\
    \ data fusion as the model input. \n \na. \n \nb. \n \nc. \nFigure 8  (a) Reference\
    \ determination versus of calibration (*) and \nprediction (ο) set data of BP-ANN\
    \ model of N. \n(b) Reference determination versus of calibration (*) and prediction\
    \ \n(ο) set data of the BP-ANN model of SS. \n(c) Reference determination versus\
    \ of calibration (*) and prediction \n(ο) set data of the SVM model of SS/N \n\
    \ \n3.4  Discussion  \nThe combination of polarization reflectance spectra and\
    \ \nhyperspectral data analysis was systematically studied in this work \nand\
    \ achieved a good performance in quantitatively measuring \nnutrition in tomato\
    \ leaves, compared with the individual results of \nthe two systems.  The detailed\
    \ discussion of the results can be \nsummarized as follows. \nRegarding the effect\
    \ of different nitrogen levels on SS \naccumulation, the five growth cycle stages\
    \ of the tomato plant \ncorrespond to different nitrogen gradient levels, and\
    \ many other \nelements and trace elements are normally supplied.  The \ndistribution\
    \ of SS content in each growth cycle was analyzed and \nsome conclusions can be\
    \ drawn.  Firstly, with the continuous \ngrowth of tomato plants, the accumulation\
    \ of SS showed a steady \ngrowth trend.  Studies have shown that tomato SS gradually\
    \ \nincreases in the mid-term and harvest period, especially glucose \nand fructose,\
    \ and reaches the highest level at maturity.  This may \nbe related to the change\
    \ of sucrase activity, because it is reported \nthat the sucrase activity is gradually\
    \ increased with the \ndevelopment of the fruit, and reaches a maximum, resulting\
    \ in the \nstrongest hydrolysis of sugar[50].  Secondly, in one of the five \n\
    growth cycles, the SS content showed a gradual decrease with the \nincrease of\
    \ nitrogen application rate.  In short, the amount of \nnitrogen applied was negatively\
    \ correlated with the content of SS in \ntomato leaves.  This is not consistent\
    \ with the report of Li \nYuanxin, whose study showed that nitrogen fertilizer\
    \ can increase \nthe tomato SS content[51].  This is because the object of Yuanxin’s\
    \ \ndetection was tomato fruit, while the object of this study is the leaf.  \n\
    After the fruit is developed, the fruit is a strong metabolic pool, and \nthe\
    \ leaves need to transport more SS to the fruit.  This result is \nconsistent\
    \ with Wang Li's study, which pointed out that the content \nof SS in the leaves\
    \ decreased rapidly 25 days after the flowering \ntime and early fruiting period[50].\
    \  Moreover, in the early stage of \nnitrogen deficiency, plant leaf color is\
    \ not very different from the \nnaked eye, making it difficult to distinguish\
    \ whether the virus is \ninfected only with color characteristics[52].  Texture\
    \ parameters, \nsuch as ENT, CON, and HOM, may change according to leaf \nmicrostructure\
    \ structure and color shading in the canopy[42].  \nHowever, the image and reflectance\
    \ spectra cannot be fully \ncaptured at the early stage when the leaf changes\
    \ are not obvious.  \nThe degree of polarization calculated by the scanning polarization\
    \ \nreflected light contains information related to the surface and inner \nlayer\
    \ of the leaf[28,53].  In this study, polarization detection is \nregarded as\
    \ an effective supplement, and it is believed that \npolarization measurement\
    \ can obtain some information that cannot \nbe obtained by traditional intensity\
    \ measurement, and thus can \neffectively improve the model accuracy. \nThe limitations\
    \ of computer vision technique to evaluate \nnutrition deficiency in tomato leaves\
    \ lead to the idea of introducing \na fusion technique by combining data from\
    \ two systems, which \nconsiders texture, morphology as well as color features.\
    \ \nFinally, in order to highlight the efficiency of the data fusion \nmethodology\
    \ proposed in this study, models calibrated with \nseparate data sources were\
    \ compared.  Table 4 shows the results \nfrom the separate features and models\
    \ of data fusion for N, SS, and \nSS/N.  In this table, Mp, Mt, and Mr represent\
    \ models built by \npolarization degree features, texture features, and reflection\
    \ \nspectrum features, respectively.  Mp+Mt+Mr represents the data \nfusion model.\
    \  The best model from each separate feature (Mp, Mt, \nand Mr) is also shown\
    \ in Table 4, they conducted all the \npossibilities and found the optimum combination.\
    \  It can be \nobserved that the data fusion models achieved better performance\
    \ \nthan the separate feature models, regardless of the linear model \n(MLR, PLS)\
    \ or the nonlinear model (SVM and BPANN).  In \naddition, the SS/N models obtained\
    \ sharply increased predictive \n196   March, 2020                        Int\
    \ J Agric & Biol Eng      Open Access at https://www.ijabe.org               \
    \          Vol. 13 No. 2 \naccuracy.  This proved that the information extracted\
    \ from the \npolarized spectrum, reflection spectrum, and texture data, were \n\
    mutually complementary, and the models could provide better \nresults.  In addition,\
    \ among the models based on data fusion, the \nresults did not have significant\
    \ differences take N, SS, or SS/N into \nconsideration separately. \n \nTable\
    \ 4  Evaluation of the data fusion models \nModels \nMethods \nN \nSS \nSS/N \n\
    RC \nRMSEC \nRP \nRMSEP \nRC \nRMSEC \nRP \nRMSEP \nRC \nRMSEC \nRP \nRMSEP \n\
    Mp \nPLS \n0.9283 \n0.5426 \n0.9087 \n0.7465 \n0.6922 \n1.1527 \n0.6426 \n1.2392\
    \ \n0.7402 \n1.2552 \n0.7046 \n1.3085 \nMt \nMLR \n0.9633 \n0.3832 \n0.9183 \n\
    0.4525 \n0.9321 \n0.3216 \n0.7937 \n0.7965 \n0.8797 \n0.8202 \n0.8207 \n1.4568\
    \ \nMr \nMLR \n0.9044 \n0.6524 \n0.8678 \n0.9817 \n0.8848 \n1.1598 \n0.7510 \n\
    1.8761 \n0.7128 \n1.5964 \n0.5584 \n1.5507 \nMp + Mt + Mr \nMLR \n0.9624 \n0.5006\
    \ \n0.9373 \n0.5448 \n0.9483 \n0.5205 \n0.9142 \n0.6139 \n0.9378 \n0.1211 \n0.9315\
    \ \n0.1734 \nMp + Mt + Mr \nPLS \n0.9434 \n0.5161 \n0.9310 \n0.5921 \n0.9074 \n\
    0.6838 \n0.9075 \n0.6544 \n0.9351 \n0.1236 \n0.9307 \n0.1738 \nMp + Mt + Mr \n\
    SVR-GS \n0.9562 \n0.1166 \n0.9291 \n0.2217 \n0.9520 \n0.0047 \n0.9147 \n0.0068\
    \ \n0.9679 \n0.0077 \n0.9466 \n0.0259 \nMp + Mt + Mr \nSVR-PSO \n0.9521 \n0.1268\
    \ \n0.9289 \n0.2215 \n0.9389 \n0.0061 \n0.9136 \n0.0068 \n0.9663 \n0.0081 \n0.9422\
    \ \n0.0285 \nMp + Mt + Mr \nBP-ANN \n0.9845 \n0.1031 \n0.9400 \n0.1995 \n0.9845\
    \ \n0.2893 \n0.9315 \n0.5559 \n0.9859 \n0.0585 \n0.9254 \n0.1794 \n \nThis combination\
    \ of two sets of data could produce different \nresponses and may provide further\
    \ information.  Thus, the data \nfusion technique of different modalities[54]\
    \ could provide better \ninformation compared to a single modal system.  If the\
    \ \neigenvector is too large, the running time and effort of the fusion \nsystem\
    \ will be affected.  Nowadays, there are many information \nresources and the\
    \ requirement of information processing ability is \nincreasing.  Data fusion\
    \ technology can help people deal with \ncomplex information processing and judgment,\
    \ and has high \nprecision, practicability and feasibility. \n4  Conclusions \n\
    This paper proposed a novel data fusion methodology of \nfeature variables for\
    \ an advanced instrumental measurement of \nplant physiological indexes.  N, SS,\
    \ and SS/N in tomato leaves \nwere used to verify the feasibility of this methodology.\
    \  The \nproposed data fusion methodology showed obvious superiority in \nboth\
    \ predictability and stability of models in contrast to traditional \nmethodologies.\
    \  It can be concluded that this method is not as \nlimited by traditional computer\
    \ vision and spectroscopy technology, \nespecially regarding the poor accuracy\
    \ of the type of nutrition \ndetermination.  It provides a theoretical basis for\
    \ the establishment \nof a more precise automatic greenhouse system and promises\
    \ to be \nsignificant in the development of precision agriculture technology.\
    \ \n \nAcknowledgements \nThe authors are grateful to the financial support by\
    \ China \nnational key research and development plan (2017YFD0700504), \nChina\
    \ Postdoctoral Science Foundation (2016M601743), Senior \ntalent research start-up\
    \ fund of Jiangsu University (14JDG151), \nNatural Science Foundation of China\
    \ \nYouth \nFund(61901194), \nNatural \nScience \nYouth \nFoundation \nof \nJiangsu\
    \ \nProvince \n(BK20180863, BK20180861) and Priority Academic Program \nDevelopment\
    \ \nof \nJiangsu \nHigher \nEducation \nInstitutions \n(PAPD-2018-87). \n \n[References]\
    \ \n[1] \nLequeue G, Draye X, Baeten V.  Determination by near infrared \nmicroscopy\
    \ of the nitrogen and carbon content of tomato (Solanum \nlycopersicum L.) leaf\
    \ powder.  Sci Rep, 2016; 6: 33183. \n[2] \nSchlüter U, Mascher M, Colmsee C,\
    \ Scholz U, Bräutigam A,  Fahnenstich \nH, et al.  Maize source leaf adaptation\
    \ to nitrogen deficiency affects not \nonly nitrogen and carbon metabolism but\
    \ also control of phosphate \nhomeostasis.  Plant Physiology, 2012; 160(3): 1384–1406.\
    \ \n[3] \nJia F F, Liu G S, Liu D S, Zhang Y Y, Fan W G, Xing X X.  Comparison\
    \ \nof different methods for estimating nitrogen concentration in flue-cured \n\
    tobacco leaves based on hyperspectral reflectance.  Field Crops Research, \n2013;\
    \ 150: 108–114. \n[4] \nDe Pascale S, Maggio A, Orsini F, Barbieri G.  Cultivar,\
    \ soil type, \nnitrogen source and irrigation regime as quality determinants of\
    \ organically \ngrown tomatoes.  Scientia Horticulturae, 2016; 199: 88–94. \n\
    [5] \nSritontip C, Khaosumain Y, Changjeraja S.  Different nitrogen \nconcentrations\
    \ affecting chlorophyll and dry matter distribution in \nsand-cultured longan\
    \ trees.  Acta Hortic, 2014; 1024: 227–233. \n[6] \nOroka F O.  Mulching effects\
    \ and nitrogen application on the performance \nof Zea mays L: crop growth and\
    \ nutrient accumulation.  International \nLetters of Natural Sciences, 2016; 51:\
    \ 36–42. \n[7] \nGuzman J G, Godsey C B, Pierzynski G M, Whitney D A, Lamond R\
    \ E.  \nEffects of tillage and nitrogen management on soil chemical and physical\
    \ \nproperties after 23 years of continuous sorghum.  Soil and Tillage \nResearch,\
    \ 2006; 91(1-2): 199–206. \n[8] \nHorchani F, Hajri R, Aschi-Smiti S.  Effect\
    \ of ammonium or nitrate \nnutrition on photosynthesis, growth, and nitrogen assimilation\
    \ in tomato \nplants.  Journal of Plant Nutrition and Soil Science, 2010; 173(4):\
    \ \n610–617. \n[9] \nFatima T, Teasdale J R, Bunce J, Mattoo A K.  Tomato response\
    \ to legume \ncover crop and nitrogen: differing enhancement patterns of fruit\
    \ yield, \nphotosynthesis and gene expression.  Functional Plant Biology, 2012;\
    \ \n39(3): 246–254. \n[10] Kruse J, Hänsch R, Mendel R R, Rennenberg H.  The role\
    \ of root nitrate \nreduction in the systemic control of biomass partitioning\
    \ between leaves \nand roots in accordance to the C/N-status of tobacco plants.\
    \  Plant and \nSoil, 2010; 332: 387–403. \n[11] Prause J  Fernandez Lopez C. \
    \ Litter decomposition and lignin/cellulose \nand lignin/total nitrogen rates\
    \ of leaves in four species of the Argentine \nSubtropical forest.  Agrochimica,\
    \ 2007; 51(6): 294–300. \n[12] Korus K, Conley M E, Paparozzi E T.  Qualitative\
    \ and quantitative \nanalysis of soluble sugar content in leaves of hydroponically\
    \ grown \nSwedish ivy at varying periods of nitrogen deficiency and subsequent\
    \ \nre-greening.  HortScience, 2008; 1285. \n[13] Barickman T C, Kopsell D A,\
    \ Sams C E.  Abscisic acid improves tomato \nfruit quality by increasing soluble\
    \ sugar concentrations.  Journal of Plant \nNutrition, 2017; 40(7): 964–973. \n\
    [14] Baxter C J, Carrari F, Bauke A, Overy S, Hill S A, Quick P W, et al.  Fruit\
    \ \ncarbohydrate metabolism in an introgression line of tomato with increased\
    \ \nfruit soluble solids.  Plant and Cell Physiology, 2005; 46(3): 425–437. \n\
    [15] Taiz L Zeiger E, Møller I M, Murphy A.  Plant physiology and \ndevelopment.\
    \ Sinauer Associates, 2015. \n[16] Zhao D L, Reddy K R, Kakani V G, Reddy V R.\
    \  Nitrogen deficiency \neffects on plant growth, leaf photosynthesis, and hyperspectral\
    \ reflectance \nproperties of sorghum.  European Journal of Agronomy, 2005; 22(4):\
    \ \n391–403. \n[17] Shi.J Y, Zou X B, Zhao J W, Wang K L, Chen Z W, Huang X W,\
    \ et al.  \nNondestructive diagnostics of nitrogen deficiency by cucumber leaf\
    \ \nchlorophyll distribution map based on near infrared hyperspectral imaging.\
    \  \nScientia Horticulturae, 2012; 138: 190–197. \n[18] Sun J, Wang Y, Wu X H,\
    \  Zhang X D.  The nitrogen quantitative model \nbased on hyperspectral image\
    \ of tomato leaf.  Advanced Materials \nMarch, 2020   Zhu W J, et al.  Nondestructive\
    \ diagnostics of soluble sugar, total nitrogen and their ratio of tomato leaves\
    \ in greenhouse    Vol. 13 No.2   197 \nResearch, Trans, 2012; 466–467: 191–195.\
    \ \n[19] Liu Y L, Lyu Q, He S L, Yi S L, Liu X F, Xie R J, et al.  Prediction\
    \ of \nnitrogen and phosphorus contents in citrus leaves based on hyperspectral\
    \ \nimaging.  Int J Agric & Biol Eng , 2015; 8(2): 80–88. \n[20] Zhu Y, Wang W,\
    \ Yao X.  Estimating leaf nitrogen concentration (LNC) of \ncereal crops with\
    \ hyperspectral data.  In: Thenkabail P S (Ed.), Lyon J G \n(Ed.).  Hyperspectral\
    \ Remote Sensing of Vegetation.  Boca Raton: CRC \nPress, 2012; pp.187–206. \n\
    [21] Sun J, Shi S, Gong W, Yang J, Du L, Song S L, et al.  Evaluation of \nhyperspectral\
    \ LiDAR for monitoring rice leaf nitrogen by comparison with \nmultispectral LiDAR\
    \ and passive spectrometer.  Sci Rep, 2017; 7: 40362. \n[22] Clevers J G P W,\
    \ Kooistra L.  Using hyperspectral remote sensing data for \nretrieving canopy\
    \ chlorophyll and nitrogen content.  IEEE Journal of \nSelected Topics in Applied\
    \ Earth Observations and Remote Sensing, 2012; \n5(2): 574–583. \n[23] Song X,\
    \ Xu D Y, He L, Feng W, Wang Y H, Wang Z J, et al.  Using \nmulti-angle hyperspectral\
    \ data to monitor canopy leaf nitrogen content of \nwheat.  Precision Agriculture,\
    \ 2016; 17: 721–736. \n[24] Zhang C, Liu F, Kong W W, Cui C, He Y, Zhou W J. \
    \ Estimation and \nvisualization of soluble sugar content in oilseed rape leaves\
    \ using \nhyperspectral imaging.  Transaction of the ASABE, 2016; 59(6): \n1499–1505.\
    \ \n[25] Xu X G, Gu X H, Song X Y, Xu B, Yu H Y, Yang G J, et al.  Assessing the\
    \ \nratio of leaf carbon to nitrogen in winter wheat and spring barley based on\
    \ \nhyperspectral data.  In: Proc. SPIE 9998, Remote Sensing for Agriculture,\
    \ \nEcosystems, and Hydrology XVIII.  Edinburgh: SPIE, 2016. 999810. \n[26] Shi\
    \ R H, Niu Z, Zhuang D F.  Feasibility of estimating leaf C/N ratio with \nhyperspectral\
    \ remote sensing data.  Remote Sensing Technology and \nApplication, 2003; 18(2):\
    \ 76–80. (in Chinese) \n[27] Chen H B, Fan X H, Han Z G.  A review on remote sensing\
    \ from \nPOLDER multispectral, multidirectional and polarized measurements.  \n\
    Remote Sensing Technology and Application, 2006; 21(2): 83–92. (in \nChinese)\
    \ \n[28] Raven P N, Jordan D L, Smith C E.  Polarized directional reflectance\
    \ \nfrom laurel and mullein leaves.  Optical Engineering, 2002; 41(5): \n1002–1012.\
    \ \n[29] Shibayama M, Sakamoto T, Kimura A.  A multiband polarimetric imager \n\
    for field crop survey:―Instrumentation and preliminary observations of \nheading-stage\
    \ wheat canopies―.  Plant Production Science, 2011; 14(1): \n64–74. \n[30] del\
    \ Ŕıo L F, Arwin H, Järrendahl K.  Polarization of light reflected from \nChrysina\
    \ gloriosa under various illuminations.  Materials Today: \nProceedings, 2014;\
    \ 1: 172–176. \n[31] Maxwell D J, Partridge J C, Roberts N W, Boonham N, Foster\
    \ G D.  The \neffects of surface structure mutations in Arabidopsis thaliana on\
    \ the \npolarization of reflections from virus-infected leaves.  PloS One, 2017;\
    \ \n12(3): e0174014. \n[32] Pourreza A, Lee W S, Etxeberria E, Zhang Y.  Identification\
    \ of citrus \nHuanglongbing disease at the pre-symptomatic stage using polarized\
    \ \nimaging technique.  IFAC-PapersOnLine, 2016; 49(16): 110–115. \n[33] Wu T\
    \ X, Zhang L F, Cen Y, Huang C P, Sun X J, Zhao H Q, et al.  \nPolarized spectral\
    \ measurement and analysis of sedum spectabile boreau \nusing a field imaging\
    \ spectrometer system.  IEEE Journal of Selected \nTopics in Applied Earth Observations\
    \ and Remote Sensing, 2013; 6(2): \n724–730. \n[34] Tan S X, Kabir Khan A S M.\
    \  Water stress detection of lilac leaves using a \npolarized laser.  In: Proc.\
    \ SPIE 9610, Remote Sensing and Modeling of \nEcosystems for Sustainability XII.\
    \  San Diego: SPIE, 2015. 96100M. \n[35] Song K S, Zhang B, Zhao Y S, Wang Z M,\
    \ Du J.  Study of polarized \nreflectance of corn leaf and its relationship to\
    \ laboratory measurements of \nbi-directional reflectance.  Journal of Remote\
    \ Sensing, 2007; (5): 632–640. \n[36] Liao Q H, Zhao C J, Yang G J, Coburn C,\
    \ Wang J H, Zhang D Y, et al.  \nEstimation of leaf area index by using multi-angular\
    \ hyperspectral imaging \ndata based on the two-layer canopy reflectance model.\
    \  Intelligent \nAutomation & Soft Computing, 2013; 19(3): 295–304. \n[37] Lü\
    \ Y F.  Study of hyperspectral polarized reflectance of vegetation canopy \nat\
    \ nadir viewing direction.  Spectroscopy and Spectral Analysis, 2013; \n33(4):\
    \ 1028–1031. \n[38] Jay S, Maupas F, Bendoula R, Gorretta N.  Retrieving LAI,\
    \ chlorophyll \nand nitrogen contents in sugar beet crops from multi-angular optical\
    \ remote \nsensing: Comparison of vegetation indices and PROSAIL inversion for\
    \ \nfield phenotyping.  Field Crops Research, 2017; 210: 33–46. \n[39] Mao H P,\
    \ Zhu W J, Liu H Y.  Determination of nitrogen and potassium \ncontent in greenhouse\
    \ tomato leaves using a new spectro-goniophotometer.  \nCrop and Pasture Science,\
    \ 2014; 65(9): 888–898. \n[40] Zhu W J, Mao H P, Li Q L, Liu H Y, Sun J, Zuo.Z\
    \ Y, et al.  Study on the \npolarized reflectance-hyperspectral information fusion\
    \ technology of \ntomato leaves nutrient diagnoses.  Spectroscopy and Spectral\
    \ Analysis, \n2014; 34(9): 2500–2505. \n[41] Chen Y W, Zeng Q Y, Pan Y X, Zhao\
    \ Y.  A new method of military false \ntarget identification.  Electronic Design\
    \ Engineering, 2011; 19(16): 89–92. \n(in Chinese) \n[42] Story D, Kacira M, Kubota\
    \ C, Akoglu A, An L L.  Lettuce calcium \ndeficiency detection with machine vision\
    \ computed plant features in \ncontrolled environments.  Computers and electronics\
    \ in agriculture, 2010; \n74(2): 238–243. \n[43] Vittayapadung S, Zhao J W, Quansheng\
    \ C, and Chuaviroj R.  Application \nof FT-NIR spectroscopy to the measurement\
    \ of fruit firmness of\" Fuji\" \napples.  Maejo International Journal of Science\
    \ and Technology, 2008; \n2(1): 13–23. \n[44] Ouyang Q, Zhao J W, Chen Q S.  Measurement\
    \ of non-sugar solids \ncontent in Chinese rice wine using near infrared spectroscopy\
    \ combined \nwith an efficient characteristic variables selection algorithm. \
    \ Spectrochim \nActa A: Mol Biomol Spectrosc, 2015; 151: 280–285. \n[45] Norgaard\
    \ L, Saudland A, Wagner J, Nielsen J P, Munck L, Engelsen S B.  \nInterval \n\
    partial \nleast-squares \nregression \n(iPLS): \nA \ncomparative \nchemometric\
    \ study with an example from near-infrared spectroscopy.  \nApplied Spectroscopy,\
    \ 2000; 54(3): 413–419. \n[46] Tewari J C, Dixit V, Cho B K, Malik K A.  Determination\
    \ of origin and \nsugars of citrus fruits using genetic algorithm, correspondence\
    \ analysis and \npartial least square combined with fiber optic NIR spectroscopy.\
    \  \nSpectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy, \n2008;\
    \ 71(3): 1119–1127. \n[47] Ouyang Q, Zhao J W, Chen Q S.  Instrumental intelligent\
    \ test of food \nsensory quality as mimic of human panel test combining multiple\
    \ \ncross-perception sensors and data fusion.  Anal Chim Acta, 2014; 841: \n68–76.\
    \ \n[48] Cortes C, Vapnik V.  Support-vector networks.  Mach Learn, 1995; 20:\
    \ \n273–297. \n[49] Cybenko G.  Approximation by superpositions of a sigmoidal\
    \ function.  \nMath. Control Signals Systems, 1989; 2: 303–314. \n[50] Wang L,\
    \ Cui N, Fan Y H, Miao Q, Qu B,,Li T L.  Comparison of \ncarbohydrate metabolism\
    \ after anthesis in the leaves of two tomato types.  \nJournal of Shenyang Agriculture\
    \ University, 2012; 43(4): 482–485. (in \nChinese) \n[51] Li Y X, Li J H, He L\
    \ L, Gong G Y, Li T L.  The effect of N. P. K mixed \napplication on yields and\
    \ quality of tomato in solar greenhouse.  China \nVegetables, 1997; 4: 12–15.\
    \ \n[52] Story D, Kacira M, Kubota C, Akoglu A.  Morphological and textural \n\
    plant feature detection using machine vision for intelligent plant health, \n\
    growth and quality monitoring.  Acta Hortic, 2011; 893: 299–306. \n[53] Vanderbilt\
    \ V C, Grant L, Daughtry C S T.  Polarization of light scattered \nby vegetation.\
    \  Proceedings of the IEEE, 1985; 73(6): 1012–1024. \n[54] Mao H P, Gao H Y, Zhang\
    \ X D, Kumi F.  Nondestructive measurement of \ntotal nitrogen in lettuce by integrating\
    \ spectroscopy and computer vision.  \nScientia Horticulturae, 2015; 184: 1–7.\
    \ \n \n"
  inline_citation: '>'
  journal: International Journal of Agricultural and Biological Engineering
  limitations: '>'
  pdf_link: https://ijabe.org/index.php/ijabe/article/download/4280/pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Nondestructive diagnostics of soluble sugar, total nitrogen and their ratio
    of tomato leaves in greenhouse by polarized spectra–hyperspectral data fusion
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
