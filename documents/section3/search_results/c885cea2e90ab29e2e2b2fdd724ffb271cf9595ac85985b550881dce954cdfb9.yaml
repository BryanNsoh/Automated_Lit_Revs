- analysis: '>'
  authors:
  - Vosough M.
  - Schmidt T.C.
  - Renner G.
  citation_count: '0'
  description: This trend article provides an overview of recent advancements in Non-Target
    Screening (NTS) for water quality assessment, focusing on new methods in data
    evaluation, qualification, quantification, and quality assurance (QA/QC). It highlights
    the evolution in NTS data processing, where open-source platforms address challenges
    in result comparability and data complexity. Advanced chemometrics and machine
    learning (ML) are pivotal for trend identification and correlation analysis, with
    a growing emphasis on automated workflows and robust classification models. The
    article also discusses the rigorous QA/QC measures essential in NTS, such as internal
    standards, batch effect monitoring, and matrix effect assessment. It examines
    the progress in quantitative NTS (qNTS), noting advancements in ionization efficiency-based
    quantification and predictive modeling despite challenges in sample variability
    and analytical standards. Selected studies illustrate NTS’s role in water analysis,
    combining high-resolution mass spectrometry with chromatographic techniques for
    enhanced chemical exposure assessment. The article addresses chemical identification
    and prioritization challenges, highlighting the integration of database searches
    and computational tools for efficiency. Finally, the article outlines the future
    research needs in NTS, including establishing comprehensive guidelines, improving
    QA/QC measures, and reporting results. It underscores the potential to integrate
    multivariate chemometrics, AI/ML tools, and multi-way methods into NTS workflows
    and combine various data sources to understand ecosystem health and protection
    comprehensively.
  doi: 10.1007/s00216-024-05153-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Analytical and Bioanalytical Chemistry
    Article Non-target screening in water analysis: recent trends of data evaluation,
    quality assurance, and their future perspectives Trends Open access Published:
    01 February 2024 Volume 416, pages 2125–2136, (2024) Cite this article Download
    PDF You have full access to this open access article Analytical and Bioanalytical
    Chemistry Aims and scope Submit manuscript Maryam Vosough, Torsten C. Schmidt
    & Gerrit Renner   1012 Accesses 2 Altmetric Explore all metrics Abstract This
    trend article provides an overview of recent advancements in Non-Target Screening
    (NTS) for water quality assessment, focusing on new methods in data evaluation,
    qualification, quantification, and quality assurance (QA/QC). It highlights the
    evolution in NTS data processing, where open-source platforms address challenges
    in result comparability and data complexity. Advanced chemometrics and machine
    learning (ML) are pivotal for trend identification and correlation analysis, with
    a growing emphasis on automated workflows and robust classification models. The
    article also discusses the rigorous QA/QC measures essential in NTS, such as internal
    standards, batch effect monitoring, and matrix effect assessment. It examines
    the progress in quantitative NTS (qNTS), noting advancements in ionization efficiency-based
    quantification and predictive modeling despite challenges in sample variability
    and analytical standards. Selected studies illustrate NTS’s role in water analysis,
    combining high-resolution mass spectrometry with chromatographic techniques for
    enhanced chemical exposure assessment. The article addresses chemical identification
    and prioritization challenges, highlighting the integration of database searches
    and computational tools for efficiency. Finally, the article outlines the future
    research needs in NTS, including establishing comprehensive guidelines, improving
    QA/QC measures, and reporting results. It underscores the potential to integrate
    multivariate chemometrics, AI/ML tools, and multi-way methods into NTS workflows
    and combine various data sources to understand ecosystem health and protection
    comprehensively. Similar content being viewed by others A comprehensive review
    of water quality indices (WQIs): history, models, attempts and perspectives Article
    11 March 2023 Advantages and disadvantages of techniques used for wastewater treatment
    Article 31 July 2018 Worldwide cases of water pollution by emerging contaminants:
    a review Article 18 April 2022 Introduction Water ecosystems are essential for
    all living but have endured various anthropogenic threats, ranging from pollution
    to over-exploitation. As a safeguard, the tools used to assess water quality are
    crucial. However, the world is rapidly evolving, and the compounds finding their
    way into water systems are changing. Therefore, continuously adapting modern measurement
    and data analysis techniques has become desirable and imperative to keep pace.
    The dominant approaches are targeted screening methods, i.e., target analysis
    and suspect screening, that have been instrumental in detecting known contaminants
    [1, 2]. However, their restrictive scope limits their efficacy, potentially allowing
    unknown or unexpected pollutants to go unnoticed. Non-target screening (NTS),
    powered by high-resolution mass spectrometry (HRMS), extended those tools, offering
    a dynamic solution unconstrained by the need to search for specific, pre-identified
    contaminants [3]. While NTS has ushered in new possibilities for pollutant detection,
    it has its challenges [4]. Firstly, the sheer volume of data generated by HRMS
    instruments such as Orbitrap or time-of-flight systems coupled with gas chromatography
    (GC) or liquid chromatography (LC) can be daunting, demanding sophisticated data
    management and processing tools. Analyzing and interpreting the vast spectra of
    chemical signatures also requires much expertise and refined methodologies [5].
    Additionally, distinguishing between signal noise, naturally occurring compounds,
    and emerging pollutants poses another intricate trilemma [6]. The double-edged
    sword of comprehensive chemical databases is evident in non-target screening.
    On the one hand, a more extensive database offers more opportunities for compound
    identification. Conversely, the enormous volume of information can make pinpointing
    a specific compound akin to finding a needle in a vast haystack. To address this
    challenge, adopting approaches like PubChemLite can be beneficial, which refines
    the vastness of the database by focusing on the most relevant and commonly encountered
    compounds, thus combining the power of comprehensive data with the ease of targeted
    searches [7]. Beyond these technical hurdles, practical challenges arise regarding
    standardization across laboratories and regulatory adaptation. As the field of
    NTS continues to evolve, addressing these hurdles is crucial to harnessing its
    full potential. In light of the challenges inherent to NTS, the field has witnessed
    a surge of innovations and methodologies, underscoring the ever-evolving landscape
    of water quality assessment [3]. These trends promise to refine our understanding
    and detection capabilities and foster a proactive stance toward potential threats.
    From advancements in instrument technology, data processing, and method performance
    assessment to collaborative efforts for database expansion and standardization,
    these trends represent a renewed commitment to preserving the sanctity of water
    ecosystems. This Trend article highlights advances in data evaluation approaches,
    the importance of quality assurance/quality control guidelines and validation
    protocols for NTS study outputs, and quantitative NTS strategies by focusing on
    works conducted within the past five years. In this way, different challenges,
    opportunities, and related concepts to NTS will be explored. In the conclusion
    section, an outlook for the future trajectory of non-target screening in aquatic
    ecosystems will be presented based on emerging and expected needs. Current data
    evaluation trends in NTS The evaluation of HRMS data in NTS consists of two aspects:
    (1) initial raw spectra processing, aiming to condense via multi-step approaches
    relevant chemical information from individual measurement data, i.e., feature
    extraction, and (2) data analysis, putting the chemical information obtained into
    a meaningful context, e.g., comparing samples for monitoring trend analysis in
    time series experiments. The outer structure of NTS processing workflows, namely
    centroiding, peak detection, etc., looks almost harmonized and familiar with target
    or suspect screenings. However, in detail, the explicit algorithms of the individual
    steps can differ significantly, making result comparability challenging. In data
    analysis, advanced multivariate chemometrics methods and machine learning (ML)
    have become indispensable tools for discovering hidden patterns and identifying
    correlations where conventional methods are limited. Moreover, high-throughput
    annotations of unknown features are being performed using open-source molecular
    discovery workflows that utilize spectral libraries, ML, and cheminformatic tools
    [8]. Full scan HRMS/MS data storage allows retrospective analysis without re-run
    sample analyses. This potential has been proposed as an early warning system worldwide
    to detect emerging pollution threats [9]. In 2019, the NORMAN Digital Sample Freezing
    Platform (DSFP) was launched to enable digital sample archiving and retrospective
    suspect screening [10]. However, challenges exist due to the sheer size and complex
    nature of NTS data, particularly in archiving HRMS data, meta-data, and processing
    capabilities [11]. As outlined in the following sections, data processing workflows
    in the NTS generally consist of several steps, from feature extraction to structural
    elucidation. Data processing: feature extraction Hyphenated HRMS instruments generally
    produce complex and diverse datasets requiring specific processing and robust
    processing tools and procedures. Briefly, the main aim of the processing step
    is to provide a compact “component table” from the original raw data. Processing
    parameters are crucial to further screening success. The significant points here
    are extracting highly qualified features, proper feature alignment, redundant
    feature filtering, and reliable componentization of features to the same unique
    molecular structure. Recently, different NTS data processing algorithms and methods
    have been extensively reviewed [6]. Nowadays, in addition to vendor software,
    open-source tools such as XCMS [12], MZmine [13], SIRIUS [14], MS-DIAL [15], enviMass
    [16], have been used for HRMS data handling of environmental samples. However,
    since most of these packages were developed for -omics data, the workflow and
    parameters are not necessarily appropriate for environmental studies regarding
    issues like frequent appearances of low-intensity peaks, more highlighted matrix
    effects, and long-term exposure monitoring. The impact of data preprocessing on
    the production of different outputs and quality would be another noticeable concern
    at this stage [68]. This insight requires more thorough research and collaboration
    to ensure reliable and robust NTS tools and procedures. The open-source platform
    PatRoon [17] enables comprehensive environmental NTS data processing, from HRMS
    pre-treatment to compound annotation. With PatRoon, comparison between algorithms
    is possible, and feature coverage can be increased by utilizing both common and
    distinctive features among algorithms. InSpectra, an open-source and modular web-based
    NTS and suspect screening workflow, was released in 2023 [18]. LC-HRMS data can
    be archived, shared, and curated through parallel computing with InSpectra. Moreover,
    InSpectra can identify, track, and prioritize pollution threats in a reproducible
    and transparent manner. According to [19], while not limited to water samples,
    it has been found that vendor software like Thermo Compound Discoverer or Agilent
    MassHunter is frequently used for LC or GC-HRMS NTS data processing. However,
    commercial software often presents proprietary constraints, such as limited customization
    options and exclusive data formats, making transitions or data sharing cumbersome.
    The closed nature of these tools obscures the understanding of data processing,
    posing challenges for rigorous scientific evaluations. Furthermore, such software
    may face interoperability issues due to restrictions on available platforms and
    variations in tools used by collaborators, hampering seamless teamwork and comprehensive
    system compatibility. Thus, due to the main disadvantage of commercial software,
    developing and validating open software that leverages NTS for both GC- and LC-HRMS
    data while deconvoluting MS2 data for more coverage of chemical exposure in water
    environments is highly required. An alternative to these “feature profile” (FP)
    packages is to exploit multi-way chemometric methods for direct “component profile”
    (CP) production. For example, a set of LC-HRMS measurements can be efficiently
    modeled by a multi-way algorithm after initial data compression [20]. Multivariate
    curve resolution-alternating least squares (MCR-ALS) and parallel factor analysis
    2 (PARAFAC2) are the most efficient methods. However, other tensor factorization
    algorithms can also be implemented after extra preprocessing steps. Upon modeling
    on each data array, three matrices of CPs are generated: resolved “pure” LC profiles,
    their mass spectral counterpart, and the area under resolved LC profiles as quantification
    scores. Therefore, data dimensionality can be reduced, decreasing the risk of
    incomplete componentization and missing compounds that cannot be detected by feature-based
    peak detection [21]. Data processing protocols based on multiple samples align
    well with real-world aquatic NTS advancements and perspectives, such as monitoring
    pollution pathways in river water, wastewater samples undergoing chemical or biological
    treatment, or water samples measured under a variety of extraction/instrumental
    conditions [20, 22, 23]. Nevertheless, multi-way methods have prerequisites to
    consider when applied in GC/LC-MS data processing [24]. As the most flexible algorithm,
    MCR-ALS provides additional benefits in NTS of water samples with complex GC-
    or LC-HRMS data structures (e.g., substantial RT shifts and co-elution issues)
    designed in response to environmental conditions. However, MCR workflows are mainly
    semi-automated, and solutions may have some ambiguities, though their extent can
    be mitigated by incorporating sensible ALS optimization constraints. A comparison
    of FP packages and CP approaches in different water NTS categories would help
    us understand how they differ qualitatively and quantitatively when extracting
    a holistic or prioritized subset of chemical spaces. An overall emphasis should
    be placed on expanding and optimizing fully automated multi-way-based workflows
    designed to process NTS raw GC- and LC-HRMS data and produce CPs with high completeness
    and minimal user input. Data analysis: pattern recognition With the “component
    table” in place, various chemometrics/ML algorithms and visualization tools could
    be employed for downstream tasks such as uncovering chemical trends and pollution
    events, monitoring the occurrence and fate of pollutants, assessing of water treatment
    process, and developing intelligent prioritization criteria to select environmentally
    relevant features. Hence, it is also crucial to consider data transformations,
    intensity normalization, and scaling for removing unwanted variations before further
    analysis. Data processing challenges with NTS include the high dimensionality
    of HRMS records and the possibility of redundant information, the lack of adequate
    sample size, and the number of replicates required for statistical testing. An
    effective data reduction strategy and exploration are therefore required in the
    first place. In unsupervised NTS, principal component analysis (PCA), hierarchical
    clustering analysis (HCA), and co-occurrence analysis using Venn diagrams are
    the most popular methods to evaluate similarities, co-variations, and differences
    between various NTS cases [5]. While these methods benefit data exploration, one
    must know their potential and limitations about high dimensional HRMS data [25,
    26]. Fig. 1 Representation of ASCA(+) loading plot (Factor “sampling location”)
    to display the final subsets of highly prioritized chemicals (ESI+) strongly associated
    with different stream water sources by the joint implementation of univariate
    (volcano test) and multivariate statistical methods (PLS-DA-VIP and ASCA+) [28].
    Note: ASCA(+): extension of ASCA (ANOVA Simultaneous Component Analysis) for unbalanced
    experimental design; PLS-DA-VIP: Partial Least Squares-Discriminant Analysis-Variable
    Importance in Projection Full size image The recently used supervised classification
    and multivariate statistical tools in NTS of water samples are partial least squares-discriminant
    analysis (PLS-DA), its orthogonal directed model (OPLS-DA), support vector machines
    with linear kernels (SVM), PLS-path modeling, and multivariate ANOVA models [22,
    23, 27, 28]. Due to their multivariate advantage, these methods complement univariate
    statistics (e.g., volcano plots, Mann–Whitney U test, ANOVA) because they consider
    all features simultaneously and can consider pollutant compound interrelations
    associated with the outcome (Fig. 1). Multiple validation criteria must be applied
    to evaluate the performance and generalizability of predictive models following
    model training, suggesting that procedures for assessing the effectiveness of
    NTS methods may need to be improved and standardized [6, 29]. However, the previous
    reports show that NTS data classification can be non-linear in more complex scenarios,
    such as heterogeneous or highly dynamic behavior of pollution sources or when
    there is a substantial overlap in the chemical composition of water sources [25,
    28]. Therefore, more flexible and robust non-parametric learners such as SVM models
    with non-linear kernels, artificial neural networks (ANNs), deep learning (DL)
    algorithms, and ensemble classifiers such as random forest (RF) and gradient boosting
    algorithms (such as XGBoost and CatBoost) can be exploited in cases where linear
    classification methods are not robust tools. Moreover, ML tools have a high potential
    for use in many other fields within NTS research, including retention time (RT)
    prediction [30] and collision cross section (CCS) prediction in ion mobility spectrometry
    (IMS) coupled with HRMS as well [31]. Furthermore, constructing several robust
    classification models on the same sample set may be necessary to avoid interpretation
    biases and false discoveries. The final explanatory and predictive outputs would
    be highly beneficial for future comparison of aquatic ecosystems and uncovering
    the exposure of relevant pollutants and proper chemical fingerprints in various
    situations. Such benefits align with what has been emphasized in the literature,
    which is that NTS data sharing and the development of data repository platforms
    are essential to facilitate collaborative trials, QA/QC, method developments and
    comparisons, unify protocols, and contribute to policy and regulatory development.
    Quality assurance/quality control efforts in NTS The complex nature of NTS requires
    stringent quality assurance and quality control (QA/QC) measures to ascertain
    the reliability and validity of results [32, 33]. Many of these QA/QC protocols
    are general and originate from conventional target analyses, with scarce specific
    developments for NTS [34]. Therefore, exploring and discussing these measures
    within the NTS framework and evolving a perspective view on future developments
    and trends within this field is crucial. In the meaning of QA/QC, there are three
    layers in the analytical process where quality has to be ensured: Samples, Measurement
    System, and Data Processing. NTS measurements are organized in long-running batch
    series, so ensuring stability should be prioritized [33]. It is necessary to ensure
    sample stability, signifying that samples and their components remain intact and
    unchanged throughout the analysis. In this situation, one suggestion is to include
    internal standards (ISs) multiple times within the batch analysis, as Caballero-Casero
    et al. (2021) put forth [33]. The stability can then be assessed by evaluating
    the consistency of these internal standards and pooled sample results over time.
    Next to sample stability, monitoring measurement system stability using quality
    control charts and batch effect correction is integral for NTS [33, 35]. Including
    and considering blank samples can be used to identify memory effects at an early
    stage and prevent cross-contamination. Moreover, these blank measurements can
    be considered for later background corrections [34]. Assessing matrix effects
    is integral to preserving the NTS process’s reliability [33]. Matrix compounds
    can cause significant shifts in retention time and m/z, with the latter primarily
    originating from data processing and being associated with increased data dispersion
    and overlapping effects in the extracted ion traces [6]. QC samples fortified
    with ISs are vital in assessing matrix effects by comparing QC samples in the
    absence and presence of respective matrices [33]. Moreover, QC samples provide
    information about potential interferences, recovery efficiency, error rates for
    identifications, and precision throughout the analytical process [32, 33, 35].
    The number of ISs depends on the complexity of the sample matrices, with each
    IS requiring successful identification by the workflow’s conclusion [34]. These
    standards help to normalize data, minimize variability, and augment reliability
    [36]. Additionally, QC samples are a reference point to validate the outcomes
    derived from the NTS process [36]. Ensuring that the sample used in the laboratory
    accurately reflects the real environmental matrix is crucial. It can be done by
    comparing the standard deviations of sample replicates with those from pooled
    samples [33, 35]. Pooled samples should also be used for method development to
    assess and optimize various parameters during sample extraction, data acquisition,
    and data processing, which is common in metabolomics and can easily be transferred
    to NTS in water analysis [34, 37]. In the context of QA/QC, quantifying substances
    identified in NTS accurately presents challenges due to complexities tied to sample
    matrices, concentrations, and compound characteristics. Although methods like
    predicted ionization efficiency can be employed to calibrate concentrations against
    known standards — which will be discussed further in the subsequent section —
    it should be noted that existing predictive quantification methods in NTS do not
    meet the same standards as conventional quantification techniques [36, 38]. Predictive
    quantification in non-target screening involves estimating the quantity of substances
    based on signal normalization. Due to inherent variations and uncertainties, an
    acceptable signal enhancement is often set between 30 and 150% of the original
    signal, depending on the specific context of the analysis. Deviations outside
    these limits may indicate potential errors or biases in the NTS process [33].
    The advancement and adoption of open-access data processing tools are pivotal
    for transparent and reproducible NTS results [34]. Reporting data processing and
    data analysis details is crucial as various algorithms provide significantly different
    findings, which is still one of the biggest challenges in NTS data processing
    [6, 20]. However, encouraging interlaboratory comparability investigations can
    enhance the consistency and comparability of results across different laboratories
    [33]. Regarding data evaluation, the human element remains a significant unknown.
    Consequently, it is essential to utilize objective calculation methods, such as
    those for data similarity considerations, like isotope matching. These methods
    provide an unbiased evaluation for identification and should be given precedence
    over subjective approaches such as visual assessments [6, 39]. Clear and detailed
    guidelines are required for reporting the identification of unknown features and
    assessing the confidence level [34, 35, 40]. The proposed Identification Points
    (IP) scoring system by Alygizakis et al. (2023) is based on the already well-established
    scoring system by Schymanski et al. (2014) [39, 40]. It aims to establish objective
    standards for non-target screening results, thus improving consistency and quality
    in data interpretation [39]. In certain instances, penalties can be applied to
    the IP score, such as when recorded data-dependent scans are lacking or poor fragmentation
    is observed [39]. An overview in Fig. 2 summarizes the essential elements of QA/QC
    for NTS. Fig. 2 Overview of the essential elements of QA/QC for NTS in water analysis
    Full size image Selected NTS approaches in water analysis studies NTS has been
    rapidly advancing in recent years since it provides a more holistic view of chemical
    exposure patterns in different environmental samples by combining HRMS with LC
    or GC. However, GC-HRMS is still insufficiently utilized compared with LC-HRMS
    despite the large spectral databases available (e.g., NIST20 and Wiley 11) [5,
    19]. Moreover, combined methods have only been used in very few cases [41, 42].
    Undoubtedly, further progress would be in developing GC-/LC-HRMS-based NTS workflows
    for the same aqueous environment. As a result, chemical space coverage can be
    efficiently increased in various NTS applications, depending on the study design
    and research questions. The same holds for multidimensional chromatographic systems,
    HILIC-to-reverse phase chromatography, and combined workflows using different
    ionization technologies. NTS workflows have been advancing by adopting passive
    sampling devices (PSDs) to sample streams and multi-watersheds, which have advantages
    such as time integrity, cost-effectiveness, fewer disruptions, and greater sensitivity,
    all of which are beneficial for determining pollution status in streams and determining
    pollution level [5, 11, 28, 43]. Thus, PSD-based strategies can be exploited to
    capture both “aggregate” and “cumulative” exposure information about environmental
    pollution. In summary, using combined strategies in sampling, sample preparation,
    instrumental measurements, and data acquisition modes, namely data-dependent acquisition
    (DDA) and data-independent acquisition (DIA) in NTS workflows leading to higher
    chemical coverage in the aquatic environment, is highly recommended. However,
    experimental and operational limitations, availability, and complexity of recorded
    data should be considered [11, 44]. These considerations also apply to the combined
    target, suspect, and non-target strategies in different NTS scenarios [5, 45].
    However, exhaustive elucidations remain challenging and laborious since they need
    to be confirmed with reference standards and manually verified. Thus, peak prioritization,
    or specifying the most reliable subset of chemicals based on the research question,
    is a fundamental part of many NTS studies. Such prioritization ensures that elucidation
    efforts are focused on the most relevant chemicals and mitigate the risk of overlooking
    potentially hazardous pollutants. As a result, environmental risk assessment and
    management efforts could be facilitated using such an approach. There has been
    a growing number of strategies for prioritizing, whether by effect-directed analysis
    (EDA), exposure-driven approaches, or their combination [11, 36]. The most recent
    examples of exposure-driven prioritization strategies are time series LC-HRMS
    prioritization based on deep learning convolutional neural network (DL-CNN) [46]
    and group-wise PCA (GPCA) [26], spatiotemporal-based prioritization using PLS-DA-VIP,
    ASCA(+) [28], binary comparisons using sparse ASCA (GASCA) [22], highly polar
    pollutant prioritization [47], removal rate ranking in wastewater treatment plants
    (WWTPs) [48] and source (urban and agricultural) related prioritization [49].
    NTS chemical fingerprinting and quantitative source tracking have also been improved
    in recent years from a method development perspective [50] and in real-world applications,
    suggesting that a reliable diagnostic subset of features or “smart tracers” can
    predict pollution sources in watersheds, their relative locations, and quantify
    processes that are beyond the capabilities of conventional ecosystem sensors through
    the advancement of ML-based workflows [25]. Alternatively, recent studies suggest
    that hazard-driven prioritization based on MLinvitroTOX [51], a classifier based
    on both structural and MS2 data, and online prioritization [52] based on an intelligent
    MS2 acquisition method could be helpful to focus identification efforts on features
    with the most significant potential to harm the environment, rather than those
    that are most abundant. The use of NTS in advanced oxidation processes (AOPs)
    is on the rise. It offers several advantages, including the identification of
    unknown transformation products (TPs), understanding how the water matrix influences
    the formation of new TPs, determining treatment efficiency on a global scale,
    detecting abnormalities early, optimizing processes, and making decisions [5,
    53]. Furthermore, it was found that AOP treatment evaluation can be improved by
    adding supercritical fluid chromatography (SFC)-HRMS to the LC-HRMS-based workflow
    for identifying unknown persistent and mobile organic compounds (PMOCs) found
    in groundwater samples to achieve greater detection coverage of highly polar compounds
    [54]. NTS workflows face significant challenges in identifying a prioritized chemical
    based solely on its accurate mass and MS/MS fragmentation patterns. In particular,
    for LC-HRMS and with available databases for GC-EI, the assignment of tentative
    structures in an expert and validated way is still a great challenge and is tedious
    and time-consuming. Searching in large compound databases (e.g., Massbank, mzCloud,
    ChemSpider, and PubChem) for possible structures of an elemental formula typically
    results in numerous hits that need to be ranked and considered further by available
    MS/MS recorded data, retention time (RT) plausibility, provided meta-data, and
    limited comparability among instruments, as well. Moreover, developing different
    in silico tools (Metfrag and CSI: FingerID integrated with SIRIUS) for predicting
    MS fragmentation patterns and RT predictions using QSPR models are additional
    validation criteria in NTS workflows and reducing false positive findings. Smaller
    databases such as STOFF-IDENT [55] and CompTox [56] could facilitate the identification
    of prioritized unknown chemicals. PubChemLite, a subset of PubChem, was introduced
    by Schymanski et al. in 2020 for more efficient identification efforts in exposomics
    and environmental research [7]. Recent progress has been made regarding improving
    chemical identifications through integrating experimental spectra (ENTACT) into
    MassBank and developing automated workflows to support non-targeted exposomics
    and NTS surveys on aquatic samples [57, 58]. A further improvement may be achieved
    by adding spectral data measured over different types of HRMS systems under varying
    conditions. Quantification in NTS Quantitative non-target screening (qNTS) is
    one of the newest analytical fields with substantial challenges, such as the absence
    of analytical standards, variable ionization efficiency across laboratories and
    instruments, and the complexity of samples that range from effluents to drinking
    water [38, 59, 60, 61, 62, 63]. However, recent advancements have brought promising
    solutions to the forefront. Methods like ionization efficiency-based quantification
    and machine learning models have improved accuracy [38, 59, 60, 62]. These models,
    predominantly based on the Random Forest approach, provide reliable predictions
    and allow for applying different prediction strategies depending on the confidence
    level of the analytical signal. Additionally, they enable comparability across
    different laboratories, instruments, and analysis methods [64]. Nevertheless,
    these advancements have their limitations. Discrepancies in complex matrices,
    significant errors in modeling approaches, and variability between different instruments
    and matrices pose challenges [59, 60, 64]. Notably, the need for reference standards
    for the vast diversity of chemicals can limit quantification accuracy, hindering
    the progress of non-targeted screening [3]. Looking ahead, the field of NTS is
    ripe for evolution. The development and improvement of models, especially those
    utilizing machine learning, are vital future directions. Increasing the range
    of detectable chemicals and optimizing prediction accuracy could revolutionize
    the NTS era. Moreover, there is a pressing need to continue monitoring and managing
    identified substances. This need will require authorities to control the release
    of prioritized substances and new chemical candidates, an effort that will undoubtedly
    shape the future landscape of NTS in aquatic environments. Recent advancements
    in qNTS emphasize its adaptability and the scientific community’s dedication to
    refining its methods for more accurate results. From grappling with quantification
    accuracy to harnessing the power of machine learning, qNTS continues to evolve
    in response to the challenges it faces. As we look to the future, the field stands
    ready to embrace new techniques and strategies, all in pursuit of more effective
    detection, identification, and quantification of unknown chemicals. Fig. 3 Schematic
    representation of the emerging NTS-Omics integration framework within an aquatic
    monitoring and ecosystem health study. Note: PPCPs: Pharmaceuticals and Personal
    Care Products; TPs: Transformation Products; PFAS: Perfluoroalkyl and Polyfluoroalky
    Substances; DBP: Disinfection Byproducts; eDNA: environmental DNA Full size image
    Conclusion and outlook Recent advances in NTS of aquatic environments include
    improved QA/QC guidelines, advanced data processing tools, more comprehensive
    coverage of “chemical space,” novel prioritization strategies, and increased quantitative
    approaches. The number of environmental chemicals, suspects, and HRMS spectral
    databases is growing, and workflows are becoming increasingly automated for chemical
    identification and structural analysis. Future research needs and challenges related
    to NTS workflow performance assessment include establishing guidelines for all
    steps of NTS workflows, improving QA/QC measures, and refining result reporting.
    In this context, a first step has already been made by esteemed research networks,
    such as the NORMAN Network, BP4NTA, and the German Water Chemistry Society, who
    provided guidelines and QA/QC service tools for reporting [3, 4, 66]. Additionally,
    the initiation of the official norm process for ISO standards marks a significant
    stride toward standardizing QA/QC in NTS, and its establishment will undoubtedly
    shape and enhance the practice of QA/QC in NTS in the foreseeable future [67].
    The IP scoring system can be further enhanced and adjusted as technological advancements
    increase data availability. A transition from primarily qualitative to quantitative
    approaches in NTS could improve the overall quality and accuracy of NTS [36].
    QA/QC in NTS data processing must become a focal point for future developments.
    Currently, valid uncertainty estimation of NTS results is seldom achievable as
    individual intermediate results are not collected or considered during the complex
    multi-step NTS data processing [6]. The role of the evaluating analyst in making
    critical decisions, such as setting input parameters for data processing and analysis,
    has yet to be studied. Future developments should strive for uniform evaluation
    criteria, standardized protocols, and the inclusion of statistical methods to
    describe qualities and uncertainties. From a QA/QC perspective, developing and
    openly providing reference datasets is crucial, enabling result verification and
    characterizing strengths, weaknesses, and application ranges of data processing
    methods. Different open-source software platforms and statistical packages have
    been developed and are available for HRMS data preprocessing/processing water
    samples. Significant progress has been made in integrating multivariate chemometrics
    methods and AI/ML tools into feature engineering, data formatting, peak annotations,
    and downstream data analysis, which has led to several opportunities for NTS discoveries
    in the future. These trends can be expanded to accommodate a variety of statistical
    approaches, comprehensive LC- and GC-HRMS measurements, and different MS2 acquisition
    modes. Moreover, incorporating the multi-way methods in NTS workflows as an alternative
    or complementary to the current packages can provide benefits such as providing
    “pure” qualitative and quantitative information on individual pollutants globally
    and robustly. Combination protocols and complementary tools have shown promise
    in different NTS areas. In this regard, the future expects researchers to conduct
    multiple robust ML and multivariate/univariate-based prioritizing strategies for
    selecting the most reliable subset of emerging pollutants based on significant
    ecological/health risks, environmental exposure/persistence/co-occurrence, or
    both in several real-world NTS scenarios. Furthermore, retrospective analysis
    and usage of archived HRMS data will increase due to the increasing trend of sharing
    data in FAIR repositories. Additionally, an emerging trend will be the integration
    of chemical NTS data with other data sources in water ecosystems, including Environmental
    DNA (eDNA) methods (e.g., for microbial communities), -omics-based approaches,
    ecological and hydrological parameters, and other environmental gradients, using
    advanced multiblock and statistical analysis tools (Fig. 3). Using integrative
    approaches allows us to better understand the environment and anthropogenic contaminants’
    effect on ecosystem health and ultimately facilitate efforts to protect ecosystems.
    References Kaserzon SL, Heffernan AL, Thompson K, Mueller JF, Gomez Ramos MJ.
    Rapid screening and identification of chemical hazards in surface and drinking
    water using high resolution mass spectrometry and a case-control filter. Chemosphere.
    2017;182:656–64. https://doi.org/10.1016/j.chemosphere.2017.05.071. Article   CAS   PubMed   Google
    Scholar   Tröger R, Klöckner P, Ahrens L, Wiberg K. Micropollutants in drinking
    water from source to tap - method development and application of a multiresidue
    screening method. Sci Total Environ. 2018;627:1404–32. https://doi.org/10.1016/j.scitotenv.2018.01.277.
    Hollender J, Schymanski EL, Ahrens L, Alygizakis N, Béen F, Bijlsma L, et al.
    NORMAN guidance on suspect and non-target screening in environmental monitoring.
    Environ Sci Eur. 2023;35. https://doi.org/10.1186/s12302-023-00779-4 Place BJ,
    Ulrich EM, Challis JK, Chao A, Du BL, Favela KA, et al. An introduction to the
    benchmarking and publications for non-targeted analysis working group. Anal Chem.
    2021;93(49):16289–96. https://doi.org/10.1021/acs.analchem.1c02660. Article   CAS   PubMed   PubMed
    Central   Google Scholar   González-Gaya B, Lopez-Herguedas N, Bilbao D, Mijangos
    L, Iker AM, Etxebarria N, et al. Suspect and non-target screening: the last frontier
    in environmental analysis. Anal Methods. 2021;13:1876–904. https://doi.org/10.1039/D1AY00111F.
    Article   PubMed   Google Scholar   Renner G, Reuschenbach M. Critical review
    on data processing algorithms in non-target screening: challenges and opportunities
    to improve result comparability. Anal Bioanal Chem. 2023;415:4111–23. https://doi.org/10.1007/s00216-023-04776-7.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Schymanski EL, Kondić
    T, Neumann S, Thiessen PA, Zhang J, Bolton EE. Empowering large chemical knowledge
    bases for exposomics: PubChemLite meets MetFrag. J Cheminform. 2020;13:19. https://doi.org/10.1186/s13321-021-00489-0.
    Article   CAS   Google Scholar   Krier J, Singh RR, Kondić T, Lai A, Diderich
    P, Zhang J, et al. Discovering pesticides and their TPs in Luxembourg waters using
    open cheminformatics approaches. Environ Int. 2022;158:106885. https://doi.org/10.1016/j.envint.2021.106885
    Alygizakis NA, Samanipour S, Hollender J, Ibáñez M, Kaserzon S, Kokkali V, et
    al. Exploring the potential of a global emerging contaminant early warning network
    through the use of retrospective suspect screening with high-resolution mass spectrometry.
    Environ Sci Technol. 2018;52:5135–5144. https://doi.org/10.1021/acs.est.8b00365
    Alygizakis NA, Oswald P, Thomaidis NS, Schymanski EL, Aalizadeh R, Schulze T,
    et al. NORMAN digital sample freezing platform: a European virtual platform to
    exchange liquid chromatography high resolution-mass spectrometry data and screen
    suspects in “digitally frozen” environmental samples. TrAC - Trends Anal Chem.
    2019;115:129–37. https://doi.org/10.1016/j.trac.2019.04.008. Menger F, Gago-Ferrero
    P, Wiberg K, Ahrens L. Wide-scope screening of polar contaminants of concern in
    water: a critical review of liquid chromatography-high resolution mass spectrometry-based
    strategies. Trends Environ Anal Chem. 2020;28: e00102. https://doi.org/10.1016/j.teac.2020.e00102.
    Article   CAS   Google Scholar   Smith CA, Want EJ, O’Maille G, Abagyan R, Siuzdak
    G. XCMS: processing mass spectrometry data for metabolite profiling using nonlinear
    peak alignment, matching, and identification. ACS Publ. 2006;78:779–87. https://doi.org/10.1021/ac051437y
    Pluskal T, Castillo S, Villar-Briones A, Orešič M. MZmine 2: modular framework
    for processing, visualizing, and analyzing mass spectrometry-based molecular profile
    data. BMC Bioinformatics. 2010;11:395. https://doi.org/10.1186/1471-2105-11-395.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Böcker S, Letzel MC,
    Lipták Z, Pervukhin A. SIRIUS: decomposing isotope patterns for metabolite identification\\(\\dagger
    \\). Bioinformatics. 2009;25:218–24. https://doi.org/10.1093/bioinformatics/btn603.
    Tsugawa H, Cajka T, Kind T, Ma Y, Higgins B, Ikeda K, et al. MS-DIAL: data-independent
    MS/MS deconvolution for comprehensive metabolome analysis. Nat Methods. 2015;12:523–526.
    https://doi.org/10.1038/nmeth.3393 Loos M, Schmitt U.: enviMass version 3.5. Available
    online. Available from: https://doi.org/10.5281/zenodo.1213098 Helmus R, ter Laak
    TL, van Wezel AP, de Voogt P, Schymanski EL. patRoon: open source software platform
    for environmental mass spectrometry based non-target screening. J Cheminform.
    2021;13:1. https://doi.org/10.1186/s13321-020-00477-w. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Feraud M, O’Brien JW, Samanipour S, Dewapriya P, van
    Herwerden D, Kaserzon S, et al. InSpectra - A platform for identifying emerging
    chemical threats. J Hazard Mater. 2023;455: 131486. https://doi.org/10.1016/j.jhazmat.2023.131486.
    Article   CAS   PubMed   Google Scholar   Manz KE, Feerick A, Braun JM, Feng YL,
    Hall A, Koelmel J, et al. Non-targeted analysis (NTA) and suspect screening analysis
    (SSA): a review of examining the chemical exposome. J Expo Sci Environ Epidemiol.
    2023;33:524–536. https://doi.org/10.1038/s41370-023-00574-6 Hohrenk LL, Vosough
    M, Schmidt TC. Implementation of chemometric tools to improve data mining and
    prioritization in LC-HRMS for nontarget screening of organic micropollutants in
    complex water matrixes. Anal Chem. 2019;91:9213–20. https://doi.org/10.1021/acs.analchem.9b01984.
    Article   CAS   PubMed   Google Scholar   Gorrochategui E, Jaumot J, Tauler R.
    ROIMCR: a powerful analysis strategy for LC-MS metabolomic datasets. BMC Bioinformatics.
    2019;20:256. https://doi.org/10.1186/s12859-019-2848-8. Article   PubMed   PubMed
    Central   Google Scholar   Khatoonabadi RL, Vosough M, Hohrenk LL, Schmidt TC.
    Employing complementary multivariate methods for a designed nontarget LC-HRMS
    screening of a wastewater-influenced river. Microchem J. 2021;160: 105641. https://doi.org/10.1016/j.microc.2020.105641.
    Article   CAS   Google Scholar   Cairoli M, van den Doel A, Postma B, Offermans
    T, Zemmelink H, Stroomberg G, et al. Monitoring pollution pathways in river water
    by predictive path modelling using untargeted GC-MS measurements. npj Clean Water.
    2023;6:48. https://doi.org/10.1038/s41545-023-00257-7 Vosough M. Current challenges
    in second-order calibration of hyphenated chromatographic data for analysis of
    highly complex samples. J Chemom. 2018;32:1–15. https://doi.org/10.1002/cem.2976.
    Article   CAS   Google Scholar   Dávila-Santiago E, Shi C, Mahadwar G, Medeghini
    B, Insinga L, Hutchinson R, et al. Machine learning applications for chemical
    fingerprinting and environmental source tracking using non-target chemical data.
    Environ Sci Technol. 2022;56:4080–90. https://doi.org/10.1021/acs.est.1c06655.
    Article   CAS   PubMed   Google Scholar   Purschke K, Vosough M, Leonhardt J,
    Weber M, Schmidt TC. Evaluation of nontarget long-term LC-HRMS time series data
    using multivariate statistical approaches. Anal Chem. 2020;92:12273–81. https://doi.org/10.1021/acs.analchem.0c01897.
    Bonnefille B, Karlsson O, Rian MB, Raqib R, Parvez F, Papazian S, et al. Nontarget
    analysis of polluted surface waters in bangladesh using open science workflows.
    Environ Sci Technol. 2023;57:6808–24. https://doi.org/10.1021/acs.est.2c08200.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Hohrenk-Danzouma LL,
    Vosough M, Merkus VI, Drees F, Schmidt TC. Non-target analysis and chemometric
    evaluation of a passive sampler monitoring of small streams. Environ Sci Technol.
    2022;56:5466–77. https://doi.org/10.1021/acs.est.1c08014. Article   CAS   PubMed   Google
    Scholar   Fisher CM, Peter KT, Newton SR, Schaub AJ, Sobus JR. Approaches for
    assessing performance of high-resolution mass spectrometry-based non-targeted
    analysis methods. Anal Bioanal Chem. 2022;414:6455–71. https://doi.org/10.1007/s00216-022-04203-3.
    Boelrijk J, van Herwerden D, Ensing B, Forré P, Samanipour S. Predicting RP-LC
    retention indices of structurally unknown chemicals from mass spectrometry data.
    J Cheminform. 2023;15:28. https://doi.org/10.1186/s13321-023-00699-8 Yang F, van
    Herwerden D, Preud’homme H, Samanipour S. Collision cross section prediction with
    molecular fingerprint using machine learning. Molecules. 2022;27:6424. https://doi.org/10.3390/molecules27196424.
    Pourchet M, Debrauwer L, Klanova J, Price EJ, Covaci A, Caballero-Casero N, et
    al. Suspect and non-targeted screening of chemicals of emerging concern for human
    biomonitoring, environmental health studies and support to risk assessment: from
    promises to challenges and harmonisation issues. Environ Int. 2020;139: 105545.
    https://doi.org/10.1016/j.envint.2020.105545. Article   CAS   PubMed   Google
    Scholar   Caballero-Casero N, Belova L, Vervliet P, Antignac JP, Castaño A, Debrauwer
    L, et al. Towards harmonised criteria in quality assurance and quality control
    of suspect and non-target LC-HRMS analytical workflows for screening of emerging
    contaminants in human biomonitoring. TrAC - Trends Anal Chem. 2021;136:116201.
    https://doi.org/10.1016/j.trac.2021.116201 Schulze B, Jeon Y, Kaserzon S, Heffernan
    AL, Dewapriya P, O’Brien J, et al. An assessment of quality assurance/quality
    control efforts in high resolution mass spectrometry non-target workflows for
    analysis of environmental samples. TrAC - Trends Anal Chem. 2020;133: 116063.
    https://doi.org/10.1016/j.trac.2020.116063. Article   CAS   Google Scholar   Jewell
    KS, Hermes N, Ehlig B, Thron F, Köppe T, Thorenz U, et al. Methodik zur Anwendung
    von Non-Target-Screening (NTS) mittels LC-MS/MS in der Gewässerüberwachung. Umweltbundesamt;
    2021. Available from: https://www.umweltbundesamt.de/publikationen/methodik-zur-anwendung-von-non-target-screening-nts.
    Minkus S, Bieber S, Letzel T. Spotlight on mass spectrometric non-target screening
    analysis: advanced data processing methods recently communicated for extracting,
    prioritizing and quantifying features. Anal Sci Adv. 2022;3:103–112. https://doi.org/10.1002/ansa.202200001
    Broadhurst D, Goodacre R, Reinke SN, Kuligowski J, Wilson ID, Lewis MR, et al.
    Guidelines and considerations for the use of system suitability and quality control
    samples in mass spectrometry assays applied in untargeted clinical metabolomic
    studies. Metabolomics. 2018;14:1–17. https://doi.org/10.1007/s11306-018-1367-3.
    Article   CAS   Google Scholar   Kruve A, Kiefer K, Hollender J. Benchmarking
    of the quantification approaches for the non-targeted screening of micropollutants
    and their transformation products in groundwater. Anal Bioanal Chem. 2021;413:1549–59.
    https://doi.org/10.1007/s00216-020-03109-2. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Alygizakis N, Lestremau F, Gago-Ferrero P, Gil-Solsona R, Arturi K,
    Hollender J, et al. Towards a harmonized identification scoring system in LC-HRMS/MS
    based non-target screening (NTS) of emerging contaminants. TrAC - Trends Anal
    Chem. 2023;159: 116944. https://doi.org/10.1016/j.trac.2023.116944. Article   CAS   Google
    Scholar   Schymanski EL, Jeon J, Gulde R, Fenner K, Ruff M, Singer HP, et al.
    Identifying small molecules via high resolution mass spectrometry:communicating
    confidence. Environ Sci Technol. 2014;48:2097–8. https://doi.org/10.1021/es5002105.
    Article   CAS   PubMed   Google Scholar   Diera T, Thomsen AH, Tisler S, Karlby
    LT, Christensen P, Rosshaug PS, et al. A non-target screening study of high-density
    polyethylene pipes revealed rubber compounds as main contaminant in a drinking
    water distribution system. Water Res. 2023;229: 119480. https://doi.org/10.1016/j.watres.2022.119480.
    Article   CAS   PubMed   Google Scholar   Simonnet-Laprade C, Bayen S, McGoldrick
    D, McDaniel T, Hutinet S, Marchand P, et al. Evidence of complementarity between
    targeted and non-targeted analysis based on liquid and gas-phase chromatography
    coupled to mass spectrometry for screening halogenated persistent organic pollutants
    in environmental matrices. Chemosphere. 2022;293: 133615. https://doi.org/10.1016/j.chemosphere.2022.133615.
    Article   CAS   PubMed   Google Scholar   Wang S, Basijokaite R, Murphy BL, Kelleher
    CA, Zeng T. Combining passive sampling with suspect and nontarget screening to
    characterize organic micropollutants in streams draining mixed-use watersheds.
    Environ Sci Technol. 2022;56:16726–16736. https://doi.org/10.1021/acs.est.2c02938
    Yang Y, Yang L, Zheng M, Cao D, Liu G. Data acquisition methods for non-targeted
    screening in environmental analysis. TrAC Trends Anal Chem. 2023;160: 116966.
    https://doi.org/10.1016/j.trac.2023.116966. Article   CAS   Google Scholar   Hinnenkamp
    V, Balsaa P, Schmidt TC. Target, suspect and non-target screening analysis from
    wastewater treatment plant effluents to drinking water using collision cross section
    values as additional identification criterion. Anal Bioanal Chem. 2022;414:425–38.
    https://doi.org/10.1007/s00216-021-03263-1. Article   CAS   PubMed   Google Scholar   Nikolopoulou
    V, Aalizadeh R, Nika MC, Thomaidis NS. TrendProbe: time profile analysis of emerging
    contaminants by LC-HRMS non-target screening and deep learning convolutional neural
    network. J Hazard Mater. 2022;428: 128194. https://doi.org/10.1016/j.jhazmat.2021.128194.
    Article   CAS   PubMed   Google Scholar   Minkus S, Bieber S, Letzel T. (Very)
    polar organic compounds in the Danube river basin: a non-target screening workflow
    and prioritization strategy for extracting highly confident features. Anal Methods.
    2021;13:2044–2054. https://doi.org/10.1039/D1AY00434D Qian Y, Wang X, Wu G, Wang
    L, Geng J, Yu N, et al. Screening priority indicator pollutants in full-scale
    wastewater treatment plants by non-target analysis. J Hazard Mater. 2021;414:
    125490. https://doi.org/10.1016/j.jhazmat.2021.125490. Article   CAS   PubMed   Google
    Scholar   Kiefer K, Du L, Singer H, Hollender J. Identification of LC-HRMS nontarget
    signals in groundwater after source related prioritization. Water Res. 2021;196:
    116994. https://doi.org/10.1016/j.watres.2021.116994. Article   CAS   PubMed   Google
    Scholar   Peter KT, Kolodziej EP, Kucklick JR. Assessing reliability of non-targeted
    high-resolution mass spectrometry fingerprints for quantitative source apportionment
    in complex matrices. Anal Chem. 2022;94:2723–2731. https://doi.org/10.1021/acs.analchem.1c03202
    Arturi K, Hollender J. Machine Learning-Based Hazard-Driven Prioritization of
    Features in Nontarget Screening of Environmental High-Resolution Mass Spectrometry
    Data. Environ Sci Technol. 2023. https://doi.org/10.1021/acs.est.3c00304. Article   PubMed   PubMed
    Central   Google Scholar   Meekel N, Vughs D, Béen F, Brunner AM. Online prioritization
    of toxic compounds in water samples through intelligent hrms data acquisition.
    Anal Chem. 2021;93:5071–5080. https://doi.org/10.1021/acs.analchem.0c04473 Vazquez
    L, Llompart M, Dagnac T. Complementarity of two approaches based on the use of
    high-resolution mass spectrometry for the determination of multi-class antibiotics
    in water. Photodegradation studies and non-target screenings. Environ Sci Pollut
    Res Int. 2023;30:1871–1888. https://doi.org/10.1007/s11356-022-22130-9. Tisler
    S, Tüchsen PL, Christensen JH. Non-target screening of micropollutants and transformation
    products for assessing AOP-BAC treatment in groundwater. Environ Pollut. 2022;309:
    119758. https://doi.org/10.1016/j.envpol.2022.119758. Article   CAS   PubMed   Google
    Scholar   Letzel T, Bayer A, Schulz W, Heermann A, Lucke T, Greco G, et al. LC-MS
    screening techniques for wastewater analysis and analytical data handling strategies:
    sartans and their transformation products as an example. Chemosphere. 2015;137:198–206.
    https://doi.org/10.1016/j.chemosphere.2015.06.083. Article   CAS   PubMed   Google
    Scholar   Williams AJ, Grulke CM, Edwards J, McEachran AD, Mansouri K, Baker NC,
    et al. The CompTox Chemistry Dashboard: a community data resource for environmental
    chemistry. J Cheminform. 2017;9:61. https://doi.org/10.1186/s13321-017-0247-6.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Elapavalore A, Kondić
    T, Singh RR, Shoemaker BA, Thiessen PA, Zhang J, et al. Adding open spectral data
    to MassBank and PubChem using open source tools to support non-targeted exposomics
    of mixtures. Environ Sci Process Impacts. 2023. https://doi.org/10.1039/D3EM00181D
    Lestremau F, Levesque A, Lahssini A, de Bornier TM, Laurans R, Assoumani A, et
    al. Development and implementation of automated qualification processes for the
    identification of pollutants in an aquatic environment from high-resolution mass
    spectrometric nontarget screening data. ACS ES &T Water. 2023;3:765–72. https://doi.org/10.1021/acsestwater.2c00545.
    Palm E, Kruve A. Machine learning for absolute quantification of unidentified
    compounds in non-targeted LC/HRMS. Molecules. 2022;27(3):1013. https://doi.org/10.3390/molecules27031013.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Bieber S, Letzel T,
    Kruve A. Electrospray ionization efficiency predictions and analytical standard
    free quantification for SFC/ESI/HRMS. J Am Soc Mass Spectrom. 2023;34:1511–1518.
    https://doi.org/10.1021/jasms.3c00156 Choi Y, Lee JH, Kim K, Mun H, Park N, Jeon
    J. Identification, quantification, and prioritization of new emerging pollutants
    in domestic and industrial effluents, Korea: application of LC-HRMS based suspect
    and non-target screening. J Hazard Mater. 2021;402: 123706. https://doi.org/10.1016/j.jhazmat.2020.123706.
    Liigand J, Wang T, Kellogg J, Smedsgaard J, Cech N, Kruve A. Quantification for
    non-targeted LC/MS screening without standard substances. Sci Rep. 2020;10(1):5808.
    https://doi.org/10.1038/s41598-020-62573-z. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Wang YQ, Hu LX, Liu T, Zhao JH, Yang YY, Liu YS, et al. Per-and polyfluoralkyl
    substances (PFAS) in drinking water system: target and non-target screening and
    removal assessment. Environ Int. 2022;163: 107219. https://doi.org/10.1016/j.envint.2022.107219.
    Article   CAS   PubMed   Google Scholar   Wang T, Liigand J, Frandsen HL, Smedsgaard
    J, Kruve A. Standard substances free quantification makes LC/ESI/MS non-targeted
    screening of pesticides in cereals comparable between labs. Food Chem. 2020;318:
    126460. https://doi.org/10.1016/j.foodchem.2020.126460. Article   CAS   PubMed   Google
    Scholar   McCord JP, II LCG, Sobus JR. Quantitative non-targeted analysis: bridging
    the gap between contaminant discovery and risk characterization. Environ Int.
    2022;158:107011. https://doi.org/10.1016/j.envint.2021.107011 Schulz W, Achten
    C, Oberleitner D, Balsaa P, Hinnenkamp V, Brüggen S, et al. Anwendung des Non-Target-Screenings
    mittels LC-ESI-HRMS in der Wasseranalytik. 1st ed. Fachausschuss “Non-Target Screening”
    der Wasserchemische Gesellschaft; 2019. Available from: https://www.wasserchemische-gesellschaft.de/de/veroeffentlichungen/publikationen
    Petri M. Non-Target Screening für die Wasseranalytik - Auf dem Weg zur internationalen
    Normung. GIT Laborfachzeitschrift. 2023;Ausgabe 09/23:46.https://analyticalscience.wiley.com/do/10.1002/was.000600531/
    Hohrenk LL, Itzel F, Baetz N, Tuerk J, Vosough M, Schmidt TC. Comparison of software
    tools for liquid chromatography-high-resolution mass spectrometry data processing
    in nontarget screening of environmental samples. Anal Chem. 2020;92:1898–1907.
    https://doi.org/10.1021/acs.analchem.9b04095 Download references Acknowledgements
    We thank the University of Duisburg-Essen for their support and resources during
    this research endeavor. Funding Open Access funding enabled and organized by Projekt
    DEAL. Author information Author notes Maryam Vosough and Gerrit Renner contributed
    equally to this work. Authors and Affiliations Instrumental Analytical Chemistry,
    University of Duisburg-Essen, Universitätsstr. 5, Essen, 45141, North Rhine-Westphalia,
    Germany Maryam Vosough, Torsten C. Schmidt & Gerrit Renner Centre for Water and
    Environmental Research (ZWU), University of Duisburg-Essen, Universitätsstr. 2,
    Essen, 45141, North Rhine-Westphalia, Germany Maryam Vosough, Torsten C. Schmidt
    & Gerrit Renner Department of Clean Technologies, Chemistry and Chemical Engineering
    Research Center of Iran, P.O. Box 14335-186, Tehran, Iran Maryam Vosough IWW Water
    Centre, Moritzstr. 26, Mülheim an der Ruhr, 45476, North Rhine-Westphalia, Germany
    Torsten C. Schmidt Contributions \\(\\bullet \\) Maryam Vosough: Contributed equally
    to this work as a corresponding author. Involved in conceptualization, investigation,
    writing, and editing the manuscript. \\(\\bullet \\) Gerrit Renner: Contributed
    equally to this work as a corresponding author. Involved in conceptualization,
    investigation, writing, and editing the manuscript. \\(\\bullet \\) Torsten C.
    Schmidt: Contributed as a reviewing author. Provided critical reviews, insights,
    and comments that significantly enhanced the quality of the manuscript. Corresponding
    authors Correspondence to Maryam Vosough or Gerrit Renner. Ethics declarations
    Conflict of interest The authors declare no competing interests. Additional information
    Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Published in the topical
    collection Advances in (Bio-)Analytical Chemistry: Reviews and Trends Collection
    2024 Rights and permissions Open Access This article is licensed under a Creative
    Commons Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article’s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article’s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Vosough, M., Schmidt, T.C. &
    Renner, G. Non-target screening in water analysis: recent trends of data evaluation,
    quality assurance, and their future perspectives. Anal Bioanal Chem 416, 2125–2136
    (2024). https://doi.org/10.1007/s00216-024-05153-8 Download citation Received
    02 October 2023 Revised 12 January 2024 Accepted 12 January 2024 Published 01
    February 2024 Issue Date April 2024 DOI https://doi.org/10.1007/s00216-024-05153-8
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Non-target screening High-resolution mass spectrometry QA/QC
    in water analysis Chemometrics/machine learning Quantitative non-target screening
    Aquatic contaminants Data standardization Use our pre-submission checklist Avoid
    common mistakes on your manuscript. Associated Content Part of a collection: Advances
    in (Bio-)Analytical Chemistry: Reviews and Trends Collection 2024 Sections Figures
    References Abstract Introduction Current data evaluation trends in NTS Quality
    assurance/quality control efforts in NTS Selected NTS approaches in water analysis
    studies Quantification in NTS Conclusion and outlook References Acknowledgements
    Funding Author information Ethics declarations Additional information Rights and
    permissions About this article Advertisement Discover content Journals A-Z Books
    A-Z Publish with us Publish your research Open access publishing Products and
    services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Analytical and Bioanalytical Chemistry
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Non-target screening in water analysis: recent trends of data evaluation,
    quality assurance, and their future perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Onda K.
  - Chavez-Valdez R.
  - Graham E.M.
  - Everett A.D.
  - Northington F.J.
  - Oishi K.
  citation_count: '0'
  description: Neonatal hypoxic-ischemic encephalopathy (HIE) is the leading cause
    of acquired neonatal brain injury with the risk of developing serious neurological
    sequelae and death. An accurate and robust prediction of short- and long-term
    outcomes may provide clinicians and families with fundamental evidence for their
    decision-making, the design of treatment strategies, and the discussion of developmental
    intervention plans after discharge. Diffusion tensor imaging (DTI) is one of the
    most powerful neuroimaging tools with which to predict the prognosis of neonatal
    HIE by providing microscopic features that cannot be assessed by conventional
    magnetic resonance imaging (MRI). DTI provides various scalar measures that represent
    the properties of the tissue, such as fractional anisotropy (FA) and mean diffusivity
    (MD). Since the characteristics of the diffusion of water molecules represented
    by these measures are affected by the microscopic cellular and extracellular environment,
    such as the orientation of structural components and cell density, they are often
    used to study the normal developmental trajectory of the brain and as indicators
    of various tissue damage, including HIE-related pathologies, such as cytotoxic
    edema, vascular edema, inflammation, cell death, and Wallerian degeneration. Previous
    studies have demonstrated widespread alteration in DTI measurements in severe
    cases of HIE and more localized changes in neonates with mild-to-moderate HIE.
    In an attempt to establish cutoff values to predict the occurrence of neurological
    sequelae, MD and FA measurements in the corpus callosum, thalamus, basal ganglia,
    corticospinal tract, and frontal white matter have proven to have an excellent
    ability to predict severe neurological outcomes. In addition, a recent study has
    suggested that a data-driven, unbiased approach using machine learning techniques
    on features obtained from whole-brain image quantification may accurately predict
    the prognosis of HIE, including for mild-to-moderate cases. Further efforts are
    needed to overcome current challenges, such as MRI infrastructure, diffusion modeling
    methods, and data harmonization for clinical application. In addition, external
    validation of predictive models is essential for clinical application of DTI to
    prognostication.
  doi: 10.1159/000530938
  full_citation: '>'
  full_text: '>

    "All Content All Journals Developmental Neuroscience                              Advanced
    Search Register University of Nebraska-Lincoln Libraries Login CONTENT ABOUT SUBMISSION
    Volume 46, Issue 1 February 2024 Abstract Introduction Basic Principles of DTI
    and Quantification Neonatal HIE and DTI Changes DTI for Neuroprognostication in
    HIE Remaining Challenges and Future Directions Conclusion Conflict of Interest
    Statement Funding Sources Author Contributions References REVIEW ARTICLES| MAY
    10 2023 Quantification of Diffusion Magnetic Resonance Imaging for Prognostic
    Prediction of Neonatal Hypoxic-Ischemic Encephalopathy Subject Area: Further Areas
    , Neurology and Neuroscience , Women''s and Children''s Health Kengo Onda ; Raul
    Chavez-Valdez ; Ernest M. Graham ; Allen D. Everett; Frances J. Northington ;
    Kenichi Oishi Dev Neurosci (2024) 46 (1): 55–68. https://doi.org/10.1159/000530938
    Article history PubMed: 37231858 Split-Screen Views Download Share Tools Abstract
    Neonatal hypoxic-ischemic encephalopathy (HIE) is the leading cause of acquired
    neonatal brain injury with the risk of developing serious neurological sequelae
    and death. An accurate and robust prediction of short- and long-term outcomes
    may provide clinicians and families with fundamental evidence for their decision-making,
    the design of treatment strategies, and the discussion of developmental intervention
    plans after discharge. Diffusion tensor imaging (DTI) is one of the most powerful
    neuroimaging tools with which to predict the prognosis of neonatal HIE by providing
    microscopic features that cannot be assessed by conventional magnetic resonance
    imaging (MRI). DTI provides various scalar measures that represent the properties
    of the tissue, such as fractional anisotropy (FA) and mean diffusivity (MD). Since
    the characteristics of the diffusion of water molecules represented by these measures
    are affected by the microscopic cellular and extracellular environment, such as
    the orientation of structural components and cell density, they are often used
    to study the normal developmental trajectory of the brain and as indicators of
    various tissue damage, including HIE-related pathologies, such as cytotoxic edema,
    vascular edema, inflammation, cell death, and Wallerian degeneration. Previous
    studies have demonstrated widespread alteration in DTI measurements in severe
    cases of HIE and more localized changes in neonates with mild-to-moderate HIE.
    In an attempt to establish cutoff values to predict the occurrence of neurological
    sequelae, MD and FA measurements in the corpus callosum, thalamus, basal ganglia,
    corticospinal tract, and frontal white matter have proven to have an excellent
    ability to predict severe neurological outcomes. In addition, a recent study has
    suggested that a data-driven, unbiased approach using machine learning techniques
    on features obtained from whole-brain image quantification may accurately predict
    the prognosis of HIE, including for mild-to-moderate cases. Further efforts are
    needed to overcome current challenges, such as MRI infrastructure, diffusion modeling
    methods, and data harmonization for clinical application. In addition, external
    validation of predictive models is essential for clinical application of DTI to
    prognostication. Journal Section: Neurodevelopmental Consequences of Perinatal
    Brain Injuries: Review Article Keywords: Hypoxic-ischemic encephalopathy, Magnetic
    resonance imaging, Diffusion tensor imaging, Prognostic prediction, Neonatal brain
    atlas, Neonatal brain injury, Hypothermia, Myelination, Outcome prediction, Neural
    development Introduction Neonatal hypoxic-ischemic encephalopathy (HIE) is the
    most common cause of neonatal encephalopathy and one of the major causes of neonatal
    acquired brain injury. HIE is caused by the disruption of cerebral blood flow
    and oxygen supply during the perinatal period [1]. Neonatal HIE may lead to significant
    long-term neurological disorders or death, and its incidence rate is approximately
    1.5 cases per 1,000 total live births [2]. Although therapeutic hypothermia (TH)
    has reduced mortality and morbidity by one-third and has become a standard treatment
    [3‒7], a significant proportion of survivors still develop residual neurological
    outcomes. However, identifying those babies at risk remains a significant challenge.
    An accurate and robust prediction of short- and long-term outcomes may provide
    clinicians and families with fundamental evidence for their decision-making, the
    design of treatment strategies, and the discussion of developmental intervention
    plans after discharge [8]. Therefore, the development of accurate prognostic indicators
    has been the focus of recent research in the field of neonatal HIE. In light of
    this trend, various prognostic markers have been studied, including clinical scores
    [9‒12], serum proteins [13], neurophysiological examinations [14‒17], and neuroimaging
    modalities, particularly magnetic resonance imaging (MRI). Brain MRI has been
    widely used in clinical practice, playing an essential role as a noninvasive technology
    with which to locate and evaluate brain lesions in detail. MRI findings correlate
    with short- and long-term neurological outcomes [18‒24] and in combination with
    conventional biomarkers can improve prognostic predictive ability [19, 25, 26].
    However, one of the biggest challenges in prognostication using conventional anatomical
    MRI (e.g., T1- and T2-weighted images) is the low sensitivity. HIE children with
    severe neurological outcomes may lack significant MRI findings during neonatal
    period (first 28 days of life) [19, 27‒30]. To overcome this challenge, various
    types of image quantification methodologies have been introduced and applied to
    quantitative MRI modalities, such as diffusion MRI [31, 32] and MR spectroscopy
    [28, 33‒35]. Diffusion MRI quantifies the thermal motion of the water molecules
    in the tissue and uses it as a probe to detect microstructural alterations that
    cannot be visualized by anatomical MRI [36‒38]. MR spectroscopy utilizes the chemical
    properties of the tissue to probe metabolic alterations in the brain. This review
    focuses on the prognostic value of diffusion MRI, particularly diffusion tensor
    imaging (DTI). An overview of study designs and image quantification methods,
    including hypothesis-driven approaches based on quantification of specific regions
    of interest (ROI) [39] to data-driven approaches, such as voxel-based analysis
    using tract-based spatial statistics (TBSS) [40], and atlas-based methods for
    image quantification (ABA), is provided. Studies that examine how DTI quantification
    values are changed by HIE and the correlation between these changes and HIE severity
    will be introduced. Methods with which to set cutoff values for DTI quantification
    values, and thus, identify severe cases and prognostic models based on whole-brain
    quantification approaches combined with machine learning, will also be presented,
    along with the ability of these methods to predict the prognosis of HIE patients.
    Basic Principles of DTI and Quantification Diffusion MRI quantifies the thermal
    motion of water molecules in tissues using mathematical models, from which microscopic
    anatomical structures can be characterized [36‒38]. The simplest model is the
    average of the magnitude of diffusion in three orthogonal directions, called a
    mean apparent diffusion coefficient (ADC) map, which is widely used in clinical
    practice to diagnose acute ischemic events in the brain, including neonatal HIE.
    A mean ADC map can detect acute to subacute HIE lesions that are normal or only
    slightly abnormal on conventional MRI [19, 27‒30]. However, the information obtained
    from the mean ADC map alone does not provide a detailed description of the various
    pathological processes seen in HIE. DTI introduces a second-order tensor model
    that fits the diffusion of water in each voxel to an ellipsoid. Compared to the
    simple average model that provides the mean ADC map, DTI enables the generation
    of various measures that represent more detailed properties of the tissue, such
    as the size, shape, and direction of water diffusion that can be calculated from
    the tensor. Widely used scalar measures are fractional anisotropy (FA), mean diffusivity
    (MD), axial diffusivity, and radial diffusivity, which can be calculated from
    the first, second, and third eigenvalues (λ1, λ2, λ3) of a tensor [41‒44] (Fig.
    1). Since the characteristics of the diffusion of water molecules in tissues are
    affected by histological factors, such as the density of macromolecules, the orientation
    of structural components, and cell density, these parameters have often been used
    to study the normal developmental trajectory of the brain, particularly the white
    matter. Fig. 1. VIEW LARGEDOWNLOAD SLIDE Schematic representation of the relationship
    between water diffusion, tensor fitting, and scalar values obtained from the tensor.
    The water molecule (red circle) and diffusion trajectory (red line) in the left
    figure are approximated by an ellipsoid (pink ellipsoid surrounded by a red dotted
    line) through tensor fitting. The three orthogonal axes of the ellipsoid are called
    the first, second, and third axes, and the length of each axis is called the first
    eigenvalue (λ1), second eigenvalue (λ2), and third eigenvalue (λ3; λ1≥λ2≥λ3).
    Various scalar measures can be obtained from the three eigenvalues. Fractional
    anisotropy (FA) and mean diffusivity (MD) are often used to estimate the tissue
    microstructure of the brain. Maturation of cerebral white matter is characterized
    by the formation of dense and thick fiber bundles and myelin sheaths, both of
    which are actively ongoing processes during the perinatal period [45‒47]. The
    formation of tightly packed fiber bundles and the myelination both result in reduced
    extracellular space and restriction of water diffusion toward the direction perpendicular
    to the direction of fiber bundles [48, 49]. MD is the mean of the three diffusion
    tensor eigenvalues and represents the magnitude of diffusion (Fig. 1). When the
    extracellular space is reduced due to white matter maturation, MD decreases because
    water diffusion is more restricted. FA is a measure of the degree of anisotropy
    of diffusion and takes a scalar value between 0 and 1. Zero means that diffusion
    is isotropic, i.e., there is no preference for a particular direction, and 1 means
    that diffusion is completely anisotropic, i.e., it occurs only in a particular
    direction (Fig. 1). FA increases as fiber bundles pack more tightly during white
    matter maturation, reducing diffusivity in the direction perpendicular to the
    white matter tracts. Another major feature of DTI is the delineation of white
    matter fiber bundles as streamlines [50‒52]. Since water molecules tend to diffuse
    in the direction of the fiber bundles, quantifying the orientation of the diffusion
    distribution of water molecules from diffusion MRI data can provide indirect information
    about how the white matter fiber bundles travel. The three-dimensional modeling
    technique used to create these streamlines is called tractography, and the streamlines
    created are called tractograms. During the course of brain development, fiber
    density and the diameter of fiber bundles increase. Therefore, the types of white
    matter fiber bundles for which tractograms can be delineated increase [53]. The
    various scalar maps produced from DTI can be used for statistical analysis, such
    as group comparisons, or as input variables for creating prognostic models. For
    this purpose, scalar values derived from the tensor (e.g., MD and FA) must be
    quantified for each anatomical region. Commonly used methods for this purpose
    include ROI-based analysis, TBSS, and ABA [54]. For study designs based on hypotheses,
    such as that certain brain regions are more likely to be impaired by HIE, ROIs
    are often used to quantify DTI. In this case, preselected ROIs are drawn in the
    specific regions based on the hypotheses, and the average scalar value of pixels
    in the ROI is calculated. ROI drawing can be done manually or by automatic image
    parcellation methods. When a specific fiber tract defines the ROI, fiber tractography
    is used as a means of delineating the ROI. The term “tract-based analysis” is
    commonly used to describe this type of DTI quantification method. To search brain
    regions affected by HIE in a data-driven approach, rather than being based on
    a specific hypothesis, DTI is quantified for the whole brain. TBSS is a method
    with which to quantify scalar values on a voxel-by-voxel basis for entire white
    matter regions based on skeletonization of group-averaged FA maps [55, 56]. This
    method is often used for the voxel-by-voxel statistical analysis as described
    in the following sections. ABA is based on whole-brain image parcellation. Each
    DTI is divided into multiple ROIs that cover the whole brain, allowing quantification
    of DTI-derived scalar values for each ROI that defines a specific anatomical structure
    or region (Fig. 2). Since whole-brain image parcellation is prohibitively difficult
    to perform manually, various automated methods have been proposed [57‒60]; once
    DTI is parcellated, whole-brain ROI-by-ROI statistical analysis can be performed.
    A limited number of ROIs can also be selected from the set of all ROIs, based
    on a hypothesis, and used for ROI analysis for hypothesis testing. Fig. 2. VIEW
    LARGEDOWNLOAD SLIDE An example of a multi-contrast neonatal brain atlas (top)
    and atlas-based DTI parcellation (bottom). Lesions seen in the mean diffusivity
    (MD) map are highlighted in red (bottom middle). By deforming the neonatal atlas
    to fit the patient’s brain and superimposing the deformed atlas parcellation map
    on the MD map (bottom right), MD values can be quantified for each anatomic region
    defined in the parcellation map. TBSS and ABA can be used in combination: after
    image quantification of the FA skeleton on a voxel-by-voxel basis, anatomical
    parcellation maps obtained with the atlas-based approach can be overlaid on the
    FA skeleton to segment the FA skeleton and perform image quantification based
    on the skeletons-of-interest. It is also possible to annotate the skeleton based
    on the anatomical labels of the reference atlas [61, 62]. DTI has been instrumental
    in elucidating the developmental patterns of brain white matter fibers. In principle,
    brain maturation proceeds from the most ventro-caudal parts of the brain toward
    the most dorso-rostral parts. That is, maturation begins from the brainstem toward
    the cerebrum [47, 54]. Within the cerebrum, the direction of maturation is from
    the center to the periphery and from posterior to anterior [63‒65]. Among the
    centrally located fiber bundles, the corticospinal tracts (CSTs) are the earliest
    to mature [66]. Within the cerebral cortex, the primary cortex matures ahead of
    the association cortex [65, 67]. Understanding the relationship between structure-specific
    maturation patterns of white matter bundles and the changes observed in DTI measurements
    (especially FA and MD) is fundamental to the interpretation of DTI results seen
    in pathological conditions, including neonatal HIE. There is a caveat to the use
    of DTI in research: because DTI measurements are very sensitive to differences
    in scanner types and scan parameters, it is preferable to use a single scanner
    and fixed scan protocol in scientific studies. When analysis is based on DTI scanned
    with various scanners and scan parameters, as is often the case in multicenter
    studies, mathematical models need to be introduced to harmonize the data [68‒70].
    In general, the larger the voxel size, the lower the FA value. This is due to
    the loss of diffusion anisotropy as voxel size increases, as it is more likely
    to contain multiple fiber structures oriented differently within each voxel. Neonatal
    HIE and DTI Changes The main pathogenesis of HIE can be roughly divided into primary
    (0–6 h after initial injury) and secondary (6–48 h after initial injury) periods
    of energy failure [1, 71]. During primary energy failure, insufficient cerebral
    blood flow and oxygen supply cause acute dysfunction of mitochondria and transition
    from aerobic to anaerobic glycolysis, leading to ATP deficit and the disruption
    of the Na+/K+ ATPase pump. These events result in cytotoxic edema, which is a
    redistribution of water from the extracellular to the intracellular space due
    to the osmotic stress [1, 72]. In DTI, cytotoxic edema is expressed by a decrease
    in MD and ADC, reflecting a decrease in extracellular space that restricts the
    available space for free water molecules to diffuse [73]. In white matter regions,
    an FA increase can be observed in the acute phase of brain injury because diffusion
    in the direction orthogonal to the fiber bundle is more reduced than diffusion
    in the direction parallel to the fiber bundle [74] (Fig. 3). Fig. 3. VIEW LARGEDOWNLOAD
    SLIDE Schematic representation of the pathological changes seen in neonatal HIE.
    The upper panel shows gray matter areas, and the lower panel shows white matter
    areas where myelinated axons form densely packed fiber bundles. Neurons are color
    coded in blue, myelin sheaths in light blue, astrocytes in orange, and microglia
    in light green. Water molecules (red circles) and their diffusion trajectories
    (red lines) are also shown as fitted ellipsoids (pink ellipsoids surrounded by
    red dotted lines). Changes in the FA and MD values associated with each pathology
    is represented by: up arrow, increase; no change, n.c.; decrease, down arrow.
    During secondary energy failure, free radical release due to the reperfusion and
    reoxygenation of cerebral tissues causes oxidation stress. This stress damages
    the blood-brain barrier function, which is followed by increased vascular permeability,
    and causes extravascular leakage of serum proteins. Because of the low capacity
    to remove free radicals, this leakage induces prolonged extracellular fluid retention
    and results in vasogenic edema [30]. In DTI, vasogenic edema is expressed by increased
    MD, reflecting increased extracellular fluid retention, thus allowing water molecules
    to diffuse more freely with less restriction [75]. In white matter regions, FA
    tends to decrease with increasing MD because of relatively free diffusion in the
    extracellular space [76] (Fig. 3). Hypoxic-ischemic conditions cause neuronal
    death, followed by Wallerian degeneration, a retrograde degeneration of the distal
    end of the axon [77, 78]. Hours to 2 weeks after neuronal death, axonal and myelin
    debris accumulate in lesions where Wallerian degeneration has occurred. This restricts
    water diffusion, resulting in a marked decrease in FA and MD values [79, 80].
    Wallerian degeneration is typically seen throughout the CST, including the posterior
    limb of the internal capsule (PLIC) and the cerebral peduncle [81] (Fig. 3, 4).
    In the chronic phase, neuronal loss and gliosis cause a decrease in FA and an
    increase in MD (Fig. 3). Fig. 4. VIEW LARGEDOWNLOAD SLIDE Wallerian degeneration
    seen after a hypoxic-ischemic event. The subacute cerebral infarct area (red arrow)
    is depicted as a high-signal area on the mean diffusion-weighted image (DWI),
    with low mean diffusivity (MD) and low fractional anisotropy (FA). Wallerian degeneration
    (yellow arrow) in the left CST is hyperintense on DWI, with low MD and FA. The
    results of tractography are also influenced by pathological changes related to
    HIE. If a major fiber tract, such as the CST, is damaged by HIE, the tractograms
    of that fiber may be disrupted, reduced in number, or lost [82]. In HIE neonates,
    the mixed effects of cytotoxic and vasogenic edemas, cell death, and Wallerian
    degeneration can be observed [77, 83]. According to the dominant effect of the
    pathogenesis, the DTI findings change chronologically (Fig. 5). In general, over
    the first week of HIE, FA values may increase (Fig. 6) or decrease (Fig. 7), while
    MD and ADC values decrease (Fig. 6, 7). The decreased MD and ADC values return
    to the normal range in the second week, a finding termed pseudo-normalization
    [84‒86]. Thereafter, MD and ADC values increase over the chronic phase, but FA
    values decrease [84, 86] (Fig. 8). As discussed in the following sections, the
    degree of change and the location where the change is seen depend on the severity
    of the HIE. Fig. 5. VIEW LARGEDOWNLOAD SLIDE Graphical representation of changes
    over time in diffusivity measures seen in neonatal HIE. Fig. 6. VIEW LARGEDOWNLOAD
    SLIDE Acute ischemic changes seen in HIE. a Lesion (yellow arrow) is depicted
    as a high-signal area on mean diffusion-weighted image (DWI), showing low mean
    diffusivity (MD) and high fractional anisotropy (FA), possibly dominated by cytotoxic
    edema. b Lesion (yellow arrow) is depicted as a high-signal area on mean diffusion-weighted
    image (DWI), with low mean diffusivity (MD) and normal or slightly high fractional
    anisotropy (FA). Fig. 7. VIEW LARGEDOWNLOAD SLIDE Subacute ischemic changes seen
    after a hypoxic-ischemic event. The lesion (yellow arrow) is depicted as a high-signal
    area on mean diffusion-weighted images (DWI), showing low mean diffusivity (MD)
    and low fractional anisotropy (FA), probably representing a mixed lesion of cytotoxic
    edema and cell death. Fig. 8. VIEW LARGEDOWNLOAD SLIDE Chronic changes seen after
    a hypoxic-ischemic event. The lesion (yellow arrow) is depicted as a slightly
    high-signal area on mean diffusion-weighted images (DWI), showing high mean diffusivity
    (MD) and low fractional anisotropy (FA), probably representing the gliosis that
    occurs after cell death. The first step for predicting neurological outcomes is
    to identify the anatomical structures affected by HIE and to understand how DTI
    measurements are altered in these structures. Toward this goal, group comparisons
    have been made between HIE and control groups. In a cross-sectional study, an
    ROI-based study performed on DTI acquired within 1 week after birth identified
    decreased MD in the putamen, thalamus, anterior and posterior limbs of internal
    capsule, occipital white matter, and the CST, compared to age-matched healthy
    infants [87]. A study targeting DTIs acquired within 15 days after birth and focused
    on the metencephalon and thalamus demonstrated that decreased MD in the superior
    cerebellar peduncle and decreased FA in the middle cerebellar peduncle were seen
    in the severe HIE group compared to the control group [88]. Another study that
    included even older neonates scanned within 3 weeks after birth demonstrated increased
    ADC accompanied by decreased FA in the HIE group compared to the non-HIE control
    group, in the basal ganglia (BG), thalamus, PLIC, cerebral peduncle, CST, and
    peripheral white matter areas [89], and increased MD accompanied by decreased
    FA in the PLIC, superior corona radiata, corpus callosum (CC), and external capsule
    [90]. The discrepancy between decreased and increased diffusivity between these
    studies probably reflects the timing of the scan; the former study might have
    mainly captured the effects of cytotoxic edema, while the latter mainly captured
    vascular edema and cell death. These studies highlight the importance of the timing
    of the scan in interpreting the findings. While ROI-based studies could delineate
    which DTI measures are affected in which anatomical structures, only a limited
    number of preselected brain areas have been explored; huge brain areas remain
    unexplored. To overcome the limitation of ROI-based analysis, TBSS was used to
    explore the whole white matter skeleton in a voxel-by-voxel manner. According
    to the group comparisons between HIE and healthy controls that targeted DTIs acquired
    at 1–4 weeks [90‒92], decreased FA [91, 92] and increased MD [90] and radial diffusivity
    (mean of the second and third eigenvalues) were observed and were widespread,
    especially in patients with HIE who had not received TH [91]. The areas of impairment
    were significantly smaller for the group of HIE neonates who received TH compared
    to those without TH, suggesting the neuroprotective effect of TH and importance
    of subgrouping HIE neonates by the use of TH. The next issue is whether DTI findings
    are associated with the severity of the neurological outcome. In general, severe
    HIE has been proven to be associated with more severe and extensive gray matter
    and white matter alterations compared to neonates with moderate or mild HIE. On
    DTI images obtained within 6 days of birth, HIE neonates with low FA in the PLIC
    and cerebral peduncle and low ADC in the PLIC and BG were demonstrated to have
    poor neurological function according to the Amiel-Tison neurological assessment
    [93]. A reduction of FA was observed in moderate-to-severe HIE for the first 3
    weeks of life, but ADC was reduced in the white matter, the BG, and the thalamus
    in only some severe HIE newborns, with pseudo-normalization at 2 weeks [86]. This
    suggests that FA is a sensitive marker for the detection of anatomical changes
    associated with HIE, and ADC is a marker for differentiating severe cases from
    moderate HIE. ABA performed with diffusion MRI showed that severe HIE neonates
    scanned at approximately 1 week of age [94] showed an extensive reduction in FA,
    while moderate HIE neonates showed a reduction in FA only in the inferior association
    fibers (inferior longitudinal fasciculus and inferior fronto-occipital fasciculus).
    In addition, diffusion MRI measurements obtained by non-tensor modeling, such
    as fiber density, fiber cross-section, and fiber density and cross-section obtained
    by pixel-based analysis, demonstrate the potential for highly sensitive detection
    of anatomical changes seen in mild-to-moderate HIE newborns [94]. The topic of
    non-tensor modeling is discussed in the section “Remaining Challenges and Future
    Directions.” DTI for Neuroprognostication in HIE Attempts have been made to predict
    the severity of neurological symptoms from DTI measurements in HIE neonates. Such
    studies are categorized as those aimed at predicting neurological status at the
    time of scanning (state prediction), around 1 month (short-term prognosis), and
    after 1 year of age (long-term prognosis) [95]. In a study aimed at predicting
    the neurological state during the first 2 weeks of life, FA values quantified
    at the anterior limb of the internal capsule using the JHU_MNI atlas [96] could
    predict good neurological status, defined by a neonatal behavioral neural assessment
    score [97] of 35 or higher, with a sensitivity of 70% and specificity of 80%,
    using FA ≥0.395 as the cutoff value [98]. Note that this cutoff FA value may not
    necessarily be generalizable since FA values can take different values depending
    on scan parameters, as described in the section “Basic Principles of DTI and Quantification.”
    In an early attempt to use FA values obtained from thalamic ROIs at 2 weeks of
    age to predict short-term prognosis, FA values were lower in HIE than in controls
    but did not correlate with length of hospital stay, time to oral intake, or presence
    of seizures [39]. However, a recent study combined atlas-based image quantification
    with a least absolute shrinkage and selection operator regression model to detect
    changes in DTI, i.e., FA and MD changes in limbic, frontotemporal, and corticospinal
    projection fibers, even in mild HIE. The score obtained from the least absolute
    shrinkage and selection operator regression model, named the composite DTI (cDTI)
    score, correlated strongly with short-term neurological outcome (rho = 0.83) and
    was also associated with changes in serum biomarkers, such as IL10 and tau [99].
    Notably, although voxel size was controlled, the study included four different
    scanners and slightly different scanning protocols, but there was no significant
    effect of scanner and scanning parameters on the cDTI score. This study demonstrates
    the potential of whole-brain, data-driven approaches in quantifying lesions related
    to neurological outcomes and the potential use of DTI for neurological prognostication
    in a multicenter setting. Since fine motor and language skills are difficult to
    assess in infancy, it would be important to evaluate whether DTI acquired in the
    neonatal period can predict neurological prognosis after the age of 1 year, when
    these neurological functions can be assessed [95, 100]. A tracts-of-interest study
    of CC and CST showed an association between lower FA and lower mental and psychomotor
    developmental indices assessed at 15 and 21 months [101]. In a study [40] using
    the TBSS to identify white matter skeletons associated with neurological ability
    at 12–28 months of age based on the Griffiths Mental Development Scales-Revised,
    FA values in the most significant voxels of the cerebral peduncle correlated well
    with developmental quotient (R2 = 0.42), locomotor score (R2 = 0.28), and eye-hand
    coordination (R2 = 0.31). FA values in the CC showed good correlations with the
    personal-social score (R2 = 0.33) and the hearing and language score (R2 = 0.3)
    and FA values in the PLIC with the performance score (R2 = 0.37). Several attempts
    have been made to establish cutoff values with which to predict the occurrence
    of neurological sequelae that become evident after 1 year. MD and FA assessed
    at 4–16 days of age could be used as predictive markers for poor prognosis assessed
    at 1 year of age, i.e., death, cerebral palsy, dysarthria, and general developmental
    delay (general developmental quotient less than 88.7) as measured by the Griffith
    scale [102]. In this study, receiver operating characteristic curve analysis was
    introduced to establish cutoff MD and FA values with which to predict poor prognosis.
    MD measured in the CC, thalamus, caudate, and frontal white matter had an area
    under the curve (AUC) of 0.72–0.78, a positive predictive value of 1, and a negative
    predictive value of 0.83, while FA in the frontal white matter had an AUC of 0.94,
    a positive predictive value of 0.71, and a negative predictive value of 1, indicating
    a reliable predictability of poor outcome. A recent study has largely replicated
    the finding: the FA value measured at the PLIC, centrum semiovale, and cerebral
    peduncle from images scanned at 10–14 days of age could predict the occurrence
    of cerebral palsy at 1 year of age [103]. In particular, PLIC with a cutoff FA
    value of ≤0.435 exhibited an AUC of 1.0 (sensitivity 1.0, specificity 1.0) and
    the centrum semiovale with a cutoff FA value of ≤0.235 exhibited an AUC of 0.95
    (sensitivity 0.94, specificity of 0.94), showing excellent predictive ability.
    Rather than establishing a specific cutoff value for prediction, an attempt has
    been made to introduce a classification algorithm: a multidimensional marker derived
    from tractography using the non-tensor method was introduced to predict the presence
    of abnormal muscle tone on neurological examination at 2 years of age [104]. This
    prediction was based on a random forest classifier that used 19 pairwise connections
    with abnormal tractography findings as input values, with an excellent classification
    performance of 0.99 sensitivity and 1.0 specificity to identify a group of neonates
    who developed abnormal muscle tone at 2 years of age. These studies demonstrate
    that neonatal DTI has excellent potential to accurately predict both short- and
    long-term severe neurological outcomes. However, the number of HIE neonates included
    in each study was less than 60, and none of the prediction models have been validated
    in an external cohort of neonatal HIE; multicenter validation studies with larger
    neonatal HIE cohorts are essential for the clinical application of DTI as a prognostic
    predictor. In addition, there are no studies of milder neurological sequelae that
    become evident after school age, such as neuropsychiatric symptoms and learning
    disabilities; further research is needed to determine whether DTI can predict
    longer term outcomes. Remaining Challenges and Future Directions The purpose of
    prognostication is to provide clues for medical decision-making. Although DTI
    has shown potential as an accurate prognostic tool, there are still issues that
    need to be resolved before its clinical application. First, performing a MRI scan
    on vulnerable neonates is not always easy. Especially for neonates who are fitted
    with infusion lines and monitors, transfer to the scanner room requires the assistance
    of multiple staff members. The noise and vibration caused by the gradient applied
    during the scan arouses the neonate, causing motion artifacts and reducing the
    success rate of the scan. Hardware improvements, including improvements to hospital
    facilities, may be needed to solve these problems. For example, some advanced
    neonatal intensive care units have solved these problems by installing an MRI
    scanner in the neonatal intensive care unit [105, 106]. Attempts are also being
    made to improve the success rate of scans by reducing motion artifacts and scan
    time [107‒109]. Such efforts can also improve the quality of the studies. The
    quality of neonatal DTI studies is compromised by the small number of participants;
    improving the success rate of DTI scans would allow better studies. Second, some
    of the studies presented in the previous section used non-tensor models to quantify
    the effects of HIE on the brain [90, 104], suggesting the possibility of detecting
    minor anatomical abnormalities with higher sensitivity than conventional DTI measurements.
    Conventional tensor models assume that the diffusion follows a three-dimensional
    Gaussian distribution and estimate one tensor field within each voxel. While this
    method is effective for extracting gross features of diffusion, such approximations
    are considered oversimplified when more detailed features are needed [110]. Non-tensor
    models were introduced to describe non-Gaussian diffusion and to resolve fiber
    orientation within voxels that cannot be quantified by tensor models [111‒113].
    Further studies are needed to determine whether non-tensor models such as fixel-based
    analysis and non-tensor-based probabilistic tractography can provide better prognostic
    prediction than a conventional tensor model. Predictive modeling by applying machine
    learning to input variables from the whole brain has also been introduced in several
    studies [99, 104], showing the potential to maximize the prognostic predictive
    ability of DTI by comprehensively identifying imaging changes that may contribute
    to the pathogenesis of HIE. Such an approach is expected to improve the sensitivity
    of detecting minor pathological changes, as demonstrated in [99]. Third, standardized
    methods to harmonize DTI data have not yet been established. Most studies, to
    date, have been based on small numbers of subjects, resulting in the risk of selection
    bias and overfitting of predictive models to the study population. Predictive
    models with a larger number of subjects are needed for clinical application, thus
    necessitating multicenter studies and data sharing. However, DTI measurements
    are sensitive to differences in scanner and scan parameters, hindering the use
    of technically heterogeneous DTI. Several attempts have been made to harmonize
    DTI data [68‒70], and such efforts are essential for future multicenter studies
    and clinical applications. Conclusion DTI is one of the most powerful neuroimaging
    tools for the prediction of neonatal HIE prognosis by providing microscopic features
    that cannot be assessed by conventional MRI. Applying a data-driven, unbiased
    approach using machine learning techniques to these features obtained by whole-brain
    imaging quantification could provide more accurate predictions of HIE prognosis
    than those based on DTI quantitative values obtained from a single ROI. For clinical
    application, further efforts are needed to overcome current challenges, such as
    MRI infrastructure, diffusion modeling methods, and data harmonization. In addition,
    external validation of the prediction model is essential for the clinical application
    of DTI for prognostic prediction. Conflict of Interest Statement KOi is a consultant
    for “AnatomyWorks” and “Corporate-M.” This arrangement is being managed by the
    Johns Hopkins University in accordance with its conflict of interest policies.
    The remaining authors have no conflict of interest to declare. Funding Sources
    This work was supported by the National Institutes of Health R01HD065955 (K.Oi.),
    R01NS126549 (K.Oi.), R01HD086058 (A.D.E. and F.J.N.), R0-1HD070996 (F.J.N.), R21AG061643
    (F.J.N.), R21NS123814 (F.J.N.), R01HD1100919 (F.J.N., A.D.E., R.C.-V., and E.M.G.),
    R01HD074593 (F.J.N.), U01NS114144 (F.J.N. and A.D.E.), K08NS096115 (R.C.-V.),
    K08NS096115-03S1 (R.C.-V.), and the Thomas Wilson Foundation (R.C.-V.). Author
    Contributions Kengo Onda and Kenichi Oishi: substantial contributions to the conception
    of the work; drafting of significant portions of the manuscript, tables, and figures;
    and final approval of the manuscript. Raul Chavez-Valdez, Ernest M. Graham, Allen
    D. Everett, and Frances J. Northington: substantial contributions to the conception
    of the work, revising the manuscript critically for important intellectual content,
    and final approval of the manuscript. References 1.Allen KA, Brandon DH. Hypoxic
    ischemic encephalopathy: pathophysiology and experimental treatments. Newborn
    Infant Nurs Rev. 2011 Sep 111312533. Google ScholarCrossref   2.Kurinczuk JJ,
    White-Koning M, Badawi N. Epidemiology of neonatal encephalopathy and hypoxic-ischaemic
    encephalopathy. Early Hum Dev. 2010 Jun86632938. Google ScholarCrossref PubMed  3.Gluckman
    PD, Wyatt JS, Azzopardi D, Ballard R, Edwards AD, Ferriero DM. Selective head
    cooling with mild systemic hypothermia after neonatal encephalopathy: multicentre
    randomised trial. Lancet. 2005 Feb365946066370. Google ScholarCrossref PubMed  4.Shankaran
    S, Laptook AR, Ehrenkranz RA, Tyson JE, McDonald SA, Donovan EF. Whole-body hypothermia
    for neonates with hypoxic: ischemic encephalopathy. N Engl J Med. 2005;353(15):1574–84.
    Google ScholarCrossref PubMed  5.Edwards AD, Brocklehurst P, Gunn AJ, Halliday
    H, Juszczak E, Levene M. Neurological outcomes at 18 months of age after moderate
    hypothermia for perinatal hypoxic ischaemic encephalopathy: synthesis and meta-analysis
    of trial data. BMJ. 2010340c363. Google ScholarCrossref PubMed  6.Tagin MA, Woolcott
    CG, Vincer MJ, Whyte RK, Stinson DA. Hypothermia for neonatal hypoxic ischemic
    encephalopathy: an updated systematic review and meta-analysis. Arch Pediatr Adolesc
    Med. 2012 Jun 1166655866. Google ScholarCrossref   7.Jacobs SE, Berg M, Hunt R,
    Tarnow-Mordi WO, Inder TE, Davis PG. Cooling for newborns with hypoxic ischaemic
    encephalopathy. Cochrane Database Syst Rev. 2013 Jan 3120131Cd003311. Google Scholar  8.Robertson
    CM, Perlman M. Follow-up of the term infant after hypoxic-ischemic encephalopathy.
    Paediatr Child Health. 2006;11(5):278–82. Google ScholarPubMed  9.Sarnat HB, Sarnat
    MS. Neonatal encephalopathy following fetal distress. A clinical and electroencephalographic
    study. Arch Neurol. 1976 Oct3310696705. Google ScholarCrossref PubMed  10.Einspieler
    C, Prechtl HF, Ferrari F, Cioni G, Bos AF. The qualitative assessment of general
    movements in preterm, term and young infants: review of the methodology. Early
    Hum Dev. 1997;50(1):47–60. Google ScholarCrossref PubMed  11.Haataja L, Mercuri
    E, Regev R, Cowan F, Rutherford M, Dubowitz V. Optimality score for the neurologic
    examination of the infant at 12 and 18 months of age. J Pediatr. 19991352 Pt 115361.
    Google ScholarPubMed  12.Apgar V. A proposal for a new method of evaluation of
    the newborn infant. Originally published in july 1953, volume 32, pages 250-259.
    Anesth Analg. 2015 May120510569. Google ScholarCrossref PubMed  13.Douglas-Escobar
    M, Weiss MD. Biomarkers of hypoxic-ischemic encephalopathy in newborns. Front
    Neurol. 2012;3:144. Google ScholarCrossref PubMed  14.Spitzmiller RE, Phillips
    T, Meinzen-Derr J, Hoath SB. Amplitude-integrated EEG is useful in predicting
    neurodevelopmental outcome in full-term infants with hypoxic-ischemic encephalopathy:
    a meta-analysis. J Child Neurol. 2007 Sep229106978. Google ScholarCrossref PubMed  15.Nagarajan
    L, Palumbo L, Ghosh S. Neurodevelopmental outcomes in neonates with seizures:
    a numerical score of background encephalography to help prognosticate. J Child
    Neurol. 2010;25(8):961–8. Google ScholarCrossref PubMed  16.Lori S, Bertini G,
    Molesti E, Gualandi D, Gabbanini S, Bastianelli ME. The prognostic role of evoked
    potentials in neonatal hypoxic-ischemic insult. J Matern Fetal Neonatal Med. 201124Suppl
    16971. Google ScholarPubMed  17.Stewart AM, Chapman KE. Prognostication in pediatrics.
    In: Husain AM, Sinha SR, editors. Continuous EEG monitoring: principles and practiceChamSpringer
    International Publishing2017. p. 46581. Google Scholar  18.Wilkinson D. MRI and
    withdrawal of life support from newborn infants with hypoxic-ischemic encephalopathy.
    Pediatrics. 2010;126(2):e451–8. Google ScholarCrossref PubMed  19.van Laerhoven
    H, de Haan TR, Offringa M, Post B, van der Lee JH. Prognostic tests in term neonates
    with hypoxic-ischemic encephalopathy: a systematic review. Pediatrics. 2013 Jan13118898.
    Google ScholarCrossref PubMed  20.Douglas-Escobar M, Weiss MD. Hypoxic-ischemic
    encephalopathy: a review for the clinician. JAMA Pediatr. 2015 Apr1694397403.
    Google ScholarCrossref PubMed  21.Massaro AN. MRI for neurodevelopmental prognostication
    in the high-risk term infant. Semin Perinatol. 2015 Mar39215967. Google ScholarCrossref
    PubMed  22.Shankaran S, McDonald SA, Laptook AR, Hintz SR, Barnes PD, Das A. Neonatal
    magnetic resonance imaging pattern of brain injury as a biomarker of childhood
    outcomes following a trial of hypothermia for neonatal hypoxic-ischemic encephalopathy.
    J Pediatr. 2015 Nov167598793.e3. Google ScholarCrossref PubMed  23.Ouwehand S,
    Smidt LCA, Dudink J, Benders M, de Vries LS, Groenendaal F. Predictors of outcomes
    in hypoxic-ischemic encephalopathy following hypothermia: a meta-analysis. Neonatology.
    2020;117(4):411–27. Google ScholarCrossref PubMed  24.Parmentier CEJ, de Vries
    LS, Groenendaal F. Magnetic resonance imaging in (Near-)Term infants with hypoxic-ischemic
    encephalopathy. Diagnostics. 2022;12(3):645. Google ScholarCrossref PubMed  25.Leijser
    LM, Vein AA, Liauw L, Strauss T, Veen S, Wezel-Meijler G. Prediction of short-term
    neurological outcome in full-term neonates with hypoxic-ischaemic encephalopathy
    based on combined use of electroencephalogram and neuro-imaging. Neuropediatrics.
    2007;38(5):219–27. Google ScholarCrossref PubMed  26.Nanavati T, Seemaladinne
    N, Regier M, Yossuck P, Pergami P. Can we predict functional outcome in neonates
    with hypoxic ischemic encephalopathy by the combination of neuroimaging and electroencephalography.
    Pediatr Neonatol. 2015;56(5):307–16. Google ScholarCrossref PubMed  27.Thayyil
    S, Chandrasekaran M, Taylor A, Bainbridge A, Cady EB, Chong WK. Cerebral magnetic
    resonance biomarkers in neonatal encephalopathy: a meta-analysis. Pediatrics.
    2010 Feb1252e38295. Google ScholarCrossref PubMed  28.Alderliesten T, de Vries
    LS, Benders MJ, Koopman C, Groenendaal F. MR imaging and outcome of term neonates
    with perinatal asphyxia: value of diffusion-weighted MR imaging and ¹H MR spectroscopy.
    Radiology. 2011 Oct261123542. Google ScholarCrossref PubMed  29.Martinez-Biarge
    M, Diez-Sebastian J, Kapellou O, Gindner D, Allsop JM, Rutherford MA. Predicting
    motor outcome and death in term hypoxic-ischemic encephalopathy. Neurology. 2011
    Jun 147624205561. Google ScholarCrossref   30.Utsunomiya H. Diffusion MRI abnormalities
    in pediatric neurological disorders. Brain Dev. 2011;33(3):235–42. Google ScholarCrossref
    PubMed  31.Krishnan P, Shroff M. Neuroimaging in neonatal hypoxic ischemic encephalopathy.
    Indian J Pediatr. 2016;83(9):995–1002. Google ScholarCrossref PubMed  32.Bano
    S, Chaudhary V, Garga UC. Neonatal hypoxic-ischemic encephalopathy: a radiological
    review. J Pediatr Neurosci. 2017 Jan–Mar12116. Google ScholarCrossref PubMed  33.Roelants-Van
    Rijn AM, van der Grond J, de Vries LS, Groenendaal F. Value of (1)H-MRS using
    different echo times in neonates with cerebral hypoxia-ischemia. Pediatr Res.
    2001 Mar49335662. Google ScholarCrossref PubMed  34.Khong PL, Tse C, Wong IY,
    Lam BC, Cheung PT, Goh WH. Diffusion-weighted imaging and proton magnetic resonance
    spectroscopy in perinatal hypoxic-ischemic encephalopathy: association with neuromotor
    outcome at 18 months of age. J Child Neurol. 2004 Nov191187281. Google ScholarCrossref
    PubMed  35.L’Abee C, de Vries LS, van der Grond J, Groenendaal F. Early diffusion-weighted
    MRI and 1H-Magnetic Resonance Spectroscopy in asphyxiated full-term neonates.
    Biol Neonate. 2005;88(4):306–12. Google ScholarCrossref PubMed  36.Basser PJ.
    Inferring microstructural features and the physiological state of tissues from
    diffusion-weighted images. NMR Biomed. 199587–833344. Google ScholarPubMed  37.Beaulieu
    C. The basis of anisotropic water diffusion in the nervous system: a technical
    review. NMR Biomed. 2002157–843555. Google ScholarPubMed  38.Basser PJ, Pierpaoli
    C. Microstructural and physiological features of tissues elucidated by quantitative-diffusion-tensor
    MRI. J Magn Reson. 2011;213(2):560–70. Google ScholarCrossref PubMed  39.Salas
    J, Reddy N, Orru E, Carson KA, Chavez-Valdez R, Burton VJ. The role of diffusion
    tensor imaging in detecting hippocampal injury following neonatal hypoxic-ischemic
    encephalopathy. J Neuroimaging. 2019 Mar2922529. Google ScholarCrossref PubMed  40.Tusor
    N, Wusthoff C, Smee N, Merchant N, Arichi T, Allsop JM. Prediction of neurodevelopmental
    outcome after hypoxic-ischemic encephalopathy treated with hypothermia by diffusion
    tensor imaging analyzed using tract-based spatial statistics. Pediatr Res. 2012
    Jul721639. Google ScholarCrossref PubMed  41.Le Bihan D, Mangin JF, Poupon C,
    Clark CA, Pappata S, Molko N. Diffusion tensor imaging: concepts and applications.
    J Magn Reson Imaging. 2001;13(4):534–46. Google ScholarCrossref PubMed  42.Basser
    PJ, Jones DK. Diffusion-tensor MRI: theory, experimental design and data analysis:
    a technical review. NMR Biomed. 2002157–845667. Google ScholarPubMed  43.Alexander
    AL, Lee JE, Lazar M, Field AS. Diffusion tensor imaging of the brain. Neurotherapeutics.
    2007 Jul4331629. Google ScholarCrossref PubMed  44.Mori S. Chapter 5: mathematics
    of diffusion tensor imaging. In: Mori S, editor. Introduction to diffusion tensor
    imagingAmsterdamElsevier Science B.V.2007. p. 417. Google Scholar  45.Kinney HC,
    Back SA. Human oligodendroglial development: relationship to periventricular leukomalacia.
    Semin Pediatr Neurol. 1998 Sep531809. Google ScholarCrossref PubMed  46.Back SA,
    Luo NL, Borenstein NS, Levine JM, Volpe JJ, Kinney HC. Late oligodendrocyte progenitors
    coincide with the developmental window of vulnerability for human perinatal white
    matter injury. J Neurosci. 2001 Feb 15214130212. Google ScholarCrossref   47.Stiles
    J, Jernigan TL. The basics of brain development. Neuropsychol Rev. 2010 Dec20432748.
    Google ScholarCrossref PubMed  48.Dubois J, Dehaene-Lambertz G, Kulikova S, Poupon
    C, Hüppi PS, Hertz-Pannier L. The early development of brain white matter: a review
    of imaging studies in fetuses, newborns and infants. Neuroscience. 2014;276:48–71.
    Google ScholarCrossref PubMed  49.Oishi K. Chapter 11: pediatric brain atlases
    and parcellations. In: Huang H, Roberts TPL, editors. Advances in magnetic resonance
    technology and applicationsAcademic Press2021. p. 24164. Google Scholar  50.Conturo
    TE, Lori NF, Cull TS, Akbudak E, Snyder AZ, Shimony JS. Tracking neuronal fiber
    pathways in the living human brain. Proc Natl Acad Sci U S A. 1999;96(18):10422–7.
    Google ScholarCrossref PubMed  51.Mori S, Crain BJ, Chacko VP, Van Zijl PCM. Three-dimensional
    tracking of axonal projections in the brain by magnetic resonance imaging. Ann
    Neurol. 1999;45(2):265–9. Google ScholarCrossref PubMed  52.Basser PJ, Pajevic
    S, Pierpaoli C, Duda J, Aldroubi A. In vivo fiber tractography using DT-MRI data.
    Magn Reson Med. 2000;44(4):625–32. Google ScholarCrossref PubMed  53.Mukherjee
    P, McKinstry RC. Diffusion tensor imaging and tractography of human brain development.
    Neuroimaging Clin N Am. 2006;16(1):19–43. Google ScholarCrossref PubMed  54.Oishi
    K, Faria AV, Yoshida S, Chang L, Mori S. Quantitative evaluation of brain development
    using anatomical MRI and diffusion tensor imaging. Int J Dev Neurosci. 2013;31(7):512–24.
    Google ScholarCrossref PubMed  55.Smith SM, Jenkinson M, Johansen-Berg H, Rueckert
    D, Nichols TE, Mackay CE. Tract-based spatial statistics: voxelwise analysis of
    multi-subject diffusion data. Neuroimage. 2006;31(4):1487–505. Google ScholarCrossref
    PubMed  56.Bach M, Laun FB, Leemans A, Tax CMW, Biessels GJ, Stieltjes B. Methodological
    considerations on tract-based spatial statistics (TBSS). Neuroimage. 2014;100:358–69.
    Google ScholarCrossref PubMed  57.Oishi K, Mori S, Donohue PK, Ernst T, Anderson
    L, Buchthal S. Multi-contrast human neonatal brain atlas: application to normal
    neonate development analysis. Neuroimage. 2011;56(1):8–20. Google ScholarCrossref
    PubMed  58.Blesa M, Serag A, Wilkinson AG, Anblagan D, Telford EJ, Pataky R. Parcellation
    of the healthy neonatal brain into 107 regions using atlas propagation through
    intermediate time points in childhood. Front Neurosci. 2016;10:220. Google ScholarCrossref
    PubMed  59.Feng L, Li H, Oishi K, Mishra V, Song L, Peng Q. Age-specific gray
    and white matter DTI atlas for human brain at 33, 36 and 39 postmenstrual weeks.
    Neuroimage. 2019;185:685–98. Google ScholarCrossref PubMed  60.Oishi K, Chang
    L, Huang H. Baby brain atlases. Neuroimage. 2019;185:865–80. Google ScholarCrossref
    PubMed  61.Wang D, Luo Y, Mok VCT, Chu WCW, Shi L. Tractography atlas-based spatial
    statistics: statistical analysis of diffusion tensor image along fiber pathways.
    Neuroimage. 2016;125:301–10. Google ScholarCrossref PubMed  62.Chen HJ, Gao YQ,
    Che CH, Lin H, Ruan XL. Diffusion tensor imaging with tract-based spatial statistics
    reveals white matter abnormalities in patients with vascular cognitive impairment.
    Front Neuroanat. 2018;12:12. Google ScholarPubMed  63.Zhai G, Lin W, Wilber KP,
    Gerig G, Gilmore JH. Comparisons of regional white matter diffusion in healthy
    neonates and adults performed with a 3.0-T head-only MR imaging unit. Radiology.
    2003;229(3):673–81. Google ScholarCrossref PubMed  64.Takahashi E, Folkerth RD,
    Galaburda AM, Grant PE. Emerging cerebral connectivity in the human fetal brain:
    an MR tractography study. Cereb Cortex. 2012 Feb22245564. Google ScholarCrossref
    PubMed  65.Yoshida S, Oishi K, Faria AV, Mori S. Diffusion tensor imaging of normal
    brain development. Pediatr Radiol. 2013 Jan4311527. Google ScholarCrossref PubMed  66.Dubois
    J, Dehaene-Lambertz G, Perrin M, Mangin JF, Cointepas Y, Duchesnay E. Asynchrony
    of the early maturation of white matter bundles in healthy infants: quantitative
    landmarks revealed noninvasively by diffusion tensor imaging. Hum Brain Mapp.
    2008;29(1):14–27. Google ScholarCrossref PubMed  67.Hermoye L, Saint-Martin C,
    Cosnard G, Lee S-K, Kim J, Nassogne M-C. Pediatric diffusion tensor imaging: normal
    database and observation of the white matter maturation in early childhood. Neuroimage.
    2006;29(2):493–504. Google ScholarCrossref PubMed  68.Fortin JP, Parker D, Tunç
    B, Watanabe T, Elliott MA, Ruparel K. Harmonization of multi-site diffusion tensor
    imaging data. Neuroimage. 2017;161:149–70. Google ScholarCrossref PubMed  69.Tax
    CMW, Grussu F, Kaden E, Ning L, Rudrapatna U, John Evans C. Cross-scanner and
    cross-protocol diffusion MRI data harmonisation: a benchmark database and evaluation
    of algorithms. Neuroimage. 2019;195:285–99. Google ScholarCrossref PubMed  70.Ning
    L, Bonet-Carne E, Grussu F, Sepehrband F, Kaden E, Veraart J. Cross-scanner and
    cross-protocol multi-shell diffusion MRI data harmonization: algorithms and results.
    Neuroimage. 2020;221:117128. Google ScholarCrossref PubMed  71.Greco P, Nencini
    G, Piva I, Scioscia M, Volta CA, Spadaro S. Pathophysiology of hypoxic–ischemic
    encephalopathy: a review of the past and a view on the future. Acta Neurol Belg.
    2020;120(2):277–88. Google ScholarCrossref PubMed  72.Liang D, Bhatta S, Gerzanich
    V, Simard JM. Cytotoxic edema: mechanisms of pathological cell swelling. Neurosurg
    Focus. 2007 May 15225E2. Google ScholarCrossref   73.Rai V, Nath K, Saraswat VA,
    Purwar A, Rathore RKS, Gupta RK. Measurement of cytotoxic and interstitial components
    of cerebral edema in acute hepatic failure by diffusion tensor imaging. J Magn
    Reson Imaging. 2008;28(2):334–41. Google ScholarCrossref PubMed  74.Sotak CH.
    The role of diffusion tensor imaging in the evaluation of ischemic brain injury:
    a review. NMR Biomed. 2002157–85619. Google ScholarPubMed  75.Moritani T, Shrier
    DA, Numaguchi Y, Takase Y, Takahashi C, Wang HZ. Diffusion-weighted echo-planar
    MR imaging: clinical applications and pitfalls. A pictorial essay. Clin Imaging.
    2000;24(4):181–92. Google ScholarCrossref PubMed  76.Keller E, Flacke S, Urbach
    H, Schild HH. Diffusion- and perfusion-weighted magnetic resonance imaging in
    deep cerebral venous thrombosis. Stroke. 1999;30(5):1144–6. Google ScholarCrossref
    PubMed  77.Groenendaal F, Benders MJ, De Vries LS. Pre-Wallerian degeneration
    in the neonatal brain following perinatal cerebral hypoxia–ischemia demonstrated
    with MRI. Semin Perinatol. 2006;30(3):146–50. Google ScholarCrossref PubMed  78.Neil
    JJ, Inder TE. Detection of wallerian degeneration in a newborn by diffusion magnetic
    resonance imaging (MRI). J Child Neurol. 2006 Feb2121158. Google ScholarCrossref
    PubMed  79.Yu C, Zhu C, Zhang Y, Chen H, Qin W, Wang M. A longitudinal diffusion
    tensor imaging study on Wallerian degeneration of corticospinal tract after motor
    pathway stroke. Neuroimage. 2009;47(2):451–8. Google ScholarCrossref PubMed  80.Qin
    W, Zhang M, Piao Y, Guo D, Zhu Z, Tian X. Wallerian degeneration in central nervous
    system: dynamic associations between diffusion indices and their underlying pathology.
    PLoS One. 2012;7(7):e41441. Google ScholarCrossref PubMed  81.Venkatasubramanian
    C, Kleinman JT, Fischbein NJ, Olivot JM, Gean AD, Eyngorn I. Natural history and
    prognostic value of corticospinal tract Wallerian degeneration in intracerebral
    hemorrhage. J Am Heart Assoc. 2013 Aug 224e000090. Google ScholarCrossref   82.Zhang
    F, Liu C, Qian L, Hou H, Guo Z. Diffusion tensor imaging of white matter injury
    caused by prematurity-induced hypoxic-ischemic brain damage. Med Sci Monit. 2016
    Jun 2422216774. Google ScholarCrossref   83.Wu D, Martin LJ, Northington FJ, Zhang
    J. Oscillating gradient diffusion MRI reveals unique microstructural information
    in normal and hypoxia-ischemia injured mouse brains. Magn Reson Med. 2014 Nov725136674.
    Google ScholarCrossref PubMed  84.McKinstry RC, Miller JH, Snyder AZ, Mathur A,
    Schefft GL, Almli CR. A prospective, longitudinal diffusion tensor imaging study
    of brain injury in newborns. Neurology. 2002;59(6):824–33. Google ScholarCrossref
    PubMed  85.Rutherford M, Counsell S, Allsop J, Boardman J, Kapellou O, Larkman
    D. Diffusion-weighted magnetic resonance imaging in term perinatal brain injury:
    a comparison with site of lesion and time from birth. Pediatrics. 2004 Oct1144100414.
    Google ScholarCrossref PubMed  86.Ward P, Counsell S, Allsop J, Cowan F, Shen
    Y, Edwards D. Reduced fractional anisotropy on diffusion tensor magnetic resonance
    imaging after hypoxic-ischemic encephalopathy. Pediatrics. 2006;117(4):e619–30.
    Google ScholarCrossref PubMed  87.Malik GK, Trivedi R, Gupta RK, Hasan KM, Hasan
    M, Gupta A. Serial quantitative diffusion tensor MRI of the term neonates with
    hypoxic-ischemic encephalopathy (HIE). Neuropediatrics. 2006 Dec37633743. Google
    ScholarCrossref PubMed  88.Lemmon ME, Wagner MW, Bosemani T, Carson KA, Northington
    FJ, Huisman T. Diffusion tensor imaging detects occult cerebellar injury in severe
    neonatal hypoxic-ischemic encephalopathy. Dev Neurosci. 2017391–420714. Google
    ScholarPubMed  89.Kushwah S, Kumar A, Verma A, Basu S, Kumar A. Comparison of
    fractional anisotropy and apparent diffusion coefficient among hypoxic ischemic
    encephalopathy stages 1, 2, and 3 and with nonasphyxiated newborns in 18 areas
    of brain. Indian J Radiol Imaging. 2017 Oct–Dec27444756. Google ScholarPubMed  90.Gao
    J, Li X, Li Y, Zeng L, Jin C, Sun Q. Differentiating T2 hyperintensity in neonatal
    white matter by two-compartment model of diffusional kurtosis imaging. Sci Rep.
    2016 Apr 14624473. Google ScholarCrossref   91.Porter EJ, Counsell SJ, Edwards
    AD, Allsop J, Azzopardi D. Tract-based spatial statistics of magnetic resonance
    images to assess disease and treatment effects in perinatal asphyxial encephalopathy.
    Pediatr Res. 2010;68(3):205–9. Google ScholarCrossref PubMed  92.Seo Y, Kim GT,
    Choi JW. Early detection of neonatal hypoxic-ischemic white matter injury: an
    MR diffusion tensor imaging study. Neuroreport. 2017 Sep 6281384555. Google ScholarCrossref   93.Brissaud
    O, Amirault M, Villega F, Periot O, Chateil JF, Allard M. Efficiency of fractional
    anisotropy and apparent diffusion coefficient on diffusion tensor imaging in prognosis
    of neonates with hypoxic-ischemic encephalopathy: a methodologic prospective pilot
    study. AJNR Am J Neuroradiol. 2010;31(2):282–7. Google ScholarCrossref PubMed  94.Cao
    Z, Lin H, Gao F, Shen X, Zhang H, Zhang J. Microstructural alterations in projection
    and association fibers in neonatal hypoxia–ischemia. J Magn Reson Imaging. 2023;57(4):1131–42.
    Google ScholarCrossref PubMed  95.Ahearne CE, Boylan GB, Murray DM. Short and
    long term prognosis in perinatal asphyxia: an update. World J Clin Pediatr. 2016
    Feb 8516774. Google ScholarCrossref   96.Oishi K, Faria A, Jiang H, Li X, Akhter
    K, Zhang J. Atlas-based whole brain white matter analysis using large deformation
    diffeomorphic metric mapping: application to normal elderly and Alzheimer’s disease
    participants. Neuroimage. 2009 Jun46248699. Google ScholarCrossref PubMed  97.Bao
    XL, Yu RJ, Li ZS, Zhang BL. Twenty-item behavioral neurological assessment for
    normal newborns in 12 cities of China. Chin Med J. 1991 Sep10497426. Google ScholarPubMed  98.Li
    HX, Feng X, Wang Q, Dong X, Yu M, Tu WJ. Diffusion tensor imaging assesses white
    matter injury in neonates with hypoxic-ischemic encephalopathy. Neural Regen Res.
    2017 Apr1246039. Google ScholarPubMed  99.Onda K, Catenaccio E, Chotiyanonta J,
    Chavez-Valdez R, Meoded A, Soares BP. Development of a composite diffusion tensor
    imaging score correlating with short-term neurological status in neonatal hypoxic–ischemic
    encephalopathy. Front Neurosci. 2022;16:931360. Google ScholarCrossref PubMed  100.de
    Vries LS, Jongmans MJ. Long-term outcome after neonatal hypoxic-ischaemic encephalopathy.
    Arch Dis Child Fetal Neonatal Ed. 2010953F2204. Google ScholarCrossref PubMed  101.Massaro
    AN, Evangelou I, Fatemi A, Vezina G, McCarter R, Glass P. White matter tract integrity
    and developmental outcome in newborn infants with hypoxic-ischemic encephalopathy
    treated with hypothermia. Dev Med Child Neurol. 2015 May5754418. Google ScholarCrossref
    PubMed  102.Ancora G, Testa C, Grandi S, Tonon C, Sbravati F, Savini S. Prognostic
    value of brain proton MR spectroscopy and diffusion tensor imaging in newborns
    with hypoxic-ischemic encephalopathy treated by brain cooling. Neuroradiology.
    2013 Aug558101725. Google ScholarCrossref PubMed  103.ElBeheiry AA, Elgamal MA,
    Ettaby AN, Omar TE, Badeib AO. Can diffusion tensor imaging predict cerebral palsy
    in term neonates with hypoxic ischemic encephalopathy. Egypt J Radiol Nucl Med.
    2019;50(1):66. Google ScholarCrossref   104.Jeong JW, Lee MH, Fernandes N, Deol
    S, Mody S, Arslanturk S. Neonatal encephalopathy prediction of poor outcome with
    diffusion-weighted imaging connectome and fixel-based analysis. Pediatr Res. 2022;91(6):1505–15.
    Google ScholarCrossref PubMed  105.Tkach JA, Merhar SL, Kline-Fath BM, Pratt RG,
    Loew WM, Daniels BR. MRI in the neonatal ICU: initial experience using a small-footprint
    1.5-T system. AJR Am J Roentgenol. 2014 Jan2021W95w105. Google ScholarCrossref
    PubMed  106.Hahn AD, Higano NS, Walkup LL, Thomen RP, Cao X, Merhar SL. Pulmonary
    MRI of neonates in the intensive care unit using 3D ultrashort echo time and a
    small footprint MRI system. J Magn Reson Imaging. 2017 Feb45246371. Google ScholarCrossref
    PubMed  107.Winter JD, Thompson RT, Gelman N. Efficacy of motion artifact reduction
    in neonatal DW segmented EPI at 3 T using phase correction by numerical optimization
    and segment data swapping. Magn Reson Imaging. 2007;25(9):1283–91. Google ScholarCrossref
    PubMed  108.Zhou Z, Liu W, Cui J, Wang X, Arias D, Wen Y. Automated artifact detection
    and removal for improved tensor estimation in motion-corrupted DTI data sets using
    the combination of local binary patterns and 2D partial least squares. Magn Reson
    Imaging. 2011;29(2):230–42. Google ScholarCrossref PubMed  109.Hughes EJ, Winchman
    T, Padormo F, Teixeira R, Wurie J, Sharma M. A dedicated neonatal brain imaging
    system. Magn Reson Med. 2017;78(2):794–804. Google ScholarCrossref PubMed  110.Chapter
    8: moving beyond DTI–high angular resolution diffusion imaging (HARDI). In: Mori
    S, Tournier JD, editors. Introduction to diffusion tensor imaging2nd edSan DiegoAcademic
    Press2014. p. 6578. Google Scholar  111.Descoteaux MHigh angular resolution diffusion
    imaging (HARDI)Wiley Encyclopedia of Electrical and Electronics Engineering. p.
    125. 112.Tuch DS, Reese TG, Wiegell MR, Makris N, Belliveau JW, Wedeen VJ. High
    angular resolution diffusion imaging reveals intravoxel white matter fiber heterogeneity.
    Magn Reson Med. 2002;48(4):577–82. Google ScholarCrossref PubMed  113.Qiu A, Mori
    S, Miller MI. Diffusion tensor imaging for understanding brain development in
    early life. Annu Rev Psychol. 2015;66(1):853–76. Google ScholarCrossref PubMed  ©
    2023 The Author(s). Published by S. Karger AG, Basel Open Access License / Drug
    Dosage / Disclaimer This article is licensed under the Creative Commons Attribution
    4.0 International License (CC BY). Usage, derivative works and distribution are
    permitted provided that proper credit is given to the author and the original
    publisher. Drug Dosage: The authors and the publisher have exerted every effort
    to ensure that drug selection and dosage set forth in this text are in accord
    with current recommendations and practice at the time of publication. However,
    in view of ongoing research, changes in government regulations, and the constant
    flow of information relating to drug therapy and drug reactions, the reader is
    urged to check the package insert for each drug for any changes in indications
    and dosage and for added warnings and precautions. This is particularly important
    when the recommended agent is a new and/or infrequently employed drug. Disclaimer:
    The statements, opinions and data contained in this publication are solely those
    of the individual authors and contributors and not of the publishers and the editor(s).
    The appearance of advertisements or/and product references in the publication
    is not a warranty, endorsement, or approval of the products or services advertised
    or of their effectiveness, quality or safety. The publisher and the editor(s)
    disclaim responsibility for any injury to persons or property resulting from any
    ideas, methods, instructions or products referred to in the content or advertisements.
    This work is licensed under a Creative Commons Attribution 4.0 International License
    . Sign in Don''t already have an account? Register Individual Login LOGIN TO MY
    KARGER Institutional Login Access via Shibboleth and OpenAthens Access via username
    and password View Metrics Email Alerts Online First Alert Latest Issue Alert Citing
    Articles Via Google Scholar LATEST MOST READ MOST CITED Upstream stimulating factor
    2 aggravates neuropathic pain induced in spinal nerve ligation-induced mice via
    regulating SNHG5/miR-181b-5p Septotemporal Variation of Information Processing
    in the Hippocampus of Fmr1 KO Rat Sex-Specific Behavioural Deficits in Adulthood
    following Acute Activation of the GABAA Receptor in the Neonatal Mouse Central
    Autonomic Network and Heart Rate Variability in Premature Neonates Pain/Stress,
    Mitochondrial Dysfunction, and Neurodevelopment in Preterm Infants Suggested Reading
    Diffusion Tensor Imaging Detects Occult Cerebellar Injury in Severe Neonatal Hypoxic-Ischemic
    Encephalopathy Dev Neurosci (January,2017) Effect of Therapeutic Hypothermia in
    Neonates with Hypoxic-Ischemic Encephalopathy on Platelet Function Neonatology
    (September,2011) Effect of Phenobarbitone on Amplitude-Integrated Electroencephalography
    in Neonates with Hypoxic-Ischemic Encephalopathy during Hypothermia Neonatology
    (January,2021) Temporal Trends in the Severity and Mortality of Neonatal Hypoxic-Ischemic
    Encephalopathy in the Era of Hypothermia Neonatology (September,2021) Online ISSN
    1421-9859 Print ISSN 0378-5866 INFORMATION Contact & Support Information & Downloads
    Rights & Permissions Terms & Conditions Catalogue & Pricing Policies & Information
    ABOUT US Company People & Organization Newsroom Careers Stay Up-to-Date Regional
    Offices Community Voice SERVICES FOR Researchers Authors Reviewers Healthcare
    Professionals Patients & Supporters Librarians Health Sciences Industry Medical
    Societies Agents & Booksellers KARGER INTERNATIONAL S. Karger AG P.O Box, CH-4009
    Basel (Switzerland) Allschwilerstrasse 10, CH-4055 Basel Tel: +41 61 306 11 11
    Fax: +41 61 306 12 34 Email: service@karger.com Privacy Policy Terms of Use Imprint
    Cookies © 2024 S. Karger AG, Basel"'
  inline_citation: '>'
  journal: Developmental Neuroscience
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Quantification of Diffusion Magnetic Resonance Imaging for Prognostic Prediction
    of Neonatal Hypoxic-Ischemic Encephalopathy
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gao S.
  - Hu Z.
  - Wang H.
  - Zhang X.
  - Zhang Y.
  citation_count: '0'
  description: Food security is a top priority in national governance. Since 1949,
    high-standard farmland construction, agricultural mechanization development, and
    agricultural technology promotion have all contributed to the grain production.
    To ensure grain security, China has drawn a “red line” of 1.8 billion mu (about
    120 million hectares) as the official minimum of arable land. At the same time,
    increasing the investment of capital goods such as fertilizer and pesticides can
    no longer produce more food. Due to the extensive farming method in the past,
    the continuous increase in total grain output becomes difficult in the future.
    With the rapid development of advanced technologies such as informatization, intelligence,
    Internet of Things, big data and artificial intelligence, fine management of agricultural
    production can be achieved. Through the integration of digital economy and traditional
    agricultural industries, developing smart agriculture will provide possibilities
    for increasing food production. Focusing on the three stages of grain production
    (including pre-, during- and after-production), this study puts forward the Nine-Step
    Approach of smart agriculture, namely two refinements, three changes, three reductions,
    and one use. For each step, the connotation, the existing technical bottleneck,
    and the potential of future improvement are discussed. In addition, suggestions
    for further development of smart agriculture are made from four perspectives,
    namely, data collection, data standardization, data applications, and data security.
  doi: 10.16418/j.issn.1000-3045.20230811003
  full_citation: '>'
  full_text: '>

    "服务器错误 404 - 找不到文件或目录。 您要查找的资源可能已被删除，已更改名称或者暂时不可用。"'
  inline_citation: '>'
  journal: Bulletin of Chinese Academy of Sciences
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Nine-Step Approach of smart agricultural helps grain production reduce costs,
    increase yield and efficiency
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu Z.
  - Zhang X.
  - Wang J.
  - Shen L.
  - Tang E.
  citation_count: '0'
  description: The convergence of China’s digital economy and green finance holds
    great significance for fostering a sustainable and high-quality developmental
    path. However, existing studies have not explored the coupling coordination development
    between these two crucial subsystems. To bridge this gap, this paper employs a
    modified coupling coordination degree (CCD) model to assess and affirm the coupling
    coordination degree between the digital economy and green finance across 30 provinces
    in China from 2015–2021. Based on degree results, provinces are classified into
    three clusters by using K-means and hierarchical clustering algorithm. Our findings
    unveil that the current level of coupling coordination development in China is
    at a primary coordination stage. Although regional disparities significantly exist,
    the overall level of coordination remains steadily increasing, with the eastern
    region outperforming the western region. Additionally, we determine that the COVID-19
    pandemic’s disruption on the coupling coordination development of these systems
    has been limited. This research sheds light on the evolution of coupling systems
    and offers practical recommendations for strengthening the coordinated development
    of the digital economy and green finance.
  doi: 10.1371/journal.pone.0291936
  full_citation: '>'
  full_text: '>

    "PUBLISH ABOUT BROWSE advanced search RESEARCH ARTICLE Evaluation of coupling
    coordination development between digital economy and green finance: Evidence from
    30 provinces in China Zebin Liu, Xiaoheng Zhang, Jingjing Wang, Lei Shen, Enlin
    Tang Abstract The convergence of China’s digital economy and green finance holds
    great significance for fostering a sustainable and high-quality developmental
    path. However, existing studies have not explored the coupling coordination development
    between these two crucial subsystems. To bridge this gap, this paper employs a
    modified coupling coordination degree (CCD) model to assess and affirm the coupling
    coordination degree between the digital economy and green finance across 30 provinces
    in China from 2015–2021. Based on degree results, provinces are classified into
    three clusters by using K-means and hierarchical clustering algorithm. Our findings
    unveil that the current level of coupling coordination development in China is
    at a primary coordination stage. Although regional disparities significantly exist,
    the overall level of coordination remains steadily increasing, with the eastern
    region outperforming the western region. Additionally, we determine that the COVID-19
    pandemic’s disruption on the coupling coordination development of these systems
    has been limited. This research sheds light on the evolution of coupling systems
    and offers practical recommendations for strengthening the coordinated development
    of the digital economy and green finance. Citation: Liu Z, Zhang X, Wang J, Shen
    L, Tang E (2023) Evaluation of coupling coordination development between digital
    economy and green finance: Evidence from 30 provinces in China. PLoS ONE 18(10):
    e0291936. doi:10.1371/journal.pone.0291936 Editor: Rita Yi Man Li, Hong Kong Shue
    Yan University, HONG KONG Received: April 22, 2023; Accepted: September 10, 2023;
    Published: October 13, 2023 Copyright: © 2023 Liu et al. This is an open access
    article distributed under the terms of the Creative Commons Attribution License,
    which permits unrestricted use, distribution, and reproduction in any medium,
    provided the original author and source are credited. Data Availability: All relevant
    data are within the manuscript and its Supporting Information files. In addtion,
    other data is from CSMAR database(http://www.csmar.com/channels/31.html) Funding:
    This study was financially supported by Department of Education of Anhui Province
    in the form of a grants (KJ2021A0966; 2022AH051585) received by J.W, a grant (2023AH051512)
    received by ZL, and a grant (2023AH051508) received by LS. This study was also
    financially supported by Anhui University of Science & Technology in the form
    of a grant (QNSK202001) received by XZ. This study was also financially supported
    by Mining Enterprise Safety Management of Humanities and Social Science Key Research
    Base in Anhui Province in the form of a grant (MF2022006) received by XZ. The
    funders had no role in study design, data collection and analysis, decision to
    publish, or preparation of the manuscript. Competing interests: The authors have
    declared that no competing interests exist. 1 Introduction 2 Literature review
    3 Theoretical analysis of coupled system 4 Indicators system and research methods                                        5
    Empirical results and discussion 6 Suggestions 7 Conclusions Supporting information
    References View Figures (14) View Reader Comments View About the Authors View
    Metrics View Media Coverage DOWNLOAD ARTICLE (PDF) DOWNLOAD CITATION EMAIL THIS
    ARTICLE PLOS Journals PLOS Blogs Back to Top Back to Top About Us Full Site Feedback
    Contact Privacy Policy Terms of Use Media Inquiries PLOS is a nonprofit 501(c)(3)
    corporation, #C2354500, based in San Francisco, California, US Cookie Preference
    Center Our website uses different types of cookies. Optional cookies will only
    be set with your consent and you may withdraw this consent at any time. Below
    you can learn more about the types of cookies PLOS uses and register your cookie
    preferences. Accept All Cookies Customize Your Cookie Preference + Strictly Necessary
    Always On + Functional Off + Performance and Analytics Off + Marketing Off Save
    Selected Preferences and Close For more information about the cookies and other
    technologies used by us, please read our Cookie Policy."'
  inline_citation: '>'
  journal: PLoS ONE
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Evaluation of coupling coordination development between digital economy
    and green finance: Evidence from 30 provinces in China'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gobezie T.B.
  - Biswas A.
  citation_count: '4'
  description: In the era of digital agriculture, Precision agriculture (PA) data
    sources are diverse in terms of the range of technology options and the type of
    data they generate. Government institutions, scientists, and the private sectors
    generate much of the PA data at the innovation, validation, and dissemination
    phases. At scale-up phases, farmers also generate tremendous amounts of data that
    might have privacy and ownership concerns. There remains the possibility of a
    better way to integrate PA data continent-wide in Africa. Data privacy and ownership
    issues must be addressed while still maintaining the integration of PA data at
    scale. The objective of this paper is to review the major challenges of PA data
    harmonization in Africa and discuss the existing opportunities in relation to
    technological advancements in PA data applications to address data sharing without
    compromising data privacy, ownership, and stewardship. Finally, a new PA data
    sharing and reward model—‘PrecisioNexion’ is proposed to rationalize data network
    systems by establishing a robust and self-sustaining business model. The model
    uses AI and blockchain technology to track and stamp PA data using unique dataset_IDs
    or PrecisionPrint (like a fingerprint), determining credit amounts using ‘pVouchers’
    (like eVouchers) and distributing credits between PA data owners or ‘PrecisionProprietor’,
    data clients or ‘PrecisionClient’ and funders or ‘PrecisionPatron’. The proposed
    system provides a foundation for win–win–win PA data sharing and self-sustaining
    business models for data owners, technology solutions providers and funders, while
    ensuring a strong partnership between farmers’ cooperatives, private sector, scientists,
    governments, and financial institutions, and countries.
  doi: 10.1007/s11119-022-09928-w
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Precision Agriculture Article The
    need for streamlining precision agriculture data in Africa Brief Communication
    Published: 14 June 2022 Volume 24, pages 375–383, (2023) Cite this article Download
    PDF Access provided by University of Nebraska-Lincoln Precision Agriculture Aims
    and scope Submit manuscript Tegbaru B. Gobezie & Asim Biswas   788 Accesses 2
    Citations 4 Altmetric Explore all metrics Abstract In the era of digital agriculture,
    Precision agriculture (PA) data sources are diverse in terms of the range of technology
    options and the type of data they generate. Government institutions, scientists,
    and the private sectors generate much of the PA data at the innovation, validation,
    and dissemination phases. At scale-up phases, farmers also generate tremendous
    amounts of data that might have privacy and ownership concerns. There remains
    the possibility of a better way to integrate PA data continent-wide in Africa.
    Data privacy and ownership issues must be addressed while still maintaining the
    integration of PA data at scale. The objective of this paper is to review the
    major challenges of PA data harmonization in Africa and discuss the existing opportunities
    in relation to technological advancements in PA data applications to address data
    sharing without compromising data privacy, ownership, and stewardship. Finally,
    a new PA data sharing and reward model—‘PrecisioNexion’ is proposed to rationalize
    data network systems by establishing a robust and self-sustaining business model.
    The model uses AI and blockchain technology to track and stamp PA data using unique
    dataset_IDs or PrecisionPrint (like a fingerprint), determining credit amounts
    using ‘pVouchers’ (like eVouchers) and distributing credits between PA data owners
    or ‘PrecisionProprietor’, data clients or ‘PrecisionClient’ and funders or ‘PrecisionPatron’.
    The proposed system provides a foundation for win–win–win PA data sharing and
    self-sustaining business models for data owners, technology solutions providers
    and funders, while ensuring a strong partnership between farmers’ cooperatives,
    private sector, scientists, governments, and financial institutions, and countries.
    Introduction Agriculture in Africa is characterized by diverse farming systems
    ranging from a rain fed single crop-based mixed system to irrigated expanses of
    land. About two-thirds of the smallholder farmers in sub-Saharan Africa reside
    on five different farming systems: maize-mixed, highland-perennial, cereal-root
    crop, agropastoral and highland-mixed farming systems (Dennis et al., 2012). These
    systems have been reported as the least mechanized across borders as compared
    to other parts of the world, for instance, Asia (Sims et al., 2016). The small-scale
    farmers mainly rely on fragmented lands with an average size of less than half
    a hectare. In countries like Ethiopia, the second most populous in Africa, diverse
    crop types are grown on several neighboring parcels of land owned by different
    smallholder farmers, and the average arable land holding per person was 0.15 hectare
    in 2018 (World Bank, 2018). Recent reports, however, have shown that there are
    emerging demands for agricultural mechanization in Africa for the role it plays
    in improving agricultural production systems (Daum & Birner, 2020; IFPRI, 2017).
    The mechanization options encompass diverse technologies being tested and adopted
    for different uses including precision agriculture (PA) in clusters of farmlands.
    In PA something that works well for a given farm might not be suitable for an
    adjacent farm. This implicitly indicates that PA applications for the very fragmented
    pieces of lands of African smallholder farmers, which are mainly characterized
    by high variabilities between and within farms, require more tailored PA solutions.
    Therefore, the use of PA technologies is growing at an ever-increasing rate, and
    they are diverse. Some are used to improve water use efficiency and reduce input
    costs such as water-efficient and climate smart deciduous and fruit farming in
    south-Africa, Mozamique, Tanzania and Zimbabwe (Ncube et al., 2018). Other examples
    include: (1) sensor-based computer visioning assisted small-scale PA for on-site
    fertilizer recommendations to smallholder farmers (Oliveira-JR et al., 2020) (2)
    Vital (e.g., weather forecast and pest infestation) agricultural advice and information
    services (FAO, 2020) (3) iSDAsoil, which is an immerging high-resolution information
    source of biophysical and agronomic advisories (Hengl et al., 2021) and (4) use
    of mechanized planters in PA to implement micro-dosing of fertilizer and manure
    in Semi-Arid parts of Western Africa (Aune et al., 2017). According to Wolfert
    et al. (2017), data associated with PA technologies are generally categorized
    in three major groups. The first group is machine generated PA data sources that
    include unmanned aerial vehicles (UAVs), remote sensing and proximal sensing like
    drone imaging and sensors pulled by tractors. The second group is a less structured
    type of human sourced data that are mainly past records of human experiences included
    in books, reports, arts, audio, and videos, mostly digitized. While the third
    category is process mediated and very structured data with associated metadata
    (data about a data) that include the farm management history data such as inputs,
    i.e., fertilizers, seeds, pesticide use. The PA data generation process itself
    involves several stakeholders, and the role of the stakeholders can be seen at
    different phases; at the innovation/research and validation stage, the role of
    research institutions and private sectors are super critical. While at pre-scale-up
    or demonstration phases, farmers’ organizations and government institutions play
    a key role. At the wider application phase (scale-up), the major actors are farmers
    since they produce tremendous amounts of data. One good example is the use of
    drone and artificial intelligence (AI) technologies recently adopted by Ethiopia
    for wheat yield estimation; the actors in this process were the Federal Ministry
    of Agriculture (MoA), Ethiopian Institute of Agricultural Research (EIAR), and
    the Ethiopian Agricultural Transformation Agency (ATA) from the government side,
    and Technical Centre for Agriculture and Rural Cooperation (CTA) from the multilateral
    institution wing (Technical Centre for Agriculture and Rural Cooperation (CTA),
    2019). It is evident that different actors are playing a critical role in the
    testing and application of PA in African agriculture. The diversity of PA data
    in terms of volume and variety can be considered as an important driver of big
    data in the era of digital agriculture in the case of the high success rate of
    the adoptions of PA technology. In such a scenario, a shortage of data might not
    be a problem in the future, but sustainable way of integrating PA data from different
    sources continent-wide in Africa remains the key issue. A recent report by the
    Food and Agriculture Organization of the United Nations (FAO) and the International
    Telecommunication Union (ITU), suggested that there is a need for creation of
    scalable digital public goods in the realm of digital agriculture solutions in
    sub-Saharan Africa (FAO & ITU, 2022). As such, the drive to integrate continent-wide
    PA data would instigate the urgent need to compatibly address advances in big
    data analytics in PA in line with other big-data centered initiatives such as
    digital agronomy (Mehrabi et al., 2018), and eventually to curb the challenges
    on agricultural policy recommendations generated based on none ground-truthed
    interpretations of realities (Christiaensen, 2017). The advances in big-data analytics
    have created an enormous opportunity to unleash the potential of aggregated data
    to formulate tailored solutions catered towards addressing problems in diverse
    agroecologies that would otherwise be impossible through traditional and fragmented
    approaches. Various research results have shown that different data captured from
    a variety of sources can be harmonized using data-fusion techniques for application
    of PA (Bendre et al., 2015; Ji et al., 2017, 2019; Krishnamurthi et al., 2020;
    Xu et al., 2019). State-of-the-art data-fusion and other methods such as automatic
    harmonization of data might address the discrepancies in the technical layers
    in the data value chain (Leroux et al., 2019). However, data governance piece
    (ownership, privacy, and stewardship) remains unanswered since there are mistrusts
    on the use of data and the topic crosses boarders (Fraser, 2019; Shannon et al.,
    2020). This implies that it does not matter if the economic status of countries
    (developed and developing) is not on a par. Data could be considered as a merchantable
    good but key issue related to data marketing in PA are ownership, privacy, and
    lack of a sustainable business model (Fraser, 2019; Pierce et al., 2019). Therefore,
    the objective of this paper is to review the major challenges of PA data harmonization
    in Africa and the existing opportunities in relation to the technological advancements
    in PA data applications, and to propose a new PA data sharing model to streamline
    data network systems without compromising data privacy, ownership, and stewardship.
    The scope of this paper is to investigate the entire PA data ecosystem in the
    lenses of agronomy and environmental science applications. As a result of very
    scares resources on the challenges and opportunities of PA in the Africa, different
    institutional reports, and related publications on the other parts of the world
    are used to synthesize the PA challenges and opportunities in its current state
    in Africa. Challenges in the PA data ecosystem Treating data as a marketable good
    vis-à-vis public good is arguable in the digital era when data is seen as the
    ‘new green energy’ in the twenty-first century (Fraser, 2019; Taylor, 2016). To
    have a clear picture of the challenges of PA data, it is worth considering the
    different segments of an entire data value chain. In the best-case scenario, PA
    data passes through different stages before reaching the final data marketing
    level. Mostly data capture is the initial stage of the entire process of the value
    chain and the intermediary stages include data storage, data transfer, data transformation
    and data analytics (Wolfert et al., 2017). Overall, these different segments in
    the data value chain exhibit distinct challenges and may in fact be independent.
    One of the key questions in PA is related to the technical realm, especially at
    the data capture stage, the number of data points (samples) collected using sensors
    are denser and bigger than observations generated using conventional acquisition
    methods for a given farmland, such as agronomic field observations (Bullock et
    al., 2019). The massive data acquired using sensors for a given farm is believed
    to improve the representativeness to drive better results as compared to historical
    decision making in farm management. A study conducted in Brazil in 2018 reported
    similar outputs for both sensor-based approaches and high-density representative
    soil samples; they compared apparent electrical conductivity (ECa) generated by
    a sensor pulled by a tractor on a field and topsoil collected from gridded points
    to formulate lime recommendation rates (Sanches et al., 2018). This has a direct
    implication on the volume of data required for some PA solutions and in turn,
    the investment needed for testing the technologies and the ultimate adoption rate
    of the technologies. The other key challenge is the governance of diverse PA data,
    and to determine what belongs to numerous owners. Dealing with the governance
    of data generated by an owner is entirely different from data generated from numerous
    owners or multiple owners, such as farmers cooperatives. One good example of PA
    data ecosystem is the application of free and open software system (FOSS) throughout
    the data value chain for the case of vineyard management, but all data are not
    open (Belcore et al., 2021). Obviously, there is no one-size-fits-all approach
    in terms of harmonizing PA technology applications, but there remains a question
    of providing a better way of integrating PA data, especially continent-wide in
    Africa. The data governance issue becomes more complicated at the later stage
    in the value chain (Wolfert et al., 2017). This is mainly linked with privacy
    and ownership concerns that are in turn associated with lack of viable business
    models that motivate data holders to collect, save and share their data. This
    systemic issue calls for innovative approaches that motivate data holders to share
    their data, and at the same time incentivize them for the data they shared. Tacitly,
    incentivizing data owners would serve as the common denominator for the data governance
    landscape, even if technological advancements answer the technical need to integrate
    heterogeneous datasets. The opportunities In a virtuous cycle proposed by Pierce
    et al. (2019), people who generate scientific data can be linked to the data they
    generated via data ID like the digital object identifier for published papers
    which provides a reward for sharing data. This implies that the more cited data
    means it is more valued data due to its increased reuse. The increase in data
    reuse brings rewards and promotions to the scientist or team of scientists who
    published and opened their data for wider application. This model is best suited
    for the scientific world, but it lacks intactness in embracing farmers organizations
    and commercial farmers who are generating PA data every day. For the data generated
    by the farmers organizations and largescale farmers to fit into this scientific
    model, there should be a system that embraces data governance in the PA world.
    Therefore, integration of data at the later stage of the data value chain requires
    a new win–win model, especially at the marketing stage to reward/ incentivize
    these data contributors. To address this multifaceted challenge, different technologies
    like blockchain can be used to stamp the data on an artificial intelligence superimposed
    system to track the movement of the data (Padarian et al., 2019). From a technological
    advancement point of view, the opportunities to advance PA in Africa are enormous
    (Jellason et al., 2021; Nyaga et al., 2021). Studies showed that most of the applications
    of PA in Africa revolved around the use for soil nutrient and water management,
    and yield monitoring, including the implementation of global navigation satellite
    system (Lowenberg-DeBoer & Erickson, 2019; Ncube et al., 2018). One good example
    of an advanced PA application is the use of sensor-based computer visioning assisted
    PA that involved plugging a senor mounted on farm power machineries for on-site
    fertilizer recommendations in real-time decision making (Oliveira-JR et al., 2020).
    Sensor data fusion is one of the techniques being used for the harmonization of
    data captured using different sensors in PA. These techniques are more advantageous
    than the use of individual sensors (Bendre et al., 2015; Ji et al., 2017; Xu et
    al., 2019). It is also worth noting that data-fusion techniques are used at the
    data-capture stage, but it is not always straight forward to integrate data obtained
    from heterogeneous acquisition conditions or instruments at a later stage in the
    data value chain (Leroux et al., 2019). Therefore, increasing the availability
    of PA data expands the power of innovation for data integration and harmonization.
    Initiatives such as agricultural commercialization cluster (ACC) leverages applications
    of PA on a cluster of farms which otherwise could have been fragmented and remain
    a challenge for smallholder farmers to adopt the new technologies (Joffre et al.,
    2019; Louhichi et al., 2019). The other enablers to promote PA in Africa include:
    1. Applications of handheld proximal sensors such as near infrared (NIR) and portable
    x-ray florescence (pXRF) and the potential for automated farm input management
    systems (Piikki et al., 2016). 2. Cloud computing and freely/ cheaply available
    high-resolution imageries from the google earth engine (GEE), and applications
    of artificial intelligence (AI) to produce high-resolution information (weather,
    soil, etc.) is readily available to the end-user (Gorelick et al., 2017). 3. The
    curation of continental level soil databases and the creation of high-resolution
    information on biophysical and agronomic advisories (Hengl et al., 2021). The
    outlook Conceptualizing PA data provides a situation analogous to producing an
    agricultural commodity for export that meets certain standards, e.g., like a specialty
    coffee that has well defined traceability in the entire supply chain, and this
    could help streamline a winning business model for data sharing. It is like treating
    PA data as ‘specialty data’ with traceability whereby the quality is intact in
    the new framework of PA data sharing and rewarding model ‘PrecisioNexion’ (Fig.
    1). This proposed model for the application of PA data synchronization, connects
    different players in the PA data landscape as a self-sustaining business model
    for all member countries in Africa that generate data. Fig. 1 Proposed ‘PrecisioNexion’
    model: a tripartite agreement between ‘PrecisionPatron’, ‘PrecisionClient’, and
    ‘PrecisionPropriotor’ eases data sharing supported by AI and Blockchain technologies
    to power open data and open science after data completes its cycle in this rewarding
    framework Full size image As suggested by Pierce et al. (2019), data will be tagged
    using unique identifiers that can be designated as ‘PrecisionPrint’ (like a figure
    print) for the case of PA data generated by the PA technology users. The origin
    and owner of such data could be commercial farmers or farmers organizations—‘PrecisionProprietor’,
    does not have to be limited to a single source as AI and blockchain technology
    could be used to link the ‘PrecisionPrint’ and ‘PrecisionProprietor’ to track
    and stamp the data. Superimposing blockchain technology on E-voucher system is
    a feasible technique to monitor and track valuable assets flow in a secure way
    (Hsu et al., 2020). As shown in Fig. 1, PrecisionPatron, which represents funders
    such as giant input manufacturing companies, bi-lateral and multilateral organizations
    and philanthropists could stimulate the entire PA technology applications. The
    funders motivation arises from achieving their specific project objectives and
    interests in one or another way. PrecisionPatron mainly provides funding to the
    data client or the ‘PrecisionClient’ that could be (1) PA hardware and software
    developers using their analytical tools for better decision making and/or (2)
    scientists that condense tools, technologies and practices and evaluate effectiveness.
    There could be cases where the PrecisionPatron channels funds to countries or
    umbrella institutions such as farmers’ cooperatives within countries for their
    PA priorities through ‘PrecisionProprietor’ to promote PA. To ensure the smooth
    functioning of the model, ‘PrecisionProprietor’ will have an agreement to share
    PA data with ‘PrecisionClient’ that will be valued using pVoucher (like eVoucher);
    there might not be any physical monitory transactions. The equivalent pVoucher
    of data shared by ‘PrecisionPropritor’ to ‘PrecisionClient’ will help the data
    owners obtain equivalent services by the ‘PrecisionClient’. In return funding
    will be channeled to ‘PrecisionClient’ by ‘PrecisionPatron’, the total values
    of the ‘pVouchers’ could be used to claim fulfilment of agreements made with the
    ‘PrecisionPatron’. The agreement could be a bi-lateral (i.e., between the ‘PrecisionPatron’
    and ‘PrecisionClient’ or between ‘PrecisionPatron’ and ‘PrecisionProprietor’),
    a tripartite or multipartite. These agreements potentially create a platform to
    collaborate on data sharing; in turn, they ensure data quality is intact and create
    a data infrastructure that could be plugged into the FAIR (Findability, Accessibility,
    Interoperability, and Reusability) framework. It can also be argued that establishing
    and networking strong PA data-nodes in all of Africa is timely to bolster the
    future of PA big data applications. Conclusion Solving the challenges of PA data
    governance and lack of viable business model are as crucial as answering the questions
    posed on PA data harmonization from the technological realm. PA data with a privacy,
    ownership, and stewardship apprehensions can be treated as a marketable good to
    incentivize the owners while powering open data and open access at later stage
    of data cycle. The smooth execution of the ‘PrecisioNexion’ model ensures the
    interconnectedness of the three major actors, i.e., PA data owners (‘PrecisionClient’),
    PA technology/ solution providers (‘PrecisionClient’) and funders (‘PrecisionPatron’).
    At the same time, the system creates a self-sustaining PA ecosystem that incentivize
    data contributors and paves the way for building confidences in data owners for
    more collaboration and partnership, which will ultimately address the big question—the
    “Why to share my data?”. References Aune, J. B., Coulibaly, A., & Giller, K. E.
    (2017). Precision farming for increased land and labour productivity in semi-arid
    West Africa: A review. Agronomy for Sustainable Development, 37(3), 16. https://doi.org/10.1007/s13593-017-0424-z
    Article   CAS   Google Scholar   Belcore, E., Angeli, S., Colucci, E., Musci,
    M. A., & Aicardi, I. (2021). Precision agriculture workflow, from data collection
    to data management using FOSS tools: An application in Northern Italy Vineyard.
    ISPRS International Journal of Geo-Information, 10(4), 236. https://doi.org/10.3390/ijgi10040236
    Article   Google Scholar   Bendre, M. R., Thool, R. C., & Thool, V. R. (2015).
    Big data in precision agriculture: Weather forecasting for future farming. In
    2015 1st International Conference on Next Generation Computing Technologies (NGCT)
    (pp. 744–750). Presented at the 2015 1st International Conference on Next Generation
    Computing Technologies (NGCT). https://doi.org/10.1109/NGCT.2015.7375220 Bullock,
    D. S., Boerngen, M., Tao, H., Maxwell, B., Luck, J. D., Shiratsuchi, L., Puntel,
    L., & Martin, N. F. (2019). The data-intensive farm management project: Changing
    agronomic research through on-farm precision experimentation. Agronomy Journal,
    111(6), 2736–2746. https://doi.org/10.2134/agronj2019.03.0165 Article   Google
    Scholar   Christiaensen, L. (2017). Agriculture in Africa—Telling myths from facts:
    A synthesis. Food Policy, 67, 1–11. https://doi.org/10.1016/j.foodpol.2017.02.002
    Article   Google Scholar   Daum, T., & Birner, R. (2020). Agricultural mechanization
    in Africa: Myths, realities and an emerging research agenda. Global Food Security,
    26, 100393. https://doi.org/10.1016/j.gfs.2020.100393 Article   Google Scholar   Dennis,
    G., Dixon, J., & Jean-Mark, B. (2012). Understanding African farming systems:
    Science and policy implications. Australia. Google Scholar   FAO, F. and A. O.
    of the U. (2020). Ten years of the Ethiopian Agricultural Transformation Agency:
    An FAO evaluation of the Agency’s impact on agricultural growth and poverty reduction.
    Food & Agriculture Org. FAO & ITU. (2022). Status of digital agriculture in 47
    sub-Saharan African countries. Rome, Italy: FAO, ITU. https://doi.org/10.4060/cb7943en
    Fraser, A. (2019). Land grab/data grab: Precision agriculture and its new horizons.
    The Journal of Peasant Studies, 46(5), 893–912. https://doi.org/10.1080/03066150.2017.1415887
    Article   Google Scholar   Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko,
    S., Thau, D., & Moore, R. (2017). Google earth engine: Planetary-scale geospatial
    analysis for everyone. Remote Sensing of Environment, 202, 18–27. https://doi.org/10.1016/j.rse.2017.06.031
    Article   Google Scholar   Hengl, T., Miller, M. A. E., Križan, J., Shepherd,
    K. D., Sila, A., Kilibarda, M., Antonijević, O., Glušica, L., Dobermann, A., Haefele,
    S. M., & McGrath, S.P. (2021). African soil properties and nutrients mapped at
    30 m spatial resolution using two-scale ensemble machine learning. Scientific
    Reports, 11(1), 6130. https://doi.org/10.1038/s41598-021-85639-y Article   CAS   Google
    Scholar   Hsu, C.-S., Tu, S.-F., & Huang, Z.-J. (2020). Design of an e-voucher
    system for supporting social welfare using blockchain technology. Sustainability,
    12(8), 3362. https://doi.org/10.3390/su12083362 Article   Google Scholar   IFPRI.
    (2017). Agricultural mechanization in Africa: Insights from Ghana’s experience
    (0 ed.). Washington, DC: International Food Policy Research Institute. https://doi.org/10.2499/9780896292963
    Jellason, N. P., Robinson, E. J. Z., & Ogbaga, C. C. (2021). Agriculture 4.0:
    Is Sub-Saharan Africa Ready? Applied Sciences, 11(12), 5750. https://doi.org/10.3390/app11125750
    Ji, W., Adamchuk, V., Chen, S., Biswas, A., Leclerc, M., & Viscarra Rossel, R.
    (2017). The use of proximal soil sensor data fusion and digital soil mapping for
    precision agriculture. In Pedometrics 2017 (p. 298). Wageningen, Netherlands.
    Retrieved 30 Nov, 2020 from https://hal.archives-ouvertes.fr/hal-01601278. Ji,
    W., Adamchuk, V. I., Chen, S., Mat, Su., A. S., Ismail, A., Gan, Q., Shi, Z.,
    & Biswas, A. (2019). Simultaneous measurement of multiple soil properties through
    proximal sensor data fusion: A case study. Geoderma, 341, 111–128. https://doi.org/10.1016/j.geoderma.2019.01.006
    Article   CAS   Google Scholar   Joffre, O. M., Poortvliet, P. M., & Klerkx, L.
    (2019). To cluster or not to cluster farmers? Influences on network interactions,
    risk perceptions, and adoption of aquaculture practices. Agricultural Systems,
    173, 151–160. https://doi.org/10.1016/j.agsy.2019.02.011 Article   Google Scholar   Krishnamurthi,
    R., Kumar, A., Gopinathan, D., Nayyar, A., & Qureshi, B. (2020). An overview of
    IoT sensor data processing, fusion, and analysis techniques. Sensors, 20(21),
    6076. https://doi.org/10.3390/s20216076 Article   Google Scholar   Leroux, C.,
    Jones, H., Pichon, L., Taylor, J., & Tisseyre, B. (2019). Automatic harmonization
    of heterogeneous agronomic and environmental spatial data. Precision Agriculture,
    20(6), 1211–1230. https://doi.org/10.1007/s11119-019-09650-0 Article   Google
    Scholar   Louhichi, K., Temursho, U., Liesbeth, C., & Gomez y Paloma, S. (2019).
    Upscaling the productivity performance of the Agricultural Commercialization Cluster
    Initiative in Ethiopia. Lowenberg-DeBoer, J., & Erickson, B. (2019). Setting the
    record straight on precision agriculture adoption. Agronomy Journal, 111(4), 1552–1569.
    https://doi.org/10.2134/agronj2018.12.0779 Article   Google Scholar   Mehrabi,
    Z., Jimenez, D., & Jarvis, A. (2018). Smallholders need access to big-data agronomy
    too. Nature, 555(7694), 30–30. https://doi.org/10.1038/d41586-018-02566-1 Article   CAS   Google
    Scholar   Ncube, B., Mupangwa, W., & French, A. (2018). Precision Agriculture
    and Food Security in Africa. In P. Mensah, D. Katerere, S. Hachigonta, & A. Roodt
    (Eds.), Systems Analysis Approach for Complex Global Challenges (pp. 159–178).
    Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-71486-8_9
    Nyaga, J. M., Onyango, C. M., Wetterlind, J., & Söderström, M. (2021). Precision
    agriculture research in sub-Saharan Africa countries: A systematic map. Precision
    Agriculture, 22(4), 1217–1236. https://doi.org/10.1007/s11119-020-09780-w Article   Google
    Scholar   Oliveira-JR, A., Resende, C., Gonçalves, J., Soares, F., & Moriera,
    W. (2020). IoT Sensing Platform for e-Agriculture in Africa. In 2020 IST-Africa
    Conference (IST-Africa) (pp. 1–8). Presented at the 2020 IST-Africa Conference
    (IST-Africa). Padarian, J., Minasny, B., & McBratney, A. B. (2019). Online machine
    learning for collaborative biophysical modelling. Environmental Modelling & Software,
    122, 104548. https://doi.org/10.1016/j.envsoft.2019.104548 Article   Google Scholar   Pierce,
    H. H., Dev, A., Statham, E., & Bierer, B. E. (2019). Credit data generators for
    data reuse. Nature, 570(7759), 30–32. https://doi.org/10.1038/d41586-019-01715-4
    Article   CAS   Google Scholar   Piikki, K., Söderström, M., Eriksson, J., Muturi
    John, J., Ireri Muthee, P., Wetterlind, J., & Lund, E. (2016). Performance evaluation
    of proximal sensors for soil assessment in smallholder farms in Embu County, Kenya.
    Sensors, 16(11), 1950. https://doi.org/10.3390/s16111950 Article   Google Scholar   Sanches,
    G. M., Magalhães, P. S. G., Remacre, A. Z., & Franco, H. C. J. (2018). Potential
    of apparent soil electrical conductivity to describe the soil pH and improve lime
    application in a clayey soil. Soil and Tillage Research, 175, 217–225. https://doi.org/10.1016/j.still.2017.09.010
    Article   Google Scholar   Shannon, D. K., Clay, D. E., & Kitchen, N. R. (2020).
    Precision agriculture basics. Wiley. Google Scholar   Sims, B. G., Hilmi, M.,
    & Kienzle, J. (2016). Agricultural mechanization: A key input for sub-Saharan
    Africa smallholders. Integrated Crop Management (FAO) eng v. 23(2016). Retrieved
    15 Sept, 2021 from http://www.fao.org/3/a-i6044e.pdf. Taylor, L. (2016). The ethics
    of big data as a public good: Which public? Whose good? Philosophical Transactions:
    Mathematical, Physical and Engineering Sciences, 374(2083), 1–13. Google Scholar   Technical
    Centre for Agriculture and Rural Cooperation (CTA). (2019). Smart farming—Transforming
    agriculture with artificial intelligence. Spore, (195). Retrieved 17 Sept, 2021
    from https://cgspace.cgiar.org/handle/10568/106118 Wolfert, S., Ge, L., Verdouw,
    C., & Bogaardt, M.-J. (2017). Big data in smart farming—A review. Agricultural
    Systems, 153, 69–80. https://doi.org/10.1016/j.agsy.2017.01.023 Article   Google
    Scholar   World Bank. (2018). Arable land (hectares per person) - Ethiopia | Data.
    Retrieved 15 Sept, 2021 from https://data.worldbank.org/indicator/AG.LND.ARBL.HA.PC?locations=ET.
    Xu, D., Chen, S., Viscarra Rossel, R. A., Biswas, A., Li, S., Zhou, Y., & Shi,
    Z. (2019). X-ray fluorescence and visible near infrared sensor fusion for predicting
    soil chromium content. Geoderma, 352, 61–69. https://doi.org/10.1016/j.geoderma.2019.05.036
    Article   CAS   Google Scholar   Download references Funding Funding was provided
    by Canadian Network for Research and Innovation in Machining Technology, Natural
    Sciences and Engineering Research Council of Canada (RGPIN-2014-4100) and Ontario
    Ministry of Agriculture, Food and Rural Affairs (UofG 22017-2889). Author information
    Authors and Affiliations School of Environmental Sciences, University of Guelph,
    50 Stone Road East, Guelph, ON, N1G 2W1, Canada Tegbaru B. Gobezie & Asim Biswas
    Corresponding author Correspondence to Asim Biswas. Additional information Publisher''s
    Note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Reprints and permissions
    About this article Cite this article Gobezie, T.B., Biswas, A. The need for streamlining
    precision agriculture data in Africa. Precision Agric 24, 375–383 (2023). https://doi.org/10.1007/s11119-022-09928-w
    Download citation Accepted 28 May 2022 Published 14 June 2022 Issue Date February
    2023 DOI https://doi.org/10.1007/s11119-022-09928-w Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Precision agriculture Data AI Blockchain Use our pre-submission checklist Avoid
    common mistakes on your manuscript. Sections Figures References Abstract Introduction
    Challenges in the PA data ecosystem The opportunities The outlook Conclusion References
    Funding Author information Additional information Rights and permissions About
    this article Advertisement Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Precision Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: The need for streamlining precision agriculture data in Africa
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Venkataramani K.
  - Marshak C.Z.
  - Bekaert D.
  - Simard M.
  - Denbina M.
  - Handwerger A.L.
  - Chan S.
  citation_count: '0'
  description: In this work, we demonstrate how harmonized optical and SAR satellite
    imagery can be utilized for robust identification of open water surfaces at a
    global scale. We train an image segmentation architecture based convolutional
    neural network (CNN) to extract the most salient features from the input data
    and generate a per-pixel water/not-water classification. We find that combining
    optical and radar imagery helps reduce false positive and false negative inferences,
    illustrating the effectiveness of this harmonization. The resulting model is able
    to classify water surfaces at the resolution of the SAR sensor (12.5 meters) with
    a validation set precision and recall of 0.74 and 0.81 respectively. We also demonstrate
    that the trained model is capable of generating inferences beyond the geographic
    bounds of the training data.
  doi: 10.1109/IGARSS52108.2023.10282284
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >IGARSS 2023 - 2023 IEEE Inter... Harmonizing
    SAR and Optical Data to Map Surface Water Extent: A Deep Learning Approach Publisher:
    IEEE Cite This PDF Karthik Venkataramani; Charles Z Marshak; David Bekaert; Marc
    Simard; Michael Denbina; Alexander L. Handwerger; Steven Chan All Authors 38 Full
    Text Views Abstract Document Sections 1. INTRODUCTION 2. DATASETS 3. MODEL PARAMETERS
    AND MODEL TRAINING 4. RESULTS 5. CONCLUSIONS AND FUTURE WORK Authors Figures References
    Keywords Metrics Footnotes Abstract: In this work, we demonstrate how harmonized
    optical and SAR satellite imagery can be utilized for robust identification of
    open water surfaces at a global scale. We train an image segmentation architecture
    based convolutional neural network (CNN) to extract the most salient features
    from the input data and generate a per-pixel water/not-water classification. We
    find that combining optical and radar imagery helps reduce false positive and
    false negative inferences, illustrating the effectiveness of this harmonization.
    The resulting model is able to classify water surfaces at the resolution of the
    SAR sensor (12.5 meters) with a validation set precision and recall of 0.74 and
    0.81 respectively. We also demonstrate that the trained model is capable of generating
    inferences beyond the geographic bounds of the training data. Published in: IGARSS
    2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium Date of
    Conference: 16-21 July 2023 Date Added to IEEE Xplore: 20 October 2023 ISBN Information:
    ISSN Information: DOI: 10.1109/IGARSS52108.2023.10282284 Publisher: IEEE Conference
    Location: Pasadena, CA, USA Funding Agency: SECTION 1. INTRODUCTION Surface water
    is a highly dynamic component of the water cycle, and time varying information
    on the location and extent of surface water is critical for hydrology, water resource
    management, and disaster response. At a global scale, long-term monitoring of
    water bodies has been achieved using satellite-based remote sensing including
    optical and synthetic aperture radar (SAR) instruments. Open water surfaces tend
    to stand out within the near-infrared and short wave infrared wavelength channels
    of optical datasets, and accurate water maps can be generated by applying scene-wise
    thresholds to normalized difference indices [1]. However, optical observations
    rely on cloud-free and sunlit conditions which is limiting for areas with frequent
    cloud cover (e.g., tropics) and observations during extreme weather events. In
    contrast, spaceborne radar imagery can be acquired independent of lighting and
    weather conditions and can often penetrate through vegetation to illuminate underlying
    water surfaces, depending on the SAR frequency and the density of vegetation.
    However, open water surfaces (e.g. rivers) and paved roads can appear as low backscatter
    targets similar to paved roads, while trees and rough water surfaces can produce
    high backscatter. In the absence of contextual knowledge or other sources of information,
    this can lead to false positive and false negative inferences of water extent.
    As such, developing a model that can infer water extent from harmonized optical
    and SAR data has the potential to mitigate these individual drawbacks [2]. In
    this work, we demonstrate how deep learning [3] can be applied to harmonized SAR
    and optical data to map surface water at a global scale. SECTION 2. DATASETS The
    Phased Array type L-band Synthetic Aperture Radar (PALSAR) instrument aboard the
    JAXA Advanced Land Observing Satellite (ALOS-1) spacecraft produced earth surface
    observations in the L-band between 2006 and 2011. For this work, we utilize high
    resolution (12.5 m) data collected in the fine beam dual polarization (HH and
    HV) instrument mode. To eliminate the effects of topographic variation on signal
    backscatter and geometric distortions introduced by the side-viewing nature of
    the SAR observations, we use radiometric terrain corrected (RTC) data [4]. The
    ALOS RTC backscatter data expressed as γ0 is available for public use from the
    Alaska Satellite Facility (ASF) [5]. The RTC processing step uses elevation information
    from the National Elevation Dataset (NED) and the Shuttle Radar Topography Mission
    (SRTM) which collectively span the United States at the resolution of the SAR
    sensor. These datasets use NAVD88 and EGM96 respectively as the vertical datum.
    This elevation information is also available for download from ASF, and is included
    as a model input. The Global Forest Change (GFC) dataset [6] is a publicly available
    dataset derived from Landsat (7 and 8) imagery (30 m resolution) that provides
    annually averaged, cloud-free image composites in the red, near-infrared (NIR),
    and two short wave infrared (SWIR) bands. Reference mosaics for all four channels
    collected around the year 2000 is available in the version 1.8 product, and provides
    the best temporal overlap with the ALOS dataset. Fig. 1. The model inputs contain
    512×512 pixel sub-images along with target labels. Show All Table 1. Datasets
    used in this work Training labels were obtained from the USGS Dynamic Surface
    Water Extent (DSWE) data product [1], which uses surface reflectance data obtained
    from Landsat observations (30 m resolution). Specifically, the DSWE algorithm
    distinguishes between pixels entirely devoid of water (classified as not-water),
    pixels fully containing water (water), and the presence of water at a sub-pixel
    level (partial surface water). The Interpreted layer With All Masks applied (INWAM)
    layer [7] separates pixels containing surface water into 5 classes (pixel values):
    not water (0), water - high confidence (1), water - moderate confidence (2) partial
    surface water - conservative (3) and partial surface water - aggressive (4). The
    dataset also identifies the presence of cloud and cloud shadow (9), and no data
    regions (255). For our purposes, we reduce the number of classes to 3 by combining
    the water - high confidence and partial surface water - conservative pixels into
    a single water class, and combining the existing no-data regions with the water
    – moderate confidence, partial surface water - aggressive, and cloud/cloud shadow
    pixels. This ensures a higher likelihood of the corresponding SAR pixels being
    labeled correctly (Fig. 1). We note that a component failure on the Enhanced Thematic
    Mapper Plus (ETM+) instrument aboard Landsat 7 causes missing data to appear in
    a \"scan-line\" pattern, which is subsequently present in the USGS DSWE data and
    our training data. While this reduces the overall number of labeled pixels available
    for training, it does not hinder the model inferences. The datasets used in this
    work are summarized in Table 1. To generate the dataset for training and validating
    our model, we begin by querying the ASF Distributed Active Archive Centers (DAAC)
    for ALOS observations over the United States, typically over inland water bodies.
    We then search for coincident USGS DSWE data that correspond to Landsat observations
    that are within 2 days of the SAR observation and contain less than 30% cloud
    cover. These requirements, along with the 46 day average repeat frequency of ALOS
    observations and relative sparsity of inland water bodies largely limits the number
    of ALOS observations available for model training. The number of water pixels
    in these scenes varies significantly (between 4-90%), with mean and standard deviation
    values of 25% and 21% respectively. For the set of available SAR observations,
    we subsequently download overlapping GFC mosaics. The GFC mosaics and DSWE data
    are then reprojected into the reference frame of the SAR observations, and are
    up-sampled using a nearest neighbors approach to match the SAR sensor resolution.
    For each ALOS observation, we create an image stack containing the HH and HV SAR
    data, the red, NIR, SWIR1, SWIR2 GFC data, the DEM information, and a corresponding
    set of labels. The stack is then divided into non-overlapping 512x512x7 pixel
    sub-images, discarding instances where more than 90% of the corresponding labels
    are no-data. A sub-image, along with the corresponding pixel-wise water/not-water/no-data
    labels forms a single training sample (Fig. 1). Our dataset comprises of 4799
    such samples. SECTION 3. MODEL PARAMETERS AND MODEL TRAINING In order to extract
    the most salient features from the large volume of data presented by the harmonized
    dataset, we use a UNet [8] with a ResNet-50 [9] encoder section, initialized with
    pre-trained weights based on imagenet data [10]. We use a learning rate of 5 ×
    10–5 and train for a maximum of 300 epochs with a batch size of 12. We use binary
    cross-entropy as the training loss function, and assign empirically determined
    weights to the water and not-water classes to account for class imbalance. The
    model utilizes batch normalization after each convolution layer to ensure that
    the associated activation layer is consistently provided normally distributed
    inputs, resulting in faster training. The dataset described in the previous section
    is randomly split into training (90%) and validation (10%) sets. This ensures
    that both datasets contain a similar distribution of chips in terms of geography
    and water content. As a preprocessing step, we convert the SAR images from linear
    units to dB, which reduces their large dynamic range, making them better suited
    for use with neural networks. We also apply total variation denoising (weight
    parameter λ = .35) [11] on each SAR sub-image to reduce speckle. We employ data
    augmentation in the form of image transformations (vertical/horizontal mirroring,
    random 90o rotations) during model training, and implement channel dropouts to
    ensure the model relies on the SAR data for inferences rather than learning a
    mapping between the optical data inputs and the (optically-derived) DSWE labels.
    We utilize the number of true positive (TP), false positive (FP), and false negative
    (FN) pixel-level inferences produced by the model to calculate precision, recall
    and intersection over union (IoU) as additional performance metrics: Precision=
    TP TP+FP Recall= TP TP+FN IoU = TP TP+FN+FP (1) View Source SECTION 4. RESULTS
    We trained two models of identical architectures with the same training data,
    differing only in the inclusion of optical data. Figure 2 shows examples of these
    model inferences generated for the inputs shown in Figure 1. The water extent
    maps are produced at the resolution of the SAR sensor (12.5 meters) While both
    models produce visually reasonable inferences, a number of false positive and
    false negative water inferences can be noted in the model relying only on SAR
    data, particularly in the middle of the river channel. Performance metrics averaged
    over the validation dataset for both models given in Table 2. Inclusion of the
    optical data is observed to generally improve all of the monitored performance
    metrics, with water classification precision increasing by 1% and recall increasing
    by 9%, and not-water classification precision and recall increasing by 1% and
    3% respectively. The IoU for water and not-water classification increased by 8%
    and 3% respectively. Figure 3 shows an example of time-series water maps generated
    along the Amazon river in Brazil, demonstrating the model''s ability to produce
    inferences for regions beyond the geographical extent of the training data. The
    figure also shows that the model can be used to study dynamic surface water extent.
    In this case, we provide the model with ALOS acquisitions 3 months apart along
    with a common set of optical and DEM data. It is seen that the model is able to
    produce water maps that show the receding water extent on land as well as in the
    middle of the river channel. SECTION 5. CONCLUSIONS AND FUTURE WORK In this work,
    we have presented an approach to harmonize SAR and optical satellite imagery in
    order to generate water extent maps, and demonstrate that including optical data
    helps reduce commission and omission errors in inferring water extent. We use
    deep neural networks to learn relevant features from this dataset, and demonstrate
    that a model trained on geographically limited data can be used to generate accurate
    time series water maps in other parts of the globe. This work does not account
    for the presence of floating vegetation or partial surface water pixels, which
    will be the subject of future work. We will also extend this approach to utilize
    current and upcoming SAR datasets such as those from the C-band Sentinel-1 mission
    and the upcoming L-band NASA-ISRO SAR (NISAR) mission. We will also utilize data
    from the Observational Products for End-Users from Remote Sensing Analysis (OPERA)
    project [12], which currently produces near-global water extent maps. Fig. 2.
    Inferences using both SAR and optical data (left) compared to inferences using
    only SAR data (right). Show All Table 2. Performance metrics for water and not-water
    detection in the validation dataset Fig. 3. SAR images and model inferences over
    the Amazon river in Brazil for June (top) and September (bottom) 2007. We only
    show the HH channel of the input image stack for reference alongside the model
    outputs. Show All Authors Figures References Keywords Metrics Footnotes More Like
    This Matching between radar image and optical image 1997 IEEE International Conference
    on Intelligent Processing Systems (Cat. No.97TH8335) Published: 1997 Feature extraction
    in Through-the-Wall radar imaging 2010 IEEE International Conference on Acoustics,
    Speech and Signal Processing Published: 2010 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: International Geoscience and Remote Sensing Symposium (IGARSS)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Harmonizing SAR and Optical Data to Map Surface Water Extent: A Deep Learning
    Approach'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mohammed M.A.
  - Lakhan A.
  - Abdulkareem K.H.
  - Abd Ghani M.K.
  - Marhoon H.A.
  - Kadry S.
  - Nedoma J.
  - Martinek R.
  - Zapirain B.G.
  citation_count: '0'
  description: 'Introduction: The Industrial Internet of Water Things (IIoWT) has
    recently emerged as a leading architecture for efficient water distribution in
    smart cities. Its primary purpose is to ensure high-quality drinking water for
    various institutions and households. However, existing IIoWT architecture has
    many challenges. One of the paramount challenges in achieving data standardization
    and data fusion across multiple monitoring institutions responsible for assessing
    water quality and quantity. Objective: This paper introduces the Industrial Internet
    of Water Things System for Data Standardization based on Blockchain and Digital
    Twin Technology. The main objective of this study is to design a new IIoWT architecture
    where data standardization, interoperability, and data security among different
    water institutions must be met. Methods: We devise the digital twin-enabled cross-platform
    environment using the Message Queuing Telemetry Transport (MQTT) protocol to achieve
    seamless interoperability in heterogeneous computing. In water management, we
    encounter different types of data from various sensors. Therefore, we propose
    a CNN-LSTM and blockchain data transactional (BCDT) scheme for processing valid
    data across different nodes. Results: Through simulation results, we demonstrate
    that the proposed IIoWT architecture significantly reduces processing time while
    improving the accuracy of data standardization within the water distribution management
    system. Conclusion: Overall, this paper presents a comprehensive approach to tackle
    the challenges of data standardization and security in the IIoWT architecture.'
  doi: 10.1016/j.jare.2023.10.005
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Outline Highlights
    Graphical abstract Keywords Introduction Related work IIoWT architecture Proposed
    algorithm methodology: CNN-LSTM and BCTD Schemes Performance evaluation and implementation
    Conclusion and future work Code and data availability Funding Compliance with
    ethics requirements CRediT authorship contribution statement Declaration of Competing
    Interest References Show full outline Figures (7) Show 1 more figure Tables (5)
    Table 1 Table Table Table 2 Table 3 Journal of Advanced Research Available online
    13 October 2023 In Press, Corrected ProofWhat’s this? Industrial Internet of Water
    Things architecture for data standarization based on blockchain and digital twin
    technology☆☆ Author links open overlay panel Mazin Abed Mohammed a i j, Abdullah
    Lakhan b i j, Karrar Hameed Abdulkareem c d, Mohd Khanapi Abd Ghani e, Haydar
    Abdulameer Marhoon f g, Seifedine Kadry h, Jan Nedoma i, Radek Martinek j, Begonya
    Garcia Zapirain k Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.jare.2023.10.005
    Get rights and content Under a Creative Commons license open access Highlights
    • We present a hybrid model called CNN-LSTM. The CNN-LSTM model is designed to
    efficiently extract and process data from diverse sources, thereby addressing
    the issue of data modality. • We devise the cross-platform runtime environment
    based on Message Queuing Telemetry Transport (MQTT) protocol. This environment
    allows for the seamless processing of diverse data sources and types through heterogeneous
    computing nodes in IIoWT. By leveraging MQTT, the devised runtime environment
    ensures interoperability and facilitates the execution of data across heterogeneous
    computing nodes. • We devise a blockchain scheme that utilizes SHA-256 for data
    hashing. This scheme incorporates a proof-of-work mechanism and ensures data validity
    across heterogeneous nodes in IIoWT. Introduction: The Industrial Internet of
    Water Things (IIoWT) has recently emerged as a leading architecture for efficient
    water distribution in smart cities. Its primary purpose is to ensure high-quality
    drinking water for various institutions and households. However, existing IIoWT
    architecture has many challenges. One of the paramount challenges in achieving
    data standardization and data fusion across multiple monitoring institutions responsible
    for assessing water quality and quantity. Objective: This paper introduces the
    Industrial Internet of Water Things System for Data Standardization based on Blockchain
    and Digital Twin Technology. The main objective of this study is to design a new
    IIoWT architecture where data standardization, interoperability, and data security
    among different water institutions must be met. Methods: We devise the digital
    twin-enabled cross-platform environment using the Message Queuing Telemetry Transport
    (MQTT) protocol to achieve seamless interoperability in heterogeneous computing.
    In water management, we encounter different types of data from various sensors.
    Therefore, we propose a CNN-LSTM and blockchain data transactional (BCDT) scheme
    for processing valid data across different nodes. Results: Through simulation
    results, we demonstrate that the proposed IIoWT architecture significantly reduces
    processing time while improving the accuracy of data standardization within the
    water distribution management system. Conclusion: Overall, this paper presents
    a comprehensive approach to tackle the challenges of data standardization and
    security in the IIoWT architecture. Graphical abstract Download : Download high-res
    image (182KB) Download : Download full-size image Keywords IIoWTBlockchainWater
    managementDigital twinTransparencyData modality and standardization Introduction
    Recently, there have been notable advancements in the Industrial Internet of Things
    (IIoT) paradigm, which focuses on implementing Internet of Things (IoT) applications
    for water management [1]. IIoT incorporates automated sensors, sensing devices,
    and data analytics to improve sustainability, efficiency, data security, and transparency
    in industrial operations involving multiple stakeholders. IIoT fetches about a
    revolutionary transformation in water management by facilitating precise calculations,
    real-time monitoring, and comprehensive profiling of resources within distributed
    infrastructures [2]. This paradigm opens up many possibilities for diverse water
    applications, incorporating various data modalities and features. Some notable
    examples of these applications include using sensors to monitor and assess water
    quality, track water consumption, analyze water composition, measure water levels,
    detect leaks, evaluate wastage, and understand consumption patterns. Data standardization
    plays a vital role in harnessing the potential of IIoT-enabled applications. Establishing
    uniformity and consistency in data representation and formats across different
    platforms and systems is imperative [3], [4], [5]. IIoT recognizes the potential
    of blockchain technology in creating new data standards that can seamlessly integrate
    and operate across various platforms. This standardized data format would benefit
    multiple stakeholders involved in water management, including end customers, government
    regulatory bodies, and water distributors. By adopting standardized data formats,
    valuable insights can be gained from water quality data. For instance, information
    regarding the precise amount of chlorine dosage and hormonal levels can be efficiently
    communicated to both government entities responsible for the regulation and end
    customers who are concerned about water quality and safety. This data standardization
    ensures transparency, reliability, and interoperability in the exchange of water-related
    information, ultimately enhancing the efficiency and effectiveness of water management
    processes. Effective data modalities and standardized data management are key
    factors in enhancing IIoT performance. Implementing technologies like blockchain
    and digital twins allows for this improvement. The convergence of the Industrial
    Internet of Things (IIoT) paradigm with blockchain technology and Digital Twin
    (DT) concepts brings numerous advantages to the water industry [6], [7]. Blockchain
    technology (BCT) plays a pivotal role in processing and managing the data generated
    by interconnected water industries. It operates with transparency, security, and
    immutability, ensuring the data remains tamper-proof and protected against unauthorized
    modifications. By leveraging blockchain technology, applications in the water
    industry can establish a reliable and auditable data trail throughout the supply
    chain, ensuring data integrity and enabling authenticated data processes. BCT
    provides a robust framework for data asset management in the water industry. Through
    the utilization of Digital Twin (DT) technology, physical data from various locations
    and industries can be replicated and managed virtually. DT offers scalability,
    cost-efficiency, and interoperability, making it an invaluable tool for water
    industry applications. With DT, stakeholders in the water industry can create
    virtual replicas of their physical assets, enabling real-time monitoring, analysis,
    and simulations. This facilitates improved decision-making, predictive maintenance,
    and optimization of water management processes. In summary, integrating IIoT,
    blockchain technology, and Digital Twin concepts within the water industry offers
    benefits such as transparent and secure data management, traceability throughout
    the supply chain, authenticated data processes, and efficient data asset management
    through virtual replication. These technologies empower water industry applications
    to enhance operations, refine decision-making processes, and achieve cost-effective
    and scalable solutions [8], [9], [10], [11]. Deep learning (DL) has garnered considerable
    attention in research and academia due to its potential for automating processes
    within the Industrial Internet of Things (IIoT) framework, particularly in water
    applications. DL, an integral part of artificial intelligence (AI), encompasses
    a range of mechanisms and algorithms. Notably, CNN and RNN strategies were utilized
    in DL. The primary objective is to effectively process and analyze the vast volume
    of data generated by water sensors in real-time. DL techniques excel in handling
    diverse data modalities associated with water sensors, including image data captured
    by sensors, time series data, and numeric and textual data obtained from distributed
    infrastructures. However, data standardization in water data management within
    the Industrial Internet of Things (IIoT) presents numerous challenges that must
    be addressed. This paper considers these research questions to solve the data
    standardization problem in IIoWT architecture. (i) The first challenge arises
    from the diverse data modalities encountered in water management. (ii) The data
    originates from various sources and is represented in different formats, necessitating
    suitable processing across multiple nodes. Secondly, the attainment of data interoperability
    and integration poses a significant obstacle. The IIoT involves collecting data
    from disparate sources, making it arduous to convert and consolidate it into a
    standardized format that can be seamlessly processed and yield meaningful outcomes
    on the nodes. (iii) Data security is another pivotal challenge, particularly when
    distributing data among IIoT water nodes involving end users, industries, and
    government entities. To address the aforementioned challenges, this paper presents
    the Industrial Internet of Water Things (IIoWT) architecture, which includes components
    such as a water quality monitoring application, computing and wireless communication
    infrastructure, and data-generating sensors. The application aims to monitor water
    quality by implementing secure data standardization protocols that enable smooth
    data exchange among end users, service providers, and government entities. The
    integration of CNN and blockchain reveals many potential advantages and applications,
    particularly within the IIoWT (Industrial Internet of Things) industry. CNNs can
    be employed to analyze various types of data, such as images, text, and content,
    while blockchain technology ensures robust security and confidentiality, especially
    for sensitive data like patients’ medical records. By storing the training data
    and predictions of the CNN model on the blockchain, it becomes possible to authenticate
    and validate the accuracy and reliability of the model’s results, thereby fostering
    increased confidence in the system. To answer the research questions raised earlier,
    this paper offers the following research findings and contributions to the existing
    body of knowledge. • We present a hybrid model called CNN-LSTM. The CNN-LSTM model
    is designed to efficiently extract and process data from diverse sources, thereby
    addressing the issue of data modality. • We devise the cross-platform runtime
    environment based on the Message Queuing Telemetry Transport (MQTT) protocol.
    This environment allows for the seamless processing of diverse data sources and
    types through heterogeneous computing nodes in IIoWT. By leveraging MQTT, the
    devised runtime environment ensures interoperability and facilitates the execution
    of data across heterogeneous computing nodes. • We devise a blockchain scheme
    that utilizes SHA-256 for data hashing. This scheme incorporates a proof-of-work
    mechanism and ensures data validity across heterogeneous nodes in IIoWT. The existing
    blockchain schemes, such as proof of credibility, byzantine fault-tolerant, and
    hash validation schemes, consume more resources and support only homogeneous nodes
    in distributed networks. The paper is organized into several sections. Section
    1 focuses on the related work, providing an overview of previous studies in the
    field. Section 1 explains the IIoWT architecture, outlining its components and
    functionality. Section 1 presents the proposed methodology, detailing how it operates
    and addresses the research objectives. The implementation of the system and performance
    evaluation are discussed in Section 28. Finally, Section 28 concludes the paper
    and outlines potential avenues for future research. Related work The IIoT paradigm
    has been widely applied in water management across various smart cities. Its primary
    goal is to enable water quality monitoring across different gradients and stakeholders.
    In these studies [1], [2], [3], [4], [5], an industrial IoT architecture was presented
    to collect gradient data from raw water at different plants. These studies showcased
    various applications for monitoring raw water quality using plant ultrasonic sensors.
    The surveys and case studies discussed both application and infrastructure-related
    challenges. However, data standardization and security about data modality still
    need to be addressed in these works. To ensure data security standardization across
    heterogeneous computing nodes in water management, previous studies [6], [7] have
    proposed decentralized architectures based on blockchain technology. The primary
    objective was to facilitate the secure transfer of valid data among interconnected
    nodes using blockchain. The key contribution of this architecture lies in its
    ability to process data modalities in a transparent, immutable, and authenticated
    format across nodes. The studies presented blockchain-based systems for IoT water
    management in smart cities. The objective of these systems was to enable the transformation
    of water quality, sensor, and application data through a distributed ledger among
    involved stakeholders. Different processing levels implemented proof-of-work-validity,
    proof-of-credibility, and fault-tolerant schemes within the blockchain. These
    blockchain schemes aimed to ensure consistency in water data for quality enhancement
    and compliance with standards across various locations. However, developing and
    implementing blockchain blocks for data standardization with the aforementioned
    studies incurred significant resource and cost requirements at different nodes.
    Previous studies [8], [9], [10], [11] explored the integration of Digital Twin
    (DT) technology to mitigate the implementation cost associated with blockchain
    technology in water management systems and applications. They presented a DT-enabled
    blockchain solution wherein the implementation relied on virtualizing the consensus
    rules of physical blockchain blocks. The aim was to decrease the implementation
    cost of water management across various nodes by leveraging the combination of
    blockchain technology and DT. However, resource. A case study by Xu et al. [12]
    presented a water quality monitoring system utilizing underwater sensors in fish
    ponds. The study’s objective was to analyze the water quality and levels in various
    tanks using IoT technology and examine the flow after storage from different water
    sources in distributed fog cloud networks. Another study by Smith et al. [13]
    focused on a drinking water quality system that performs real-time checks on raw
    materials and their quality. The researchers aimed to develop a lightweight and
    cost-efficient IoT water management system. Furthermore, several studies [14],
    [15], [16], [17], [18] presented distributed architectures for water management,
    where raw data collected from diverse sensors is processed at different nodes
    and aggregated centrally for further analysis. However, it should be noted that
    these water management systems can only process one data modality at a time. By
    leveraging DL, improvements can be made in various aspects of water management.
    For example, DL algorithms can enhance water quality assessments, enable the detection
    of vulnerabilities and faults, provide fault tolerance mechanisms, and extract
    relevant contextual features during the data processing phase. Overall, DL’s capabilities
    make it a valuable tool for addressing the challenges present in water management.
    DL empowers water industry stakeholders to make informed decisions, optimize processes,
    and improve water-related operations’ overall efficiency and effectiveness through
    its ability to process and analyze complex and diverse data. In previous studies
    [18], [19], [20], various DL and machine learning schemes were proposed to address
    data modality processing and feature extraction in distributed water management
    environments. Specifically, using CNN and LSTM models was widely explored for
    handling data modality and data fusion challenges, particularly in scenarios where
    sensor data processing is distributed across multiple nodes. These studies successfully
    processed diverse data formats, including images, text, numeric, and other data
    types. The paper [21] proposed a framework based on blockchain technology for
    environment and distributed data management. The objective was to establish a
    trusted water management process among interconnected nodes belonging to different
    stakeholders. To enhance the dynamic adoption of IoT-based water management in
    distributed blockchain-enabled networks, several studies [22], [23], [24], [25]
    presented their findings. In these studies, IoT sensors were deployed across various
    water management components to monitor water conditions at different locations.
    Implementing Chinese smart water systems based on blockchain technology enabled
    transparent, secure, and valid record-keeping across diverse locations within
    smart cities. The studies [26], [27], [28], [29], [30], [31], [32], [33] presented
    architectures for water management in agriculture applications based on blockchain
    technology. These studies proposed various components to facilitate water management
    in different architectural contexts. The presented architectures comprised several
    elements: source linking, processing nodes, pump speed control, water flow management,
    and scheduling mechanisms across distributed nodes. Based on our current understanding,
    unresolved challenges are still related to the standardization of data modalities,
    security aspects, real-time water quality monitoring across various nodes, and
    interoperability issues. In light of these challenges, this paper aims to address
    the issues of data modality security, interoperability, water management, and
    data validation among nodes. IIoWT architecture The architecture IIoWT, depicted
    in Fig. 1, comprises various components, including the water source, harvest system,
    storage facility, distribution pump, and end-user institutions such as houses,
    hospitals, schools, and more. Fig. 1 shows the flow of water management from raw
    material to the distribution of clean water in the different institutions for
    drinking. The water source and water treatment plant are responsible for the purification
    of water obtained from diverse sources. The water source gathers water from various
    points and employs sensors to generate data. These sensors include Residual Chlorine
    [34], Organic Carbon [35], Turbidity [36], Conductivity [37], pH [38], and others,
    which are utilized to monitor the quality of water at the collection sources for
    the water treatment plant. The architecture consists of physical and virtual servers
    and application interfaces. The physical servers are integrated into the power
    plant and government monitoring offices, while digital twin-enabled virtual servers
    are deployed in various water management offices to handle water flow and distribution.
    The physical servers exhibit heterogeneity, whereas the virtual servers possess
    homogeneous characteristics. Additionally, the computing node is integrated with
    various components to monitor the water quality within the architecture. The architecture
    collects data from diverse sources, each with its unique format. This heterogeneity
    poses a significant challenge in terms of interoperability and processing. To
    address this issue, our architecture incorporates a runtime design enabled by
    Message Queuing Telemetry Transport (MQTT) [39], ensuring seamless interoperability
    among data formats and runtimes. Furthermore, we have integrated deep learning
    convolutional neural network (CNN) [40] and least memory (LSTM) [41] schemes into
    our architecture to effectively process data with varying features and formats.
    This enables efficient analysis and utilization of the collected data. Our study
    integrates lightweight blockchain schemes within different nodes to ensure security
    and maintain data standards. This ensures the immutability and transparency of
    data sharing among the nodes, enhancing overall security and data integrity. The
    architecture incorporates various algorithm schemes, including CNN-LSTM and blockchain
    transactions data schemes (BCTD). The primary objective is to manage standardized
    data and process it efficiently across different nodes while ensuring transparency,
    immutability, and validity. Download : Download high-res image (768KB) Download
    : Download full-size image Fig. 1. Industrial Internet of Water Things architecture
    for data standardization based on blockchain and digital twin technology. All
    the notations of the problem are defined in Table 1. Table 1. Notations and definition.
    Notations Definitions CNN Convolutional Neural Networks RPC Remote Procedure Call
    T Number raw source data of water Features of dataset and particular feature T
    M Features of dataset Z t Particular water ingredient data Z Total number of sensors
    data z Particular sensor S Number of physical computing nodes s Particular physical
    computing nodes Computing capability of node ∊ Resource capability of node DT
    Number of virtual DT computing nodes s Particular virtual DT computing nodes Computing
    capability of DT node ∊ Resource capability of DT node L Number of virtual DT
    computing nodes l Particular virtual DT computing nodes Computing capability of
    DT node ∊ Resource capability of DT node Determine the distance between different
    nodes Number of blockchain blocks and Attributes and blockid Network of communication
    networks and particular network bandwidh Attributes of particular blockchain block
    Public key, private key, current hash and previous hash Problem mathematical formation
    The architecture comprises S physical servers, each denoted as s. Each physical
    server possesses computing speed ( ) and resource capability ( ∊ ). Additionally,
    virtual servers are denoted as DT, shared components among the physical servers.
    Each virtual server, represented by dt, has its own computing speed ( ) and resource
    capability ( ∊ ). Moreover, computing nodes exist in houses and institutions,
    denoted as L, which are shared components among the physical servers. Each local
    interface, referred to as l, possesses its own computing speed ( ) and resource
    capability ( ∊ ). The architecture encompasses a collection of T sensors, where
    each sensor t is responsible for monitoring specific data within a particular
    water plant and storage facility. These T sensors generate numeric data through
    various sensing mechanisms. Specifically, sensors such as those detecting water
    leakage, pump water flow, and faults are denoted as Z. In our analysis, we examine
    a total of B blockchain blocks, each denoted by ba, representing the specific
    attributes associated with the blockchain block in the architecture. We establish
    the blockchain validation mechanism using the formula presented below. (1) Eq.
    (1) determines the encryption of all datasets data based on SHA-256 [42]. (2)
    Eq. (2) determines the decryption of all datasets data based on SHA-256 [42].
    The data validation among nodes is determined in the following way. (3) Eq. (3)
    determines the validation of data transactions of all datasets between nodes based
    on current and previous hash transactions. The blockchain validation determines
    for all nodes in the following way. (4) Eq. (4) determines the encryption, decryption
    and validation between nodes. The physical servers process the sensor datasets
    data in the following way. (5) Eq. (5) determines the physical server processing
    for all datasets. The sensor communication processing and distance calculation
    are determined in the following way. (6) Eq. (6) determines the communication
    time and distance during reconvening and sending data between sensors and nodes.
    (7) Eq. (7) determines the DT virtual server processing on different nodes. (8)
    Eq. (8) determines the local processing in different institutions. The total processing
    time is calculated as follows. (9) Eq. (9) determines the total processing for
    all datasets in architecture. Proposed algorithm methodology: CNN-LSTM and BCTD
    Schemes The study integrates techniques, including CNN, LSTM, and blockchain,
    to handle water management’s diverse data modalities (e.g., image, text, numeric
    data) in IIoWT architecture [45], [46], [47]. We combine both CNN-LSTM to process
    the diversity of data generated from different sensors on the different computing
    nodes. The main reason is that CNN extracts the features of data, and LSTM sequences
    the data properly. In IIoWT architecture, the computing nodes, such as physical
    servers, virtual servers, and end-user nodes, the BCTD scheme, share the validated
    data among nodes during processing in IIoWT. To address this, we employ a combination
    of methods in our approach, enabling data feature extraction of sensor data using
    CNN. The sensors’ data requires capturing data dependencies. Therefore, we exploit
    LSTM for time series analysis and ensuring data validation across nodes (leveraging
    blockchain). The CNN scheme is utilized for extracting data features, while another
    scheme, like LSTM, captures data dependencies among various data points. Additionally,
    considering the heterogeneity of the computing nodes involved in processing, analyzing,
    and monitoring water from different perspectives, we propose data validation based
    on blockchain technology. This work showcases the methodology, highlighting the
    distinctions. The study illustrates two methodologies: the Flow Management of
    CNN-LSTM and BCTD Schemes. These schemes are integrated and operate collaboratively
    in distinct steps. The process begins with the data modality, which encompasses
    various data sensors. The data is then processed using CNN to extract the relevant
    features. In addition, for capturing data dependencies, the output layer of CNN
    serves as the input for LSTM. The LSTM module accumulates the results and shares
    them across different nodes based on the blockchain scheme implemented in our
    work. Data sensors modality process based on CNN and LSTM Schemes The combination
    of CNN and LSTM serves different purposes. We utilize the CNN scheme to extract
    and identify water level, leakage, and flow in various nodes. On the other hand,
    LSTM is employed to monitor water quantity and analyze its gradients within the
    architecture. We provide a detailed description of this scenario of the proposed
    methodology. Initially, we monitor water activity and data from different sensor
    modalities, denoted as T. The CNN scheme extracts features from diverse datasets,
    such as T. The output of the CNN model serves as the input for LSTM. This allows
    LSTM to analyze the data sequence based on time series analysis within the architecture.
    Please note that the mentioned description is a general example, and the details
    may vary depending on the context and data being analyzed. We are currently monitoring
    the water pump components and collecting data from various sensors, represented
    as Z. Due to heterogeneous components in the IIoWT architecture, we use the common
    runtime platform protocol such as X86 for communication. CNN is responsible for
    extracting features from different types of inputs, including images, numeric
    data, and text data from diverse sensors. LSTM is used to generate sequences based
    on the output of CNN. For example, a water pump is depicted with a blue color
    and consists of multiple components. The water tank is also connected to various
    sensors and provides output accordingly. The storage tank contains 30,000 liters
    of water and includes parameters such as pH level, chloride concentration, and
    others. The water is distributed to different residential houses, institutions,
    and hospitals. The CNN schemes analyze the features of various data inputs, denoted
    as T and Z. Each CNN scheme incorporates distinct hyper-parameters, including
    filters, padding, and size. CNN and LSTM are supervised learning algorithms that
    process different data sources across layers. In our methodology, we employ pre-trained
    data models [43] for CNN and avoid any data pre-processing. This approach allows
    us to handle non-linear data with diverse formats. The CNN architecture consists
    of multiple layers, with the convolutional layer of size 28 28 responsible for
    extracting features based on the trained and tested models. These features are
    then passed to the pooling layer. The fully connected layers and softmax activation
    function [44] generate the final feature results. Algorithm 1 CNN-LSTM data feature
    extractions The combination of CNN and LSTM Algorithm 1 incorporates various steps
    to achieve optimal results. The CNN’s convolutional layer, pooling layer, and
    output layer are combined with the different states of LSTM. This integration
    enables the determination of water flow, analysis of water quality and its constituents,
    identification of water flow patterns, and detection of component leakage in real-time
    within distributed computing components. In this paper, we present a combined
    CNN and LSTM Algorithm 1 that involves the following steps: • The combined algorithm
    schemes take several parameters as inputs, such as . • The initial results, denoted
    as R, are generated by CNN. These results involve the extraction of features from
    datasets based on pre-trained models. • Steps 1 to 3 focus on extracting features
    and labeling them on 3D images. For numeric and text data, matrixes are generated
    using the Word2Vec algorithm. • Steps 4 to 8 involve appending all the generated
    features and storing them into the initial results generated by CNN-LSTM, represented
    as . The results are appended using the operation ”Append R[t, f, z, m]” and then
    undergo training and testing. • For handling the sequence of water quantity, LSTM
    is applied as described in step 9. • From steps 9 to 14, the existing trained
    and tested model is utilized. • We consider this problem a vanishing gradient
    problem, where backpropagation aims to achieve minimum errors in different time
    series analyses. • Different states occur during the LSTM process, including the
    input, hidden, and output. • The previous time stamp and current timestamp are
    utilized to obtain accurate results from the inputs provided by CNN. • The algorithm
    produces results with cumulative accuracy, denoted as , as shown in steps 15 to
    17. Blockchain transactional data validation (BCTD) This paper’s computing nodes
    encompass various data processing and analysis entities, including physical servers,
    digital twin servers, and end users’ computing interfaces. Physical and virtual
    servers are responsible for processing data modalities, leveraging their respective
    resource capabilities and computing power. On the other hand, user end nodes solely
    receive data based on the validation scheme. To ensure interoperability across
    different nodes, we have designed a runtime environment based on MQTT for all
    cross-platform computing nodes. As mentioned earlier, our approach involves processing
    data modalities and fusion, employing a combination of CNN and LSTM for monitoring
    water level feature extraction and sequencing data dependencies in the algorithm.
    Physical servers are established exclusively at the power plant, while identical
    virtual servers are deployed at various monitoring institutions, such as government
    water municipal institutes. To ensure the scalability and effective deployment
    of the IIoWT architecture as the complexity and scale of the water management
    network grow over time. We set the deadlines for each transaction in the blockchain
    network as shown in Algorithm 2. The main concern is ensuring the resource’s scalability
    and overall execution time (e.g., computation time and communication time) must
    be less than the given deadlines on all transactions. All nodes have sufficient
    resources for scaling up and down in the blockchain networks. Consequently, physical
    servers share all processing data with virtual servers in different locations.
    Additionally, the water distributor digitally shares water quality data with end
    users based on their applications and interfaces throughout the processing phase
    within the architecture. However, ensuring security is crucial when dealing with
    heterogeneous nodes. Therefore, we propose utilizing blockchain technology to
    generate hash data before offloading it to another node, employing asymmetric
    SHA-256 hashing schemes. Algorithm 2 BCTD scheduling scheme To ensure secure communication,
    we implemented a combination of public and private keys for encryption and decryption,
    allowing for the transformation of data into readable forms at different computing
    nodes. In addition, we developed the Blockchain Transaction Data (BCTD) scheme,
    where encryption and decryption processes on current nodes are treated as blocks.
    Each block contains essential information such as a unique identifier (bid), current
    hashing (ch), previous hashing (phs), data transactions, offloading details, validation
    rules, and acknowledgments during the validation process between data transfers
    among nodes. This scheme transforms data transactions into a transparent and immutable
    format, enhancing the security and efficiency of our study. The study introduces
    the BCTD Algorithm 2 for data validation among nodes in the blockchain, which
    encompasses the following steps. • The blockchain scheme utilizes the input from
    the combined results obtained from the CNN and LSTM schemes on the given data
    modalities within the architecture. For example, the blockchain input is across
    all blockchain blocks. • Steps 1 to 8 illustrate the blockchain’s encryption,
    decryption, and data validation phases, implemented across different nodes. •
    Steps 9 to 13 involve data validation in its immutable form, based on predetermined
    rules. • These rules consist of generated hashing patterns, where the miner node
    establishes the blockchain rules for the other nodes. • Steps 10 to 21 demonstrate
    the processing of blockchain data at various nodes, following the predefined rules
    and utilizing the current and previous hash values. • The miner node, which in
    our case is the government institution, holds the authority to add or remove blocks
    within the blockchain architecture. • Steps 22 to 24 outline the matching of each
    transaction across nodes, using a designed proof-of-work pattern and rules during
    offloading and generating acknowledgments if the transaction validations are successful.
    • In cases where transactions are deemed invalid, the blockchain identifies them
    as suspicious within the architecture. Time complexity and space complexity of
    CNN-LSTM for considered problem In this study, we exploited different schemes
    such as CNN, LSTM, and blockchain validation for the data standardization process.
    We determined the time complexity of CNN based on . The N shows the number of
    convolutional layers, pooling layer, fitness, and sigmoid function of the processing.
    The LSTM has three distinct gates: input, output, and forget gates. Therefore,
    time complexity becomes . The blockchain has many blocks for validation. Therefore,
    the time complexity is determined in number of blocks for data standardization
    on heterogeneous nodes. The space complexity is determined in heterogeneous nodes
    based on available resources in IIoWT architecture. These classes include raw
    water ingredient monitoring, supply tank management, storage tank management,
    water flow control, water motor pump distribution, and water volume calculation
    based on data collected from different sensors. The digital twin system involves
    physical and virtual components, utilizing Amazon Lambda services. It incorporates
    blockchain technology to enhance the security and transparency of the virtual
    water management process. The physical and virtual server resources are determined
    in ∊ and ∊ . Performance evaluation and implementation In this part, we designed
    the simulation environment based on various components. We mainly focused on the
    IIoWT (Industrial Internet of Water Things) architecture components we prepared
    for the simulation. We compared these components with existing architectures,
    algorithms, data validations, and performance for distributed water management
    in smart cities. The IIoWT architecture comprises various components, including
    sensors, computing nodes, wireless communication, and different programming application
    interfaces (API). We defined and listed the simulation parameters in Table 2.
    Table 2. Experiment configuration. Parameter Description Parameter Description
    Empty Cell Empty Cell PH, chlorine METTLERs Hardness,Solid Ultrasonic Sensor Sulfate,
    Conductive HYDROS, Sonar Organic–carbons Sonars Trihalomethanes,Turbidity Sonars
    Potability Portable sonars Raspberry-Pi Water Pump IoT-Clock GPIO-Module Languages
    JAVA, Python, XML Ineroperateability MQTT Client Server Brocker Layer (type) Output
    Shape Param Learning Rate input(Input Layer) (None, 224, 224, 1) 0 0.5 model-T
    (Model) (None, 7, 7, 1) 492897 0.3 model-Z (Model) (None, 224, 224, 1) 62711 0.5
    Total params 555,608 Trainable params 553,702 Non-trainable params 1,906 Convolutional
    layer 128 128 In IIoWT, different types are implemented with different characteristics.
    Table 2 displays the configuration of nodes in the IIoWT architecture. It encompasses
    various computing nodes, including physical servers, digital twin virtual servers,
    and end-user interfaces. All the nodes in the architecture utilize Amazon’s virtualization
    technology. The physical and digital twin servers have adaptive and scalable storage,
    allowing for flexible expansion or reduction based on specific resource requirements.
    On the other hand, the end-user devices or interfaces are fixed and possess resource-constrained
    characteristics. IIoWT architecture implementation based on Amazon baseline architecture
    The open-source code for industrial IoT digital twin in water management, designed
    for simulation and design purposes, is available on GitHub by Amazon. This code
    provides a baseline application programming interface (API) that comprises various
    classes essential for water management, as shown in Fig. 2. These classes include
    raw water ingredient monitoring, supply tank management, storage tank management,
    water flow control, water motor pump distribution, and water volume calculation
    based on data collected from different sensors. The digital twin system involves
    physical and virtual components, utilizing Amazon Lambda services. It incorporates
    blockchain technology to enhance the security and transparency of the virtual
    water management process. Additionally, different sensors within the system generate
    images to detect and monitor water leakage. Overall, the digital twin code developed
    by Amazon for water management demonstrates the integration of virtual and physical
    components, leveraging blockchain technology to ensure efficient and secure operations
    in water management processes. The code is publicly available on Github for further
    exploration and utilization. The Amazon Cloud Development Kit (CDK) is a simulator
    that enables the creation of virtual physical servers and digital twin servers.
    This functionality is made possible through the Amazon Virtual Lab, which provides
    the necessary hardware and software resources. It offers an open environment where
    mobile applications for industrial water management can be seamlessly connected
    to mobile devices. The environment is based on Docker containers, allowing the
    encryption of complete image data within blockchain blocks within the IIoWT architecture.
    In our research, we utilized two datasets that had been trained and tested: one
    focused on water quality and the other on water leakage detection. In our study,
    we utilized trained and tested datasets for our analysis. The CNN-LSTM algorithm
    collected data from sensors and made decisions based on the models that had been
    trained and tested. Blockchain technology was implemented on both physical and
    virtual digital twin servers. However, in the case of end-user applications or
    interfaces, blockchain was solely utilized to ensure the validity of drinking
    water data within the home environment. All nodes within the system were interconnected
    through distributed networks, ensuring interoperability and seamless communication
    between them. Download : Download high-res image (467KB) Download : Download full-size
    image Fig. 2. IIoWT implementation with different components. Water sensors datasets
    benchmark To evaluate existing studies’ performance, we exploited the same datasets
    with different features to compare the proposed work with the existing studies.
    The datasets consisted of different sensor data as the benchmark with the data
    modality as shown in Table 3. We applied blockchain technology to sensor data
    based on the SHA-256 scheme to collect valid sensor data among fog and cloud-enabled
    water components. Drinking water plays a vital role in human life, and therefore,
    government healthcare agencies always prioritize ensuring clean drinking water
    in smart cities. At national and international levels, water is generated from
    raw materials such as seawater and other resources, collected at power plants,
    and treated following predefined government regulations. Consequently, it is imperative
    to adhere to these regulations to purify drinking water at the power plants. To
    monitor compliance with these regulations, we continuously monitor the readings
    of various sensors in real-time using other computing nodes. We collected the
    real-time water quality and monitoring data in different time zones with different
    parameters. We generated sensor values using various hyperparameters, such as
    Sensor, Count, Mean, STD (standard deviation), Min (minimum), 12 am-8 am, 9 am-5
    pm, 6 pm-1.55 pm, and Max (maximum). Among these sensors, the pH sensor is of
    utmost importance as it measures the acidity level in water. As per the guidelines
    set by the government and the World Health Organization, the pH values must fall
    within the range of 6 to 9. Hardness in water consisted of different gradients
    such as magnesium and calcium salts. These salts are observed in water randomly
    during flows in various sources. Solid in water consisted of other ingredients.
    For instance, bicarbonates, chlorides, sulfates, calcium, magnesium, sodium, and
    potassium. Therefore, it has a standardized threshold to make the water into drink
    form. For instance, 499 mg/l to 999 mg/l were incorporated in water during processing
    at the power plants. The study adjusted the chlorine ratio to clean water at a
    concentration of 4 to 6 milligrams per liter. A certain percentage of ammonia
    was added to the water to render it suitable for drinking. Additionally, the water
    was supplemented with a sulfate level ranging from 2500 to 3000 milligrams per
    liter. The water leakage datasets included images such as Raw Water Plant, Motor
    Pump Filter, Water Flow Pump, Purify Pump, Drinking Clean Water, and Storage.
    The water leakage and flow and storage level of water tanks are determined and
    monitored in the IIoWT based on trained and tested models of dataset Z. Table
    3. Data modality samples from different sensors for water quality. Sensor Count
    Mean STD Min 12am-8am 9am-5 pm 6 pm-11.55 pm Max Empty Cell t1 310112.0 3.372221
    0.512227 0.500000 1.438831 3.456539 3.499826 1.549016 t2 319951.0 27.591611 2.296666
    1.000000 26.310760 28.133678 39.479160 26.727430 t3 090301.0 3.8392 1.88820 7.159720
    8.0620 9.9300 23.57770 2.032990 t4 10301.0 1.752481 1.418887 1 5.640620 32.838539
    12.227428 19.2500 12.220490 t5 9301.0 6.673936 4.023912 0.798032 0.620400 0.638916
    0.615723 10.000000 t6 490301.0 1.396414 0.298247 0.44440 1.976260 0.6790 1.912150
    0.9998 t7 7890301.0 2.396414 0.558247 0.67440 0.976260 0.99790 0.912150 0.9998
    t8 84869.0 0.843152 0.201155 1.0000 1.907120 5.167530 0.7950 1.596640 t9 111213.0
    4.200721 2.037390 0.8935 12.183740 13.494790 11.697340 15.348960 t10 31213.0 2.200721
    5.037390 0.3935 1.183740 7.494790 7.697340 2.348960 t11 41213.0 3.200721 7.037390
    0.4935 2.183740 5.494790 8.697340 3.348960 t12 71213.0 5.200721 6.037390 0.5935
    17.183740 7.494790 3.697340 4.348960 t13 21213.0 6.200721 7.037390 0.6935 10.183740
    1.494790 6.697340 6.348960 t14 231213.0 8.200721 5.037390 0.9935 9.183740 2.494790
    8.697340 7.348960 t15 3241213.0 9.200721 3.037390 0.7935 8.183740 3.494790 1.697340
    9.348960 ph Hard Solids Chloramines Sulfate Conduct carbon methanes Turbidity
    0.1 1890455 2.318981 1.300212 123.516441 8.308654 1.379783 8.990970 3.963135 1.716080
    45.422921 5450.057858 2.635246 0.1 2.885359 1.180013 12.329076 1.500656 2.099124
    12.236259 5454.541732 3.275884 0.1 46213 16.868637 66.420093 3.055934 3.316766
    45.373394 5644.417441 4.059332 4.886136 1.266516 18.436524 5.341674 4.628771 5.092223
    18.101509 342.986339 2.546600 3.135738 12.410813 1.558279 4.997993 4.075075 Result
    analysis and discussion The study assessed the performance of existing industrial
    IoT water management architectures and compared them with the proposed IIoWT architecture,
    which utilized different components. Baseline approaches, namely IoT-Water [13],
    [14], [15] and random IoT-Water (RIoT-Water) [16], [17], [18], [19], were considered
    in this evaluation. These baseline architectures were employed to manage water
    quality in smart cities’ drinking water distribution. The architectures comprised
    computing nodes, sensors, wireless networks, and raw material sources for water,
    all simulated in the study. The architecture played a crucial role in determining
    the flow of raw water, the ingredients present in it, and the additional ingredients
    added to the water plant. In the experiments, we implemented component calibration
    using multi-ANOVA testing techniques. The random data and solution were assumed
    to follow normal distributions. We compared IoT water architectures using different
    sensor datasets, namely T and Z. These sensors’ values were collected and presented
    in Table 3. In Fig. 3, the x-axis represents the sensor data values ranging from
    100 to 1200, while the y-axis displays the total processing time involved in receiving
    raw data at the power plant and processing it using the given dataset in the architectures.
    This figure illustrates the performance evaluation of the architectures when water,
    obtained from various sources, undergoes processing based on different inputs
    outlined in Table 3. Remarkably, the IIoWT architecture outperformed all existing
    water architectures regarding raw data processing. This superiority can be attributed
    to monitoring water flow at different intervals, considering the availability
    of water flow in various time zones. We can effectively minimize delays by adjusting
    the water flow and raw material mechanism during nighttime compared to daytime.
    Consequently, IIoWT demonstrated superior performance in reducing total delays,
    measured in seconds, compared to other existing architectures. Download : Download
    high-res image (204KB) Download : Download full-size image Fig. 3. Water plant
    trained and tested datasets processing total time (seconds). The powerful physical
    servers at the power plant retrieve data from various sources to transform raw
    water into drinkable form. These physical servers are solely responsible for executing
    all dataset tasks related to data modality, as depicted in Fig. 3. With minimal
    processing time, measured in seconds, these physical servers efficiently handle
    the data modality data, even when presented in different formats. In Fig. 3, the
    proposed IIoWT architecture surpasses all other existing water architectures in
    data modality processing while adhering to the established standards in distributed
    water management. In IIoWT, the training and testing time is adaptive and provides
    enhanced accuracy for various modalities. By employing a CNN, the architecture
    effectively extracts water ingredient features from sensors and utilizes the trained
    and tested data for decision-making regarding water level and quality management.
    As illustrated in Fig. 3, IIoWT demonstrates significantly improved training and
    testing validation times compared to existing IoT water and RIoT water architectures
    in physical and virtual server environments during the processing phase. We have
    leveraged digital twin technology to create virtual servers for managing distributed
    water processing. This innovation has fostered seamless collaboration among government
    entities, companies, and users, enabling the sharing of data and components. Instead
    of relying on physical servers in different places, we have opted for virtual
    servers in the distributed water system. These integrated virtual servers, which
    are real and constantly updated, ensure the sharing of up-to-date data. In summary,
    digital twin technology facilitates the smooth sharing of components and data,
    leading to improved collaboration, real-time monitoring, enhanced efficiency,
    and better decision-making through simulations and predictive analysis. The virtual
    servers in digital twins replicate the physical servers and can be easily scaled
    up or down during dataset processing. In Fig. 3, it is evident that the processing
    and data sharing time of the digital twin virtual servers is significantly reduced
    compared to physical servers. Consequently, the IIoWT architecture yields optimal
    results, and the successfully shared trained and tested models outperform existing
    IoT and RIoT water architectures. In addition to physical and virtual servers,
    the system incorporates other nodes, such as end-processing nodes. The end-user
    interface retrieves information about drinking water from various datasets, which
    include gradient details. End users can access a limited set of dataset sensors,
    specifically catering to incoming water data. These sensors provide parameters
    such as water cleanliness, pH, salt content, chlorine level, and other relevant
    factors. Such data points assist end users in evaluating the effectiveness of
    water in maintaining good health, aligning with predefined guidelines. Fig. 4
    showcases the IIoWT architecture, specifically designed with distinct data modalities
    for end users. This design reduces processing time and real-time data updates
    compared to IoT architectures. Consequently, end users receive timely and accurate
    information about drinking water quality. Download : Download high-res image (190KB)
    Download : Download full-size image Fig. 4. Water drinking quality ingredients
    at end users interfaces with optimal processing. Standardizing data is crucial
    for effective distributed water management in smart cities. We have implemented
    data standardization across various nodes, including physical servers, virtual
    servers, and end users. Maintaining data processing and sharing accuracy is fundamental
    to the IIoWT architecture. Data standardization ensures that data conforms to
    a predefined set of standards across different nodes while maintaining data privacy
    and security (as depicted in Fig. 4). To assess the accuracy of data standardization,
    we conducted evaluations using different IoT architectures and varied datasets,
    such as T and Z. Fig. 4 demonstrates that IIoWT achieves a high level of accuracy
    in data standardization and sharing, nearing 99.9%, surpassing existing architectures.
    This achievement can be attributed to the robust data validation processes and
    the interoperability of the architecture, which are optimized components of the
    IIoWT framework. Standardizing data has different data formats and distributed
    water management in smart cities. We have implemented data standardization with
    the given datasets on different nodes, including physical servers, virtual servers,
    and end users. Maintaining data processing and sharing accuracy is fundamental
    to the IIoWT architecture. Data standardization ensures that data conforms to
    a predefined set of standards across different nodes while maintaining data privacy
    and security (as depicted in Fig. 4). To assess the accuracy of data standardization,
    we conducted evaluations using different IoT architectures and varied datasets,
    such as T and Z. Fig. 4 demonstrates that IIoWT achieves a high level of accuracy
    in data standardization and sharing, nearing 95% to 99%, surpassing existing architectures.
    These optimal results and robustness in data validation processes are obtained
    due to the interoperability of the architecture, which are optimized components
    of the IIoWT framework. Interoperability is the seamless cross-platform in heterogeneous
    nodes, where technologies and protocols are suggested in existing IoT water architectures.
    The existing interoperate technologies, such as socket programming interface [48]
    and remote procedure call [49] were widely exploited in cross-platform water with
    heterogeneous nodes. These are client and server message and data broker call
    protocols, where different data formats and data standardization are followed
    by nodes. However, socket and RPC are slow when they travel and offload the different
    formats in IoT water architectures. Fig. 5 (a) shows the accuracy of data sharing
    with the different formats among physical servers and virtual servers during water
    monitoring and processing in the architectures. Fig. 5 (b) shows the data format
    standardization among physical and virtual and end users interface with the higher
    accuracy with the MQTT broker application programming interface. Fig. 5 shows
    the proposed MQTT interoperability with IIoWT architecture has more optimal results
    than socket and RPC protocols. The main reason is that the existing protocols
    only share and convert the data into one form, such as extensible markup language
    and java object notation protocols. Therefore, they lose their accuracy when different
    data formats are shared among nodes. Therefore, MQTT obtained the optimal accuracy
    as compared to existing protocols. The research incorporated blockchain validation
    at the end of the CNN-LSTM process. Data encryption and decryption were performed
    using SHA-256, and transaction validation relied on proof of work validation across
    different nodes. Fig. 6(a) illustrates the three security schemes employed in
    IoT water architectures. Specifically, the integration of CNN-LSTM security and
    machine learning (ML) blockchain was explored in IoT water management to analyze
    data security through blockchain technology. Download : Download high-res image
    (272KB) Download : Download full-size image Fig. 5. Accuracy of interoperability
    for water quality and level monitoring in physical, virtual and end users nodes.
    Download : Download high-res image (352KB) Download : Download full-size image
    Fig. 6. Data validation and data standardization water quality and level monitoring
    in physical, virtual and end users nodes based on blockchain. By integrating machine
    learning techniques, blockchain technology becomes more adaptable and efficient
    in validating data formats and hashes among nodes. Throughout the experiment,
    the transparency and immutability of shared data demonstrated higher accuracy
    when utilizing the CNN-LSTM-BCTD scheme. Fig. 6(a) showcases the blockchain validation
    process between virtual servers and physical servers after implementing the CNN-LSTM
    schemes on the data before offloading them to each other. The study involved executing
    1200 transactions per block, and the results displayed in Fig. 6(a) indicated
    that the CNN-LSTM-BCTD approach achieved an optimal accuracy of approximately
    99% compared to existing security and blockchain technologies. Moreover, the CNN-LSTM-BCTD
    method attained higher accuracy in data validation among servers and end-user
    interfaces, as depicted in Fig. 6(b). CNN-LSTM-BCTD achieves higher data validation
    among different nodes compared to existing studies due to several key factors.
    One crucial aspect is the validation process through multiple interrogations within
    the CNN-LSTM-BCTD framework. By leveraging this approach, the miner node has the
    capability to add and delete blockchain blocks, ensuring the integrity of the
    transactional data. An important feature integrated into the CNN-LSTM-BCTD approach
    is validating the ”vanish ingredient” transaction. This validation process involves
    verifying each node’s original and transactional data before transferring them
    to other nodes. By implementing this step, the proposed method significantly enhances
    data validation across different nodes. As a result, CNN-LSTM-BCTD offers superior
    data validation among various nodes compared to existing studies. Its rigorous
    validation process and inclusion of the ”vanish ingredient” transaction verification
    contribute to the higher data integrity and accuracy achieved in the proposed
    method. Finding and Limitation We presented the IIoWT architecture incorporating
    blockchain and digital twin technologies and processed the data modality using
    the CNN-LSTM approach. The main finding of IIoWT architecture is to examine the
    existing IoT water systems and discuss their limitations. We presented the simulation
    parameters and described the implementation of IIoWT. We discussed each deployment
    step in detail. Furthermore, we analyzed the obtained results, demonstrating the
    efficiency and effectiveness of the proposed IIoWT and CNN-LSTM-BCTD schemes for
    water management across various computing nodes. There are many limitations of
    the proposed IIoWT architecture. In our proposed IIoWT architecture, we did not
    consider the failure ratio of different components in the large and distributed
    setup. The work did not consider the motor’s energy and power consumption of computing
    nodes. Conclusion and future work In this paper, we have presented the IIoWT architecture
    incorporating blockchain and digital twin technologies and processed the data
    modality using the CNN-LSTM approach. We aimed to address the research challenges
    by formulating relevant research questions and highlighting the contributions
    of this study. We thoroughly examined the existing IoT water systems and discussed
    their limitations. We also provided a comprehensive overview of the problem formulation
    and proposed algorithm, detailing the steps and scenarios involved. In the evaluation
    section, we presented the simulation parameters and described the implementation
    of IIoWT. We discussed each deployment step in detail. Furthermore, we analyzed
    the obtained results, demonstrating the efficiency and effectiveness of the proposed
    IIoWT and CNN-LSTM-BCTD schemes for water management across various computing
    nodes. However, there is still considerable room for improvement in the existing
    IIoWT architecture. The current system faces challenges such as an increasing
    failure ratio of different components in the large and distributed setup. The
    processing cost has also become prohibitively high due to the numerous components
    involved. We will address these issues by considering new constraints such as
    fault tolerance, processing cost optimization, and data safety within the proposed
    IIoWT architecture. These improvements will enhance the reliability and efficiency
    of the system, ensuring its smooth operation in the future. We will also improve
    the motor’s energy and power consumption of computing nodes in the proposed architecture
    IIoWT. Code and data availability The study exploited the existing Amazon industrial
    IoT water management simulator and integrated the digital twin and blockchain
    to meet the requirements of data security and standardization in IIoWT. The source
    code is available on the following URL. https://github.com/aws-samples/water-tank-digital-twin.
    The study exploited water quality and water leakage datasets available on the
    following url. https://www.kaggle.com/code/miklgr500/leakage-detection-about-8-test-dataset/input.
    The water leakage dataset is available on the following URL. https://www.kaggle.com/datasets/adityakadiwal/water-potability.
    Funding This article was co-funded by the European Union under the REFRESH -Research
    Excellence For REgion Sustainability and High-tech Industries project number CZ.10.03.01/00/22_003/0000048
    via the Operational Programme Just Transition. Also, this work was supported by
    the Ministry of Education, Youth and Sports of the Chezk Republic conducted by
    VSB -Technical University of Ostrava, Czechia under Grants SP2023/039 and SP2023/042.
    CRediT author statement:Mazin:Idea Building,Abdullah: Methodology, Simulation,
    Karrar:Data curation,Khanapi:Writing- Original Manuscript preparation,Haydar:Visualization,Kadry:Simulation
    and Experiment Design,Jan and Radek: English Grammar and Editing, Begonya: Cloud
    Service Validation. Compliance with ethics requirements This article was supported
    by the Ministry of Education of the Czech Republic (Project No. SP2023/039 and
    No. SP2023/042). This article does not contain any studies involving human participants
    performed by any authors. This article does not contain any studies involving
    animals performed by any of the authors. CRediT authorship contribution statement
    Mazin Abed Mohammed: Conceptualization, Methodology, Formal analysis, Writing
    – Review & Editing. Abdullah Lakhan: Conceptualization, Methodology, Formal analysis,
    Writing – Review & Editing. Karrar Hameed Abdulkareem: Conceptualization, Methodology,
    Formal analysis, Writing – Review & Editing. Mohd Khanapi Abd Ghani: Formal analysis,
    Visualization, Supervision. Haydar Abdulameer Marhoon: Formal analysis, Visualization,
    Supervision. Seifedine Kadry: Formal analysis, Visualization, Supervision. Jan
    Nedoma: Project administration, Funding acquisition. Radek Martinek: Project administration,
    Funding acquisition. Begonya Garcia Zapirain: Formal analysis, Visualization,
    Supervision. Declaration of Competing Interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. References [1] A.C.D.S.
    Júnior, R. Munoz, M.D.L.Á. Quezada, A.V.L. Neto, M.M. Hassan, V.H.C. De Albuquerque
    ”Internet of water things: a remote raw water monitoring and control system IEEE
    Access, 9 (2021), pp. 35 790-35 800 Google Scholar [2] A. Suresh, M. Nandagopal,
    P. Raj, E. Neeba, J.-W. Lin Industrial IoT application architectures and use cases
    CRC Press (2020) Google Scholar [3] S. Khan, M. Altayar Industrial internet of
    things: Investigation of the applications, issues, and challenges Int J Adv Appl
    Sci, 8 (1) (2021), pp. 104-113 CrossRefView in ScopusGoogle Scholar [4] Sundaresan
    S, Kumar KS, Kumar TA, Ashok V, Julie EG. Blockchain architecture for intelligent
    water management system in smart cities. In: Blockchain for smart cities. Elsevier;
    2021. p. 57–80. Google Scholar [5] W. Xia, X. Chen, C. Song A framework of blockchain
    technology in intelligent water management Front Environ Sci, 10 (2022), p. 753
    Google Scholar [6] S. Bhattacharya, N. Victor, R. Chengoden, M. Ramalingam, G.C.
    Selvi, P.K.R. Maddikunta, P.K. Donta, S. Dustdar, R.H. Jhaveri, T.R. Gadekallu
    Blockchain for internet of underwater things: State-of-the-art, applications,
    challenges, and future directions Sustainability, 14 (23) (2022), p. 15659 CrossRefView
    in ScopusGoogle Scholar [7] Z.Y. Algamal, M.R. Abonazel, A.F. Lukman Modified
    jackknife ridge estimator for beta regression model with application to chemical
    data Int J Math, Stat, Comput Sci, 1 (2022), pp. 15-24 Google Scholar [8] A. Sasikumar,
    S. Vairavasundaram, K. Kotecha, V. Indragandhi, L. Ravi, G. Selvachandran, A.
    Abraham Blockchain-based trust mechanism for digital twin empowered industrial
    internet of things Future Gener Comput Syst, 141 (2023), pp. 16-27 View in ScopusGoogle
    Scholar [9] S. Suhail, R. Hussain, R. Jurdak, C.S. Hong Trustworthy digital twins
    in the industrial internet of things with blockchain IEEE Internet Comput, 26
    (3) (2021), pp. 58-67 Google Scholar [10] A. Sasikumar, S. Vairavasundaram, K.
    Kotecha, V. Indragandhi, L. Ravi, G. Selvachandran, A. Abraham Blockchain-based
    trust mechanism for digital twin empowered industrial internet of things Future
    Gener Comput Syst, 141 (2023), pp. 16-27 View in ScopusGoogle Scholar [11] X.
    Feng, J. Wu, Y. Wu, J. Li, W. Yang Blockchain and digital twin empowered trustworthy
    self-healing for edge-ai enabled industrial internet of things Inf Sci (2023),
    p. 119169 View PDFView articleView in ScopusGoogle Scholar [12] M. Manoj, V. Dhilip
    Kumar, M. Arif, E.-R. Bulai, P. Bulai, O. Geman State of the art techniques for
    water quality monitoring systems for fish ponds using iot and underwater sensors:
    A review Sensors, 22 (6) (2022), p. 2088 CrossRefView in ScopusGoogle Scholar
    [13] F. Jan, N. Min-Allah, D. Düştegör Iot based smart water quality monitoring:
    Recent techniques, trends and challenges for domestic applications Water, 13 (13)
    (2021), p. 1729 CrossRefView in ScopusGoogle Scholar [14] S. Pasika, S.T. Gandla
    Smart water quality monitoring system with cost-effective using iot Heliyon, 6
    (7) (2020), p. e04096 View PDFView articleView in ScopusGoogle Scholar [15] J.
    Huan, H. Li, F. Wu, W. Cao Design of water quality monitoring system for aquaculture
    ponds based on nb-iot Aquacult Eng, 90 (2020), p. 102088 View PDFView articleView
    in ScopusGoogle Scholar [16] A. Bhardwaj, V. Dagar, M.O. Khan, A. Aggarwal, R.
    Alvarado, M. Kumar, M. Irfan, R. Proshad Smart iot and machine learning-based
    framework for water quality assessment and device component monitoring Environ
    Sci Pollut Res, 29 (30) (2022), pp. 46 018-46 036 Google Scholar [17] F.J.J. Joseph,
    ”Iot based aquarium water quality monitoring and predictive analytics using parameter
    optimized stack lstm,” in 2022 6th International Conference on Information Technology
    (InCIT). IEEE, 2022, pp. 342–346. Google Scholar [18] S. Chopade, H.P. Gupta,
    R. Mishra, A. Oswal, P. Kumari, T. Dutta A sensors-based river water quality assessment
    system using deep neural network IEEE Internet of Things Journal, 9 (16) (2021),
    pp. 14 375-14 384 Google Scholar [19] H.H. Loc, Q.H. Do, A. Cokro, K.N. Irvine
    Deep neural network analyses of water quality time series associated with water
    sensitive urban design (wsud) features J Appl Water Eng Res, 8 (4) (2020), pp.
    313-332 CrossRefView in ScopusGoogle Scholar [20] G. Kaur, M. Braveen, S. Krishnapriya,
    S.G. Wawale, J. Castillo-Picon, D. Malhotra, J. Osei-Owusu, et al. Machine learning
    integrated multivariate water quality control framework for prawn harvesting from
    fresh water ponds J Food Qual, 2023 (2023) Google Scholar [21] U.W. Chohan, “Blockchain
    and environmental sustainability: Case of ibm’s blockchain water management”,
    Notes on the 21st Century (CBRI), 2019. Google Scholar [22] E.M. Dogo, A.F. Salami,
    N.I. Nwulu, and C.O. Aigbavboa, ”Blockchain and internet of things-based technologies
    for intelligent water management system,” Artificial intelligence in IoT, pp.
    129–150, 2019. Google Scholar [23] A. Poberezhna, ”Addressing water sustainability
    with blockchain technology and green finance,” in Transforming climate finance
    and green investment with blockchains. Elsevier, 2018, pp. 189–196. Google Scholar
    [24] Y. Zhang, W. Luo, F. Yu Construction of chinese smart water conservancy platform
    based on the blockchain: technology integration and innovation application Sustainability,
    12 (20) (2020), p. 8306 CrossRefGoogle Scholar [25] Y. Zhang, W. Luo, F. Yu Construction
    of chinese smart water conservancy platform based on the blockchain: technology
    integration and innovation application Sustainability, 12 (20) (2020), p. 8306
    CrossRefGoogle Scholar [26] S.J. Pee, J.H. Nans, J.W. Jans A simple blockchain-based
    peer-to-peer water trading system leveraging smart contracts Proceedings on the
    International Conference on Internet Computing (ICOMP). The Steering Committee
    of The World Congress in Computer Science, Computer? (2018), pp. 63-68 Google
    Scholar [27] H. Li, X. Chen, Z. Guo, J. Xu, Y. Shen, X. Gao Data-driven peer-to-peer
    blockchain framework for water consumption management Peer-to-Peer Network Appl,
    14 (2021), pp. 2887-2900 CrossRefView in ScopusGoogle Scholar [28] M. Pincheira,
    M. Vecchio, R. Giaffreda, and S.S. Kanhere, ”Exploiting constrained iot devices
    in a trustless blockchain-based water management system,” in 2020 IEEE International
    Conference on Blockchain and Cryptocurrency (ICBC). IEEE, 2020, pp. 1–7. Google
    Scholar [29] Y. Liu, C. Shang Application of blockchain technology in agricultural
    water rights trade management Sustainability, 14 (12) (2022), p. 7017 CrossRefView
    in ScopusGoogle Scholar [30] A. Predescu, D. Arsene, B. Pahon?u, M. Mocanu, and
    C. Chiru, “A serious gaming approach for crowdsensing in urban water infrastructure
    with blockchain support,” Applied Sciences, vol. 11, no. 4, p. 1449, 2021. Google
    Scholar [31] W. Xia, X. Chen, C. Song A framework of blockchain technology in
    intelligent water management Front Environ Sci, 10 (2022), p. 753 Google Scholar
    [32] B. Pahon?u, D. Arsene, A. Predescu, and M. Mocanu, ”Application and challenges
    of blockchain technology for real-time operation in a water distribution system,”
    in 2020 24th International Conference on System Theory, Control and Computing
    (ICSTCC). IEEE, 2020, pp. 739–744. Google Scholar [33] Y. Chang, J. Xu, K.Z. Ghafoor
    An iot and blockchain approach for the smart water management system in agriculture
    Scalable Comput: Pract Exp, 22 (2) (2021), pp. 105-116 View in ScopusGoogle Scholar
    [34] T. Li, Z. Wang, C. Wang, J. Huang, M. Zhou Chlorination in the pandemic times:
    The current state of the art for monitoring chlorine residual in water and chlorine
    exposure in air Sci Total Environ, 838 (2022), p. 156193 View PDFView articleView
    in ScopusGoogle Scholar [35] I.D. Joshi, D. Stramski, R.A. Reynolds, D.H. Robinson
    Performance assessment and validation of ocean color sensor-specific algorithms
    for estimating the concentration of particulate organic carbon in oceanic surface
    waters from satellite observations Remote Sens Environ, 286 (2023), p. 113417
    View PDFView articleView in ScopusGoogle Scholar [36] M. Cui, Y. Sun, C. Huang,
    M. Li Water turbidity retrieval based on uav hyperspectral remote sensing Water,
    14 (1) (2022), p. 128 CrossRefView in ScopusGoogle Scholar [37] C. Gao, C. Huang,
    J. Wang, Z. Li Modelling dynamic hydrological connectivity in the zoigê area (china)
    based on multi-temporal surface water observation Remote Sens, 14 (1) (2021),
    p. 145 CrossRefGoogle Scholar [38] L. Manjakkal, D. Szwagierczak, R. Dahiya Metal
    oxides based electrochemical ph sensors: Current progress and future perspectives
    Prog Mater Sci, 109 (2020), p. 100635 View PDFView articleView in ScopusGoogle
    Scholar [39] C. Patel, N. Doshi A novel mqtt security framework in generic iot
    model Proc Comput Sci, 171 (2020), pp. 1399-1408 View PDFView articleView in ScopusGoogle
    Scholar [40] S. Cong, Y. Zhou A review of convolutional neural network architectures
    and their optimizations Artif Intell Rev, 56 (3) (2023), pp. 1905-1969 CrossRefView
    in ScopusGoogle Scholar [41] A.A. Nasser, M.Z. Rashad, S.E. Hussein A two-layer
    water demand prediction system in urban areas based on micro-services and lstm
    neural networks IEEE Access, 8 (2020), pp. 147 647-147 661 Google Scholar [42]
    R. Fotohi, F.S. Aliee Securing communication between things using blockchain technology
    based on authentication and sha-256 to improving scalability in large-scale iot
    Comput Netw, 197 (2021), p. 108331 View PDFView articleView in ScopusGoogle Scholar
    [43] T.-H. Tan, C.-J. Huang, M. Gochoo, Y.-F. Chen Activity recognition based
    on fr-cnn and attention-based lstm network 2021 30th Wireless and Optical Communications
    Conference (WOCC), IEEE (2021), pp. 146-149 CrossRefView in ScopusGoogle Scholar
    [44] C.-C. Dong, M.-H. Tian, W.-C. Zhao Research on water quality prediction model
    of rf-softmax based on ga optimization Hubei Agric Sci, 61 (7) (2022), p. 60 Google
    Scholar [45] G. Di Gennaro, A. Buonanno, F.A. Palmieri Considerations about learning
    word2vec J Supercomput (2021), pp. 1-16 View in ScopusGoogle Scholar [46] Y. Yuan,
    L. Lin, Q. Liu, R. Hang, Z.-G. Zhou Sits-former: A pre-trained spatio-spectral-temporal
    representation model for sentinel-2 time series classification Int J Appl Earth
    Obs Geoinf, 106 (2022), p. 102651 View PDFView articleView in ScopusGoogle Scholar
    [47] I. Abuqaddom, B.A. Mahafzah, H. Faris Oriented stochastic loss descent algorithm
    to train very deep multi-layer neural networks without vanishing gradients Knowl-Based
    Syst, 230 (2021), p. 107391 View PDFView articleView in ScopusGoogle Scholar [48]
    Nida HS, Bura RO, Abdurahman. Multi-network transmission using socket programming
    to support command and control systems. Cyber physical, computer and automation
    system: a study of new technologies 2021;p. 59–68. Google Scholar [49] Zhao B,
    Wu W, Xu W. {NetRPC}: Enabling {In-Network} computation in remote procedure calls.
    In: 20th USENIX symposium on networked systems design and implementation (NSDI
    23); 2023. p. 199–217. Google Scholar Cited by (0) ☆ Peer review under responsibility
    of Cairo University. © 2023 The Authors. Published by Elsevier B.V. on behalf
    of Cairo University. Recommended articles Synergistic integration of digital twins
    and sustainable industrial internet of things for new generation energy investments
    Journal of Advanced Research, 2023 Gang Kou, …, Muhammet Deveci View PDF Fppsv-NHSS:
    Fuzzy parameterized possibility single valued neutrosophic hypersoft set to site
    selection for solid waste management Applied Soft Computing, Volume 140, 2023,
    Article 110273 Atiqe Ur Rahman, …, Radek Martinek View PDF TS HGRNet: A paradigm
    of two stream best deep learning feature fusion assisted framework for human gait
    analysis using controlled environment in smart cities Future Generation Computer
    Systems, Volume 147, 2023, pp. 292-303 Muhammad Attique Khan, …, Abdullah Alqatani
    View PDF Show 3 more articles Article Metrics Captures Readers: 11 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Journal of Advanced Research
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Industrial Internet of Water Things architecture for data standarization
    based on blockchain and digital twin technology☆
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Li L.
  - Wu F.
  - Cao Y.
  - Cheng F.
  - Wang D.
  - Li H.
  - Yu Z.
  - You J.
  citation_count: '12'
  description: While shale gas could complement the world's natural gas supply, its
    environmental tradeoffs and sustainability potential should be cautiously assessed
    before using it to satisfy future energy needs. Shale gas development in China
    is still in its infancy but has been progressing by the Central Government at
    a fast pace nowadays. Advanced experience from North America would greatly benefit
    sustainable design and decision-making for energy development in China. However,
    the lack of consistency concerning internal and external parameters among previous
    investigations does not allow an integrated impact comparison among shale gas-rich
    countries. Herein, we applied a meta-analysis to harmonize environmental tradeoff
    data through a comprehensive literature review. Greenhouse gas emission, water
    consumption, and energy demand were selected as environmental tradeoff indicators
    during shale gas production. Data harmonization suggested that environmental tradeoffs
    ranged from 5.6 to 37.4 g CO2-eq, 11.0–119.7 mL water, and 0.027–0.127 MJ energy
    to produce 1 MJ shale gas worldwide. Furthermore, sustainable development indexes
    (SDIs) for shale gas exploitation in China were analyzed and compared to the United
    States and the United Kingdom by considering environment, economy, and social
    demand through an analytic hierarchy process. The United States and China elicit
    higher SDIs than the United Kingdom, indicating higher feasibility for shale gas
    exploitation. Although China has relatively low scores in the environmental aspect,
    large reservoirs and high future market demand make Chinese shale gas favorable
    in the social demand aspect. Region-specific SDI characteristics identified among
    representative countries could improve the sustainability potential of regional
    development and global energy supply.
  doi: 10.1016/j.ese.2022.100202
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords List of abbreviations
    1. Introduction 2. Materials and methods 3. Results and discussion 4. Conclusions
    and perspectives Declaration of competing interest Acknowledgements Appendix A.
    Supplementary data References Show full outline Cited by (13) Figures (8) Show
    2 more figures Tables (1) Table 1 Extras (1) Multimedia component 1 Environmental
    Science and Ecotechnology Volume 12, October 2022, 100202 Original Research Sustainable
    development index of shale gas exploitation in China, the UK, and the US Author
    links open overlay panel Liang Li a, Fan Wu a b, Yuanyu Cao a, Fei Cheng a, Dali
    Wang a, Huizhen Li a, Zhiqiang Yu c, Jing You a b Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.ese.2022.100202 Get rights and content Under a
    Creative Commons license open access Highlights • Impacts of shale gas exploitation
    vary greatly among counties. • Environmental tradeoffs are harmonized for worldwide
    impact comparisons. • Environmental tradeoffs of China shale gas are higher than
    that of the US and the UK. • Environmental and socioeconomics are integrated to
    present sustainability potential. • China has intermittent sustainability potential
    due to high scores in social demand. Abstract While shale gas could complement
    the world''s natural gas supply, its environmental tradeoffs and sustainability
    potential should be cautiously assessed before using it to satisfy future energy
    needs. Shale gas development in China is still in its infancy but has been progressing
    by the Central Government at a fast pace nowadays. Advanced experience from North
    America would greatly benefit sustainable design and decision-making for energy
    development in China. However, the lack of consistency concerning internal and
    external parameters among previous investigations does not allow an integrated
    impact comparison among shale gas-rich countries. Herein, we applied a meta-analysis
    to harmonize environmental tradeoff data through a comprehensive literature review.
    Greenhouse gas emission, water consumption, and energy demand were selected as
    environmental tradeoff indicators during shale gas production. Data harmonization
    suggested that environmental tradeoffs ranged from 5.6 to 37.4 g CO2-eq, 11.0–119.7
    mL water, and 0.027–0.127 MJ energy to produce 1 MJ shale gas worldwide. Furthermore,
    sustainable development indexes (SDIs) for shale gas exploitation in China were
    analyzed and compared to the United States and the United Kingdom by considering
    environment, economy, and social demand through an analytic hierarchy process.
    The United States and China elicit higher SDIs than the United Kingdom, indicating
    higher feasibility for shale gas exploitation. Although China has relatively low
    scores in the environmental aspect, large reservoirs and high future market demand
    make Chinese shale gas favorable in the social demand aspect. Region-specific
    SDI characteristics identified among representative countries could improve the
    sustainability potential of regional development and global energy supply. Graphical
    abstract Download : Download high-res image (232KB) Download : Download full-size
    image Previous article in issue Next article in issue Keywords Unconventional
    natural gasLife cycle assessmentMeta-analysisAnalytic hierarchy processEnvironmental
    tradeoffs List of abbreviations AHP analytic hierarchy process bcf billion cubic
    feet bcm billion cubic meter D/P domestic natural gas demand versus production
    EC shale gas exploitation cost ED energy demand EUR estimated ultimate recovery
    FPW flowback and produced water HF-FPW hydraulic fracturing flowback and produced
    water FT future trends of natural gas demand GHG greenhouse gas HF hydraulic fracturing
    IP natural gas imported price LCA life cycle assessment mcf million cubic feet
    SDIs sustainable development indexes SP shale gas production volume SR shale gas
    reservoir tcf trillion cubic feet UK United Kingdom US United States WC water
    consumption 1. Introduction Since the transition from fossil fuel to renewable
    energy is unlikely to be substantially achieved within the short term [1], exploring
    substitute energy sources (such as natural gas) becomes a promising complement
    to the world''s energy shortage and benefits toward carbon neutrality. Shale gas
    is an unconventional natural gas. Sustainable exploitation of shale gas has the
    potential to adequately supply the continuously growing energy demand [2], stabilize
    the soaring gas prices for global economic security, and promote responsible energy
    strategy transformation for the next few decades [3]; however, opposition to the
    development of shale gas has been raised due to complicated environmental and
    socioeconomic implications [4,5]. Though there are expanding studies relating
    to the impacts of shale gas development in several areas, including climate change
    [6], water quality [7], water scarcity [8], ecosystem and human health [9], jobs
    [10], and energy prices [11], comprehensive impact assessments that leverage environmental
    and socioeconomic parameters are still lacking. The conditions for developing
    shale gas vary greatly among regions and nations, such as geological features,
    technology levels, market demands, and regulation systems, which could influence
    the sustainable development of shale gas. Thus, recognizing environmental tradeoffs
    and barriers that dictate the sustainability of shale gas exploitation among regions
    is vital to understanding the rapidly evolving worldwide energy landscape. Life
    cycle assessment (LCA) is an environmental tradeoff assessment tool that has been
    previously used to investigate macroscale environmental impacts of shale gas development
    during different operational stages, including drilling and hydraulic fracturing
    (HF) [12,13], transportation [14], and combustion for electricity and vehicles
    [15]. These studies suggest that the three most concerning life cycle environmental
    tradeoff categories are greenhouse gas (GHG) emission, water consumption (WC),
    and energy demand (ED) [16,17]. Laurenzi et al. [18] evaluated GHG emissions in
    Marcellus shales in the United States (US) and revealed that GHG emissions ranged
    from 62.8 to 79.1 g CO2-eq MJ−1, whereas Dale et al. [16] reported GHG emissions
    of 10.7 CO2-eq MJ−1 in the same location. Large variations were found due to different
    functional units and system boundaries applied. Comparatively, China''s GHG emissions
    ranged from 18.8 to 39.7 g CO2-eq MJ−1 [19,20]. Large variations noted within
    and between the two countries were likely due to different geographical characteristics,
    technologies, assumptions, comparison baselines, and other internal/external factors
    [[21], [22], [23]]. Despite an internationally recognized standard of practice
    [24,25], directly pooling LCA results for comparison inhibits fair evaluation
    and hinders common understandings in environmental tradeoffs of shale gas development.
    To minimize this inconsistency, Heath et al. [6] applied a meta-analysis to develop
    robust life cycle GHG emission comparisons among shale gas and other conventional
    fuel sources from production to end-use in the US. Their meta-analysis revealed
    that median estimates of GHG emissions from shale gas-generated electricity are
    similar to those from conventional natural gas and half the GHG emissions from
    coal [6]. However, WC and ED were ignored in the study by Heath et al., which
    hindered more comprehensive environmental tradeoffs analyses. In addition, the
    previous research was limited to comparing shale resources in the US only. Thus,
    systematic evaluations considering broader comparisons of other global shale resources
    are necessary to reveal environmental tradeoffs of shale gas exploitation. Few
    studies have considered the integrated implication of environment, economy and
    social demand perspectives when evaluating the sustainability potential of shale
    gas exploitation. Grecu et al. [26] assessed the sustainability of shale gas exploitation
    in Romania through a cost-benefit analysis, yet several important environmental
    impact parameters (such as GHG emission and ED) were ignored. Thomas et al. [5]
    deliberated the social benefits and impacts of shale gas and oil extraction in
    the US and the United Kingdom (UK), but environmental impacts were only confined
    to the hydraulic fracturing stage. Wang et al. [3,27,28] applied different models
    to compare the sustainability of the shale gas industry in different regions of
    China from technical and construction perspectives. The results demonstrated that
    high environmental impacts and core technical capability were the major obstacles
    to sustainable development. Taken together, it is imperative to systematically
    integrate major parameters to reflect the overall sustainability for shale gas
    development across important shale gas reservoirs. The present work aims to reveal
    life cycle environmental tradeoffs and identify major environmental and socioeconomical
    parameters that dictate the sustainability potential during shale gas exploitation.
    Data from environment, economy, and social demand aspects were integrated to evaluate
    the sustainability potential from three shale gas research-intensive countries
    (the US, the UK, and China). The system boundary of the present study was refined
    to shale gas exploitation for environmental tradeoff exploration (Fig. 1). GHG
    emission, WC, and ED data are first summarized through a comprehensive literature
    survey, and the meta-analysis is applied to holistically compare environmental
    impacts during shale gas exploitation. The harmonized database reduces the variability
    caused by inconsistent methods and assumptions among studies. Through this process,
    we can find the central tendency for each environmental impact category. Furthermore,
    sustainability development indexes (SDI) are generated to examine the feasibility
    and sustainability potential for shale gas in the US, the UK, and China through
    the analytic hierarchy process (AHP). Download : Download high-res image (392KB)
    Download : Download full-size image Fig. 1. System overview of shale gas development
    from cradle to grave. Solid lines indicate stages considered in the present analysis.
    Black dash arrows in the upstream stage indicate well recompletion. 2. Materials
    and methods 2.1. Literature review Based on keywords search on the Web of Science
    (“shale gas” AND “life cycle assessment” OR “shale gas” AND “LCA”), approximately
    300 articles were identified by December 1, 2021. After manual selection, life
    cycle GHG emission, WC, and ED of shale gas development were examined across 50
    shale gas cases from 42 articles since 2010. Over half of these studies were based
    in North America (the US and Canada), followed by China, the UK, Australia, and
    Spain. Original environmental impact data from the literature are summarized in
    the Supplemental Information (SI, Table S1). Our literature review identified
    that the US, the UK, and China are the primary countries where the life cycle
    environmental impacts of shale gas exploitation have been evaluated. Seven studies
    have investigated shale gas life cycle environmental impacts in Europe, and five
    of these were targeted in the UK. Whitelaw et al. [29] revealed the UK Bowland
    shale might contain 140 ± 55 trillion cubic feet (tcf) of shale gas, and Monaghan
    [30] suggested a total of 49.4–134.6 tcf gas in the UK Scotland Midland Valley.
    Both sites are considered large reservoirs in Europe. The decisions for shale
    gas exploration in the UK have been debated for years. Although the UK attempted
    shale gas exploitation within its territories, large-scale extraction has been
    banned since 2019. European countries, including the UK, have large natural gas
    demands; however, nearly half of their current natural gas supply relies on imports
    [31]. Sustainable shale gas exploitation may substantially relieve the energy
    demand in Europe. The US has the most advanced shale gas exploitation technology
    and a mature large-scale commercial development market. The US was the first country
    to start exploiting shale gas and possesses the fourth-largest shale gas reserves
    in the world. With their well-studied shale gas life cycle impacts, advanced experience
    from North America would greatly benefit sustainable design and decision-making
    for energy development in other countries. China contains the current world''s
    largest shale gas reserves. Due to recent incentive policies, shale gas is significant
    for ensuring China''s energy security and long-term stable economic development.
    Although China has a high potential for shale gas development, large-scale commercial
    exploitation is still under early-stage investigation, and the sustainability
    remains largely unknown. Inconsistent functional units for life cycle impact assessments
    prohibited a direct comparison among various studies. Among all, 31 studies used
    energy-based (1 kWh per MWh of electricity generated or 1 MJ per GJ shale gas
    produced) functional units, and the remaining articles used either mass-based
    or shale gas well-based functional units. Here, functional units were first normalized
    to “1 MJ shale gas extracted” to enable cross-comparison among studies. Conversion
    factors for different functional units are depicted in Table 1. Table 1. Conversion
    factors of different functional units to MJ shale gas. EUR: estimated ultimate
    recovery. Functional unit Conversion factor 1 kWh electricity 1 kWh electricity
    =  MJ shale gasa 1 well shale gas 1 well shale gas = 1 × EUR × Heat value MJ shale
    gasb 1 mcf or m3 shale gas 1 mcf or m3 shale gas = 1 × Heat value MJ shale gasb
    a Power efficiency is assumed to be 50% if not provided in the original study.
    b Heat value is assumed to be 1040 MJ per million cubic feet (mcf) or 36.73 MJ
    m−3 if it is not provided in the original study [32]. 2.2. Meta-analysis and data
    harmonization Meta-analysis aims to harmonize the life cycle environmental tradeoffs
    of the currently available studies, holistically compare impacts during shale
    gas exploitation, and obtain unified environmental parameter ranges to be integrated
    into AHP for sustainability assessment. System boundary refinement. Generally,
    shale gas development can be divided into upstream (production and processing),
    midstream (transmission and distribution), and downstream (end-use) stages (Fig.
    1). The upstream stage includes well site identification, well site preparation,
    design, drilling, casing, cementing, hydraulic fracturing, well completion, extraction,
    processing, and well abandonment. Midstream includes shale gas transmission, distribution,
    and storage. The end-use stage refers to power generation through a power plant
    and household combustion, automobile fuel, and industrial use. The major differences
    in environmental impacts between shale and conventional natural gas generally
    occur during the exploitation stage due to various technologies applied [33].
    In addition, the transmission and use phases of shale gas development could be
    considerably influenced by the modes and distances of transmission [14,34], as
    well as the efficiency of the end-use stage [6]. Thus, the system boundary of
    the current study was refined at the production and processing stage for environmental
    tradeoff exploration (Fig. 1). Harmonization steps. Many activities could contribute
    to environmental impacts. Various studies made different decisions about the inclusion
    and exclusion of activities within the life cycle of shale gas development. During
    the production phase, several activities such as well completion, well recompletion,
    and liquid unloading were highlighted as major contributors to the environmental
    burden, yet they were often ignored in previous studies [6]. Herein, these activities
    were included and harmonized to enable consistent comparisons. The following harmonization
    steps were performed sequentially. First, impacts of the end-use, transmission,
    and distribution phases were excluded from the refined system boundaries. Second,
    impacts associated with shale gas processing were added when they were not previously
    evaluated. Notably, when shale gas processing impacts were unavailable in certain
    cases, the country average was used instead. Third, emissions associated with
    well recompletion were adjusted. A well recompletion was adjusted based on the
    combined impacts of well completion and hydraulic fracturing and adjusted by the
    latest USEPA''s well recompletion frequency. Since the latest recompletion frequency
    was downward from 10% to 1% [35,36], environmental impacts from well recompletion
    were calculated using equation (1), (1) where Erc, Ec, and Ehf represent environmental
    impacts of well recompletion, original well completion, and hydraulic fracking,
    respectively. Additionally, GHG emission associated with liquid unloading was
    added. This step was applied to GHG emission only because liquid unloading has
    been shown as the main methane leakage source during shale gas production [37],
    but few studies demonstrated this phase would cause great WC and ED during shale
    gas production. The GHG emission from liquid unloading was adjusted by equation
    (2), (2) where 7.8 g CO2-eq kWh−1 is the predicted average GHG emission for liquid
    uploading. When EUR and well lifetime are unavailable, a median value for unconventional
    gas (2.2 bcf) was used here as the baseline EUR, and 30 years was used as the
    well lifetime. The number 3.6 is the unit conversation factor from kWh to MJ,
    and 51% is the powering efficiency suggested in the study by Heath et al. [6].
    Detailed harmonized values and harmonization procedures for each shale gas site
    are summarized in Table S2 and Table S3−S5, respectively. 2.3. Sustainable development
    analyses The triple bottom line of sustainability comprises three aspects: environment,
    economy, and society. We created an SDI to evaluate the sustainability potential
    of shale gas development in three representative shale gas reservoirs (the US,
    the UK, and China). Economy and social demand aspects were integrated with harmonized
    environmental impacts through the AHP. Unlike the environment category, economy
    and social demand were collected from official resources. These values usually
    represent the country''s average value in a specific year, which does not require
    further data harmonization. Economic parameters (including natural gas imported
    price and exploitation cost) from 2014 to 2020 were collected for each country,
    and domestic natural gas demand/supply ratio, shale gas production volume, shale
    gas reservoir, and potential growth rate from 2021 to 2025 were considered to
    project the social demand potential of shale gas in that country. The local hydrological
    and geological conditions could significantly affect shale gas production and
    exploitation costs; various production volume and cost data in different shale
    locations were collected to reflect possible variations. The AHP is a decision-making
    technique that uses pairwise comparisons based on a numerical scale to structure
    the decision-making process. As a method originally proposed for multi-objective
    and multi-criterion conflicting problems [38], AHP has become an effective tool
    for sustainable assessment in energy systems [39]. Fig. 2 presents the AHP''s
    hierarchical structure considered in the present work. The criteria were chosen
    and organized in a hierarchy structure descending from an overall goal to the
    main criteria, followed by sub-criteria levels. The main criteria consist of environment,
    economy, and social demand aspects. The subsequent level of the hierarchy consists
    of a series of sub-criteria related to the main criteria. Detailed AHP steps are
    explained in the SI (Section 1). AHP integrates sorted data to compare the sustainability
    of shale gas exploitation among different countries. Download : Download high-res
    image (219KB) Download : Download full-size image Fig. 2. Analytic hierarchy process
    structure for shale gas sustainable assessment. The sustainable development potential
    of shale gas exploitation is assessed by generating a sustainable development
    index (SDI). The lowest level of the hierarchy is three representative countries,
    including the US, the UK, and China. Despite many advantages of the AHP methodology,
    AHP partially reflects human thinking and experts’ opinion with some degree of
    uncertainty [40]. Herein, we constructed two scenarios with different emphases
    to minimize uncertainties associated with criteria weights through the AHP methodology
    using yaAHP (V12.8.8049). Detailed procedures and values for each discriminant
    matrix are presented in the SI (Section 1 and Table S9−S18). Consistency checks
    for both scenarios are presented in Tables S19 and S20. Scenario 1 is an environment-emphasized
    scenario in which the environment was set with a higher weight compared to economy
    and social demand (Table S9). Scenario 2 is a balanced scenario, and environment,
    economy, and social demand were assigned an equal weight (Table S10). Moreover,
    cumulative SDI variation subjects to the changes in weight score for each main
    criterion were analyzed. The SDI was regulated with full marks of 100. Higher
    SDI indicates more sustainable potential for shale gas exploitation in the study
    area. The relationship of each SDI with the sub-criterion was calculated using
    equations (3), (4). Eventually, each SDI was obtained by varying an individual
    input parameter while the remaining parameters held constant. (3) (4) Numbers
    in the two equations represent the rescaled weight factors (by a factor of 100)
    for sub-criteria based on the AHP under each scenario (Tables S17 and S18). Sub-criterion
    values are shown in Table S21. The best-case (best) represents the lowest values
    in GHG, WC, ED, natural gas imported price (IP), shale gas exploitation cost (EC),
    and domestic natural gas demand versus production (D/P), and the highest values
    in shale gas production volume (SP), shale gas reservoir (SR), and future trend
    of natural gas demand (FT). Differences in SDIs and main criteria among study
    regions were compared by one-way ANOVA using SigmaPlot (V14.0; Systat Software
    Inc., CA, USA). 2.4. Uncertainty and sensitivity Weight scores for main and sub-criteria
    are assigned based on a subjective judgment that inevitably introduces uncertainty
    to the SDI [40]. In addition, slight variations of the weight allocated might
    have a pronounced influence on the SDI. Here, appropriate probability distribution
    functions were fitted based on the software selection for each SDI and main criterion,
    and Anderson-Darling, Kolmogorov-Smirnov, and Chi-squared tests were used for
    normality testing. Based on the best-fitted probability distribution function
    (Beta, Weibull, Uniform, Log-normal, and Extreme value distribution), Monte-Carlo
    simulations were conducted to obtain 95th confidence intervals and other statistical
    parameters using Oracle Crystal Ball (V11, Decisioning, Denver, CO, US) for 10000
    runs. Sensitivity was also assessed as the percentage change of the average value
    of SDI by increasing 20% the weight of one main criterion while the weights of
    the other criteria were held constant. The purpose of the sensitivity analysis
    is to monitor how much influence one criterion has on the overall SDI, and as
    such, the key factor that influences the shale gas sustainable development can
    be identified. 3. Results and discussion 3.1. Literature review GHG emission:
    Among the 50 investigated shale gas cases, GHG emissions were analyzed in 32 cases.
    The production stage emits 0.1–44.8 g CO2-eq per MJ of shale gas, contributing
    up to 47% of the total GHG emission during the entire life stage (Fig. 3a). GHG
    emissions during transmission and distribution stages emit 0.5–27.5 g CO2-eq per
    MJ of shale gas. Fifteen cases assessed GHG emissions from production to end-use,
    and results suggest the end-use phase contributed predominantly to the overall
    GHG emission (11.0–60.1 g CO2-eq per MJ of shale gas), with more than 51% contribution
    during the shale gas life cycle. Download : Download high-res image (543KB) Download
    : Download full-size image Fig. 3. Environmental tradeoff comparisons among different
    shale gas development studies based on a systematic literature review. a, GHG
    emission (g CO2-eq per MJ of shale gas) [[15], [16], [17], [18], [19], [20], [21],23,33,[41],
    [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54],
    [55], [56], [57], [58]]; b, water consumption (mL per MJ of shale gas) [8,[16],
    [17], [18],20,46,47,49,51,52,[59], [60], [61], [62], [63], [64], [65], [66], [67],
    [68]]; c, energy demand (MJ per MJ of shale gas) [16,17,23,45,47,51,[69], [70],
    [71], [72]]. When a color is not shown in the figure for a site, it suggests the
    corresponding phase was not considered in the original research. Water consumption
    (WC): In the literature, 22 cases evaluated WC during shale gas development (Fig.
    3b). WC ranges from 2.6 to 313.1 mL per MJ of shale gas, with 2.6–118.4 mL per
    MJ of shale gas during shale gas production, and 68.1–263.9 mL water per MJ of
    shale gas during the end-use phase. Energy demand (ED): ED in the present study
    refers to the amount of energy consumed during exploitation to extract each energy
    unit of shale gas income (MJ cost per MJ of shale gas) [72]. Life cycle ED is
    an important index for revealing energy payback for project investment. Most current
    studies focused on ED during the shale gas production phase, ranging from 0.002
    to 0.149 MJ per MJ of shale gas (Fig. 3c). Collectively, environmental tradeoffs
    for shale gas development vary greatly among different studies. This variability
    is attributed to various external and internal reasons. Externally, life cycle
    environmental tradeoffs are substantially affected by the lack of consistent system
    boundaries. For instance, the end-use phase has been identified as the environmental
    hotspot during the shale gas life cycle, contributing more than 50% of GHG emissions
    and 60% of WC throughout the shale gas life cycle (Fig. 3a and b). During the
    end-use stage, direct release of CO2 into the atmosphere after shale gas combustion
    [73], consumption of steams, and cooling water for electricity generation are
    the major reasons for the high emissions [17,61], but these activities have been
    only considered in 33% of sorted studies. Furthermore, other activities, including
    well recompletion and liquid unloading, have also been largely ignored during
    the exploitation stage. Environmental tradeoffs of shale gas development could
    also be strongly affected by several internal factors. Electricity generation
    efficiency varies between 28% and 58% during the end-use phase [6], which is a
    confounding factor affecting corresponding environmental impacts [42,74]. For
    the transmission phase, modes and transportation distances are major confounding
    factors. For instance, liquefied natural gas chains have been found to generate
    significantly higher GHG emissions than pipeline chains due to extra liquefication
    processes for liquefied natural gas [34]. When considering the transmission of
    shale gas over distance, GHG emission ranges from 0.77 to 1.78 g CO2-eq GJ−1 km−1
    during pipeline transmission, and long-distance transmission inevitably generates
    high environmental burdens and operational costs [14]. During shale gas production,
    estimated ultimate recovery (EUR), drilling and fracturing technologies, and fugitive
    methane emission exert tremendous environmental tradeoffs. EUR indicates the cumulated
    shale gas production throughout the life span of a well. First, a higher EUR indicates
    that more shale gas will be produced with the same amount of energy being put
    into a well. As the most sensitive parameter to GHG emissions and ED of shale
    gas wells [18,71,73], EUR is subject to the geology, geomechanics, and petrophysical
    conditions of the formation, affecting the initial shale gas production, exploitation
    technology, and the decline of gas production over time [19,75,76]. Previous studies
    indicated the shale gas EUR ranged from 0.8 to 7.5 billion cubic feet (bcf) in
    China [19,67], 0.7–6.3 bcf in the US [55,61], and 0.1–44.5 bcf in the UK [33,48].
    The low- and high-end ranges of EUR were quite large, especially in the UK, introducing
    high uncertainty to environmental impact assessment. Secondly, drilling and fracturing
    represent the largest share of the total energy consumption and GHG emissions
    associated with diesel and material usage [23]. Drilling and hydraulic fracking
    technologies vary dramatically under different geological conditions, resulting
    in considerable deviations in environmental tradeoffs among various shale gas
    basins [13]. Hydraulic fracking has been reported to consume more than 80% of
    water resources during shale gas production [8,18], and many shale gas sites are
    located in water-scarce areas, potentially exacerbating regional water shortage
    [[77], [78], [79]]. As flowback and produced water (FPW) contains a variety of
    inorganic (e.g., salts, heavy metals, radionuclides) and organic (e.g., hydrocarbons,
    chemicals additives) contaminants [80,81], the treatment of FPW often requires
    a large quantity of clean water for dilution, exacerbating WC in shale gas exploitation
    sites [60]. Moreover, fugitive methane from shale gas production sites is a major
    uncertain source of GHG emissions, ranking as the 10th largest anthropogenic methane
    emission source in the US [82]. Research suggests that gas production processes
    involve 3.6–7.9% of methane leakage over the lifetime of a shale gas well [42].
    Lifetime GHG emissions from methane leakage range from 0.51 to 4.4 g CO2-eq MJ−1
    during the entire shale gas life cycle [42,43,73,83,84]. The average methane emission
    rate reaches 6 g h−1 per well in Canada and the US, depending on plugging status,
    well type, and region [82]. Most studies have considered the GHG emission by fugitive
    methane emissions but have omitted it after well abandonment [21,33,49]. Taken
    together, refining system boundaries and harmonizing unique features of shale
    gas exploitation sites could potentially decrease uncertainties and enable fair
    environmental tradeoff comparisons among regions. 3.2. Meta-analysis Meta-analysis
    was applied to minimize uncertainties and reveal environmental tradeoffs for shale
    gas exploitation. Table S6 summarizes harmonized results that were categorized
    by different regions through meta-analysis. GHG emission resulted in a 70–92%
    reduction after harmonization (Fig. 4a). Harmonized GHG emissions (CO2-eq per
    MJ of shale gas) are 12.4 (5.6–32.5) (median (range)) in the US, 9.2 (6.0–10.8)
    in the UK, 10.9 (6.7–12.3) in Canada, and 15.8 (7.2–23.5) in China (Table S6).
    The average GHG emission in China is relatively higher than in other countries.
    Download : Download high-res image (269KB) Download : Download full-size image
    Fig. 4. Environmental tradeoff comparisons between published original and harmonized
    results for shale gas exploitation among various countries. Original results (pink
    bars) were only adjusted for units. a, GHG emission (g CO2-eq per MJ of shale
    gas); b, water consumption (mL per MJ of shale gas); c, energy demand (MJ per
    MJ of shale gas). Harmonized results (blue bars) were adjusted using meta-analysis.
    Scattered dots next to each bar represent corresponding data points before (pink)
    and after (blue) harmonization. Harmonized WC is 11.1 (7.5–49.5), 79.9 (40.1–119.7),
    20.4, and 22.4 (10.0–83.0) mL per MJ of shale gas in the US, the UK, Canada, and
    China, respectively (Fig. 4b, Table S6). Meanwhile, harmonized ED range from 0.041
    (0.027–0.172) MJ per MJ of shale gas among all studied regions (Fig. 4c). Harmonized
    ED are 0.040 (0.027–0.172), 0.036, and 0.031–0.139 (0.048) MJ per MJ of shale
    gas in the US, the UK, and China, respectively (Table S6). All three environmental
    tradeoffs in China are found to be higher than those in the US during shale gas
    exploitation. This could be attributed to different geological features of shale
    gas sites and varying drilling and fracturing technologies applied [23,64]. Higher
    environmental impacts may occur during shale gas exploitation in China than in
    the US (Fig. 4). The deeper well depth in China could elevate higher environmental
    tradeoffs than in the US. Research has found that more than 65% of shale gas resources
    are buried at depths exceeding 3500 m in China, whereas the shale burial depths
    in the US are within 1500–3500 m [85,86]. Deeper wells require more diesel for
    powering drilling and fracturing equipment, oil-based drilling fluids, and more
    cement and casing utilization for well casing [23], which would increase both
    GHG emissions and energy required to extract the shale gas. Drilling technologies
    could also affect environmental tradeoffs. Most shale gas sites in the US use
    water-based fluid for well drilling. In China, however, both water and oil-based
    fluids are used due to the lack of techniques to maintain the stability of the
    wellbore [19,23]. Oil-based drilling fluid is used after the drill bit enters
    the desired geological formation, resulting in a large amount of diesel and oil
    consumption [23]. In addition, since GHG emission and energy usage from upstream
    material production account for a considerable proportion of the total GHG emission
    of shale gas development [19], emissions associated with diesel and oil combustion
    (for powering the drilling rig and fracturing fleet) and raw material inputs could
    result in higher GHG emissions in China than those in the US. The amount of fracturing
    water used for each well in China is generally higher than that in the US. Shi
    et al. [13] reported that volumes of fracturing fluid per meter were 20.15 m3
    m−1 and 23.97 m3 m−1 in Weiyuan and Fuling shale gas basin in China, respectively.
    Alternatively, less water utilization was found in the US, for example, 14.35
    m3 m−1 in Marcellus and 11.2 m3 m−1 in Barnett shale. In addition, almost all
    shale gas wells in China use slick-water fracturing, while many shale gas wells
    in the US consume a mixture of energized slick-water and gel slick-water for fracturing,
    which consumes less water [67,87]. Together, well length and fracturing water
    recipe are both critical for water consumption in shale gas exploitation. Previous
    studies reported that GHG emissions are similar between shale gas and conventional
    natural gas production [41,57,73,88,89]. However, the average WC for shale gas
    production (32.7 mL MJ−1) is much higher than the conventional natural gas production
    (9.3–9.6 mL MJ−1) [61]. Higher WC is primarily driven by hydraulic fracturing
    fluid used for shale gas production. Dale et al. [16] and Chen et al. [72] claimed
    that the higher ED for shale gas than conventional natural gas resulted from more
    resource inputs and auxiliary services desired. Nevertheless, three other studies
    drew contrary conclusions due to higher EUR in shale formation [23,45,71]. Taken
    together, environmental tradeoffs for shale gas may be higher than for conventional
    natural gas during production. However, the potential sustainability of shale
    gas exploitation should be evaluated systematically by integrating environmental
    and socioeconomic aspects. 3.3. Sustainability potential assessments The AHP was
    applied to construct sustainability assessments for shale gas production based
    on environment, economy, and social demand aspects. Fig. 5 shows functional relationships
    between each sub-criterion and the corresponding SDI to better illustrate possible
    SDI ranges and how sub-criterion indexes were constructed. With increasing environmental
    tradeoffs, costs, and demand/supply, the corresponding SDI decreases (Fig. 5a−f).
    Conversely, SDI increases linearly for three social indicators until reaching
    a plateau (Fig. 5g−i). The overall SDI for each country consists of the sum of
    nine sub-criterion indexes. Download : Download high-res image (585KB) Download
    : Download full-size image Fig. 5. Functional relationships between each sub-criterion
    and sustainable development indexes (SDIs) are plotted based on equations (3),
    (4). a, GHG emission; b, water consumption; c, energy demand; d, imported price;
    e, exploitation cost; f, natural gas demand/supply; g, shale gas production volume;
    h, shale gas reservoir; i, future trend. Vertical dash lines represent the maximum
    value (x) of each sub-criterion that could potentially achieve based on the current
    best scenarios (optimized data from all three countries were adopted). Environmental
    tradeoff data were collected from shale gas wells located in different regions
    within a country, and these data covered most available impact ranges of shale
    gas extraction in that country with different characteristics among regions. Country-level
    economic and social demand''s indicators were considered because these parameters
    generally remain similar within a country, especially in China following the central
    government policies. Together, 680400, 1680, and 370440 data points were integrated
    to show sustainability potential for the three analyzed countries (Fig. 6). In
    the environment-emphasized scenario 1, SDI scores for China (48.6 ± 6.3) are significantly
    higher than the UK (38.0 ± 3.3) but lower than the US (63.5 ± 8.9) (Fig. 6a) (Dunn''s
    test, p < 0.001). Environment and economy indexes in China are both significantly
    lower than those in the US, but the social demand index in China (13.4 ± 0.5)
    is higher than that in both the US (13.4 ± 0.5) and the UK (3.2 ± 0.1) (Fig. 6b).
    In scenario 2 (balanced case), although SDIs and main-criteria index values are
    modified due to the changes in assigned scores, the rank of SDIs for three countries
    remained consistent. Download : Download high-res image (286KB) Download : Download
    full-size image Fig. 6. Comparison of SDI and main-criterion index among the US,
    the UK, and China (n indicates the sample size generated through AHP): a–b, scenario
    1 (environment-emphasized); c–d, scenario 2 (balanced case). Error bars represent
    uncertainties of each index. n represents the sample size of SDI, which is obtained
    from different results calculated by equations (3), (4). Scattered dots next to
    each bar represent corresponding data points calculated by AHP under each main
    criterion for scenarios 1 (b) and 2 (d). In the present study, environment, economy,
    and social demand parameters were quantified from previous research, government
    reports, and official resources. Previous studies discussed the sustainability
    of shale gas exploitation considering many qualitative technological and political
    parameters, such as pipe network monopoly, local employment, regulatory system,
    local proved reserves, and market risks [27,90]. These parameters are either more
    focused on local-scale impact rather than national scale or considered subjective
    qualitative parameters, which are difficult to quantitatively compare among various
    countries and may increase subjectivity and uncertainty. Additionally, AHP partially
    reflects human thinking and experts’ opinion with some degrees of uncertainty.
    In scenario 1, higher weight was subjectively assigned to the environment criterion,
    whereas equal weights were given to environment, economy, and social demand in
    scenario 2. Since the SDI score is subject to the weight score of each criterion,
    to minimize the subjectivity and explore the sustainable development potential
    of shale gas exploitation under different weighting scenarios, the main-criteria
    weights varied to analyze all possible scenarios where SDI ranks shift for the
    three countries (Fig. S1). Green surfaces above the intersection lines (L1, L3,
    and L5) suggest China could have higher SDIs than the US (pink surfaces) under
    an extreme condition, where social demand should contribute over 67% of the total
    weight score. Similarly, green surfaces below the intersection lines (L2, L4,
    and L6) indicate that China may have lower SDIs than the UK (blue surfaces) under
    certain conditions. The SDI in the US is always higher than that in the UK since
    there is no intersection between pink and blue surfaces. The uncertainty of SDIs
    and main-criteria indexes are presented in Fig. S2−S25. Uncertainty analyses suggest
    that the variation of SDIs is dictated by environmental and economic parameters
    (Table S22−S27). Uncertainty analyses for scenario 1 revealed that the SDI value
    and 95% confident intervals are 63.3 (48.8–78.0), 37.8 (9.1–48.2), and 50.2 (40.0–61.1)
    for the US, the UK, and China, respectively (detailed information is shown in
    the SI, Tables S22–S27). In addition, all three parameters showed relatively similar
    sensitivity for the SDI in the US. Increasing 20% the weight of one main criterion
    led to a 3–12% change in the average value of SDIs. Environment and social demand
    were identified as the most sensitive SDI parameters for the UK and China, respectively
    (Fig. S26, Table S28). Based on Tables S29 and S30, each median sub-criterion
    index was calculated and then integrated into a spider graph to illustrate how
    individual sub-criteria dictate the overall sustainability potential of shale
    gas development in three countries (Fig. 7). The sustainable potential for the
    US (area covered in pink) is higher than that for the other two countries under
    both scenarios. Higher scores represent more advantages in the corresponding categories
    for each country. The US performs better than the other countries in WC, IP, EC,
    D/P, and SP. The UK leads in GHG and ED due to the lowest GHG emission and ED
    during shale gas exploitation, while the other sub-criteria for this country show
    the lowest performance. China displays great advantages in the SR and FT, with
    intermediate levels in most sub-criteria. The high social demand trend is more
    noticeable in scenario 2 for the balanced scenario. Download : Download high-res
    image (191KB) Download : Download full-size image Fig. 7. Sustainable development
    potential of shale gas exploitation in the US, the UK, and China based on median
    sub-criterion indexes in analytic hierarchy process: a, scenario 1; b, scenario
    2. GHG: greenhouse gas emission; WC: water consumption; ED: energy demand; IP:
    natural gas imported price; EC: shale gas exploitation cost; D/P: domestic natural
    demand versus production; SP: shale gas production volume; SR: shale gas reservoir;
    FT: future trends of natural gas demand. The US has a mature market and industry
    for shale gas exploitation. With an average annual growth rate of 12.7% from 2015
    to 2020, the shale gas production volume reached 805 bcm in 2020 for the US, much
    higher than China''s 20 bcm [31], which provided the US higher SDI score in the
    SP category. In addition, owing to their advanced and developed technologies,
    SDI scores for WC and EC are relatively high in the US. In China, the market demand
    for natural gas is rapidly growing at an annual rate of 7–9% by 2025 [91]. However,
    present natural gas production in China falls far behind market demand. Nowadays,
    natural gas in China still relies primarily on imports from other countries with
    relatively high import prices, which leads to low IP values. With a strong driving
    force of government support, the exploitation of shale gas is expected to decrease
    China''s dependence on natural gas imports. On the other hand, hydraulic fracturing
    for shale gas production has encountered opposition due to its potential environmental
    risks despite the huge economic benefit and employment depressurization. Although
    China has the largest shale gas reservoir worldwide [92], weak technology, along
    with complex geological structures, insufficient geological surveys, and water
    scarcity in China resulted in low production, high exploitation cost, and high
    environmental pollution in the past few years [90]. This led to higher environmental
    burdens during shale gas exploitation in China (Fig. 7). To change this situation,
    China has established an independent horizontal well drilling and completion technology
    system in recent years to enhance shale gas production yield [93]. The estimated
    production volume of shale gas has increased by 30% from 2019 to 2020, accounting
    for 10% of natural gas production in China in 2020 [94]. Comparatively, dry shale
    gas production in the US attributed to about 79% of total US dry natural gas production
    in 2020 [95]. Additionally, policy incentives, including financial subsidies,
    tax breaks, and technical support, are important factors for local and national
    shale gas producers [96]. Both China and the US have issued tax incentives and
    subsidies for shale gas explorations. The US issued a $0.014 per m3 of shale gas
    tax relief and a subsidy of $22.05 per ton oil equivalent for unconventional gas.
    Over $45 million per year in shale gas research and development have been invested
    from 2005 to 2015 [97]. In China, a two-year tax incentive was exempted for Chinese-foreign
    joint ventures in shale gas projects to promote international technical cooperation
    since 2011 [98]. The financial subsidies for shale gas exploitation from 2012
    to 2020 were approximately $0.03–0.06 per m3 of shale gas [99]. In addition, the
    construction of nation-level demonstration zones by breaking down barriers among
    businesses, advanced technologies, and best practices is an efficient way to accelerate
    shale gas development in China, and the Chinese shale gas production has increased
    from 4.6 billion cubic meters (bcm) to 20 bcm from 2015 to 2020 [31,96]. However,
    on November 2, 2019, the UK Government announced that it would take a presumption
    against issuing any further hydraulic fracturing consents in England. An annual
    survey commissioned by the Department for Business, Energy & Industrial Strategy
    in 2021 found that only 17% of the UK public supported fracking. This may extremely
    hinder shale gas development in the UK [100]. Taken together, based on the growing
    production volume, government promotion, and evolving drilling technologies, there
    remains a growing opportunity for shale gas production in China. It is worth mentioning
    that shale gas production not only consumes a significant amount of water resources
    but also causes severe water contamination due to the lack of proper management
    of hydraulic fracturing flowback and produced water (HF-FWP) which returns to
    the surface after fracturing activity. HF-FPW is a complex, tripartite mixture
    of injected HF fluid components, deep formation water, and secondary byproducts
    of downhole reactions with the formation environment [101,102]. Although our work
    did not consider potential water contamination, due to the limited knowledge of
    the chemical contents and lacks standardized treatment technologies, this could
    become an increasingly stringent issue facing shale developers and determining
    whether shale development would be allowed in many countries. Shale oil (also
    known as tight oil) is oil embedded in low-permeable shale, sandstone, and carbonate
    rock formations. Although shale oil may have great potential to revolutionize
    the traditional oil industry, the development of shale oil in China is still in
    its very infant stage and has not been widely exploited yet [103]. Comparatively,
    the US EIA estimates that in 2021, about 2.64 billion barrels (or about 7.22 million
    barrels per day) of crude oil will be produced directly from tight oil resources
    in the US. This was about 65% of total US crude oil production in 2021 [104].
    Although shale oil extraction also relies on hydraulic fracturing technologies,
    this study only considered the sustainability of shale gas production, and all
    data utilized in the present study focused on shale gas exploitation rather than
    shale oil. 4. Conclusions and perspectives Understanding the limitations of current
    knowledge, recent research regarding life cycle environmental tradeoffs of shale
    gas has come to different conclusions due to varied system boundaries, geological
    features, and exploitation technologies. Through meta-analyses, we have developed
    a consistent and robust foundation regarding three major life cycle environmental
    tradeoffs from shale gas exploitation in various countries. Furthermore, major
    environmental and socioeconomic characteristics dictate the sustainability of
    nationwide assessment of shale gas exploitation, which varies among countries.
    There are higher environmental tradeoffs in China than in the US during shale
    gas exploitation. However, the current assessment suggests that a high potential
    for sustainable shale gas production remains in China under certain conditions.
    As the largest shale gas reservoir worldwide, China is motivated to develop shale
    gas processes due to the urgency in energy strategy transformation and to meet
    the carbon peak and neutrality pledge. Research predicts that the Chinese government
    will increase the mixed grid of non-fossil fuels and natural gas from 23% to 45%
    by 2030. Future technology innovation could increase EUR and decrease the extraction
    costs, providing robust and substantial long-term sustainable paybacks. The present
    work defines uncertainties of environmental tradeoffs and demonstrates the sustainability
    tendency of shale gas development in different countries under multiple levels.
    Although our large-scale comparisons mainly focus on nation-level sustainability
    assessment, local governments could also consider the regional specific environmental,
    economic and social factors to develop locally-based standards towards achieving
    sustainable energy supply, a global issue with regional solutions. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgements This work is supported by the National
    Key Research and Development Program of China (2019YFC1805501), the National Natural
    Science Foundation of China (42107286 and U1901220), the Guangdong Basic and Applied
    Basic Research Foundation (2020A1515110788), the Guangzhou Basic and Applied Basic
    Research Project (202102020681), and the Innovative Research Team of the Department
    of Education of Guangdong Province (2020KCXTD005). We thank Anton Jamieson for
    language editing. Appendix A. Supplementary data The following is the Supplementary
    data to this article: Download : Download Word document (18MB) Multimedia component
    1. References [1] J. Tollefson Can the world kick its fossil-fuel addiction fast
    enough Nature, 556 (7702) (2018), pp. 422-425 CrossRefView in ScopusGoogle Scholar
    [2] H. Yang, J.R. Thompson Shale gas is a fraught solution to emissions Nature,
    513 (7518) (2014) 315-315 Google Scholar [3] Q. Wang, S. Li Shale gas industry
    sustainability assessment based on WSR methodology and fuzzy matter-element extension
    model: the case study of China J. Clean. Prod., 226 (2019), pp. 336-348 View PDFView
    articleCrossRefView in ScopusGoogle Scholar [4] E.N. Mayfield, J.L. Cohon, N.Z.
    Muller, I.M. Azevedo, A.L. Robinson Cumulative environmental and employment impacts
    of the shale gas boom Nat. Sustain., 2 (12) (2019), pp. 1122-1131 CrossRefView
    in ScopusGoogle Scholar [5] M. Thomas, T. Partridge, B.H. Harthorn, N. Pidgeon
    Deliberating the perceived risks, benefits, and societal implications of shale
    gas and oil extraction by hydraulic fracturing in the US and UK Nat. Energy, 2
    (5) (2017), pp. 1-7 View in ScopusGoogle Scholar [6] G.A. Heath, P. O''Donoughue,
    D.J. Arent, M. Bazilian Harmonization of initial estimates of shale gas life cycle
    greenhouse gas emissions for electric power generation Proc. Natl. Acad. Sci.
    U.S.A., 111 (31) (2014), pp. E3167-E3176 View in ScopusGoogle Scholar [7] R.D.
    Vidic, S.L. Brantley, J.M. Vandenbossche, D. Yoxtheimer, J.D. Abad Impact of shale
    gas development on regional water quality Science, 340 (2013), p. 6134 Google
    Scholar [8] M. Jiang, C.T. Hendrickson, J.M. VanBriesen Life cycle water consumption
    and wastewater generation impacts of a Marcellus shale gas well Environ. Sci.
    Technol., 48 (3) (2014), pp. 1911-1920 CrossRefView in ScopusGoogle Scholar [9]
    B.W. Allred, W.K. Smith, D. Twidwell, J.H. Haggerty, S.W. Running, D.E. Naugle,
    S.D. Fuhlendorf Ecosystem services lost to oil and gas in North America Science,
    348 (6233) (2015), pp. 401-402 CrossRefView in ScopusGoogle Scholar [10] D. Paredes,
    T. Komarek, S. Loveridge Income and employment effects of shale gas extraction
    windfalls: evidence from the Marcellus region Energy Econ., 47 (2015), pp. 112-120
    View PDFView articleView in ScopusGoogle Scholar [11] J.G. Weber The effects of
    a natural gas boom on employment and income in Colorado, Texas, and Wyoming Energy
    Econ., 34 (5) (2012), pp. 1580-1588 View PDFView articleView in ScopusGoogle Scholar
    [12] W. Lin, A.M. Bergquist, K. Mohanty, C.J. Werth Environmental impacts of replacing
    slickwater with low/no-water fracturing fluids for shale gas recovery ACS Sustain.
    Chem. Eng., 6 (6) (2018), pp. 7515-7524 CrossRefView in ScopusGoogle Scholar [13]
    W. Shi, X. Wang, M. Guo, Y. Shi, A. Feng, R. Liang, A. Raza Water use for shale
    gas development in China''s Fuling shale gas field J. Clean. Prod., 256 (2020),
    Article 120680 View PDFView articleView in ScopusGoogle Scholar [14] G. Di Lullo,
    A.O. Oni, E. Gemechu, A. Kumar Developing a greenhouse gas life cycle assessment
    framework for natural gas transmission pipelines J. Nat. Gas Sci. Eng., 75 (2020),
    Article 103136 View PDFView articleView in ScopusGoogle Scholar [15] C. Clark,
    J. Han, A. Burnham, J. Dunn, M. Wang Life-cycle Analysis of Shale Gas and Natural
    Gas Argonne National Laboratory, Argonne, IL, United States (2012) Google Scholar
    [16] A.T. Dale, V. Khanna, R.D. Vidic, M.M. Bilec Process based life-cycle assessment
    of natural gas from the Marcellus Shale Environ. Sci. Technol., 47 (10) (2013),
    pp. 5459-5466 CrossRefView in ScopusGoogle Scholar [17] J. Gao, F. You Design
    and optimization of shale gas energy systems: overview, research challenges, and
    future directions Comput. Chem. Eng., 106 (2017), pp. 699-718 View PDFView articleView
    in ScopusGoogle Scholar [18] I.J. Laurenzi, G.R. Jersey Life cycle greenhouse
    gas emissions and freshwater consumption of Marcellus shale gas Environ. Sci.
    Technol., 47 (9) (2013), pp. 4896-4903 CrossRefView in ScopusGoogle Scholar [19]
    X. Li, Hongmin Mao, Yongsong Ma, Bing Wang, Wenshi Liu, Wenjia Xu Life cycle greenhouse
    gas emissions of China shale gas Resour. Conserv. Recycl., 152 (2020), Article
    104518 View PDFView articleView in ScopusGoogle Scholar [20] Y. Chen, J. Li, H.
    Lu, J. Xia Tradeoffs in water and carbon footprints of shale gas, natural gas,
    and coal in China Fuel, 263 (2020), Article 116778 View PDFView articleView in
    ScopusGoogle Scholar [21] D. Costa, B. Neto, A.S. Danko, A. Fiuza Life cycle assessment
    of a shale gas exploration and exploitation project in the province of Burgos,
    Spain Sci. Total Environ., 645 (2018), pp. 130-145 View PDFView articleView in
    ScopusGoogle Scholar [22] D.S. Mallapragada, E. Reyes-Bastida, F. Roberto, E.M.
    McElroy, D. Veskovic, I.J. Laurenzi Life cycle greenhouse gas emissions and freshwater
    consumption of liquefied Marcellus shale gas used for international power generation
    J. Clean. Prod., 205 (2018), pp. 672-680 View PDFView articleView in ScopusGoogle
    Scholar [23] J. Wang, M. Liu, B.C. McLellan, X. Tang, L. Feng Environmental impacts
    of shale gas development in China: a hybrid life cycle analysis Resour. Conserv.
    Recycl., 120 (2017), pp. 38-45 View PDFView articleCrossRefGoogle Scholar [24]
    ISO 14040: Environmental Management — Life Cycle Assessment — Principles and Framework
    (2006) Google Scholar [25] ISO 14044: Environmental Management — Life Cycle Assessment
    — Requirements and Guidelines (2006) Google Scholar [26] E. Grecu, M.I. Aceleanu,
    C.T. Albulescu The economic, social and environmental impact of shale gas exploitation
    in Romania: a cost-benefit analysis Renew. Sustain. Energy Rev., 93 (2018), pp.
    691-700 View PDFView articleView in ScopusGoogle Scholar [27] Q. Wang, L. Zhan
    Assessing the sustainability of the shale gas industry by combining DPSIRM model
    and RAGA-PP techniques: an empirical analysis of Sichuan and Chongqing, China
    Energy, 176 (2019), pp. 353-364 View PDFView articleView in ScopusGoogle Scholar
    [28] Q. Wang, X. Yang Evaluating the potential for sustainable development of
    China''s shale gas industry by combining multi-level DPSIR framework, PPFCI technique
    and RAGA algorithm Sci. Total Environ., 780 (2021), Article 146525 View PDFView
    articleView in ScopusGoogle Scholar [29] P. Whitelaw, C.N. Uguna, L.A. Stevens,
    W. Meredith, C.E. Snape, C.H. Vane, V. Moss-Hayes, A.D. Carr Shale gas reserve
    evaluation by laboratory pyrolysis and gas holding capacity consistent with field
    data Nat. Commun., 10 (1) (2019), pp. 1-10 View in ScopusGoogle Scholar [30] A.
    Monaghan The Carboniferous Shales of the Midland Valley of Scotland: Geology and
    Resource Estimation (2014) Google Scholar [31] BP, Statistical Review of World
    Energy 2021 | 70th edition. (Accessed 28 June 2022). Google Scholar [32] M. Wang
    The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (Greet)
    Model (2018) https://greet.es.anl.gov/, Accessed 8th Oct 2021 Google Scholar [33]
    J. Cooper, L. Stamford, A. Azapagic Environmental impacts of shale gas in the
    UK: current situation and future scenarios Energy Technol., 2 (12) (2014), pp.
    1012-1026 CrossRefView in ScopusGoogle Scholar [34] K. Shaton, A. Hervik, H.M.
    Hjelle The environmental footprint of natural gas transportation: LNG vs. pipeline
    Econ. Energy Environ. Policy, 9 (1) (2020) Google Scholar [35] U.S. Environmental
    Protection Agency (EPA Oil and natural gas sector: new source performance standards
    and national emission standards for hazardous air pollutants reviews Final rule.
    Fed. Regist., 77 (159) (2012) 49,490-449,600 Google Scholar [36] U.S. Environmental
    Protection Agency (EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks: 1990–2009
    (2011) accessed 1 June 2022) Google Scholar [37] T. Shires, M. Lev-On Characterizing
    Pivotal Sources of Methane Emissions from Unconventional Natural Gas Production:
    Summary and Analysis of API and ANGA Survey Responses Final Report (2012) Google
    Scholar [38] T.L. Saaty Decision making with the analytic hierarchy process Int.
    J. Serv. Sci., 1 (1) (2008), pp. 83-98 CrossRefGoogle Scholar [39] E. Strantzali,
    K. Aravossis Decision making in renewable energy investments: a review Renew.
    Sustain. Energy Rev., 55 (2016), pp. 885-898 View PDFView articleView in ScopusGoogle
    Scholar [40] A. Król-Badziak, S.H. Pishgar-Komleh, S. Rozakis, J. Księżak Environmental
    and socio-economic performance of different tillage systems in maize grain production:
    application of life cycle assessment and multi-criteria decision making J. Clean.
    Prod., 278 (2021), Article 123792 View PDFView articleView in ScopusGoogle Scholar
    [41] T. Stephenson, J.E. Valle, X. Riera-Palou Modeling the relative GHG emissions
    of conventional and shale gas production Environ. Sci. Technol., 45 (24) (2011),
    pp. 10757-10764 CrossRefView in ScopusGoogle Scholar [42] R.W. Howarth, R. Santoro,
    A. Ingraffea Methane and the greenhouse-gas footprint of natural gas from shale
    formations Clim. Change, 106 (4) (2011), pp. 679-690 CrossRefView in ScopusGoogle
    Scholar [43] M. Jiang, W. Michael Griffin, C. Hendrickson, P. Jaramillo, J. VanBriesen,
    A. Venkatesh Life cycle greenhouse gas emissions of Marcellus shale gas Environ.
    Res. Lett., 6 (3) (2011), Article 034014 CrossRefView in ScopusGoogle Scholar
    [44] Q. Chen, J.B. Dunn, D.T. Allen Greenhouse gas emissions of transportation
    fuels from shale gas-derived natural gas liquids Procedia CIRP, 80 (2019), pp.
    346-351 View PDFView articleView in ScopusGoogle Scholar [45] H. Yaritani, J.
    Matsushima Analysis of the energy balance of shale gas development Energies, 7
    (4) (2014), pp. 2207-2227 CrossRefGoogle Scholar [46] A.C. Brown, A. Korre, Z.
    Nie A life cycle assessment model development of CO2 emissions and water usage
    in shale gas production Energy Proc., 114 (2017), pp. 6579-6587 View PDFView articleView
    in ScopusGoogle Scholar [47] R. Wilkins, A.H. Menefee, A.F. Clarens Environmental
    life cycle analysis of water and CO2-based fracturing fluids used in unconventional
    gas production Environ. Sci. Technol., 50 (23) (2016), pp. 13134-13141 CrossRefView
    in ScopusGoogle Scholar [48] L. Stamford, A. Azapagic Life cycle environmental
    impacts of UK shale gas Appl. Energy, 134 (2014), pp. 506-518 View PDFView articleView
    in ScopusGoogle Scholar [49] C. Tagliaferri, R. Clift, P. Lettieri, C. Chapman
    Shale gas: a life-cycle perspective for UK production Int. J. Life Cycle Assess.,
    22 (6) (2016), pp. 919-937 Google Scholar [50] Y. Qin, R. Edwards, F. Tong, D.L.
    Mauzerall Can switching from coal to shale gas bring net carbon reductions to
    China? Environ. Sci. Technol., 51 (5) (2017), pp. 2554-2562 CrossRefView in ScopusGoogle
    Scholar [51] Y. Chang, R. Huang, R.J. Ries, E. Masanet Life-cycle comparison of
    greenhouse gas emissions and water consumption for coal and shale gas fired power
    generation in China Energy, 86 (2015), pp. 335-343 View PDFView articleView in
    ScopusGoogle Scholar [52] S. Zeng, J. Gu, S. Yang, H. Zhou, Y. Qian Comparison
    of techno-economic performance and environmental impacts between shale gas and
    coal-based synthetic natural gas (SNG) in China J. Clean. Prod., 215 (2019), pp.
    544-556 View PDFView articleView in ScopusGoogle Scholar [53] R. Raj, S. Ghandehariun,
    A. Kumar, M. Linwei A well-to-wire life cycle assessment of Canadian shale gas
    for electricity generation in China Energy, 111 (2016), pp. 642-652 View PDFView
    articleView in ScopusGoogle Scholar [54] S. Bista, P. Jennings, M. Anda Comprehensive
    environmental impacts and optimization of onshore shale gas development and delivery
    Energy Technol., 7 (5) (2019), Article 1800871 View in ScopusGoogle Scholar [55]
    M. Ahsan, A. Korre, S. Durucan, Z. Nie Geological Life Cycle Inventory Model Development
    for Shale Gas Resources. 14th Greenhouse Gas Control Technologies Conference (2018),
    pp. 21-26 Melbourne CrossRefGoogle Scholar [56] M. Hauck, A. Ait Sair, Z. Steinmann,
    A. Visschedijk, D. O''Connor, H. Denier van der Gon Future European shale gas
    life-cycle GHG emissions for electric power generation in comparison to other
    fossil fuels Carbon Manag., 10 (2) (2019), pp. 163-174 CrossRefView in ScopusGoogle
    Scholar [57] T.J. Skone, J. Robert Life Cycle Analysis: Natural Gas Combined Cycle
    (NGCC) Power Plants National Energy Technology Laboratory (NETL), United States
    (2012) 2012 Google Scholar [58] G. Heath, J. Meldrum, N. Fisher, D. Arent, M.
    Bazilian Life cycle greenhouse gas emissions from Barnett Shale gas used to generate
    electricity J. Unconv. Oil Gas Resour., 8 (2014), pp. 46-55 View PDFView articleView
    in ScopusGoogle Scholar [59] E. Grubert, S. Kitasei How Energy Choices Affect
    Fresh Water Supplies: a Comparison of US Coal and Natural Gas Worldwatch Institute.
    Natural Gas and Sustainable Energy Initiative (2010) Google Scholar [60] S.M.
    Absar, A.-M. Boulay, M.F. Campa, B.L. Preston, A. Taylor The tradeoff between
    water and carbon footprints of Barnett shale gas J. Clean. Prod., 197 (2018),
    pp. 47-56 View PDFView articleView in ScopusGoogle Scholar [61] C.E. Clark, R.M.
    Horner, C.B. Harto Life cycle water consumption for shale gas and conventional
    natural gas Environ. Sci. Technol., 47 (20) (2013), pp. 11829-11836 CrossRefView
    in ScopusGoogle Scholar [62] S. Goodwin, K. Carlson, K. Knox, C. Douglas, L. Rein
    Water intensity assessment of shale gas resources in the wattenberg field in northeastern
    Colorado Environ. Sci. Technol., 48 (10) (2014), pp. 5991-5995 CrossRefView in
    ScopusGoogle Scholar [63] C. Tagliaferri, P. Lettieri, C. Chapman Life cycle assessment
    of shale gas in the UK Energy Proc., 75 (2015), pp. 2706-2712 View PDFView articleView
    in ScopusGoogle Scholar [64] X. Wu, J. Xia, B. Guan, P. Liu, L. Ning, X. Yi, L.
    Yang, S. Hu Water scarcity assessment based on estimated ultimate energy recovery
    and water footprint framework during shale gas production in the Changning play
    J. Clean. Prod., 241 (2019), Article 118312 View PDFView articleView in ScopusGoogle
    Scholar [65] R. Liu, J. Wang, L. Lin Water scarcity footprint assessment for China''s
    shale gas development Extr. Ind. Soc., 8 (2) (2021), Article 100892 View PDFView
    articleView in ScopusGoogle Scholar [66] B. Ali, A. Kumar Development of life
    cycle water footprints for gas-fired power generation technologies Energy Convers.
    Manag., 110 (2016), pp. 386-396 View PDFView articleView in ScopusGoogle Scholar
    [67] J. Wang, M. Liu, Y. Bentley, L. Feng, C. Zhang Water use for shale gas extraction
    in the Sichuan Basin, China J. Environ. Manag., 226 (2018), pp. 13-21 View PDFView
    articleCrossRefGoogle Scholar [68] X. Xie, T. Zhang, M. Wang, Z. Huang Impact
    of shale gas development on regional water resources in China from water footprint
    assessment view Sci. Total Environ., 679 (2019), pp. 317-327 View PDFView articleView
    in ScopusGoogle Scholar [69] N. Lior Exergy, energy, and gas flow analysis of
    hydrofractured shale gas extraction J. Energy Resour. Technol., 138 (6) (2016)
    Google Scholar [70] D. Moeller, D. Murphy Net energy analysis of gas production
    from the Marcellus shale Biophys. Econ. Resour. Qual., 1 (1) (2016) Google Scholar
    [71] M.L. Aucott, J.M. Melillo A preliminary energy return on investment analysis
    of natural gas from the Marcellus shale J. Ind. Ecol., 17 (5) (2013), pp. 668-679
    CrossRefView in ScopusGoogle Scholar [72] Y. Chen, L. Feng, S. Tang, J. Wang,
    C. Huang, M. Höök Extended-exergy based energy return on investment method and
    its application to shale gas extraction in China J. Clean. Prod., 260 (2020),
    Article 120933 View PDFView articleView in ScopusGoogle Scholar [73] A. Burnham,
    J. Han, C.E. Clark, M. Wang, J.B. Dunn, I. Palou-Rivera Life-cycle greenhouse
    gas emissions of shale gas, natural gas, coal, and petroleum Environ. Sci. Technol.,
    46 (2) (2012), pp. 619-627 CrossRefView in ScopusGoogle Scholar [74] G.A.H. Michael
    Whitaker, Patrick O''Donoughue, Vorum Martin Life cycle greenhouse gas emissions
    of coal-fired electricity generation: systematic review and harmonization J. Ind.
    Ecol., 16 (2012), pp. S53-S72 Google Scholar [75] P. De Silva, S. Simons, P. Stevens
    Economic impact analysis of natural gas development and the policy implications
    Energy Pol., 88 (2016), pp. 639-651 View PDFView articleView in ScopusGoogle Scholar
    [76] C. Sun, H. Nie, W. Dang, Q. Chen, G. Zhang, W. Li, Z. Lu Shale gas exploration
    and development in China: current status, geological challenges, and future directions
    Energy Fuels, 35 (8) (2021), pp. 6359-6379 CrossRefView in ScopusGoogle Scholar
    [77] A. Vengosh, R.B. Jackson, N. Warner, T.H. Darrah, A. Kondash A critical review
    of the risks to water resources from unconventional shale gas development and
    hydraulic fracturing in the United States Environ. Sci. Technol., 48 (15) (2014),
    pp. 8334-8348 CrossRefView in ScopusGoogle Scholar [78] L. Rosa, M.C. Rulli, K.F.
    Davis, P. D''Odorico The water-energy nexus of hydraulic fracturing: a global
    hydrologic analysis for shale oil and gas extraction Earth''s Future, 6 (5) (2018),
    pp. 745-756 CrossRefView in ScopusGoogle Scholar [79] K. Cao, P. Siddhamshetty,
    Y. Ahn, M.M. El-Halwagi, J.S.-I. Kwon Evaluating the spatiotemporal variability
    of water recovery ratios of shale gas wells and their effects on shale gas development
    J. Clean. Prod., 276 (2020), Article 123171 View PDFView articleView in ScopusGoogle
    Scholar [80] T. Tasker, W. Burgos, P. Piotrowski, L. Castillo-Meza, T. Blewett,
    K. Ganow, A. Stallworth, P. Delompré, G. Goss, L. Fowler Environmental and human
    health impacts of spreading oil and gas wastewater on roads Environ. Sci. Technol.,
    52 (12) (2018), pp. 7081-7091 CrossRefView in ScopusGoogle Scholar [81] N. Shrestha,
    G. Chilkoor, J. Wilder, V. Gadhamshetty, J.J. Stone Potential water resource impacts
    of hydraulic fracturing from unconventional oil production in the Bakken shale
    Water Res., 108 (2017), pp. 1-24 View PDFView articleView in ScopusGoogle Scholar
    [82] J.P. Williams, A. Regehr, M. Kang Methane emissions from abandoned oil and
    gas wells in Canada and the United States Environ. Sci. Technol., 55 (1) (2020),
    pp. 563-570 View in ScopusGoogle Scholar [83] N. Hultman, D. Rebois, M. Scholten,
    C. Ramig The greenhouse impact of unconventional gas for electricity generation
    Environ. Res. Lett., 6 (4) (2011), Article 049504 CrossRefView in ScopusGoogle
    Scholar [84] R.W. Howarth, R. Santoro, A. Ingraffea Venting and leaking of methane
    from shale gas development: response to Cathles et al Clim. Change, 113 (2) (2012),
    pp. 537-549 CrossRefView in ScopusGoogle Scholar [85] Y. Chang, R. Huang, R.J.
    Ries, E. Masanet Shale-to-well energy use and air pollutant emissions of shale
    gas production in China Appl. Energy, 125 (2014), pp. 147-157 View PDFView articleView
    in ScopusGoogle Scholar [86] H. Yang, R.J. Flower, J.R. Thompson Shale-gas plans
    threaten China''s water resources Science, 340 (6138) (2013) 1288-1288 Google
    Scholar [87] D.S. Alessi, A. Zolfaghari, S. Kletke, J. Gehman, D.M. Allen, G.G.
    Goss Comparative analysis of hydraulic fracturing wastewater practices in unconventional
    shale development: water sourcing, treatment and disposal practices Can. Water
    Resour. J., 42 (2) (2017), pp. 105-121 CrossRefView in ScopusGoogle Scholar [88]
    C.E. Bond, J. Roberts, A.F.S.J. Hastings, Z. Shipton, E. Joao, K. Tabyldy, M.
    Stephenson Life-cycle Assessment of Greenhouse Gas Emissions from Unconventional
    Gas in Scotland (2014) Google Scholar [89] C.L. Weber, C. Clavin Life cycle carbon
    footprint of shale gas: review of evidence and implications Environ. Sci. Technol.,
    46 (11) (2012), pp. 5688-5695 CrossRefView in ScopusGoogle Scholar [90] Y. Yang,
    L. Wang, Y. Fang, C. Mou Integrated value of shale gas development: a comparative
    analysis in the United States and China Renew. Sustain. Energy Rev., 76 (2017),
    pp. 1465-1478 View PDFView articleView in ScopusGoogle Scholar [91] S. Corenot-gandolphe
    A Review of Recent Trends in China''s Gas Sector and a Glimpse into the 14th Five-Year
    Plan Publications Notes de l''Ifri (2020), p. 979 -10-373-0252-6 Google Scholar
    [92] U.S. Energy Information Administration (EIA) World Shale Resource Assessments
    (2015) https://www.eia.gov/analysis/studies/worldshalegas/, Accessed 28th Jun
    2022 Google Scholar [93] G. Zhai, Y. Wang, Z. Zhou, S. Yu, X. Chen, Y. Zhang Exploration
    and research progress of shale gas in China China Geology, 1 (2) (2018), pp. 257-272
    View PDFView articleCrossRefView in ScopusGoogle Scholar [94] National Energy
    Administration (NEA) of China China Natural Gas Development Report (2021) http://www.nea.gov.cn/2021-08/21/c_1310139334.htm,
    Accessed 28th Jun 2022 Google Scholar [95] U.S. Energy Information Administration
    (EIA) How Much Shale Gas Is Produced in the United States? (2021) https://www.eia.gov/tools/faqs/faq.php?id=907&t=8,
    Accessed 28th Jun 2022 Google Scholar [96] S. Gao, D. Dong, K. Tao, W. Guo, X.
    Li, S. Zhang Experiences and lessons learned from China''s shale gas development:
    2005–2019 J. Nat. Gas Sci. Eng., 85 (2021), Article 103648 View PDFView articleView
    in ScopusGoogle Scholar [97] Energy Policy Act of 2005, US Congress (2005) Google
    Scholar [98] The State Administration of Taxation Announcement on enterprise income
    tax policies issues concerning the further implementation of the western China
    development strategy CaiShui (2011) No. 58 Google Scholar [99] J. Wei, H. Duan,
    Q. Yan Shale gas: will it become a new type of clean energy in China?—a perspective
    of development potential J. Clean. Prod., 294 (2021), Article 126257 View PDFView
    articleView in ScopusGoogle Scholar [100] BEIS, Public Attitudes Tracker Energy
    Infrastructure and Energy Sources (2021) UK Google Scholar [101] B.D. Drollette,
    K. Hoelzer, N.R. Warner, T.H. Darrah, O. Karatum, M.P. O''Connor, R.K. Nelson,
    L.A. Fernandez, C.M. Reddy, A. Vengosh Elevated levels of diesel range organic
    compounds in groundwater near Marcellus gas operations are derived from surface
    activities Proc. Natl. Acad. Sci. U.S.A., 112 (43) (2015), pp. 13184-13189 CrossRefView
    in ScopusGoogle Scholar [102] G.T. Llewellyn, F. Dorman, J. Westland, D. Yoxtheimer,
    P. Grieve, T. Sowers, E. Humston-Fulmer, S.L. Brantley Evaluating a groundwater
    supply contamination incident attributed to Marcellus shale gas development Proc.
    Natl. Acad. Sci. U.S.A., 112 (20) (2015), pp. 6325-6330 CrossRefView in ScopusGoogle
    Scholar [103] Y.Z. Zou Caineng, Dazhong Dong, Qun Zhao, Zhenhong Chen, Youliang
    Feng, Jiarui Li, Xiaoni Wang Formation, distribution and prospect of unconventional
    hydrocarbons in source rock strata in China Earth Sci., 47 (5) (2022), pp. 1517-1533
    Google Scholar [104] U.S. Energy Information Administration (EIA) How Much Shale
    (Tight) Oil Is Produced in the United States? (2022) https://www.eia.gov/tools/faqs/faq.php?id=847&t=6,
    Accessed 28th Jun 2022 Google Scholar Cited by (13) Integrating experimental study
    and intelligent modeling of pore evolution in the Bakken during simulated thermal
    progression for CO<inf>2</inf> storage goals 2024, Applied Energy Show abstract
    Does shale gas exploitation contribute to regional sustainable development? Evidence
    from China 2023, Sustainable Production and Consumption Show abstract A novel
    zero-emission process for the co-production of power, natural gas liquid, and
    carbon dioxide based on shale gas 2023, Applied Thermal Engineering Show abstract
    Toxicity identification evaluation for hydraulic fracturing flowback and produced
    water during shale gas exploitation in China: Evidence from tissue residues and
    gene expression 2023, Water Research Show abstract Hydraulic fracturing performance
    analysis by the mutual information and Gaussian process regression methods 2023,
    Engineering Fracture Mechanics Show abstract Utilization of oil-based drilling
    cuttings as asphalt pavement surface: Study on the mechanical characteristics
    and long-term environmental impact 2023, Environmental Technology and Innovation
    Show abstract View all citing articles on Scopus © 2022 The Authors. Published
    by Elsevier B.V. on behalf of Chinese Society for Environmental Sciences, Harbin
    Institute of Technology, Chinese Research Academy of Environmental Sciences. Recommended
    articles A tartrate-EDTA-Fe complex mediates electron transfer and enhances ammonia
    recovery in a bioelectrochemical-stripping system Environmental Science and Ecotechnology,
    Volume 11, 2022, Article 100186 De-Xin Zhang, …, Hao-Yi Cheng View PDF Vertically-resolved
    indoor measurements of air pollution during Chinese cooking Environmental Science
    and Ecotechnology, Volume 12, 2022, Article 100200 Shuxiu Zheng, …, Shu Tao View
    PDF Core fungal species strengthen microbial cooperation in a food-waste composting
    process Environmental Science and Ecotechnology, Volume 12, 2022, Article 100190
    Yuxiang Zhao, …, Baolan Hu View PDF Show 3 more articles Article Metrics Citations
    Citation Indexes: 11 Captures Readers: 24 View details About ScienceDirect Remote
    access Shopping cart Advertise Contact and support Terms and conditions Privacy
    policy Cookies are used by this site. Cookie settings | Your Privacy Choices All
    content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Environmental Science and Ecotechnology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Sustainable development index of shale gas exploitation in China, the UK,
    and the US
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ermoliev Y.
  - Zagorodny A.G.
  - Bogdanov V.L.
  - Ermolieva T.
  - Havlik P.
  - Rovenskaya E.
  - Komendantova N.
  - Obersteiner M.
  citation_count: '6'
  description: Traditional integrated modeling (IM) is based on developing and aggregating
    all relevant (sub)models and data into a single integrated linear programming
    (LP) model. Unfortunately, this approach is not applicable for IM under asymmetric
    information (ASI), i.e., when “private” information regarding sectoral/regional
    models is not available, or it cannot be shared by modeling teams (sectoral agencies).
    The lack of common information about LP submodels makes LP methods inapplicable
    for integrated LP modeling. The aim of this paper is to develop a new approach
    to link and optimize distributed sectoral/regional optimization models, providing
    a means of decentralized cross-sectoral coordination in the situation of ASI.
    Thus, the linkage methodology enables the investigation of policies in interdependent
    systems in a “decentralized” fashion. For linkage, the sectoral/regional models
    do not need recoding or reprogramming. They also do not require additional data
    harmonization tasks. Instead, they solve their LP submodels independently and
    in parallel by a specific iterative subgradient algorithm for nonsmooth optimization.
    The submodels continue to be the same separate LP models. A social planner (regulatory
    agency) only needs to adjust the joint resource constraints to simple subgradient
    changes calculated by the algorithm. The approach enables more stable and resilient
    systems’ performance and resource allocation as compared to the independent policies
    designed by separate models without accounting for interdependencies. The paper
    illustrates the application of the methodology to link detailed energy and agricultural
    production planning models under joint constraints on water and land use.
  doi: 10.3390/su14031255
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sustainability All Article Types Advanced   Journals
    Sustainability Volume 14 Issue 3 10.3390/su14031255 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Ripon Kumar
    Chakrabortty Subscribe SciFeed Recommended Articles Related Info Link More by
    Authors Links Article Views 2148 Citations 7 Table of Contents Abstract Introduction
    Linking Distributed Optimization Models under Joint Resource Constraints Linking
    Energy and Agricultural Models for Food-Energy-Water Nexus Conclusions Author
    Contributions Funding Institutional Review Board Statement Informed Consent Statement
    Data Availability Statement Acknowledgments Conflicts of Interest Appendix A References
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessArticle Linking Distributed Optimization Models for Food, Water, and Energy
    Security Nexus Management by Yuri Ermoliev 1, Anatolij G. Zagorodny 2, Vjacheslav
    L. Bogdanov 3, Tatiana Ermolieva 1,*, Petr Havlik 1, Elena Rovenskaya 1, Nadejda
    Komendantova 1 and Michael Obersteiner 1 1 International Institute for Applied
    Systems Analysis, 2361 Laxenburg, Austria 2 Bogolyubov Institute for Theoretical
    Physics, National Academy of Sciences of Ukraine, 03142 Kiev, Ukraine 3 Timoshenko
    Institute of Mechanics, National Academy of Sciences of Ukraine, 03142 Kiev, Ukraine
    * Author to whom correspondence should be addressed. Sustainability 2022, 14(3),
    1255; https://doi.org/10.3390/su14031255 Submission received: 9 December 2021
    / Revised: 19 January 2022 / Accepted: 20 January 2022 / Published: 23 January
    2022 (This article belongs to the Special Issue Modeling and Managing Catastrophic
    Risks in Heterogeneous Systems) Download keyboard_arrow_down     Browse Figure
    Review Reports Versions Notes Abstract Traditional integrated modeling (IM) is
    based on developing and aggregating all relevant (sub)models and data into a single
    integrated linear programming (LP) model. Unfortunately, this approach is not
    applicable for IM under asymmetric information (ASI), i.e., when “private” information
    regarding sectoral/regional models is not available, or it cannot be shared by
    modeling teams (sectoral agencies). The lack of common information about LP submodels
    makes LP methods inapplicable for integrated LP modeling. The aim of this paper
    is to develop a new approach to link and optimize distributed sectoral/regional
    optimization models, providing a means of decentralized cross-sectoral coordination
    in the situation of ASI. Thus, the linkage methodology enables the investigation
    of policies in interdependent systems in a “decentralized” fashion. For linkage,
    the sectoral/regional models do not need recoding or reprogramming. They also
    do not require additional data harmonization tasks. Instead, they solve their
    LP submodels independently and in parallel by a specific iterative subgradient
    algorithm for nonsmooth optimization. The submodels continue to be the same separate
    LP models. A social planner (regulatory agency) only needs to adjust the joint
    resource constraints to simple subgradient changes calculated by the algorithm.
    The approach enables more stable and resilient systems’ performance and resource
    allocation as compared to the independent policies designed by separate models
    without accounting for interdependencies. The paper illustrates the application
    of the methodology to link detailed energy and agricultural production planning
    models under joint constraints on water and land use. Keywords: asymmetric information;
    linkage; nonsmooth optimization; subgradient; integrated modeling; food-energy-water-land
    nexus; machine learning 1. Introduction The increasing interdependencies among
    food-energy-water-environmental (FEWE) sectors require integrated coherent planning
    and coordinated policies for sustainable development and security nexus. The sectors
    become more interconnected because they utilize common, often rather limited,
    resources, both natural (e.g., land, water, air quality) and socio-economic (e.g.,
    investments, labor force). For example, land and water are needed not only for
    agricultural production but also for hydropower generation, coal mining and processing,
    power plants cooling, renewable energy, and hydrogen production. The energy sector
    is one of the largest and fast-growing water consumers. The more water is used
    by the energy sector, the more vulnerable energy production and production in
    other water-dependent sectors, becomes [1]. Climate change concerns and rapid
    energy sector transition towards renewable energy sources tighten the links between
    energy and agricultural markets. Agricultural commodities have become an important
    energy resource because of biofuels mandates. Vulnerability of crop yields, increasing
    grain demand, and price volatility directly and indirectly influence the market
    for fuel transportation and transportation costs [2]. At the same time, crude
    oil, gas, and electricity markets and prices have an effect on agricultural production
    costs and prices [2,3,4]. Additional linkages and interactions in FEWE systems
    emerge due to the introduction of new technologies, e.g., intermittent renewables,
    advanced irrigation, hydrogen production, water desalination, etc. The interdependencies
    can trigger systemic failures if sectoral policies ignore cross-sectoral interconnectedness
    [5]. The FEWE security nexus management requires an integrated approach to understand
    and deal with the numerous interactions between the FEWE systems [6]. This approach,
    compared to independent analysis, contributes immensely to sustainable development
    within and across sectors and scales. Comprehensive sectoral models are being
    developed for planning and policy assessment in respective sectors. These models
    account for multiple details of sectoral production planning and resource utilization,
    including the analysis of factors and drivers determining demand, supply, and
    commodity price relationships [7,8]. For example, energy sector models investigate
    the interactions between renewables and fossil fuels, address energy market volatilities,
    and analyze the effects of new technologies and policy interventions to develop
    energy scenarios [7,8,9,10,11,12]. Land use and agricultural models support decision
    making regarding agricultural policies, analyze land potentials for the production
    of sufficient agricultural commodities to fulfill food security and biofuels mandates
    [13,14,15], and assess the effects of policy responses, including export bans
    and high export taxes, to cope with production shortfalls and offset increasing
    prices [2]. As a rule, these models consider and optimize sectoral goals accounting
    for respective production, demand, resource availability, and environmental quality
    constraints. Goals, production targets, resource demand, and quality in other
    sectors are hardly, if at all, accounted for. Thus, the limitation of the detailed
    sectoral models for FEWE security nexus lies in their restricted ability to consider
    dependencies and interactions beyond the defined sectoral system [2,3,12], e.g.,
    cross-sectoral resource competition and joint production and demand relationships.
    Sectoral models cannot properly account for the objectives of a larger system.
    The feedbacks and interactions among FEWE systems are often analyzed through CGE
    (Computational General Equilibrium) and/or IAMs (Integrated Assessment models)
    [6]. These models, unfortunately, suffer from the lack of necessary details of
    sectoral models. They involve considerable simplifications and aggregations and,
    therefore, may not be sufficiently fit to provide insightful conclusions [16,17,18,19].
    In this situation, the analysis of systemic regulations for FEWE security nexus
    can rely on distributed models’ optimization and linkage methods enabling the
    establishment of relationships and dialogues between separate models of FEWE systems
    for the analysis of coordinated solutions without requiring to share or reveal
    systems-specific information. In this paper, we consider the problem of linking
    sectoral and/or regional linear programming (LP) models into a cross-sectoral
    integrated model (IM) in the presence of joint constraints when “private” information
    regarding sectoral/regional models’ is not available or cannot be shared by modeling
    teams (sectoral agencies), i.e., under asymmetric information (ASI). Such linkage
    is necessary for producing truly integrative management scenarios, especially
    when sectors utilize and compete for common resources or act under joint regulatory
    constraints or environmental mandates. The approach provides a means of decentralized
    cross-sectoral coordination and enables the investigation of policies in interdependent
    systems in a “decentralized” fashion. This facilitates more stable and resilient
    systems’ performance and resource allocation as compared to the independent policies
    designed by separate models without accounting for interdependencies. Cross-sectoral
    policy analysis in the presence of joint constraints can be addressed, e.g., with
    the generalized Nash equilibrium (GNE) approach [20]. Böhringer and Rutherford
    (2009) [21] consider linking energy system mathematical programming models into
    a general equilibrium (GE) model of the overall economy. Ermoliev and von Winterfeldt
    (2012) [22] discuss the game-theoretic approaches, e.g., the Stackelberg leadership
    model, and their complexity due to the assumptions that each player (sector/region)
    has information about other players’ goals and constraints. Traditional integrated
    deterministic optimization modeling also assumes full knowledge about the systems.
    It incorporates goals, individual and joint constraints, and data of all systems
    into a single code (hard integration), which can be considered as a multi-criteria
    optimization problem [23]. Our approach for linking separate optimization models
    under ASI is based on the parallel solving of equivalent nonsmooth optimization
    models by a simple iterative stochastic quasigradient (SQG) procedure [24,25]
    based on subgradients or generalised gradients [25,26,27] converging to an optimal
    welfare-maximizing linkage solution, i.e., to the solution of a “hard-integrated”
    model. This approach does not require sharing details about models’ specifications.
    We can assume there is a network of distributed computers connecting computer
    models of a “social planner” (decision-makers or regulatory agencies), who attempt
    to achieve the best result for all sectors/regions (parties) involved. The linkage
    procedure can be interpreted as a kind of a “decentralized market system” [28].
    According to this procedure, sectors/regions independently and in parallel optimize
    their goal functions under individual constraints without considering joint constraints.
    In general, joint constraints impose restrictions on total production, resource
    use, and emissions by all sectors/regions. The constraints can establish supply–demand
    relationships between the systems enabling the estimation of optimal production,
    resource use, and emission quotas for each system. The balance between the total
    energy (including biofuels) production and demand defines energy security; agricultural
    production and consumption reflect food security; total emissions and pollution
    constraints correspond to environmental security. The joint FEWE constraints satisfaction
    establishes the FEWE security nexus [29]. After independent optimization using
    initial approximations of various (e.g., production, resource use, emission) quotas,
    the sectors/regions provide the social planner with the information on their actual
    production, resource use, and respective shadow prices. The planner checks if
    the joint constraints are fulfilled. If not, i.e., there is “excess demand” or
    “excess supply” (i.e., total resource use, production, emissions by all systems
    are higher/lower than required), the planner revises the individual systems’ quotas
    via shifting their current approximation in the direction defined by the corresponding
    dual variables. Thus, shadow prices signal systems to adjust their activities
    accordingly. Formally, the procedure is described in Section 2.3 and Appendix
    A. In this way, the linkage allows us to avoid the “hard linking” of models in
    a single code, which is not possible because the systems do not want to share
    the information or because the individual models are too detailed and complex
    to be “hard-linked”. The approach saves reprogramming efforts and allows parallel
    distributed (decentralized) computations of sectoral models instead of a large-scale
    integrated (centralized) model. This also preserves the original models in their
    initial state for other linkages. The use of detailed sectoral and regional models
    instead of their aggregated simplified versions also enables us to account for
    critically important local details. Similar computerized decentralized “negotiation”
    processes between distributed models (agents) have been developed for the design
    of robust carbon trading markets (e.g., [30] and references therein) and for the
    allocation of water quotas (e.g., [31]). The linkage procedure can be considered
    as a new machine learning algorithm, namely, as a general endogenous reinforced
    learning problem of how software agents (models) take decisions in order to maximize
    the cumulative reward (total welfare) [32]. The paper is organized as follows.
    Section 2 discusses the problem of models’ linkage under joint constraints. Section
    2.1 presents a short overview and the main shortfalls of several existing approaches,
    Section 2.2. formulates the problem of distributed LP models’ linkage in the presence
    of joint resource constraints and ASI, and Section 2.3 outlines the linkage solution
    procedure based on the parallel solving of equivalent nonsmooth optimization model
    following a simple iterative subgradient algorithm. The details and main properties
    of the algorithms are presented in the Appendix A. Section 3 illustrates the application
    of the methodology to link detailed energy and agricultural production planning
    models under joint constraints on water and land use. In addition, the joint constraints
    can impose restrictions on total energy production by the energy sector (electricity,
    gas, diesel, etc.) and land use sector (biodiesel, methanol); total energy use
    by energy and agricultural sectors; total agricultural production by distributed
    farmers/regions, etc. Section 4 concludes and outlines potential further extensions
    of the approach, for example, to include more details of energy and natural resources
    dynamics in general. 2. Linking Distributed Optimization Models under Joint Resource
    Constraints 2.1. Social Equilibrium Game Approach In the absence of coordination
    between systems (sectors, regions), they can act selfishly and aim at maximizing
    their own objective function. They attempt to secure as high resource quotas as
    possible. Such a situation can be modeled using the non-cooperative game-theoretic
    framework. For example, social equilibrium games [20] have been formulated to
    include joint constraints. The generalized Nash equilibrium (GNE) solution, if
    it exists, allocates production and resources among systems (sectors/regions),
    fulfilling the joint constraint. However, the decisions are made independently,
    and collective efforts for managing common resources are ignored. Importantly,
    the existence, uniqueness, and stability of the GNE, as well as a realistic large-scale
    implementation of this concept, cannot be guaranteed, as emphasized by Harker
    (1991) [20]. Moreover, in [20], it is highlighted that the GNE solutions set are
    rarely connected. Hence, a complete analysis of equilibriums, in this case, is
    a complex task, requiring additional assumptions. The analysis can become even
    more complex if the joint constraints are based on the equilibrium (optimality)
    conditions arising from the problem formulated in the form of a principal-agent
    game or a leader-follower Stackelberg game [22]. For example, in the case of nonsmooth
    goal functions required for linking systems under ASI (distributed models’ optimization),
    the use of optimality conditions would require implicit sets of generalized gradients.
    Due to the computational complexity, heuristic methods are often used; however,
    they lack rigorous convergence proof. Linking bottom-up mathematical programming
    models of the energy system into a top-down general equilibrium model of the overall
    economy is discussed by Böhringer and Rutherford in [21]. The paper shows that
    the formulation of market equilibrium conditions using complementarity equations
    permit the integration of models, but the convergence of the iterative procedure
    integrating the models cannot be guaranteed. In specific cases, models of general
    equilibrium are reduced to optimization problems [33]. Ermoliev and von Winterfeldt
    (2012) [3] demonstrate that the complexity of the game-theoretic approaches is
    due to quite unrealistic assumptions that each player (sector/region) is in possession
    of the knowledge on exact and unique responses of other players. Therefore, even
    in the simplest linear cases, this assumption leads to extremely complex discontinuous
    problems. More realistic assumptions of uncertain response functions in combination
    with a concept of robust decisions results in stable large-scale solutions. There
    exists a vast literature on important problems and methods for distributed systems’
    optimization under joint constraints, e.g., optimal control and economic dispatch
    in smart grids [34], agricultural production planning for the multi-farmer systems
    [35], network optimization [36,37,38], and optimal transportation problems [39,40,41].
    Yet, these approaches consider the optimization of a total objective function
    representing a sum of individual objective functions of the involved systems.
    Thus, the problems assume full information regarding the systems is available
    to a social planner. They are formulated similarly to traditional integrated “centralized”
    optimization modeling, combining goals, constraints, and data of all models into
    a single code. Our problem is more complex as it deals with the coordination of
    decentralized systems’ models in the presence of joint constraints and ASI. In
    this case, the approach is based on a specific iterative nonsmooth optimization
    procedure (see Section 2.2 and Section 2.3 and Appendix A). As we noted, the integrated
    solution of separate LP models under ASI cannot be accomplished by LP methods.
    In Section 2.2, we formulate the problem of distributed systems optimization in
    the presence of joint resource constraints under ASI, and in Section 2.3, we present
    the models’ linkage algorithm. 2.2. LP Models under Joint Constraints Let us formulate
    the basic problem of separate sectoral or regional LP models optimization under
    joint resource constraints. Consider separate models of 𝐾 systems in the following
    LP form: 〈 𝑐 (𝑘) , 𝑥 (𝑘) 〉→𝑚𝑎𝑥 (1) 𝑥 (𝑘) ≥0 (2) 𝐴 (𝑘) 𝑥 (𝑘) ≤ 𝑏 (𝑘) (3) where
    components of vector 𝑥 (𝑘) are variables to be determined, vector 𝑏 (𝑘) defines
    system-specific demand or resource constraints, and vector 𝑐 (𝑘) corresponds to
    net unit profits, 𝑘=1,2,…,𝐾 . The dependence of system 𝑘 on common resources are
    defined by constraint (4) 𝐵 (𝑘) 𝑥 (𝑘) ≤ 𝑦 (𝑘) (4) where 𝑦 (𝑘) defines resource
    quota allocated to system 𝑘 . Therefore, Formula (3) represents system-specific
    constraints and Formula (4) establishes systemic relations among systems by allocating
    quotas 𝑦 (𝑘) . The quotas 𝑦 (𝑘) fulfil the joint resource constraint on the use
    of common resources ∑ 𝑘=1 𝐾 𝐷 (𝑘) 𝑦 (𝑘) ≤𝑑, (5) where matrix 𝐷 (𝑘) defines the
    marginal resource use by system 𝑘 and 𝑑 is the total available resource, 𝑑≥0 .
    Thus, each system 𝑘 maximizes its objective function (1) by choosing 𝑥 (𝑘) and
    𝑦 (𝑘) from the feasible set defined by (2) and (3), so that (4) and (5) are also
    fulfilled. In the presence of full information regarding a system, the problem
    of models’ linkage can be formulated and solved by a central planner (regulator)
    as a total net profit maximization ∑ 𝐾 𝑘=1 〈 𝑐 (𝑘) , 𝑥 (𝑘) 〉→𝑚𝑎𝑥 (6) s.t. to constraints
    (2)–(5), 𝑘=1,2,…,𝐾 . In this model, the net profits are defined as the amount
    of money left after subtracting production costs from the total profit. In a more
    general case, the net profits can account for taxes, interest, and other expenses.
    However, when the information on 𝑏 (𝑘) , 𝑐 (𝑘) , 𝐴 (𝑘) , 𝐵 (𝑘) , 𝑥 (𝑘) of system
    𝑘 is not available to the planner, the integrated LP model (2)–(6) under ASI cannot
    be solved by LP method due to the lack of common information about submodels.
    We propose the consistent approach for linking distributed optimization models
    under ASI based on the parallel solving of equivalent nonsmooth optimization models
    following a simple iterative subgradient algorithm. The convergence and other
    properties of the algorithm are presented in Appendix A. The proposed linkage
    approach does not require full, common information regarding the models’ specification,
    and it can be seen as an endogenous reinforced learning algorithm describing how
    distributed agents (models) can make decisions to maximize the “cumulative reward”.
    Section 2.3. outlines the algorithm. 2.3. Nonsmooth Model and Linking Algorithm
    The basic nonsmooth optimization model under ASI can be solved by a specific iterative
    subgradient linkage algorithm. For a given vector 𝑦=( 𝑦 (1) ,…, 𝑦 (𝐾) ) let us
    denote by 𝐹(𝑦) the optimal value of function (6) under constraints (2)–(4). Therefore,
    𝐹(𝑦)= ∑ 𝐾 𝑘=1 𝑓 (𝑘) (𝑦) , where 𝑓 (𝑘) (𝑦)=( 𝑐 (𝑘) ,( 𝑥 (𝑘) (𝑦)) are concave nonsmooth
    functions. In this function 𝑥 (𝑘) (𝑦) are optimal solutions of (1)–(4). The required
    linkage algorithm is defined as a subgradient procedure maximizing function 𝐹(𝑦)
    s.t. the joint constraints (5). These constraints identify the feasible set of
    the algorithm, which can be denoted as 𝑌 . Therefore, an optimal solution maximizing
    𝐹(𝑦) , 𝑦∈𝑌 , also defines an optimal linkage or a solution of the integrated LP
    model under ASI. In the following, we assume the existence of solutions 𝑥 (𝑘)
    (𝑦) , 𝑦∈𝑌 , for all 𝑘 . The linkage algorithm can be summarized as follows. Imagine
    there is a network of distributed computers connecting submodels, say sectors,
    with a computer of a social planner. At the initial step, sectors 𝑘 , 𝑘=1,…,𝐾
    , use arbitrary chosen vectors 𝑦 0(𝑘) of resource quotas. They submit the information
    on 𝑦 0(𝑘) to the central computer. The computer updates quotas 𝑦 0 =( 𝑦 0(1) ,…,
    𝑦 0(𝐾) ) by projecting them onto set 𝑌, defining a first feasible approximation
    𝑦 1 =( 𝑦 1(1) ,…, 𝑦 1(𝐾) ). All sectors independently solve models (1)–(4) with
    resource quotas 𝑦 1 , calculate shadow prices 𝑣 1(𝑘) of common resources (constraint
    (4)), and submit them to the central computer. The central computer calculates
    𝑦 1 + 𝜌 1 𝑣 1 with a step-size 𝜌 1 such that the product 𝜌 1 𝑣 1 corresponds to
    the scale of 𝑦 1 . Vector 𝑦 1 + 𝜌 1 𝑣 1 is projected onto 𝑌 to derive quotas 𝑦
    2 . At the iteration 𝑠+1, the algorithm derives the next approximation of quotas
    𝑦 𝑠+1 =( 𝑦 𝑠+1(1) ,…, 𝑦 𝑠+1(𝐾) ) by shifting 𝑦 𝑠 in the direction of vector 𝑣
    𝑠 =( 𝑣 𝑠(1) ,…, 𝑣 𝑠(𝐾) ) , according to the following procedure. 𝑦 𝑠+1 = 𝜋 𝑌 (
    𝑦 𝑠 + 𝜌 𝑠 𝑣 𝑠 ), 𝑠=1,2,…, (7) where 𝜌 𝑠 is a step-dependent multiplier, which
    is a method’s parameter, 𝜋 𝑌 (⋅) is the orthogonal projection operator onto set
    𝑌 defined by (5). Vector 𝑣 𝑠 is a generalized gradient or a subgradient of function
    𝐹(𝑦) at 𝑦= 𝑦 𝑠 . The step-size 𝜌 𝑠 is chosen from rather general and natural requirements:
    𝜌 𝑠 ≥0 , 𝜌 𝑠 →0 , ∑ ∞ 𝑠=1 𝜌 𝑠 =∞ , (e.g., 𝜌 𝑠 =1/𝑠) , because subgradients (generalized
    gradients) are not, in general, the increasing directions of functions. At each
    iteration, all sectors independently calculate stopping criteria 𝜀 𝑘 (𝑠)=( 𝑏 (𝑘)
    , 𝑢 𝑠(𝑘) ( 𝑦 𝑠 ))+( 𝑦 𝑠(𝑘) , 𝑣 𝑠(𝑘) ( 𝑦 𝑠 ))− 𝑤 𝑘 ( 𝑐 (𝑘) , 𝑥 𝑠(𝑘) ( 𝑦 𝑠 )) and
    submit values 𝜀 𝑘 (𝑠) to the central computer. If ∑ 𝑘 𝜀 𝑘 (𝑠)≤𝜀≥0 , where 𝜀 is
    an admissible accuracy, then the algorithm stops. Otherwise, it continues further.
    The convergence theorem shows that the parallel independent optimization (linkage)
    of sectoral/regional models according to this algorithm without revealing sectoral/regional
    information is possible due to the additional requirement ∑ 𝑠 𝜌 2 𝑠 <∞ . This
    allows us to prove the convergence of solutions (linkages) 𝑦 𝑠 rather than the
    convergence of objective function 𝐹( 𝑦 𝑠 ) . The convergence of the proposed linkage
    algorithm under ASI is based on the theory of (continuously) non-differentiable
    optimization. The details of convergence theorem, stopping criterion, subgradients,
    and computing projections can be found in Appendix A. 3. Linking Energy and Agricultural
    Models for Food-Energy-Water Nexus The proposed iterative algorithm has been applied
    for linking energy and agricultural sectoral models under joint constraints on
    water and land use. Both models can be used for optimal energy and agricultural
    production and allocation planning. In the following, we only briefly outline
    the models. Further details can be found, for example, in [9,10,11,13,14,15].
    The models are spatially explicit, which allows us to link the models across locations
    and thus control local drivers having significant implications on the overall
    results of models’ integration. The energy model incorporates the main stages
    of energy flows from resources to demands: energy extraction from energy resources,
    primary energy conversion into secondary energy forms, transport and distribution
    of energy to the point of end, and conversion into products for end-users to fulfill
    specific demands. The structure of the model is such that it can incorporate various
    energy resources, e.g., coal, gas, crude oil, renewables. Primary energy sources
    include coal, crude oil, gas, solar, wind, etc.; secondary energy sources are
    fuel oil, methanol, hydrogen, electricity, ammonia, etc.; final energy products
    are coal, fuel oil, gas, hydrogen, ammonia, methanol, electricity, etc. Demands
    for useful energy products come from main sectors of the economy: industrial,
    residential, transport, agricultural, water, and energy. Each technology is characterized
    by uniting costs, efficiency, lifetime, emissions, etc. Additional sectoral (and
    cross-sectoral joint) constraints are imposed to capture the requirements and
    the limitations on natural resource use and availability and investments. The
    model can include existing technologies, as well as new zero-carbon green technologies,
    at the beginning of implementation or even in the research stage, e.g., various
    renewable and carbon-capturing technologies. The agricultural model includes main
    crops and livestock production and management systems, characterized by systems-specific
    production costs, water and fertilizer requirements, emission factors, and other
    parameters. The supply of crops and livestock products need to cover food, feed,
    and biofuel demands and fulfill security constraints. The food security constraint
    requires that the energy and nutrients consumption from grain and livestock products
    is not less than the required kilocalories and nutrients needed to satisfy dietary
    requirements in cereals, vegetables, and animal products (meat and dairy products).
    Livestock feeds fulfill livestock dietary requirements in energy intake measured
    in megacalories. Biofuel production from crops (and agricultural residues) must
    fulfill biofuel mandates. In the model, land uses comprise agricultural (crop
    and pasture) land, grassland, and natural land. Land use changes can be regulated
    by setting regulatory constraints on land expansion and conversion. Security constraints
    introduce competition for limited natural resources (land and water) among different
    land uses. Energy and agricultural sectors compete for common land and water resources.
    Assume that regional planners, decision makers, and sectoral authorities pursue
    a goal to minimize costs and maximize profits from energy and agricultural production
    under various joint balance (supply-demand) and resource constraints to fulfill
    the energy and agricultural demands. Namely, the goal is to choose a portfolio
    of energy technologies to be installed and operated to produce, convert, and transfer
    energy products among locations; and a portfolio of agricultural technologies
    and management systems to produce and transfer among locations agricultural commodities
    fulfilling constraints on natural resources, environmental pollution, and end-product
    demands. The models include relevant risk-related systems’ performance criteria.
    These performance measures enable a better understanding of how systems (individually
    and jointly) might perform in the uncertain environment, in the presence of climate
    change, weather variability, market uncertainties, etc. A better understanding
    of how interdependent energy-water-agricultural systems may operate and how dangerous
    impacts of inappropriate decisions may be can motivate regional and sectoral planners,
    experts, and involved stakeholders in making cross-sectoral coherent and risk-adjusted
    robust decisions [9,10,11,14,15,22,29]. 3.1. Energy, Water, and Agricultural Security
    Nexus in Shanxi Province, China In the case study in Shanxi province, China [15],
    the energy model includes coal-based industries and processes, i.e., mining, washing,
    chemical production, and power generation. Most of the electricity in China comes
    from coal, which accounted for approximately 65% of the electricity generation
    mix in 2019. The coal-based technologies consume a vast amount of water, for example,
    for coal mining and washing, coal power plants cooling and steam production. About
    51% of China’s coal reserves lie in areas of high or extreme water scarcity, and
    about 30% are in water-stressed regions. Shanxi province is one of them. The integrated
    energy-agricultural-water model is formulated as follows. A regional planner decides
    on the amount of coal 𝑥 𝑖𝑗𝑙𝑚𝑡 and type 𝑖, 𝑖=1,…,𝐼 , to be extracted in location
    𝑗, 𝑗=1,…,𝐽 , transported to location 𝑚 , 𝑚=1,…,𝑀 , and converted by technology
    𝑡, 𝑡=1,…,𝑇 . In addition, decisions 𝑧 𝑘𝑗𝑚 are made concerning the amount of agricultural
    commodities k, 𝑘=1,…,𝐾, to be produced in location j and exported to location
    𝑚 . The overall goal of the planner is to minimize the total costs related to
    energy (coal) and agricultural production, transportation, and conversion. Individual
    sectoral goal functions are formulated as follows ∑ 𝑖,𝑗,𝑘,𝑚,𝑡 [ 𝑐 𝐶𝑃 𝑖𝑗 + 𝑐 𝐶𝑇
    𝑖𝑗𝑚 + 𝑐 𝐶𝐶 𝑖𝑗𝑡 ] 𝑥 𝑖𝑗𝑚𝑡 → 𝑚𝑖𝑛 (8) and ∑ 𝑖,𝑗,𝑘,𝑚,𝑡 [ 𝑐 𝐴𝑃 𝑘𝑗 + 𝑐 𝐴𝑇 𝑘𝑗 ] 𝑧 𝑘𝑗𝑚
    → 𝑚𝑖𝑛 (9) for energy (8) and agricultural (9) sectors. Production costs 𝑐 𝐶𝑃 𝑖𝑗
    define all components of coal production costs, including extraction and washing,
    of a unit (tonne) coal of type 𝑖 in location 𝑗 , transportation costs 𝑐 𝐶𝑇 𝑖𝑗𝑚
    represent all costs associated with transporting unit coal 𝑖 from location 𝑗 to
    location 𝑚 , 𝑐 𝐶𝐶 𝑖𝑗𝑡 define conversion costs of a unit coal 𝑖 by technology 𝑡
    in location 𝑗 , 𝑐 𝐴𝑃 𝑘𝑗 denote agricultural production cost per unit (tonne) agricultural
    commodity 𝑘 in location 𝑗 , and 𝑐 𝐴𝑇 𝑘𝑗𝑚 stands for the transportation costs of
    a unit agricultural commodity 𝑘 from 𝑗 to 𝑚 , 𝑖=1,…,𝐼;  𝑗=1,…,𝐽;𝑚=1,…,𝑀; 𝑡=1,…,𝑇;
    𝑘=1,…,𝐾. The energy model includes energy security constraints ensuring consumers
    demands for end products from coal, for example, electricity, heat, coke, gas,
    and oil: ∑ 𝑖𝑗𝑡 𝛼 𝑑 𝑖𝑚𝑡 𝑥 𝑖𝑗𝑚𝑡 ≥ 𝐷 𝑑 𝑚 (10) where 𝛼 𝑑 𝑖𝑗𝑡 denotes the conversion
    efficiency of coal 𝑖 in location 𝑗 by technology 𝑡 , the end product 𝑑 , and 𝐷
    𝑑 𝑗 stands for the end product 𝑑 demand. Agricultural production is required to
    fulfil food security constraints defined by the sufficient kilocalories and nutrients
    provided to the population from agricultural commodities k in location 𝑚 : ∑ 𝑗
    𝑧 𝑘𝑗𝑚 ≥ 𝐷 𝐴 𝑘𝑚 (11) where 𝐷 𝐴 𝑘𝑚 is the required production of agricultural commodity
    𝑘 in location m to meet food security requirements, which can be calculated according
    to daily nutrients and calories requirements per capita approved by the World
    Health Organization (WHO). Sectoral land use constraints ∑ 𝑖,𝑚,𝑡 𝑥 𝑖𝑗𝑚𝑡 (1− 𝑟
    𝑖𝑗 )Δ 𝑙 𝑗 𝑆 𝑖𝑗 +𝑔 ∑ 𝑖,𝑚,𝑡 𝑥 𝑖𝑗𝑚𝑡 ≤ 𝐿 𝐶 𝑗 (12) and ∑ 𝑘,𝑚 𝑙 𝑘𝑗 𝑧 𝑘𝑗𝑚 ≤ 𝐿 𝐴 𝑗 (13)
    incorporate land demand by coal (12) and crop (13) production activities, where
    𝐿 𝐶 𝑗 and 𝐿 𝐴 𝑗 are land use constraints for coal and agricultural sectors in
    location 𝑗 , respectively. In Equation (12), parameter 𝑆 𝑖𝑗 defines the area that
    can deteriorate (e.g., subside) as a result of coal mining of unit coal 𝑖 in location
    𝑗 , Δ 𝑙 𝑗 is a portion of agricultural land overlapping with a coal field in location
    𝑗 , land reclamation rate (or efficiency rate) is defined by parameter 𝑟 𝑖𝑗 for
    coal 𝑖 in location 𝑗 , and 𝑔 𝑖𝑗 is a coal fraction that allows us to calculate
    the land under reject material. In Equation (13), parameter 𝑙 𝑘𝑗 defines the area
    required for a unit crop 𝑘 production in location 𝑗 . Equation (14) introduces
    the restriction on the total land use in location 𝑗 by energy and agricultural
    sectors ∑ 𝑘,𝑚 𝑙 𝑘𝑗 𝑦 𝑘𝑗𝑚 + ∑ 𝑖,𝑚,𝑡 𝑥 𝑖𝑗𝑚𝑡 (1− 𝑟 𝑖𝑗 )Δ 𝑙 𝑗 𝑙 𝑖𝑗 + 𝑔 𝑖𝑗 ∑ 𝑖,𝑚,𝑡
    𝑥 𝑖𝑗𝑚𝑡 ≤ 𝐿 𝑗 (14) Total available water 𝑊 𝑗 for both sectors and sectoral water
    quotas ( 𝑊 𝐸 𝑗 and 𝑊 𝐴 𝑗 ) significantly affect the choice of coal and crop (energy)
    production technologies through water utilization constraints: ∑ 𝑖,𝑚,𝑡 [ 𝑤 𝑃 𝑖𝑗
    + 𝑤 𝑑 𝑖𝑗 ] 𝑥 𝑖𝑗𝑚𝑡 ≤ 𝑊 𝐸 𝑗 (15) and ∑ 𝑘,𝑚 𝑤 𝑐 𝑘𝑗 𝑧 𝑘𝑚𝑗 ≤ 𝑊 𝐴 𝑗 (16) where 𝑊 𝐸 𝑗
    and 𝑊 𝐴 𝑗 are quotas on water use by coal and agricultural activities in location
    𝑗 , 𝑤 𝑃 𝑖𝑗 defines water requirement for a unit coal 𝑖 production in location
    𝑗 , 𝑤 𝑑 𝑖𝑗 is water required for a unit coal 𝑖 conversion in location 𝑗 , and
    𝑤 𝑐 𝑘𝑚 is water required for a unit crop k production in location j. Water use
    𝑊 𝐸 𝑗 for coal and 𝑊 𝐴 𝑗 for agricultural production are constrained by the total
    water 𝑊 𝑗 available in 𝑗 : 𝑊 𝐸 𝑗 + 𝑊 𝐴 𝑗  ≤ 𝑊 𝑗 (17) The models can be extended
    by including various other constraints, for example, water and air quality, SO2
    and CO2 emissions targets, biofuel production mandates, etc. In the condition
    of ASI, the planner does not have full information regarding separate LP energy
    ((8), (10), (12) and (15)) and agricultural ((9), (11), (13) and (14)) submodels.
    To link the models under joint constraints (14) and (17) we implement procedure
    (7). Thus, at the initial step s = 0, individual sectoral models are solved using
    initial sectoral land and water quotas 𝐿 𝐶 𝑗 (0) , 𝐿 𝐴 𝑗 (0) and 𝑊 𝐶 𝑗 (0) , 𝑊
    𝐴 𝑗 (0) . Resource quotas 𝑦 𝑠 =( 𝐿 𝐶 𝑗 (s) , 𝐿 𝐴 𝑗 (s),  𝑊 𝐶 𝑗 (s) , 𝑊 𝐴 𝑗 (s))
    at step 𝑠 are adjusted according to (7) using shadow prices (dual variables) of
    energy and agricultural sectors land and water resource constraints ∑ 𝑖,𝑚,𝑡 𝑥
    𝑖𝑗𝑚𝑡 (1− 𝑟 𝑖𝑗 )Δ 𝑙 𝑗 𝑆 𝑖𝑗 +𝑔 ∑ 𝑖,𝑚,𝑡 𝑥 𝑖𝑗𝑚𝑡 ≤ 𝐿 𝐶 𝑗 (𝑠−1) (18) ∑ 𝑘,𝑚 𝑙 𝑘𝑗 𝑦 𝑘𝑗𝑚
    ≤ 𝐿 𝐴 𝑗 (𝑠−1) (19) ∑ 𝑖,𝑚,𝑡 𝑤 𝑃 𝑖𝑗 𝑥 𝑖𝑚𝑙𝑡 + ∑ 𝑖,𝑚,𝑡 𝑤 𝑑 𝑖𝑗 𝑥 𝑖𝑗𝑚𝑡 ≤ 𝑊 𝐶 𝑗 (𝑠−1)
    (20) ∑ 𝑘,𝑚 𝑤 𝑐 𝑘𝑗 𝑦 𝑘𝑚𝑗 ≤ 𝑊 𝐴 𝑗 (𝑠−1) (21) and constraints (14) and (17). 3.2.
    Selected Results Results are compared for three cases: 1. Separately optimized
    energy and agricultural models; 2. The hard-linked integrated model (one-code
    model); 3. Separate models integrated via the linkage procedure (7). In case 1,
    the sectors are not restricted by joint resource constraints, and therefore the
    net profits can be higher than in cases 2 and 3; however, this is a misleading
    conclusion. In cases 2 and 3, results show that the iterative linkage process
    converges rather quickly. In 10 iterations, the optimal value of the integrated
    linked model (case 3) is almost equal to the optimal value of the integrated “hard-linked”
    model (case 2). In case 3, the proposed linkage algorithm allows to link models
    installed on remote computers through an iterative dialogue establishing optimal
    redistribution of water and land quotas among the sectors and locations. Figure
    1 illustrate the non-monotonic convergence of the linkage algorithm for three
    different scenarios of initial 𝑦 0 quotas allocated to energy and agricultural
    sectors. The choice of the step-size 𝜌 𝑠 in (7) affects the convergence rate,
    the value of the product 𝜌 𝑠 𝜈 𝑠 must correspond to the value of solutions 𝑦 𝑠
    . Figure 1. Convergence of the iterative linking procedure in terms of the goal
    function values 𝐹( 𝑦 𝑠 ) . Vertical axis displays net profits; the iteration step
    is marked on the horizontal axis. The three curves (Scen1, Scen2, Scen3) correspond
    to three different initial land and water quota scenarios at s = 0. 4. Conclusions
    In the paper, we consider the problem of linking separate distributed sectoral
    and/or regional optimization models into an inter-sectoral integrated model. The
    approach for linking models is based on an iterative algorithm that does not require
    models to exchange full information regarding their specifications. The resource
    quotas for each system and each resource are recalculated by systems independently
    and in parallel via shifting their current approximation in the direction defined
    by the corresponding dual variables from the primal sectoral optimization problem.
    In this way, the approach allows to avoid hard linking of the models in a single
    code that saves programming time and enables parallel distributed computations
    of sectoral models instead of a large-scale integrated model, i.e., addressing
    the well-known “curse of dimensionality” and large-scale data harmonization (management).
    This also preserves the original models in their initial state for other possible
    linkages. The proposed computational algorithm is based on an iterative stochastic
    quasigradient (SQG) procedure of, in general, nonsmooth nondifferentiable optimization
    converging to a socially optimal solution maximizing an implicit nested non-differentiable
    social welfare function. The convergence of the algorithm relies on the duality
    theory and non-differentiable optimization [20]. The iterative solution procedure
    can be used for robust estimation and machine learning problems. In particular,
    it can be viewed as an endogenous reinforced learning problem [32]. The iterative
    SQG-based methods and their stochastic versions are intended for the robust optimization
    of deterministic and stochastic systems with a large number of decision variables
    and scenarios of uncertainties due to the ability of these methods to link scenario-simulation
    and optimization procedures [42,43]. Therefore, the proposed method will be developed
    further for linking stochastic models enabling integrated management of global
    systemic risks, which are not detectable under traditional independent sectorial
    management ignoring cross-sectoral risk exposures. Fundamentally important to
    the possible extension of the presented method is the case of stochastic sectoral/regional
    models with interdependent systemic uncertainties and risks shaped by decisions
    of various agents. This includes the mitigation of floods by new land use decisions,
    for example, affecting flood scenarios. As a rule, this makes it impossible to
    separate scenario-generation and optimization processes calling for linking both
    simulation and optimization procedures in a way similar to algorithm (7), thus
    combining simulations of scenarios, new optimization steps, new simulation of
    scenarios, and so on. In this case, we can determine new types of machine learning
    processes. In the paper, we referred to linking regional and/or sectoral models.
    More generally, the problem can address the linkage of models at different spatial
    and temporal resolutions (e.g., local-global, considering more details of energy
    and natural resources dynamics in general). Therefore, the linkage problem can
    be formulated much more generally in terms of sub-models and integrated models,
    and the approach presented in this paper can still be applicable. The linkage
    of models is, in a sense, opposite to decomposition methods. While in the decomposition
    (e.g., [44,45]), we split an existing integrated optimization model into a number
    of smaller sub-models, in the linkage, we obtain an integrated model of the system
    by linking existing explicitly unknown sub-models. The proposed linkage procedure
    provides flexibility, enabling the simultaneous use of linkage and decomposition
    procedures, in other words, endogenously disaggregating models to make their further
    integration (linkage) more efficient. Author Contributions Conceptualization,
    Y.E., A.G.Z., V.L.B., T.E., P.H., E.R., N.K. and M.O.; methodology, Y.E., T.E.,
    E.R.; software, Y.E., T.E., E.R.; validation, Y.E., A.G.Z., V.L.B., T.E., P.H.,
    E.R., N.K. and M.O.; formal analysis, Y.E., A.G.Z., V.L.B., T.E., P.H., E.R.,
    N.K. and M.O.; investigation, Y.E., A.G.Z., V.L.B., T.E., P.H., E.R. and N.K.;
    data curation, T.E. and E.R.; writing—original draft preparation, Y.E., T.E.,
    P.H., E.R. and M.O.; writing—review and editing, Y.E., A.G.Z., V.L.B., T.E., P.H.,
    E.R., N.K. and M.O.; visualization, Y.E., T.E. and E.R. All authors have read
    and agreed to the published version of the manuscript. Funding This research was
    partially funded by EU projects COACCH (776479). Institutional Review Board Statement
    Not applicable. Informed Consent Statement Not applicable. Data Availability Statement
    The study does not report any data. Acknowledgments The development of linkage
    algorithms and case studies are part of methodological research of EU projects
    COACCH (776479) and a joint project between IIASA and National Academy of Sciences
    (Ukraine) on “Integrated robust management of food-energy-water-land use nexus
    for sustainable development”. Conflicts of Interest The authors declare no conflict
    of interest. Appendix A Appendix A.1. Convergence Proposition (stopping criterion,
    subgradients): Assume there exist solutions 𝑥 (𝑘) (𝑦) of all 𝐾 sectoral/regional
    models for feasible 𝑦 satisfying constraints (5). Then: (a) Functions 𝑓 (𝑘) (𝑦)=(
    𝑐 (𝑘) , 𝑥 (𝑘) (𝑦)) , 𝐹(𝑦)= ∑ 𝐾 𝑘=1 𝑓 (𝑘) ( 𝑥 (𝑘) ) are continuously concave non-differentiable
    functions for all 𝑘 . (b) The dual problem to (1)–(3) has a solution ( 𝑢 (𝑘) (𝑦),
    𝑣 (𝑘) (𝑦)) for all 𝑘 , and these solutions satisfy the stopping criterion of the
    linkage algorithm: 𝑓 (𝑘) (𝑦)=( 𝑐 (𝑘) , 𝑥 (𝑦))=( 𝑏 (𝑘) , 𝑢 (𝑦))+(𝑦, 𝑣 (𝑦)). From
    this proposition follows the following important fact ([8,10,25]), which is fundamental
    for solving the linkage problem through maximizing non-differentiable function
    𝐹(𝑦) using algorithm (7): (c) For any feasible solution 𝑧 and 𝑦 , 𝑓 (𝑘) (𝑦)− 𝑓
    (𝑘) (𝑧)≥( 𝑣 (𝑘) (𝑦),𝑦−𝑧) , that is, 𝑣 (𝑘) (𝑦) is a subgradient of the concave
    non-differentiable function 𝑓 (𝑘) (𝑦). Vector 𝑣(𝑦)=( 𝑣 (1) (𝑦),…, 𝑣 (𝐾) (𝑦)) is
    a subgradient of function 𝐹(𝑦)= ∑ 𝐾 𝑘=1 𝑓 (𝑘) (𝑦) , 𝐹 𝑦 (𝑦)=𝑣(𝑦) , that is, 𝐹(𝑦)−𝐹(𝑧)≥(𝑣(𝑦),𝑦−𝑧)
    . Therefore, procedure (7) is a specific subgradient algorithm for maximizing
    the (continuously) non-differentiable concave function 𝐹(𝑦) . The following proposition
    shows that 𝑦 𝑠 converges to an optimal linking vector 𝑦 ∗ , maximizing 𝐹(𝑦) subject
    to joint constraints (5). Let us denote this feasible set by 𝑌 . Appendix A.2.
    Convergence Theorem (Non-Monotonic Convergence) Assume that (1) The feasible set
    𝑌 is bounded; (2) Step size 𝜌 𝑠 satisfies the conditions: 𝜌 𝑠 ≥0 , ∑ ∞ 𝑠=1 𝜌 𝑠
    =∞ , ∑ ∞ 𝑠=1 𝜌 2 𝑠 <∞ , say 𝑝 𝑠 =1/𝑠 . Then 𝑙𝑖𝑚  𝑦 𝑠 ∈ 𝑌 ∗ for 𝑠→∞ . The following
    sequence of 𝜌 𝑠 satisfies the conditions of the theorem: 𝜌 𝑠 = 𝛾 𝑠 /𝑠 , 0≤ 𝛾 
        ≤ 𝛾 𝑠 ≤ 𝛾      <∞ for some positive constants 𝛾      and 𝛾 
        . Appendix A.3. Computing the Projection The orthogonal projection 𝑦 𝑠+1
    of vector 𝑦 ¯ 𝑠 = 𝑦 𝑠 + 𝜌 𝑠 𝑣 𝑠 onto 𝑌 is calculated by minimizing the quadratic
    function ∥ 𝑦 𝑠 + 𝜌 𝑠 𝑣 𝑠 − 𝑦∥ 2 subject to joint constraints (5). This minimization
    is very fast due to 𝜌 𝑠 𝜈 𝑠 →0 , as vectors 𝑣 𝑠 are bounded optimal dual solutions,
    and if 𝑦 𝑠 is used as an initial approximation for 𝑦 𝑠+1 . Appendix A.4. Mixed
    Constraints Joint constraints (5) may have the following form: ∑ 𝑘=1 𝐾 𝑀 (𝑘) 𝑥
    (𝑘) + ∑ 𝑘=1 𝐾 𝐷 (𝑘) 𝑦 (𝑘) ≤𝛿 (A1) with some matrices 𝑀 (𝑘) . Yet, problem (1)–(4)
    s.t. (A1) can be reformulated similar to problem (1)–(5). Let us define vectors
    𝑦 (𝐾+𝑘) such that 𝑀 (𝑘) 𝑥 (𝑘) = 𝑦 (𝐾+𝑘) , 𝑘=1,…,𝐾. Now it is possible to rewrite
    (A1) as ∑ 𝐾 𝑘=1 𝐷 (𝑘) 𝑦 (𝑘) ≤𝛿− ∑ 2𝐾 𝑘=𝑘+1 𝑦 (𝑘) and after some renotation, derive
    the problem in the form (1)–(5). References Carter, N. Energy’s Water Demand:
    Trends, Vulnerabilities, and Management. CRS (Congressional Research Service)
    Report for Congress, 7-5700, R41507. 2020. Available online: https://digital.library.unt.edu/ark:/67531/metadc31387/
    (accessed on 4 January 2021). Baffes, J.; Dennis, A. Long Term Drivers of Food
    Prices. In Working Paper 2013; World Bank: Washington, DC, USA, 2013. [Google
    Scholar] Taghizadeh-Hesary, F.; Rasoulinezhad, E.; Yoshino, N. Energy and food
    security: Linkages through price volatility. Energy Policy 2019, 128, 796–806.
    [Google Scholar] [CrossRef] Van Eyden, R.; Difeto, M.; Gupta, R.; Wohar, M.E.
    Oil price volatility and economic growth: Evidence from advanced economies using
    more than a century’s data. Appl. Energy 2019, 233–234, 612–621. [Google Scholar]
    [CrossRef] [Green Version] Grafton, Q.; McLindin, M.; Hussey, K.; Wyrwoll, P.;
    Wichelns, D.; Ringler, C.; Garrick, D.; Pittock, J.; Wheeler, S.; Orr, S.; et
    al. Responding to global challenges in Food, Energy, Environment and Water: Risks
    and options assessment for decision-making. Asia Pac. Policy Stud. 2016, 3, 275–299.
    [Google Scholar] [CrossRef] [Green Version] Howells, M.; Hermann, S.; Welsch,
    M.; Bazilian, M.; Segerstrom, R.; Alfstad, T.; Gielen, D.; Rogner, H.-H.; Fischer,
    G.; Van Velthuizen, H.; et al. Integrated analysis of climate change, land-use,
    energy and water strategies. Nat. Clim. Chang. 2013, 3, 621–626. [Google Scholar]
    [CrossRef] Foster, E.; Contestabile, M.; Blazquez, J.; Manzano, B. The unstudied
    barriers to widespread renewable energy deployment: Fossil fuel price responses.
    Energy Policy 2017, 103, 258–264. [Google Scholar] [CrossRef] Cansado-Bravo, P.;
    Rodríguez-Monroy, C. The Effects of structural breaks on energy resources in the
    long run. Evidence from the last two oil price crashes before COVID-19. Designs
    2020, 4, 49. [Google Scholar] [CrossRef] Gritsevskyi, A.; Nakicenovic, N. Modeling
    uncertainty of induced technological change. Energy Policy 2000, 26, 907–921.
    [Google Scholar] [CrossRef] [Green Version] Messner, S.; Golodnikov, A.; Gritsevskyi,
    A. A stochastic version of the dynamic linear programming model MESSAGE III. Energy
    1996, 21, 775–784. [Google Scholar] [CrossRef] [Green Version] Cano, E.L.; Moguerza,
    J.M.; Ermolieva, T.; Yermoliev, Y. A strategic decision support system framework
    for energy-efficient technology investments. TOP 2016, 25, 249–270. [Google Scholar]
    [CrossRef] [Green Version] Dodder, R.; Elobeid, A.; Johnson, T.; Kaplan, P.; Kurkalova,
    L.; Tokgoz, S. Environmental Impacts of Emerging Biomass Feedstock Markets: Energy,
    Agriculture, and the Farmer; CARD Working Paper Series; Iowa State University:
    Ames, IA, USA, 2011. [Google Scholar] Havlík, P.; Schneider, U.A.; Schmid, E.;
    Boettcher, H.; Fritz, S.; Skalský, R.; Aoki, K.; de Cara, S.; Kindermann, G.;
    Kraxner, F.; et al. Global land-use implications of first and second generation
    biofuel targets. Energy Policy 2011, 39, 5690–5702. [Google Scholar] [CrossRef]
    Borodina, O.; Borodina, E.; Ermolieva, T.; Ermoliev, Y.; Fischer, G.; Makowski,
    M.; van Velthuizen, H. Sustainable agriculture, food security, and socio-economic
    risks in Ukraine. In Managing Safety of Heterogeneous Systems, Lecture Notes in
    Economics and Mathematical Systems; Ermoliev, Y., Makowski, M., Marti, K., Eds.;
    Springer: Berlin/Heidelberg, Germany, 2012; pp. 169–185. [Google Scholar] Gao,
    J.; Xu, X.; Cao, G.; Ermoliev, Y.M.; Ermolieva, T.Y.; Rovenskaya, E.A. Optimizing
    regional food and energy production under limited water availability through integrated
    modeling. Sustainability 2018, 10, 1689. [Google Scholar] [CrossRef] [Green Version]
    Gambhir, A.; Butnar, I.; Li, P.-H.; Smith, P.; Strachan, N. A review of criticisms
    of Integrated Assessment Models and proposed approaches to address these through
    the lens of BECCS. Energies 2019, 12, 1747. [Google Scholar] [CrossRef] [Green
    Version] Bosetti, V.; Marangoni, G.; Borgonovo, E.; Diaz Anadon, L.; Barron, R.;
    McJeon, H.C.; Politis, S.; Friley, P. Sensitivity to energy technology costs:
    A multi-model comparison analysis. Energy Policy 2015, 80, 244–263. [Google Scholar]
    [CrossRef] [Green Version] Gielen, D.J.; Gerlagh, T.; Bos, A.J.M. MATTER 1.0—A
    MARKAL Energy and Materials System–Model Characterisation; ECN Report ECN-C-98-085;
    ECN: Petten, The Netherlands, 1998. [Google Scholar] Doukas, H.; Nikas, A.; González-Eguino,
    M.; Arto, I.; Anger-Kraavi, A. From Integrated to integrative: Delivering on the
    Paris Agreement. Sustainability 2018, 10, 2299. [Google Scholar] [CrossRef] [Green
    Version] Harker, P.T. Generalized Nash games and quasi-variational inequalities.
    Eur. J. Oper. Res. 1991, 54, 81–94. [Google Scholar] [CrossRef] Böhringer, C.;
    Rutherford, T.F. Integrated assessment of energy policies: Decomposing top-down
    and bottom-up. J. Econ. Dyn. Control 2009, 33, 1648–1661. [Google Scholar] [CrossRef]
    Ermoliev, Y.; von Winterfeldt, D. Systemic risks and security management. In Managing
    Safety of Heterogeneous Systems: Decisions under Uncertainty and Risks. Lecture
    Notes in Econom. Math. Systems; Ermoliev, Y., Makowski, M., Marti, K., Eds.; Springer:
    New York, NY, USA, 2012; Volume 658, pp. 19–49. [Google Scholar] Ogryczak, W.
    Multiple criteria linar programming model for portfolio selection. Ann. Oper.
    Res. 2000, 97, 143–162. [Google Scholar] [CrossRef] Ermoliev, Y. Two-stage stochastic
    programming: Stochastic quasigradient methods. In Encyclopedia of Optimization;
    Floudas, C.A., Pardalos, P.M., Eds.; Springer: New York, NY, USA, 2009; pp. 3955–3959.
    [Google Scholar] Ermoliev, Y. Methods of Stochastic Programming; Nauka: Moscow,
    Russia, 1976. (In Russian) [Google Scholar] Ermoliev, Y.; Norkin, V. On nonsmooth
    and discontinuous problems of stochastic systems optimization. Eur. J. Oper. Res.
    1997, 101, 230–244. [Google Scholar] [CrossRef] Rockafeller, T. The Theory of
    Subgradient and Its Application to Problems of Optimization: Convex and Nonconvex
    Functions; Helderman: Berlin, Germany, 1981. [Google Scholar] Ermoliev, Y.; Klaassen,
    G.; Nentjes, A. Incomplete Information and the Cost-Efficiency of Ambient Charges;
    Working Paper WP-93-72; International Institute for Applied Systems Analysis,
    IIASA: Laxenburg, Austria, 1993. [Google Scholar] Ermolieva, T.; Havlik, P.; Ermoliev,
    Y.; Khabarov, N.; Obersteiner, M. Robust management of systemic risks and food-water-energy-environmental
    security: Two-stage strategic-adaptive GLOBIOM model. Sustainability 2021, 13,
    857. [Google Scholar] [CrossRef] Ermoliev, Y.; Ermolieva, T.; Jonas, M.; Obersteiner,
    M.; Wagner, F.; Winiwarter, W. Integrated model for robust emission trading under
    uncertainties: Cost-effectiveness and environmental safety. Technol. Forecast.
    Soc. Chang. 2015, 98, 234–244. [Google Scholar] [CrossRef] Ermoliev, Y.; Michalevich,
    M.; Uteuliev, N.U. Economic modeling of international water use (The case of the
    Aral Sea Basin). Cybern. Syst. Anal. 1995, 30, 10–19. [Google Scholar] [CrossRef]
    [Green Version] Ermolieva, T.; Ermoliev, Y.; Obersteiner, M.; Rovenskaya, E. Two-Stage
    Nonsmooth Stochastic Optimization and Iterative Stochastic Quasigradient Procedure
    for Robust Estimation, Machine Learning and Decision Making. In Resilience in
    the Digital Age. Lecture Notes in Computer Science, Vol 12660; Roberts, F.S.,
    Sheremet, I.A., Eds.; Springer: Berlin/Heidelberg, Germany, 2021; pp. 45–74. [Google
    Scholar] [CrossRef] Norkin, V. Reducing models of general economic equilibrium
    to optimization problems. Cybern. Syst. Anal. 1999, 35, 743–753. [Google Scholar]
    [CrossRef] Prakash, R.; Nygard, K.E. Distributed Linear Programming Models in
    a Smart Grid; Springer International Publishing: Berlin/Heidelberg, Germany, 2017.
    [Google Scholar] Alemany, M.; Esteso, A.; Ortiz, A.; Pino, M.D. Centralized and
    distributed optimization models for the multi-farmer crop planning problem under
    uncertainty: Application to a fresh tomato Argentinean supply chain case study.
    Comput. Ind. Eng. 2021, 153, 107048. [Google Scholar] [CrossRef] Liang, S.; Wang,
    L.; Yin, G. Distributed smooth convex optimization with coupled constraints. IEEE
    Trans. Autom. Control. 2019, 1, 347–353. [Google Scholar] [CrossRef] Yang, T.;
    Yi, X.; Wu, J.; Yuan, Y.; Wu, D.; Meng, Z.; Hong, Y.; Wang, H.; Lin, Z.; Johansson,
    K. A survey of distributed optimization. Annu. Rev. Control 2019, 47, 278–305.
    [Google Scholar] [CrossRef] Hughes, J.; Juntao, C. Fair and distributed dynamic
    optimal transport for resource allocation over networks. In Proceedings of the
    2021 55th Annual Conference on Information Sciences and Systems (CISS), Baltimore,
    MD, USA, 24–26 March 2021; IEEE: Piscataway, NJ, USA, 2021; pp. 1–6. [Google Scholar]
    Dean, R.; Cortés, J. Robust distributed linear programming. IEEE Trans. Autom.
    Control 2015, 60, 2567–2582. [Google Scholar] Villani, C. Optimal Transport: Old
    and New; Springer: Berlin/Heidelberg, Germany, 2009. [Google Scholar] Galichon,
    A. Optimal Transport Methods in Economics; Princeton University Press: Princeton,
    NJ, USA, 2016. [Google Scholar] Ermolieva, T.; Obersteiner, M. Abrupt climate
    change: Lessons from integrated catastrophic risks management. World Resour. Rev.
    2004, 16, 57–82. [Google Scholar] Ermoliev, Y.M.; Robinson, S.M.; Rovenskaya,
    E.; Ermolieva, T. Integrated catastrophic risk management: Robust balance between
    ex-ante and ex-post measures. SIAM News 2018, 51, 4. [Google Scholar] Dantzig,
    G.B.; Wolfe, P. The decomposition principle for linear programming. Econometrica
    1960, 29, 767–778. [Google Scholar] [CrossRef] Kim, K.; Nazareth, J.L. The decomposition
    principle and algorithms for linear programming. Linear Algebra Appl. 1991, 152,
    119–133. [Google Scholar] [CrossRef] [Green Version] Publisher’s Note: MDPI stays
    neutral with regard to jurisdictional claims in published maps and institutional
    affiliations.  © 2022 by the authors. Licensee MDPI, Basel, Switzerland. This
    article is an open access article distributed under the terms and conditions of
    the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Ermoliev, Y.; Zagorodny, A.G.; Bogdanov, V.L.;
    Ermolieva, T.; Havlik, P.; Rovenskaya, E.; Komendantova, N.; Obersteiner, M. Linking
    Distributed Optimization Models for Food, Water, and Energy Security Nexus Management.
    Sustainability 2022, 14, 1255. https://doi.org/10.3390/su14031255 AMA Style Ermoliev
    Y, Zagorodny AG, Bogdanov VL, Ermolieva T, Havlik P, Rovenskaya E, Komendantova
    N, Obersteiner M. Linking Distributed Optimization Models for Food, Water, and
    Energy Security Nexus Management. Sustainability. 2022; 14(3):1255. https://doi.org/10.3390/su14031255
    Chicago/Turabian Style Ermoliev, Yuri, Anatolij G. Zagorodny, Vjacheslav L. Bogdanov,
    Tatiana Ermolieva, Petr Havlik, Elena Rovenskaya, Nadejda Komendantova, and Michael
    Obersteiner. 2022. \"Linking Distributed Optimization Models for Food, Water,
    and Energy Security Nexus Management\" Sustainability 14, no. 3: 1255. https://doi.org/10.3390/su14031255
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   7
    Scopus   6 Web of Science   4 Google Scholar   [click to view] Article Access
    Statistics Article access statistics Article Views 6. Jan 16. Jan 26. Jan 5. Feb
    15. Feb 25. Feb 6. Mar 16. Mar 26. Mar 0 500 1000 1500 2000 2500 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Sustainability, EISSN 2071-1050, Published by MDPI
    RSS Content Alert Further Information Article Processing Charges Pay an Invoice
    Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers
    For Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sustainability (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Linking Distributed Optimization Models for Food, Water, and Energy Security
    Nexus Management
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Akshay R.
  - Tarun G.
  - Kiran P.U.
  - Devi K.D.
  - Vidhyalakshmi M.
  citation_count: '0'
  description: One of the most serious and alarming problems facing humanity is the
    degradation of natural water resources such as lakes as well as rivers is one
    of the most serious and vexing problems we are facing today. The long-term effects
    of polluted water affect all areas of life. Therefore, it is essential to manage
    water resources if you want tomaximize the quality of your water. If data are
    examinedand water quality can be predicted, the effects of watercontamination
    can be dealt with effectively. The purpose of this study is to use machine learning
    to make a water qualityprediction model based on water quality measurements. Machine
    learning can be used for building models fromalgorithms with some data gathered
    from the sick ones. For a better examination of parametric findings, the acquired
    data will be pre-processed, separated into training and testing parts, and subjected
    to machine learning classification techniques. Decision tree, Naive Bayes, Random
    Forest, Support Vector Machine, and K-Nearest Neighbor are some of the classification-type
    algorithms employed in this work. All the model's performance indicators are calculated,
    and they change for each model. A technique for improving machine learning model
    performance metrics is hyperparameter tuning.
  doi: 10.1109/SMART55829.2022.10047533
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2022 11th International Confe... Water-Quality-Analysis
    using Machine Learning Publisher: IEEE Cite This PDF Raavi Akshay; Gadiraju Tarun;
    Pinapothini Uday Kiran; K Durga Devi; M. Vidhyalakshmi All Authors 1 Cites in
    Paper 171 Full Text Views Abstract Document Sections I. Introduction II. Literature
    Survey III. Proposed Method VI. Methodology & Evaluation IV. Results and Discussion
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    One of the most serious and alarming problems facing humanity is the degradation
    of natural water resources such as lakes as well as rivers is one of the most
    serious and vexing problems we are facing today. The long-term effects of polluted
    water affect all areas of life. Therefore, it is essential to manage water resources
    if you want tomaximize the quality of your water. If data are examinedand water
    quality can be predicted, the effects of watercontamination can be dealt with
    effectively. The purpose of this study is to use machine learning to make a water
    qualityprediction model based on water quality measurements. Machine learning
    can be used for building models fromalgorithms with some data gathered from the
    sick ones. For a better examination of parametric findings, the acquired data
    will be pre-processed, separated into training and testing parts, and subjected
    to machine learning classification techniques. Decision tree, Naive Bayes, Random
    Forest, Support Vector Machine, and K-Nearest Neighbor are some of the classification-type
    algorithms employed in this work. All the model''s performance indicators are
    calculated, and they change for each model. A technique for improving machine
    learning model performance metrics is hyperparameter tuning. Published in: 2022
    11th International Conference on System Modeling & Advancement in Research Trends
    (SMART) Date of Conference: 16-17 December 2022 Date Added to IEEE Xplore: 24
    February 2023 ISBN Information: ISSN Information: DOI: 10.1109/SMART55829.2022.10047533
    Publisher: IEEE Conference Location: Moradabad, India SECTION I. Introduction
    In the scientific field of machine learning, it is investigated how computers
    learn via experience. Since the capacity to learn is the fundamental quality of
    an entity regarded as intelligent in the broadest meaning of the word, the words
    “Machine Learning” and “Artificial Intelligence” are frequently used synonymously
    in the minds of scientists. Building adaptable, experience-based computer systems
    is the goal of machine learning. It is now possible to discover a solution to
    this problem because of the development of machine learning methods. We have developed
    a technique that uses data mining to identify whether the water is portable or
    not. The enormous amount of data related to water quality can be mined for hidden
    knowledge. As a result, it now has a more significant role in the study. This
    research aims to develop a system that can predict water quality more precisely.
    SECTION II. Literature Survey An analysis of the use of soft computing techniques
    in water resources and tested support Vector Machine Algorithm in analysing water
    quality in clear streams [1], [2]. [16] proposed significant opportunities exist
    for improving the classification and forecasting of water quality with artificial
    intelligence (AI). In this paper, they proposed a reliable framework for classifying
    water quality using machine learning methods (WQ). This study compares various
    AI algorithms to manage water quality data accumulated over time and offers a
    trustworthy approach for projecting water quality as precisely as feasible. [17]
    this study examines the effectiveness of artificial intelligence approaches such
    as artificial neural network (ANN), group method of data handling (GMDH), and
    support vector machine (SVM) for forecasting water quality using machine learning
    techniques. Different transfer and kernel function types were investigated to
    construct the ANN and SVM, respectively. [18] this work provides an intelligent
    wireless sensor network (WSN) system for water quality measurement utilizing machine
    learning that can assess the river water quality. This study''s principal goal
    as a case study is to evaluate the river''s status along its path by producing
    data reports into an interactive user interface. Utilizing machine learning to
    analyse and forecast water quality: Our analysis of the literature suggests that
    a complete model is needed to comprehend what safe, potablewater is and to identify
    it from non-potable water using machine learning approaches. ANN has received
    widespread recognition as a tool for classifying complicated information, including
    those environmental dynamics. It describes the complex water quality dataset''s
    non-linear connection. [15] Predict irrigation water quality characteristics in
    a semi- arid environment using machine learning models. Conventional methods for
    evaluating water suitability for irrigation are typically expensive because they
    call for multiple characteristics, especially in underdeveloped nations. Therefore,
    creating precise and trustworthy models could be useful to solve this problem
    of managing the water utilized in agriculture. Eight Machine Learning (ML) models
    are utilized to do this. [19] The cutting-edge artificial intelligence algorithms
    NARNET and LSTM models were used to forecast the WQI utilizing the proposed technique.
    Additionally, the WQI data was categorized utilizingmachine learning techniques
    like SVM, KNN, and Naive Bayes. The proposed models were assessed and looked at
    using a few statistical factors. Water Quality Prediction Using Artificial Intelligence
    Algorithms. [20] For the conservation of the water environment, water qualityprediction
    is very important. In this research, A method for predicting water quality based
    on IGRA and LSTM is suggested, considering the multivariate correlation and temporal
    sequence of the water quality information. From the survey made [3]–[15], the
    major research gaps were listed below: This research on the effectiveness of current
    or planned strategies for addressing WS&D needs a lot of information on these
    causal relationships, as well as the original problems with water scarcity and
    drought and their explanatory factors. There is a great need for knowledge of
    these causal links, as well as the initial issues with water scarcity and drought
    and their explanatory factors, in this research on the success of present or proposed
    measures for treating WS&D. They are frequently the least accurate and unreliable
    data regarding water resources (Gleick, 2006). Other investigations concluded
    that it is difficult to find accurate and comprehensive data on water availability
    and demand (Gleick et al., 2002). Consequently, there is insufficient information
    regarding the motivators, constraints, and effects of the measures and support
    actions. SECTION III. Proposed Method Analyzing water quality is an important
    research area. Here shows the proposed step-by-step process to predictwater quality.
    Fig. 1: Overall architecture Show All The machine learning model is used to detect
    whether the water is potable or non-potable. Import relevant libraries to test
    and train our data set and required to install some packages related to nature-inspired
    algorithms. Split the data as training data set and testing dataset they should
    bein the ratio 80:20 respectively and perform the Model Selection. The Support
    Vector Classifier (SVC), Decision Tree, GaussianNB, Random Forest and XGBoost
    are these different classifiersthat are taken into consideration. The model is
    assessed using several performance measures, suchas Test Accuracy and Train Accuracy.
    We got to get more Accuracy by using Hyperparameter Tuning in the Random Forest
    algorithm. Based on different performance metrics values the classifier with the
    greatest value is considered the best model. Data Set Dataset consists of observations
    of water quality for 3276 different sources of water: Ph The water''s pH (0 to
    14). According to EPA recommendations, tap water''s pH should range between (6.5
    and 8.5). The pH level is a crucial factor in determining the acid-base nature
    of water. Additionally, it shows if the water is either alkaline or acidic. The
    present investigation''s range fell between 6.52 to 6.83, which is within WHO
    guidelines. Hardness It is the amount of soap that may dissolve in one litre of
    water. Salts made of calcium and magnesium are the major causes of hardness. How
    long water is exposed to a hardness-producing substance influences how hard the
    water is while it is in the raw state. The ability of water to form soap due to
    calcium and magnesium precipitation was the original definition of hardness. Total
    Dissolved Solids (TDS) Water can dissolve a wide variety of chemicals and certain
    organic minerals or salts, including sodium, calcium, iron, zinc, bicarbonate
    ions, chloride ions, magnesium, and sulphates. These minerals affected the water''s
    appearance and gave it foul smells. This is an important consideration while using
    water. A highTDS rating indicates that the water contains a lot of minerals. For
    drinking purposes, the maximum and desired TDS limits are 500 mg/1 and 100 mg/1,
    respectively. Sulfates These are the organic substances that are found naturally
    in minerals, soil, and rocks. They are present in the air, groundwater, plants,
    and food in the area. Sulfate is mostly utilized for business purposes in the
    chemical sector. Around 2,700 mg/L of sulphate can be found in seawater. While
    certain places have significantly higher levels (1000 mg/L), most freshwater sources
    have values between 3 and 30 mg/L. Conductivity Pure water is great insulation
    of electrical current. By raising the ion concentration, the liquid''s electric
    conductivity has been enhanced. The quantity of dissolved particles in the liquid
    often determines its conductivity. Electrical Conductivity measures how well they
    carry electricity through their ionic mechanism (EC). WHO recommendations state
    that the EC value shouldn''t be higher than 400 S/cm. Chloramines The two primary
    disinfectants used in public water systems are chloride and chlorine. Ammonia
    is used in combination with chlorine to clean potable water. Drinking water can
    include up to 4 mg/L of chlorine, which is regarded as a safe quantity. Potability
    It is a metric for determining whether water is fit for human consumption. Unpotable
    equals zero (0), while potable is one (1). SECTION VI. Methodology & Evaluation
    A. Data Description The dataset consists of observations of water quality for
    3276 different sources of water: Fig. 1: Data description Show All B. Data Pre-Processing
    The data quality must be improved at the processing stage of the data analysis
    process. The Water quality index has been determined in this phase using the important
    dataset parameters. The act of converting collected data into something an algorithm
    for machine learning can use is known as data preparation. The most important
    and first stage in building an algorithm for machine learning is this one. Remove
    all instances where the value is 0. (zero). Zero is not a possible value. Therefore,
    this instance is terminated. The process of deciding on feature subsets, which
    decreases the dimension of the data and helps to work more quickly, involves removing
    irrelevant characteristics and instances. Fig. 2 Data processing Show All C. Histogram
    Visualize each aspect of the provided data set as shown below; now examine the
    graphs. It demonstrates how each feature and label is dispersed across many ranges,
    further demonstrating the necessity of scalability. Second, categorical variables
    are indicated whenever there are discrete bars. Before using machine learning,
    we need to address these categorical data. Fig. 3: Histogram Show All D. Correlation
    Matrix By Visualizing the correlation of all characteristics using a thermal foot
    map function. But you can see from the heat map below that there is no correlation
    between any characteristic; this means that we cannot reduce the dimension. Fig.
    4: Correlation matrix Show All E. Data Standardization In a scaling procedure
    called data standardization centers, the values on the mean of the unit standard
    deviation. Data standardization is the act of converting data into a commonly
    accepted format so that users may analyze and process it. We can import Data Standardization
    in many ways. It helps build unique, consistently defined components and features
    by providing acomprehensive catalogue of your data. No matter what insights or
    problems you''re trying to address, a solid understanding of your data is a necessary
    first step. To get there, the information must be changed into a dependable format
    with clear meanings. F. Training and Testing of Data In machine learning, the
    model is instructed to perform a variety of tasks using a training set of data.
    The model is trained using certain features from the training set. Therefore,
    the prototype contains these structures. Words or word clusters are taken from
    tweets for sentiment analysis. They build connections, understand concepts, come
    to judgments, and assess their level of confidence using the training data. The
    quality and quantity of the Machine Learning training data, we use determines
    how well our data project performs, just as much as the algorithms they do. As
    a result, provided the training set is correctly labelled, the model will be able
    to learn about the features. Divide the information into independent and dependent
    features. Fig. 5: Data standardization Show All G. Decision Tree The decision
    tree is a Machine Learning algorithm, it is mostly focused on classification-related
    issues. The decision tree has a structured classifier in which the nodes within
    display the components of a particular dataset. Decisionnodes and leaf nodes are
    both types of nodes found in decision trees. H. Logistic Regression It is machine
    learning which is used for the binary classification model where one of two possible
    values is taken as output. Logistic regression is our output prediction that can
    take either of these values. Logistic regression is simpler to use, analyze, and
    train. I. Support Vector Machine The SVM is an algorithm which is used in machine
    learning to categorize the task. It is frequently used for classification problems.
    SVM separates the data into two classes by mapping the data points to a high-dimensional
    space and then locating the best hyperplane. J. Random Forest Classifier The popular
    learning algorithm Random Forest is a part of the supervised learning methodology.
    It may be applied to ML issues involving both classification and regression. It
    is built on the idea of ensemble learning, which is a method of integrating many
    classifiers to address difficult issues and enhance model performance. Random
    Forest, as the name implies, is a classifier that uses several decision trees
    on different subsets of the provided dataset and averages them to increase the
    dataset''s prediction accuracy. Instead, then depending on a single decision tree,
    the random forest uses forecasts from each tree and predicts the result based
    on the votes of most predictions. K. XGBoost Extreme Gradient Boosting is a framework
    that can run on multiple languages. It is popular supervised learning which works
    on large datasets. It is implemented on top of the gradient boost. The way the
    XGBoost algorithm is designed to work uses the parallelization concept. It uses
    sequentially generated shallow decision trees and a highly scalable training method
    to minimize overfitting to deliver accurate results. L. Performance Metrics Accuracy
    Accuracy is measured as the total count of actual predictions to the available
    predictions and it is multiplied by 100. Precision The ratio of actual positives
    to the total available positives is known as precision. Recall It mainly focuses
    on type-2 errors the ratio of true positives to false negatives is called recall.
    F1-Score The harmonic mean performance metric parameters precision with recall
    known as f1-score. SECTION IV. Results and Discussion 1) Support Vector Machine
    Support Vector Machine is giving an Accuracy of 62 per cent. Fig. 6: Svm Show
    All 2) Random Forest Classifier Random Forest Classifier is giving an Accuracy
    of 70 per cent. Fig. 7: Random forest classifier Show All 3) K Nearest Neighbour
    K Nearest Neighbour is giving an Accuracy of 65 per cent. Fig. 8: KNN Show All
    4) Decision Tree Decision Tree is giving an Accuracy of 62 per cent. Fig. 9: Decision
    tree Show All 5) GaussianNB GaussianNB is giving an Accuracy of 62 per cent. Fig.
    10: Gaussiannb Show All 6) XGBoost XGBoost is giving an Accuracy of 64 per cent.
    Fig. 11: Xgboost Show All Performance Metrics for machine learning algorithms
    with Hyperparameter Tuning. Fig. 12: Performance metrics Show All Random Forest
    worked the best to train the model, giving us an f1 score (Balanced with precision
    & recall) of around 70%. SECTION V. Conclusion Future cities would benefit from
    real-time monitoring and evaluation of water quality due to the advancement of
    machine learning techniques. This work presented the results of our most recent
    literature analysis and comparative recent studies on the assessment of water
    quality using big data analytics and machine learning models and methods. Finally,
    it offers a few insights into theproblems, demands, and needs of future studies.
    Environmental protection greatly benefits from the modelling and forecasting of
    water quality. The algorithm implemented in this work improves the performance
    of water quality classifiers. We previously examined the performance metrics of
    machine learning algorithms, andwe found that by utilizing Hyperparameter Tuning
    along with Random Forest Classifier, we delivered a better improvement in the
    execution of different performance metrics of the models using Hyperparameter
    Tuning. We have got better improvement in performance metrics. This strategy may
    be applied and improved for automated water quality monitoring. Authors Figures
    References Citations Keywords Metrics More Like This A Comprehensive Analysis
    of the Effectiveness of Machine Learning Algorithms for Predicting Water Quality
    2023 International Conference on Innovative Data Communication Technologies and
    Application (ICIDCA) Published: 2023 A Proficient Prediction Mechanism for Analyzing
    Water Quality Using Machine Learning Algorithms 2023 International Conference
    on Self Sustainable Artificial Intelligence Systems (ICSSAS) Published: 2023 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the 2022 11th International Conference on System Modeling
    and Advancement in Research Trends, SMART 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Water-Quality-Analysis using Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Serino V.
  - Cavaliere D.
  - Senatore S.
  citation_count: '3'
  description: IoT technology spread led to the development of smart solutions for
    Precision Agriculture, employing multiple smart sensors to acquire and process
    data to support vegetation monitoring and crucial tasks such as seeding, irrigation,
    etc. to improve crop quality and production. However, the gathering of data from
    multiple devices requires a data integration stage that strictly depends on the
    context and features of the environment, including the type of environment, species,
    and phenology of the area considered. To this purpose, this paper introduces a
    multi-agent model that allows a swarm of IoT devices to perform environmental
    monitoring and anomaly detection on Regions of Interest (ROIs) accomplishing several
    tasks, including harmonization of spectral images taken from different sources,
    phenology data extraction about ROIs to build contextual knowledge over the ROIs
    and anomaly detection through vegetation index classification. A case study in
    a simulated real-time environment demonstrates the potential of the model to promptly
    alert humans about ROIs affected by critical vegetation, burned areas, and ROIs
    that can be at risk after critical events occurred in their surroundings.
  doi: 10.1109/ICETCI51973.2021.9574046
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2021 International Conference... Sensing
    multi-agent system for anomaly detection on crop fields exploiting the phenological
    and historical context Publisher: IEEE Cite This PDF Valerio Serino; Danilo Cavaliere;
    Sabrina Senatore All Authors 137 Full Text Views Abstract Document Sections I.
    Introduction II. The approach III. A Case Study In Simulated Real-Time IV. Conclusion
    Authors Figures References Keywords Metrics Footnotes Abstract: IoT technology
    spread led to the development of smart solutions for Precision Agriculture, employing
    multiple smart sensors to acquire and process data to support vegetation monitoring
    and crucial tasks such as seeding, irrigation, etc. to improve crop quality and
    production. However, the gathering of data from multiple devices requires a data
    integration stage that strictly depends on the context and features of the environment,
    including the type of environment, species, and phenology of the area considered.
    To this purpose, this paper introduces a multi-agent model that allows a swarm
    of IoT devices to perform environmental monitoring and anomaly detection on Regions
    of Interest (ROIs) accomplishing several tasks, including harmonization of spectral
    images taken from different sources, phenology data extraction about ROIs to build
    contextual knowledge over the ROIs and anomaly detection through vegetation index
    classification. A case study in a simulated real-time environment demonstrates
    the potential of the model to promptly alert humans about ROIs affected by critical
    vegetation, burned areas, and ROIs that can be at risk after critical events occurred
    in their surroundings. Published in: 2021 International Conference on Emerging
    Techniques in Computational Intelligence (ICETCI) Date of Conference: 25-27 August
    2021 Date Added to IEEE Xplore: 27 October 2021 ISBN Information: DOI: 10.1109/ICETCI51973.2021.9574046
    Publisher: IEEE Conference Location: Hyderabad, India SECTION I. Introduction
    In recent years, the spread of IoT technology brought benefits to precision agriculture
    applications, in fact, smart systems based on swarms of multiple devices have
    been proposed to acquire data and accomplish tasks (e.g., seeding, irrigation,
    pesticide spraying, etc. ). Swarms can perform constant monitoring of fields with
    the aim of improving crop production to better satisfy food demand. IoT swarm
    employment introduces benefits and drawbacks. From one side, the use of many devices
    helps data collection providing great amounts of data that can better serve the
    analysis of the situation. On the other side, great amounts of data, returned
    by multiple sources, may lead to data integration and interfacing issues [1],
    which generally depend on different data types, features and formats. Moreover,
    the vegetation evaluation changes depending on the environment and cultivar to
    monitor, therefore, the data acquired need to be interpreted differently according
    to the environmental context [2]. Historically, precision agriculture exploits
    spectral images taken from satellites, which are processed to get vegetation indices
    (VIs) providing an evaluation of the vegetation quality and plant health. However,
    satellites return images with different spatial resolution qualities and acquired
    at long time intervals, which do not satisfy the demand of mediumhigh quality
    images with a near-daily acquisition frequency necessary to robustly monitor vegetation
    over time. Some works in literature proposed agent-based solutions to deal with
    the constant monitoring of cultivated fields and issue or anomaly detection. In
    [3], the authors present an approach to deal with the control of multi-UAV systems
    leading them in handling collisions and monitoring a cultivated area. In [4],
    a multi-agent system is proposed to acquire data from the fields and guide decision-making
    aimed at governing actuators that regulate the irrigation system. In [5], the
    authors propose an agent-based model to allow a sensor network to gather information
    from the environment, build knowledge from the various sensors that is collected
    and returned to the end-user to make actions. In [6], an agent-based approach
    governs different IoT devices to perform online vegetation monitoring of crop
    fields. All these works in literature perform monitoring by checking exclusively
    current data on the environment (e.g., VIs, temperature) to assess the vegetation
    status without considering the environmental context. To avoid wrong evaluations,
    our approach, instead, builds knowledge on the environment phenology and historical
    vegetation trends of the specific area to achieve robust monitoring through an
    enhanced contextualization. In detail, this paper introduces an IoT swarm system
    model to contextualize data about the crop fields with their extracted phenology
    and detect anomalies concerning the historical data on the area. The swarm system
    is built as a multi-agent system, where different types of agents are in charge
    of different tasks, from data acquisition to anomaly detection. The agents perform
    data fusion by harmonizing spectral images coming from satellites and drones and
    calculating VIs, hence the phenological data are extracted and used to build contextual
    knowledge on the monitored environment, then the anomalies are detected by classifying
    VIs occurring in specific growing seasons. The rest of the paper is organized
    as follows: Section II details the IoT swarm system model along with its tasks
    and agents involved; Section III demonstrates system capabilities through a case
    study carried out in a simulated real-time; Conclusions close the paper. SECTION
    II. The approach The proposed method is based on a multi-agent model employing
    different types of agents: Data agents, Param agents, Contextualizer agents and
    Monitoring agents. Each agent type is in charge of a specific task. The complete
    agent-based system architecture is highlighted in Figure 1 showing the agent interaction
    necessary to accomplish the tasks and perform environment monitoring. Firstly,
    the Data agents collect spectral images from satellites and drones, hence they
    perform the Preprocessing and data integration task to fuse spectral images coming
    from different sources and then apply the harmonic analysis to extract phenological
    data about the monitored Regions of Interest (ROIs). The Param agents process
    the dataset containing the harmonized images and perform the Parameter assessment
    task aimed at assessing two VIs: Standardized Vegetation Index (SVI) and Normalized
    Burn Ratio (NBR). In the meantime, the Contextualizer agents accomplish the Ontology-based
    contextual data reading task by building a Knowledge Base (KB) on the ROIs and
    their phenological data (i.e., growing seasons) and providing queries to extract
    knowledge from the KB to support environment monitoring. Finally, the Monitoring
    agents perform the Environment monitoring task that allows anomaly detection on
    each ROI by running if-then rules on VIs occurring in growing seasons of the ROI,
    individuated by asking the Contextualizer agents to query the KB. According to
    the found anomalies, the Monitoring agents alert humans on vegetation issues and
    eventual worsening of vegetation on time. The remainder of the section details
    the tasks of the agent architecture. A. Preprocessing and data integration Agriculture
    monitoring requires near-daily imagery at medium to high spatial resolution (10-30
    m), even though Landsat and Sentinel-2 fail in satisfying this request. Landsat
    has a medium-high spatial resolution (30 m) and revisit time of 16 days, while
    Sentinel-2 has a more frequent revisit time (5 to 10 days) and spatial resolution
    changing from band to band (from 10 to 60 m). To overcome these issues, inspired
    by the work described in [7], the Data agents perform the Preprocessing and data
    integration task which consists in applying the harmonization analysis to achieve
    harmonized images from the data of Sentinel and Landsat satellites. For the implementation,
    Google Earth Engine (GEE) API1 has been used. The process requires several steps,
    as illustrated in Figure 2. As the first step, images captured at bottom of atmosphere
    (BOA) radiance, or surface radiance, are acquired from the satellites. These images
    capture the light reflectance from the land surface. The second step consists
    of removing cloud covering, to prevent these phenomena, only images with cloud
    covering at 0% of cloud covering have been acquired. The co-registration step
    of Landsat 8 and Sentinel-2 images consists in correcting the misalignment of
    38 m between the images acquired by the two satellites [8]. In the re-projection
    and re-scaling step, the different projections and scaling of the bands are re-projected
    according to the red band of Sentinel 2 (WGS84) and re-scaled to 30 m by using
    the cubic interpolation method [9]. Then, the Bidirectional Reflectance Distribution
    Functions (BRDF) approach [10] is applied to reduce the differences on solar and
    view angles between the satellites. The Sun-Canopy-Sensor Topographic Correction
    method [9] is used to handle mountain shadowing on the fields. The band adjustment
    step consists of fixing bands across satellites to make them compatible. The collection
    of harmonized band images generated by the above-mentioned steps form the so called
    harmonized dataset. Fig. 1. The framework architecture, showing the interactions
    among the agents on the various tasks. Show All Fig. 2. Preprocessing and data
    integration module. Show All The Data agents acquire from the Param agents the
    time series of Normalized Difference Vegetation Index (NDVI), and Data agents
    run the harmonic analysis over NDVI time series to detect the growing seasons
    of each ROI. The harmonic regression model is employed to determine fitted values
    on the NDVI time series. The resulting curve allows the detection of cultivated
    fields and distinction of these areas from non-cultivated fields, as well as the
    extraction of phenological data on the cropland from the time series. The seasonal
    variations in vegetation are recognized according to curve amplitude and phase
    angle: the higher the amplitude and phase angle, the higher the seasonal variations.
    The extracted phenological data include the starting and ending dates of the various
    plant growth stages (seeding, flowering, fruiting, etc.) for each ROI considered
    which are passed to the Contextualizer agents. B. Parameter assessment In the
    Parameter assessment task, the Param agents process the merged data from spectral
    images to assess VIs, such as the NDVI, that is defined by relating the red band
    image with the NIR band image. Let us consider the RED band value and the NIR
    band value in the pixel p, the NDVI in p is assessed as follows: NDVI= NIR−RED
    NIR+RED (1) View Source This way, by processing all the pixels of the image, a
    new raster image with the NDVI values is generated. This image evidences high
    coverage and vigour in vegetation with higher pixel values, and low coverage and
    vegetation with lower pixel values. An NDVI image allows vegetation evaluation
    in time, therefore, to evaluate vegetation over time other VIs have been considered.
    To this purpose, the Param agents calculate the Standardized Vegetation Index
    (SVI) [11], which describes the probability of variation from the normal NDVI
    over multiple years of data, on a weekly time step. Let us consider the pixel
    i in the image I , the week j in the year k over a period of n years, the SVI
    is defined as follows: Fig. 3. The ontology model. Show All z i,j,k = NDV I i,j,k
    − μ i,j σ i,j (2) View Source where z i,j,k is the z-score for the pixel i, in
    the week j during year k; NDV I i,j,k is the NDVI value on the same pixel (i)
    in the same period (week j, year k); μ i,j is the mean for pixel i in the week
    j over the years considered (n) and σ i,j is the standard deviation for the pixel
    i in the week j over the n years considered. The Param agents also consider the
    Normalized Burn Ratio (NBR), that relates the reflectance of the land surface
    in the NIR and SWIR bands to detect burnt areas. Let NIR be the value of NIR band
    in a pixel p and SWIR be the SWIR value in p, the NBR is defined as follows: NBR=
    NIR−SWIR NIR+SWIR (3) View Source Low values of NBR identify recently burnt areas,
    while high values denote vegetation. Values around 0 denote a non-burnt area.
    To assess the burn severity of an area over time, delta NBR (dNBR) is calculated
    over NBR values assessed over time to detect recently burnt ROIs. Greater-than-0
    dNBR values denote more severe damages caused by the fire, while negative values
    indicate post-fire regrowth situations. The dNBR is assessed on pixels of consecutive
    images to evaluate the severity of fire between two distinct consecutive moments
    in time on ROIs. USGS provides a classification table to evaluate the dNBR value
    and report a corresponding burn severity status. The assessed SVI and dNBR values
    are shared with the Monitoring agents. C. Ontology-based contextual data reading
    In the Ontology-based contextual data reading task, the Contextualizer agents
    encode the ROIs and the acquired phenological data into semantic triples. To this
    purpose, the Territorial Observation Model (TOM) ontology2, has been used; its
    Geo Feature and GeoFeatureObservationPropertyType classes represent the ROIs and
    ROI property to monitor, respectively. The SVI and dNBR calculated on a ROI will
    be encoded as GeoFeatureObservationProperty instances, while classes GeoFeatureObservation
    and GeoFeatureObservationCollection represent the evalutions of a single property
    and all properties, respectively. To model anomalies and phenology, the TOM ontology
    has been extended by adding further classes including: GeoFeatureCollection, GrowingSeason
    and Anomaly. The GeoFeatureCollection represents an Area Of Interest (AOI) as
    a set of all the ROIs lying within the same AOI area. ROIs are related to AOI
    through the belongsTo property. The GrowingSeason class represents the growing
    seasons of a ROI, which are related to literals expressing the start of the season
    (SOS), end of season (EOS) and senescence through ad-hoc data properties. The
    Anomaly class represents the anomaly affecting a ROI and it is related to literals
    expressing the anomaly value (SVI value), the status or level of the anomaly and
    the day of the year when the anomaly happened. By using this ontology model, the
    Contextualizer agents populate the ontology with facts about the ROIs and their
    growing seasons extracted by the Data agents. In this task, the Contextualizer
    agents also query the knowledge base on ROIs and phenological data when it is
    required. Therefore, whenever the Monitoring agents need to interpret the SVI
    and dNBR (see Section II-D), a request is sent to the Contextualizer agents that
    query the knowledge base get the SVI values on a ROI according to its growing
    seasons. Then, the query results are returned to the Monitoring agents that check
    possible anomaly presence. Whenever an anomaly is detected by the Monitoring agents,
    it is added by the Contextualizers as an Anomaly instance to the ontology. The
    anomalous SVI value, the level of the anomaly and the day of the year when the
    anomaly happened are also added to the ontology and related to the anomaly through
    ad-hoc data properties. The data about the detected anomalies are stored for supporting
    vegetation analyses in retrospect. D. Environment monitoring The Monitoring agents
    read the SVI and dNBR values, assessed by the Param agents to check if anomalies
    have happened. The Monitoring agents require contextual data from the Contextualizer
    agents, that run queries and report the Monitoring agents data about the monitored
    ROIs. For each ROI, the SVI values occurring in a growing season of the ROI are
    considered and processed with if-then rules to determine the level of anomaly.
    The rules check SVI value, and, accordingly, return a string corresponding to
    that range expressing the level of anomaly in natural language. The rules are
    designed according to a classification table that relates the SVI ranges to a
    specific class (level) of anomaly, as reported in Table I. The ranges have been
    chosen in accordance with the SVI definition as z-score and the sigma rule [12]
    for normal distributions (µ = 0 and σ = 1) where the most of the data lie within
    the three standard deviations closer to the mean. Positive SVI values are filtered
    out, since they represent non-anomalous scenarios, hence the agents keep monitoring
    the environment by analysing new data. In case of negative values (i.e., some
    anomalies happened), rule-based classification is applied to determine the level
    of anomaly. Table I Anomaly level according to SVI. Table II Burn severity classification
    table based on dNBR values provided by usgs. For what concern burn severity evaluation,
    the Monitoring agents evaluate the dNBR values by using a classification table
    developed by USGS and reported in Table II. The table reports various ranges of
    the dNBR values associated to a class of burn severity; the positive values evidence
    increasing levels of severity, while values around 0 and negative values denote
    unburned and post-fire regrowth scenarios. When an anomaly occurs, the Monitoring
    agents send an alert to humans reporting about the detected anomalies, along with
    the level of anomaly and burn severity determined through classification of the
    VIs. SECTION III. A Case Study In Simulated Real-Time For demonstration purposes,
    the framework has been run on a case study simulating real-time event occurrence
    by using Apache Kafka. Let us remark that real data acquired from the satellites
    have been used in the simulation, they are collected by using Google Earth Engine
    (GEE). The case study consists in monitoring an AOI in the province of Avellino,
    Italy; let us focus on ROI (RO I 1 ) inside this AOI including a vineyard (see
    Figure 4a). This area is captured as 176 spectral images, where 66 of them are
    taken from Landsat 7, 63 from Landsat 8 (an example is shown in Figure 4b) and
    47 from Sentinel-2 (an example is shown in Figure 4c). The data are integrated
    by the Data agents, and from them, NDVI time series are calculated. Then, the
    harmonic regression is applied to fit the model to the NDVI time series, as shown
    in Figure 5. The amplitude and phase of the resulting regression curve helps depicting
    changes in vegetation. From the curve amplitude and phase analysis, the Data agents
    detect two growing seasons, identified through SOS, SEN and EOS parameters expressed
    in day of the year (doy), for (RO I 1 ) , they are: [SOS = 100, SEN = 150, EOS
    = 200], [SOS = 250, SEN = 300, EOS = 200]. Then, the Contextualizer agents store
    the ROI data and its growing seasons as semantic triples on the knowledge base.
    Meantime, the Param agents assess SVI and dNBR from the 01/01/2020 to 01/01/2021,
    which are stored on structured files for the real-time simulation. Fig. 4. Comparison
    between images captured from the editor map before the Band Adjustment phase:
    (a) is the ROI1 on the map;(b) is the L8 image;(c) is S2 image. Show All To simulate
    the passing of time, the time thread has been built: each day lasts around 4 seconds,
    hence by considering 365 days, the duration of the whole simulation on one year
    is around 24 minutes (4 • 365/60 = 24.33). During this time, each satellite/UV
    is simulated by a Kafka producer that acquires data from the structured files
    containing the VIs on the ROIs. Three data producer threads have been defined:
    P 1 , P 2 , P 3 . The P 1 thread represents a satellite that sends SVI values,
    the P 2 thread simulates a UV that provides SVI values by using a random generator
    and the P 3 thread is a satellite providing NBR values. All these threads exploit
    a random number generator to choose a number of data entries to be sent in a day
    to consumers, in accordance with the time passage simulated by the time thread.
    Therefore, each time a producer sends new data, they are written on a specific
    topic, according to the type of data detected (e.g., SVI, dNBR). The data are
    streamed from the producers to the consumers in the format (doy, RO I id ), val;
    where doy is the day of the year when the data is collected, RO I id is the ID
    number for a ROI and val is the SVI or dNBR value found on that ROI. As time goes
    by, the data may be streamed in simultaneity from the three producers P 1 , P
    2 , P 3 to consumers. The detected SVI and dNBR anomalies found on RO I 1 are
    stored on Fuseki, and written in output to the consumers. The consumer threads
    are two, representing an analysis laboratory and a farmhouse, respectively. Table
    III displays some SVI and dNBR anomalies read by the consumers on RO I 1 from
    doy 103 to doy 143, along with the type of anomaly and its severity. In this case,
    on average, low and medium unfavourable conditions are found on the RO I 1 and
    nearby ROIs according to the SVI values, the ROI is also found unburnt according
    to the dNBR. ROIs at risk in the surroundings of the monitored ones can be easily
    individuated, in fact, for what concern post-fire dNBR-based evaluations, RO I
    1 is anomaly free, but another ROI RO I 3 in the surroundings is not (see Table
    IV), therefore, ROIs near RO I 3 , that can also be at risks, are notified. The
    utility of the collected knowledge is shown by the example in Listing 1, that
    shows a query allowing farmers and experts to individuate the ROIs with the highest
    number of anomalies occurred in a season. Results, present in Table V, report
    that RO I 3 and RO I 1 are the ROIs affected by the highest number of anomalies.
    Table III Anomalies detected for RO I 1 between doy 103 and doy 143. Table IV
    Post-fire scenarios: ROIs at risks in the surroundings of RO I 1 , RO I 2 and
    RO I 3 at doy 143 according to dNBR. Listing 1. Query to get the number of the
    anomalies per each ROI monitored. Show All To evaluate the efficiency of the Kafka-based
    implementation, a stress test has been carried out on one of the producers: P
    2 ; it simulates the UV that generates the SVI values on a ROI. The stress test
    consists in three different simulations, where the minimum time interval between
    the sending of two consecutive messages, sent by P 2 , is set to different values.
    The time interval is increased at each simulation, in detail, the producer can
    send messages every 4000 ms in the first simulation, 400 ms in the second one
    and 40 ms in the third one. Table VI shows how the parameters about the sent messages
    vary; let us notice that as the number of sent records vary across simulations,
    all the records sent (Output topic number) to the consumers are received and consumed
    (Consumed record number) in any situations, even in Simulation 3 where the system
    is under strong stress (i.e. a message is sent every 40 ms). These results show
    that the system can be implemented and used to deal with real-time scenarios.
    Table V Query results reported for each ROI considered. Fig. 5. Harmonic model
    over multiple years. Show All Table VI Test results reported in each simulation.
    SECTION IV. Conclusion The approach introduced a multi-agent model to allow a
    swarm of IoT devices to keep cultivated fields under monitoring and alert humans
    in case of anomalies on vegetation. This agent model brought several benefits
    to vegetation monitoring for precision agriculture, including: Data harmonization
    to fuse spectral images coming from multiple sources and build a near-daily high-resolution
    image dataset. Harmonic regression to individuate the growing seasons of a specific
    ROI from the VIs calculated throughout a year. A knowledge base about the ROIs
    and their growing seasons that can be reused by experts to make analyses in retrospect
    (agroanalytics) on the area. Context-aware classification of VIs, considering
    historical values, to detect anomalies according to the growing and senescence
    seasons of the cultivars and the historical growth trends of plants present in
    the monitored ROIs. promptly alert messages about the critical ROIs and nearby
    ROIs that may be at risks. Authors Figures References Keywords Metrics Footnotes
    More Like This Simulation-based performance evaluation of queuing for e-learning
    real time system International Conference on Education and e-Learning Innovations
    Published: 2012 Dynamic Process Modeling and Real-Time Performance Evaluation
    of Rework Production System With Small-Lot Order IEEE Robotics and Automation
    Letters Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the 2021 International Conference on Emerging Techniques
    in Computational Intelligence, ICETCI 2021
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Sensing multi-agent system for anomaly detection on crop fields exploiting
    the phenological and historical context
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Antille D.L.
  - Lobsey C.R.
  - McCarthy C.L.
  - Thomasson J.A.
  - Baillie C.P.
  citation_count: '11'
  description: 'Crop nitrogen (N) management is one of many important agricultural
    applications that can benefit from crop sensing. The technologies in this field
    are advancing rapidly, including: (1) sensor-carrying platforms, (2) the sensors
    themselves, and (3) the analytical techniques used to derive actionable information
    from the data. A review of commercially and semi-commercially available platforms
    was undertaken to inform sensor mounting, with particular focus on unmanned aerial
    vehicle (UAV) sensor platforms and unmanned ground vehicle (UGV) sensor platforms.
    The UAV and UAG platforms provide indirect and direct measurements for crop monitoring
    and N mapping with the goals of being low-cost, on-site, and versatile. Optical
    crop sensing techniques and systems for N management are also discussed, because
    destructive sampling and laboratory analyses are expensive and often not practical
    for site-specific management of N. The optical properties of the plant are significant
    because they are related to water content, leaf senescence, disease, and nutrient
    status, which can inform farming decisions. Additionally, Red, Green and Blue
    (RGB) imaging can provide a plant height assessment for multiple measurements,
    including: yield potential, biomass, density, uniformity, and planter skips. The
    work reported in this paper includes a comparison of various optical sensors for
    plant measurements, including: vis-NIR, Machine Vision, and 3D-imaging, with camera
    varieties such as multispectral, fluorescence, hyperspectral, thermal, and visible.
    Key recommendations have been provided for the development of data aggregation
    and decision support tools including the data sources to be used in development
    of machine learning models, software/data standardization efforts, and corporate
    collaborations regarding big data. In conjunction with the sensors and their platforms,
    this advancing field of management technology can provide intelligent sensing
    and intelligent decisions.'
  doi: 10.13031/aim.201801593
  full_citation: '>'
  full_text: '>

    "Publications Included Search Help About Contact Us Join   If you are not an ASABE
    member or if your employer has not arranged for access to the full-text, Click
    here for options. A review of the state of the art in agricultural automation.
    Part IV: Sensor-based nitrogen management technologies Published by the American
    Society of Agricultural and Biological Engineers, St. Joseph, Michigan www.asabe.org
    Citation:  2018 ASABE Annual International Meeting  1801593.(doi:10.13031/aim.201801593)
    Authors:   Diogenes L. Antille, Craig R. Lobsey, Cheryl L. McCarthy, J. Alex Thomasson,
    Craig P. Baillie Keywords:   Machine vision, Optical plant sensors, Sensor-carrying
    platforms, 3D-imaging, unmanned aerial vehicle (UAV), unmanned ground vehicles
    (UGV), vis-NIR. Abstract. Crop nitrogen (N) management is one of many important
    agricultural applications that can benefit from crop sensing. The technologies
    in this field are advancing rapidly, including: (1) sensor-carrying platforms,
    (2) the sensors themselves, and (3) the analytical techniques used to derive actionable
    information from the data. A review of commercially and semi-commercially available
    platforms was undertaken to inform sensor mounting, with particular focus on unmanned
    aerial vehicle (UAV) sensor platforms and unmanned ground vehicle (UGV) sensor
    platforms. The UAV and UAG platforms provide indirect and direct measurements
    for crop monitoring and N mapping with the goals of being low-cost, on-site, and
    versatile. Optical crop sensing techniques and systems for N management are also
    discussed, because destructive sampling and laboratory analyses are expensive
    and often not practical for site-specific management of N. The optical properties
    of the plant are significant because they are related to water content, leaf senescence,
    disease, and nutrient status, which can inform farming decisions. Additionally,
    Red, Green and Blue (RGB) imaging can provide a plant height assessment for multiple
    measurements, including: yield potential, biomass, density, uniformity, and planter
    skips. The work reported in this paper includes a comparison of various optical
    sensors for plant measurements, including: vis-NIR, Machine Vision, and 3D-imaging,
    with camera varieties such as multispectral, fluorescence, hyperspectral, thermal,
    and visible. Key recommendations have been provided for the development of data
    aggregation and decision support tools including the data sources to be used in
    development of machine learning models, software/data standardization efforts,
    and corporate collaborations regarding big data. In conjunction with the sensors
    and their platforms, this advancing field of management technology can provide
    intelligent sensing and intelligent decisions. (Download PDF)    (Export to EndNotes)
    Share Facebook X Email LinkedIn WeChat     Library Home Search Obtaining Full-Text
    E-mail Alert ASABE Home Authors, please use the Guide for Authors when creating
    your articles.  Public Access Information   = Public Access (PA)   = PA Limited
    Time   = Open Access   = Contact Us For Purchase  American Society of Agricultural
    and Biological Engineers 2950 Niles Road, St. Joseph, MI 49085 Phone: +12694290300
    Fax: +12694293852 Copyright © 2024 American Society of Agricultural and Biological
    Engineers"'
  inline_citation: '>'
  journal: ASABE 2018 Annual International Meeting
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A review of the state of the art in agricultural automation. Part IV: Sensor-based
    nitrogen management technologies'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ferreira D.
  - Corista P.
  - Giao J.
  - Ghimire S.
  - Sarraipa J.
  - Jardim-Goncalves R.
  citation_count: '13'
  description: With the introduction of paradigms like Internet of Things, Cyber Physical
    Systems and Cloud Computing, Smart Factories are becoming a central part of today's
    manufacturing systems. Even though there already are some solutions in the market
    the full potential for smart manufacturing hasn't yet been achieved. In order
    to fulfil the gap European researchers are developing vf-OS, a platform that aims
    to be the future reference in future factories operating systems. In this work
    is presented some preliminary results regarding the modules related to IoT, event
    processing, situational awareness and data harmonization that are being researched
    in the scope of vf-OS to achieve holistic solution for industries, specifically
    targeting the agriculture domain.
  doi: 10.1109/ICE.2017.8280066
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2017 International Conference... Towards
    smart agriculture using FIWARE enablers Publisher: IEEE Cite This PDF Diogo Ferreira;
    Pedro Corista; João Gião; Sudeep Ghimire; João Sarraipa; Ricardo Jardim-Gonçalves
    All Authors 13 Cites in Papers 508 Full Text Views Abstract Document Sections
    I. Introduction II. Factories of the Future and Industrial Internet of Things
    III. Fiware IV. IoT as a Service V. Vf-OS Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: With the introduction of paradigms like Internet
    of Things, Cyber Physical Systems and Cloud Computing, Smart Factories are becoming
    a central part of today''s manufacturing systems. Even though there already are
    some solutions in the market the full potential for smart manufacturing hasn''t
    yet been achieved. In order to fulfil the gap European researchers are developing
    vf-OS, a platform that aims to be the future reference in future factories operating
    systems. In this work is presented some preliminary results regarding the modules
    related to IoT, event processing, situational awareness and data harmonization
    that are being researched in the scope of vf-OS to achieve holistic solution for
    industries, specifically targeting the agriculture domain. Published in: 2017
    International Conference on Engineering, Technology and Innovation (ICE/ITMC)
    Date of Conference: 27-29 June 2017 Date Added to IEEE Xplore: 05 February 2018
    ISBN Information: DOI: 10.1109/ICE.2017.8280066 Publisher: IEEE Conference Location:
    Madeira, Portugal SECTION I. Introduction With the development of technology,
    new forms of agriculture are emerging in our society. The use of technology is
    growing in order to improve agriculture production and have higher profits. To
    achieve these goals, agricultural producers or distributors use Information Technology
    and Communication (ICT) to help to manage agriculture. Sensors can be used to
    enable real time monitoring food''s parameters, such as pH, temperature, earth''s
    moisture or oxygen flow. With these sensors and its connection with the Internet,
    it is possible to monitor all cultivations, even if they are apart, and predict
    and control its quality and how much food can be sold. With the knowledge of position,
    its values and seasonal care, tractors can supply plant''s daily needs, without
    human intervention. This sensor technology that makes farms more intelligent,
    with real-time data gathering, processing and analysis is called ‘smart farming’
    [1]. Several plants production is increasingly threatened. Climate changes, insufficient
    available lands, air toxins are some problems that agriculture faces these days.
    One agricultural solution to address these problems are plant factories. They
    can be defined as horticulture greenhouses or automated system facilities that
    have artificially controlled environments in order to produce vegetables and seedling
    year-round [2]. By using the benefits of smart farming, it is possible to turn
    the entire growth process automated and leading economies in order to control
    cost, quantity and quality against the required harvest time. All these features
    have been addressed by European Research Initiatives as the European Technology
    Platform on the “Future Manufacturing Technologies” (MANUFUTURE). This is a project
    which has the goal to implement research and innovation methodologies to increase
    the rate of industrial transformation in Europe [3], where Agriculture Engineering
    Technologies (‘AET’), even being from a cross-sectorial domain, is taking an effort
    to be integrated in the future of the industry research [4]. With the uprising
    of technologies linked to industry 4.0 now it is possible to create virtual factories.
    Virtual factory is one digital representation of the real factory where actors
    can simulate and operate the normal operation of the factory, predict, find and
    solve malfunctions and take advantage of scalability. It is also possible to add
    new services to the virtual factory and perform impact analysis before applying
    them to the physical one. To create a global standard virtual factory operating
    system, the European Union under the Horizon 2020 Programme commissioned the creation
    of a Virtual Factory Open Operating System called vf-OS. The vf-OS project aims
    to provide an economical multi-sided market platform with the aim of creating
    value by enabling interactions between different customer groups: Software Developers;
    Manufacturing Users; ICT Providers and Service Providers. In order to create value
    in such multi-sided platform, vf-OS will provide a range of services to the connected
    factory of the future to integrate better manufacturing and logistics processes.
    Additionally, Manufacturing Applications Store (vf-App) will be open to software
    developers who, using the free Open Applications Development Kit provided, will
    be able to quickly develop and deploy smart applications to address different
    aspects of manufacturing industries. Vf-OS solution will provide technology stack
    for device and legacy system integration, sensorial system virtualization, complex
    event processing, data harmonization and analytics, situational awareness etc.
    This work aims to propose a solution for the agriculture industry using FIW ARE
    solutions to implement some core functionalities and being aligned with the proposed
    methodology formulated by vf-OS. SECTION II. Factories of the Future and Industrial
    Internet of Things According to [5] factories will tend towards flat management
    structures with a more highly skilled and IT literate workforce focusing on more
    product design, optimization, monitoring and controlling of processes. These factories
    will rely on sensors and actuators to allow production monitoring and control
    systems in real-time. The information coming from the sensors will be correlated
    with digital models and simulations resulting on a high efficient situational
    analysis and task management. At the same time the current trend of increasing
    development of technology used in factories and in industry in general aims to
    further improve the way robots and machines in industry are being used. Briefly
    speaking they are being more and more interconnected and optimized, which are
    specifically addressed by a branch of the Internet of Things (IoT) dedicated especially
    to the industry, the Industrial Internet of Things (IIoT). Simply put, such as
    [6] ascribes to [7], “IoT is an information network of physical objects (sensors,
    machines, cars, buildings, and other items) that allows interaction and cooperation
    of these objects to reach common goals”. The IoT has today many applications in
    different domains such as transportation, healthcare, smart homes and industrial
    environments [8]. When applied to core industries, it can be called IIoT and its
    main purpose is to transform the way field assets (e.g., machines or robots) connect
    and communicate within a factory or between factories, resorting to the use of
    sensors, advanced analytics, and intelligent decision making [9]. Some efforts
    are being made to implement the IIoT in today''s industry, starting by addressing
    the standardization challenges, promoting open interoperability and the widespread
    usage of a common architecture. Despite all progress and improvement, there are
    still many ways to where IIoT can be further improved and optimized. According
    to [10] test-beds for smart production technologies (called experimental factories)
    have been created with the purpose of establishing interoperability guidelines
    and applying new IT technologies in existing automated systems, and thus demonstrate
    how technologies from different organizations can work together and support new
    innovations. However, still according to [10] has been no attempt to interconnect
    these experimental factories and allow them to flexibly adapt their production
    capabilities based on cross-site demands. In order to manage the usage of all
    these smart embedded devices in industry, applications become necessary to better
    integrate real-time state of the physical world, and hence, provide services that
    are highly dynamic, scalable and efficient. To incorporate these functionalities
    principles of service oriented computing (SOC) it is necessary to adopt the paradigm
    of industrial systems. SOC is built upon services that provide autonomous, platform-independent,
    computational elements that can be described, published, discovered, orchestrated
    and programmed using standard protocols to build networks of collaborating applications
    distributed within and across organizational boundaries. This needs to be implemented
    in the scope of factory systems where infrastructures are composed of large numbers
    of networked, resource-limited devices and machines. Service orientation for modelling,
    representation and implementation of such systems can enable such devices to offer
    its functionalities and, at the same time, discover and invoke others functionalities
    offered by services of other devices dynamically and on-demand, as suggested by
    [11]. A. Important FoF Concepts Based on the findings from the literature review,
    in [12] are presented the principles that are used to obtain design principles
    to factories of the future and that are relevant to this work''s main goal. These
    principles are interoperability, virtualization, decentralization and real-time
    capability and are explained below. Interoperability is the ability of two or
    more systems to communicate and exchange data, even if the languages and models
    used in their implementation are not the same [13]. This means that the concept
    of interoperability is an important factor on factories of the future because
    it will allow to enable communications between the CPS, the installation network
    and the people connected throw it. The virtualization of the factories of the
    future can be achieved using cyber-physical systems that are composed by a physical
    entity embedded with a cyber entity [14]. The cyber part contains a virtualization
    of the physical part so that it can replicate virtually any behaviour of the physical
    machine. Being a virtual/physical system allows the system to easily access and
    store information in the cloud. CPS also contain wireless embed wireless devices
    [15] which allow the various machines to interact and communicate with each other
    and with the network. SECTION III. Fiware FIWARE is a research driven project
    with the goal for creation of a core reference platform in the scope of Future
    Internet [16]. The project has delivered a stack of solutions for internet technology
    which can be used in smart infrastructures of different domains and that will
    contribute for the technological competitiveness and Europe''s growth. FIW ARE
    offers customized open-source resources through functional building blocks that
    are called Generic Enablers (GEs) that ease creation of smart Internet Applications
    and services. This GEs offer reusable and shared components, which provides a
    concrete set of APIs and interoperable interfaces that are in compliance with
    open specifications published for that GE [17]. The FIWARE GEs are divided into
    technical chapters, each one of them has its technological focus. For this paper,
    it will be analyzed the GEs from FIWARE chapters: “IoT Services Enablement” and
    “Data/Context Management”. IoT Services Enablement chapter provides the GEs to
    allow any physical object to become available, searchable, accessible, and usable
    context resources fostering Apps interaction with real-life objects. On the other
    hand, the Data/Context Management enablers ease development in applications that
    require management, processing and exploitation of context information as well
    as data streams in real-time data. Together, these enablers will help achieving
    functionalities to collect and analyze a large quantity of information. Additionally,
    FIWARE data/context management chapter looks forwards to develop GEs that provide
    an easier and faster way to create or integrate data analysis algorithms [18].
    A. IoT Enablers Examples IoT enablers provide functionalities for connecting devices
    using standard IoT communication protocols and providing interfaces for the enablers
    at data/context layer. Basically, IoT GEs are spread over two different domains
    i.e. IoT Backend and IoT Edge. IoT Backend comprises the set of functions, logical
    resources and services hosted in a Cloud mainly focused on management of IoT devices.
    While, IoT Edge is made of all on-field IoT infrastructure elements needed to
    connect physical devices to other applications. The GEs of interest are Protocol
    Adapter - MR CoAP and Backend Device Management - IDAS which fall in IoT Edge
    and IoT Backend domain respectively. 1) Protocol Adapter-MRCoAP The “Protocol
    Adapter -MRCoAP” GE serves in between of the registered devices (e.g. the patient
    sensor) and the gateway that offers the communication layer to the host environment.
    It supports data collection, sensor detection and connectivity. The particular
    GEi for instantiation is MR CoAP Protocol Adapter, which provides functionality
    to plug devices using on CoaP over 6LowPan protocol. The protocol adapter is designed
    to work with IBMs Moterunner platform and communicates via 6LoWPAN and uses CoAP
    as application layer protocol. This GE is linked with the Gateway Device Management
    GE or the Data Handling GE [19]. 2) Backend Device Management-IDAS The “Backend
    Device Management -IDAS” GE allows communication between backend and devices (e.g.
    via the protocol adapter). This GE provides functionalities to connect physical
    devices to a FIW ARE platform. Other functionalities of this GE includes manages
    IoT -related NGSI Context Entities, handles the connections and provides IoT Integrators
    with the ability of transforming devices specific Data Models into the Data Models
    defined at the NGSI level by different verticals (Smartcities, SmartAgrifood,
    Smartports, etc.). The GEi that can be utilized is Backend Device Management -
    IDAS that provides functionality to translate IoT -specific protocols into the
    NGSI context information protocol, which is the FIW ARE standard data exchange
    model. You do not need this component if your devices or gateways natively support
    the NGSI API [20]. B. Data/Context Management Enablers Examples The Data/Context
    Management GEs provide functionalities that will ease development and the provisioning
    of applications that require management, processing and exploitation of context
    information as well as data streams in real-time and at massive scale. In this
    regard two of those enablers, are: FIW ARE Publish/Subscribe Context Broker Generic
    Enabler - Orion Context Broker and FIW ARE Short Time Historic (STH) - Comet and
    are described below. 1) Fiware Publish/Subscribe Context Broker Generic Enabler-Orion
    Context Broker The Orion Context Broker is an implementation of the Publish/Subscribe
    Context Broker GE used to develop a Data/Context Scenario through the NGSI9 and
    NGSI10 interfaces (Next Generation Services Interface). In order to deal to physical
    devices through an Internet approach i.e. to create the so called IoT devices,
    FIW ARE associated with OMA (Open Mobile Alliance) to create the NGSI concept
    which enables a transition from device-level information to Thing-level information
    and vice versa. In order to accomplish so, two interfaces were created NGSI9 and
    NGSI10. Both are RESTful API''s via HTTP but with slight differences. While NGSI10
    purpose is to exchange context information, the NGSI9 is used to exchange information
    about the availability of context information. Using both the described NGSI interfaces
    the FIWARE Orion Context Broker makes possible the creation of entities with all
    the listed information. Making it also possible to update that information and
    to subscribe to that information (be informed when a change has occurred). The
    FIW ARE Orion Context Broker serves than an intermediate crossing point between
    the Context Producers and the Context Consumers, storing all the information in
    a Database (MongoDB is the database used by Orion by default) which can be accessed
    by both the Producers (to update or generate information) and the Consumers (to
    query the entities or be notified through the subscriptions). 2) Fiware Short
    Time Historic (STH)-Comet The Short Time Historic is a FIW ARE component capable
    of managing (storing and retrieving) historical raw and aggregated time series
    information evolution in time of context data (i.e., entity attribute values)
    registered in an Orion Context Broker instance. Similarly, to the FIW ARE Orion
    Context Broker, all the communications performed by the FIW ARE STH make use of
    the NGSI9 and NGSI10 interfaces. As stated, even though the FIW ARE STH supports
    the storing and retrieval of raw context information, (the concrete entity attribute
    value which were registered in an Orion Context Broker instance over time), its
    main capability and responsibility is the generation of aggregated time series
    context information about the evolution in time of those entity attribute values.
    In order to keep track of the different values a certain attribute takes over
    time, the S TH makes use of the Orion Context Broker subscription function, so
    that every time that attribute''s value changes, the STH can be informed that
    the change occurred and to which value the attribute has changed into. With this
    information arriving every time a change of values occurs, the STH keeps a history
    of the changes in a database (Mongo DB by default). Using an HTTP RESTful API,
    external clients can query the available historical raw context information maintained
    by the FIW ARE STH. By doing so the consumers get a list of the different values
    that attribute has taken over time and at what time the changes occurred. These
    results can be filtered to be shown, for example, the changes the occurred in
    the last hour or day, or the 10 last changes that happened, or from a specific
    date to another specific date, etc. On the other hand, if it is historical aggregated
    time series context information that the consumer is after, the FIW ARE STH, also
    through an HTTP RESTful API, has many ways of interpreting the information and
    returning it through many different aggregations. SECTION IV. IoT as a Service
    The Internet of Things presupposes the interaction between physical world, objects
    and their virtual counterparts, allowing these objects to capture and send information
    to the Internet. Or as defined by [21], “the Internet of Things (IoT) consists
    of networks of sensors attached to obj ects and communications devices, providing
    data that can be analysed and used to initiate automated actions”. In order to
    integrate all this physical objects with the digital world, it is necessary to
    find a way to make them accessible in the Internet, and therefore an Internet
    of Things as a Service approach is necessary. 1) Model The first step is the creation
    of a model through which the physical “things” can be found and accessed. According
    to [22], the research so far developed in this area, has focused mainly on “sensor
    descriptions and observation data modelling” that offer sensor measurement data
    services on the web. 2) Framework The second step in the Internet of Things as
    a Service paradigm is the creation and usage of a Framework. In order to make
    the paradigm accessible to everyone, and to make sure all will benefit from it,
    an open service framework needs to be created. According to [23], even though
    there have already been made some IoT Frameworks, those were mainly created by
    big enterprises such as governments or companies and are mostly based on B2B (Business
    to Business) and B2G (Business to Government) business models. However, there
    are also open service platforms for IoT such as Cosm, former Pachube or EVRYTHNG.
    3) Mashup The idea behind a “Mashup” is to create new content by reusing or recombining
    previous existing content from various sources. A Mashup is, therefore, a way
    to compose a new service from existing services and, “when applied directly to
    the Web domain, a Mashup is a Web-based application that is created by combining
    and processing on-line third party resources that contribute with data, presentation
    or functionality” [24]. Unlike regular Web Services that are provided in a specific
    domain and available at any time in the web, IoT devices, providing a service,
    are not always available and not always working in the same place, in addition
    to the number and variety of connected devices. To solve this problem, a solution
    is presented in [25], where an IoTMaaS (IoT Mashup as a Service) is introduced.
    IoTMaaS is defined as “a mashup of things, software, and computation resource”,
    and is presented as a cloud-based IoT mashup service model. In IoTMaaS, thing
    is described as “any identifiable obj ect which can have sensing and actuation
    services; software is an “assembly description of software components”; and computation
    resource is “a current computer model consisting of CPU, memory, disk, persistent
    storage, network, etc. How these components interact with the IoT world is depicted
    in Fig. 1. Fig. 1. Iotmaas concept, adapted from [25] Show All When applied to
    the Internet of Things, the IoTMaaS whose concept is depicted in Fig. 1, allows
    every IoT device to provide its service disregarding which platform it works on
    or which protocol it uses to communicate, because regardless of this vast heterogeneity
    of devices they are all treated equally by the IoTMaaS. Therefore, users and or
    developers can make use of all their IoT devices and enhance their functions resorting
    to this Mashup. Technically, the enablers from the FIW ARE IoT enablement services
    viz. Protocol Adapter -MRCoAP and Backend Device Management- IDAS as discussed
    previously will provide core functionalities for realization of IoTMaaS. SECTION
    V. Vf-OS The European vf-OS Project aims to be an Open Virtual Factories Operating
    System, including a Virtual Factory System Kernel (vf-SK), a Virtual Factory Application
    Programming Interface (vf-API) and a Virtual Factory Middleware (vf- MW) specifically
    designed for the factories of the future. Positioning of these different components
    in layered view of vf-OS is as show in Fig 2. It can be noted that the App are
    the applications that will be developed to implement different functional/business
    requirements while other components provide core functionalities envisioned in
    general for manufacturing processes. Fig. 2. High level view of different layers
    of vf-OS Show All This Open Framework will be able to manage network of collaborative
    manufacturing and logistics environment, and therefore enable humans, applications
    and devices to communicate and interoperate in the interconnected operative environment.
    Additionally, vf-OS will provide a set of Open Services, rooted in the cloud and
    instantiated at the vf-OS Platform, moving the industry from the device-centric
    to the user-centric paradigm. This Open Platform is to be linked by strong and
    advanced ICT (like CPS, loT Cloud-Models, M2M, etc.) in order to fulfil the actual
    need on the market for open services interoperability based on data exchange.
    V f-OS will act as an intermediary between the applications of the factory and
    the factory devices, eventually enabling factory functionalities, services, devices
    etc. to be virtualized, executed and accessed in ubiquitous manner. Similarly,
    to a regular OS the vf-OS comprehends core functionalities, but mainly focused
    in a manufacturing environment. An analogy between some components of a regular
    Software OS and the vf-OS Manufacturing OS is presented in Table I (adapted from
    the vf-OS wiki glossary). Table I. Analogy between software OS and vf-OS manufacturing
    OS As described before, the vf-OS intends to be the reference system/platform
    for providing common services for manufacturing industry. This solution will be
    the part of system software in a real industrial. In order to accomplish this,
    some generic case scenarios have been established which can be addressed with
    the vf-OS Platform and its Smart Applications. The current industrial environment
    faces some problems related to specific industries and respective sectors some
    of which are addressed by vf-OS use cases. The developed scenarios integrate both
    industrial and user scenarios as they propose to produce advanced technical solutions
    to some of the existing industrial scenarios by developing suitable applications,
    whereas the user scenarios are described following a standard methodology through
    well-defined objectives, processes, actuators and possible sets of data. The scenarios
    addressed by the vf-OS proj ect capture the needs of different industrial sectors
    and process domains. Table II, briefly explains some of generic apps that vf-OS
    aims to develop that are equally applicable for the research domain of this paper.
    Table II. Examples of vf-OS generic use case apps SECTION VI. The Agriculture
    Scenario In order to further explain the practical application of the vf-OS paradigm,
    let us consider a scenario in the domain of agriculture industry. This scenario
    is simply described as a real fruit production chain. The process begins with
    the harvesting of various types of fruits by several different farmers. The collected
    fruit is then manually or automatically split according to different factors (type,
    size, weight, brand, quality, etc.), and stored in boxes grouped by the desirable
    factors. When a buyer seeks a buy with a specific type of fruit with specific
    features, he/she can make use of a buying application where he can find the sellers
    of the fruit with those specifications and from where he can emit an order to
    acquire the fruits taking into account the price of the fruit, the seller, the
    quality and so on. Once the buying order reaches the producer, the latter dispatches
    the stock as described in the order. Fig. 3. Practical scenario illustration Show
    All All the steps are enumerated in Fig. 3 and are listed and briefly explained
    in Table III. Table III. Scenario steps explanation In the course of these steps
    different technologies can be used for harvesting, sorting, packing, tracking
    etc. The producer, while dispatching its fruits is able to control, in real time,
    all the steps taken by the fruit since the its harvest to the time the fruit is
    delivered to the buyer. Besides, he is always aware of any faults or failures
    occurring during the process as well as keeping track of the amount of fruit that
    is being harvested, shipped, or removed due to not being in conformity with the
    quality standards. At the end of this process, the buyer can have all information
    of what was bought, i.e. access to all information from the various process in
    the food chain from agricultural production to consumption (which is commonly
    called “from farm to fork”) [3]. This scenario demands different types of functionalities
    such as transportation tracking, packing status tracking, environmental monitoring
    of transportation condition, create buying order, dynamic notifications of production/delivery
    status etc. These types of functionalities are provided by vf-OS apps. In Fig.
    4 it is presented the connections between the real scenario and the applications
    that answer to the generic Use Case Scenarios. All applications are connected
    in the generic use case scenario. Fig. 4. Use case scenarios applications applied
    to the practical scenario Show All Comparing with ancestral or traditional agriculture
    approaches, where the entire farming and harvesting are handmade and it is not
    possible to get explicit information from the plantation''s conditions, the methodologies
    presented in this scenario will bring an automatic and better overview of all
    the agriculture supply chain that will facilitate its improvement. Nevertheless,
    this scenario could be always enhanced with the addition of new types of sensors
    and greater controlling procedures, which may include data mining that aggregated
    with a recommender component, would bring a more autonomous and efficient function
    to the proposed system. SECTION VII. Implementation Using the FIWARE Generic Enablers
    and other technologies it was possible to create answers to the Use Case Scenarios
    applications presented in the vf-OS project, always bearing in mind an answer
    to the described practical scenario. Four FIWARE GEs were used (MRCoAP, Orion
    Context Broker, STH - Comet and IDAS). In Orion''s model we can represent several
    entities associated with resources that are accessed by services. The entities
    allow creation of model of a sensor, along with the value retrieved by the physical
    device. By querying features on this enabler and applying certain filters on sensors''
    values we can collect data from the sensors as necessary for specified scenarios.
    Fig. 5 shows a very simple case of querying data collected by an entity which
    happens to be a sensor. In the use case scenario, the STH-Comet is used to process
    the information stored within the repository, comparing it with time periods.
    It allows to aggregate information retrieved in a particular period of time which
    allows to drop meaningless information. It also allows the system to perform several
    sample captures within a defined timestamp. The use of these enablers provides
    competitive testing options by providing its own nodes and networks. The sets
    of tools provided allow users to access and use high level functions/applications
    that otherwise would have to be built from scratch. Fig. 5. Orion query result
    for a specific sensor Show All The MRCoAP enabler is used for connecting various
    low powered devices that follow CoAP communication protocol. This class of devices
    includes sensor nodes and actuators that can be used for collecting data from
    the floor and passing actions to be undertaken by devices respectively. Note that
    if the devices are capable of communicating with internet protocols then they
    don''t need to use the MRCoAP enabler. IDAS enabler is used for management of
    different devices deployed on the floor. IDAS also acts as the interface between
    the devices and orion context broker enabler. In this case the devices are the
    data producers and orion is the data distributor which are consumed by different
    vApps. Fig. 6. Scenario + V apps + generic enablers Show All By utilizing these
    different enablers, the v Apps as described can be implemented. Fig 6 shows the
    mapping with various GEs that will be used for implementing the apps. For instance,
    vOrder app provides functionalities for tracking transportation and ensures successful
    delivery. Firstly, to track transportation devices like GPS, speedometer etc.
    needs to be connected to the truck and eventually connected to the Internet. These
    functionalities can be achieved by using IDAS and MRCoAP GEs. Then, to find the
    current status of the delivery, Orion and Comet will process the data. Orion can
    also be utilized to generate notifications of different events that can be detected
    by time series analysis functionality provided by STH-Comet. As an example of
    the implementation that is already being developed, Fig. 7 shows a functionality
    of the vOrder-based application. Fig. 7. Vorder application functionality example
    Show All The same mentioned in Fig. 7 presents the layout of the application during
    normal operation of one of its tasks, the Checking Transport Conditions to be
    precise. Making use of this vOrder application''s functionality the user can check
    in real time the transport conditions of the goods being carried by a truck. These
    include the values that truck''s sensors have read and the maximum, minimum and
    average value of the last trip. Besides the numeric information, the application
    also shows a qualitative evaluation of the transportation conditions at the checking
    time. SECTION VIII. Conclusions Next generation solutions for smart industries
    should provide functionalities for machine-to-machine (M2M) communication, advanced
    human-machine-interaction, and data analytics through the device management. Additionally,
    it is necessary to provide plug-and-play connections based on a standardised toolkit
    among the different Virtual Factory I/O devices (sensors and actuators). In such
    case, the device manager will be responsible for establishing an efficient integration
    between factory devices and apps, enabling an efficient response to changing designs
    at the virtual factory level, and allowing a scalable platform through plug-and-play
    capabilities. Concepts from loT as service will be very important to achieve flexible
    and scalable loT based solutions. On this regard vf-OS captures the notion of
    a manufacturing “operating system” and makes full use of the “concepts” of a classical
    software operating system such as that on a desktop computer and provides the
    necessary tools to realise it either in-cloud or on-premise. This manufacturing
    approach can enable a whole new level of flexibility and scalability in the manufacturing
    domain. In this paradigm, to support the core functionalities of vf-OS, FIW ARE
    provides technical solutions that provide implementations for loT service enablement,
    complex data processing, context management etc. The research work presents possible
    solution that could be a successful strategy for innovation on smart applications
    for agriculture industry that will support collaborative production and logistics
    processes in value chains. The proposes solution will provide efficient and innovative
    cloud-enabled tools to collaborate and optimize logistics assets, involving the
    whole supply chain in the agriculture industry which is explained with the scenario
    being tested and within the scope of this research work. A range of services to
    enable connected fields of the future to integrate better production and logistics
    processes will provide space for independent developers to develop smart applications
    and create value over the processes and data from selected industrial domain.
    These smart applications will provide functionalities such as demand forecast,
    planning, supply, manufacturing, distribution, storage, and replacement / recycling
    etc. in all the stages of production, delivery and postproduction processes. In
    order to validate the proposed solution, we have planned to develop all the apps
    as mentioned above and experiment with some real cases of agriculture industries
    within the scope of the proj ect virtual factory operating system (vf-OS). ACKNOWLEDGMENT
    The research leading to these results has received funding from the European Union
    H2020 Programs under grant agreements No 611312 vf-OS, as well as from FCT - Fundação
    para a Ciência e Tecnologia, research unit CTS - Centro de Tecnologia e Sistemas
    - reference number UID/EEA/0006612013. Authors Figures References Citations Keywords
    Metrics More Like This Research on Wireless Sensor Network Localization Method
    for Real-Time System 2023 China Automation Congress (CAC) Published: 2023 Smart
    Agriculture Wireless Sensor Routing Protocol and Node Location Algorithm Based
    on Internet of Things Technology IEEE Sensors Journal Published: 2021 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: '2017 International Conference on Engineering, Technology and Innovation:
    Engineering, Technology and Innovation Management Beyond 2020: New Challenges,
    New Approaches, ICE/ITMC 2017 - Proceedings'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Towards smart agriculture using FIWARE enablers
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Pons X.
  - Cristóbal J.
  - González O.
  - Riverola A.
  - Serra P.
  - Cea C.
  - Domingo C.
  - Díaz P.
  - Monterde M.
  - Velasco E.
  citation_count: '9'
  description: On 2002, a novel initiative was undertaken by the local water administration
    of Catalonia (the Agència Catalana de l'Aigua) and the Universitat Autònoma de
    Barcelona, leading to a ten-year project where a high number of medium resolution
    satellite images (MODIS and Landsat) were integrated to the daily water management
    to improve decision making effectiveness. This paper describes the methodology
    followed in the successful application of remote sensing, as well as the main
    problems that had to be overcome during its execution. It also presents the products
    that have been calculated. These are integrated into the Agency's corporate GIS
    and immediately available via the intranet for the staff, and a selection is available
    on the Internet.
  doi: 10.5721/EuJRS20124528
  full_citation: '>'
  full_text: '>

    "Access provided by University of Nebraska, Lincoln Log in  |  Register Cart Home
    All Journals European Journal of Remote Sensing List of Issues Volume 45, Issue
    1 Ten Years of Local Water Resource Manage .... Search in:                                        This
    Journal                                                                                Anywhere                                                                  Advanced
    search European Journal of Remote Sensing Volume 45, 2012 - Issue 1 Submit an
    article Journal homepage Open access 251 Views 2 CrossRef citations to date 0
    Altmetric Original Articles Ten Years of Local Water Resource Management: Integrating
    Satellite Remote Sensing and Geographical Information Systems Xavier Pons , Jordi
    Cristóbal, Oscar González, Anna Riverola, Pere Serra, Cristina Cea, show all Pages
    317-332 | Received 14 Oct 2011, Accepted 05 May 2012, Published online: 17 Feb
    2017 Cite this article https://doi.org/10.5721/EuJRS20124528   References Citations
    Metrics Licensing Reprints & Permissions View PDF Abstract On 2002, a novel initiative
    was undertaken by the local water administration of Catalonia (the Agència Catalana
    de l''Aigua) and the Universitat Autònoma de Barcelona, leading to a ten-year
    project where a high number of medium resolution satellite images (MODIS and Landsat)
    were integrated to the daily water management to improve decision making effectiveness.
    This paper describes the methodology followed in the successful application of
    remote sensing, as well as the main problems that had to be overcome during its
    execution. It also presents the products that have been calculated. These are
    integrated into the Agency''s corporate GIS and immediately available via the
    intranet for the staff, and a selection is available on the Internet. Keywords:
    Operational Remote SensingGISWater Resource ManagementData HarmonizationCrop MappingSnow
    Cover Previous article View issue table of contents Next article Download PDF
    X Facebook LinkedIn Email Share Related research  People also read Recommended
    articles Cited by 2 Barriers to adopting satellite remote sensing for water quality
    management Blake A. Schaeffer et al. International Journal of Remote Sensing Published
    online: 22 Aug 2013 Information for Authors R&D professionals Editors Librarians
    Societies Open access Overview Open journals Open Select Dove Medical Press F1000Research
    Opportunities Reprints and e-prints Advertising solutions Accelerated publication
    Corporate access solutions Help and information Help and contact Newsroom All
    journals Books Keep up to date Register to receive personalised research and resources
    by email Sign me up Copyright © 2024 Informa UK Limited Privacy policy Cookies
    Terms & conditions Accessibility Registered in England & Wales No. 3099067 5 Howick
    Place | London | SW1P 1WG     Cookies Button About Cookies On This Site We and
    our partners use cookies to enhance your website experience, learn how our site
    is used, offer personalised features, measure the effectiveness of our services,
    and tailor content and ads to your interests while you navigate on the web or
    interact with us across devices. By clicking \"Continue\" or continuing to browse
    our site you are agreeing to our and our partners use of cookies. For more information
    seePrivacy Policy CONTINUE"'
  inline_citation: '>'
  journal: European Journal of Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Ten years of local water resource management: Integrating satellite remote
    sensing and geographical information systems'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Car N.J.
  - Moore G.A.
  citation_count: '1'
  description: 'Irrigators use many data sources such as automatic weather stations
    (AWS), flow metres, soil moisture probes etc. to inform them when they try to
    make objective decisions at the farm level. Crop models and irrigation Decision
    Support Systems (DSS) that they could use to help them in this are hindered by
    their ability to use the data from all of these sources which diminishes their
    relevance to irrigators and reduces their uptake. Factors causing this hindrance
    are: • Physical connectivity access to data; • Data ownership restrictions; •
    Incompatibility of the data formats; • Unsatisfactory or non-existent quality
    assurance and metadata for reliable use. The Internet and wide radio and cellular
    network coverage across Australia''s irrigation districts has greatly improved
    the physical connectivity aspect of data access while ownership restriction on
    access have no technical solution. This paper addresses the remaining issues through
    discussing on-farm irrigation-specific adaptations or extensions, particularly
    relating to evapotranspiration, of the Bureau of Meteorology''s (BoM) currently
    implemented Water Data Transfer Format (WDTF) described by Walker et al. (2009),
    the international WaterML project described by Zaslavsky et al. (2007) and their
    joint future WaterML 2.0 standard as described by Taylor (2009). These projects
    have evolved rapidly since the authors'' previous work on irrigation data standardisation
    (see Car et al. (2009)) and are now seeing large-scale implementation in Australia
    and thus immediate adaptation and adoption is now possible. A trial adoption of
    the extended standard is described in this paper whereby by data from two different
    AWS networks are able to be used by a previous DSS built by one of the authors
    and others (see Hornbuckle et al. (2009) for the DSS details). In addition to
    the technical implementation of data formats at the network scale, implementation
    by irrigation-related hardware vendors is discussed. This is crucial to the acceptance
    of standards for use at the farm scale. Irrigation sensors and other hardware
    must be natively standards compliant - i.e. not requiring further software layers
    to be so - as irrigation model and DSS users at this scale will not have the ability
    to undertake further development. Specifically, on-farm DSS, irrigation model
    and WDTF/WaterML background is given in Section 1, conceptual work relating to
    on-farm irrigation model requirements in Section 2, test implementations of format
    extensions in Section 3 and case studies of hardware vendor adoption in Section
    4. This proposed exercise in standardisation is part of a larger project by the
    authors looking to improve the utilisation of on-farm DSS through both technical
    and non-technical methods. Increased DSS use for on-farm irrigation management
    decisions not only assists irrigators to use more science in their decisions but
    also enables decision data to be captured which can then be used post-event by
    researchers for analysis.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 'MODSIM 2011 - 19th International Congress on Modelling and Simulation
    - Sustaining Our Future: Understanding and Living with Uncertainty'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Towards standardising irrigation DSS inputs data formats through adaptation
    of the WDTF/WaterML
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ferdov S.
  - Lengauer C.
  - Petrov O.
  - Kostov-Kytin V.
  citation_count: '13'
  description: The hydrothermalsynthesis of Grace titanium silicate-1 from a sodium
    gel at 90°C, without using any organic reactants, was analyzed. In the synthesis
    process, 3.0 g SiO2 of 12μm particle size was added to a heated solution of 7.1
    g NaOH in 40 ml distilled water. The powder x-ray diffraction (XRD) pattern of
    the synthesized material was collected in a step-scan regime on a PW3710 diffractometer.
    The primary powder XRD data standardization, interpretation, and standardization
    of GTS-1 were performed with the PDI package, which provides indexation for the
    known unit cell parameters and refinements.
  doi: 10.1023/B:JMSC.0000033422.72782.99
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Materials Science Article
    A rapid method for low-temperature synthesis of the Na analogue of the microporous
    titanosilicate GTS-1 Published: July 2004 Volume 39, pages 4343–4344, (2004) Cite
    this article Download PDF Access provided by Big Ten Academic Alliance (BTAA)
    Journal of Materials Science Aims and scope Submit manuscript S. Ferdov, C. Lengauer,
    O. Petrov & V. Kostov-Kytin  157 Accesses 11 Citations Explore all metrics Article
    PDF References D. M. CHAPMAN and L. A. ROE Zeolites 10 (1990) 730. Google Scholar   W.
    T. HARRISON, E. G. THURMAN and G. D. STUCKY ibid. 15 (1995) 408. Google Scholar   E.
    A. BEHRENS, D. M. POOJARY and A. CLEARFIELD Chem. Mater. 8 (1996) 1236. Google
    Scholar   E. A. BEHRENS and A. CLEARFIELD Microp. Mater. 10 (1997) 56. Google
    Scholar   S. MINTOVA, B. STEIN, J. M. READER and T. BEIN Stud. Surf. Sci. Catal.
    135 (2001) (Zeolites and Mesoporous Materials at the Dawn of the 21st Century)
    1702. Google Scholar   S. NAIR, H.-K. JEONG, A. CHANDRASEKARN, C. M. BRAUNBARTH,
    M. TSAPATSIS and S. KUZNICKI Chem. Mater. 13 (2001) 4247. Google Scholar   M.
    S. DADACHOV and W. T. A. HARRISON J. Solid. State Chem. 134 (1997) 409. Google
    Scholar   J. MACICEK, PDI: A Powder Data Interpretation Package, Internal Report
    of the Institute of Applied Mineralogy, Sofia (1988) p. 36. PC-APD Version 4.0g,
    Application Laboratory for X-ray Diffraction Philips AXR, Leyweg 1 7602 EA Almelo,
    Holland. Download references Author information Authors and Affiliations Marie
    Curie Fellowship of the European Community, Institut für Mineralogie und Kristallographie,
    Universität Wien, Geozentrum, Althanstrasse 14, A-1090, Wien, Austria S. Ferdov
    Institut für Mineralogie und Kristallographie, Universität Wien, Geozentrum, Althanstrasse
    14, A-1090, Wien, Austria C. Lengauer Central Laboratory of Mineralogy and Crystallography,
    Bulg. Acad. Sci., bl. 107, Academic G. Bonchev Str., No 2, 1113, Sofia, Bulgaria
    O. Petrov & V. Kostov-Kytin Rights and permissions Reprints and permissions About
    this article Cite this article Ferdov, S., Lengauer, C., Petrov, O. et al. A rapid
    method for low-temperature synthesis of the Na analogue of the microporous titanosilicate
    GTS-1. Journal of Materials Science 39, 4343–4344 (2004). https://doi.org/10.1023/B:JMSC.0000033422.72782.99
    Download citation Issue Date July 2004 DOI https://doi.org/10.1023/B:JMSC.0000033422.72782.99
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Polymer Rapid Method Use our pre-submission checklist Avoid
    common mistakes on your manuscript. Sections References Article PDF References
    Author information Rights and permissions About this article Advertisement Discover
    content Journals A-Z Books A-Z Publish with us Publish your research Open access
    publishing Products and services Our products Librarians Societies Partners and
    advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress
    Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Materials Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A rapid method for low-temperature synthesis of the Na analogue of the microporous
    titanosilicate GTS-1
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
