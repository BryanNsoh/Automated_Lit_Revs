- analysis: '>'
  authors:
  - Taneja A.
  - Nair G.
  - Joshi M.
  - Sharma S.
  - Sharma S.
  - Jambrak A.R.
  - Roselló-Soto E.
  - Barba F.J.
  - Castagnini J.M.
  - Leksawasdi N.
  - Phimolsiripol Y.
  citation_count: '9'
  description: Artificial intelligence (AI) involves the development of algorithms
    and computational models that enable machines to process and analyze large amounts
    of data, identify patterns and relationships, and make predictions or decisions
    based on that analysis. AI has become increasingly pervasive across a wide range
    of industries and sectors, with healthcare, finance, transportation, manufacturing,
    retail, education, and agriculture are a few examples to mention. As AI technology
    continues to advance, it is expected to have an even greater impact on industries
    in the future. For instance, AI is being increasingly used in the agri-food sector
    to improve productivity, efficiency, and sustainability. It has the potential
    to revolutionize the agri-food sector in several ways, including but not limited
    to precision agriculture, crop monitoring, predictive analytics, supply chain
    optimization, food processing, quality control, personalized nutrition, and food
    safety. This review emphasizes how recent developments in AI technology have transformed
    the agri-food sector by improving efficiency, reducing waste, and enhancing food
    safety and quality, providing particular examples. Furthermore, the challenges,
    limitations, and future prospects of AI in the field of food and agriculture are
    summarized.
  doi: 10.3390/agronomy13051397
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agronomy All Article Types Advanced   Journals
    Agronomy Volume 13 Issue 5 10.3390/agronomy13051397 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Baohua Zhang
    Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 8905 Citations 9 Table of Contents Abstract Introduction Role of
    AI in the Agriculture Role of AI in the Food Processing Role of AI in Food Quality
    and Food Safety Role of AI in the Personalized Nutrition Conclusions and Future
    Perspectives Author Contributions Funding Data Availability Statement Acknowledgments
    Conflicts of Interest References Altmetric share Share announcement Help format_quote
    Cite question_answer Discuss in SciProfiles thumb_up Endorse textsms Comment first_page
    settings Order Article Reprints Open AccessReview Artificial Intelligence: Implications
    for the Agri-Food Sector by Akriti Taneja 1, Gayathri Nair 1, Manisha Joshi 1,
    Somesh Sharma 1,*, Surabhi Sharma 2, Anet Rezek Jambrak 3, Elena Roselló-Soto
    4,*, Francisco J. Barba 4, Juan M. Castagnini 4, Noppol Leksawasdi 5 and Yuthana
    Phimolsiripol 5,* 1 School of Bioengineering and Food Technology, Shoolini University,
    Solan 173229, Himachal Pradesh, India 2 School of Agricultural Science and Technology,
    RIMT, Fatehgarh Sahib 147301, Punjab, India 3 Faculty of Food Technology and Biotechnology,
    University of Zagreb, 10000 Zagreb, Croatia 4 Department of Preventive Medicine
    and Public Health, Food Science, Toxicology and Forensic Medicine, Faculty of
    Pharmacy, Universitat de València, Avda. Vicent Andrés Estellés s/n, 46100 Burjassot,
    Spain 5 Faculty of Agro-Industry, Chiang Mai University, Chiang Mai 50100, Thailand
    * Authors to whom correspondence should be addressed. Agronomy 2023, 13(5), 1397;
    https://doi.org/10.3390/agronomy13051397 Submission received: 8 April 2023 / Revised:
    14 May 2023 / Accepted: 15 May 2023 / Published: 18 May 2023 (This article belongs
    to the Section Precision and Digital Agriculture) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Artificial intelligence (AI) involves the development
    of algorithms and computational models that enable machines to process and analyze
    large amounts of data, identify patterns and relationships, and make predictions
    or decisions based on that analysis. AI has become increasingly pervasive across
    a wide range of industries and sectors, with healthcare, finance, transportation,
    manufacturing, retail, education, and agriculture are a few examples to mention.
    As AI technology continues to advance, it is expected to have an even greater
    impact on industries in the future. For instance, AI is being increasingly used
    in the agri-food sector to improve productivity, efficiency, and sustainability.
    It has the potential to revolutionize the agri-food sector in several ways, including
    but not limited to precision agriculture, crop monitoring, predictive analytics,
    supply chain optimization, food processing, quality control, personalized nutrition,
    and food safety. This review emphasizes how recent developments in AI technology
    have transformed the agri-food sector by improving efficiency, reducing waste,
    and enhancing food safety and quality, providing particular examples. Furthermore,
    the challenges, limitations, and future prospects of AI in the field of food and
    agriculture are summarized. Keywords: machine learning; smart farming; internet
    of things; sustainable management; food quality; food safety 1. Introduction The
    world’s population is rapidly growing and is expected to reach around 9.7 billion
    by 2050 [1]. As a result, there is a growing concern about how to meet the increasing
    demand for food while also ensuring food security and sustainability. In this
    regard, the use of artificial intelligence (AI) applications in the agri-food
    sector has the potential to revolutionize the industry and increase sustainability
    in several ways. It can help farmers, food manufacturers, and distributors make
    more informed decisions, improve efficiency, reduce waste, and improve food security
    and sustainability. The Nobel-prize-winning economist Herbert Simon in 1965 said,
    “Machines will be capable of doing any work a man can do”. His visionary perspective
    has come true today through the remarkable achievements that occurred through
    AI applications [2]. AI refers to the ability of machines or computer programs
    to perform tasks that normally require human intelligence, such as learning, reasoning,
    problem-solving, and decision-making. There are various subfields of AI, including
    machine learning (ML), deep learning, natural language processing, computer vision,
    robotics, and cognitive computing. There are several algorithms, for instance,
    reinforcement learning [3], swarm intelligence, cognitive science, expert system,
    fuzzy logic (FL), Artificial Neural Networks (ANN), and Logic Programming, that
    can be used in AI technology [4]. Each of these algorithms has its own unique
    advantages and limitations, and the choice of algorithm will depend on the specific
    task or problem at hand. AI is being used in a wide range of applications, such
    as speech recognition, image and video analysis [5], autonomous vehicles [6],
    medical diagnosis, financial forecasting, and many others [7]. Similar to any
    other industry, AI can also be used in the agri-food sector to improve efficiency
    [8] and develop new, more nutritious crops [9], reduce waste [10], and ensure
    safety [11]. AI can be used to optimize crop yields [12] and improve distribution
    and logistics [13]. Table 1 presents summary of the equipment and product’ developed
    by various AI technologies and their domains. Table 1. Summary of the equipment
    and product’ developed by various AI technologies and their domains. In precision
    agriculture, AI can be used to analyze data from sensors, drones, and satellites
    to optimize farming practices, such as irrigation, fertilization, and pest management.
    This can lead to higher yields, lower costs, and reduced environmental impact
    [29,30]. In crop monitoring, AI-powered cameras and sensors can monitor crops
    in real-time, detecting diseases, pests, and nutrient deficiencies. This allows
    farmers to take action quickly and prevent crop loss [31,32]. AI algorithms can
    analyze weather patterns, soil conditions, and historical data to predict crop
    yields and market demand. This can help farmers plan their planting and harvesting
    schedules and optimize their pricing strategies [33]. During supply chain optimization,
    AI can help streamline the supply chain by predicting demand, optimizing logistics,
    and reducing waste. For example, AI algorithms can be used to predict the optimal
    time to harvest crops and route trucks, and optimize inventory levels [34,35].
    In the food processing industry, AI can be used to optimize food processing operations,
    such as sorting and grading, and to detect defects or contaminants in food products
    [30,36]. AI can also be used to identify and sort fruits and vegetables based
    on their size, color, and other attributes [37]. This can help improve the quality
    and consistency of food products and reduce waste. AI can be used to monitor food
    safety by analyzing data from sensors and cameras to detect potential contaminants
    or other hazards. This can help prevent foodborne illness and improve public health
    [38,39]. In addition, AI can be used to analyze individual consumer data, such
    as age, gender, and activity level, to provide personalized nutritional recommendations.
    This can help consumers make more informed choices about their diet and improve
    their overall health [40]. While these studies provide some valuable insights
    into AI applications in the agri-food sector, a detailed review is still needed
    to understand the current advancements of AI technology in the agri-food sector.
    Therefore, the objective of this review is to highlight the recent developments
    in the food and agriculture sector along with the application of AI technology,
    providing specific examples by the databases during 2010–2023. The review also
    summarizes the future prospects, challenges, and limitations in the field. 2.
    Role of AI in the Agriculture The food industry has always been dependent on the
    agriculture sector since its inception. An increase in food production by the
    agriculture sector can lead to a larger supply of raw materials for the Fast-Moving
    Consumer Goods (FMCG) industries, which rely on these raw materials for processing
    and manufacturing products [26]. The COVID-19 pandemic has significantly affected
    innumerable lives and the supply of these industries pessimistically [41,42].
    The government’s decision to declare a state of emergency led to the closure of
    numerous industries worldwide, which had an effect on the entire supply chain,
    from the farmer to the consumer [43]. The unexpected decline in output and income,
    the drop in oil prices, the drop in tourism receipts, the issues with climate
    change, and other reasons are all connected to the COVID-19 pandemic [44]. According
    to the FAO, the number of people suffering from hunger and malnutrition has been
    on the rise in recent years [45]. However, by introducing AI and ML in crop management
    and using high-tech automated systems, the agriculture industry can tackle many
    of the problems that affect crop production and improve the quality and quantity
    of raw materials available to the food industry. Figure 1 depicts the impact of
    AI on the Argo food sector and FMCG. Some of the ML technologies introduced in
    the agriculture sector that have contributed to improving crop management are
    discussed in this section. Figure 1. Impact of artificial intelligence (AI) and
    machine learning (ML) in the Agrofood and FMCG sector. 2.1. Grain Quality Manual
    grain inspection is a time-consuming process and is prone to human error, which
    can result in the selection of lower-quality grains. This is because manual inspection
    relies on human visual acuity and can be affected by factors, such as fatigue,
    distractions, or variability in lighting conditions. Therefore, the use of computer
    vision systems in grain inspection is becoming increasingly popular. These systems
    use advanced imaging techniques and ML algorithms to analyze images of grains
    and identify defects or impurities, such as broken kernels, foreign materials,
    or fungal infestations [46]. ANN, dense scale-invariant feature transform (DSIFT)
    algorithm, and support vector machines (SVM) are ML techniques that have been
    successfully applied in the agriculture sector for the classification and identification
    of grains and other agricultural products. ANNs are used to classify different
    wheat species based on their visual characteristics, such as shape, size, and
    color [47]. DSIFT algorithm is a computer vision technique that can identify features,
    such as the size, shape, and texture of the wheat grains, and use them to classify
    the grains into different categories [48]. SVM is another ML technique that is
    used for the categorization of wheat grains, identification of fungal species
    in rice, germinated wheat grains, and analysis of milled rice grains. Some technologies
    apply computer vision systems for the inspection of grains in the agricultural
    sector: (i) examination of milled rice grains using SVM, (ii) computerized wheat
    quality assessment system, and (iii) development of a method using hyperspectral
    imaging system for the detection of Fusarium infected wheat grains [49,50]. Computer
    vision systems can help in the accurate and automated monitoring of various plant
    phenology stages, such as seedling emergence, leaf unfolding, flowering, and fruit
    ripening. They can also aid in the early detection of plant stress and diseases,
    allowing for timely interventions and preventing crop losses. In 2015, researchers
    proposed a computer vision system that uses disease-specific image processing
    algorithms to identify the presence and severity of leaf spot diseases in rice
    plants [51]. Backpropagation neural networks (BPNN) have been used in conjunction
    with other technologies, such as wavelets and fuzzy inference systems, for crop
    disease detection and classification [52]. In 2017, a study was conducted to investigate
    the risks of chlorosis due to iron deficiency in soybean plants using real-time
    phenotyping and ML techniques [53]. This approach allowed for the early detection
    and monitoring of iron deficiency stress in soybean plants, enabling researchers
    to optimize iron fertilization strategies and improve crop yields [54,55,56,57].
    2.2. Pest Detection and Weed Management Accurate identification of insect species,
    size variation, and stage of development is crucial for effective pest management
    in agriculture. By identifying the type and number of insects present in a crop
    field, farmers can take appropriate measures to control the pest population and
    prevent damage to their crops. Several AI and ML technologies are being developed
    and tested for insect detection and counting. Some of these technologies use computer
    vision algorithms, while others rely on ML algorithms to identify and classify
    different insect species [58,59]. However, it is important to note that these
    technologies are still in their testing stages and have not yet been widely adopted
    in the agricultural industry. Similarly, herbicides have been widely used by farmers
    for many years to control weeds and improve crop yields. However, the overuse
    or improper application of herbicides can have negative impacts on both human
    health and the environment. To minimize the negative impacts of herbicides, there
    is a growing need for more precise and accurate application methods [60]. Precision
    agriculture techniques, such as site-specific application, can help farmers apply
    herbicides only where they are needed, reducing the amount of chemicals used and
    minimizing the risk of contamination. The development of AI-based technologies
    which use ML algorithms and computer vision techniques to detect and classify
    different types of weeds in crop fields has the potential to improve the efficiency
    and sustainability of agriculture while also reducing the need for herbicides
    and improving crop yields [61]. Unmanned aircraft systems (UAS) and counter propagation-artificial
    neural networks (CP-ANN) were used for the detection of the weed Silybum marianum
    [62]. The use of ANN and Multispectral/Hyperspectral imaging technologies can
    be very effective in detecting and recognizing weed species in crop fields. CP-ANN
    and multispectral imaging captured by UAS were used to detect the weed Silybum
    marianum [63]. CP-ANN is a type of artificial neural network that can be used
    for pattern recognition tasks, while multispectral imaging involves capturing
    images of crop fields at different wavelengths of light. The combination of these
    technologies allowed the researchers to identify the presence of the weed with
    high accuracy and precision. In another research, hyperspectral imaging and ML
    techniques were used to develop a method for crop and weed species recognition
    [3]. Hyperspectral imaging involves capturing images of crop fields at many different
    wavelengths of light, which can provide more detailed information about the spectral
    properties of different plant species. ML algorithms were then used to analyze
    these images and classify different plant species, including both crops and weeds.
    Researchers have also developed SVM based algorithm for the classification of
    different types of weeds in grassland cropping systems based on images captured
    by unmanned aerial vehicles (UAVs) [64]. Robotic weed control is also an emerging
    technology that shows great promise for the future of agriculture. Robotic weed
    control systems typically use computer vision and ML algorithms to detect and
    identify weeds in crop fields, then use robotic arms or other mechanical tools
    to remove or destroy the weeds. These systems can operate in a wide range of crop
    environments, including greenhouses, where traditional weed control methods, such
    as herbicides, may not be effective or appropriate [65]. There is the possibility
    of cultivars being equipped with finger weeders or elastic tines for both inter
    and intra-row types of weed control [66]. For analyzing site-specific weed control,
    precision weed management as a part of precision farming is grounded on the utilization
    of information technology [67]. Although intelligent mechanical weed control would
    be more felicitous than weeding devices with cutting action, contrary to time-based
    weed removal [68], it is possible to remotely regulate the tendency of tines of
    spring-tine harrow prototype systems based on the conditions of soil, the density
    of weed, and crop production [69]. 2.3. Crop Selection and Yield Improvement Agricultural
    planning performs an important role in food security around the world, especially
    in countries with the agro-based sector. The challenge in selection of suitable
    crops with improved yield is critical as this could be varied depending on numerous
    conditions, such as weather, soil quality, water access, and pests and diseases
    [8]. AI and ML technologies are being increasingly used in crop selection and
    yield improvement in agriculture. These technologies are particularly useful in
    crop breeding and genetic improvement. By analyzing genetic data from different
    crop varieties and using ML algorithms to identify key traits associated with
    yield and other desirable characteristics, plant breeders can develop new crop
    varieties that are better adapted to specific environmental conditions and produce
    higher yields. Automation technologies, such as robots, are being increasingly
    used to improve crop yields by reducing labor costs and improving efficiency in
    various agricultural tasks, including spraying herbicides, removing weeds, and
    harvesting fruits and vegetables [4,7,8]. Robots, such as the Berry 5 Robot from
    Harvest Croo Robotics (Tampa, FL, USA), are designed to automate the harvesting
    of strawberries, which is a labor-intensive and time-consuming process [12]. The
    robot uses computer vision and ML algorithms to identify and pick ripe strawberries
    at a faster rate than humans can. This can help farmers to reduce labor costs
    and improve their yields by ensuring that more strawberries are harvested at the
    optimal time. Similarly, robots, such as the “Robocrop”, are being developed for
    specific agricultural tasks, such as pruning flowers on strawberry plants. Furthermore,
    the image-processing robot being developed for picking ripened strawberries uses
    computer vision and ML algorithms to identify and pick the strawberries, reducing
    labor costs and improving the speed and efficiency of the harvesting process [70].
    The National Physical Laboratory (NPL) in London is developing robots that use
    computer vision and ML to identify water and nutrient levels, control weeds, and
    perform sorting and packaging [71]. Researchers have developed a method for measuring
    plant water retention using image processing techniques in combination with software,
    such as Adobe Photoshop CC 2021 (version 22.0.0.) and MATLAB (version R2022b (9.13)).
    For the purpose of using X-ray CT to study unsaturated Hostun sand and its water
    retention behavior, a complete configuration and setup were created. A “step-by-step”
    technique for obtaining sufficiently high-quality reconstructions that allow the
    three phases of the material (grain, water, and air) to be differentiated was
    also provided. The visualization and characterization of the three stages inside
    the specimen were made easier using picture post-processing. This made it easier
    to create a measuring map that encompasses the full specimen field [72]. Robotic
    chassis are developed for robot software where they are assigned their specific
    tasks. This robot system includes navigation through a field, robotic arms to
    eliminate unwanted flowers, and image capturing [70]. Similarly, Agboka et al.
    [73] applied Agroecological breeding methods, such as maize–legume intercropping
    (MLI) and push-pull technology (PPT), that have been found to be effective in
    minimizing the losses due to insects. Two simple and explainable models, namely,
    the hybrid fuzzy logic combined with the genetic algorithm and symbolic regression,
    are used to forecast maize production. This study also reported that the scale-up
    of MLI and PPT systems improved productivity in sustainable farming. 2.4. Big
    Data and IoT in Smart Farming With the use of modern technology called the Internet
    of Things (IoT), gadgets may link remotely to enable smart farming. To improve
    efficiency and performance across all sectors, the IoT has started to have an
    impact on a wide variety of businesses, including those in health, trade, communications,
    energy, and agriculture [8]. The adoption of modernized technologies in agriculture
    has led to the emergence of “smart farming”, which is a revolutionary approach
    that leverages advanced technologies to increase the quality and quantity of agricultural
    production. AI encourages smart farming, a sustainable technique that helps to
    avoid resource waste (such as fertilizers and pesticides) and achieve sustainable
    development, to replace conventional agricultural practices and methodologies
    [62]. By providing farmers with detailed information on specific crops, such as
    soil nutritional deficiencies, and moisture levels, and hyper-spectral data to
    prevent damage, smart farming enables farmers to make more informed decisions
    about their crops and to optimize their production processes [9]. According to
    research, the Supply Chain Big Data Analytics Market will climb to $9.28 billion
    by 2026 [74]. The Agri-IoT framework has the potential to significantly benefit
    farmers by providing them with real-time data and alerts. By integrating social
    media trends, farm council alerts, and automatic reasoning, the platform can help
    farmers to make informed decisions and take action to mitigate the impact of climatic
    conditions on their crops [75]. Another aspect of smart farming is climate condition-based
    irrigation. The Specialty Crop Research Initiative-Managing Irrigation and Nutrients
    with Distributed Sensing (SCRI-MINDS) project is a great initiative aimed at improving
    plant production. It has been developed to increase efficiency in plant production
    while controlling the excessive use of irrigation water and nutrients [8]. Microsoft
    (Redmond, WA, USA) has also developed an AI-based sowing application that provides
    recommendations, such as the optimal period for sowing seeds, preparing land for
    cultivation, etc. The model by mobile phone app uses remote sensing data from
    geo-stationary satellite images to predict crop yields through every stage of
    farming. To determine the optimal sowing period, the moisture adequacy index was
    calculated. The input data include historical sowing area, production, yield,
    and weather. The app sends sowing advisories to participating farmers on the optimal
    date to sow. The farmers do not need to install any sensors in their fields or
    incur any capital expenditure; they just need a feature phone capable of receiving
    text messages [76]. It is thus imperative that smart solutions are being developed
    for global food safety and security, sustainability of food consumption, and the
    well-being of society. Likewise, environmentally friendly strategies could reduce
    the use of resources (water, fertilizers, herbicides, etc.) for agriculture, reduce
    losses, and shelf-life extension of food products for global food security [77].
    Low altitude spectral imaging for identifying pest infestation, nutrient or moisture
    deficiency, and many more computer-aided systems are being introduced for the
    protection of natural resources and sustainable agriculture. The use of sensors
    deployed to monitor farm conditions and low-altitude air-borne hyperspectral imaging
    is an example of smart farming [78]. Smart farming is one of the biggest methods
    or systems of precision farming. Precision farming involves the precise number
    of inputs, such as soil, water, fertilizer, etc., to be distributed in an accurate
    time and at an accurate place, such as weed control [79]. Trimble Agriculture
    (Westminster, CO, USA), an industrial technology company, has developed a system
    called WeedSeeker spot spray which is an innovative solution for efficient and
    targeted weed control. By using sensors to detect the presence of weeds and a
    spray nozzle to deliver a precise amount of chemicals, the system can help reduce
    the use of herbicides and minimize the environmental impact of weed control [65,66].
    This system can be mounted even on traditional spraying machines with some modifications
    and is most effective in areas with intermittent growth of weeds. Precision Agriculture
    (PA) can be described as a management concept having the ability to recognize
    variability within the soil environment and maximize agricultural production while
    minimizing environmental concussion, i.e., temperature and humidity changes, for
    a particular location. Yield Technology (Carrollton, MO, USA) and Bosch (Stuttgart,
    Germany) have developed a range of technologies that can be used in precision
    agriculture to optimize crop yield and reduce resource waste. These technologies
    include drones, computers, data analytics, and robots, among others [77]. 3. Role
    of AI in the Food Processing AI and ML technologies are being increasingly adopted
    in the food processing industry. These technologies are helping to optimize various
    processes and improve overall efficiency and quality control. The capabilities
    of the intelligent systems in various tasks, such as intelligent food packaging,
    product sorting, foreign object detection, new food product development, equipment
    cleaning, and supply chain management, are elucidated along with the equipment
    and products developed by various AI technologies. 3.1. Intelligent Food Packaging
    The proper arrangement and packing of food products are among the challenging
    tasks and time-consuming processes in the manufacturing sections of the food industry.
    Food packing has four vital roles—protecting the food, displaying the product,
    sanitation, and transportation ability [80]. It protects the food from damage
    caused by biological, chemical, and physiological reasons during the complete
    food logistic system. The visual appearance of the package helps consumers to
    judge the quality of the food and is the first impression about the product. Therefore,
    effective packaging is an essential part of the food manufacturing sector [81].
    AI and ML are increasingly being used in the food packaging industry to improve
    the design, production, and functionality of packaging materials. AI and ML technologies
    are being used in the food packaging industry to improve packaging design. Liu
    [82] applied a packaging design model based on deep convolution generative adversarial
    networks (DCGAN). A packaging design image can be enhanced using visual communication
    technology, resulting in better visual communication ability, a higher degree
    of image information fusion, and an improved packaging design effect. However,
    the development of AI-based systems is a tedious task in the fruits and vegetables
    sector owing to the inconsistency in shape, color, and size [83]. Thus, a copious
    quantity of data is required to train the system properly and perform the task
    in a structured manner. Intelligent tools, such as robotics and drones, can also
    perform a critical role in reducing the packaging cost significantly [14,84].
    For instance, robotics can be used to automate the packaging process, reducing
    the need for human labor and improving the speed and efficiency of the process.
    Robots can be used to sort and inspect food products to ensure that they are properly
    packaged and meet quality standards. Robotics can be used to manage inventory,
    ensuring that packaging materials are available when needed and reducing the risk
    of shortages or excess inventory. In addition, drones can be used to deliver food
    products directly to consumers, reducing the need for packaging and transportation
    [85]. 3.2. Product Sorting AI-based systems can incorporate a variety of technologies,
    such as laser technology, X-ray systems, high-resolution cameras, and infrared
    (IR) spectroscopy, to evaluate the parameters of products at the input level [86].
    These technologies can help identify defects, contaminants, and inconsistencies
    in the products, enabling intelligent decision-making and improving the overall
    quality of the products. However, the inconsistent product homogeneity can be
    a major drawback for sorting methods that rely on input-level evaluation. Inhomogeneous
    products may result in inaccurate sorting decisions, leading to increased waste
    or lower product quality [15]. To overcome this challenge, some sorting systems
    use multiple sensors or technologies to assess product quality and identify defects
    from multiple perspectives. Additionally, ML algorithms can be trained to recognize
    patterns and variations in product properties, helping to improve accuracy and
    consistency in sorting decisions. TOMRA (Asker Municipality, Norway), the global
    provider of advanced collection and sorting systems, has developed an AI system
    that can efficiently perform the sorting task with 90% efficiency. Industries,
    by utilizing such systems, have gained some advantages, such as increased production,
    high-quality yielding, and reduced labor cost. It has been reported that the segregation
    and arrangement issues can be enhanced by 5–10% in the case of potatoes [14].
    In the apple processing industry, deep learning, a subdivision of ML, and a sub-field
    of AI help to categorize the apples with the help of datasets through pattern
    recognition and decision-making. Similarly, Deep Convolutional Neural Network
    (CNN) assists in identifying the type of apple with the support of CVS (Concurrent
    Versions System). The deep learning model was processed by the data from image
    processing, apple detection, and ripeness classification. The classifiers are
    able to achieve the best result, i.e., the ripeness class of an apple from a given
    digital image [15]. In addition, coffee beans are classified based on the standards,
    category, defects, and nature of the beverage produced. The types of Arabic espresso
    are numbered from the grouping by type or imperfection, from two to eight [87].
    3.3. Foreign Object Detection Contamination by foreign objects causes major issues,
    including food recalls, rejection by consumers, harm to customers, and leads to
    a fall in brand reliability. Foreign matter, including insects, glass, metal,
    or rubber, may accidentally enter the food or packing material during food processing,
    handling, or preparation. Although the magnitude of risks associated with foreign
    matter depends on the size, type, clarity, and hardness of the object, the consumption
    of food contaminated with such objects could lead to choking or other complications
    [88]. As identification of such contaminants with the unaided eye is tough, AI
    and ML technologies can perform an important role in their detection by analyzing
    images of the food products to identify any foreign objects that may be present
    [47]. One approach to foreign object detection using AI and ML is to use image
    recognition algorithms that are trained on large datasets of images of contaminated
    and uncontaminated food products. The algorithm can then analyze images of the
    food products in real-time and identify any foreign objects that may be present
    [89]. Shimonomura et al. [90] applied a cylindrical tactile image sensor for detecting
    foreign objects in food based on differences in hardness. Small, hard foreign
    bodies that were sub-millimeter in size and mixed in with soft food could be successfully
    detected by using a reflective membrane-type sensor surface with high sensitivity.
    Through investigations to find shell pieces left on the surface of raw shrimp
    and bones left in fish fillets, the effectiveness of the suggested method was
    confirmed. 3.4. New Food Product Development As new product development in the
    food industry completely relies on the consumer’s perspectives, the data collected
    by the various decision-making systems are useful in the launch of new products.
    By analyzing the data gathered by the system, the ML-based module could answer
    the question “what exactly the consumers are looking for” and make proper decisions.
    One of the multinational companies has installed automatic vending machines throughout
    the USA for delivering soft drinks, and consumers have thousands of options to
    select their favorite flavors. The information stored by the machine could be
    analyzed by the ML module and deep learning algorithms for the development of
    a new product; one such example is Cherry Sprite, launched by the company. It
    has also been proposed that, in the upcoming decades, many of the food industries
    will benefit from the ML-based decision-making system for the launch of new food
    products [91]. A biotechnology company has launched the world’s first bioactive
    peptide through AI technology. A sports nutrition ingredient is a unique peptide
    network derived from rice protein for alleviating inflammation via modulating
    cytokine responses and for improving immune activity. The company has become the
    world’s first company to demonstrate the potential of AI in improving human health
    [92]. Another US-based IoT-focused technology company has introduced AI-powdered
    ‘home cooking sidekick’, a web and mobile application that integrates with smart
    kitchen assistant Hello Egg to fully automate kitchen needs. The home assistant
    is powered through voice technology to recommend a diet plan based on the preferences
    of an individual. This can also manage the pantry, categorize shopping cart, exhibits
    video recipes, and assists in the delivery of groceries [93]. The role of sensory
    panelists employed in food and beverage industries aim to sensory evaluate the
    new products based on the flavor preferences of consumers. Unfortunately, it is
    difficult to predict the perception and preferences of the target group. This
    led industries to develop a robust methodology for measuring and predicting consumer
    preferences through an AI-based Gastrograph system which uses ML and predictive
    algorithms to understand market preference [94]. 3.5. Equipment Cleaning and Maintenance
    Pieces of machinery and processing tools used in food processing industries must
    be cleaned regularly for proper maintenance. AI-based systems, such as Cleaning
    in place (CIP) and Clean-out-of-place (COP) systems, assist the food industry
    to ensure hygiene and maintain product quality at high standards. For its implementation,
    various cameras and sensors are installed to carry out the tasks. Currently, a
    European company specializing in providing cleaning solutions has introduced SOCIP,
    a Self-Optimizing-Clean-In-Place system to autonomously optimize the cleaning
    process for food manufacturing equipment using AI technology. SOCIP employs ultrasonic
    sensing imaging methods and optical fluorescence methods to assess the number
    of food particles and microbial debris that is present inside the equipment. The
    SOCIP system works by using sensors to scan the inside of the equipment and create
    a real-time image of the surface. This information is then used to determine how
    much cleaning is necessary to achieve the desired level of cleanliness [36,74].
    3.6. Demand-Supply Chain Management Presently, food industries are concerned about
    food safety policies, which are necessary for the transparent execution of all
    food logistic activities [2,62]. To monitor every stage of the process, for instance,
    from cost regulation to resource management, AI is being employed. It manages
    and predicts the passage of possessions from where they are grown to the place
    where consumers gather them [95]. One company provides integrated AI-enabled solutions
    for the retail industry. Their solutions can help to optimize various aspects
    of retail operations, including transportation, billing, resource management,
    and inventory control. The system can improve the retail industry using AI algorithms
    to optimize packaging and improve shelf life [96]. By analyzing data on product
    characteristics, environmental conditions, and other factors, AI algorithms can
    help to identify the optimal packaging materials and designs to improve product
    quality and extend shelf life. Additionally, AI can help to improve food safety
    by providing greater transparency and visibility into the logistics and supply
    chain process. By tracking products from farm to table, retailers can identify
    and address potential food safety issues before they become a problem. There will
    be a need for more contributions that make use of a variety of data sources in
    order to realize the goal of an expanded agri-food supply chain that involves
    more stakeholders and the whole supply chain lifetime. Additionally, to complete
    the loop in sustainable agri-food, the extended AI support for agri-food needs
    to increase the use of contextual information, food consumption, and food waste
    reduction [97]. 3.7. 3D/4D Food Printing-Extrusion Technology AI can be of particular
    interest when combined with 3D and 4D printing technologies. By integrating AI
    into 3D/4D printing process, it is possible to increase the performance of the
    printers, reduce the risk of errors, and facilitate automated production. The
    combination of AI and 3D/4D printing technologies can lead to the establishment
    of start-ups and research projects that integrate AI into 3D/4D printing products
    and services [98,99]. The food processing sectors worldwide are now adopting 3D
    food printing technology to engender operations more systematically and independently.
    This technology can cause active food value chains more client-friendly and viable
    by delivering on-demand food manufacturing, empowering computerized food customization,
    and reducing food wastage. A 3D/4D food printer operates in a similar way to a
    regular 3D printer, with the primary difference being the printing medium used.
    Instead of using melted plastic, a 3D/4D food printer uses a food material as
    the printing medium. Consumers can materialize designs from an e-commerce platform
    via websites or mobile applications, and this would minimize warehousing, packaging,
    and delivery charges [100]. The effectiveness of printing food is improved while
    food processing expenses and time are minimized, and time is saved by refining
    3D printing techniques and equipment [101]. An additional aspect is that personalized
    products will be delivered very quickly using 3D printing technology than regular
    food processing technology. As the products are delivered much faster, the need
    for synthetic polymer-based packing materials and chemical preservatives could
    be eliminated, and this will also improve food safety. Furthermore, multiphase
    processing of food products could be minimized to a single stage [102]. 4. Role
    of AI in Food Quality and Food Safety AI’s captivating capability has made it
    an extremely appealing technology to use in industries not only for decision-making,
    process estimation, cost savings, and high profitability but also for overall
    quality improvement [103]. AI, combined with data science, has the potential to
    improve the quality of food and service offered by cafes, restaurants, online
    food delivery systems, and food stores, leading to increased sales, profitability,
    and customer satisfaction. By analyzing data and applying algorithms, AI can help
    to improve sales prediction, menu optimization, personalized recommendations,
    and supply chain optimization [16]. AI has made significant contributions to various
    aspects of the food industry, and these contributions can be broadly classified
    into three main categories, food quality management, food security management,
    and food waste management [74]. AI has significant roles and potential applications
    in several food sectors, including dairy, beverage, and bakery, and are presented
    in Figure 2. Figure 2. The layout of the functions of AI in several food sectors.
    AI can be used to analyze large amounts of data from dairy production processes
    to identify patterns and make predictions that can optimize production and improve
    product quality. By using AI tools, such as fuzzy logic and ANN, dairy producers
    can make more accurate predictions and adjust their processes accordingly, leading
    to more efficient and higher-quality production. Similarly, AI tools, such as
    e-nose, e-tongue, CVS, and image analysis, can help producers optimize their production
    processes and understand consumer preferences in the beverage industry. AI can
    also help the bakery industry to improve product quality, increase productivity,
    and by using AI-powered tools, such as robots and visualization, bakeries can
    create more innovative products and optimize their production processes [85].
    4.1. Food Quality Management Fresh fruits and vegetables are highly perishable
    and can quickly spoil if not stored and monitored properly. In the past, many
    vendors did not have access to the necessary tools to monitor the real-time condition
    of fruits in storage, which resulted in significant food waste. However, with
    the advancements in technology, there are now several solutions available that
    can help vendors to monitor the condition of fresh fruits and vegetables in real-time
    [104]. Digital twin (DT) technology is a promising tool for monitoring the quality
    and condition of fresh fruits and vegetables throughout the cold chain. The extent
    of tissue damage that occurs in fruits depends on several factors, including physical
    and biochemical properties, and environmental factors, such as temperature, humidity,
    and postharvest treatments. By using DT technology to monitor these factors, it
    is possible to identify potential issues early on and take corrective action to
    prevent further damage [17]. In addition to monitoring the cold chain, DT technology
    can also be used to optimize the storage and transportation of fresh fruits and
    vegetables [17,105]. Thermal imaging is a non-contact and emerging technology
    that is becoming increasingly popular for fruit quality examination in the fruit
    and food industry. It offers a non-destructive way to examine a product without
    the need for extraction, which could cause permanent damage. Infrared thermal
    imaging detects the presence of damage or defects in fresh fruits and vegetables
    by measuring the change in temperature between undamaged and damaged tissues,
    which is caused by the variation in thermal diffusion coefficients [105]. Likewise,
    CVS technologies are increasingly being used in the food industry to examine the
    quality of different kinds of food products. By using different types of CVS,
    including traditional CVS, hyperspectral CVS, and multispectral CVS to examine
    the exterior quality of food products, it is possible to identify potential issues
    early on and take corrective action to prevent further deterioration or contamination
    [106]. Electric noses (Ens) and electric tongues (Ets) are among the most promising
    inventions of AI in the food industry [107]. EN is an instrument that consists
    of an array of electronic chemical sensors with an appropriate pattern recognition
    system and partial specificity, capable of recognizing complex or simple odors
    and Ets are multisensory systems for liquid analysis based on chemical sensor
    arrays and pattern recognition [108]. The sensors used in Ens are able to collect
    data on the different smells and flavors present in the food or beverage being
    analyzed. This data is then transferred to a data center, where it can be accessed
    by ML algorithms [109]. These algorithms are able to analyze the data and make
    decisions based on the information gathered. In addition, EN technology can also
    be used to improve the overall quality of food products. By detecting subtle differences
    in the smells and flavors of different batches of food products, companies can
    make adjustments to their production processes to ensure that their products are
    consistent and of the highest quality [110]. The coffee cupping method was developed
    by the Specialty Coffee Association of America (SCAA). It is a well-established
    process for assessing the quality of roasted coffee, and it involves steeping
    ground coffee beans in boiling water and evaluating the aroma, flavor, and other
    sensory characteristics of the resulting brew. In recent years, AI technologies,
    such as Ens and ANNs, have been used to enhance the coffee cupping process. E-noses
    can be used to detect and analyze the volatile organic compounds (VOCs) that are
    responsible for the aroma of coffee. By using an EN in combination with an ANN,
    it is possible to predict the quality and flavor of roasted coffee with a high
    degree of accuracy [111]. Ets are another promising technology in the food industry
    that can be used to assess the qualities of various types of beverages, including
    dairy and alcoholic beverages. Ets can detect different taste characteristics,
    such as sweetness, saltiness, sourness, and bitterness, which can be important
    factors in determining the overall quality of a product. For example, in the case
    of tea, there are certain flavor compounds, such as theaflavin (TF) and thearubigin
    (TR), which can vary in concentration depending on the age of the tea. By using
    a pulse voltametric ET in combination with a UV-VIS spectrophotometer-based analysis,
    these compounds can be measured and used to identify the type of tea being analyzed
    [112,113]. One of the key advantages of the e-tongue is its ability to detect
    dissolved solids and volatile compounds that are responsible for the aroma and
    can give off odors after evaporation. This makes it a useful tool for analyzing
    the overall flavor profile of a sample, and its aroma [114]. The use of Ets for
    food recognition has been studied extensively, including for the differentiation
    of liquid and flesh foods. In a study by Rudnitskaya et al. [113], an ET was used
    to analyze a range of liquid and flesh food samples, including juices, wines,
    and meat products. The results of the study showed that the ET was able to successfully
    differentiate between different types of liquid and flesh foods based on their
    taste profiles. The study conducted by Tan and Xu [114] reviewed the applications
    of electronic noses (e-noses) and electronic tongues (e-tongues) in the determination
    of food quality-related properties. The study found that e-noses and e-tongues
    are increasingly being used in the food industry due to their ability to detect
    and identify various volatile and non-volatile compounds that contribute to food
    aroma and taste. 4.2. Food Safety Management Food safety ensures the absence of
    any harmful/toxic substances in it and fulfills the obligatory nutritional requirements.
    A multidisciplinary approach is necessary to ensure food safety and good hygiene
    during food processing, storage, and sale, and to eliminate the risk of biotic
    and abiotic contaminants, which causes food poisoning. Image processing techniques
    can be used to analyze various characteristics of food products, including size,
    shape, color, and texture. By estimating the projected area and perimeter of food
    items, one can quantify their size and shape, which can be useful for quality
    control purposes [115]. However, it is worth noting that image processing techniques
    alone cannot detect the presence of harmful microorganisms or other potential
    food safety hazards. That is where next-generation sequencing (NGS) comes into
    perform by the determination of the whole genome sequence of a single cultured
    isolate (e.g., a bacterial colony, a virus, or any other organism), also known
    as “whole genome sequencing” (WGS), and “metagenomics”, in which NGS is used to
    generate sequences of several microorganisms in a biological sample [11]. Furthermore,
    the use of AI and automation can greatly speed up the analysis of NGS data, allowing
    for more rapid identification of potential food safety hazards. This can help
    prevent widespread illness by allowing for quick intervention before contaminated
    products reach consumers [116]. 4.3. Food Waste Management Food waste is a significant
    issue that affects not only the environment but also food security and financial
    sustainability. It is estimated that 1.6 billion tons of food are wasted annually,
    and most of this waste (81%) is made up of inedible by-products of food production
    practices. There is a growing recognition within the food industry that food waste
    is not just an unavoidable cost of doing business but also a significant sustainability
    issue and an underutilized resource [85]. According to McKinsey & Company (New
    York, NY, USA), a consulting firm that has been at the forefront of researching
    and implementing AI in various industries, AI could offer a $127 billion opportunity
    by reducing food waste by 2030 [74]. Modern techniques, such as omics, can be
    exploited to overcome food waste reduction and management challenges. For example,
    metagenomics, proteomics, transcriptomics, waste omics, and disease omics can
    be used to understand the biochemical processes that occur during food waste decomposition
    and identify potential hazards and contaminants in food waste [10]. There have
    been various conglomerate concepts and solutions developed and tested by researchers
    and government organizations to mitigate and manage food waste issues. For instance,
    Black Soldier Fly (BSF) farming is a promising practice for its versatility and
    multi-roles in various applications, such as sustainable food production and food
    waste management. The implementation of Internet of Things (IoT) technology in
    BSF farming can offer significant benefits in terms of efficient production and
    waste management. By integrating sensors and devices with software and mobile
    applications, farmers can remotely monitor and control various parameters of BSF
    farming. This allows for more precise and customized control over the growing
    conditions of the BSF larvae, leading to higher yields and better-quality biomass
    [117,118]. 4.4. Predicting Shelf Life AI and ML techniques, including ANNs, have
    been widely applied to predict the shelf life of foods. These techniques use mathematical
    models and data from various physicochemical and sensory parameters of the food
    product to develop predictive models that can estimate the expected shelf life
    [18,119]. A research study conducted by Goyal and Goyal [120] proposed the use
    of time-delayed neural network (TDNN) models for predicting the shelf life of
    processed cheese. The study aimed to develop a model that could accurately predict
    the shelf life of processed cheese and reduce the need for time-consuming physical
    testing. The results of the study showed that the TDNN model was able to accurately
    predict the shelf life of processed cheese. AI models for shelf-life prediction
    of mangoes stored under different conditions were developed based on respiration
    rate and ripeness levels under different supply chain scenarios. A deep-CNN was
    fine-tuned on 1524 image data of mangoes that can classify the ripeness levels
    of mangoes [121]. 5. Role of AI in the Personalized Nutrition Nutrition can be
    a complex and individualized aspect of life, and what works for one person may
    not work for another. Personalized nutrition is an approach that considers an
    individual’s unique nutritional needs, preferences, and health goals. Advancements
    in technology, such as AI and ML, are enabling the development of personalized
    nutrition solutions. These solutions use data about an individual’s genetics,
    microbiome, lifestyle, and dietary habits to provide personalized nutrition recommendations
    [122]. One example of a personalized nutrition solution is a digital health company
    that offers personalized nutrition solutions for individuals and healthcare organizations,
    which uses ML to analyze an individual’s dietary habits and provide personalized
    food recommendations. Another example is a biotechnology company that offers personalized
    nutrition solutions based on an individual’s microbiome. The company uses AI and
    ML to analyze an individual’s gut microbiome and provide personalized dietary
    recommendations based on the types and amounts of gut bacteria present [123].
    Similarly, another digital health company offers a range of personalized health
    coaching solutions, including personalized nutrition coaching, weight management,
    diabetes management, and hypertension management. This company uses speech recognition
    and voice AI technologies to provide personalized health coaching to individuals.
    The company’s mobile app uses speech recognition technology to analyze an individual’s
    voice and provide personalized coaching based on their responses. One is a mobile
    app and website that allows individuals to track their daily food intake and monitor
    their nutrition goals. The app provides users with access to a database of over
    800,000 food items, allowing them to easily log their meals and track their calories,
    macronutrients, and micronutrients. Im2Calories uses a combination of computer
    vision algorithms and deep learning to analyze the visual features of food images
    and estimate the number of calories in the food [124]. 6. Conclusions and Future
    Perspectives AI has the potential to revolutionize the food and agriculture sector
    by improving efficiency, increasing productivity, and promoting sustainability.
    However, the future of AI in the food and agriculture sector also raises some
    concerns. For example, there are concerns about the potential for AI to increase
    inequality and reduce jobs in rural areas. A major constraint is the high cost
    of implementing AI systems. AI requires significant investment in hardware, software,
    and training, which can be prohibitively expensive for small and medium-sized
    businesses. Additionally, there are concerns about the reliability and accuracy
    of AI systems, particularly when it comes to making decisions about crop management
    and food safety. Smart, robotic farming and factories are just some of the ways
    in which AI and ML are being used to improve efficiency, productivity, and sustainability
    in the Agri-food industry. The future of the agriculture and food industry is
    likely to be shaped by AI and ML technologies with a range of potential applications
    across farming, pest management, food processing, packaging, quality control,
    shelf-life extension, and supply chain management. While there is a lot of potential
    for AI to revolutionize the agri-food sector, making it more efficient, sustainable,
    and innovative, it also raises important ethical, legal, and social implications
    that need to be carefully considered and addressed. It is important to ensure
    that these technologies are developed and used in a sustainable and ethical manner
    to ensure their long-term benefits. The sustainability of AI will depend on a
    range of factors, including the development and deployment of AI technologies,
    the policies and regulations that govern their use, and the way in which society
    adapts to the changes that AI brings. The sustainability of AI encompasses a range
    of environmental, social, and economic factors. There are several key considerations
    that need to be considered when it comes to the sustainability and future of AI.
    There is a need to address the skills gap and to ensure that there is a sufficient
    pool of talent to develop and deploy AI systems in a sustainable and responsible
    manner. This requires investment in education and training programs that can equip
    individuals with the skills and knowledge needed to work in the field of AI. While
    there are still challenges to be overcome, such as data privacy concerns, high
    cost, ethical issues, and the need for specialized training, the future looks
    promising for AI in this industry. As more and more farmers adopt AI-powered technologies,
    one can expect to see significant improvements in food production and distribution
    in the years to come. Future works could include a comparison of different ML
    algorithms in terms of predictive performance on operational processes in the
    agri-food sector. Author Contributions A.T., G.N., M.J., S.S. (Somesh Sharma),
    S.S. (Surabhi Sharma), A.R.J., E.R.-S., F.J.B., J.M.C., N.L. and Y.P.: conceptualization
    (equal); project administration (equal); supervision (equal); writing—original
    draft (equal); writing—review and editing (equal). All authors have read and agreed
    to the published version of the manuscript. Funding Chiang Mai University COE66.
    Data Availability Statement The data that support the findings of this study are
    available from the corresponding author upon reasonable request. Acknowledgments
    Authors thank the Center of Excellence in Materials Science and Technology, Chiang
    Mai University, for financial support under the administration of the Materials
    Science Research Center, Faculty of Science, Chiang Mai University. This research
    work was also partially supported by Chiang Mai University under the Cluster of
    Agro Bio-Circular-Green Industry. Conflicts of Interest The authors declare no
    conflict of interest. References Godfray, H.C.J.; Beddington, J.R.; Crute, I.R.;
    Haddad, L.; Lawrence, D.; Muir, J.F.; Pretty, J.; Robinson, S.; Thomas, S.M.;
    Toulmin, C. Food security: The challenge of feeding 9 billion people. Science
    2010, 327, 812–818. [Google Scholar] [CrossRef] [PubMed] Pournader, M.; Ghaderi,
    H.; Hassanzadegan, A.; Fahimnia, B. Artificial intelligence applications in supply
    chain management. Int. J. Prod. Econ. 2021, 241, 108250. [Google Scholar] [CrossRef]
    Pantazi, X.-E.; Moshou, D.; Bravo, C. Active learning system for weed species
    recognition based on hyperspectral sensing. Biosyst. Eng. 2016, 146, 193–202.
    [Google Scholar] [CrossRef] Kumar, K.; Thakur, G.S.M. Advanced applications of
    neural networks and artificial intelligence: A review. Int. J. Inf. Technol. Comput.
    Sci. 2012, 4, 57–68. [Google Scholar] [CrossRef] Saeed, W.; Omlin, C. Explainable
    AI (XAI): A systematic meta-survey of current challenges and future opportunities.
    Knowl. Based Syst. 2023, 263, 110273. [Google Scholar] [CrossRef] Miglani, A.;
    Kumar, N. Deep learning models for traffic flow prediction in autonomous vehicles:
    A review, solutions, and challenges. Veh. Commun. 2019, 20, 100184. [Google Scholar]
    [CrossRef] Goodell, J.W.; Kumar, S.; Lim, W.M.; Pattnaik, D. Artificial intelligence
    and machine learning in finance: Identifying foundations, themes, and research
    clusters from bibliometric analysis. J. Behav. Exp. Financ. 2021, 32, 100577.
    [Google Scholar] [CrossRef] Dhanaraju, M.; Chenniappan, P.; Ramalingam, K.; Pazhanivelan,
    S.; Kaliaperumal, R. Smart farming: Internet of Things (IoT)-based sustainable
    agriculture. Agriculture 2022, 12, 1745. [Google Scholar] [CrossRef] Chukkapalli,
    S.; Mittal, S.; Gupta, M.; Abdelsalam, M.; Joshi, A.; Sandhu, R.; Joshi, K. Ontologies
    and artificial intelligence systems for the cooperative smart farming ecosystem.
    IEEE Access 2020, 8, 164045–164064. [Google Scholar] [CrossRef] Sharma, P.; Vimal,
    A.; Vishvakarma, R.; Kumar, P.; de Souza Vandenberghe, L.; Kumar Gaur, V.; Varjani,
    S. Deciphering the blackbox of omics approaches and artificial intelligence in
    food waste transformation and mitigation. Int. J. Food Microbiol. 2022, 372, 109691.
    [Google Scholar] [CrossRef] Jagadeesan, B.; Gerner-Smidt, P.; Allard, M.W.; Leuillet,
    S.; Winkler, A.; Xiao, Y.; Chaffron, S.; Van Der Vossen, J.; Tang, S.; Katase,
    M.; et al. The use of next generation sequencing for improving food safety: Translation
    into practice. Food Microbiol. 2019, 79, 96–115. [Google Scholar] [CrossRef] Xiong,
    Y.; Ge, Y.; Grimstad, L.; From, P.J. An autonomous strawberry-harvesting robot:
    Design, development, integration, and field evaluation. J. Field Robot. 2020,
    37, 202–224. [Google Scholar] [CrossRef] Wang, Y.; Jin, L.; Mao, H. Farmer cooperatives’
    intention to adopt agricultural information technology—Mediating effects of attitude.
    Inf. Syst. Front. 2019, 21, 565–580. [Google Scholar] [CrossRef] Kumar, I.; Rawat,
    J.; Mohd, N.; Husain, S. Opportunities of artificial intelligence and machine
    learning in the food industry. J. Food Qual. 2021, 2021, 4535567. [Google Scholar]
    [CrossRef] Dewi, T.; Risma, P.; Oktarina, Y. Fruit sorting robot based on color
    and size for an agricultural product packaging system. Bull. Electr. Eng. Inform.
    2020, 9, 1438–1445. [Google Scholar] [CrossRef] Pérez-Gomariz, M.; López-Gómez,
    A.; Cerdán-Cartagena, F. Artificial neural networks as artificial intelligence
    technique for energy saving in refrigeration systems—A review. Clean Technol.
    2023, 5, 116–136. [Google Scholar] [CrossRef] Melesse, T.; Bollo, M.; Pasquale,
    V.; Centro, F.; Riemma, S. Machine learning-based digital twin for monitoring
    fruit quality evolution. Procedia Comput. Sci. 2022, 200, 13–20. [Google Scholar]
    [CrossRef] Phimolsiripol, Y.; Siripatrawan, U.; Cleland, D.J. Weight loss of frozen
    bread dough under isothermal and fluctuating temperature storage conditions. J.
    Food Eng. 2011, 106, 134–143. [Google Scholar] [CrossRef] Haff, R.; Toyofuku,
    N. X-ray detection of defects and contaminants in the food industry. Sens. Instrum.
    Food Qual. Saf. 2008, 2, 262–273. [Google Scholar] [CrossRef] Medus, L.; Saban,
    M.; Francés-Víllora, J.; Bataller-Mompeán, M.; Rosado-Muñoz, A. Hyperspectral
    image classification using CNN: Application to industrial food packaging. Food
    Control 2021, 125, 107962. [Google Scholar] [CrossRef] Benouis, M.; Medus, L.;
    Saban, M.; Łabiak, G.; Rosado-Muñoz, A. Food tray sealing fault detection using
    hyperspectral imaging and PCANet. IFAC Pap. 2020, 53, 7845–7850. [Google Scholar]
    [CrossRef] Sharma, S.; Patil, S. Key indicators of rice production and consumption,
    correlation between them and supply-demand prediction. Int. J. Product. Perform.
    Manag. 2015, 64, 1113–1137. [Google Scholar] [CrossRef] Sahni, V.; Srivastava,
    S.; Khan, R. Modelling techniques to improve the quality of food using artificial
    intelligence. J. Food Qual. 2021, 2021, 2140010. [Google Scholar] [CrossRef] Cheraghalipour,
    A.; Paydar, M.; Hajiaghaei-Keshteli, M. A bi-objective optimization for citrus
    closed-loop supply chain using Pareto-based algorithms. Appl. Soft Comput. 2018,
    69, 33–59. [Google Scholar] [CrossRef] Ketsripongsa, U.; Pitakaso, R.; Sethanan,
    K.; Srivarapongse, T. An improved differential evolution algorithm for crop planning
    in the Northeastern region of Thailand. Math. Comput. Appl. 2018, 23, 40. [Google
    Scholar] [CrossRef] Sharma, A.; Zanotti, P.; Musunur, L. Drive through robotics:
    Robotic automation for last mile distribution of food and essentials during pandemics.
    IEEE Access 2020, 8, 127190–127219. [Google Scholar] [CrossRef] Wardah, S.; Djatna,
    T.; Marimin, M.; Yani, M. New product development in coconut-based agro-industry:
    Current research progress and challenges. IOP Conf. Ser. Earth Environ. Sci. 2020,
    472, 012053. [Google Scholar] [CrossRef] Bo, W.; Qin, D.; Zheng, X.; Wang, Y.;
    Ding, B.; Li, Y.; Liang, G. Prediction of bitterant and sweetener using structure-taste
    relationship models based on an artificial neural network. Food Res. Int. 2022,
    153, 110974. [Google Scholar] [CrossRef] Zhang, P.; Guo, Z.; Ullah, S.; Melagraki,
    G.; Afantitis, A.; Lynch, I. Nanotechnology and artificial intelligence to enable
    sustainable and precision agriculture. Nat. Plants 2021, 7, 864–876. [Google Scholar]
    [CrossRef] Ben Ayed, R.; Hanana, M. Artificial intelligence to improve the food
    and agriculture sector. J. Food Qual. 2021, 2021, 5584754. [Google Scholar] [CrossRef]
    Subeesh, A.; Mehta, C.R. Automation and digitization of agriculture using artificial
    intelligence and internet of things. Artif. Intell. Agric. 2021, 5, 278–291. [Google
    Scholar] [CrossRef] Talaviya, T.; Shah, D.; Patel, N.; Yagnik, H.; Shah, M. Implementation
    of artificial intelligence in agriculture for optimisation of irrigation and application
    of pesticides and herbicides. Artif. Intell. Agric. 2020, 4, 58–73. [Google Scholar]
    [CrossRef] Eli-Chukwu, N.C. Applications of artificial intelligence in agriculture:
    A review. Eng. Technol. Appl. Sci. Res. 2019, 9, 4377–4383. [Google Scholar] [CrossRef]
    Toorajipour, R.; Sohrabpour, V.; Nazarpour, A.; Oghazi, P.; Fischl, M. Artificial
    intelligence in supply chain management: A systematic literature review. J. Bus.
    Res. 2021, 122, 502–517. [Google Scholar] [CrossRef] Antonucci, F.; Figorilli,
    S.; Costa, C.; Pallottino, F.; Raso, L.; Menesatti, P. A review on Blockchain
    applications in the agri-food sector. J. Sci. Food Agric. 2019, 99, 6129–6138.
    [Google Scholar] [CrossRef] Kakani, V.; Nguyen, V.H.; Kumar, B.P.; Kim, H.; Pasupuleti,
    V.R. A critical review on computer vision and artificial intelligence in food
    industry. J. Agric. Food Res. 2022, 2, 100033. [Google Scholar] [CrossRef] Abbas,
    H.M.T.; Shakoor, U.; Khan, M.J.; Ahmed, M.; Khurshid, K. Automated Sorting and
    Grading of Agricultural Products based on Image Processing. In Proceedings of
    the 2019 8th International Conference on Information and Communication Technologies
    (ICICT), Karachi, Pakistan, 16–17 November 2019; pp. 78–81. [Google Scholar] [CrossRef]
    Friedlander, A.; Zoellner, C. Artificial intelligence opportunities to improve
    food safety at retail. Food Prot. Trends 2020, 40, 272–278. [Google Scholar] Qian,
    C.; Murphy, S.I.; Orsi, R.H.; Wiedmann, M. How Can AI Help Improve Food Safety?
    Annu. Rev. Food Sci. Technol. 2022, 14, 517–538. [Google Scholar] [CrossRef] Sak,
    J.; Suchodolska, M. Artificial intelligence in nutrients science research: A review.
    Nutrients 2021, 13, 322. [Google Scholar] [CrossRef] Duncan, E.; Ashton, L.; Abdulai,
    A.; Sawadogo-Lewis, T.; King, S.; Fraser, E. Connecting the food and agriculture
    sector to nutrition interventions for improved health outcomes. Food Secur. 2022,
    14, 657–675. [Google Scholar] [CrossRef] Kolahchi, Z.; De Domenico, M.; Uddin,
    L.Q.; Cauda, V.; Grossmann, I.; Lacasa, L.; Grancini, G.; Mahmoudi, M.; Rezaei,
    N. COVID-19 and its global economic impact. Adv. Exp. Med. Biol. 2021, 1318, 825–837.
    [Google Scholar] [CrossRef] [PubMed] US FDA. Computerized Systems in Food Processing
    Industry. U.S. Food and Drug Administration. 2022. Available online: https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/computerized-systems-food-processing-industry
    (accessed on 10 March 2023). Sridhar, A.; Balakrishnan, A.; Jacob, M.M.; Sillanpää,
    M.; Dayanandan, N. Global impact of COVID-19 on agriculture: Role of sustainable
    agriculture and digital farming. Environ. Sci. Pollut. Res. 2023, 30, 42509–42525.
    [Google Scholar] [CrossRef] Kakaei, H.; Nourmoradi, H.; Bakhtiyari, S.; Jalilian,
    M.; Mirzaei, A. Effect of COVID-19 on food security, hunger, and food crisis.
    In COVID-19 and the Sustainable Development Goals; Dehghani, M.H., Karri, R.R.,
    Roy, S., Eds.; Elsevier: Amsterdam, The Netherlands, 2022; Chapter 1; pp. 3–29.
    [Google Scholar] [CrossRef] Mir, S.A.; Mir, M.B.; Shah, M.A.; Hamdani, A.M.; Sunooj,
    K.V.; Phimolsiripol, Y.; Khaneghah, A.M. New prospective approaches in controlling
    the insect infestation in stored grains. J. Asia-Pac. Entomol. 2023, 26, 102058.
    [Google Scholar] [CrossRef] Sabanci, K.; Kayabasi, A.; Toktas, A. Computer vision-based
    method for classification of wheat grains using artificial neural network. J.
    Sci. Food Agric. 2016, 97, 2588–2593. [Google Scholar] [CrossRef] [PubMed] Patrício,
    D.; Rieder, R. Computer vision and artificial intelligence in precision agriculture
    for grain crops: A systematic review. Comput. Electron. Agric. 2022, 153, 69–81.
    [Google Scholar] [CrossRef] Singh, K.; Chaudhury, S. Efficient technique for rice
    grain classification using back-propagation neural network and wavelet decomposition.
    IET Comput. Vis. 2016, 10, 780–787. [Google Scholar] [CrossRef] Zareiforoush,
    H.; Minaei, S.; Alizadeh, M.; Banakar, A. A hybrid intelligent approach based
    on computer vision and fuzzy logic for quality measurement of milled rice. Measurement
    2015, 66, 26–34. [Google Scholar] [CrossRef] Peruzzi, A.; Martelloni, L.; Frasconi,
    C.; Fontanelli, M.; Pirchio, M.; Raffaelli, M. Machines for non-chemical intra-row
    weed control in narrow and wide-row crops: A review. J. Agric. Eng. 2017, 48,
    57. [Google Scholar] [CrossRef] Bellocchio, F.; Ferrari, S.; Piuri, V.; Borghese,
    N. Hierarchical approach for multiscale support vector regression. IEEE Trans.
    Neural Netw. Learn. Syst. 2012, 23, 1448–1460. [Google Scholar] [CrossRef] Naik,
    H.; Zhang, J.; Lofquist, A.; Assefa, T.; Sarkar, S.; Ackerman, D.; Singh, A.;
    Singh, A.K.; Ganapathysubramanian, B. A real-time phenotyping framework using
    machine learning for plant stress severity rating in soybean. Plant Methods 2017,
    13, 23. [Google Scholar] [CrossRef] Guo, W.; Fukatsu, T.; Ninomiya, S. Automated
    characterization of flowering dynamics in rice using field-acquired time-series
    RGB images. Plant Methods 2015, 11, 7. [Google Scholar] [CrossRef] [PubMed] Sadeghi-Tehran,
    P.; Sabermanesh, K.; Virlet, N.; Hawkesford, M. Automated method to determine
    two critical growth stages of wheat: Heading and flowering. Front. Plant Sci.
    2017, 8, 252. [Google Scholar] [CrossRef] [PubMed] Lu, H.; Cao, Z.; Xiao, Y.;
    Fang, Z.; Zhu, Y.; Xian, K. Fine-grained maize tassel trait characterization with
    multi-view representations. Comput. Electron. Agric. 2015, 118, 143–158. [Google
    Scholar] [CrossRef] Lu, H.; Cao, Z.; Xiao, Y.; Li, Y.; Zhu, Y. Region-based colour
    modelling for joint crop and maize tassel segmentation. Biosyst. Eng. 2016, 147,
    139–150. [Google Scholar] [CrossRef] De Cesaro Júnior, T.; Rieder, R.; Di Domênico,
    J.R.; Lau, D. InsectCV: A system for insect detection in the lab from trap images.
    Ecol. Inform. 2022, 67, 101516. [Google Scholar] [CrossRef] Li, W.; Zhu, T.; Li,
    X.; Dong, J.; Liu, J. Recommending advanced deep learning models for efficient
    insect pest detection. Agriculture 2022, 12, 1065. [Google Scholar] [CrossRef]
    Gaba, S.; Gabriel, E.; Chadœuf, J.; Bonneu, F.; Bretagnolle, V. Herbicides do
    not ensure for higher wheat yield, but eliminate rare plant species. Sci. Rep.
    2016, 6, 30112. [Google Scholar] [CrossRef] Liakos, K.; Busato, P.; Moshou, D.;
    Pearson, S.; Bochtis, D. Machine learning in agriculture: A review. Sensors 2018,
    18, 2674. [Google Scholar] [CrossRef] Rejeb, A.; Rejeb, K.; Zailani, S.; Keogh,
    J.G.; Appolloni, A. Examining the interplay between artificial intelligence and
    the Agri-Food Industry. Artif. Intell. Agric. 2022, 6, 111–128. [Google Scholar]
    [CrossRef] Pantazi, X.; Tamouridou, A.; Alexandridis, T.; Lagopodi, A.; Kashefi,
    J.; Moshou, D. Evaluation of hierarchical self-organising maps for weed mapping
    using UAS multispectral imagery. Comput. Electron. Agric. 2017, 139, 224–230.
    [Google Scholar] [CrossRef] Binch, A.; Fox, C. Controlled comparison of machine
    vision algorithms for Rumex and Urtica detection in grassland. Comput. Electron.
    Agric. 2017, 140, 123–138. [Google Scholar] [CrossRef] Wu, X.; Spaeth, M.; Saile,
    M.; Peteinatos, G.G.; Gerhards, R. Precision chemical weed management strategies:
    A review and a design of a new CNN-based modular spot sprayer. Agronomy 2022,
    12, 1620. [Google Scholar] [CrossRef] Wu, X.; Aravecchia, S.; Lottes, P.; Stachniss,
    C.; Pradalier, C. Robotic weed control using automated weed and crop classification.
    J. Field Robot. 2020, 37, 322–340. [Google Scholar] [CrossRef] Christensen, S.;
    Sogaard, H.; Kudsk, P.; Norremark, M.; Lund, I.; Nadimi, E.; Jorgensen, R. Site-specific
    weed control technologies. Weed Res. 2009, 49, 233–241. [Google Scholar] [CrossRef]
    Rasmussen, J.; Griepentrog, H.; Nielsen, J.; Henriksen, C. Automated intelligent
    rotor tine cultivation and punch planting to improve the selectivity of mechanical
    intra-row weed control. Weed Res. 2012, 52, 327–337. [Google Scholar] [CrossRef]
    Rueda-Ayala, V.; Weis, M.; Keller, M.; Andújar, D.; Gerhards, R. Development and
    testing of a decision making based method to adjust automatically the Harrowing
    intensity. Sensors 2013, 13, 6254–6271. [Google Scholar] [CrossRef] Bucher, S.;
    Ikeda, K.; Broszus, B.; Gutierrez, A.; Low, A. Adaptive Robotic Chassis (ARC):
    RoboCrop a smart agricultural robot toolset. In Interdisciplinary Design Senior
    Theses; Santa Clara University: Santa Clara, CA, USA, 2021; p. 69. [Google Scholar]
    Cooper, P.; Jones, T.; Tuffy, F.; Windsor, S. Knowledge transfer and the National
    Physical Laboratory, UK. In Innovation through Knowledge Transfer: Smart Innovation,
    Systems and Technologies; Howlett, R.J., Ed.; Springer: Berlin/Heidelberg, Germany,
    2010; Volume 5, pp. 257–267. [Google Scholar] [CrossRef] Khaddour, G.; Riedel,
    I.; Andò, E.; Charrier, P.; Bésuelle, P.; Desrues, J.; Viggiani, G.; Salager,
    S. Grain-scale characterization of water retention behaviour of sand using X-ray
    CT. Acta Geotech. 2018, 13, 497–512. [Google Scholar] [CrossRef] Agboka, K.M.;
    Tonnang, H.E.Z.; Abdel-Rahman, E.M.; Odindi, J.; Mutanga, O.; Niassy, S. Data-driven
    artificial intelligence (AI) algorithms for modelling potential maize yield under
    maize–legume farming systems in East Africa. Agronomy 2022, 12, 3085. [Google
    Scholar] [CrossRef] Kovalenko, O. Machine learning and AI in food industry solutions
    and potential—SPD group blog. In Full-Cycle Software Development Solutions; SPD-Group:
    Seattle, WA, USA, 2022; Available online: https://spd.group/machine-learning/machine-learning-and-ai-in-food-industry/
    (accessed on 10 March 2023). Kamilaris, A.; Gao, F.; Prenafeta-Boldu, F.; Ali,
    M. Agri-IoT: A semantic framework for internet of things-enabled smart farming
    applications. In Proceedings of the 2016 IEEE 3rd World Forum on Internet of Things
    (WF-IoT), Reston, VA, USA, 12–14 December 2016; pp. 442–447. [Google Scholar]
    [CrossRef] Meshram, V.; Patil, K.; Meshram, V.; Hanchate, D.; Ramkteke, S.D. Machine
    learning in agriculture domain: A state-of-art survey. Artif. Intell. Life Sci.
    2021, 1, 100010. [Google Scholar] [CrossRef] Misra, N.; Dixit, Y.; Al-Mallahi,
    A.; Bhullar, M.; Upadhyay, R.; Martynenko, A. IoT, Big Data, and Artificial Intelligence
    in Agriculture and Food Industry. IEEE Internet Things J. 2022, 9, 6305–6324.
    [Google Scholar] [CrossRef] Akhtman, Y.; Golubeva, E.; Tutubalina, O.; Zimin,
    M. Application of hyperspectural images and ground data for precision farming.
    Geogr. Environ. Sustain. 2017, 10, 117–128. [Google Scholar] [CrossRef] Snyder,
    C. Enhanced nitrogen fertiliser technologies support the ‘4R’ concept to optimise
    crop production and minimise environmental losses. Soil Res. 2017, 55, 463. [Google
    Scholar] [CrossRef] Soltani Firouz, M.; Mohi-Alden, K.; Omid, M.A. Critical review
    on intelligent and active packaging in the food industry: Research and development.
    Food Res. Int. 2021, 141, 110113. [Google Scholar] [CrossRef] [PubMed] Anetta,
    B.; Joanna, W. Innovations in the food packaging market—Intelligent packaging—A
    review. Czech J. Food Sci. 2017, 35, 1–6. [Google Scholar] [CrossRef] Liu, J.
    Packaging design based on deep learning and image enhancement. Comput. Intell.
    Neurosci. 2022, 2022, 9125234. [Google Scholar] [CrossRef] Mushiri, T.; Tende,
    L. Automated grading of tomatoes using artificial intelligence: The case of Zimbabwe.
    In AI and Big Data’s Potential for Disruptive Innovation; Strydom, M., Buckley,
    S., Eds.; IGI Global: Hershey, PA, USA, 2020; pp. 216–239. [Google Scholar] [CrossRef]
    Ahmad, U.; Alvino, A.; Marino, S. Solar fertigation: A sustainable and smart IoT-based
    irrigation and fertilization system for efficient water and nutrient management.
    Agronomy 2022, 12, 1012. [Google Scholar] [CrossRef] Tavill, G. Industry challenges
    and approaches to food waste. Physiol. Behav. 2020, 223, 112993. [Google Scholar]
    [CrossRef] Cai, W.; Wang, J.; Jiang, P.; Cao, L.; Mi, G.; Zhou, Q. Application
    of sensing techniques and artificial intelligence-based methods to laser welding
    real-time monitoring: A critical review of recent literature. J. Manuf. Syst.
    2020, 57, 1–18. [Google Scholar] [CrossRef] Xiao, B.; Nguyen, M.; Yan, W.Q.; Ho,
    H. Apple ripeness identification using deep learning. In Geometry and Vision:
    First International Symposium, ISGV 2021, Auckland, New Zealand, 28–29 January
    2021, Revised Selected Papers 1; Springer: Berlin/Heidelberg, Germany, 2021; Volume
    1386, pp. 53–67. [Google Scholar] [CrossRef] Pizzaia, J.P.L.; Salcides, I.R.;
    de Almeida, G.M.; Contarato, R.; de Almeida, R. Arabica coffee samples classification
    using a Multilayer Perceptron neural network. In Proceedings of the 13th IEEE
    International Conference on Industry Applications (INDUSCON), Sao Paulo, Brazil,
    12–14 November 2018; pp. 80–84. [Google Scholar] [CrossRef] Papadopoulos, E.;
    Gonzalez, F. UAV and AI application for runway Foreign Object Debris (FOD) detection.
    In Proceedings of the IEEE Aerospace Conference (50100), Big Sky, MT, USA, 6–13
    March 2021. [Google Scholar] [CrossRef] Shimonomura, K.; Chang, T.; Murata, T.
    Detection of foreign bodies in soft foods employing tactile image sensor. Front.
    Robot. AI 2021, 8, 774080. [Google Scholar] [CrossRef] Rahman, W. Why Coca Cola
    Uses AI to Create Intelligent Vending Machines. Available online: https://towardsdatascience.com/why-coca-cola-uses-ai-to-create-intelligent-vending-machines-ae97ce952082
    (accessed on 10 March 2023). Ferrer, B. Nuritas Links Up with Healthgevity to
    Tackle Aging with AI-Based Peptide Solution. Available online: https://www.personalcareinsights.com/news/nurital-links-up-with-healthgevity-to-tackle-aging-with-ai-based-peptide-solution.html
    (accessed on 10 March 2023). Takahashi, D. Hello Egg Is An AI-Based Meal-Planning
    and Cooking Gadget. Available online: https://venturebeat.com/business/hello-egg-is-ai-based-meal-planning-and-cooking-gadget/
    (accessed on 10 March 2023). Trencher, G. Towards the smart city 2.0: Empirical
    evidence of using smartness as a tool for tackling social challenges. Technol.
    Forecast. Soc. Chang. 2019, 142, 117–128. [Google Scholar] [CrossRef] Shankar,
    V. How Artificial Intelligence (AI) is reshaping retailing. J. Retail. 2018, 94,
    vi–xi. [Google Scholar] [CrossRef] Yoo, H.; Park, D. AI-based 3D food printing
    using standard composite materials. Stud. Comput. Intell. 2021, 929, 123–135.
    [Google Scholar] [CrossRef] Monteiro, J.; Barata, J. Artificial Intelligence in
    extended agri-food supply chain: A short review based on bibliometric analysis.
    Procedia Comput. Sci. 2021, 192, 3020–3029. [Google Scholar] [CrossRef] Bedoya,
    M.; Montoya, D.; Tabilo-Munizaga, G.; Pérez-Won, M.; Lemus-Mondaca, R. Promising
    perspectives on novel protein food sources combining artificial intelligence and
    3D food printing for food industry. Trends Food Sci. Technol. 2022, 128, 38–52.
    [Google Scholar] [CrossRef] Wilms, P.; Daffner, K.; Kern, C.; Gras, S.L.; Schutyser,
    M.A.I.; Kohlus, R. Formulation engineering of food systems for 3D-printing applications—A
    review. Food Res. Int. 2021, 148, 110585. [Google Scholar] [CrossRef] Li, G.;
    Hu, L.; Liu, J.; Huang, J.; Yuan, C.; Takaki, K.; Hu, Y. A review on 3D printable
    food materials: Types and development trends. Int. J. Food Sci. Technol. 2022,
    57, 164–172. [Google Scholar] [CrossRef] Nachal, N.; Moses, J.; Karthik, P.; Anandharamakrishnan,
    C. Applications of 3D printing in food processing. Food Eng. Rev. 2019, 11, 123–141.
    [Google Scholar] [CrossRef] Taneja, A.; Sharma, R.; Ayush, K.; Sharma, A.; Khaneghah,
    A.M.; Regenstein, J.M.; Barba, F.J.; Phimolsiripol, Y.; Sharma, S. Innovations
    and applications of 3-D printing in food sector. Int. J. Food Sci. Technol. 2022,
    57, 3326–3332. [Google Scholar] [CrossRef] Mavani, N.R.; Ali, J.M.; Othman, S.;
    Hussain, M.A.; Hashim, H.; Rahman, N.A. Application of artificial intelligence
    in food industry—A guideline. Food Eng. Rev. 2022, 14, 134–175. [Google Scholar]
    [CrossRef] Hussein, Z.; Fawole, O.; Opara, U. Harvest and postharvest factors
    affecting bruise damage of fresh fruits. Hortic. Plant J. 2020, 6, 1–13. [Google
    Scholar] [CrossRef] Gowen, A.; Tiwari, B.; Cullen, P.; McDonnell, K.; O’Donnell,
    C. Applications of thermal imaging in food quality and safety assessment. Trends
    Food Sci. Technol. 2010, 21, 190–200. [Google Scholar] [CrossRef] Addanki, M.;
    Patra, P.; Kandra, P. Recent advances and applications of artificial intelligence
    and related technologies in the food industry. Appl. Food Res. 2022, 2, 100126.
    [Google Scholar] [CrossRef] Jiang, H.; Zhang, M.; Bhandari, B.; Adhikari, B. Application
    of electronic tongue for fresh foods quality evaluation: A review. Food Rev. Int.
    2018, 34, 746–769. [Google Scholar] [CrossRef] Calvini, R.; Pigani, L. Toward
    the development of combined artificial sensing systems for food quality evaluation:
    A review on the application of data fusion of electronic noses, electronic tongues
    and electronic eyes. Sensors 2022, 22, 577. [Google Scholar] [CrossRef] [PubMed]
    Thazin, Y.; Pobkrut, T.; Kerdcharoen, T. Prediction of acidity levels of fresh
    roasted coffees using e-nose and artificial neural network. In Proceedings of
    the 10th International Conference on Knowledge and Smart Technology (KST), Chiang
    Mai, Thailand, 31 January–3 February 2018; Volume 1, pp. 210–215. [Google Scholar]
    [CrossRef] Jambrak, A.; Šimunek, M.; Petrović, M.; Bedić, H.; Herceg, Z.; Juretić,
    H. Aromatic profile and sensory characterisation of ultrasound treated cranberry
    juice and nectar. Ultrason. Sonochem. 2017, 38, 783–793. [Google Scholar] [CrossRef]
    [PubMed] Ullah, A.; Liu, Y.; Wang, Y.; Gao, H.; Wang, H.; Zhang, J.; Li, G. E-Taste:
    Taste sensations and flavors based on tongue’s electrical and thermal stimulation.
    Sensors 2022, 22, 4976. [Google Scholar] [CrossRef] Deisingh, A.K.; Stone, D.C.;
    Thompson, M. Applications of electronic noses and tongues in food analysis. Int.
    J. Food Sci. Technol. 2004, 39, 587–604. [Google Scholar] [CrossRef] Rudnitskaya,
    A.L.A.; Seleznev, B.; Vlasov, Y. Recognition of liquid and flesh food using an
    electronic tongue’. Int. J. Food Sci. Technol. 2002, 37, 375–385. [Google Scholar]
    [CrossRef] Tan, J.; Xu, J. Applications of electronic nose (e-nose) and electronic
    tongue (e-tongue) in food quality-related properties determination: A review.
    Artif. Intell. Agric. 2020, 4, 104–115. [Google Scholar] [CrossRef] Zhu, X.; Yuan,
    X.; Zhang, Y.; Liu, H.; Wang, J.; Sun, B. The global concern of food security
    during the COVID-19 pandemic: Impacts and perspectives on food security. Food
    Chem. 2022, 370, 130830. [Google Scholar] [CrossRef] Kondakci, T.; Zhou, W. Recent
    applications of advanced control techniques in food industry. Food Bioproc. Technol.
    2016, 10, 522–542. [Google Scholar] [CrossRef] Van Wynsberghe, A. Sustainable
    AI: AI for sustainability and the sustainability of AI. AI Ethics 2021, 1, 213–218.
    [Google Scholar] [CrossRef] Hassoun, A.; Prieto, M.A.; Carpena, M.; Bouzembrak,
    Y.; Marvin, H.J.P.; Pallarés, N.; Barba, F.J.; Bangar, S.P.; Chaudhary, V.; Ibrahim,
    S.; et al. Exploring the role of green and industry 4.0 technologies in achieving
    sustainable development goals in food sectors. Food Res. Int. 2022, 162, 112068.
    [Google Scholar] [CrossRef] Ferreira, C.; Gonçalves, G. Remaining useful life
    prediction and challenges: A literature review on the use of machine learning
    methods. J. Manuf. Syst. 2022, 63, 550–562. [Google Scholar] [CrossRef] Goyal,
    S.; Goyal, G.K. Time—Delay simulated artificial neural network models for predicting
    shelf life of processed cheese. Int. J. Intell. Syst. Appl. 2012, 4, 30–37. [Google
    Scholar] [CrossRef] Dutta, J.; Deshpande, P.; Rai, B. AI-based soft-sensor for
    shelf life prediction of ‘Kesar’ mango. SN Appl. Sci. 2021, 3, 657. [Google Scholar]
    [CrossRef] Verma, M.; Hontecillas, R.; Tubau-Juni, N.; Abedi, V.; Bassaganya-Riera,
    J. Challenges in personalized nutrition and health. Front Nutr. 2018, 29, 117.
    [Google Scholar] [CrossRef] King, J. Viome Launches World’s First at-Home Service
    to Measure and Improve Immunity, Inflammation, Gut Health and Aging. 2022. Available
    online: https://apnews.com/article/science-technology-health-business-aging-4901af98bfd5ae5769d5a78428768d84
    (accessed on 10 March 2023). Ma, T.; Wang, H.; Wei, M.; Lan, T.; Wang, J.; Bao,
    S.; Ge, Q.; Fang, Y.; Sun, X. Application of smart-phone use in rapid food detection,
    food traceability systems, and personalized diet guidance, making our diet more
    health. Food Res. Int. 2022, 152, 110918. [Google Scholar] [CrossRef]              Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Taneja, A.; Nair, G.; Joshi, M.; Sharma, S.;
    Sharma, S.; Jambrak, A.R.; Roselló-Soto, E.; Barba, F.J.; Castagnini, J.M.; Leksawasdi,
    N.; et al. Artificial Intelligence: Implications for the Agri-Food Sector. Agronomy
    2023, 13, 1397. https://doi.org/10.3390/agronomy13051397 AMA Style Taneja A, Nair
    G, Joshi M, Sharma S, Sharma S, Jambrak AR, Roselló-Soto E, Barba FJ, Castagnini
    JM, Leksawasdi N, et al. Artificial Intelligence: Implications for the Agri-Food
    Sector. Agronomy. 2023; 13(5):1397. https://doi.org/10.3390/agronomy13051397 Chicago/Turabian
    Style Taneja, Akriti, Gayathri Nair, Manisha Joshi, Somesh Sharma, Surabhi Sharma,
    Anet Rezek Jambrak, Elena Roselló-Soto, Francisco J. Barba, Juan M. Castagnini,
    Noppol Leksawasdi, and et al. 2023. \"Artificial Intelligence: Implications for
    the Agri-Food Sector\" Agronomy 13, no. 5: 1397. https://doi.org/10.3390/agronomy13051397
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   9
    Scopus   9 Web of Science   4 Google Scholar   [click to view] Article Access
    Statistics Article access statistics Article Views 8. Jan 18. Jan 28. Jan 7. Feb
    17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0k 10k 2.5k 5k 7.5k For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Agronomy, EISSN 2073-4395, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: '>'
  journal: Agronomy
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Artificial Intelligence: Implications for the Agri-Food Sector'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Andrade R.
  - Ramires T.
  citation_count: '0'
  description: Sugarcane cultivation has been concentrated in several countries due
    to its diversity of use, such as in fuel, sugar, as well as other areas. Among
    the 80 largest sugarcane producers, Brazil occupies the first place, representing
    22% of world production in the 2020/2021 harvest. The modernization of agriculture,
    called agriculture 4.0, has allowed greater productivity, which are directly affected
    by the invasion of weeds. A survey presented by [1] shows that the invasion of
    Brachiaria decumbens and Panicum (weed varieties) were responsible for the loss
    of 40% of the sugarcane production. Integrated weed management, which includes
    constant mapping in a crop and the appropriate choice of control strategies, can
    be achieved through a better understanding of the structure and production system
    in relation to the behaviour of weeds in the field, as well as the optimization
    of its control. The adoption of the soil mapping method in the regular network
    allows producers, who use the localized application of fertilizers and herbicides,
    to make agribusiness more competitive and efficient in agricultural management
    and in increasing productivity [2]. In a study carried out by [3] it was observed
    that with the application of targeted herbicide (punctually) in beet, corn, wheat
    and others cultivars, it was possible to obtain a reduction from 6 to 81% in applications
    directed to weeds of broad-leaved and a 20 to 79% reduction in applications targeting
    narrow-leaf weeds. In this survey, we propose a supervised machine learning model,
    which was able to identify weed invasion in a sugarcane cultivar, using four colour
    spectra as input variables, being NIR, RE, R and G, which were obtained by a multispectral
    camera adapted to an unmanned aerial vehicle. The model used to predict weed infestation
    was Random Forest, which was validated using cross-validation techniques, such
    as the k-fold method. With the exact identification of the infestation, it was
    possible to carry out the management in the field with applications of herbicides
    precisely, thus avoiding the increase in the cost of production as well avoiding
    the use of herbicides in unnecessary places. Results of this survey shows that,
    without the precision agriculture techniques, spraying was carried out in 100%
    of the field; however, it was necessary to be applied in only 39.6% of the total
    cultivated area, based on the infestation level obtained by the model. Finally,
    the model was estimated using the randomForest package [4] of the R software,
    which presented an accuracy of 97%.
  doi: 10.11159/icsta22.152
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: '>'
  journal: Proceedings of the International Conference on Statistics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Precision Agriculture: Herbicide Reduction with AI Models'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 25 papers. The topics discussed include: classification
    of coffee bean varieties based on a deep learning approach; machine learning-based
    predictions of form accuracy for curved thin glass by vacuum assisted hot forming
    process; failure analysis of a smart sensor node for precision agriculture; energy
    distribution on surge arrester elements selected by genetic algorithm in railway
    systems; based on deep convolutional neural network and machine vision applied
    to the surface defect detection of hard disk metal gaskets; deterioration score
    of cold forging dies by using acoustic emission signals; adaptive methods for
    fault detection on research engine test beds; reliability estimation of inertial
    measurement units using accelerated life test; an analysis of block sizes for
    compressive sensing reconstruction applied in image processing optimization; and
    metrological comparison between instruments for status monitoring of buildings
    after earthquake.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 18th IMEKO TC10 Conference on Measurement for Diagnostic, Optimisation
    and Control to Saupport Sustainability and Resilience 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 18th IMEKO TC10 Conference on Measurement for Diagnostic, Optimisation and
    Control to Support Sustainability and Resilience 2022
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Dolata P.
  - Wróblewski P.
  - Mrzygłód M.
  - Reiner J.
  citation_count: '10'
  description: Modern agriculture is based on control and optimization, where monitoring
    is essential. But yield monitoring limited to spatial mapping of biomass is unsatisfactory
    for uniformity optimization. Acquiring this information as soon as during the
    harvest could improve efficiency of precision agriculture. Current machine vision
    solutions do not allow this for several reasons, starting with difficulties with
    recognition of heavily cluttered, often mutually occluding objects, or requirements
    for elaborate hardware. Acquiring images in on-line conditions often results in
    perspective distortion which makes measurement difficult. This is even more challenging
    in the case of root crops, as they are usually graded basing on their minimal
    diameter, which is not directly observable via monocular vision. We propose a
    method for developing yield monitoring systems that can estimate physical dimensions
    of crops directly on-line. In our approach, individual plants are segmented using
    Mask R-CNN, allowing clear separation even in heavily cluttered conditions and
    in the presence of occlusions. Then, a nonlinear regression model is used to predict
    the minimal diameter of each plant basing on their observed contours. This model
    is capable of jointly correcting the perspective distortion and estimate the non-observed
    dimension. Training this model requires per-object annotations with their true
    diameters. Since this information is very difficult to acquire for real crops,
    we build a simulation environment where we model the crops, eventually rendering
    a synthetic dataset to train the regression model on. The size estimation model
    transfers the knowledge from simulated data, correctly predicting the sizes of
    real plants. We demonstrate our method on the case of potatoes, but it can be
    applied to other crops as well.
  doi: 10.1016/j.compag.2021.106451
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract BetaPowered by GenAIQuestions answered
    in this article Keywords 1. Introduction 2. Materials and methods 3. Results 4.
    Discussion 5. Conclusions Declaration of Competing Interest Acknowledgements References
    Show full outline Cited by (14) Figures (12) Show 6 more figures Tables (4) Table
    1 Table 2 Table 3 Table 4 Computers and Electronics in Agriculture Volume 190,
    November 2021, 106451 Original papers Instance segmentation of root crops and
    simulation-based learning to estimate their physical dimensions for on-line machine
    vision yield monitoring Author links open overlay panel Przemysław Dolata a, Paweł
    Wróblewski a, Mariusz Mrzygłód a, Jacek Reiner a Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.compag.2021.106451 Get rights and content Highlights
    • Methodology for designing on-line yield estimation systems for root crops. •
    Instance segmentation of crowded objects in the presence of occlusions. • Size
    estimation under heavy perspective distortion using nonlinear regression. • Simulation
    environment for synthesizing data to train the regression model. Abstract Modern
    agriculture is based on control and optimization, where monitoring is essential.
    But yield monitoring limited to spatial mapping of biomass is unsatisfactory for
    uniformity optimization. Acquiring this information as soon as during the harvest
    could improve efficiency of precision agriculture. Current machine vision solutions
    do not allow this for several reasons, starting with difficulties with recognition
    of heavily cluttered, often mutually occluding objects, or requirements for elaborate
    hardware. Acquiring images in on-line conditions often results in perspective
    distortion which makes measurement difficult. This is even more challenging in
    the case of root crops, as they are usually graded basing on their minimal diameter,
    which is not directly observable via monocular vision. We propose a method for
    developing yield monitoring systems that can estimate physical dimensions of crops
    directly on-line. In our approach, individual plants are segmented using Mask
    R-CNN, allowing clear separation even in heavily cluttered conditions and in the
    presence of occlusions. Then, a nonlinear regression model is used to predict
    the minimal diameter of each plant basing on their observed contours. This model
    is capable of jointly correcting the perspective distortion and estimate the non-observed
    dimension. Training this model requires per-object annotations with their true
    diameters. Since this information is very difficult to acquire for real crops,
    we build a simulation environment where we model the crops, eventually rendering
    a synthetic dataset to train the regression model on. The size estimation model
    transfers the knowledge from simulated data, correctly predicting the sizes of
    real plants. We demonstrate our method on the case of potatoes, but it can be
    applied to other crops as well. Graphical abstract Download : Download high-res
    image (100KB) Download : Download full-size image Previous article in issue Next
    article in issue Questions answered in this article BetaPowered by GenAI This
    is generative AI content and the quality may vary. Learn more. Why is reasoning
    about crops using machine vision challenging? What is the challenge in analyzing
    the acquired images of crops? What is the challenge in distinguishing between
    individual instances of root crops? When were quality inspection and grading solutions
    for agriculture based on machine vision first known? What is meant by instance
    segmentation? Keywords Yield estimationMachine visionMachine learningImage segmentationSynthetic
    dataset 1. Introduction 1.1. Background Yield monitoring is an important area
    of agricultural research and practice. Information on crop growth, quality, and
    yield constitutes vital feedback for planning fertilization and seeding strategies.
    In other uses, the yield information can be a guiding factor in research. Machine
    vision-based methods of yield monitoring are receiving increasing attention, because
    of their potential for easy integration with existing harvesting machines – without
    the need of significant hardware modifications. In some applications, simply counting
    the produce or estimating its total mass is not enough. Root crops, for example,
    are commercially graded basing on the physical dimensions of individual tubers,
    among other factors. Plants with different intended purpose must meet different
    sets of standard criteria. Ensuring uniformity of produce quality within a batch
    is important. Therefore, acquiring the size information sooner, preferably during
    the harvest, could improve the efficiency of the harvest. Harvester-based imaging
    solutions must overcome several technical difficulties that are not present in
    a laboratory setting. Hardware conditions limit the possibilities for installation
    of imaging sensors and additional illumination. Environmental conditions such
    as sunshine, dust or debris make sensing with complex devices, such as time-of-flight
    cameras or laser scanners, either difficult or infeasible. On the application
    side, relying on such expensive laboratory equipment will likely inhibit adoption
    of such systems in agricultural practice. Some information about the crops can
    be, however, extracted from images acquired using simple RGB cameras. Mounting
    such a simple system directly on the harvester is simple, but the real challenge
    concerns analyzing the acquired images. Due to the nature of the harvesting process
    and the way crops are transported on the harvester transmission belt, they often
    resemble a stream of dense mass rather than a series of separate objects. While
    it is relatively straightforward to distinguish between the mass of crops and
    the background, lack of clear boundaries between the crops makes it difficult
    to recognize individual objects. Performing two-dimensional per-instance measurement
    necessitates not only recognizing them (predicting bounding boxes around the objects
    of interest – object detection) but also extracting detailed contours of the touching
    objects (instance segmentation). Moreover, imaging a heavily cluttered scene using
    a single sensor means that a complete three-dimensional representation of the
    detected objects cannot be reconstructed. Consequently, if a measurement of some
    particular dimension is desired – as is the case with potatoes, for instance,
    where the minimal diameter is of interest – the required information is not directly
    available. Another difficulty is related to perspective, which distorts the perception
    of the scene. When imaging three-dimensional objects, the typical simple approach
    to perspective correction – based on camera calibration and affine transformation
    – fails. In these cases, the often-followed practice is to install the imaging
    system directly overhead the scene. However, such a setting might not allow measurement
    of the minimal diameter of oblate objects that align themselves flat on the imaging
    plane. Viewing the scene at an inclined angle can increase the probability of
    capturing of the minimal diameter but increases the effect of perspective distortion
    and occlusion. It means that when the objects appear on the scene in a dense stream
    over a flat surface, imaging at an angle will cause the objects located closer
    to the sensor to occlude those located further. This is an important factor, because
    measuring only the directly visible parts of the interesting objects will necessarily
    result in underestimating their sizes. 1.2. Related work Reasoning about crops
    using machine vision is often challenging due to the inherent variability of plant
    products, owing to their biological nature. There exists a large body of knowledge
    in this area, with varying degree of complexity of the specific problem approached.
    The general trend is that more information can be extracted from images acquired
    in very good conditions, while field-based imaging systems usually only allow
    a more basic type of analysis, such as counting of fruits. Most of the works on
    the subject of precise crop characterization are done in idealized conditions,
    often using specialized hardware. For example (Khojastehnazhand et al., 2019)
    describe a system for volume estimation and ripeness classification of apricots,
    in which single fruits are placed on a highly contrasting background and imaged
    using a custom system comprising a CCD camera and LED illumination installed in
    an enclosed chamber. Similar machine vision measurement systems are shown in (Momin
    et al., 2017) for mango fruits and in (Zhang et al., 2018) for betel nuts. Quality
    inspection and grading solutions for agriculture basing on machine vision were
    known at least since (Noordam et al., 2000) who used line-scan cameras and mirrors
    to obtain images of whole potatoes at high speed. Many current studies are also
    based on custom imaging systems, often utilizing more advanced sensors to retrieve
    information about the objects. (Kusumam et al., 2017) built a tractor attachment
    housing an RGB-D camera and LED illuminators, allowing not only to identify and
    locate broccoli heads in the field but also to estimate their size. In a more
    controlled environment, (Wang et al., 2016) demonstrated the use of laser triangulation
    to reconstruct the surface of diced potatoes in order to measure and grade them.
    Other examples include (ElMasry et al., 2012, Su et al., 2018, Long et al., 2018),
    but commercial solutions are also available for post-harvest grading and sorting,
    similarly based on purpose-built imaging systems for highly detailed analysis
    – e.g. Ellips TrueSort. The approaches basing on specialized imaging solutions
    cannot however be easily deployed in field conditions due to the risk of damage
    or the presence of dust, adversely influencing the highly sensitive equipment.
    That is why some research has been devoted to applying simple and inexpensive
    hardware to solve grading, measurement or quality inspection problems. However,
    when only simple cameras with no additional illumination are available, the identification
    of objects of interest becomes difficult. Thus, some works assume very good imaging
    conditions. For example, (Pandey et al., 2019) present an approach to grading
    and defect detection in potatoes but assume that their source material is easily
    distinguishable against the contrasting background and there are no significant
    occlusions between the objects. (Hofstee and Molema, 2003) managed to estimate
    volume of potatoes, dealing with separating clusters of objects in greater detail,
    but again omitting the problem of segmentation or occlusions. (Razmjooy et al.,
    2012) also perform geometric measurements in the case when the objects are imaged
    in separation and highly contrast with the background. Field conditions incur
    limitations not only on the hardware one can deploy, but also when each plant
    species can be imaged. Pre-harvest observations of fruit trees in order to estimate
    crop load are common, and the typical problem encountered in this area is segmentation
    of the fruits or other interesting parts of the plant against the stems and leaves
    (Lin et al., 2020). In another study, (Lee et al., 2018) distinguish potatoes
    from the underlying soil in order to estimate their mass. However, this problem
    is rarely encountered in practice, as potatoes are not naturally observed in such
    conditions. Other works related to detecting and counting crops in the field include
    also e.g. (Chen et al., 2017). Many of above works start with a certain other
    assumption that simplifies image processing: that the objects rarely touch or
    overlap one another. With some crops however this is not the case: for example,
    many root crops. Because they are not visible before the harvest, the first opportunity
    to image them is during that process. However, when moving through the harvester
    pipeline, the objects tend to clutter, and distinguishing between individual instances
    becomes a challenge. This is shown in (Boatswain Jacques et al., 2018) who installed
    an RGB camera on an onion harvester to count shallot onions. While their solution
    for segmentation of onions against the harvester hardware works fairly well, they
    struggled with identifying the heavily crowded objects. Similar difficulties were
    encountered by (Chinchuluun et al., 2009) with counting of citrus fruits, despite
    integrating a custom housing containing a camera and illumination with the harvester.
    Recent years and the success of convolutional neural networks (CNNs) brought solutions
    based on deep learning to precision agriculture. These methods rely on the concept
    of feature learning, as opposed to extracting a series of predefined features,
    e.g. using MaZda (Szczypiński et al., 2009). Approaches based on deep learning
    are often applied in the area of fruit detection and counting. (Bargoti and Underwood,
    2017) developed an apple counting system based on a CNN with a custom solution
    for separating the touching objects. It should be noted however, that the apples
    in their images are not as heavily cluttered as e.g. onions on a harvester. Similar
    approach with a similar assumption about the object separation can be seen in
    (Kestur et al., 2019). (Habaragamuwa et al., 2018) approach a much more difficult
    problem of detecting strawberries that often occlude one another. They employ
    a region-based convolutional neural network (R-CNN) that returns a bounding box
    for each detected object, allowing to separate even partially occluded fruits.
    (Yu et al., 2019) take strawberry detection a step further and segment individual
    objects using Mask R-CNN. (Sa et al., 2016) propose a detection framework for
    fruits based on Faster R-CNN, called DeepFruit, that is simple to apply for other
    species. Those approaches are almost universally applicable only to the problems
    of detection, which allows counting of produce, but provide limited information
    on their size. When a more precise measurement is desired, CNN-based approaches
    are not always employed. For example, (Si et al., 2018) developed their own algorithm
    to acquire information on the shape of potatoes moving on a conveyor belt at a
    high speed, accounting for a considerable motion blur resulting from using a simple
    imaging system with no additional illumination. When CNNs are used to detect objects
    in order to perform geometric measurements, as for example done by (Yu et al.,
    2020) in the case of fish, the imaging conditions are selected to facilitate simple
    measurement. In this and above works, the small scale of the imaging scene and
    the right angle between the optical axis and the imaging plane ensure that perspective
    only negligibly influences the measurements. Calibration in such cases can be
    conducted simply by computing a constant scale factor between the image space
    and the physical space. For a broader context on machine vision in agriculture,
    one can consult the surveys (Gongal et al., 2015, Mavridou et al., December 2019).
    Among the studies done on yield monitoring using machine vision, all works that
    we are aware of base on at least one of the simplifying assumptions: either employ
    specialized and expensive hardware to acquire images, or perform the imaging in
    idealized conditions, or the imaged plants are mostly well-separated, or the objects
    are not measured in detail but only detected. To the best of our knowledge, there
    are no approaches to yield monitoring performed in the field conditions (on a
    harvester) with minimally invasive hardware, able to provide size information
    on the highly cluttered crops. 1.3. Aims We propose a method for counting and
    estimating the size of root crops during harvest basing on images acquired using
    a simple RGB camera mounted directly on the harvester, without any additional
    hardware. The approach consists of two steps: object detection and size estimation.
    We perform the object detection and instance segmentation using a convolutional
    neural network, specifically a Mask R-CNN. This framework allows good detection
    rate even in difficult conditions and can also detect and segment objects that
    are occluded to some extent. We propose a nonlinear regression model for estimation
    of the physical size of the object. Since calibration of this model requires reference
    data, which is not available for the real-world problem, we construct a simulation
    environment in order to generate it. Creating a three-dimensional model of the
    crop allows us to render a synthetic dataset for calibration of the regression
    model. The proposed method is independent of the particular crop, but its particular
    implementation is crop-specific; therefore, we demonstrate the results for the
    case of potato (Solanum tuberosum). 2. Materials and methods 2.1. Outline of the
    method Our proposed method consists of several steps. In order to estimate physical
    dimensions of objects from images, first these objects need to be detected and
    individually segmented. We employ instance segmentation techniques basing on convolutional
    neural networks (CNN) for this purpose. In the next step, the physical dimensions
    of the objects need to be estimated from their image-space contours. To solve
    this, we implement a neural-network-based regression model. However, this model
    requires a constant-length representation of the input. Contours obtained in the
    instance segmentation step are variable-length, therefore we approximate them
    using ellipse fitting. Fig. 1 (blue pathway) summarily illustrates the concept.
    Download : Download high-res image (158KB) Download : Download full-size image
    Fig. 1. Outline of the proposed approach. Both crucial elements of the system
    are basing on machine learning, therefore annotated data is needed to train them.
    We train the instance segmentation model using manually annotated images of real
    potatoes, acquired in field conditions. However, the same is not possible for
    the physical dimension regressor, as it is infeasible to directly measure every
    single potato tuber. To overcome this challenge, we propose a simulation environment
    for rendering “synthetic” images of potatoes, equipped both with segmentation
    masks as well as exact physical dimensions of the displayed objects. The potatoes
    are modelled to closely match the real tubers, according to some statistical distribution
    describing their shape. This way the image-space contours can be matched with
    the corresponding physical-space dimensions, enabling training of the regression
    model (green pathway on Fig. 1). 2.2. Image acquisition We acquire images of potatoes
    on a harvester in the field conditions using a simple 4 megapixels RGB camera
    (RunCam 2, RunCam Technology Co.) housed in a custom 3D-printed case with integrated
    microcontroller and GPS receiver. The imaging system was fastened to the frame
    of the harvester, as shown in Fig. 2. To calibrate the images, we applied a standard
    checkerboard-pattern calibration routine. The complete system was configured to
    acquire images and geolocation metadata at a rate of approximately image every
    seconds, storing them on an SD card. Download : Download high-res image (523KB)
    Download : Download full-size image Fig. 2. Field experiment. Image acquisition
    system deployed during harvest (left); close-up on the system enclosed in a pink
    3D-printed case (right). Images undergo no specific preprocessing before being
    fed to the instance segmentation CNN. The only additional work beyond acquisition
    is manual annotation of the images. A custom in-house annotation software is used
    to draw contours around every object, including the outlines of their occluded
    fragments. 2.3. Instance segmentation Our approach to instance segmentation is
    based on a convolutional neural network (CNN), following the Mask R-CNN framework
    (He et al., 2017). Although not dedicated for dealing with heavy occlusions, we
    found it to be sufficiently capable of segmenting moderately overlapping objects.
    For details of the architecture, we refer the reader to the original paper. We
    make no structural changes to the default architecture, using the implementation
    provided in Detectron 2 toolbox (Wu et al., 2019). This toolbox is built on top
    of PyTorch framework (Paszke et al., 2017), which we also base our research code
    on. The specific architecture used throughout this paper follows the built-in
    model mask_rcnn_R_50_FPN_3x – using ResNet-50 (He et al., 2016) as the feature
    extraction backbone. To initialize our models, we use pre-trained weights from
    the Detectron 2 model repository (“model zoo”): ResNet-50 feature extraction backbone
    has been pre-trained on the ImageNet-1k, while the instance segmentation model
    on the MS COCO dataset. We train our instance segmentation models using the standard
    Stochastic Gradient Descent (SGD) optimization algorithm. To make reproduction
    of our results easier, we leave all of the hyperparameters at their default values
    except where noted. In order to artificially increase the dataset size and improve
    robustness of the model, we apply basic data augmentations during training. In
    order to preserve all of the contextual information in our images, we refrain
    from using highly distorting augmentations such as elastic deformations. Instead,
    we only apply random horizontal and vertical flips and random scaling (from the
    input crop of 800 pixels to a random size between 700 and 1000 pixels). Quality
    evaluation of instance segmentation algorithms is often performed using the Panoptic
    Quality measure (PQ) (Kirillov et al., 2019). PQ measure multiplicatively combines
    two separate terms: Recognition Quality (RQ), which is an F1-score of correctly/incorrectly
    predicted bounding boxes, and Segmentation Quality (SQ), expressed in terms of
    the average intersection-over-union (IoU) over all correctly detected objects.
    The metric is given by the following formula: (1) in which the first element constitutes
    RQ and the second – SQ. Whenever we report the overall PQ metric, we also give
    the values of its components, as they carry different information. Low RQ means
    that the detection system fails to recognize all of the objects present in the
    image or outputs false positives, which are penalized equally with false negatives.
    Low SQ informs that the shapes of the correctly recognized objects are not properly
    predicted. The latter will influence the size estimation quality. Since the PQ
    measure as implemented in Detectron 2 assumes that all detected objects are separate,
    we implement a custom version of the metric to allow measuring overlapping objects
    as well. 2.4. Learning to estimate the physical dimensions 2.4.1. Geometry of
    the imaging system Instance segmentation provides information on each visible
    (detected) object as a contour in the image space. The task of estimating the
    true dimensions of the corresponding object in the physical space is nontrivial
    for two reasons. First, the images of objects are significantly distorted by the
    perspective projection. Simple correction via perspective transformation allows
    performing accurate geometrical measurements only when the measured objects lie
    on the imaged plane. Objects that protrude from the imaged plane will be unnaturally
    distorted by this transformation, making their measurements incorrect. This idea,
    in a one-dimensional simplification, is illustrated in Fig. 3. Second problem
    lies in the fact that in order to grade root crops, the minimum diameter needs
    to be obtained. Due to the behavior of the physical objects on the transporter,
    this diameter is not oriented with respect to other dimensions in any predictable
    way, making its direct measurement impossible. The information obtained by imaging
    is the diameter in the image space, , expressed in pixels. Using a simple perspective
    transformation to the imaged plane would yield the projected size (in world dimensions,
    e.g. millimeters). This is however not the diameter that has generated the imaged
    projection, . The relationship between and depends not only on the geometry of
    the imaging system, but also on the shape of the three-dimensional object itself.
    The image alone does not contain information sufficient to exactly calculate neither
    nor the minimum diameter . Download : Download high-res image (67KB) Download
    : Download full-size image Fig. 3. Geometry of the imaging system. The back-projected
    dimension dproj is not the same as the actual object diameter, dtrue. dtrue, in
    turn, is not necessarily equal to the minimal object diameter, dmin. 2.4.2. Ellipse
    fitting and angle-based description As previously mentioned, we parameterize the
    detected contours by ellipse fitting. We use the fitEllipse implementation from
    the OpenCV toolbox, which returns the ellipse parameters in terms of position
    of its center, length of its axes, and its rotation angle . However, estimating
    physical dimensions from a pixel-based representation in the image space suffers
    from a distortion problem. We circumvent this by transforming the ellipse parameters
    into an angle-based representation in the frame of reference of the camera (camera
    space). Following a standard camera calibration procedure, we obtain the matrix
    , which enables transformation of a point in the two-dimensional image space into
    a point in the three-dimensional camera space: (2) This in turn allows representing
    the ellipse in terms of its angular height and width via a simple procedure (illustrated
    in Fig. 4): compute extreme points , , , of the ellipse in the image space, transform
    these points into vectors , , , in the camera space, and finally compute angles
    between the resulting vectors. Formulae are included in Table 1. Download : Download
    high-res image (115KB) Download : Download full-size image Fig. 4. Extreme points
    of the ellipse in the camera space. Table 1. Features constituting the input vector
    for the diameter estimation model. Symbol Description Formula Angular height of
    the ellipse (tangent of the angle) Angular width of the ellipse (tangent of the
    angle) Rotation angle of the ellipse (sine of the angle) Direct output of fitEllipse
    Viewing angle of the object (tangent of the angle) This angular representation
    is not enough to correctly estimate the size of the objects, however. Information
    on the relation between the camera space and the real world space is crucial.
    We provide it in the form of the angle defined as the angle between vector representing
    the center of the object in the world space and vector normal to the plane of
    the transporter (Fig. 5). The reason for selecting this angle is twofold: first,
    embedded within this angle is the approximate distance between the object and
    the camera, providing the necessary information on scale; second, this angle provides
    information on direction of observation of the object, which we assume to influence
    the relation between its observed and real dimensions. Download : Download high-res
    image (68KB) Download : Download full-size image Fig. 5. Calculation of Θ in the
    camera space. To compute , we first conduct the standard calibration procedure
    involving a checkerboard pattern placed on the surface of the transporter. This
    procedure allows finding in the camera space. All calculations can thus be performed
    on camera-space vectors ( , ). In order to fully separate the size estimation
    from the geometry of a specific imaging system, we introduce a scaling factor
    . All of the computations are performed in a coordinate system normalized so that
    , and later multiplied by the original factor to return to the physical coordinates.
    This allows deployment of the estimation system in any acquisition system of a
    known geometry without the need to retrain the regression model. 2.4.3. Regression
    model for diameter estimation We have established that the information required
    to exactly measure is not contained within the image. However, we propose that
    it is possible to statistically estimate this diameter if assumptions can be made
    regarding the shape of the object. We also propose that a nonlinear model can
    predict real dimensions of an object in the presence of distortions that a standard
    linear transform could not rectify. We propose a single regression function to
    estimate the minimal diameter : (2) where is a vector of input measurements (described
    in detail in Table 1), and model parameters vector implicitly describes the perspective
    distortion and the shape distribution of the objects. Our nonlinear regression
    function is constructed as a feed-forward fully connected neural network. The
    structure consists of three dense (following PyTorch terminology: linear) layers
    interleaved by two ELU activation functions. Details of the structure are explained
    in Table 2. Table 2. Structure of the diameter regression model. Layer Inputs
    Outputs Number of parameters Linear 4 4 20 ELU ( ) 4 4 n/a Linear 4 3 15 ELU (
    ) 3 3 n/a Linear 3 1 4 We train this model using gradient descent, optimizing
    a mean squared error (L2) loss function with the Adam optimizer with constant
    scaling coefficients of and and an L2 weight decay. Due to the small size of the
    model, we do not use mini-batches, instead optimizing on the entire dataset each
    iteration. Table 3 summarizes the training parameters and their scheduling. Table
    3. Training hyperparameters schedule for the regression model. Stage Epochs Learning
    rate Weight decay 1 250 2 500 3 750 Dataset used in training consists of a series
    of feature vectors – obtained by detecting objects using Mask R-CNN and fitting
    ellipses to their contours – with corresponding true values of each object’s minimal
    diameter. The model is trained on one subset of the data and validated on the
    other subset using the measure to quantify the goodness of fit. 2.5. Simulation-based
    learning 2.5.1. Synthetic dataset generation Images acquired using a simple optical
    system installed on the harvester do not provide any information on the true sizes
    of the objects, due to the technical infeasibility of performing accurate measurements
    on a per-object basis directly during imaging. Therefore, no data exists to train
    the size estimation model. To circumvent this problem we create a simulated environment
    in order to render synthetic images containing any distribution of objects, at
    the same time providing object-level labels not only in the image space (contours
    for segmentation) but also in the physical domain (true dimensions). This environment
    needs to be modelled in a way that closely resembles the underlying physical phenomena.
    In this subsection, we distinguish four key aspects of this resemblance and briefly
    analyze their importance: the object of interest, physics describing behavior
    of the objects in the environment, camera and optics, and rendering quality. The
    object of interest – in our case potato tubers – is the single most important
    component. Its shape needs to follow the distribution of the real potatoes. We
    follow (Torppa et al., 2007) and model our objects according to the shape coefficients
    (mean elongation, mean flatness, and the corresponding standard deviations – for
    details, see Table 1 in the original paper) for the Van Gogh cultivar, version
    1. For the irregularities in the shape of potatoes however, we do not implement
    the proposed spherical-harmonics component, instead creating a custom random deformation
    model basing on a series of reference meshes. Those are then deformed to follow
    the size distribution mentioned above. The minimal diameter – which can be thought
    of as a scaling factor for the synthetic potatoes – follows a normal distribution
    with mean and standard deviation . These values allow the distribution to cover
    all possible sizes of potatoes, while including less than of tubers so small that
    they would be rejected by the internal sieve on the harvester (diameter less than
    ). Physics, in particular collisions between potatoes and their rolling behavior
    on the transporter, needs to be realistically modelled. This is important to correctly
    simulate the occlusion conditions that occur in the real-world harvest scenario.
    Our simulation environment is built using Blender (Blender Online Community, 2019),
    owing to its capability of simulating highly detailed physics as well as integrated
    support for Python programming language. Due to the angle-based representation
    of the objects at the size estimation stage, the camera settings are unimportant,
    as long as sufficient information can be extracted to allow calculation of the
    viewing angle for each pixel in the image. However, for reasons that will be discussed
    separately, we set the virtual camera to ensure that the output resolution as
    well as the relative scale of rendered objects match those of the real dataset.
    Finally, the rendering quality is also an important factor, as differences between
    the visual style of the resulting synthetic images and the real acquired images
    might cause errors on the object detection stage. Many computer vision algorithms
    – including CNN-based – do not work equally well across different domains, meaning
    that a detector trained on real data might not predict well enough on synthetic
    data, and vice versa. There is a growing body of research done in the directions
    of domain adaptation and training data synthesis (Barth et al., 2018), but experimentally
    confirming these approaches in the context of instance segmentation of root crops
    is beyond the scope of this work. Alternatively, one could build a photorealistic
    rendering pipeline, but we deem this approach infeasible in practice due to the
    complexity involved. Instead, we propose a pipeline based on simple rendering.
    2.5.2. Simple rendering and dual segmentation pipelines Rendering images is not
    the main reason behind building a simulation environment. The actual objective
    is to generate a dataset suitable for training the size estimation model, consisting
    of image-space contours equipped with physical-space dimensions. The generating
    physical process needs to closely match that of the real potatoes, so that the
    resulting contours are indistinguishable from those produced by an instance segmentation
    algorithm working on real images. However, the actual rendered images do not need
    to appear visually similar to the real ones. In a way, images can be thought of
    as just a representation intermediate between a physical object and a detected
    contour. Therefore, we render our images using simple settings, but we employ
    a separate instance segmentation model to detect objects on these images. This
    “synthetic” segmentation model is incompatible with the “real” model in terms
    of image-space inputs. But as long as errors in detection and segmentation between
    the two models are similar, we can use the contours produced by the “synthetic”
    model in order to train the size estimation regressor, and expect this regressor
    to produce correct predictions even when it is fed contours produced by the “real”
    model. In order to assess the error similarity between the two models, we consider
    their PQ measures (specifically, recognition and segmentation components separately)
    on their respective validation sets. The desired result is that both models achieve
    similar RQ and SQ measures, meaning the models predict similar contours given
    similar inputs (we dub this “PQ-correspondence”, after the measure). Ensuring
    that this is the case is critical for the development. However, it might so happen
    that one of the models will outperform the other. We recommend applying a corrective
    strategy such as early stopping of the training process to prevent this from happening.
    More elaborate strategies could be object of further research. The detailed map
    of our approach is shown in Fig. 6. First, the instance segmentation model for
    the real images is trained and validated (1). Then, the simulation environment
    is built, with 3D models of the objects matching the shapes of the real objects
    (2). Next, the instance segmentation model for the rendered images is trained
    and validated (3) in such a way that prediction errors are similar to those produced
    by the real-data model (4). Finally, the regression model for size estimation
    is trained on data produced by the “synthetic” pathway (5). Consequently, basing
    on correspondence in the physical domain (accurate modelling of potato tubers)
    and PQ-correspondence (prediction errors of the instance segmentation models),
    this size estimator can be transferred to the “real” pathway. Download : Download
    high-res image (437KB) Download : Download full-size image Fig. 6. Map of our
    proposed approach. Notice how images (in the “vision domain”) are only an intermediate
    representation. 3. Results 3.1. Instance segmentation We have performed the first
    image acquisition in October 2018 in Oława, Poland. The imaging system has been
    installed on a Grimme SE-170/60 harvester. The scene was naturally illuminated;
    the sun light was diffuse due to a rather cloudy sky on the day of acquisition,
    resulting in soft edges and no hard shadows in the images. Overall, 2589 images
    in resolution 2304 × 1728 pixels were acquired. Only a subset of these images
    was selected to build the dataset that we used in our research: a total of 13
    images have received 564 detailed, per-object labels (segmentation contours and
    bounding boxes). The images were split into training and validation subsets, 8
    and 5 images (453 and 111 annotations) respectively. We show an example of a real
    image and its annotation in Fig. 6. The synthetic dataset has been rendered in
    1792 × 1344 resolution, which we have chosen arbitrarily in order to ensure similar
    relative sizes between objects in synthetic and real images. A total of 50 images
    were rendered in 5 groups, from 17 to 149 objects per image, to simulate different
    density conditions on the transporter. The process has taken less than 2 h on
    a computer equipped with an AMD Ryzen 7 3700X CPU and an Nvidia RTX 2070 Super
    graphics card. The resulting dataset has been split into training and validation
    subsets, each containing 5 images per group. An example synthetic image and a
    corresponding annotation has been shown in Fig. 7, compared to a real image. Download
    : Download high-res image (954KB) Download : Download full-size image Fig. 7.
    Comparison between real (left column) and synthetic (right) data. In the top row:
    example images; middle row: example predictions (with degree of classification
    confidence in upper left corner of each bounding box); bottom row: ground truth
    labels. Both instance segmentation models have been trained in under an hour each
    on a workstation equipped with an Nvidia RTX 2080 Ti GPU. We do not report the
    final losses of these models, instead skipping directly to the PQ metrics, giving
    them in Table 4. Table 4. Quality metrics of the instance segmentation models.
    Model Panoptic Quality Recognition Quality Segmentation Quality Real Synthetic
    The models displayed a very similar overall quality. The synthetic model made
    slightly more recognition mistakes, which could be due to some of the synthetic
    images being generated in a more difficult density conditions, making some of
    the objects more challenging to detect. Segmentation Quality is more important,
    as it defines the accuracy in shape prediction. Both models performed equally
    well in this domain – the differences are statistically insignificant. Thus, the
    real and synthetic detection models satisfied the correspondence requirement and
    no corrective action was necessary. Fig. 7 showcases example predictions of both
    models on unseen (validation) data. Next, we analyze the synthetic prediction
    errors in the function of density. When there are more objects on the scene, the
    occlusion effect becomes more significant and both recognition and segmentation
    are more difficult. The bar plot in Fig. 8 displays the quality metrics in the
    function of density, expressed as the total count of the visible objects. Until
    about 83 objects per image – which corresponds to approximately of the transporter
    area being covered with potatoes – the total metric is above , with recognition
    score nearly unaffected. When density increases above that level, both of the
    partial metrics start degrading, leading to a rapid decrease in the panoptic quality.
    However, increasing density affects the recognition component more significantly
    than the segmentation: Mask R-CNN does not detect all of the objects when they
    are heavily crowded, but when it does, it can still segment them relatively well.
    Download : Download high-res image (116KB) Download : Download full-size image
    Fig. 8. Panoptic Quality (PQ) scores of the synthetic model, broken down into
    components – Recognition Quality (RQ) and Segmentation Quality (SQ) – and displayed
    as a function of object density. 3.2. Diameter regression In order to train the
    minimal diameter regression model, we have generated a separate synthetic dataset.
    The same settings have been applied as in the case of instance segmentation dataset,
    except a constant density factor which we set at 83 objects per image. We have
    chosen this density because of a satisfactory compromise between the number of
    objects per image and the recognition rate of the detection model. The exact value
    is irrelevant at this point of the development, as the diameter regression model
    is trained on a per-object, as opposed to per-image manner. In order to simulate
    different geometries of the imaging system, and thus ensure the regression model
    does not overfit to one setting, we have generated several versions of this dataset
    with different camera angles. The angles spanned every of a range. This resulted
    in 5 groups of 10 annotated images each. Predictions on this dataset have been
    acquired using the synthetic detector and then ellipse fitting, and subsequently
    matched with the object-level annotations. As a result, we created a dataset matching
    the four ellipse parameters ( , , , ) with the true diameter ( ) of the corresponding
    3D object. The 3382 samples in this dataset were then randomly split into training
    and validation subsets in a 70:30 ratio, with stratification by camera angle.
    Training the regression model is very fast even in CPU mode, taking only a few
    seconds to cover the entire 1500 epochs-iterations (they are equivalent due to
    the training batch being equal to the entire dataset), reaching a final L2 loss
    on the order of (log-loss =  ). Finally, we validated the model on the remaining
    subset of the data. Fig. 9 displays scatter plots of predictions against the true
    values of the minimal diameter, grouped by the camera angle to illustrate potential
    failures at some angles. We found no such errors, however, except for a small
    number of outliers belonging to the group. The metric for all groups varied between
    and , with the average equal to . Overall, our model correctly captured the perspective
    transformation, providing accurate predictions for various camera angles. Not
    all variance in the data has been accounted for, however. This is naturally due
    to the estimating nature of the model, which attempts to reconstruct answers in
    the lack of complete information. Download : Download high-res image (329KB) Download
    : Download full-size image Fig. 9. Analysis of the regression model predictions
    on the validation subset for different view angles. 3.3. Experiment in real conditions
    To test our proposed solution in real conditions, in December 2019 we have repeated
    the image acquisition on new material. We were granted access to two big-bags
    of potatoes of two different sorts, labelled “small” and “large”. Those bags were
    then poured onto the harvester to emulate true harvest conditions. Other than
    a general description of the grades, we had no other reference information. The
    imaging system was the same as during the first acquisition, although we have
    not installed it in exact same geometrical configuration. Therefore, calibration
    with checkerboard pattern was performed again to allow calculation of the viewing
    angles. Illumination conditions were somewhat different than in October 2018,
    but the sun light was similarly diffuse. Two series of images were acquired, one
    for the bag of “small” potatoes, the other for the “large” bag, 18 and 13 images
    respectively. The images were then undistorted to correct for the radial distortion
    of the lens. Example images are shown in Fig. 10. Download : Download high-res
    image (451KB) Download : Download full-size image Fig. 10. Example images of two
    different batches of potatoes. Sample from the “large” batch on the left side,
    sample from the “small” on the right. After visually assessing the acquired images,
    we have found many cases where tubers of very similar sizes appear in both groups.
    Several examples of this can be seen in the right image, on the center-right as
    well as lower left (compare with the sizes of objects on the left image). On the
    other hand, very few small tubers were present in the batch labelled as “large”.
    Consequently, we had to assume a larger than anticipated uncertainty in the results,
    with a certain asymmetry. We tested the proposed system in two steps. First, we
    performed instance segmentation using the real model, using it directly with no
    additional training beyond the original procedure described in 3.1. The model
    generalizes well and correctly recognized objects in the new images. Problems
    appeared in the upper ¼ of the image, where the objects were heavily crowded,
    and Mask R-CNN did not always detect all of them. Also, objects in the bottom
    section of the image often appeared cropped. Therefore, we only accepted objects
    located below a threshold of 450 pixels and above 1500 pixels in horizontal dimension
    (from a total resolution of 1728 pixels). Then, ellipses were fitted to the detected
    objects, and after conversion to the angular coordinate system, they were fed
    to the size estimation model. We show the estimation results in Fig. 11 in the
    form of histograms summarized over each batch. Download : Download high-res image
    (217KB) Download : Download full-size image Fig. 11. Histograms of the minimal
    diameters predicted by our system for each bag of reference potatoes, summed across
    batches. 4. Discussion Our proposed method allows estimating the minimal diameter
    of potato tubers from a single image acquired using a consumer-grade RGB camera
    without any additional illumination. At the core of the method lies an instance
    segmentation algorithm based on Mask R-CNN. This model can be trained from just
    a handful of images and still deliver very high accuracy of contour prediction
    – even when boundaries between neighboring objects are not very clear. As shown
    in Fig. 6, it is capable of predicting even the occluded parts of the objects,
    using the contextual information provided in the local curvature of each object
    together with the learned statistic of the shape of the objects. Our key contribution
    is a nonlinear regression model for size estimation, able to estimate the minimal
    diameter of a tuber from the ellipse fitted to its perceived contour, and a method
    of training it using simulated data. The model allows estimating this dimension
    of the object which is not directly observable, basing on the known statistical
    properties of the shape distribution of the objects. Thanks to a change in coordinate
    system, our method is not specific to the particular geometry of the imaging system.
    This coordinate change is done in a simple way, without the need of any additional
    overhead beyond the ordinary camera calibration procedure. Although our method
    is generic, several steps must be taken in order to apply it to estimating the
    sizes of any specific crop. Manual annotation of a dataset is necessary prior
    to training the instance segmentation model, which is moderately tedious and needs
    to be done with high precision. Then, in order to calibrate the size estimation
    regression algorithm, a detailed 3D model of the specific crop must be prepared,
    following the shape characteristics of that crop. Subsequently, PQ-correspondence
    between the segmentation models must be ensured. In order to apply a system based
    on our proposed method in a practical scenario, the limitation of maximum object
    density must be taken into account. Finally, we stress that we do not propose
    a method of measurement but only of size estimation. We build a statistical model
    to predict a quantity that is not directly visible in the images. The most important
    effect that would negatively influence the results would be a difference in shape
    characteristics between the potato cultivar used to model the synthetic data and
    the cultivar that is actually grown in the field. Potential improvements might
    be achieved when using a more accurate potato model, possibly with the spherical-harmonics
    deformation model by (Torppa et al., 2007). Further ways to extend the proposed
    method could involve improving the quality of rendering, or implementing domain
    adaptation-based approaches to training of the instance segmentation model. Training
    a single model that could process both real and synthetic images equally well
    would circumvent the difficulty in manually annotating the dataset of real images.
    To improve the detection quality, solution such as Ellipse R-CNN (Dong et al.,
    2020) could be applied to regress the ellipses directly from the images, instead
    of fitting them to contours obtained via segmentation with varying accuracy. That
    approach could be less desired if our method was to be applied to crops that display
    more regularity than potatoes. If additional geometrical information could be
    utilized (e.g. location and size of stems in onions), the size estimator could
    take that into account to improve the predictions. For problems involving even
    higher degree of inter-object occlusions, amodal segmentation techniques could
    be applied. 5. Conclusions We proposed a method for developing detection and size
    estimation systems for on-line yield monitoring of root crops. Thanks to the use
    of a state-of-the-art instance segmentation meta-algorithm, Mask R-CNN, the system
    can detect and segment crops in images acquired using simple, consumer-grade RGB
    camera with no additional illumination. The segmentation algorithm works correctly
    even in the case of moderate occlusions between the objects. Our method allows
    estimating true sizes of the objects even when the images were acquired under
    significant perspective distortion. It is capable of estimating the minimal diameter
    of the objects in the case when it is not directly observable in the processed
    images. This is made possible by learning the shape characteristics of the crop
    in question from a simulation model. We showed a specific implementation of our
    method to estimate sizes of potatoes, first testing the resulting system in simulated
    conditions, and then deploying it in near-real environment. The system successfully
    transferred knowledge acquired during simulation-based learning to predict sizes
    of real crops. Our method can be applied to other crops (not only root crops)
    by preparing an annotated dataset for training an instance segmentation model
    and constructing a simulation environment with a proper three-dimensional model
    of the desired crop. Declaration of Competing Interest The authors declare that
    they have no known competing financial interests or personal relationships that
    could have appeared to influence the work reported in this paper. Acknowledgements
    We wish to thank Piotr Lampa for designing and assembling the image acquisition
    system. We wish to thank Piotr Lampa and Emil Nowosielski for their work on the
    image acquisition process itself. We also wish to extend our thanks to Aleksy
    Kubik and Klaudiusz Matejka Farm, who supported the work by hosting the field
    experiments and providing the potatoes. This research was supported by the National
    Centre for Research and Development (BIOSTRATEG3/343547/8/NCBR/2017). References
    Bargoti and Underwood, 2017 S. Bargoti, J.P. Underwood Image segmentation for
    fruit detection and yield estimation in apple orchards J. Field Robot., 34 (6)
    (2017), pp. 1039-1060, 10.1002/rob.21699 View in ScopusGoogle Scholar Barth et
    al., 2018 R. Barth, J. IJsselmuiden, J. Hemming, E.J. Van Henten Data synthesis
    methods for semantic segmentation in agriculture: a capsicum annuum dataset Comput.
    Electron. Agric., 144 (2018), pp. 284-296, 10.1016/j.compag.2017.12.001 View PDFView
    articleView in ScopusGoogle Scholar Blender Online Community, 2019 Blender Online
    Community Blender – A 3D Modelling and Rendering Package Stichting Blender Foundation,
    The Blender Foundation, Amsterdam (2019) http://www.blender.org Google Scholar
    Boatswain Jacques et al., 2018 Boatswain Jacques, A.A., Adamchuk, V.I., Cloutier,
    G., Clark, J.J., Miller, C., 2018. Development of a machine vision yield monitor
    for shallot onion harvesters. In: Proceedings of the 14th International Conference
    on Precision Agriculture. Montreal, Quebec, Canada. https://ispag.org/proceedings/?action=author_abstracts.
    Google Scholar Chen et al., 2017 Steven W. Chen, Shreyas S. Shivakumar, Sandeep
    Dcunha, Jnaneshwar Das, Edidiong Okon, Qu Chao, Camillo J. Taylor, Vijay Kumar
    Counting apples and oranges with deep learning: a data-driven approach IEEE Robot.
    Automat. Lett., 2 (2) (2017), pp. 781-788, 10.1109/LRA.2017.2651944 View in ScopusGoogle
    Scholar Chinchuluun et al., 2009 R. Chinchuluun, W.S. Lee, R. Ehsani Machine vision
    system for determining citrus count and size on a canopy shake and catch harvester
    Appl. Eng. Agric., 25 (4) (2009), pp. 451-458 View in ScopusGoogle Scholar Dong
    et al., 2001 Dong, Wenbo, Roy, Pravakar, Peng, Cheng, Isler, Volkan, Ellipse R-CNN:
    Learning to Infer Elliptical Object from Clustering and Occlusion, ArXiv:2001.11584
    [Cs], January 30, 2020. http://arxiv.org/abs/2001.11584 (preprint). Google Scholar
    ElMasry et al., 2012 Gamal ElMasry, Sergio Cubero, Enrique Moltó, José Blasco
    In-line sorting of irregular potatoes by using automated computer-based machine
    vision system J. Food Eng., 112 (1) (2012), pp. 60-68, 10.1016/j.jfoodeng.2012.03.027
    View PDFView articleView in ScopusGoogle Scholar Gongal et al., 2015 A. Gongal,
    S. Amatya, M. Karkee, Q. Zhang, K. Lewis Sensors and systems for fruit detection
    and localization: a review Comput. Electron. Agricul., 116 (2015), pp. 8-19, 10.1016/j.compag.2015.05.021
    View PDFView articleView in ScopusGoogle Scholar Habaragamuwa et al., 2018 Harshana
    Habaragamuwa, Yuichi Ogawa, Tetsuhito Suzuki, Tomoo Shiigi, Masanori Ono, Naoshi
    Kondo Detecting greenhouse strawberries (mature and immature), using deep convolutional
    neural network Eng. Agric. Environ. Food, 11 (3) (2018), pp. 127-138, 10.1016/j.eaef.2018.03.001
    View PDFView articleView in ScopusGoogle Scholar He et al., 2017 He, Kaiming,
    Gkioxari, Georgia, Dollar, Piotr, Girshick, Ross, 2017. Mask R-CNN, 2961–69, 2017.
    https://openaccess.thecvf.com/content_iccv_2017/html/He_Mask_R-CNN_ICCV_2017_paper.html.
    Google Scholar He et al., 2016 K. He, X. Zhang, S. Ren, J. Sun Deep residual learning
    for image recognition 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR) (2016), pp. 770-778, 10.1109/CVPR.2016.90 Google Scholar Hofstee et al.,
    2003 J.W. Hofstee, G.J. Molema Volume estimation of potatoes partly covered with
    dirt tare 2003, Las Vegas, NV July 27–30, 2003, American Society of Agricultural
    and Biological Engineer (2003), 10.13031/2013.15380 Google Scholar Kestur et al.,
    2019 Ramesh Kestur, Avadesh Meduri, Omkar Narasipura MangoNet: a deep semantic
    segmentation architecture for a method to detect and count mangoes in an open
    orchard Eng. Appl. Artificial Intell., 77 (2019), pp. 59-69, 10.1016/j.engappai.2018.09.011
    View PDFView articleView in ScopusGoogle Scholar Khojastehnazhand et al., 2019
    Mostafa Khojastehnazhand, Vahid Mohammadi, Saeid Minaei Maturity detection and
    volume estimation of apricot using image processing technique Scientia Horticulturae,
    251 (2019), pp. 247-251, 10.1016/j.scienta.2019.03.033 View PDFView articleView
    in ScopusGoogle Scholar Kirillov et al., 2019 A. Kirillov, K. He, R. Girshick,
    C. Rother, P. Dollár Panoptic segmentation 2019 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR) (2019), pp. 9396-9405, 10.1109/CVPR.2019.00963
    View in ScopusGoogle Scholar Kusumam et al., 2017 K. Kusumam, T. Krajník, S. Pearson,
    T. Duckett, G. Cielniak 3D-vision based detection, localization, and sizing of
    broccoli heads in the field J. Field Robot., 34 (8) (2017), pp. 1505-1518, 10.1002/rob.21726
    View in ScopusGoogle Scholar Lee et al., 2018 Young-Joo Lee, Ki-Duck Kim, Hyeon-Seung
    Lee, Beom-Soo Shin Vision-based potato detection and counting system for yield
    monitoring J. Biosyst. Eng., 43 (2) (2018), pp. 103-109, 10.5307/JBE.2018.43.2.103
    View in ScopusGoogle Scholar Lin et al., 2020 Guichao Lin, Yunchao Tang, Xiangjun
    Zou, Jiabing Cheng, Juntao Xiong Fruit detection in natural environment using
    partial shape matching and probabilistic hough transform Precis. Agric., 21 (1)
    (2020), pp. 160-177, 10.1007/s11119-019-09662-w View in ScopusGoogle Scholar Long
    et al., 2018 Yaowei Long, Yuhao Wang, Ziming Zhai, Li Wu, Minzan Li, Hong Sun,
    Qinghua Su Potato volume measurement based on RGB-D camera IFAC-PapersOnLine,
    6th IFAC Conference on Bio-Robotics BIOROBOTICS 2018, vol. 51, no. 17, Springer
    (2018), pp. 515-520, 10.1016/j.ifacol.2018.08.157 View PDFView articleView in
    ScopusGoogle Scholar Mavridou et al., December 2019 E. Mavridou, E. Vrochidou,
    G.A. Papakostas, T. Pachidis, V.G. Kaburlasos Machine vision systems in precision
    agriculture for crop farming J. Imag., 5 (12) (2019), p. 89, 10.3390/jimaging5120089
    View in ScopusGoogle Scholar Momin et al., 2017 M.A. Momin, M.T. Rahman, M.S.
    Sultana, C. Igathinathane, A.T.M. Ziauddin, T.E. Grift Geometry-based mass grading
    of mango fruits using image processing Inform. Process. Agricul., 4 (2) (2017),
    pp. 150-160, 10.1016/j.inpa.2017.03.003 View PDFView articleView in ScopusGoogle
    Scholar Noordam et al., 2000 Jacco C. Noordam, Gerwoud W. Otten, Toine J.M. Timmermans,
    Bauke H. van Zwol High-speed potato grading and quality inspection based on a
    color vision system Machine Vision Applications in Industrial Inspection VIII,
    vol. 3966, International Society for Optics and Photonics (2000), pp. 206-217,
    10.1117/12.380075 View in ScopusGoogle Scholar Pandey et al., 2019 Nikhil Pandey,
    Suraj Kumar, Raksha Pandey Grading and defect detection in potatoes using deep
    learning Communication, Networks and Computing, edited by Shekhar Verma, Ranjeet
    Singh Tomar, Brijesh Kumar Chaurasia, Vrijendra Singh, and Jemal Abawajy. Communications
    in Computer and Information Science, Springer, Singapore (2019), pp. 329-339,
    10.1007/978-981-13-2372-0_29 View in ScopusGoogle Scholar Paszke et al., 2017
    Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito,
    Zachary, Lin, Zeming, 2017. Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic
    Differentiation in PyTorch. In NIPS-W. Google Scholar Razmjooy et al., 2012 Navid
    Razmjooy, B. Somayeh Mousavi, F. Soleymani A real-time mathematical computer method
    for potato inspection using machine vision Comput. Math. Appl., 63 (1) (2012),
    pp. 268-279, 10.1016/j.camwa.2011.11.019 View PDFView articleView in ScopusGoogle
    Scholar Sa et al., August 2016 I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez,
    C. McCool DeepFruits: a fruit detection system using deep neural networks Sensors,
    16 (8) (2016), p. 1222, 10.3390/s16081222 View in ScopusGoogle Scholar Si et al.,
    2018 Yongsheng Si, N. Sindhuja Sankaran, Richard Knowles, Mark J. Pavek Image-based
    automated potato tuber shape evaluation J. Food Measur. Charact., 12 (2) (2018),
    pp. 702-709, 10.1007/s11694-017-9683-2 View in ScopusGoogle Scholar Su et al.,
    2018 Qinghua Su, Naoshi Kondo, Minzan Li, Hong Sun, Dimas Firmanda Al Riza, Harshana
    Habaragamuwa Potato quality grading based on machine vision and 3D shape analysis
    Comput. Electron. Agric., 152 (2018), pp. 261-268, 10.1016/j.compag.2018.07.012
    View PDFView articleView in ScopusGoogle Scholar Szczypiński et al., 2009 Piotr
    M. Szczypiński, Michał Strzelecki, Andrzej Materka, Artur Klepaczko MaZda—a software
    package for image texture analysis Comput. Methods Prog. Biomed., 94 (1) (2009),
    pp. 66-76, 10.1016/j.cmpb.2008.08.005 View PDFView articleView in ScopusGoogle
    Scholar Torppa et al., 2007 Johanna Torppa, Jari P.T. Valkonen, Karri Muinonen
    Three-dimensional stochastic shape modelling for potato tubers Potato Res., 49
    (2) (2006), pp. 109-118, 10.1007/s11540-006-9010-5 View in ScopusGoogle Scholar
    TrueSort, 2020 TrueSort, Ellips B.V., Eindhoven, Netherlands, 2020. Available
    at: https://ellips.com/fruit-and-vegetables/potato-sorting/. Google Scholar Wang
    et al., 2016 C. Wang, W. Huang, B. Zhang, J. Yang, M. Qian, S. Fan, L. Chen Design
    and implementation of an automatic grading system of diced potatoes based on machine
    vision IFIP Adv. Inform. Commun. Technol., 479 (2016), pp. 202-216, 10.1007/978-3-319-48354-2_22
    View in ScopusGoogle Scholar Wu et al., 2019 Wu, Yuxin, Kirillov, Alexander, Massa,
    Francisco, Lo, Wan-Yen, Girshick, Ross. Detectron2, 2019. https://github.com/facebookresearch/detectron2.
    Google Scholar Yu et al., 2020 Chuang Yu, Xiang Fan, Hu Zhuhua, Xin Xia, Yaochi
    Zhao, Ruoqing Li, Yong Bai Segmentation and measurement scheme for fish morphological
    features based on mask R-CNN Inform. Process. Agric. (2020), 10.1016/j.inpa.2020.01.002
    Google Scholar Yu et al., 2019 Yang Yu, Kailiang Zhang, Li Yang, Dongxing Zhang
    Fruit detection for strawberry harvesting robot in non-structural environment
    based on mask-RCNN Comput. Electron. Agric., 163 (2019), 10.1016/j.compag.2019.06.001
    Google Scholar Zhang et al., 2018 H. Zhang, Z. Li, L.i. Liu, Z. Liang Geometry-based
    mass grading of betelnut using image processing 2018 13th World Congress on Intelligent
    Control and Automation (WCICA) (2018), pp. 1014-1019, 10.1109/WCICA.2018.8630602
    View in ScopusGoogle Scholar Cited by (14) Multiview-based method for high-throughput
    quality classification of germinated oil palm seeds 2024, Computers and Electronics
    in Agriculture Show abstract Monitoring the growth of insect larvae using a regression
    convolutional neural network and knowledge transfer 2024, Engineering Applications
    of Artificial Intelligence Show abstract Box-supervised dynamical instance segmentation
    for in-field cotton 2023, Computers and Electronics in Agriculture Show abstract
    Improved yield-salinity relationship considering salt and root distribution dynamics
    2023, European Journal of Agronomy Show abstract Enhancing Yam Quality Detection
    Through Computer Vision in Iot and Robotics Applications 2024, SSRN Enhancing
    Yam Quality Detection Through Computer Vision in Iot and Robotics Applications
    2024, SSRN View all citing articles on Scopus View Abstract © 2021 Elsevier B.V.
    All rights reserved. Recommended articles Combination of computer vision and backscattering
    imaging for predicting the moisture content and colour changes of sweet potato
    ( L.) during drying Computers and Electronics in Agriculture, Volume 150, 2018,
    pp. 178-187 Daniel I. Onwude, …, Guangnan Chen View PDF Pixel level segmentation
    of early-stage in-bag rice root for its architecture analysis Computers and Electronics
    in Agriculture, Volume 186, 2021, Article 106197 Liang Gong, …, Chengliang Liu
    View PDF Peach ripeness classification based on a new one-stage instance segmentation
    model Computers and Electronics in Agriculture, Volume 214, 2023, Article 108369
    Ziang Zhao, …, Chaoxi Luo View PDF Show 3 more articles Article Metrics Citations
    Citation Indexes: 10 Captures Readers: 34 View details About ScienceDirect Remote
    access Shopping cart Advertise Contact and support Terms and conditions Privacy
    policy Cookies are used by this site. Cookie settings | Your Privacy Choices All
    content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Instance segmentation of root crops and simulation-based learning to estimate
    their physical dimensions for on-line machine vision yield monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
