- DOI: https://doi.org/10.3390/rs11121443
  analysis: '>'
  authors:
  - Hongxun Yao
  - Rongjun Qin
  - Xiaoyu Chen
  citation_count: 342
  full_citation: '>'
  full_text: ">\nremote sensing  \nReview\nUnmanned Aerial Vehicle for Remote Sensing\n\
    Applications—A Review\nHuang Yao 1,2\n, Rongjun Qin 2,3,*\nand Xiaoyu Chen 2\n\
    1\nSchool of Educational Information Technology, Central China Normal University,\
    \ Wuhan 430079, China;\nyaohuang@mail.ccnu.edu.cn\n2\nDepartment of Civil, Environmental\
    \ and Geodetic Engineering, The Ohio State University (OSU), Columbus,\nOH 43210,\
    \ USA; chenxy@mail.ccnu.edu.cn\n3\nDepartment of Electrical and Computer Engineering,\
    \ The Ohio State University (OSU), Columbus,\nOH 43210, USA\n*\nCorrespondence:\
    \ qin.324@osu.edu\nReceived: 5 May 2019; Accepted: 16 June 2019; Published: 18\
    \ June 2019\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\n\
    Abstract: The unmanned aerial vehicle (UAV) sensors and platforms nowadays are\
    \ being used in\nalmost every application (e.g., agriculture, forestry, and mining)\
    \ that needs observed information from\nthe top or oblique views. While they intend\
    \ to be a general remote sensing (RS) tool, the relevant RS\ndata processing and\
    \ analysis methods are still largely ad-hoc to applications. Although the obvious\n\
    advantages of UAV data are their high spatial resolution and ﬂexibility in acquisition\
    \ and sensor\nintegration, there is in general a lack of systematic analysis on\
    \ how these characteristics alter solutions\nfor typical RS tasks such as land-cover\
    \ classiﬁcation, change detection, and thematic mapping.\nFor instance, the ultra-high-resolution\
    \ data (less than 10 cm of Ground Sampling Distance (GSD))\nbring more unwanted\
    \ classes of objects (e.g., pedestrian and cars) in land-cover classiﬁcation;\
    \ the often\navailable 3D data generated from photogrammetric images call for\
    \ more advanced techniques for\ngeometric and spectral analysis. In this paper,\
    \ we perform a critical review on RS tasks that involve\nUAV data and their derived\
    \ products as their main sources including raw perspective images, digital\nsurface\
    \ models, and orthophotos. In particular, we focus on solutions that address the\
    \ “new” aspects\nof the UAV data including (1) ultra-high resolution; (2) availability\
    \ of coherent geometric and spectral\ndata; and (3) capability of simultaneously\
    \ using multi-sensor data for fusion. Based on these solutions,\nwe provide a\
    \ brief summary of existing examples of UAV-based RS in agricultural, environmental,\n\
    urban, and hazards assessment applications, etc., and by discussing their practical\
    \ potentials, we share\nour views in their future research directions and draw\
    \ conclusive remarks.\nKeywords: UAVs; remote sensing applications; data analysis\n\
    1. Introduction\nUnmanned aerial vehicle (UAV) applications have become an ever-expanding\
    \ area in remote\nsensing (RS) in recent years, driven by their both academic\
    \ and commercial successes [1]. However,\nthese practices are highly disparate\
    \ even for the same or similar application, primarily due to the fact\nthat data\
    \ acquisition and sensors to be used are featured to be more ﬂexible than traditional\
    \ ways.\nThese practices are often developed through a learn-by-doing process,\
    \ and there exist a few papers\nthat review current works of domain-speciﬁc urban\
    \ and environmental applications [2–4], as well as\nmethodologies (e.g., feature\
    \ extraction and classiﬁcation for speciﬁc objects) [4–6]. Although these\ncan\
    \ be valuable in their respective ﬁelds, there is in general a lack of systematic\
    \ analysis on how these\ncharacteristics alter solutions for typical RS tasks\
    \ such as land-cover classiﬁcation, change detection,\nand thematic mapping etc.\
    \ Thus, this raises challenges in identifying common practices and feasibilities\n\
    Remote Sens. 2019, 11, 1443; doi:10.3390/rs11121443\nwww.mdpi.com/journal/remotesensing\n\
    Remote Sens. 2019, 11, 1443\n2 of 22\nof UAV to be used for speciﬁc tasks, as\
    \ well as ways of benchmarking these for researchers in community.\nWe found that\
    \ in many cases, end-users simply adopt the same methods used for traditional\
    \ RS\nsources with lower resolution, while not considering the unique characteristics\
    \ of UAV-based images.\nFor instance, most of the existing methods for RS image\
    \ classiﬁcation normally focus on extracting 2D\n(2 dimensional) image features,\
    \ while when classifying UAV-based image products, the often available\ndigital\
    \ surface model (DSM) can be integrated and consistently improve the classiﬁcation\
    \ accuracy [7].\nFurthermore, the development of UAV platforms and various sensors\
    \ has motivated RS applications\nsuch as object detection and real-time tracking\
    \ at ﬁner scales [8,9], where advanced data analysis\ntechniques developed in\
    \ computer vision and machine learning elevate the ability of automated UAV\n\
    data analysis. In this review, we present a comprehensive discussion on UAV data\
    \ analysis methods\nfor UAVs RS applications based on the challenges and potentials\
    \ brought by the “new” aspects of\nthe UAVs data, being (1) ultra-high resolution\
    \ (UHR); (2) high availability of geometric and spectral\ndata; and (3) integrated\
    \ multi-sensor data fusion. Specially, the RS aspect of this review focuses on\n\
    the use of UAV-derived product, including the raw imageries, photogrammetrically\
    \ derived DSMs,\nand orthophotos for object interpretation, scene analysis, and\
    \ change detection. For reviews of the\nphotogrammetric processing and hardware\
    \ development, the readers may refer to Reference [10].\nUAVs are generally categorized\
    \ based on several related key attributes, including weight, ﬂying\naltitude,\
    \ payload, endurance, and range [11,12]. A typical classiﬁcation instance provides\
    \ 5 diﬀerent\ngroups of UAVs according to their maximum gross takeoﬀ weight (MGTW)\
    \ and normal operating\naltitude and airspeed [13]. Since we would like to present\
    \ an overview of the new methods and civil\nRS applications brought by several\
    \ distinct characteristics (e.g., low cost, ﬂexibility, and customization)\nof\
    \ UAVs, this paper will primarily focus on small UAVs (0–20 lbs, <1200 ft, <100\
    \ knots).\nDue to the low ﬂying altitude, UAVs can easily acquire very detailed\
    \ information of observed\nobjects with a spatial resolution under one decimeter\
    \ (UHR), which allows for accurate geometrical\nand semantic analysis for a reasonably\
    \ broader area than a single site, while the increased resolution\nmay not always\
    \ bring the same level of improvement in terms of data interpretation: details\
    \ of the\nobjects may increase the within-class texture complexities that often\
    \ lead to classiﬁcation errors [14].\nMoreover, when the sizes of objects in the\
    \ scene vary signiﬁcantly, multi-scale approaches [15] should\nbe used in order\
    \ to reduce unnecessary computation, especially for UHR imageries, and selecting\
    \ a set\nof appropriate scales can be particularly challenging [7].\nUAV aided\
    \ by Global Positioning System/inertial measurement unit (GPS/IMU) and autopilot\n\
    systems are able to very easily capture photogrammetric image blocks, and a UAV\
    \ data acquisition\nmission very often refers to taking either full motion videos\
    \ or high-resolution photogrammetric\nimages. With advanced photogrammetric processing\
    \ pipelines [16,17], 3D (3 dimensional) geometric\ninformation such as triangular\
    \ meshes and DSMs are nowadays becoming standard products for\nUAV-based RS missions.\
    \ Given that these ready-to-ﬂy UAV platforms are much more ﬂexible than\ntraditional\
    \ platforms, the access to orthophoto and DSM products from UAV ﬂight missions\
    \ can\nbe seamless [18]. In addition, associated light-weight/low-cost sensors\
    \ such as multispectral and\nhyperspectral cameras are becoming more available,\
    \ which has brought new opportunities for UAVs to\nattempt multi-source data fusion\
    \ solutions with much higher spatial resolution. While such data fusion\nis traditionally\
    \ cost-prohibitive in other platforms [18,19], these beneﬁts also demand more\
    \ advanced\nco-registration algorithms dealing with sensor integration and calibration,\
    \ as well as the eﬀorts for\nimplementing new/speciﬁc camera models for low-cost\
    \ sensor data (i.e., linear-array hyperspectral\ncameras and rolling shutters),\
    \ which may not be readily accessible.\nAs a means of RS data collection, UAV\
    \ data and its derived products feature sounding promises\nto serve typical RS\
    \ data analysis practices, of which two techniques/applications are mostly commonly\n\
    used in the RS community: (1) land cover/image classiﬁcation or object detection;\
    \ and (2) change\ndetection. At a ﬁrst glance, these applications can be intuitively\
    \ inherited by applying existing\ntechniques, while we argue that if the UAV data\
    \ would be under-utilized, because the ultra-high\nspatial/temporal resolution,\
    \ as well as the accessible geometric data associated with it (multi-modal\nRemote\
    \ Sens. 2019, 11, 1443\n3 of 22\ndata), creates much more opportunities and solutions,\
    \ algorithms and techniques that are particularly\nsuitable for such data are\
    \ worth reconsideration. This review tends to provide a speciﬁc summary on the\n\
    algorithmic and application aspects of RS data processing particularly related\
    \ to UAV data by analyzing\npast literature and extrapolating potential trends\
    \ and issues to be considered under general frameworks.\nOur contribution is providing\
    \ a stage update to colleagues in the community to indicate where potential\n\
    resources are, as well as the branched issues and essences when processing UAV\
    \ data as an RS resource,\nrather than explicit scientiﬁc discoveries. In general,\
    \ we hypothesize the geometric aspects of the\nstandard photogrammetric processing\
    \ for the UAV-based RS products are well handled, such as\nbundle adjustment,\
    \ DSMs and orthophotos, and co-registration (an excellent review describing how\n\
    standardized such procedures are can be found in Reference [18]). Therefore, the\
    \ review might cover\nthe geometric aspects only when necessary. The rest of the\
    \ review is organized as follows: Section 2\ngenerally introduces the recent development\
    \ of sensors that fuels the diverse data sources. In Section 3,\nwe present the\
    \ state-of-the-art UAVs remotely sensed data analysis techniques and practices\
    \ related to\n(1) land use/land cover (LULC) classiﬁcation and (2) change detection,\
    \ as well as discussing potential\nimprovements and algorithmic aspects to be\
    \ considered for the high-spatial/temporal-resolution and\nmulti-modal UAV data.\
    \ Note here we consider object detection taking a similar framework (often\nlearning-based)\
    \ as image classiﬁcation, and thus we take image classiﬁcation as the primary\
    \ category\nfor review. In Section 4, we turn our discussion to the current status\
    \ of UAVs RS applications by\ndemonstrating the potential eﬀorts in utilizing\
    \ UAV-based RS data for tasks traditionally refraining\naerial/satellite RS data.\
    \ Section 5 summarizes this review and provides recommendations to inform\nfuture\
    \ research works.\n2. Overview of UAV Sensors\nUAV datasets are mostly understood\
    \ as drones with RGB cameras, it is worth noting that there\nexists a wide range\
    \ of sensor options when considering professional applications. Many existing\n\
    (and expensive) RS instruments for aerial and satellite platforms are now embracing\
    \ their miniature\nand low-cost versions for UAV platforms, such as multispectral,\
    \ hyperspectral, short/mid-wave\nrange cameras (e.g., thermal) and light-weight\
    \ LiDAR (light detection and ranging). Knowing the\ncharacteristics of these sensors\
    \ and their speciﬁcations will better inform engineers and scientists when\nperforming\
    \ speciﬁc RS tasks. General descriptions of available sensors suitable for low-payload\
    \ aircraft\nplatforms can be found in Colomina and Molina’s review paper [19],\
    \ and other in-depth discussions\nof these sensors can be found in Reference [1].\
    \ However, amongst these relevant works, a close\ntie between the characteristics\
    \ of the sensor data and the potential applications is largely missing:\nUAV data\
    \ should be better processed and analyzed given their resolution advantages for\
    \ either the\ntraditional or novel RS applications. Our introduction to these\
    \ sensors and their data will lean towards\nthe goal of better informing researchers\
    \ who design processing algorithms for typical RS analysis\ntasks including image\
    \ classiﬁcation and change detection. The discussed sensors and speciﬁcations\n\
    as well as relevant applications and their advantages and disadvantages are summarized\
    \ in Table 1,\nand details of these sensors are introduced in the following subsections.\n\
    2.1. RGB Cameras\nModern UAV-based RS starts with remotely controlled plane models\
    \ mounting normal RGB\ncameras [20]. Basically, a consumer-grade camera mounted\
    \ on a drone, integrated with/without\nnavigation sensors such as GPS/IMU constitutes\
    \ necessary components of a UAV-surveying system.\nSuch a system, being highly\
    \ engineered in terms of (low-cost) sensor integration is becoming accessible\n\
    and has nowadays emerged to be applied in other popular uses such as entertainment\
    \ and television\n(e.g., DJI drones). As compared to other types of sensors (introduced\
    \ in Sections 2.2–2.5), there exist a\nwide range of RGB cameras on the market,\
    \ and for diﬀerent applications, selecting appropriate RGB\ncameras mounted on\
    \ a UAV can be a key to success. Common parameters for selecting RGB cameras\n\
    include camera lens (better lens come with less geometric distortions), and resolution\
    \ and quality of the\nRemote Sens. 2019, 11, 1443\n4 of 22\ncharge coupled device\
    \ (CCD)/complementary metal oxide semiconductor (CMOS) chips (pixel size and\n\
    noise level). High-quality cameras ensure good photogrammetric products and low-signal/noise-ratio\n\
    data for RS data analysis (such as image segmentation and classiﬁcation). Normally,\
    \ highly integrated\nUAV-systems are easy to transport and operate, while the\
    \ mountable RGB cameras are often conﬁned\nto a few models allowing for seamless\
    \ control. Sometimes, professional photogrammetric users favor\ncustomized/less\
    \ integrated systems in order to be able to access a larger collection of RGB\
    \ cameras\nfor diﬀerent applications. In general, many of the existing RS applications\
    \ still largely rely on RGB\ncamera-based products (e.g., orthophotos and DSMs),\
    \ such as analyses for tree crowns detection,\nvegetation growth monitoring and\
    \ change analysis in a local scale. There exists a large body of\nworks on UAV-based\
    \ photogrammetric surveying, where the selection of camera models/parameters is\n\
    well discussed [2,10], and many of these can be applied as well when determining\
    \ parameters (e.g.,\nlens distortion, focal length, and pixel size) for other\
    \ types of camera systems (e.g., multispectral,\nhyperspectral cameras).\n2.2.\
    \ Light-Weight Multispectral Cameras\nMultispectral cameras are one of the most\
    \ commonly used sensors in addition to RGB cameras in\nthe UAV sensors family,\
    \ because of their beneﬁts of obtaining spectral information in the red-edge\n\
    and near-infrared band for vegetation applications in an extremely high resolution\
    \ (comparing to\navailable products from other platforms). Although the RGB cameras\
    \ are able to provide information\nrelated to the vegetation, e.g., the normalized\
    \ greenness indices [21,22] for vegetation analysis. Their\nspectral sensitivity\
    \ to the chlorophyll level of the vegetation is, however, limited for more sophistically\n\
    analysis such as plant health quantiﬁcation and disease detection. Near-infrared\
    \ cameras (e.g., Canon\nPowerShot SX260) can be used to derive vegetation indices\
    \ (VIs) such as Normalized Diﬀerence\nVegetation Index (NDVI) and others such\
    \ as Green Normalized Diﬀerence Vegetation Index (GNDVI)\nand Enhanced Normalized\
    \ Diﬀerence Vegetation Index (ENDVI) [23].\nThe multispectral cameras mounted\
    \ on a UAV may contain up to a few tenth of bands in addition\nto normal RGB bands.\
    \ Likewise, such multispectral cameras are still metric cameras by design and\
    \ thus\ncan be easily processed using photogrammetric methods to output standard\
    \ orthophotos and DSMs.\nA great beneﬁt for UAV-based multispectral sensors is\
    \ the yielded data with much higher resolution\n(better than 30 cm Ground Sampling\
    \ Distance (GSD)) that are normally not attainable in traditional\nmultispectral\
    \ RS. This may drive the novel applications seeing through more details in farming\
    \ and\nwater quality assessments, such as leaf level disease assessment [24] and\
    \ pads level Harmful algae\nbloom studies [25].\nDiﬀerent from RGB cameras, multispectral\
    \ cameras usually come with a higher cost due to the\nadditional hardware needed\
    \ for wiring additional bands to the RGB bands, and since the multispectral\n\
    cameras are mainly for professionals on vegetation and agriculture, the number\
    \ of available products\nare far fewer than RGB cameras. Additional barriers for\
    \ such cameras are the data format compatibilities\nwith capable software packages.\
    \ Since the market is still relatively small and various manufacturers\nare producing\
    \ multispectral cameras with images in diﬀerent formats, seamless processing software\n\
    packages, particularly handling data preprocessing (e.g., photogrammetrically),\
    \ are relatively limited\nto certain multispectral camera models at this point,\
    \ while there is, in general, a good trend that these\ndata are becoming standardized\
    \ and easier to deal with.\nRemote Sens. 2019, 11, 1443\n5 of 22\nTable 1. Overview\
    \ of unmanned aerial vehicle (UAV) sensors and examples of these sensors.\nCommon\
    \ and/or Example Camera and Its Spectral Range, Resolution, and Payload\nApplications\n\
    Beneﬁts and Obstacles in Practical Applications\nRGB cameras\nSony A9\n~400–700\
    \ nm\n24.2 MP\n588 g\nVisual analysis, mapping, land\ncover/land use, classiﬁcation,\n\
    pedestrians and vehicles detection\nand tracking, etc.\nCanon EOS 5D mark IV\n\
    ~400–700 nm\n30.4 MP\n~800 g\nNikon D850\n~400–700 nm\n45.7 MP\n915 g\nAdvantages:\
    \ (1) high availability in products ranging across diﬀerent\nlevels of cost, resolution,\
    \ and weight; (2) easy to be integrated in diﬀerent\nplatforms (3) well-modeled\
    \ camera geometry with a large number of\nsoftware solutions; and (4) videos.\n\
    Disadvantages: (1) Often come without radiometric/geometric calibration;\nand\
    \ (2) lack of spectral information for many tasks.\nLight-weight multispectral\n\
    cameras\nSentera Quad\nMultispectral Sensor\n~400–700 nm\n1.2 MP\n170 g\nVisual\
    \ analysis, vegetation detection\nand analysis, crop monitoring,\nmining,\nsoil\
    \ moisture estimation, ﬁres\ndetection, water level measurement,\nland cover/land\
    \ use mapping, etc.\nAdvantages: (1) wider spectrum range and narrower bandwidth;\
    \ (2) often\ncome with means of radiometric calibration; (3) most of the sensors\
    \ still\nfollow a perspective model that can be well-processed for geometric\n\
    reconstruction; and (4) allow for sub-decimeter multispectral mapping.\nDisadvantages:\
    \ (1) data format compatibility (sometimes 12 or 16-bit) for\nsoftware packages;\
    \ (2) as a component of a UAV system, its cost remains to\nbe relatively high;\
    \ (3) sensor compatibility to drones may be limited; and\n(4) videos may not be\
    \ available.\n~655 nm\n~725 nm\n~800 nm\nQuest Condor5-UAV\n400–1000 nm\n2048\
    \ × 1088 (2.2 MP)\n~1450–1950 g\nPhaseone iXU/iXU-RS\n1000 Aerial Cameras\n~400–700\
    \ nm\n100 MP\n1430–1700 g\nHyperspectral sensors\nRikola Hyperspectral\nCamera\n\
    500–900 nm\n1.05 MP\n<600 g\nResonon Pika NIR-640\n900–1700 nm\n640 pixels\n2700\
    \ g\nHigh-Eﬃciency\nHyperspec SWIR\n1000–2500 nm\n384 pixels\n4400 g\nLand cover/land\
    \ use mapping,\nvegetation indices estimation,\nbiophysical, physiological, or\n\
    biochemical parameters estimation,\nagriculture and vegetation disease\ndetection,\
    \ disaster damage\nassessment, etc.\nAdvantages: abundant spectral information,\
    \ 10 nm-level bandwidth for\nmore advanced applications in material identiﬁcation\
    \ and so on.\nDisadvantages: (1) high cost; (2) most of them are linear-array\
    \ and require\nspecialized software, and the users may take care of the data format\
    \ and\ngeometric corrections; (3) dimension reduction is needed for typical\n\
    classiﬁcation tasks; (4) sensor compatibility to drones may be limited; and\n\
    (5) videos may not be available.\nLight-weight thermal\ninfrared sensors\nFLIR\
    \ Vue Pro\n7.5–13.5 µm\n640 × 512 pixels\n72 g\nTracking creatures, volcanos\n\
    detection, forest ﬁre detection,\nhydrothermal studies, urban heat\nisland measurement,\
    \ etc.\nWorkswell WIRIS 640\n7.5–13.5 µm\n640 × 512 pixels\n<390 g\nYUNEEC CGOET\n\
    thermal imaging camera\nand low-light camera\n8–14 µm\n2.1 MP\n278 g\nAdvantages:\
    \ (1) well-targeted sensor for surface temperature\nmeasurement that drives a\
    \ lot of new applications; (2) the camera model is\nnormally perspective, and\
    \ relatively easy to be processed than linear-array\ncameras.\nDisadvantages:\
    \ (1) lack of texture information of its imageries brings\ndiﬃculties in 3D reconstruction\
    \ tasks; (2) for direct temperature\nmeasurement, it needs careful calibration;\
    \ (3) cost is relatively high\ncomparing to that of RGB cameras; (4) comparatively\
    \ lower resolution\nthan that of RGB cameras due to sensor design; (5) sensor\
    \ compatibility to\ndrones may be limited.\nUAV LiDAR\nRIEGL VUX-240\nNear\n-infrared\n\
    Up to 1,500,000 per second\n≤3800 g\nVegetation canopy analysis,\nestimation of\
    \ forest carbon\nabsorption, mapping cultural\nheritage, building information\n\
    modeling, etc.\nVelodyne Puck LITE\n903 nm\nUp to ~600,000 per second\n~590 g\n\
    Livox Mid-40\n905 nm\n100,000 per second\n760 g\nAdvantages: (1) direct geometric\
    \ measurement; (2) multiple returns of the\nsignals are useful for terrain modeling\
    \ under thin canopies.\nDisadvantages: (1) high equipment cost; (2) highly dependent\
    \ on\nexpensive onboard GPS/IMU measurement (potentially with external\nreference\
    \ stations); (3) increased payload for surveying quality LiDAR; (4)\nmay not work\
    \ in GPS-denied regions.\nRemote Sens. 2019, 11, 1443\n6 of 22\n2.3. Light-Weight\
    \ Hyperspectral Sensors\nHyperspectral cameras in RS are often very capable while\
    \ they are comparably less accessible due\nto their high cost and constraints\
    \ in sensor compatibility to drones. In order to capture images with\nhundreds\
    \ of narrow bands (5–10 nm bandwidth), most of the current light-weight hyperspectral\
    \ sensors\nare linear-array cameras [26,27]. Undoubtedly, the hyperspectral sensors\
    \ capturing such high volumes\nof information are extremely useful for many applications\
    \ [23,28–31], while the hyperspectral sensors\nby design have certain limitations:\
    \ (1) the high spectral resolution is at the expense of spatial resolution,\n\
    which is normally lower than that of RGB cameras with equivalent speciﬁcations\
    \ (payload level);\n(2) the linear-array sensory model, although it has been mathematically\
    \ well-interpreted, is practically\ncomplicated given limited meta-information\
    \ that sensor manufacturers oﬀer; and (3) light-weight\nhyperspectral cameras\
    \ normally come with half spectral ranges (400–1100 nm or 1100–2500 nm)\nas compared\
    \ with airborne hyperspectral cameras due to the payload limit, meaning if wider\n\
    spectral ranges are needed, two or more light-weight hyperspectral cameras might\
    \ be needed, either\nsimultaneously or sequentially (with diﬀerent ﬂights) [32].\
    \ Limitation (1) is not a critical issue for a\nUAV-based hyperspectral camera,\
    \ as with a certain eﬀort in ﬂight design (i.e., with a very low ﬂying\naltitude\
    \ and a long focal length) and at the expense of limited ground coverage, the\
    \ resolution of the\nacquired images can reach up to 2–5 cm levels or lower [33].\
    \ Limitation (2) is particularly problematic in\na UAV-based hyperspectral camera,\
    \ since diﬀerent manufacturers follow their own standards and very\noften they\
    \ only expect the users to use an undistorted image coarsely geo-referenced using\
    \ the onboard\nGPS/IMU information. This becomes technically complicated when\
    \ an expert uses the camera for\naccurate geo-referencing, such as bundle adjusting\
    \ the observations to have per scan-line orientations\nin order to project them\
    \ to a currently available high-accuracy product (e.g., DSM). The accurate\nregistration\
    \ among scans becomes necessary when there are multiple ﬂights acquiring information\
    \ of\ndiﬀerent spectral bands. The hyperspectral information, combined with other\
    \ modality data, such as\ndata obtained with the DSM, can be used for more accurate\
    \ classiﬁcation and plant phenology in\nagricultural applications with a much\
    \ higher spatial/temporal resolution [34].\n2.4. Light-Weight Thermal Infrared\
    \ Sensors\nAs one of the mid-infrared-range passive sensors (wavelength between\
    \ 3 and 35 µm) [35,36],\nthe thermal infrared sensors are broadly used in various\
    \ surface temperature and thermal emission\nmeasurements. The classical issue\
    \ on kinetic temperature and emissivity determination through the\nintensity and\
    \ its distribution over the wavelength region [36] of UAV-borne sensors can be\
    \ slightly\ndiﬀerent from the airborne or spaceborne thermal sensors. As for UAV-borne\
    \ sensors, the atmospheric\neﬀects are ignorable, lab-level calibration are more\
    \ accessible [37], and the temperature measurements\nare theoretically more accurate.\
    \ However, in consideration of the limited payload, light-weight thermal\ninfrared\
    \ sensors generally do not come with cooled detectors, thus resulting in lower\
    \ capture rates,\nlower spatial resolution and lower sensitivity as a compensation\
    \ to a reduced signal-to-noise ratio.\nThe resolution beneﬁt of the UAV-borne\
    \ sensor data brought by a low ﬂying altitude still increases\nthe capability\
    \ of thermal cameras for accurate quantiﬁcation of small objects such as human\
    \ [38],\nﬁre centers [39], and pipe-leaking detection [40]. Since temperature\
    \ is highly dynamic, the thermal\nsensors are frequently used for real-time detection\
    \ with a prior decision of the qualiﬁed capture\nrate. This, on the other hand,\
    \ could be useful in RS and mapping when being integrated with\nsensors acquiring\
    \ information from other spectrum ranges (i.e., visible bands and hyperspectral\n\
    bands), and thermal infrared data are also employed for various agricultural [41,42]\
    \ and environmental\napplications [43,44]. Examples include crop biophysical parameter\
    \ estimation for precision farming [41]\nand the use of UAV-based thermal camera\
    \ to estimate water evaporation in a much ﬁner spatial scale\nfor irritation and\
    \ water resource management [45].\nTheoretically, the geometric model of a thermal\
    \ camera is exactly the same as that of a normal\nperspective camera. On the other\
    \ hand, the thermal pictures normally come with much fewer textures\nthan the\
    \ RGB images [46], and a modern photogrammetry/structure from motion process will\
    \ likely\nRemote Sens. 2019, 11, 1443\n7 of 22\nfail due to the lack of interest\
    \ points. Therefore, when the geometric aspects of the camera are involved,\n\
    it is recommended to have an RGB camera calibrated and ﬁxed relative to the thermal\
    \ camera and take\nimages simultaneously for the use of recovering the poses of\
    \ the thermal camera.\n2.5. UAV LiDAR\nLiDAR sensors have been known as one of\
    \ the most accurate ways for geometric data acquisition.\nWidely used in forestry,\
    \ cultural heritage, and building information modeling (BIM), the airborne,\n\
    mobile, and terrestrial LiDAR nowadays have been well established in both the\
    \ academia and\nindustry. Their advantages over photogrammetry are their high\
    \ reliability and the ability to penetrate\nthin forests through multiple returns\
    \ [47]. However, as a sensor depending strongly on the direct\npositioning accuracy\
    \ of the host platform, UAV-borne LiDAR is rather rudimentary as compared\nto\
    \ UAV photogrammetry. The GPS/IMU sensors in a UAV platform are very often inaccurate\
    \ with\nrespect to the sensor resolution, and the platform is also more instable\
    \ when ﬂying. Thus, even with\nwell-calibrated light-weight LiDAR sensors, the\
    \ obtained point clouds accuracy is comparatively low.\nReported highly accurate\
    \ UAV-borne LiDAR systems are normally those coming with diﬀerential\nGPS stations\
    \ [48], where high-accuracy GPS measurements are obtainable [49] in addition to\
    \ highly\naccurate IMU measurements. On the other hand, an advantage of UAV-based\
    \ RS and mapping is their\nrelative low cost. LiDAR sensors, even those with relatively\
    \ low cost, are still order-of-magnitude\nhigher than RGB cameras and require\
    \ higher payloads (up to a few kilograms). Therefore, in terms\nof the cost and\
    \ needed sensors for integration, the UAV LiDAR system is not yet as accessible\
    \ as\nUAV-based photogrammetric mapping systems. Despite the needed consideration\
    \ of payload and cost,\npotentials for the use of both RGB and LiDAR sensors are\
    \ still very promising, since it has been shown\nalready that well-registered\
    \ RGB + LiDAR sensory data can readily improve both measurement and\ninterpretation\
    \ accuracy [50,51].\n3. UAVs Remote Sensing Data Analysis\nRS data acquired through\
    \ UAV platforms with their sensors are intended to be no diﬀerent\nfrom those\
    \ traditionally used in airborne and spaceborne RS sensors. While as per argued\
    \ in our\nﬁrst section in this manuscript, this UAV sensor data world embraces\
    \ higher likelihood of distinct\ncharacteristics: (1) UHR; (2) high availability\
    \ of geometric and spectral data; and (3) integrated\nsensor data for multi-dimensional\
    \ and multi-modal data analysis. These lead to completely diﬀerent\napplication\
    \ scenarios, data quality, and availability of diﬀerent dataset that call for\
    \ more targeted\nanalysis techniques. Many existing approaches to UAV-based RS\
    \ data processing either simply adopt\nor lightly reﬁne traditional analysis techniques.\
    \ We do agree that many of the existing methods that\nare used for RS analysis\
    \ are reusable and can be ﬁne-tuned to deal with UAV-based RS data, while\nthis\
    \ has not yet been systematically discussed particularly for UAV data. In this\
    \ section, we aim to\nclose this gap by discussing expected issues and existing\
    \ works on two topical applications typically\nused in traditional RS: A) ULC\
    \ mapping and B) change detection, with sensor data acquired from UAV\nplatforms.\
    \ A summary of the discussed topics and their characteristics are listed in Table\
    \ 2.\n3.1. Land-Use/Land-Cover (LULC) Mapping\nLULC mapping, even after many years\
    \ of research, is still not fully achieved via a standard\napproach with various\
    \ types of satellite images (spectral/spatial resolution). LULC mapping using\
    \ UAV\nimages is non-trivial as per observed by the resolution crisis: since the\
    \ 1970s, the spatial resolution of\nremotely sensed multispectral, panchromatic\
    \ images has increased dramatically [52], and the relevant\nprocessing methods\
    \ are ever challenged as the resolution moves to new eras [53]. It is a stretch\
    \ for\nresearchers to turn pixel-based methods to object-based methods as the\
    \ resolution (i.e., GSD) goes\nfrom tenths of meters to half a meter, and nowadays\
    \ problems associated with it are still in active\nresearch [54–56]. The UHR RS\
    \ data with centimeter-level GSD from UAV-borne sensors presents\nan equivalent\
    \ level of resolution increase [10,57–59]. Small objects and events that are normally\n\
    Remote Sens. 2019, 11, 1443\n8 of 22\nunobservable on conventional platforms are\
    \ now becoming objects of interest, such as pedestrians on\nthe street, cars,\
    \ infected plants [60], weed patches [61], and dustbins [62].\nTable 2. Algorithmic\
    \ considerations for data acquired using diﬀerent remote sensing platforms.\n\
    LULC Mapping\nChange Detection\nLow-to-moderate-resolution\nsatellite RS data\n\
    •\nPixel-based classiﬁcation;\n•\nData transformation such as principal\ncomponent\
    \ analysis or empirical\nobject indices;\n•\nOccasionally applied object-based\n\
    analysis for very large objects;\n•\nObjects can be approximated by\nsingle pixels;\n\
    •\nMixed pixel eﬀects.\n•\nPixel-based analysis approaches;\n•\nRadiometric normalization\
    \ or reﬂectance\ncomputations are essential;\n•\nObject-based analysis is occasionally\
    \ used for\nlarge objects;\n•\nPost-classiﬁcation methods.\nHigh-to-very high-resolution\n\
    satellite or airborne data\n•\nHigh inter-pixel similarity and high\nintra-pixel\
    \ variance;\n•\nPixel-based methods are still used;\n•\nObject-based methods with\n\
    textural features;\n•\nSemantic/contextual information can\nbe implemented;\n\
    •\nDeep learning-based methods are\nused for scene analysis.\n•\nCo-registration\
    \ of images is essential and\nalgorithms handling misalignment can be a key\n\
    for images with suboptimal camera networks\nfor dense matching;\n•\nObject-based\
    \ methods are primarily used, as it\nis slightly more robust to misalignment;\n\
    •\nShape/textural features are important for\ndiﬀerence analysis;\n•\n3D change\
    \ detection can be applied for stereo\ndata, while stereo data might not often\n\
    be available.\nUltra-high-resolution\nUAV-borne data\n•\nVery high inter-pixel\
    \ similarity and\nhigh intra-pixel variance;\n•\nObject-based analysis is essential\
    \ and\nSuper-pixel based methods are\noften used;\n•\nThe need for fusing 3D information\n\
    such as height, geometric and oblique\ninformation for remote\nsensing analysis;\n\
    •\nContextual information and deep\nlearning methods are essential for\naccuracy\
    \ improvement.\n•\nData co-registration is less problematic as most\nof the UAV\
    \ data are photogrammetrically\nacquired and comes with associated 3D\ninformation\
    \ by applying rigorous multi-view\nmatching methods;\n•\nThe need for analyzing\
    \ the 3D uncertainty for\ngeometric comparison;\n•\nThe need for fusing 3D and\
    \ spectral\ninformation for change determination;\n•\nContextual information and\
    \ deep learning\nmethods are essential for\naccuracy improvement.\nIt has been\
    \ reported that for UAV-based RS data, a lower spatial resolution may provide\
    \ the best\nresults using a traditional pixel-based classiﬁcation method and the\
    \ classiﬁcation accuracy will decrease\nas the resolution increases [61,63]. The\
    \ increased spatial resolution provides information with a great\nlevel of detail,\
    \ but does not necessarily oﬀer the same level of improvement in terms of classiﬁcation\n\
    accuracy for traditional methods, as this leads to much higher within class variation\
    \ and inter-class\nsimilarities [64,65]. As compared to traditional pixel-based\
    \ LULC classiﬁcation, researchers working\nwith UAVs imagery are more inclined\
    \ to use Object-Based Image Analysis (OBIA) [66] methods.\nThe idea of OBIA is\
    \ to aggregate information through coherent and spatially connected pixels into\n\
    diverse image segments, where analyses are performed through these segments, which\
    \ additionally\nbring the beneﬁt of the shape information. With tunable parameters\
    \ (usually called scale, varying\nwith the segmentation algorithms) determining\
    \ the granularities of the segments [14,67–71]. Although\nchoosing the optimal\
    \ scale for pixels aggregation still remains a challenging problem [72–74], OBIA\
    \ has\nachieved a great success in dealing with high resolution (HR, 2–30 m GSD)\
    \ and Very High Resolution\n(VHR, 0.3–2 m GSD) imageries for ﬁltering out the\
    \ trivial and noisy information which can jeopardize\nthe interpretation results\
    \ and model the human’s hierarchical visual cognitions process facilitating\n\
    high-level reasoning [75]. By aggregating appearance coherent and spatial connected\
    \ pixels, an image\nobject (patch) can provide much more robust features than\
    \ the original ones [67].\nIn HR and VHR image processing, texture features have\
    \ been widely used to improve LULC\nclassiﬁcation accuracy [76,77], such as grey-level\
    \ concurrent matrixes (GLCMs) [78] and local binary\npatterns (LBPs) [79–81].\
    \ Most of the existing texture descriptors used for VHR imagery could be directly\n\
    applied to UHR images [82,83]. The geometric information from data such as digital\
    \ elevation models\nRemote Sens. 2019, 11, 1443\n9 of 22\n(DEMs) from multi-view/stereo-view\
    \ images [84,85], LiDAR data [86,87], or vector data [88] has also\nbeen proven\
    \ to be of great value for land-cover classiﬁcation and object recognition [89].\
    \ Guo et al. [90]\nrevealed in their classiﬁcation experiments that the relative\
    \ height plays a signiﬁcant role in classiﬁcation\naccuracy and the best set of\
    \ features should be a combination of both geometric and spectral information.\n\
    Nowadays, getting a DSM out of UAV photogrammetry data can be as easy as a simple\
    \ button click\nwith a capable software [91,92], and this is achieved through\
    \ advanced photogrammetry, structure\nfrom motion [93], and highly eﬃcient stereo-matching\
    \ algorithm. In addition, pixel-wise co-registered\nDSMs and orthophotos provide\
    \ another dimensional information and are proven to be particularly\neﬀective\
    \ in boosting the land-cover classiﬁcation accuracy [94]. The best overall accuracy\
    \ improvement\nreported may reach up to 30% [95]. Given that DSMs are raster representations,\
    \ many existing feature\nextraction algorithms can be directly applied to extract\
    \ useful information. Often, such extracted\nfeatures come with geometric interpretation;\
    \ for example, the morphological top-hat operators [7]\nextracting closed blobs\
    \ [96] can be seen as an eﬀective way to represent oﬀ-terrain objects such as\n\
    buildings, trees, and cars. Such information can largely solve the spectral ambiguities.\n\
    As the resolution of the UAV-based RS data has increased to an unprecedented level,\
    \ an OBIA\noften shows diﬃculties in getting the appropriate segmentation due\
    \ to the fact that the scale diﬀerences\namong diﬀerent/same objects are signiﬁcant\
    \ and simple shape or texture features are no longer\nsuﬃciently powerful to distinguish\
    \ them. A strategy to handle such UHR data is to adopt multi-scale\napproaches,\
    \ either on the feature level [95] or the image level [97]. The feature-level\
    \ approaches\noften take the feature proﬁles extracted with diﬀerent scale parameters\
    \ and a classiﬁer takes such\nproﬁles for classiﬁcation [95]. The image-level\
    \ approaches build a pyramid of images and then\nprogressively classify images\
    \ through the pyramid; the very prominent deep convolutional neural\nnetworks\
    \ implicitly use similar concepts by spatially correlating pixels through multi-resolution\
    \ feature\nmaps. Another branch of the idea that might aim to close such gaps\
    \ is to use a hierarchical way for\nsegmenting the images prior to classiﬁcation.\
    \ A representative example is to use superpixels [98] to\nﬁrstly over-segment\
    \ the image into small-granular segments and strategically merge them through\n\
    spectrum and/or texture analysis [99].\nAs the scene contents get complex, simple\
    \ classiﬁers, such as a single-pass support vector\nmachine (SVM) [100], random\
    \ forest [101], or maximum likelihood [102] classiﬁers, may not\nbe suﬃcient.\n\
    More advanced classiﬁers, including an ensemble of simpler classiﬁers [103,104]\n\
    or deep learning-based methods [105,106], still present a huge potential to be\
    \ explored for this\nultra-high-resolution, multi-modal data (height, thermal,\
    \ and hyperspectral information). The fully\nconvolutional network (FCN) is inherently\
    \ applicable to pixel-wise remotely sensed image classiﬁcation.\nHowever, the\
    \ down-sampling layers tend to produce round corners and smoothed edges while\n\
    increasing the receptive ﬁelds to integrate contextual information. By taking\
    \ into account the features\nfrom multiple layers, methods such as U-Net [107]\
    \ can achieve good localization and utilize contextual\ninformation at the same\
    \ time. Adopting architectures with dilated convolutions can increase the ﬁeld\n\
    of view without the need of down-sampling [108]. The spatial information loss\
    \ during the convolution\nprocess is another reason for the lack of details in\
    \ labeled imageries. Bergado et al. introduced skip\nconnections into their FuseNet\
    \ and achieved increased accuracy by recovering high spatial details [105].\n\
    Liu et al. progressively reﬁned ﬁne-structured objects using low-level features\
    \ maps learned by shallow\nlayers [109]. Using post-classiﬁcation spatial regulation\
    \ is another way to achieve higher classiﬁcation\naccuracy: Mboga et al. reﬁned\
    \ an FCN-classiﬁed map by majority voting in geographic image\nobjects [106] and\
    \ Marmanis et al. proposed an ensemble of semantic segmentation with semantically\n\
    informed edges detected by a modiﬁed holistically nested edge detection (HED)\
    \ [110]. Another new\nworth-noting architecture is Pyramid Scene Parsing Network\
    \ (PsPNet): based on residual networks, it\ntakes local and global contextual\
    \ information to perform a more reliable prediction of pixel’s label [111].\n\
    For multi-modal data integration, deep convolution neural networks also show their\
    \ capacity in both\nfeature level fusion [112] and decision level fusion [113]\
    \ for increased classiﬁcation accuracy.\nRemote Sens. 2019, 11, 1443\n10 of 22\n\
    In addition, transfer learning techniques [114] are becoming more needed as the\
    \ traditional\nsample-per-dataset learning in large-format satellite images are\
    \ no longer suitable for local scale,\nas datasets might be diverse and small\
    \ in terms of geographical regions [115]. Penatti et al. have\nshown experimentally\
    \ that deep features obtained by training on everyday objects can be used for\n\
    the classiﬁcation of aerial images [116]. Minimizing domain shift in the pixel\
    \ space also presents as a\npromising way to deal with the varying imaging conditions\
    \ of UAVs-based RS [117].\n3.2. Change Detection\nChange detection, as a very\
    \ important topical application in RS, may see many opportunities\nand unexplored\
    \ applications looking at much ﬁner spatial scales, such as the detection of illegal\n\
    waste dumping, street facility misplacement, and crowd anomaly detection [118–123].\
    \ The obvious\nadvantages of having higher spatial resolution and availability\
    \ of geometric information (i.e., DSM) are\nthe ability to detect changes of objects\
    \ in a ﬁner scale with higher accuracy. Its ﬂexibility of mounting\ndiﬀerent sensors\
    \ as well as the minimal ready-to-ﬂy logistics can facilitate a much higher temporal\n\
    data acquisition, such as daily, hourly, even real-time monitoring using video\
    \ streams [124] as well as\nthe higher accessibility to non-convention photos\
    \ such as multispectral and hyperspectral data for\ncivilian use.\nTemporal data\
    \ registration is probably one of the biggest issues for close-range UAV images\
    \ due\nto the relatively large perspective eﬀects, while this seeming disadvantage\
    \ is readily solvable to a large\nextent as long as the images are taken in a\
    \ photogrammetric fashion, and thereby DSMs and orthophotos\ngenerations are seamless.\
    \ Ground control points (GCPs) or general bundle adjustment techniques [125]\n\
    (incorporating known exterior parameters from diﬀerent temporal data into bundle\
    \ adjustment to set\ndatum, or from a high-accuracy real-time kinematic (RTK)\
    \ system with known uncertainty) can be\nused to address the registration issue\
    \ and yield sub-pixel-aligned temporal orthophotos and DSMs for\nchange detection.\n\
    The DSMs play a more important role in determining the changes than the orthophotos\
    \ as the\nillumination changes are expected to be rather signiﬁcant due to the\
    \ more complex scene contents.\nSimilar to the general land-cover classiﬁcation\
    \ problem, the UHR data bring up more unwanted\nchanges that are usually ignorable\
    \ in typical aerial and satellite RS data, such as standing pedestrians\nand cars.\
    \ Given that the interests in change detection are the temporally changed objects,\
    \ the resolution\ncrisis is minor as compared to land-cover classiﬁcation problems.\
    \ However, if semantic meanings of\nthe changed objects are needed, the classiﬁcation\
    \ still needs to be performed either on the diﬀerence\nmaps or independently on\
    \ each temporal dataset [126]. From a data point of view, the most applicable\n\
    scenario would be the 3D change detection. As per introduced in the review paper\
    \ [127], the 3D\nchange detection in general consists of three processing steps:\
    \ (1) data selection/acquisition; (2) data\nco-registration; and (3) change analysis.\
    \ Items (1) and (2) are addressable for UAV data and item (3) for\nUAVs is primarily\
    \ for geometric or image-aided geometric analysis, where object-based methods\
    \ are\nneeded to analyze changes when dealing with “salt-and-pepper” noises [128–131].\n\
    The enabling applications for change detection using UAV data are with the obvious\
    \ advantages of\nspatial-temporal resolution and cloudlessness as compared to\
    \ the satellite RS images, while presenting\nas well the limitation for mapping\
    \ a large region. When in applications the spectral information of the\nUAV dataset\
    \ is needed for analyzing the changes, it should be noted that there might exist\
    \ a much\nhigher spectral in-class variance, due to either the complexity of the\
    \ scenes and the diverse imaging\nconditions like illumination and shadows (diﬀerent\
    \ times of a day). Therefore, it is still with a certain\nconstraint that the\
    \ acquired images are better under similar external conditions.\nMost of the methods\
    \ developed so far are not particularly for UAV datasets, but in general the\n\
    object-based change detection methods [53] can be strategically applied by carefully\
    \ dealing with the\nDSMs uncertainty and image spectrum heterogeneities. To perform\
    \ the change detection using UAV\ndata, typically the changes should be analyzed\
    \ through certain units using segmentation techniques.\nWith well-overlaid multi-temporal\
    \ images, criteria in determining changes through layer arithmetic,\nRemote Sens.\
    \ 2019, 11, 1443\n11 of 22\npost-classiﬁcation, direction classiﬁcation, or change\
    \ vector analysis (CVA) are found in Reference [132].\nMore detailed reviews in\
    \ change detection techniques can be found in References [127,133].\n4. UAVs Remote\
    \ Sensing Applications\nThe European Commission listed a set of civil and commercial\
    \ applications of UAVs and\ncategorized them as (a) Government (civil security,\
    \ border security, and coastguard); (b) Fire\nFighting (forest ﬁre spotting and\
    \ coordination, major incident response co-ordination, and emergency\nrescue);\
    \ (c) Energy Sector (oil and gas industry distribution infrastructure, electricity\
    \ grids,\nand distribution networks); (d) Agriculture Forestry and Fisheries (environmental\
    \ monitoring, crop\ndusting, and optimizing use of resources); (e) Earth Observation\
    \ and RS (climate monitoring, aerial\nphotography, mapping and surveying, seismic\
    \ events, major incident, and pollution monitoring);\n(f) Communications and Broadcasting\
    \ (Very High Altitude, Long-Endurance (VHALE) platforms\nas proxy satellites,\
    \ Medium-Altitude Long-Endurance (MALE) UAVs for communication coverage,\nand\
    \ camera platforms) [134]. In this section, to show a comprehensive utilization\
    \ of the “new” aspects\nof UAV-based RS, we give examples of UAV-based RS applications\
    \ under the following umbrellas:\n(1) precision agriculture and vegetation (part\
    \ of “d”); (2) urban environment and management (part\nof “e”); (3) disaster,\
    \ hazard, and rescue (part of “a”). Selected applications of these categories\
    \ can be\nfound in Table 3.\nTable 3. An Overview of the selected UAVs remote\
    \ sensing applications.\nSelected Applications\nHighlights\nPrecision agriculture\
    \ and\nvegetation\nSoil property estimation [135];\ncrop/vegetation management\
    \ [136,137];\nforest structure assessment [138].\n•\nEasily operated platforms;\n\
    •\nHigh-spatiotemporal-resolution sensor data;\n•\nLess impact of atmospheric\
    \ factors;\n•\nFeasible access to high-resolution 3D structure\nof vegetations.\n\
    Urban environment and\nmanagement\nTraﬃc control [139]; urban infrastructure\n\
    management [140]; building observation\n[141]; urban environment mapping [142].\n\
    •\nReal-time monitoring of high dynamic objects;\n•\nHigh visibility;\n•\nHigher\
    \ redundancy & reliability;\n•\nEasily acquired 3D models of urban objects.\n\
    Disaster hazard and rescue\nPost-disaster assessment [143,144];\nemergency responses\
    \ [145]; ﬁre\nsurveillance [146]; landslide dynamic\nmonitoring [147,148]; coastal\
    \ vulnerability\nassessment [149,150]\n•\nSafer and lower-cost than in situ measurements;\n\
    •\nQuick response;\n•\nIntegrated\nsensor\ndata\nbring\nmore\neﬀective\ninterpretation.\n\
    4.1. Precision Agriculture and Vegetation\nPrecision agriculture requires mapping\
    \ the spatial variability of as many variables as can be\nmeasured (e.g., crop\
    \ yields, terrain features/topography, organic matter contents, and moisture levels)\
    \ as\nthe input of decision support system for farm management [135,151]. For\
    \ this reason, RS techniques are\nwidely used in agriculture and agronomy [152].\
    \ Due to the fact that the variables aﬀecting productivity\nare highly variable\
    \ in space and time, recent papers show a trend of adopting UAVs by researchers\n\
    and farmers to monitor their ﬁelds because of their high operability, which gives\
    \ an unprecedented\nperspective of ultra-high spatial and temporal resolution\
    \ and free of cloud occlusion as well. Moreover,\ngiven its economic eﬃciency\
    \ with ﬂight, time-series animation which reveals the change of the crop\ncan\
    \ be easily obtained with a minimum interval of several hours [136]. Opportunities\
    \ are also found in\nnew vegetation analysis applications at ﬁner scales such\
    \ as mapping, detection, and change monitoring\non a tree level.\nGenerating crop\
    \ or vegetation maps with high accuracy is critical to many tasks such as biomass\n\
    estimation, yield prediction, and crop infestation monitoring. To inspect the\
    \ crop status, Sugiura et al.\ndeveloped a system mounting an imaging sensor on\
    \ an unmanned helicopter for precisely mapping the\ncrop information with spatial\
    \ resolutions of 1.8 cm and 4.1 cm, taken from altitudes of 30 m and 70 m\nrespectively.\
    \ Image orientation distortions caused by a variation of the helicopter posture\
    \ is removed\nRemote Sens. 2019, 11, 1443\n12 of 22\nunder the assistance of a\
    \ real-time kinematic global positioning system (RTK-GPS) and an inertial\nsensor\
    \ (INS), which helps gain a reduced spatial error of 38 cm [137]. A successful\
    \ vegetation analysis\napplication of classiﬁcation in the riparian ﬁeld indicating\
    \ standing dead wood and canopy mortality\nwas reported by Dunford et al. using\
    \ object-oriented approaches at 6.8–21.8 cm GSD; this work also\nshowed that major\
    \ constraints of vegetation mapping with UAVs come from the variations in spatial\n\
    resolution and radiometry [153].\nA number of RS vegetation indices (VIs) were\
    \ developed in the past for retrieving biophysical status\n(e.g., water content,\
    \ pigments, sugar and carbohydrate contents, protein content, and abiotic/biotic\n\
    stress levels) of crops or trees. Among them, the most popular ones are NDVI,\
    \ Atmospherically\nResistant Vegetation Index (ARVI), Optimized Soil-Adjusted\
    \ Vegetation Index (OSAVI), Soil Brightness\nIndex (SBI), Green Vegetation Index\
    \ (GVI), and Yellow Vegetation Index (YVI). A great number of\nresearches have\
    \ revealed the correlation between NDVI and Leaf Area Index (LAI) with diﬀerent\n\
    imaging conditions, sites and seasons, etc. [154–157]. Most of these indices are\
    \ based on the mixture of\nvisible bands and the near-infrared (NIR) band, except\
    \ for rare cases in which only visible spectrums\nare considered [22,158]. Due\
    \ to the minimal impact of atmospheric factors during the data acquisition,\n\
    a number of VIs considering atmospheric eﬀects are not necessary for UAVs. Although\
    \ light-weight\nhigh-deﬁnition cameras containing the NIR band are available,\
    \ most of the oﬀ-the-shelf UAVs still\nmount only cameras working on visible bands\
    \ for the derived indices [159].\nThe feasibility of acquiring high-resolution\
    \ 3D structure of crops and trees with LiDAR or optical\ncameras on UAVs has been\
    \ investigated in recent years. For instance, Wallace et al. investigated the\n\
    potentials of UAV to measure the structural properties of forests by comparing\
    \ two diﬀerent methods\n(airborne laser scanning (ALS) and structure from motion\
    \ (SFM)) to obtain absolute terrain height\nand the canopy information. The results\
    \ indicate that both techniques are suitable for relatively low\ncanopy, while\
    \ ALS performs better than SFM/photogrammetry in capturing denser canopy covers\
    \ [138].\n3D information is useful in monitoring crops or trees because they demonstrate\
    \ a temporal 3D structural\nvariance accompanying growth, and the UHR data and\
    \ capability of ﬂexible revisiting of UAV allow for\nprecisely recording the temporal\
    \ changes in a short-time interval. For example, Bendig et al. produced\nmulti-temporal\
    \ crop surface models during its growing season, and then the derivate height\
    \ diﬀerence\nwas used to indicate the growth of cultivars [160]. Dong et al. proposed\
    \ a 4D (4 dimensional) crop\nmonitoring technique based on spatio-temporal reconstruction\
    \ by adopting a robust data association\nalgorithm which introduces single-row\
    \ reconstruction results as a starting point for data association\nacross rows\
    \ and times [161].\n4.2. Urban Environment and Management\nIt has been reported\
    \ that the urban population will take 66 percent of the total population on\n\
    the planet by 2050 [162]. Observed at ﬁne scales, the urban environment is highly\
    \ dynamic due to\nthe human activities, which produces the desire for various\
    \ challenging UAVs urban applications\nincluding real-time traﬃc control [139],\
    \ management of urban infrastructures [140], and building\nobservation [141].\
    \ For example, the status of paved roads like depression and crack is critical\
    \ to driving\nsafety and fuel consumption, which requires frequent inspections\
    \ at high spatial resolution since the\ndefects’ sizes are usually as small as\
    \ dozens of centimeters. However, a traditional in situ check or the\nuse of terrestrial\
    \ vehicles for detecting the damage of roads may incur high cost, or even enduring\n\
    safety risks. In this regard, UAV-based RS might be a good substitute for this\
    \ issue due to the high\nﬂexibility and availability of geometric and spectral\
    \ data at UHR.\nBranco and Segantine proposed a methodology to automatically capture\
    \ asphalt road pavement\nconditions with UAVs imageries at a spatial resolution\
    \ of 4 cm, where radiometric preprocessing\nfollowed by machine learning algorithms\
    \ is adopted to detect defects [163], and in such an application,\nthe 3D structure\
    \ itself can be particularly useful in determining the geometric distortions of\
    \ the\nroad [164]. Similar practices were performed by Phung et al. [165] to detect\
    \ cracks of buildings. In their\nwork, to ensure the information on the building\
    \ was fully captured, they ﬁrst created a coarse model\nRemote Sens. 2019, 11,\
    \ 1443\n13 of 22\nfor ﬂight path planning, and then a simple threshold-based method\
    \ was used to identify potential\ncracks with their positions.\nA natural advantage\
    \ of UAV in use for dynamic scene analysis is its high spatial-temporal\nresolution,\
    \ and thus the enabled applications can be readily extended to smaller objects\
    \ with high levels\nof details. Eﬀorts in such dynamic scene analysis include\
    \ accurate mapping urban vegetation and\nimpervious surfaces using combined high-resolution\
    \ DSMs and orthophotos [78,142], with improved\nland-cover classiﬁcation [7].\
    \ The possibility of acquiring data in a much higher frequency leads to work\n\
    focusing on small objects with faster dynamics; for example, Qin [62] detected\
    \ the geometric changes of\nbuildings and small public facilities, as well as\
    \ signiﬁcant changes of tree canopies in urban scenarios\nusing UAVs imageries.\
    \ Although these existing works primarily use normal RGB cameras, we expect\n\
    work using multispectral or hyperspectral cameras as a means of assessing temporal\
    \ dynamics, such as\nmonitoring of crop diseases and urban heat islands, is becoming\
    \ more viable [37,166–168].\n4.3. Disaster, Hazard, and Rescue\nRS is seen as\
    \ an important tool for risk assessment and rescue operations [169,170]. Low-cost\
    \ UAVs\nare now indispensable for onsite rapid data collection in aid of disaster\
    \ management [143,145,171],\nsuch as mapping, monitoring, and autonomous deployment\
    \ of ﬂying robots [146].\nAn often-reported use-case scenario in UAV-based disaster\
    \ management is a post-seismic building\ndamage assessment, where with the capability\
    \ of UAV in acquiring site data processed as information\nsuch as the area, amount,\
    \ rate, and type of the damage, the rescue teams will be better informed of\n\
    safe paths and potential corruptions due to secondary shocks. 3D building models\
    \ of the sites can be\nreconstructed through either UAV-based mapping or LiDAR\
    \ point clouds [144,172], which can be used\nby experts to identify total collapses,\
    \ partial collapses, and high-risk structures. Such identiﬁcations\ncan be further\
    \ automated through unsupervised classiﬁcation or 3D change detection techniques\n\
    (when before and after images are available) [173]. The high accuracy nature of\
    \ the UAV-derived\ntopographic data (DSM) can be now readily used at a signiﬁcantly\
    \ lower cost in disaster monitoring\nand analysis. Examples include its use in\
    \ landslide dynamic monitoring [147,148] and change detection\non coastal facilities\
    \ in aid of vulnerability assessment due to nature disasters such as tornados\
    \ [149,150].\nThe DSM or DEM acquired using UAV platforms can reach up to centimeter\
    \ levels and can be well\ngeo-referenced or co-registered. A good advantage of\
    \ using these high-resolution high-accuracy data\nis that the displacement can\
    \ be performed in simultaneously in horizontal and vertical directions,\nas compared\
    \ to image-based (in horizontal direction) [174] and Interferometry Synthetic\
    \ Aperture\nRadar (InSAR)-based displacement analysis (in a vertical direction)\
    \ [175].\n5. Conclusions and Future Trends\nIn this work, we provided an overview\
    \ of the UAVs RS data processing and their speciﬁc\napplications. The “new” aspects\
    \ brought by UAV-based RS lie in their: (1) UHR; (2) high availability\nof geometric\
    \ data (i.e., DSM); and (3) ﬂexibility in multi-sensor integration. We have presented\n\
    how these new aspects should be taken into consideration when processing UAV-based\
    \ RS on two\ntypical RS applications: land-cover classiﬁcation and change detection.\
    \ Novel and a diverse set of\nspeciﬁc applications associated with UAV sensor\
    \ data were introduced. When it comes to processing\nUAV-based images, there in\
    \ general exist high in-class appearance variances and uncertainties of the\n\
    DSM information. More advanced data-processing methods include hierarchical image\
    \ segmentation\nand deep learning-based classiﬁcation. In addition, as the UAV\
    \ data tends to characterize local-scale\nscenarios that may dramatically vary,\
    \ available training data to fuel successful machine learning\napplications can\
    \ be a challenge, and thus the transfer learning techniques being able to utilize\n\
    information from labeled datasets can be critical.\nThe characteristics of UAV\
    \ with low ﬂying altitude, low cost and high ﬂexibility provide\nnew opportunities\
    \ to RS applications in various areas with high-spatial-resolution, high-frequency,\n\
    and multi-source data. To take full advantage of these characteristics of UAVs\
    \ data, new methods have\nRemote Sens. 2019, 11, 1443\n14 of 22\nbeen proposed\
    \ in past few years, where techniques using geometric structures of scenes in\
    \ the form of\n3D information may serve as a starting point in UAVs imageries\
    \ processing, as such 3D data in most of\nthe time are naturally available. In\
    \ addition, a review of a few and non-inclusive relevant applications\non precision\
    \ agriculture and vegetation, urban environment and management, and disaster,\
    \ hazard,\nand rescue as the new and developing areas of applications that can\
    \ greatly beneﬁt from improved\nUAV-based data processing techniques were provided.\
    \ As the users have higher ﬂexibilities to\ndesign and practice with diﬀerent\
    \ ﬂying parameters, platforms, and resolutions, benchmarking the\nachievable geometric\
    \ and classiﬁcation may require standardized datasets. In addition, given the\n\
    low ﬂying altitude and nature of UHR applications (high level of detail desired),\
    \ the acquired data\nlikely introduces more information on the facades of objects,\
    \ and thus accuracy assessments in a true\n3D/volumetric scenario might be considered\
    \ as more appropriate.\nDespite the fact that UAVs platforms and onboard sensors\
    \ are easily accessible and have been\nwidely and successfully used in various\
    \ areas, more work on its data processing is still much in\nneed, such as multi-modal\
    \ data analysis (combined geometric and textures) and object tracking in\nclutters,\
    \ as with the available UAV data we are moving our observations from the static\
    \ object level\nto dynamic object level (e.g., cars and individual pedestrians),\
    \ where traditional techniques need to\nbe upgraded to more advanced methods such\
    \ as deep convolutional or graphic neural networks to\ndeal with multi-modality\
    \ data for accurate object interpretation. On the other hand, performing an\n\
    RS analysis using UAV consists of joint eﬀorts in both data acquisition and processing,\
    \ while work\nanalyzing the uncertainties associated with this two-step processes\
    \ as well as work standardizing\nachievable interpretation results (classiﬁcation\
    \ and change detection) in a controlled way is still missing,\nwhich may greatly\
    \ inform researchers in the community when adopting UAV-based RS methods for\n\
    relevant applications.\nAuthor Contributions: All authors contributed to this\
    \ paper. Huang Yao performed the literature survey and\nparticipated in the writing.\
    \ Rongjun Qin performed the major writing of the manuscript and the ﬁnal editorial\n\
    adjustments. Xiaoyu Chen provided suggestions and participated in discussions\
    \ that improved the quality of\nthe paper.\nFunding: This research received no\
    \ external funding.\nAcknowledgments: This work was established at the Geospatial\
    \ Data Analytics (GDA) group in the Department\nof Civil, Environmental and Geodetic\
    \ Engineering (CEGE) at the Ohio State University (OSU). Huang Yao was a\nvisiting\
    \ scholar of the CEGE department at OSU while performing this work.\nDisclaimer:\
    \ Mention of brand names in this paper does not constitute an endorsement by the\
    \ authors.\nConﬂicts of Interest: The authors declare no conﬂicts of interest.\n\
    References\n1.\nPajares, G. Overview and current status of remote sensing applications\
    \ based on unmanned aerial vehicles\n(UAVs). Photogramm. Eng. Remote Sens. 2015,\
    \ 81, 281–329. [CrossRef]\n2.\nNex, F.; Remondino, F. UAV for 3D mapping applications:\
    \ A review. Appl. Geomat. 2014, 6, 1–15. [CrossRef]\n3.\nBhardwaj, A.; Sam, L.;\
    \ Martín-Torres, F.J.; Kumar, R. UAVs as remote sensing platform in glaciology:\
    \ Present\napplications and future prospects. Remote Sens. Environ. 2016, 175,\
    \ 196–204. [CrossRef]\n4.\nTorresan, C.; Berton, A.; Carotenuto, F.; Di Gennaro,\
    \ S.F.; Gioli, B.; Matese, A.; Miglietta, F.; Vagnoli, C.;\nZaldei, A.; Wallace,\
    \ L. Forestry applications of UAVs in Europe: A review. Int. J. Remote Sens. 2017,\
    \ 38,\n2427–2447. [CrossRef]\n5.\nCrommelinck, S.; Bennett, R.; Gerke, M.; Nex,\
    \ F.; Yang, M.Y.; Vosselman, G. Review of automatic feature\nextraction from high-resolution\
    \ optical sensor data for UAV-based cadastral mapping. Remote Sens. 2016, 8,\n\
    689. [CrossRef]\n6.\nYan, W.Y.; Shaker, A.; El-Ashmawy, N. Urban land cover classiﬁcation\
    \ using airborne LiDAR data: A review.\nRemote Sens. Environ. 2015, 158, 295–310.\
    \ [CrossRef]\n7.\nZhang, Q.; Qin, R.; Huang, X.; Fang, Y.; Liu, L. Classiﬁcation\
    \ of ultra-high resolution orthophotos combined\nwith DSM using a dual morphological\
    \ top hat proﬁle. Remote Sens. 2015, 7, 16422–16440. [CrossRef]\nRemote Sens.\
    \ 2019, 11, 1443\n15 of 22\n8.\nMoranduzzo, T.; Melgani, F. Detecting cars in\
    \ UAV images with a catalog-based approach. IEEE Trans. Geosci.\nRemote Sens.\
    \ 2014, 52, 6356–6367. [CrossRef]\n9.\nRodríguez-Canosa, G.R.; Thomas, S.; Del\
    \ Cerro, J.; Barrientos, A.; MacDonald, B. A real-time method to\ndetect and track\
    \ moving objects (DATMO) from unmanned aerial vehicles (UAVs) using a single camera.\n\
    Remote Sens. 2012, 4, 1090–1111. [CrossRef]\n10.\nColomina, I.; Molina, P. Unmanned\
    \ aerial systems for photogrammetry and remote sensing: A review.\nISPRS J. Photogramm.\
    \ Remote Sens. 2014, 92, 79–97. [CrossRef]\n11.\nKorchenko, A.G.; Illyash, O.S.\
    \ The generalized classiﬁcation of Unmanned Air Vehicles. In Proceedings\nof the\
    \ 2013 IEEE 2nd International Conference Actual Problems of Unmanned Air Vehicles\
    \ Developments\nProceedings (APUAVD), Kiev, Ukraine, 15–17 October 2013; pp. 28–34.\n\
    12.\nDalamagkidis, K. Classiﬁcation of UAVs.\nIn Handbook of Unmanned Aerial Vehicles;\
    \ Valavanis, K.P.,\nVachtsevanos, G.J., Eds.; Springer: Dordrecht, The Netherlands,\
    \ 2015; pp. 83–91. [CrossRef]\n13.\nDepartment of Defense of USA; Oﬃce of the\
    \ Secretary of Defense. Army Roadmap for Unmanned Aircraft\nSystems, 2010–2035;\
    \ U.S. Army UAS Center of Excellence and Fort Rucker: Dale County, AL, USA, 2010.\n\
    14.\nZhang, L.; Huang, X.; Huang, B.; Li, P. A pixel shape index coupled with\
    \ spectral information for classiﬁcation\nof high spatial resolution remotely\
    \ sensed imagery. IEEE Trans. Geosci. Remote Sens. 2006, 44, 2950–2961.\n[CrossRef]\n\
    15.\nWoodcock, C.E.; Strahler, A.H. The factor of scale in remote sensing. Remote\
    \ Sens. Environ. 1987, 21, 311–332.\n[CrossRef]\n16.\nWestoby, M.; Brasington,\
    \ J.; Glasser, N.; Hambrey, M.; Reynolds, J. ‘Structure-from-Motion’photogrammetry:\n\
    A low-cost, eﬀective tool for geoscience applications. Geomorphology 2012, 179,\
    \ 300–314. [CrossRef]\n17.\nTurner, D.; Lucieer, A.; Wallace, L. Direct georeferencing\
    \ of ultrahigh-resolution UAV imagery. IEEE Trans.\nGeosci. Remote Sens. 2014,\
    \ 52, 2738–2745. [CrossRef]\n18.\nYang, B.; Chen, C. Automatic registration of\
    \ UAV-borne sequent images and LiDAR data. ISPRS J. Photogramm.\nRemote Sens.\
    \ 2015, 101, 262–274. [CrossRef]\n19.\nSegl, K.; Roessner, S.; Heiden, U.; Kaufmann,\
    \ H. Fusion of spectral and shape features for identiﬁcation of\nurban surface\
    \ cover types using reﬂective and thermal hyperspectral data. ISPRS J. Photogramm.\
    \ Remote\nSens. 2003, 58, 99–112. [CrossRef]\n20.\nEisenbeiß, H. UAV Photogrammetry;\
    \ ETH Zurich: Zürich, Switzerland, 2009.\n21.\nDu, S.; Zhang, Y.; Qin, R.; Yang,\
    \ Z.; Zou, Z.; Tang, Y.; Fan, C. Building change detection using old aerial\n\
    images and new LiDAR data. Remote Sens. 2016, 8, 1030. [CrossRef]\n22.\nGitelson,\
    \ A.A.; Kaufman, Y.J.; Stark, R.; Rundquist, D. Novel algorithms for remote estimation\
    \ of vegetation\nfraction. Remote Sens. Environ. 2002, 80, 76–87. [CrossRef]\n\
    23.\nAdam, E.; Mutanga, O.; Rugege, D. Multispectral and hyperspectral remote\
    \ sensing for identiﬁcation and\nmapping of wetland vegetation: A review. Wetl.\
    \ Ecol. Manag. 2010, 18, 281–296. [CrossRef]\n24.\nCalderón, R.; Montes-Borrego,\
    \ M.; Landa, B.B.; Navas-Cortés, J.A.; Zarco-Tejada, P.J. Detection of downy\n\
    mildew of opium poppy using high-resolution multi-spectral and thermal imagery\
    \ acquired with an\nunmanned aerial vehicle. Precis. Agric. 2014, 15, 639–661.\
    \ [CrossRef]\n25.\nKislik, C.; Dronova, I.; Kelly, M. UAVs in Support of Algal\
    \ Bloom Research: A Review of Current Applications\nand Future Opportunities.\
    \ Drones 2018, 2, 35. [CrossRef]\n26.\nSuomalainen, J.; Anders, N.; Iqbal, S.;\
    \ Roerink, G.; Franke, J.; Wenting, P.; Hünniger, D.; Bartholomeus, H.;\nBecker,\
    \ R.; Kooistra, L. A lightweight hyperspectral mapping system and photogrammetric\
    \ processing chain\nfor unmanned aerial vehicles. Remote Sens. 2014, 6, 11013–11030.\
    \ [CrossRef]\n27.\nBurkart, A.; Cogliati, S.; Schickling, A.; Rascher, U. A novel\
    \ UAV-based ultra-light weight spectrometer for\nﬁeld spectroscopy. IEEE Sens.\
    \ J. 2014, 14, 62–67. [CrossRef]\n28.\nHaboudane, D.; Miller, J.R.; Pattey, E.;\
    \ Zarco-Tejada, P.J.; Strachan, I.B. Hyperspectral vegetation indices\nand novel\
    \ algorithms for predicting green LAI of crop canopies: Modeling and validation\
    \ in the context of\nprecision agriculture. Remote Sens. Environ. 2004, 90, 337–352.\
    \ [CrossRef]\n29.\nPsomas, A.; Kneubühler, M.; Huber, S.; Itten, K.; Zimmermann,\
    \ N. Hyperspectral remote sensing for\nestimating aboveground biomass and for\
    \ exploring species richness patterns of grassland habitats. Int. J.\nRemote Sens.\
    \ 2011, 32, 9007–9031. [CrossRef]\nRemote Sens. 2019, 11, 1443\n16 of 22\n30.\n\
    Van der Meer, F.D.; Van der Werﬀ, H.M.; Van Ruitenbeek, F.J.; Hecker, C.A.; Bakker,\
    \ W.H.; Noomen, M.F.;\nVan Der Meijde, M.; Carranza, E.J.M.; De Smeth, J.B.; Woldai,\
    \ T. Multi-and hyperspectral geologic remote\nsensing: A review. Int. J. Appl.\
    \ Earth Obs. Geoinf. 2012, 14, 112–128. [CrossRef]\n31.\nBenediktsson, J.A.; Palmason,\
    \ J.A.; Sveinsson, J.R. Classiﬁcation of hyperspectral data from urban areas\n\
    based on extended morphological proﬁles. IEEE Trans. Geosci. Remote Sens. 2005,\
    \ 43, 480–491. [CrossRef]\n32.\nRuﬁno, G.; Moccia, A. Integrated VIS-NIR hyperspectral/thermal-IR\
    \ electro-optical payload system for a\nmini-UAV. In Infotech@ Aerospace; AIAA:\
    \ Reston, VA, USA, 2005; p. 7009.\n33.\nLucieer, A.; Malenovský, Z.; Veness, T.;\
    \ Wallace, L. HyperUAS—Imaging spectroscopy from a multirotor\nunmanned aircraft\
    \ system. J. Field Robot. 2014, 31, 571–590. [CrossRef]\n34.\nAasen, H.; Burkart,\
    \ A.; Bolten, A.; Bareth, G. Generating 3D hyperspectral information with lightweight\n\
    UAV snapshot cameras for vegetation monitoring: From camera calibration to quality\
    \ assurance. ISPRS J.\nPhotogramm. Remote Sens. 2015, 108, 245–259. [CrossRef]\n\
    35.\nRobles-Kelly, A.; Huynh, C.P. Imaging Spectroscopy for Scene Analysis; Springer\
    \ Science & Business Media:\nBerlin, Germany, 2012.\n36.\nPrakash, A. Thermal\
    \ remote sensing: Concepts, issues and applications. Int. Arch. Photogramm. Remote\
    \ Sens.\n2000, 33, 239–243.\n37.\nSheng, H.; Chao, H.; Coopmans, C.; Han, J.;\
    \ McKee, M.; Chen, Y. Low-cost UAV-based thermal infrared\nremote sensing: Platform,\
    \ calibration and applications. In Proceedings of the 2010 IEEE/ASME International\n\
    Conference on Mechatronics and Embedded Systems and Applications (MESA), Qingdao,\
    \ China, 15–17 July\n2010; pp. 38–43.\n38.\nRudol, P.; Doherty, P. Human body\
    \ detection and geolocalization for UAV search and rescue missions using\ncolor\
    \ and thermal imagery. In Proceedings of the IEEE Aerospace Conference, Big Sky,\
    \ MT, USA, 1–8 March\n2008; pp. 1–8.\n39.\nAmbrosia, V.G.; Wegener, S.S.; Sullivan,\
    \ D.V.; Buechel, S.W.; Dunagan, S.E.; Brass, J.A.; Stoneburner, J.;\nSchoenung,\
    \ S.M. Demonstrating UAV-acquired real-time thermal data over ﬁres. Photogramm.\
    \ Eng. Remote\nSens. 2003, 69, 391–402. [CrossRef]\n40.\nIbarguren, A.; Molina,\
    \ J.; Susperregi, L.; Maurtua, I. Thermal tracking in mobile robots for leak inspection\n\
    activities. Sensors 2013, 13, 13560–13574. [CrossRef] [PubMed]\n41.\nBerni, J.A.;\
    \ Zarco-Tejada, P.J.; Suárez, L.; Fereres, E. Thermal and narrowband multispectral\
    \ remote sensing\nfor vegetation monitoring from an unmanned aerial vehicle. IEEE\
    \ Trans. Geosci. Remote Sens. 2009, 47,\n722–738. [CrossRef]\n42.\nLaliberte,\
    \ A.S.; Winters, C.; Rango, A. UAS remote sensing missions for rangeland applications.\
    \ Geocarto Int.\n2011, 26, 141–156. [CrossRef]\n43.\nJensen, A.M.; Neilson, B.T.;\
    \ McKee, M.; Chen, Y. Thermal remote sensing with an autonomous unmanned\naerial\
    \ remote sensing platform for surface stream temperatures. In Proceedings of the\
    \ 2012 IEEE International\nGeoscience and Remote Sensing Symposium (IGARSS), Munich,\
    \ Germany, 22–27 July 2012; pp. 5049–5052.\n44.\nZarco-Tejada, P.J.; González-Dugo,\
    \ V.; Berni, J.A. Fluorescence, temperature and narrow-band indices\nacquired\
    \ from a UAV platform for water stress detection using a micro-hyperspectral imager\
    \ and a thermal\ncamera. Remote Sens. Environ. 2012, 117, 322–337. [CrossRef]\n\
    45.\nHoffmann, H.; Nieto, H.; Jensen, R.; Guzinski, R.; Zarco-Tejada, P.; Friborg,\
    \ T. Estimating evaporation with\nthermal UAV data and two-source energy balance\
    \ models. Hydrol. Earth Syst. Sci. 2016, 20, 697–713. [CrossRef]\n46.\nBendig,\
    \ J.; Bolten, A.; Bareth, G. Introducing a low-cost mini-UAV for thermal-and multispectral-imaging.\n\
    Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci 2012, 39, 345–349. [CrossRef]\n\
    47.\nDalponte, M.; Coops, N.C.; Bruzzone, L.; Gianelle, D. Analysis on the use\
    \ of multiple returns LiDAR data for\nthe estimation of tree stems volume. IEEE\
    \ J. Sel. Top. Appl. Earth Obs. Remote Sens. 2009, 2, 310–318. [CrossRef]\n48.\n\
    Wallace, L.; Lucieer, A.; Watson, C.; Turner, D. Development of a UAV-LiDAR system\
    \ with application to\nforest inventory. Remote Sens. 2012, 4, 1519–1543. [CrossRef]\n\
    49.\nRieke, M.; Foerster, T.; Geipel, J.; Prinz, T. High-Precision Positioning\
    \ and Real-Time Data Processing of UAV\nSystems. Int. Arch. Photogramm. Remote\
    \ Sens. Spat. Inf. Sci. 2011, 38, 119–124. [CrossRef]\n50.\nCampos-Taberner, M.;\
    \ Romero-Soriano, A.; Gatta, C.; Camps-Valls, G.; Lagrange, A.; Saux, B.L.; Beaupère,\
    \ A.;\nBoulch, A.; Chan-Hon-Tong, A.; Herbin, S.; et al. Processing of Extremely\
    \ High-Resolution LiDAR and RGB\nData: Outcome of the 2015 IEEE GRSS Data Fusion\
    \ Contest–Part A: 2-D Contest. IEEE J. Sel. Top. Appl. Earth\nObs. Remote Sens.\
    \ 2016, 9, 5547–5559. [CrossRef]\nRemote Sens. 2019, 11, 1443\n17 of 22\n51.\n\
    Vo, A.; Truong-Hong, L.; Laefer, D.F.; Tiede, D.; d’Oleire-Oltmanns, S.; Baraldi,\
    \ A.; Shimoni, M.; Moser, G.;\nTuia, D. Processing of Extremely High Resolution\
    \ LiDAR and RGB Data: Outcome of the 2015 IEEE GRSS\nData Fusion Contest—Part\
    \ B: 3-D Contest. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2016, 9, 5560–5575.\n\
    [CrossRef]\n52.\nBelward, A.S.; Skøien, J.O. Who launched what, when and why;\
    \ trends in global land-cover observation\ncapacity from civilian earth observation\
    \ satellites. ISPRS J. Photogramm. Remote Sens. 2015, 103, 115–128.\n[CrossRef]\n\
    53.\nHussain, M.; Chen, D.; Cheng, A.; Wei, H.; Stanley, D. Change detection from\
    \ remotely sensed images: From\npixel-based to object-based approaches. ISPRS\
    \ J. Photogramm. Remote Sens. 2013, 80, 91–106. [CrossRef]\n54.\nMa, L.; Li, M.;\
    \ Ma, X.; Cheng, L.; Du, P.; Liu, Y. A review of supervised object-based land-cover\
    \ image\nclassiﬁcation. ISPRS J. Photogramm. Remote Sens. 2017, 130, 277–293.\
    \ [CrossRef]\n55.\nCheng, G.; Han, J.; Guo, L.; Liu, Z.; Bu, S.; Ren, J. Eﬀective\
    \ and eﬃcient midlevel visual elements-oriented\nland-use classiﬁcation using\
    \ VHR remote sensing images. IEEE Trans. Geosci. Remote Sens. 2015, 53,\n4238–4249.\
    \ [CrossRef]\n56.\nCarleer, A.; Wolﬀ, E. Urban land cover multi-level region-based\
    \ classiﬁcation of VHR data by selecting\nrelevant features. Int. J. Remote Sens.\
    \ 2006, 27, 1035–1051. [CrossRef]\n57.\nFry, J.; Coan, M.; Homer, C.; Meyer, D.;\
    \ Wickham, J. Completion of the National Land Cover Database (NLCD)\n1992–2001\
    \ Land cover Change Retroﬁt Product; US Geological Survey: Reston, VA, USA, 2009;\
    \ pp. 1258–2331.\n58.\nGong, P.; Wang, J.; Yu, L.; Zhao, Y.; Zhao, Y.; Liang,\
    \ L.; Niu, Z.; Huang, X.; Fu, H.; Liu, S. Finer resolution\nobservation and monitoring\
    \ of global land cover: First mapping results with Landsat TM and ETM+ data.\n\
    Int. J. Remote Sens. 2013, 34, 2607–2654. [CrossRef]\n59.\nSalazar, A.; Baldi,\
    \ G.; Hirota, M.; Syktus, J.; McAlpine, C. Land use and land cover change impacts\
    \ on the\nregional climate of non-Amazonian South America: A review. Glob. Planet.\
    \ Chang. 2015, 128, 103–119.\n[CrossRef]\n60.\nDash, J.P.; Watt, M.S.; Pearse,\
    \ G.D.; Heaphy, M.; Dungey, H.S. Assessing very high resolution UAV imagery\n\
    for monitoring forest health during a simulated disease outbreak. ISPRS J. Photogramm.\
    \ Remote Sens. 2017,\n131, 1–14. [CrossRef]\n61.\nTamouridou, A.; Alexandridis,\
    \ T.; Pantazi, X.; Lagopodi, A.; Kasheﬁ, J.; Moshou, D. Evaluation of UAV\nimagery\
    \ for mapping Silybum marianum weed patches. Int. J. Remote Sens. 2017, 38, 2246–2259.\
    \ [CrossRef]\n62.\nQin, R. An object-based hierarchical method for change detection\
    \ using unmanned aerial vehicle images.\nRemote Sens. 2014, 6, 7911–7932. [CrossRef]\n\
    63.\nTorres-Sánchez, J.; López-Granados, F.; Peña, J.M. An automatic object-based\
    \ method for optimal thresholding\nin UAV images: Application for vegetation detection\
    \ in herbaceous crops. Comput. Electron. Agric. 2015, 114,\n43–52. [CrossRef]\n\
    64.\nTimm, B.C.; McGarigal, K. Fine-scale remotely-sensed cover mapping of coastal\
    \ dune and salt marsh\necosystems at Cape Cod National Seashore using Random Forests.\
    \ Remote Sens. Environ. 2012, 127, 106–117.\n[CrossRef]\n65.\nHayes, M.M.; Miller,\
    \ S.N.; Murphy, M.A. High-resolution landcover classiﬁcation using Random Forest.\n\
    Remote Sens. Lett. 2014, 5, 112–121. [CrossRef]\n66.\nBlaschke, T. Object based\
    \ image analysis for remote sensing. ISPRS J. Photogramm. Remote Sens. 2010, 65,\n\
    2–16. [CrossRef]\n67.\nBaatz, M.; Benz, U.; Dehghani, S.; Heynen, M.; Höltje,\
    \ A.; Hofmann, P.; Lingenfelder, I.; Mimler, M.;\nSohlbach, M.; Weber, M. ECognition\
    \ Professional User Manual 4; Deﬁniens Imaging: München, Germany, 2004.\n68.\n\
    Gaetano, R.; Scarpa, G.; Poggi, G. Hierarchical texture-based segmentation of\
    \ multiresolution remote-sensing\nimages. IEEE Trans. Geosci. Remote Sens. 2009,\
    \ 47, 2129–2141. [CrossRef]\n69.\nTrias-Sanz, R.; Stamon, G.; Louchet, J. Using\
    \ colour, texture, and hierarchial segmentation for high-resolution\nremote sensing.\
    \ ISPRS J. Photogramm. Remote Sens. 2008, 63, 156–168. [CrossRef]\n70.\nShackelford,\
    \ A.K.; Davis, C.H. A hierarchical fuzzy classiﬁcation approach for high-resolution\
    \ multispectral\ndata over urban areas. IEEE Trans. Geosci. Remote Sens. 2003,\
    \ 41, 1920–1932. [CrossRef]\n71.\nZhang, P.; Lv, Z.; Shi, W. Object-based spatial\
    \ feature for classiﬁcation of very high resolution remote sensing\nimages. IEEE\
    \ Geosci. Remote Sens. Lett. 2013, 10, 1572–1576. [CrossRef]\nRemote Sens. 2019,\
    \ 11, 1443\n18 of 22\n72.\nMing, D.; Li, J.; Wang, J.; Zhang, M. Scale parameter\
    \ selection by spatial statistics for GeOBIA: Using\nmean-shift based multi-scale\
    \ segmentation as an example. ISPRS J. Photogramm. Remote Sens. 2015, 106,\n28–41.\
    \ [CrossRef]\n73.\nDrˇagu¸t, L.; Tiede, D.; Levick, S.R. ESP: A tool to estimate\
    \ scale parameter for multiresolution image\nsegmentation of remotely sensed data.\
    \ Int. J. Geogr. Inf. Sci. 2010, 24, 859–871. [CrossRef]\n74.\nKim, M.; Madden,\
    \ M.; Warner, T. Estimation of optimal image object size for the segmentation\
    \ of forest\nstands with multispectral IKONOS imagery. In Object-Based Image Analysis;\
    \ Springer: Berlin/Heidelberg,\nGermany, 2008; pp. 291–307.\n75.\nBlaschke, T.;\
    \ Hay, G.J.; Kelly, M.; Lang, S.; Hofmann, P.; Addink, E.; Feitosa, R.Q.; van\
    \ der Meer, F.;\nvan der Werﬀ, H.; van Coillie, F. Geographic object-based image\
    \ analysis–towards a new paradigm. ISPRS J.\nPhotogramm. Remote Sens. 2014, 87,\
    \ 180–191. [CrossRef]\n76.\nAgüera, F.; Aguilar, F.J.; Aguilar, M.A. Using texture\
    \ analysis to improve per-pixel classiﬁcation of very high\nresolution images\
    \ for mapping plastic greenhouses. ISPRS J. Photogramm. Remote Sens. 2008, 63,\
    \ 635–646.\n[CrossRef]\n77.\nKim, M.; Madden, M.; Warner, T.A. Forest type mapping\
    \ using object-speciﬁc texture measures from\nmultispectral Ikonos imagery. Photogramm.\
    \ Eng. Remote Sens. 2009, 75, 819–829. [CrossRef]\n78.\nFeng, Q.; Liu, J.; Gong,\
    \ J. UAV remote sensing for urban vegetation mapping using random forest and texture\n\
    analysis. Remote Sens. 2015, 7, 1074–1094. [CrossRef]\n79.\nMoranduzzo, T.; Mekhalﬁ,\
    \ M.L.; Melgani, F. LBP-based multiclass classiﬁcation method for UAV imagery.\n\
    In Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium\
    \ (IGARSS), Milan,\nItaly, 26–31 July 2015; pp. 2362–2365.\n80.\nSmits, P.C.;\
    \ Annoni, A. Updating land-cover maps by using texture information from very high-resolution\n\
    space-borne imagery. IEEE Trans. Geosci. Remote Sens. 1999, 37, 1244–1254. [CrossRef]\n\
    81.\nKurtz, C.; Passat, N.; Gancarski, P.; Puissant, A. Extraction of complex\
    \ patterns from multiresolution remote\nsensing images: A hierarchical top-down\
    \ methodology. Pattern Recognit. 2012, 45, 685–706. [CrossRef]\n82.\nLaliberte,\
    \ A.S.; Rango, A. Texture and scale in object-based analysis of subdecimeter resolution\
    \ unmanned\naerial vehicle (UAV) imagery. IEEE Trans. Geosci. Remote Sens. 2009,\
    \ 47, 761–770. [CrossRef]\n83.\nGevaert, C.; Persello, C.; Sliuzas, R.; Vosselman,\
    \ G. Informal settlement classiﬁcation using point-cloud and\nimage-based features\
    \ from UAV data. ISPRS J. Photogramm. Remote Sens. 2017, 125, 225–236. [CrossRef]\n\
    84.\nImmitzer, M.; Stepper, C.; Böck, S.; Straub, C.; Atzberger, C. Use of WorldView-2\
    \ stereo imagery and National\nForest Inventory data for wall-to-wall mapping\
    \ of growing stock. For. Ecol. Manag. 2016, 359, 232–246.\n[CrossRef]\n85.\nSalehi,\
    \ B.; Zhang, Y.; Zhong, M. Object-based land cover classiﬁcation of urban areas\
    \ using VHR imagery\nand photogrammetrically-derived DSM. In Proceedings of the\
    \ ASPRS 2011 Annual Conference, Milwaukee,\nWI, USA, 1–5 May 2011.\n86.\nSohn,\
    \ G.; Dowman, I. Data fusion of high-resolution satellite imagery and LiDAR data\
    \ for automatic building\nextraction. ISPRS J. Photogramm. Remote Sens. 2007,\
    \ 62, 43–63. [CrossRef]\n87.\nKhan, S.; Aragão, L.; Iriarte, J. A UAV–lidar system\
    \ to map Amazonian rainforest and its ancient landscape\ntransformations. Int.\
    \ J. Remote Sens. 2017, 38, 2313–2330. [CrossRef]\n88.\nGao, Y.; Zhang, W. LULC\
    \ classiﬁcation and topographic correction of Landsat-7 ETM+ imagery in the Yangjia\n\
    River Watershed: The inﬂuence of DEM resolution. Sensors 2009, 9, 1980–1995. [CrossRef]\
    \ [PubMed]\n89.\nWatanachaturaporn, P.; Arora, M.K.; Varshney, P.K. Multisource\
    \ Classiﬁcation Using Support Vector Machines.\nPhotogramm. Eng. Remote Sens.\
    \ 2008, 74, 239–246. [CrossRef]\n90.\nGuo, L.; Chehata, N.; Mallet, C.; Boukir,\
    \ S. Relevance of airborne lidar and multispectral image data for urban\nscene\
    \ classiﬁcation using Random Forests. ISPRS J. Photogramm. Remote Sens. 2011,\
    \ 66, 56–66. [CrossRef]\n91.\nPix4D, SA. Pix4Dmapper: Professional Drone Mapping\
    \ and Photogrammetry Software | Pix4D. Available\nonline: https://www.pix4d.com/\
    \ (accessed on 30 December 2018).\n92.\nAgisoft LLC. Agisoft Metashape. Available\
    \ online: https://www.agisoft.com/ (accessed on 30 December 2018).\n93.\nFonstad,\
    \ M.A.; Dietrich, J.T.; Courville, B.C.; Jensen, J.L.; Carbonneau, P.E. Topographic\
    \ structure from\nmotion: A new development in photogrammetric measurement. Earth\
    \ Surf. Process. Landf. 2013, 38, 421–430.\n[CrossRef]\nRemote Sens. 2019, 11,\
    \ 1443\n19 of 22\n94.\nHuang, X.; Zhang, L.; Gong, W. Information fusion of aerial\
    \ images and LIDAR data in urban areas:\nVector-stacking, re-classiﬁcation and\
    \ post-processing approaches. Int. J. Remote Sens. 2011, 32, 69–84.\n[CrossRef]\n\
    95.\nQin, R. A mean shift vector-based shape feature for classiﬁcation of high\
    \ spatial resolution remotely sensed\nimagery. IEEE J. Sel. Top. Appl. Earth Obs.\
    \ Remote Sens. 2015, 8, 1974–1985. [CrossRef]\n96.\nVincent, L. Morphological\
    \ grayscale reconstruction in image analysis: Applications and eﬃcient algorithms.\n\
    IEEE Trans. Image Process. 1993, 2, 176–201. [CrossRef]\n97.\nZhang, X.; Xiao,\
    \ P.; Feng, X. Toward combining thematic information with hierarchical multiscale\n\
    segmentations using tree Markov random ﬁeld model. ISPRS J. Photogramm. Remote\
    \ Sens. 2017, 131,\n134–146. [CrossRef]\n98.\nCrommelinck, S.; Bennett, R.; Gerke,\
    \ M.; Koeva, M.; Yang, M.; Vosselman, G. SLIC superpixels for object\ndelineation\
    \ from UAV data. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, 4,\
    \ 9. [CrossRef]\n99.\nCorcoran, P.; Winstanley, A. Using texture to tackle the\
    \ problem of scale in land-cover classiﬁcation.\nIn Object-Based Image Analysis;\
    \ Springer: Berlin/Heidelberg, Germany, 2008; pp. 113–132.\n100. Pal, M.; Mather,\
    \ P. Support vector machines for classiﬁcation in remote sensing. Int. J. Remote\
    \ Sens. 2005, 26,\n1007–1011. [CrossRef]\n101. Pal, M. Random forest classiﬁer\
    \ for remote sensing classiﬁcation. Int. J. Remote Sens. 2005, 26, 217–222.\n\
    [CrossRef]\n102. Paola, J.D.; Schowengerdt, R.A. A detailed comparison of backpropagation\
    \ neural network and\nmaximum-likelihood classiﬁers for urban land use classiﬁcation.\
    \ IEEE Trans. Geosci. Remote Sens. 1995, 33,\n981–996. [CrossRef]\n103. Huang,\
    \ X.; Zhang, L. An SVM Ensemble Approach Combining Spectral, Structural, and Semantic\
    \ Features\nfor the Classiﬁcation of High-Resolution Remotely Sensed Imagery.\
    \ IEEE Trans. Geosci. Remote Sens. 2013,\n51, 257–272. [CrossRef]\n104. Merentitis,\
    \ A.; Debes, C.; Heremans, R. Ensemble Learning in Hyperspectral Image Classiﬁcation:\
    \ Toward\nSelecting a Favorable Bias-Variance Tradeoﬀ. IEEE J. Sel. Top. Appl.\
    \ Earth Obs. Remote Sens. 2014, 7,\n1089–1102. [CrossRef]\n105. Bergado, J.R.;\
    \ Persello, C.; Stein, A. Recurrent Multiresolution Convolutional Networks for\
    \ VHR Image\nClassiﬁcation. IEEE Trans. Geosci. Remote Sens. 2018, 56, 6361–6374.\
    \ [CrossRef]\n106. Mboga, N.; Georganos, S.; Grippa, T.; Lennert, M.; Vanhuysse,\
    \ S.; Wolﬀ, E. Fully Convolutional Networks\nand Geographic Object-Based Image\
    \ Analysis for the Classiﬁcation of VHR Imagery. Remote Sens. 2019, 11,\n597.\
    \ [CrossRef]\n107. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional\
    \ Networks for Biomedical Image Segmentation.\nIn International Conference on\
    \ Medical Image Computing and Computer-Assisted Intervention; Springer: Cham,\n\
    Switzerland, 2015.\n108. Yu, F.; Koltun, V. Multi-Scale Context Aggregation by\
    \ Dilated Convolutions. arXiv 2015, arXiv:1511.07122.\n109. Liu, Y.; Fan, B.;\
    \ Wang, L.; Bai, J.; Xiang, S.; Pan, C. Semantic labeling in very high resolution\
    \ images via a\nself-cascaded convolutional neural network. ISPRS J. Photogramm.\
    \ Remote Sens. 2018, 145, 78–95. [CrossRef]\n110. Marmanis, D.; Schindler, K.;\
    \ Wegner, J.D.; Galliani, S.; Datcu, M.; Stilla, U. Classiﬁcation with an edge:\n\
    Improving semantic image segmentation with boundary detection. ISPRS J. Photogramm.\
    \ Remote Sens. 2018,\n135, 158–172. [CrossRef]\n111. Zhao, H.; Shi, J.; Qi, X.;\
    \ Wang, X.; Jia, J. Pyramid Scene Parsing Network. In Proceedings of the 2017\
    \ IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), Honolulu,\
    \ HI, USA, 21–26 July 2017;\npp. 6230–6239.\n112. Audebert, N.; Le Saux, B.; Lefèvre,\
    \ S. Beyond RGB: Very high resolution urban remote sensing with\nmultimodal deep\
    \ networks. ISPRS J. Photogramm. Remote Sens. 2018, 140, 20–32. [CrossRef]\n113.\
    \ Liu, Y.; Piramanayagam, S.; Monteiro, S.T.; Saber, E. Dense Semantic Labeling\
    \ of Very-High-Resolution Aerial\nImagery and LiDAR with Fully-Convolutional Neural\
    \ Networks and Higher-Order CRFs. In Proceedings of\nthe 2017 IEEE Conference\
    \ on Computer Vision and Pattern Recognition Workshops (CVPRW), Honolulu, HI,\n\
    USA, 21–26 July 2017; pp. 1561–1570.\n114. Hu, F.; Xia, G.-S.; Hu, J.; Zhang,\
    \ L. Transferring deep convolutional neural networks for the scene classiﬁcation\n\
    of high-resolution remote sensing imagery. Remote Sens. 2015, 7, 14680–14707.\
    \ [CrossRef]\nRemote Sens. 2019, 11, 1443\n20 of 22\n115. Cheng, G.; Han, J.;\
    \ Lu, X. Remote sensing image scene classiﬁcation: Benchmark and state of the\
    \ art.\nProc. IEEE 2017, 105, 1865–1883. [CrossRef]\n116. Penatti, O.A.B.; Nogueira,\
    \ K.; Santos, J.A.d. Do deep features generalize from everyday objects to remote\n\
    sensing and aerial scenes domains? In Proceedings of the 2015 IEEE Conference\
    \ on Computer Vision and\nPattern Recognition Workshops (CVPRW), Boston, MA, USA,\
    \ 7–12 June 2015; pp. 44–51.\n117. Wu, Z.; Han, X.; Lin, Y.-L.; Uzunbas, M.G.;\
    \ Goldstein, T.; Lim, S.N.; Davis, L.S. DCAN: Dual Channel-Wise\nAlignment Networks\
    \ for Unsupervised Scene Adaptation. In Proceedings of the Computer Vision—ECCV\n\
    2018, Munich, Germany, 8–14 September 2018; pp. 535–552.\n118. LTA. Maintaining\
    \ Our Roads and Facilities. Available online: http://www.lta.gov.sg/content/ltaweb/en/roads-\n\
    and-motoring/road-safety-and-regulations/maintaining-our-roads-and-facilities.html\
    \ (accessed on 29 May 2019).\n119. Qin, R.; Gruen, A. 3D change detection at street\
    \ level using mobile laser scanning point clouds and terrestrial\nimages. ISPRS\
    \ J. Photogramm. Remote Sens. 2014, 90, 23–35. [CrossRef]\n120. Saur, G.; Krüger,\
    \ W. Change Detection in Uav Video Mosaics Combining a Feature Based Approach\
    \ and\nExtended Image Diﬀerencing. In Proceedings of the 2016 International Archives\
    \ of the Photogrammetry,\nRemote Sensing & Spatial Information Sciences, Prague,\
    \ Czech Republic, 12–19 July 2016; pp. 557–562.\n121. Ma, Y.; Wu, X.; Yu, G.;\
    \ Xu, Y.; Wang, Y. Pedestrian detection and tracking from low-resolution unmanned\n\
    aerial vehicle thermal imagery. Sensors 2016, 16, 446. [CrossRef]\n122. Gaszczak,\
    \ A.; Breckon, T.P.; Han, J. Real-time people and vehicle detection from UAV imagery.\
    \ In Proceedings\nof the IS&T/SPIE Electronic Imaging. International Society for\
    \ Optics and Photonics, San Fransico, CA, USA,\n23 January 2011; p. 78780B.\n\
    123. Butenuth, M.; Burkert, F.; Schmidt, F.; Hinz, S.; Hartmann, D.; Kneidl, A.;\
    \ Borrmann, A.; Sirmacek, B.\nIntegrating pedestrian simulation, tracking and\
    \ event detection for crowd analysis. In Proceedings of the\n2011 IEEE International\
    \ Conference on Computer Vision Workshops (ICCV Workshops), Barcelona, Spain,\n\
    6–13 November 2011; pp. 150–157.\n124. De Smedt, F.; Hulens, D.; Goedemé, T. On-board\
    \ real-time tracking of pedestrians on a UAV. In Proceedings\nof the 2015 IEEE\
    \ Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Boston,\
    \ MA,\nUSA, 7–12 July 2015; pp. 1–8.\n125. Triggs, B.; McLauchlan, P.F.; Hartley,\
    \ R.I.; Fitzgibbon, A.W. Bundle adjustment—A modern synthesis.\nIn International\
    \ Workshop on Vision Algorithms; Springer: Berlin/Heidelberg, Germany, 2000; pp.\
    \ 298–372.\n126. Walter, V. Object-based classiﬁcation of remote sensing data\
    \ for change detection. ISPRS J. Photogramm.\nRemote Sens. 2004, 58, 225–238.\
    \ [CrossRef]\n127. Qin, R.; Tian, J.; Reinartz, P. 3D change detection–approaches\
    \ and applications. ISPRS J. Photogramm. Remote\nSens. 2016, 122, 41–56. [CrossRef]\n\
    128. Lefebvre, A.; Corpetti, T.; Hubert-Moy, L. Object-oriented approach and texture\
    \ analysis for change detection\nin very high resolution images. In Proceedings\
    \ of the IEEE International Geoscience and Remote Sensing\nSymposium, Boston,\
    \ MA, USA, 7–11 July 2008; pp. IV-663–IV-666.\n129. Gamanya, R.; De Maeyer, P.;\
    \ De Dapper, M. Object-oriented change detection for the city of Harare, Zimbabwe.\n\
    Expert Syst. Appl. 2009, 36, 571–588. [CrossRef]\n130. Xian, G.; Homer, C. Updating\
    \ the 2001 National Land Cover Database impervious surface products to 2006\n\
    using Landsat imagery change detection methods. Remote Sens. Environ. 2010, 114,\
    \ 1676–1686. [CrossRef]\n131. Bontemps, S.; Bogaert, P.; Titeux, N.; Defourny,\
    \ P. An object-based change detection method accounting for\ntemporal dependences\
    \ in time series with medium to coarse spatial resolution. Remote Sens. Environ.\
    \ 2008,\n112, 3181–3191. [CrossRef]\n132. Wu, C.; Du, B.; Cui, X.; Zhang, L. A\
    \ post-classiﬁcation change detection method based on iterative slow\nfeature\
    \ analysis and Bayesian soft fusion. Remote Sens. Environ. 2017, 199, 241–255.\
    \ [CrossRef]\n133. Tewkesbury, A.P.; Comber, A.J.; Tate, N.J.; Lamb, A.; Fisher,\
    \ P.F. A critical synthesis of remotely sensed optical\nimage change detection\
    \ techniques. Remote Sens. Environ. 2015, 160, 1–14. [CrossRef]\n134. Frost, S.\
    \ Study Analysing the Current Activities in the Field of UAV. ENTR/2007/065. 2007.\
    \ Available\nonline: https://ec.europa.eu/home-aﬀairs/sites/homeaﬀairs/ﬁles/e-library/documents/policies/security/pdf/\n\
    uav_study_element_2_en.pdf (accessed on 17 June 2019).\n135. Ivushkin, K.; Bartholomeus,\
    \ H.; Bregt, A.K.; Pulatov, A.; Franceschini, M.H.D.; Kramer, H.; van Loo, E.N.;\n\
    Jaramillo Roman, V.; Finkers, R. UAV based soil salinity assessment of cropland.\
    \ Geoderma 2019, 338, 502–512.\n[CrossRef]\nRemote Sens. 2019, 11, 1443\n21 of\
    \ 22\n136. Anderson, C. Agricultural Drones: Relatively cheap drones with advanced\
    \ sensors and imaging capabilities\nare giving farmers new ways to increase yields\
    \ and reduce crop damage. MIT Technol. Rev 2014, 17, 3–58.\n137. Sugiura, R.;\
    \ Noguchi, N.; Ishii, K. Remote-sensing technology for vegetation monitoring using\
    \ an unmanned\nhelicopter. Biosyst. Eng. 2005, 90, 369–379. [CrossRef]\n138. Wallace,\
    \ L.; Lucieer, A.; Malenovský, Z.; Turner, D.; Vopˇenka, P. Assessment of forest\
    \ structure using two\nUAV techniques: A comparison of airborne laser scanning\
    \ and structure from motion (SfM) point clouds.\nForests 2016, 7, 62. [CrossRef]\n\
    139. Zhu, J.S.; Sun, K.; Jia, S.; Li, Q.Q.; Hou, X.X.; Lin, W.D.; Liu, B.Z.; Qiu,\
    \ G.P. Urban Traﬃc Density Estimation\nBased on Ultrahigh-Resolution UAV Video\
    \ and Deep Neural Network. IEEE J. Sel. Top. Appl. Earth Obs.\nRemote Sens. 2018,\
    \ 11, 4968–4981. [CrossRef]\n140. Congress, S.S.C.; Puppala, A.J.; Lundberg, C.L.\
    \ Total system error analysis of UAV-CRP technology for\nmonitoring transportation\
    \ infrastructure assets. Eng. Geol. 2018, 247, 104–116. [CrossRef]\n141. Malihi,\
    \ S.; Zoej, M.J.V.; Hahn, M. Large-Scale Accurate Reconstruction of Buildings\
    \ Employing Point Clouds\nGenerated from UAV Imagery. Remote Sens. 2018, 10, 1148.\
    \ [CrossRef]\n142. Tokarczyk, P.; Leitao, J.P.; Rieckermann, J.; Schindler, K.;\
    \ Blumensaat, F. High-quality observation of surface\nimperviousness for urban\
    \ runoff modelling using UAV imagery. Hydrol. Earth Syst. Sci. 2015, 19, 4215.\
    \ [CrossRef]\n143. Bendea, H.; Boccardo, P.; Dequal, S.; Giulio Tonolo, F.; Marenchino,\
    \ D.; Piras, M. Low cost UAV for\npost-disaster assessment. Int. Arch. Photogramm.\
    \ Remote Sens. Spat. Inf. Sci. 2008, 37, 1373–1379.\n144. Fernandez Galarreta,\
    \ J.; Kerle, N.; Gerke, M. UAV-based urban structural damage assessment using\
    \ object-based\nimage analysis and semantic reasoning. Nat. Hazards Earth Syst.\
    \ Sci. 2015, 15, 1087–1101. [CrossRef]\n145. Boccardo, P.; Chiabrando, F.; Dutto,\
    \ F.; Tonolo, F.G.; Lingua, A. UAV deployment exercise for mapping purposes:\n\
    Evaluation of emergency response applications. Sensors 2015, 15, 15717–15737.\
    \ [CrossRef] [PubMed]\n146. Schultjan, M. Towards the Deployment of UAVs for Fire\
    \ Surveillance; Hamburg University of Technology:\nHamburg, Germany, 2012.\n147.\
    \ Lucieer, A.; Jong, S.M.d.; Turner, D. Mapping landslide displacements using\
    \ Structure from Motion (SfM)\nand image correlation of multi-temporal UAV photography.\
    \ Prog. Phys. Geogr. Earth Environ. 2014, 38,\n97–116. [CrossRef]\n148. Turner,\
    \ D.; Lucieer, A.; De Jong, S.M. Time Series Analysis of Landslide Dynamics Using\
    \ an Unmanned\nAerial Vehicle (UAV). Remote Sens. 2015, 7, 1736–1757. [CrossRef]\n\
    149. Barlow, J.; Gilham, J.; Ibarra Cofrã, I. Kinematic analysis of sea cliﬀ stability\
    \ using UAV photogrammetry.\nInt. J. Remote Sens. 2017, 38, 2464–2479. [CrossRef]\n\
    150. Sturdivant, E.J.; Lentz, E.E.; Thieler, E.R.; Farris, A.S.; Weber, K.M.;\
    \ Remsen, D.P.; Miner, S.; Henderson, R.E.\nUAS-SfM for Coastal Research: Geomorphic\
    \ Feature Extraction and Land Cover Classiﬁcation from\nHigh-Resolution Elevation\
    \ and Optical Imagery. Remote Sens. 2017, 9, 1020. [CrossRef]\n151. McBratney,\
    \ A.; Pringle, M. Estimating average and proportional variograms of soil properties\
    \ and their\npotential use in precision agriculture. Precis. Agric. 1999, 1, 125–152.\
    \ [CrossRef]\n152. Atzberger, C. Advances in remote sensing of agriculture: Context\
    \ description, existing operational monitoring\nsystems and major information\
    \ needs. Remote Sens. 2013, 5, 949–981. [CrossRef]\n153. Dunford, R.; Michel,\
    \ K.; Gagnage, M.; Piégay, H.; Trémelo, M.-L. Potential and constraints of Unmanned\n\
    Aerial Vehicle technology for the characterization of Mediterranean riparian forest.\
    \ Int. J. Remote Sens. 2009,\n30, 4915–4935. [CrossRef]\n154. Johnson, L.F. Temporal\
    \ stability of an NDVI-LAI relationship in a Napa Valley vineyard. Aust. J. Grape\
    \ Wine\nRes. 2003, 9, 96–101. [CrossRef]\n155. Steltzer, H.; Welker, J.M. Modeling\
    \ the eﬀect of photosynthetic vegetation properties on the NDVI–LAI\nrelationship.\
    \ Ecology 2006, 87, 2765–2772. [CrossRef]\n156. Wang, Q.; Adiku, S.; Tenhunen,\
    \ J.; Granier, A. On the relationship of NDVI with leaf area index in a deciduous\n\
    forest site. Remote Sens. Environ. 2005, 94, 244–255. [CrossRef]\n157. Fensholt,\
    \ R.; Sandholt, I.; Rasmussen, M.S. Evaluation of MODIS LAI, fAPAR and the relation\
    \ between\nfAPAR and NDVI in a semi-arid environment using in situ measurements.\
    \ Remote Sens. Environ. 2004, 91,\n490–507. [CrossRef]\n158. Wang, X.; Wang, M.;\
    \ Wang, S.; Wu, Y. Extraction of vegetation information from visible unmanned\
    \ aerial\nvehicle images. Trans. Chin. Soc. Agric. Eng. 2015, 31, 152–159.\nRemote\
    \ Sens. 2019, 11, 1443\n22 of 22\n159. Xue, J.; Su, B. Signiﬁcant remote sensing\
    \ vegetation indices: A review of developments and applications.\nJ. Sens. 2017,\
    \ 2017, 1353691. [CrossRef]\n160. Bendig, J.; Bolten, A.; Bareth, G. UAV-based\
    \ imaging for multi-temporal, very high Resolution Crop\nSurface Models to monitor\
    \ Crop Growth VariabilityMonitoring des Pﬂanzenwachstums mit Hilfe\nmultitemporaler\
    \ und hoch auﬂösender Oberﬂächenmodelle von Getreidebeständen auf Basis von Bildern\n\
    aus UAV-Beﬂiegungen. Photogramm. Fernerkund. Geoinf. 2013, 2013, 551–562.\n161.\
    \ Dong, J.; Burnham, J.G.; Boots, B.; Rains, G.; Dellaert, F. 4d crop monitoring:\
    \ Spatio-temporal reconstruction\nfor agriculture. In Proceedings of the 2017\
    \ IEEE International Conference on Robotics and Automation\n(ICRA), Singapore,\
    \ Singapore, 29 May–3 June 2017; pp. 3878–3885.\n162. UN DESA. World Urbanization\
    \ Prospects: The 2014 Revision. 2015. Available online: https://esa.un.org/\n\
    unpd/wup/Publications/Files/WUP2014-Report.pdf (accessed on 17 June 2019).\n163.\
    \ Branco, L.H.C.; Segantine, P.C.L. MaNIAC-UAV-a methodology for automatic pavement\
    \ defects detection\nusing images obtained by Unmanned Aerial Vehicles. In Journal\
    \ of Physics: Conference Series; IOP Publishing:\nBristol, UK, 2015; p. 012122.\n\
    164. Knyaz, V.; Chibunichev, A. Photogrammetric techniques for road surface analysis.\n\
    ISPRS-Int.\nArch.\nPhotogramm. Remote Sens. Spat. Inf. Sci. 2016, 41, 515–520.\
    \ [CrossRef]\n165. Phung, M.; Dinh, T.; Hoang, V.; Ha, Q. Automatic Crack Detection\
    \ in Built Infrastructure Using Unmanned\nAerial Vehicles. In Proceedings of the\
    \ 2017 International Symposium on Automation and Robotics in\nConstruction (ISARC),\
    \ Taipei, Taiwan, 28 June–1 July 2017; pp. 823–829.\n166. Nishar, A.; Richards,\
    \ S.; Breen, D.; Robertson, J.; Breen, B. Thermal infrared imaging of geothermal\n\
    environments and by an unmanned aerial vehicle (UAV): A case study of the Wairakei–Tauhara\
    \ geothermal\nﬁeld, Taupo, New Zealand. Renew. Energy 2016, 86, 1256–1264. [CrossRef]\n\
    167. Saari, H.; Aallos, V.-V.; Akujärvi, A.; Antila, T.; Holmlund, C.; Kantojärvi,\
    \ U.; Mäkynen, J.; Ollila, J.\nNovel miniaturized hyperspectral sensor for UAV\
    \ and space applications.\nIn Sensors, Systems,\nand Next-Generation Satellites\
    \ XIII; SPIE: Bellingham, WA, USA, 2018; p. 74741M.\n168. Herold, M.; Roberts,\
    \ D.; Smadi, O.; Noronha, V. Road condition mapping with hyperspectral remote\
    \ sensing.\nIn Proceedings of the 2004 AVIRIS Workshop, Pasadena, CA, USA, 31\
    \ March–2 April 2004.\n169. Joyce, K.E.; Belliss, S.E.; Samsonov, S.V.; McNeill,\
    \ S.J.; Glassey, P.J. A review of the status of satellite remote\nsensing and\
    \ image processing techniques for mapping natural hazards and disasters. Prog.\
    \ Phys. Geogr.\n2009, 33, 183–207. [CrossRef]\n170. Dong, L.; Shan, J. A comprehensive\
    \ review of earthquake-induced building damage detection with remote\nsensing\
    \ techniques. ISPRS J. Photogramm. Remote Sens. 2013, 84, 85–99. [CrossRef]\n\
    171. Xu, Z.; Yang, J.; Peng, C.; Wu, Y.; Jiang, X.; Li, R.; Zheng, Y.; Gao, Y.;\
    \ Liu, S.; Tian, B. Development of an UAS\nfor post-earthquake disaster surveying\
    \ and its application in Ms7. 0 Lushan Earthquake, Sichuan, China.\nComput. Geosci.\
    \ 2014, 68, 22–30. [CrossRef]\n172. Vetrivel, A.; Gerke, M.; Kerle, N.; Nex, F.;\
    \ Vosselman, G. Disaster damage detection through synergistic\nuse of deep learning\
    \ and 3D point cloud features derived from very high resolution oblique aerial\
    \ images,\nand multiple-kernel-learning. ISPRS J. Photogramm. Remote Sens. 2018,\
    \ 140, 45–59. [CrossRef]\n173. Li, S.; Tang, H.; He, S.; Shu, Y.; Mao, T.; Li,\
    \ J.; Xu, Z. Unsupervised detection of earthquake-triggered\nroof-holes from UAV\
    \ images using joint color and shape features. IEEE Geosci. Remote Sens. Lett.\
    \ 2015, 12,\n1823–1827.\n174. Leprince, S.; Ayoub, F.; Klinger, Y.; Avouac, J.\
    \ Co-Registration of Optically Sensed Images and Correlation\n(COSI-Corr): An\
    \ operational methodology for ground deformation measurements. In Proceedings\
    \ of the\n2007 IEEE International Geoscience and Remote Sensing Symposium, Barcelona,\
    \ Spain, 23–28 July 2007;\npp. 1943–1946.\n175. Kotsis, I.; Kontoes, C.; Paradissis,\
    \ D.; Karamitsos, S.; Elias, P.; Papoutsis, I. A Methodology to Validate\nthe\
    \ InSAR Derived Displacement Field of the September 7(th), 1999 Athens Earthquake\
    \ Using Terrestrial\nSurveying. Improvement of the Assessed Deformation Field\
    \ by Interferometric Stacking. Sensors (Basel)\n2008, 8, 4119–4134. [CrossRef]\n\
    © 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open\
    \ access\narticle distributed under the terms and conditions of the Creative Commons\
    \ Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Remote sensing (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2072-4292/11/12/1443/pdf?version=1560993843
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Unmanned Aerial Vehicle for Remote Sensing Applications—A Review
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/lra.2017.2774979
  analysis: '>'
  authors:
  - Inkyu Sa
  - Zetao Chen
  - Marija Popović
  - Raghav Khanna
  - Frank Liebisch
  - Juan Nieto
  - Roland Siegwart
  citation_count: 232
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Robotics and Automation ... >Volume: 3 Issue: 1 weedNet: Dense Semantic
    Weed Classification Using Multispectral Images and MAV for Smart Farming Publisher:
    IEEE Cite This PDF Inkyu Sa; Zetao Chen; Marija Popović; Raghav Khanna; Frank
    Liebisch; Juan Nieto; Roland Siegwart All Authors 214 Cites in Papers 1 Cites
    in Patent 3815 Full Text Views Abstract Document Sections I. Introduction II.
    Related Work III. Methodologies IV. Experimental Results V. Conclusions Authors
    Figures References Citations Keywords Metrics Footnotes Abstract: Selective weed
    treatment is a critical step in autonomous crop management as related to crop
    health and yield. However, a key challenge is reliable and accurate weed detection
    to minimize damage to surrounding plants. In this letter, we present an approach
    for dense semantic weed classification with multispectral images collected by
    a micro aerial vehicle (MAV). We use the recently developed encoder-decoder cascaded
    convolutional neural network, SegNet, that infers dense semantic classes while
    allowing any number of input image channels and class balancing with our sugar
    beet and weed datasets. To obtain training datasets, we established an experimental
    field with varying herbicide levels resulting in field plots containing only either
    crop or weed, enabling us to use the normalized difference vegetation index as
    a distinguishable feature for automatic ground truth generation. We train six
    models with different numbers of input channels and condition (fine tune) it to
    achieve ~0.8 F1-score and 0.78 area under the curve classification metrics. For
    the model deployment, an embedded Graphics Processing Unit (GPU) system (Jetson
    TX2) is tested for MAV integration. Dataset used in this letter is released to
    support the community and future work. Published in: IEEE Robotics and Automation
    Letters ( Volume: 3, Issue: 1, January 2018) Page(s): 588 - 595 Date of Publication:
    20 November 2017 ISSN Information: DOI: 10.1109/LRA.2017.2774979 Publisher: IEEE
    Funding Agency: I. Introduction To a growing worldwide population with sufficient
    farm produce, new smart farming methods are required to increase or maintain crop
    yield while minimizing environmental impact. Precision agriculture techniques
    achieve this by spatially surveying key indicators of crop health and applying
    treatment, e.g., herbicides, pesticides, and fertilizers, only to relevant areas.
    Here, robotic systems can be often used as flexible, cost-efficient platforms
    replacing laborious manual procedures. Sign in to Continue Reading Authors Figures
    References Citations Keywords Metrics Footnotes More Like This An Experimental
    Comparison Towards Autonomous Camera Navigation to Optimize Training in Robot
    Assisted Surgery IEEE Robotics and Automation Letters Published: 2020 A Mobile
    Robot Visual SLAM System With Enhanced Semantics Segmentation IEEE Access Published:
    2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE robotics and automation letters
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'weedNet: Dense Semantic Weed Classification Using Multispectral Images and
    MAV for Smart Farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3389/fpls.2019.00714
  analysis: '>'
  authors:
  - Chunjiang Zhao
  - Ying Zhang
  - Jianjun Du
  - Xinyu Guo
  - Weiliang Wen
  - Siyi Gu
  - Jinglu Wang
  - Jiangchuan Fan
  citation_count: 238
  full_citation: '>'
  full_text: '>

    REVIEW

    published: 03 June 2019

    doi: 10.3389/fpls.2019.00714

    Edited by:

    Jose Antonio Jimenez-Berni,

    Spanish National Research Council

    (CSIC), Spain

    Reviewed by:

    Young-Su Seo,

    Pusan National University,

    South Korea

    Keisuke Nagai,

    Nagoya University, Japan

    *Correspondence:

    Chunjiang Zhao

    zhaocj@nercita.org.cn

    †Co-ﬁrst authors

    Specialty section:

    This article was submitted to

    Technical Advances in Plant Science,

    a section of the journal

    Frontiers in Plant Science

    Received: 30 October 2018

    Accepted: 14 May 2019

    Published: 03 June 2019

    Citation:

    Zhao C, Zhang Y, Du J, Guo X,

    Wen W, Gu S, Wang J and Fan J

    (2019) Crop Phenomics: Current

    Status and Perspectives.

    Front. Plant Sci. 10:714.

    doi: 10.3389/fpls.2019.00714

    Crop Phenomics: Current Status and

    Perspectives

    Chunjiang Zhao*†, Ying Zhang†, Jianjun Du, Xinyu Guo, Weiliang Wen, Shenghao Gu,

    Jinglu Wang and Jiangchuan Fan

    Beijing Key Lab of Digital Plant, Beijing Research Center for Information Technology
    in Agriculture, Beijing Academy

    of Agriculture and Forestry Sciences, Beijing, China

    Reliable, automatic, multifunctional, and high-throughput phenotypic technologies
    are

    increasingly considered important tools for rapid advancement of genetic gain
    in

    breeding programs. With the rapid development in high-throughput phenotyping

    technologies, research in this area is entering a new era called ‘phenomics.’
    The

    crop phenotyping community not only needs to build a multi-domain, multi-level,
    and

    multi-scale crop phenotyping big database, but also to research technical systems
    for

    phenotypic traits identiﬁcation and develop bioinformatics technologies for information

    extraction from the overwhelming amounts of omics data. Here, we provide an overview

    of crop phenomics research, focusing on two parts, from phenotypic data collection

    through various sensors to phenomics analysis. Finally, we discussed the challenges

    and prospective of crop phenomics in order to provide suggestions to develop new

    methods of mining genes associated with important agronomic traits, and propose
    new

    intelligent solutions for precision breeding.

    Keywords: crop phenomics, phenotyping extraction, data storage, functional–structural
    plant modeling,

    phenotype–genotype association analysis

    INTRODUCTION

    Persistent food and feed supply needs, resources shortages, climate change and
    energy use are some

    of the challenges we face in our dependence on plants. Until 2050, crop production
    will have to

    double to meet the projected production demands of the global population (Ray
    et al., 2013).

    Demand for crop production is expected to grow 2.4% a year, but the average rate
    of increase in crop

    yield is only 1.3%. Moreover, production yields have stagnated in up to 40% of
    land under cereal

    production (Fischer and Edmeades, 2010). Genetic improvements in crop performance
    remain the

    key role in improving crop productivity, but the current rate of improvement cannot
    meet the needs

    of sustainability and food security.

    Molecular breeding strategies pay more attention to selections based on genotypic
    information,

    but phenotypic data are still needed (Jannick et al., 2010; Araus and Cairns,
    2014). Phenotyping

    is necessary to improve the selection eﬃciency and reproducibility of results
    in transgenic studies

    (Gaudin et al., 2013; Yang et al., 2014; Feng et al., 2017) (Table 1). Considering
    that molecular

    Frontiers in Plant Science | www.frontiersin.org

    1

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    breeding populations can include a range from at least 200

    to at most 10,000 lines, the ability to accurately and high-

    throughput characterize hundreds of lines at the same time is

    still challenging (Lorenz et al., 2011; Araus and Cairns, 2014).

    Apparently, compared with the vast genetic information, there is

    a phenotyping bottleneck hampering progress in understanding

    the genetic basis of complex traits. To break this bottleneck

    and improve the eﬃciency of molecular breeding, reliable,

    automatic, and high-throughput phenotypic technologies were

    urgently needed to provide breeding scientists with new insights

    in selecting new species to adapt to the resources shortages and

    global climate change.

    During the past decade, plant phenomics has evolved from

    an emerging niche to a thriving research ﬁeld, which was

    deﬁned as the gathering of multi-dimensional phenotypic

    data at multiple levels from cell level, organ level, plant

    level to population level (Houle et al., 2010; Dhondt et al.,

    2013; Lobos et al., 2017). Crop phenotypes are extremely

    complicated because they are the result of interaction between

    genotypes (G) and a multitude of envirotypes (E) (Xu,

    2016). This interaction inﬂuences not only the growth and

    development process of crops measured by the structural

    traits on cellular, tissue, organ and plant level, but also the

    plant functioning measured by the physiological traits. These

    internal phenotypes in turn determine crop external phenotypes

    such as morphology, biomass and yield performance (Houle

    et al., 2010; Dhondt et al., 2013) (Figure 1). Crop phenomics

    research integrates agronomy, life sciences, information science,

    math and engineering sciences, and combines high-performance

    computing and artiﬁcial intelligence technology to explore

    multifarious phenotypic information of crop growth in a

    complex environment, of which the ultimate goal is to

    construct an eﬀective technical system able to phenotype crops

    in a high-throughput, multi-dimensional, big-data, intelligent

    and automatically measuring manner, and create a tool

    comprehensively integrating big data achieved from a multi-

    modality, multi-scale, phenotypic + environmental + genotypic

    condition, in order to develop new methods of mining genes

    associated with important agronomic traits, and propose new

    intelligent solutions for precision breeding.

    Here, we provide an overview of crop phenomics research,

    focusing on two parts, from phenotypic data collection through

    various sensors to phenomics analysis. We systematically

    introduced the crop phenotyping approaches from cellular,

    tissue, organ, and plant level to ﬁeld level, discussing application

    and practical problems in research. Based on overwhelming

    amounts of phenotypic data, we then discussed the phenotyping

    extraction, phenotype information analysis and knowledge

    storage. We emphasize Phenomics is entering the big-data era

    with multi-domain, multi-level, and multi-scale characteristics.

    We highlight necessity and importance of building multi-

    scale, multi-dimensional and trans-regional crop phenotyping

    big database, researching E-trait depth analysis schemas and

    technical systems for precise identiﬁcation of crop phenotypes,

    realizing

    functional-structural

    plant

    modeling

    based

    on

    phenomics, and developing bioinformatics technologies that

    integrate genomes and phenotypes.

    CROP PHENOTYPIC DATA

    COLLECTION: VARIOUS PHENOTYPING

    APPROACHES FOR CROP

    MORPHOLOGY, STRUCTURE, AND

    PHYSIOLOGICAL DATA FROM CELL TO

    WHOLE-PLANT

    Currently, the plant phenotyping community seems some-what

    divided between high-throughput, low-resolution phenotyping

    and in-depth phenotyping at lower throughput and higher

    resolution (Dhondt et al., 2013). Phenotyping systems and

    tools applied in diﬀerent scales are focused on diﬀerent

    key

    characteristics

    –automated

    phenotyping

    platforms

    in

    controllable environment and high-throughput methodologies

    in

    ﬁeld

    environment

    highlights

    high-throughput,

    while

    phenotyping covering the organ, tissue and cellular level

    emphasizes in-depth phenotyping and higher resolution. In

    this part, we systematically introduced the crop phenotyping

    approaches covering from cellular and tissue level to ﬁeld level,

    and discussed the application and practical problems of which

    technologies in crop researches.

    Alleviating the Micro-Phenotyping

    Bottleneck

    Compared with the whole-plant phenotying technologies,

    phenotyping at higher spatial and temporal resolutions of tissue

    and cellar scales is more diﬃcult (Hall et al., 2016). To reveal

    crop micro-phenotypes, the pre-treatment of plant or organ is

    usually destructive when sampled in a cumbersome and involves

    multi-step procedure, and the high resolution imaging of samples

    is also ineﬃcient in view of the output in micron level. More

    challenging, automated image analysis techniques are urgently

    needed to quantify cell and tissue traits of crop from the larger

    high qualiﬁed image. In recent years, many emerging algorithms

    and tools have been proposed to handle with microscopic

    images from hand-cutting slices, micro-CT, ﬂuorescent, laser, and

    paraﬃn sections imaging (Table 2).

    Root anatomical traits have important eﬀects on plant

    function, including acquisition of nutrients and water from

    the soil and transportation to the aboveground part. Over

    the last years, novel micro-image acquisition technologies

    and computer vision have been introduced to improve our

    understanding of anatomical structure and function of roots.

    In 2011, computer-aided calculation of wheat root vascular

    bundle phenotypic information extraction were introduced based

    on sequence images of paraﬃn sections (Wu et al., 2011).

    Burton et al. (2012) (Penn State College of Agricultural Sciences

    Roots Lab) have developed a high-throughput, high-resolution

    phenotyping platform, which combines laser optics and serial

    imaging with three-dimensional (3D) image reconstruction

    (RootSlice) and quantiﬁcation to understand root anatomy by

    semi-auto RootScan (Chimungu et al., 2015). Chopin et al.

    (2015) developed a fully automated RootAnalyzer software

    for root microscopic phenotypic information extraction to

    further improve image segmentation eﬃciency and ensure high

    Frontiers in Plant Science | www.frontiersin.org

    2

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 1 | The history of crop phenomics based on major advancement.

    The major advancements

    References

    Germination stage: the

    Concept formation period of

    phenotype and phenomics.

    The concept of phenotype was ﬁrst proposed by Danish geneticist Johannsen in 1911.

    Johannsen, 2014

    The concept of phenomics corresponding to genomics was ﬁrst proposed by Nicholas

    Schork in 1997 in disease research.

    Schork, 1997

    Tuberosa proposed the concept that ‘phenotyping was king and heritability was
    queen’,

    in 2012.

    Tuberosa, 2012

    Thriving development stage:

    from late 20th century, plant

    phenotypic research teams and

    commercial organizations were

    established successively, and a

    series of high-throughput,

    high-precision, automated or

    semi-automated phenotyping

    tools were developed to obtain

    high-quality, repeatable plant

    phenotype data.

    In 1998, Belgium CropDesign was the ﬁrst company to develop a high-throughput

    phenotyping platform for large-scale plant character evaluation.

    http://www.cropdesign.com; Reuzeau

    et al., 2005, 2006; Reuzeau, 2007

    The ﬁrst research center, named by phenomics-the Australian Plant Phenomics Facility,

    was established in 2007.

    https://www.plantphenomics.org.au

    In 2016, Germany Lemna Tec developed the ﬁrst ﬁeld high-throughput plant phenotype

    platform-Scanalyzer Field, which indicated that plant phenotype technology was

    formally moving toward the ﬁeld measurement.

    http://www.lemnatec.com; Virlet et al.,

    2016

    Alleviating the micro-phenotyping bottleneck: in recent years, many emerging

    algorithms and tools have been proposed to handle with microscopic traits of root,
    stalk

    and seed, such as RootAnalyzer, VesselParser, etc.

    Burton et al., 2012; Du et al., 2016

    Systematic development stage:

    is entering a new era called

    ’phenomics’, which provides

    big data and decision support

    for revealing the molecular

    mechanism and gene functions

    of plants.

    In 2011, the challenge-phenotyping bottleneck was pointed out by Furbank from
    the

    Australian Plant Phenomics Facility, discussing the bottleneck of phenotypic research

    and the problems need to be solved.

    Furbank and Tester, 2011

    The European Plant Phenotyping Network (EPPN) was originated from 2012, which

    successfully completed the ﬁrst EPPN joint research project from 2012 to 2015,
    and

    continued with the EPPN2020 and European Infrastructure for Multi-scale Plant

    Phenomics and Simulation (EMPHASIS) programs.

    https://eppn2020.plant-phenotyping.eu;

    https://emphasis.plant-phenotyping.eu

    In 2013, the concept of next-generation phenotyping was proposed by Mccoueh,

    suggesting that phenomics should be closely linked to technologies, such as

    high-resolution linkage mapping, genome-wide association studies and genomic

    selection models, etc.

    Cobb et al., 2013

    The International Plant Phenotyping Network (IPPN) was registered in 2016,

    representing the world’s major plant phenotyping centers. Over the last decade,
    a

    number of national and regional Plant Phenotyping Networks (PPNs) have been

    organized, such as FPPN. PPA, NAPPN, CPPN, etc., and the communication and

    cooperation among various PPNs became more and more close.

    https://www.plant-phenotyping.org/;

    Carrolla et al., 2019

    In 2017, Francois Tardieu and Malcolm Bennett presented strategies for multi-scale

    phenomics. Phenomics research not only needed to build a multi-domain, and

    multi-scale phenotypic big database, but also to research technical systems for

    phenotypic traits identiﬁcation and develop bioinformatics technologies for information

    extraction from the overwhelming amounts of omics data.

    Tardieu et al., 2017

    accuracy. Pan et al. (2018) (Beijing Key Laboratory of Digital

    Plant) introduced X-ray micro-CT into 3D imaging of maize root

    tissues and developed an image processing scheme for the 3D

    segmentation of metaxylem vessels. Compared with traditional

    manual measurement of vascular bundles of maize roots, the

    proposed protocols signiﬁcantly improved the eﬃciency and

    accuracy of the micron-scale phenotypic quantiﬁcation.

    Diﬀerent from the root system, crop stalks have a more

    complex

    microstructure.

    The

    complexity

    and

    diversity

    in

    microscopic

    image

    data

    poses

    greater

    challenges

    for

    developing suitable data analysis workﬂows in the detection

    and identiﬁcation of microscopic phenotypes of stalk tissue.

    Zhang et al. (2013) and Legland et al. (2017) presented semi-

    automated and automated analysis method for the stained

    microscopic images of stalk sections. Legland et al. (2014) and

    Heckwolf et al. (2015) created an image analysis tool that could

    operate on images of hand-cut stalk transections to measure

    anatomical features in high throughput. Those tools have

    signiﬁcantly improved the measurement eﬃciency of vascular

    bundle, but the anatomical traits corresponding to the rind

    and the detection accuracy remained a challenge. Du et al.

    (2016) (Beijing Key Laboratory of Digital Plant) introduced

    micro-computed tomography (CT) technology for stalk imaging

    and developed the VesselParser 1.0 algorithm, which made it

    possible to automatically and accurately analyze phenotypic

    traits of vascular bundles within entire maize stalk cross-sections.

    So far, Beijing Key Laboratory of Digital Plant has built a novel

    method to improve the X-ray absorption contrast of maize tissue

    suitable for ordinary micro-CT scanning. Based on CT images,

    they introduced a set of image processing workﬂows for maize

    root, stalk and leaf to eﬀectively extract microscopic phenotypes

    of vascular bundles (Zhang et al., 2018).

    Phenotyping at tissue and cellar scales still requires complex

    procedures. The need to simplify the sample preparation process

    and explore advanced imaging techniques is crucial to accelerate

    microscopic phenotyping studies. Moreover, image processing is

    another major bottleneck in micro-phenotyping researches. Due

    to the speciﬁc crop organ and cell phenotypic characteristics of

    the huge diﬀerences, most micro-image analysis algorithms have

    been developed for speciﬁc biological assays currently.

    Frontiers in Plant Science | www.frontiersin.org

    3

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    FIGURE 1 | The schematic diagram of genotype–phenotype–envirotype (G-P-E) interactions.

    Three-Dimensional Phenotyping at the

    Organ Level

    Most plant phenotyping platforms concentrated the high-

    throughput of individual plants (Chaivivatrakul et al., 2014;

    Cabrera-Bosquet et al., 2016). Hence the phenotypic accuracy of

    organs on the plants was always compromised (Dhondt et al.,

    2013). The most frequently used phenotyping index, such as

    leaf length, leaf area, and fruit volume, could be obtained in

    phenotyping platforms simply (Klukas et al., 2014; Zhang et al.,

    2017). The smartphone app platform was developed for ﬁeld

    phenotyping by taking pictures and image analysis at organ level

    (Confalonieri et al., 2017). It is very convenient to acquire leaf

    angles and leaf length using the app in a 2D view. The plants

    do not need to be destructively sampled indoors. 2D cameras

    are low cost and were integrated into most of the phenotyping

    platforms, which provided eﬀective phenotypic solution for

    branch structured plants, especially for tracking the dynamic

    growth of organs on plants (Brichet et al., 2017). However, 2D

    images lost another dimension data in 3D space, and some of

    the estimated morphological traits still need to be calibrated

    (Zhang et al., 2017). Multi-view stereo (MVS) approach (Duan

    et al., 2016; Vazquez-Arellano et al., 2016; He et al., 2017; Hui

    et al., 2018) is another popular low-cost alternative for organ

    level phenotyping. 3D point clouds were reconstructed using

    multi-view images through structure from motion techniques

    (Wu, 2011), and then phenotypic traits were extracted through

    segmented organs of individual plants (Thapa et al., 2018). This

    cost-eﬀective 3D reconstruction method depicts an alternative to

    an expensive laser scanner in the studied scenarios with potential

    for automated procedures (Rose et al., 2015; Yin et al., 2016; Gibbs

    et al., 2018). There are signiﬁcant diﬀerences between these organ

    traits within various cultivars, thus tiny errors could be ignored in

    large scale omics analysis (Yang et al., 2014; Zhang et al., 2017).

    However, besides these measured length, area, and volume

    phenotypic parameters, there are lots of obvious diﬀerences that

    can be observed by humans of plant organs, such as blade proﬁles,

    blade folds, vein curves, and leaf colors, of which are diﬃcult to

    obtain the morphological data and quantitatively describe their

    diﬀerences. A series of mathematical approaches (Li et al., 2018)

    need to be developed to quantitatively describe these diﬀerences

    in order to discover more detailed phenotyping traits with high

    precision phenotyping data of organ level. Researchers use high

    resolution 3D scanners to acquire the morphological structure

    of plant organs (Rist et al., 2018). High resolution 3D scanners

    are relatively expensive while the acquired morphological data

    are more accurate than MVS reconstructed organs, especially

    for non-planner surface plants. 2D LiDAR scanners (Thapa

    et al., 2018) and depth sensors (Hu et al., 2018) were also used,

    combined with turn table and translation devices to estimate

    phenotypic traits through 3D recovery of point clouds. Most 2D

    or 3D imaging could solve the 3D organ scale phenotyping data

    acquisition problems, except for tall plants for which it is diﬃcult

    due to a limited ﬁeld of vision. High resolution 3D point clouds

    of plant organs are quite complicated in order to extract more

    phenotypic traits, thus computer graphics algorithms, such as

    skeleton extraction (Huang et al., 2013), surface reconstruction

    (Yin et al., 2016; Gibbs et al., 2017), and feature-preserving

    remeshing (Wen et al., 2018), need to be applied to process

    Frontiers in Plant Science | www.frontiersin.org

    4

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 2 | Commonly used and developing approaches for crop micro-phenotypic traits
    analysis.

    Organ type

    Image source

    Software

    Parameters

    Remarks

    References

    Root

    Laser Ablation Tomography

    (LAT)

    RootScan

    RootScan2

    Root cross-section, cortex, and

    stele 3 categories of phenotype

    indicators

    Maize roots

    Burton et al., 2012

    Laser dissection

    microscope

    RootAnalyzer

    Whole root, tissue regions

    (cortex, stele, endodermis,

    metaxylem), stele phenotype

    indicators

    Wheat and maize root

    Chopin et al., 2015

    Laser Ablation Tomography

    (LAT)

    RootSlice

    Focus on root cortex, including

    variation in cell size, number of

    cell ﬁles in the radial direction,

    percentage of aerenchyma, cell

    wall thickness, amount of

    cytoplasm and vacuole size.

    Maize roots

    https://plantscience.psu.edu/

    research/labs/roots/projects

    Micro-CT images

    Simpeware

    (Commercial software)

    Not only the two-dimensional

    (2D) phenotypic parameters,

    but also the quantitative

    analysis of three-dimensional

    (3D) phenotypic parameters,

    such as volume and surface

    area of metaxylem vessels.

    Maize roots

    Pan et al., 2018

    Stalk

    Hand-cut stalk transections

    and color images of which

    were acquired using a ﬂat

    scanner

    ‘Matgeom’, a library for

    geometric computing

    with Matlab

    Average spatial organization of

    vascular bundles within maize

    stalks.

    The anatomical traits

    corresponding to the rind

    remained a challenge.

    http:

    //matgeom.sourceforge.net/

    Hand-cut stalk transections

    and color images of which

    were acquired using a ﬂat

    scanner

    The tool, written in the

    Matlab computer

    language

    Stalk diameter, rind thickness,

    vascular bundle density, and

    vascular bundle size.

    Maize, sorghum, and

    Miscanthus stalk. The

    anatomical traits

    corresponding to the rind

    and the detection accuracy

    of vascular bundle

    remained a challenge

    http://phytomorph.wisc.edu/

    download/

    HeckwolfPlantMethods2015/

    Colored with FASGA

    staining and digitalised with

    whole microscopy slide

    scanners

    The whole image

    processing workﬂow

    was developed within

    the ImageJ/Fiji platform

    Morphometry (bundle number,

    rind fraction, etc.) and

    colorimetry (rind mean red,

    ligniﬁed mean blue, etc.), 2

    categories of 19 phenotype

    indicators

    Maize stalk

    Zhang et al., 2013

    Micro-CT images

    VesselParser 3.0

    Stalk diameter, vascular bundle

    density, vascular bundle size,

    etc.

    This is the ﬁrst time that

    quantitative analysis for

    phenotypic traits of

    vascular bundles within

    entire maize stalk

    cross-section.

    Du et al., 2016; Zhang et al.,

    2018

    the high-resolution morphological data for 3D phenotypic trait

    extraction. Because of the diﬃculty and complexity of detailed

    organ data acquisition and analysis, Wen et al. (2017) proposed a

    data acquisition standard and constructed a resource database of

    plant organs using measured in situ morphological data, to realize

    the integration and sharing of high quality data of plant organs.

    Automated Phenotyping Platforms in

    Controllable Environment

    Automation and robotics, new sensors, and imaging technologies

    (hardware and software) have provided an opportunity for high-

    throughput plant phenotyping platforms (HTPPs) development

    (Gennaro et al., 2017). In the past 10 years, great improvements

    have been made in researching and developing HTPPs (Furbank

    and Tester, 2011; Fiorani and Schurr, 2013; Virlet et al., 2016).

    Depending on the overall design, HTPPs in a growth chamber or

    greenhouse can generally be classiﬁed as sensor-to-plant or plant-

    to-sensor based on whether the plants occupy a ﬁxed position

    during a measurement routine and an imaging setup moves to

    each of those positions or the plants are transported to an imaging

    station, respectively. Collectively, the techniques used in HTPPs

    in the growth chamber or greenhouse mainly include (Table 3):

    (1) RGB imaging, which obtains the phenotypes of plant

    morphology, color, and texture.

    (2) Chlorophyll

    ﬂuorescence

    imaging,

    which

    obtains

    photosynthetic phenotypes.

    (3) Hyperspectral imaging, which obtains phenotypes such as

    pigment composition, biochemical composition, nitrogen

    content, and moisture content.

    Frontiers in Plant Science | www.frontiersin.org

    5

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 3 | Summary of imaging techniques in high-throughput plant phenotyping platforms
    (HTPPs).

    Imaging

    technology

    Sensors

    Raw data

    Parameters

    Applications

    Visible light

    imaging

    Visible light camera

    Gray or color value

    images (RGB channels)

    Whole organs or organ parts, time series

    (minutes to days)

    Morphologic traits, digital biomass, height, etc.

    Assess plant growth status, nutritional status,

    and accumulated biomass.

    Fluorescence

    imaging

    Fluorescence

    cameras

    Pixel-based map of

    emitted ﬂuorescence in

    the red and far-red region

    Multiple chlorophyll ﬂuorescence parameters

    and multi-spectral ﬂuorescence parameters

    Photosynthetic status/quantum yield/seedling

    structure/leaf disease, etc.

    Infrared

    imaging

    Thermal imaging,

    Near-infrared

    cameras

    Pixel-based map of

    surface temperature in

    the infrared region

    Leaf area index, surface temperature, canopy

    and leaf water status, seed composition, time

    series (minutes to days)

    Measurements of leaf and canopy transpiration,

    heat dissipation, stomatal conductance

    differences, etc.

    Spectral

    imaging

    Spectrometers,

    hyperspectral

    cameras

    Continuous or discrete

    spectra

    Water content, seed composition, etc. indoor

    time series experiment

    Disease severity assessment/leaf and canopy

    growth potential.

    3D imaging

    Stereo camera/TOF

    camera systems

    RGB/IR/Depth images

    Plant or organ morphology, structure, and color

    parameters, time series at various resolutions

    Shoot structure, leaf angle, canopy structure,

    etc.

    Laser scanning

    Laser scanning

    instruments

    Depth maps, 3D point

    clouds

    Plant or organ morphology, structure

    parameters, time series at various resolutions

    Shoot structure, leaf angle, canopy structure,

    etc.

    MRI

    Magnetic

    resonance imagers

    Water (1H) mapping

    Water content, morphology parameters

    (200–500 µm), 1–600 s

    Morphometric parameters/water content.

    PET

    Positron emission

    detectors

    Radiotracer mapping and

    co-registration with

    positron emission signals

    Transport partitioning, sectorality, ﬂow velocity,

    1–2 mm, 10 s– 20 min

    Visualize the metabolic distribution and

    transport of radionuclides.

    CT

    X-ray tomography

    Voxels/tissue slices

    Morphometric parameters in 3D (1–100 µm),

    minutes- hours

    Tissue density, tiller number, seed quality, and

    tissue 3D reconstruction.

    (4) Thermal

    imaging,

    which

    obtains

    the

    plant

    surface

    temperature

    distribution,

    stomatal

    conductance

    and

    transpiration phenotype.

    (5) Lidar, which obtains the three-dimensional structure

    phenotype of plants.

    Moreover, other advanced imaging techniques widely used in

    medicine, such as MRI, PET, and CT, have also been introduced

    into HTPPs in the growth chamber or greenhouse. The following

    table lists the major imaging techniques used in the HTPPs in the

    growth chamber or greenhouse.

    After

    integration

    of

    information

    technology,

    digital

    technology and platform equipment with plant phenotyping,

    several HTPPs in growth chamber or greenhouse have been

    listed in Table 4. Such high-throughput phenotyping platforms

    are characterized by automation, high-throughput, and high

    precision, which greatly improve plant data collection eﬃciency

    and accuracy, in order to improve the eﬃciency of crop breeding.

    However, most HTPPs still have high construction, operating,

    and maintenance costs, and most academic and research

    institutions do not have access to these techniques as a result

    (Kolukisaoglu and Thurow, 2010). For example, the number of

    fully automatic and high-throughput phenotyping platforms is

    limited in China: one was developed by the Crop Phenotyping

    Center (CPC), http://plantphenomics.hzau.edu.cn/, and the

    others were introduced by LemnaTec, such as http://bri.caas.

    net.cn/, http://www.kenfeng.com/, and http://www.zealquest.

    com/. Because of this, there has been increased research into

    developing aﬀordable, small-scale plant phenotyping platforms

    and technology. In fact, aﬀordable phenotypic acquisition

    techniques or platforms are constantly being updated, such as

    the imaging system developed by Tsaftaris and Noutsos (2009)

    that uses wireless-connected consumer digital cameras, and

    the low-cost Glyph phenotyping platform (Pereyra-Irujo et al.,

    2012). Lowering the cost of these platforms could, therefore,

    signiﬁcantly increase the scope of phenotypic research and

    advance the rapid expansion of phenotypic–genotypes analysis

    for complex traits.

    High-Throughput Methodologies for

    Crop Phenotyping in Field Environment

    Field-based phenotyping (FBP) is a critical component of crop

    improvement through genetics, as it is the ultimate expression

    of the relative eﬀects of genetic factors, environmental factors,

    and their interaction on critical production traits, such as yield

    potential and tolerance to abiotic/biotic stresses (Araus and

    Cairns, 2014; Neilson et al., 2015). Currently, the most commonly

    ﬁeld-based phenotyping platforms (FBPPs) use ground wheeled,

    rigid motorized gantry or aerial vehicles, combined with a wide

    range of cameras, sensors and high-performance computing, to

    capture deep phenotyping data in time (throughout the crop

    cycle) and space (at the canopy level) in ﬁeld environments

    (Fritsche-Neto and Borém, 2015) (Table 5). It cannot be denied

    that the eﬃciency of ground wheeled phenotyping system is

    quite low if the plot area is too large (White et al., 2012;

    Zhang and Kovacs, 2012). In 2016, FIELD SCANALYZERS, with

    rigid motorized gantry supporting a weather proof measuring

    platform that incorporates a wide range of cameras, sensors and

    illumination systems, were developed. The facility, equipped with

    high-resolution visible, chlorophyll ﬂuorescence and thermal

    infrared camera, hyperspectral imager, and 3D laser scanner.

    Frontiers in Plant Science | www.frontiersin.org

    6

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 4 | Exhaustive list of digital technologies and platform equipments for
    crop phenotyping in controllable environment.

    HTPPs

    Sensor options

    Functions

    Application examples

    References

    WPScan Conveyor

    RGB sensor

    The world’s ﬁrst high-throughput

    phenotyping.

    Maize morphologic traits, digital biomass,

    height, et al.

    http://www.wps.eu/en

    Trait Mill

    RGB sensor

    TraitMill is the ﬁrst platform that

    combines genes on rice phenotypes.

    The TraitMill is a highly versatile tool for

    testing the effect of genes and gene

    combinations on plant phenotype.

    Focusing on japonica rice, but does some

    work on maize.

    http://www.cropdesign.com; Reuzeau,

    2007; Reuzeau et al., 2010

    Scanalyzer

    Plant-to-Sensor

    RGB Visible; PS2

    Fluorescence;

    Fluorescence; Near

    Infrared.

    Hosting different sensors to capture

    multiple data points per plant;

    Ranging from small versions for few

    plants up to large installations for

    several hundreds of plants.

    Corn leaf segment; graph to object

    converter; HSI TO GRAY converter, etc.

    Golzarian et al., 2011; Chen D. et al.,

    2014; Neilson et al., 2015; Amanda

    et al., 2016; Arend et al., 2016b; Cai

    et al., 2016; Guo et al., 2017; Liang

    et al., 2017; Majewsky et al., 2017;

    Meng et al., 2017; Neumann et al.,

    2017; Pandey et al., 2017; Parlati et al.,

    2017; Tomé et al., 2017; van de Velde

    et al., 2017

    Sensor-to-Plant

    Visible light camera;

    Chlorophyll

    ﬂuorescence camera;

    Infrared camera;

    Hyperspectral cameras;

    3D Laser scanner.

    Do not to move the plants to avoid

    mechanical stress;

    Motorized gantry can be ﬁtted inside

    the greenhouse to transport sensors

    above the plants.

    Ground cover

    Canopy height

    Plant geometry

    Growth and biomass

    Counting features

    Growth stages

    Vegetation indices

    Chlorophyll ﬂuorescence parameters

    http://www.lemnatec.com

    KeyGene digital

    phenotyping

    PhenoFab R⃝

    RGB sensor

    A greenhouse phenotyping platform;

    The capacity: 1.400 plants.

    Seed treatments in sugar beet;

    Investigate the physiological effects of corn.

    http://www.keygene.com

    Plant Screen

    PlantScreen

    Modular System

    Multiple imaging

    sensors

    Integrated robotic solution for

    high-precision digital plant phenotyping

    and plant cultivation of mid-scale size

    up to large plants in greenhouse or

    semi-controlled environment.

    High-throughput screening

    Morphology and growth assessment

    Nutrient management

    Photosynthetic performance

    Abiotic stress

    Pathogen interaction

    Trait identiﬁcation

    Chemical screening

    Nutrient effects

    Arabidopsis.

    http://www.psi.cz/;

    Silsbe et al., 2015

    (Continued)

    Frontiers in Plant Science | www.frontiersin.org

    7

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 4 | Continued

    HTPPs

    Sensor options

    Functions

    Application examples

    References

    PHENOSPEX

    PlantEye F500

    3D Laser, multispectral

    camera

    Multispectral 3D Scanner for plants

    Compute automatically a wide variety of

    morphological parameters such as: plant

    height, 3D leaf area, projected leaf area,

    digital biomass, leaf inclination, leaf area

    index, light penetration depth and leaf

    coverage.

    Typical applications like germination assays,

    drug screening, experimental control,

    documentation, quality control or

    phenotyping.

    http://phenospex.com;

    Vadez et al., 2015

    DroughtSpotter

    Automated gravimetric

    sensors

    Drought research and breeding

    MobileDevice

    PlantEye

    F400/PlantEye F500

    For lab and greenhouse automation

    Rice automatic phenotyping

    platform (RAP)

    Color imaging device,

    linear X-ray computed

    tomography (CT), etc.

    A phenotyping facility for

    high-throughput and automatic

    phenotypic screening of rice

    germplasm resources and populations

    throughout the growth period and after

    harvest.

    Combination of the multifunctional

    phenotyping tools RAP and GWAS to

    investigate the genetic control of rice and

    maize growth and development.

    http://plantphenomics.hzau.edu.cn/;

    Yang et al., 2015; Zhang et al., 2017

    SCREEN House

    PlantScreen

    Self-Contained (SC)

    Systems/

    PlantScreen

    Compact System

    RGB digital color

    imaging,

    Kinetic, chlorophyll

    ﬂuorescence Imaging,

    Hyperspectral Imaging,

    Thermal Imaging,

    3D Scanning, etc.

    The system is designed for digital

    phenotyping of small and mid-sized

    plants up to 50 cm in height.

    Arabidopsis, strawberries, turf grass,

    soybean, tobacco, corn seedlings, etc.

    Morphology and growth assessment

    Nutrient management

    Photosynthetic performance

    Abiotic stress

    Pathogen interaction

    Trait identiﬁcation

    Chemical screening

    Nutrient effects

    http://qubitphenomics.com;

    Berger et al., 2007

    SCREEN House

    RGB camera

    Monitoring plant water status; shoot

    structure of plant.

    This system is used for screening of the

    shoot structure and function of different

    plant species (e.g., canola, maize, tomato,

    cereals) in a greenhouse;

    Continuous monitoring of plant water status

    and the environmental conditions

    Nakhforoosh et al., 2016

    PHENOPSIS

    RGB camera, infrared

    camera.

    Automated phenotyping platform

    allowing a culture of approximately

    200–500 Arabidopsis plants in

    individual pots with automatic watering

    and imaging system

    Rosette area or leaf area measurements

    through image analysis;

    Photosynthesis and stomatal conductance

    measurements.

    http://bioweb.supagro.inra.fr/phenopsis;

    Granier et al., 2006

    Frontiers in Plant Science | www.frontiersin.org

    8

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    Crops within a 10–20 m × 110–200 m area can be monitored,

    which realizes the continuous, automatic, and high-throughput

    detection of crop phenotyping detection in ﬁeld (Virlet et al.,

    2016; Sadeghi-Tehran et al., 2017). Meanwhile, the cable-

    suspended ﬁeld phenotyping platform covering an area of ∼1 ha

    was also developed for rapid and non-destructive monitoring

    of crop traits (Kirchgessner et al., 2016). However, these

    large ground-based ﬁeld phenotyping platforms also have high

    construction, operating, and maintenance costs, and it has to

    be located at certain sites limiting the scale at which it can

    be used. Furthermore, the ground-based FBPPs are not very

    suitable for large crops, such as maize, except in the early stages

    (Montes et al., 2011).

    In recent years, manned aircraft and unmanned aerial vehicle

    remote sensing platforms (UAV-RSPs) are becoming a high-

    throughput tool for crop phenotyping in the ﬁeld environment

    (Berni et al., 2009; Liebisch et al., 2015), which meet the

    demands of spatial, spectral, and temporal resolutions (Yang

    et al., 2017) (Table 5). For example, thermal sensors ﬁtted

    to manned aircraft were used to measure canopy temperature

    (Deery et al., 2016; Rutkoski et al., 2016). The sensors that UAV-

    RSPs carried typically included digital cameras, infrared thermal

    imagers, light detection and ranging (LIDAR), multispectral

    cameras, and hyperspectral sensors, which are applied to:

    canopy surface modeling and crop biomass estimation based

    on visible imaging; crop physiological status monitor, such

    as chlorophyll ﬂuorescence and N levels, based on visible–

    near-infrared spectroscopy and high-resolution hyperspectral

    imaging; plant water status detection based on thermal imaging;

    and crop ﬁne-scale geometric traits analysis based on LIDAR

    point clouds (Sugiura et al., 2005; Overgaard et al., 2010; Swain

    et al., 2010; Wallace et al., 2012; Gonzalez-Dugo et al., 2013,

    2014, 2015; Mathews and Jensen, 2013; Diaz-Varela et al., 2014;

    Nigon et al., 2015; Gómez-Candón et al., 2016; Camino et al.,

    2018; Roitsch et al., 2019). There are deﬁnite advantages for UAV-

    RSPs, including portable, high monitoring eﬃciency, low-cost,

    and suitability for ﬁeld environments. On the other hand, some

    limiting factors for UAV-RSPs also exist, including the lack of

    methods for fast and automatic data processing and modeling,

    the strict airspace regulations, and vulnerable to diﬀerent weather

    conditions. Recently, combining ground-based platforms and

    aerial platforms for phenotyping oﬀers ﬂexibility. For example,

    the tractor-based proximal crop-sensing platform, combined

    with UAV-based platform, was used to target complex traits

    such as growth and RUE in sorghum (Potgieter et al., 2018;

    Furbank et al., 2019).

    CROP PHENOMICS: FROM

    PHENOTYPING EXTRACTION, DATA

    STORAGE TO KNOWLEDGE ANALYSIS

    Phenomic experiments are not directly reproducible because

    of the multi-model of sensors (structure, morphology, color,

    and physiology information), multi-scales phenotypic data (from

    cellular to population level), and variability of environmental

    conditions. Crop phenotypic data collection is only the ﬁrst step

    in Phenomic research. How to extract phenotypic traits from raw

    data? How to realize phenotype data standardization and storage?

    How to make cross-scale, cross-dimensionality meta-analyses?

    Finally, based on the phenotypic big data, how to realize the

    model-assisted phenotyping and phenotypic-genomic selection?

    Hereby this part focuses on hot topics in crop phenomics

    analysis raised above. We then suggest that research in this

    area is entering a new stage of development using artiﬁcial

    intelligence technology, such as deep learning methods, in

    which can help researchers transform large numbers of omic-

    data into knowledge.

    Phenotype Extraction

    Image-based phenotyping, as an important and all-purpose

    technique, has been applied to measure and quantify vision-

    based and model-based traits of plant in the laboratory,

    greenhouse and ﬁeld environments. So far, lots of image

    analysis methods and softwares have been developed to perform

    image-based plant phenotyping (Fahlgren et al., 2015b). From

    image analysis perspective, phenotypic traits of plant can be

    classiﬁed into 4 categories, i.e., quantity, geometry, color and

    texture, and are also classiﬁed into linear and non-linear

    features related to the pixel representation. Moreover, some

    valuable agronomic and physiological traits can be derived

    from image features. Generally, image techniques are specially

    designed according to speciﬁc crop varieties and phenotypic

    traits of interest, and always require with prior knowledge

    of research objects, as well as more or less man-machine

    interaction. Under highly controlled conditions, the classic

    image processing pipeline can provide acceptable phenotyping

    results, such as biomass (Leister et al., 1999), NDVI (Walter

    et al., 2015), chlorophyll responses (Walter et al., 2015), and

    compactness (Vylder et al., 2012) etc. However, the simple

    image processing pipeline is still very diﬃcult to handle

    with non-linear, non-geometric phenotyping tasks (Pape and

    Klukas, 2015). Facing species and environment diversities,

    fully automated and intelligent image analysis will remain a

    long-term challenge.

    Machine

    learning

    techniques,

    such

    as

    support

    vector

    machines, clustering algorithms, and neural networks, have

    been widely utilized to image-based phenotyping, which not

    only improve the robustness of image analysis tasks, also

    relieve tedious manual intervention (Singh et al., 2016). There

    is no doubt that machine learning techniques will have a

    prominent role in breaking through the bottlenecks of plant

    phenotyping (Tsaftaris et al., 2016). In a broad category of

    machine learning techniques (Ubbens and Stavness, 2017),

    deep learning demonstrates impressive advantages in many

    image-based tasks, such as object detection and localization,

    semantic segmentation, image classiﬁcation, and others (Lecun

    et al., 2015). In essence, deep convolutional neural networks

    (CNNs) are well suited to many vision-based computer

    problems, e.g., recognition (Lecun, 1990), classiﬁcation (He

    et al., 2015), and instance detection and segmentation (Girshick,

    2015). Compared with the traditional image analysis methods,

    CNNs are simultaneously trained from end to end without

    image feature description and extraction procedures. As far

    Frontiers in Plant Science | www.frontiersin.org

    9

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    TABLE 5 | Exhaustive list of characteristics and application of ﬁeld phenotypic
    platforms.

    Field-based phenotyping

    platforms (FBPPs)

    Sensor options

    Functions

    Application examples

    References

    Ground-based

    ﬁeld

    phenotyping

    platforms

    Field

    Scanalyzers

    Visible light; Infrared

    imaging; Hyperspectral

    imaging; PS2

    Fluorescence; Laser

    Scanners;

    environmental sensors.

    Capture deep phenotyping

    data from crops and other

    plants growing in ﬁeld

    environments.

    Ground cover, Canopy height, Plant

    geometry, Growth and biomass,

    Counting features, Growth stages,

    Vegetation indices, Chlorophyll

    ﬂuorescence parameters.

    Virlet et al., 2016;

    Sadeghi-Tehran et al.,

    2017

    FieldScan

    PlantEye sensors

    For ultra-high-throughput

    plant phenotyping under

    ﬁeld- or semi-ﬁeld

    conditions with throughputs

    of 5,000 plants or higher

    per hour.

    Automatically compute a wide

    variety of morphological parameters

    such as: Plant height, 3D leaf area,

    Projected leaf area, Digital biomass,

    Leaf inclination, Leaf area index,

    Light penetration depth, Leaf

    coverage.

    http://phenospex.com;

    Vadez et al., 2015

    PlantScreen

    Field Systems

    Hyperspectral imaging;

    Fluorescent imaging;

    Thermal imaging

    An autonomous drive pivot

    tower contains multiple

    sensor nodes mounted on

    an XZ– robotic arm.

    Plant height evaluation and leaf

    overlap detection, rapid

    non-invasive measurement of

    photosystem II activity, analysis of

    plant’s responses to heat load and

    water deprivation, and 3D plant

    reconstruction

    http:

    //qubitphenomics.com

    ETH Field

    Phenotyping

    Platform (FIP)

    DSLR; laser scanner;

    thermal camera.

    Cable-suspended ﬁeld

    phenotyping platform

    covering an area of ∼1 ha

    Monitoring canopy cover, canopy

    height and traits related to thermal

    and multi-spectral imaging of

    selected examples from winter

    wheat, maize and soybean.

    Kirchgessner et al.,

    2016

    Phenomobile

    Lite

    LiDAR; RGB camera;

    hyperspectral camera;

    thermal camera.

    A variety of crops less than

    1.5 m in height. Can be

    adapted for row/vine crops

    Non-destructive ﬁeld phenotyping

    of both wheat and rice yielding

    estimates of canopy height,

    fractional ground cover, greenness

    vertical distribution, leaf area, plant

    counts, visual assessments.

    https:

    //www.plantphenomics.

    org.au/;

    Jimenez-Berni et al.,

    2018

    UAV platform

    Airborn

    LiDAR; Hyperspectral

    camera

    Plant height estimation, LAI

    estimation, Biomass

    estimation, Leaf N

    concentration detection

    Maize and wheat Plant height, LAI,

    aboveground

    Biomass;

    Potato leaf N concentration

    Li et al., 2014; Nigon

    et al., 2015; Tattaris

    et al., 2016

    Multi-rotor UAV

    RGB camera;

    multispectral camera;

    hyperspectral camera;

    thermal camera.

    Physiological conditions

    assessment, crop growth

    monitoring, green canopy

    cover and LAI estimation,

    Plant height and biomass

    estimation, Vegetation

    monitoring.

    Barley, soybean, maize, sunﬂower,

    wheat, rice, onion, citrus, vineyard

    phenotypic analysis.

    Zarco-Tejada et al.,

    2014; Yang et al.,

    2017; etc.

    Fixed-wing UAV

    RGB camera;

    multispectral camera;

    hyperspectral camera;

    thermal camera.

    Lodging estimation, weed

    detection, estimation of net

    photosynthesis, grain yield

    prediction, stress detection.

    Maize, citrus, vineyard, peach

    phenotypic analysis.

    Overgaard et al., 2010;

    Li et al., 2014; Yang

    et al., 2017

    Flying wing

    Multispectral camera

    Agricultural surveillance and

    decision support.

    Cherries mature ratio.

    Herwitz et al., 2004

    Helicopter

    RGB camera;

    multispectral camera.

    Ground cover estimation;

    yield prediction, biomass

    estimation, LAI and

    Chlorophyll estimation.

    Sorghum, rice, corn, olive

    phenotyping detection.

    Berni et al., 2009;

    Swain et al., 2010;

    Chapman et al., 2014

    as plant phenotyping, CNNs have been eﬀectively applied

    to detect and diagnosis (Mohanty et al., 2016), classify fruits

    and ﬂowers (Pawara et al., 2017), and count leaf number

    (Ubbens and Stavness, 2017). It is worth noting that those

    vision-based phenotyping tasks were driven by the massive

    captured and annotation plant images. From the view of machine

    vision perspective, deep learning has been a fundamental

    technique

    framework

    in

    image-based

    plant

    phenotyping

    (Tsaftaris et al., 2016).

    Phenotype Data Standardization and

    Storage

    A huge amount of complex data and the integration of a

    wide range of image, spectral and environmental data can be

    Frontiers in Plant Science | www.frontiersin.org

    10

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    generated through by the above phenotypic technologies, usually

    up to GB or even petabytes, unstructured “Big Data.” Thus,

    the eﬃcient storage, management and retrieval of phenotypic

    data have become the important issues to be considered

    (Wilkinson et al., 2016). The current universally accepted

    principle of information standardization includes three aspects:

    (i) the ‘minimum information’ (MI) approach is recommended

    to deﬁne the content of the data set, (ii) ontology terms is

    applied for the unique and repeatable annotation of data, and

    in the form of data sharing and meta-analyses (Krajewski et al.,

    2015; Coppens et al., 2017), (iii) and proper data formats,

    such as CSV, XML, RDF, MAGE-TAB, etc., are chosen for

    the construction of data sets. Up until now, a number of

    phenotyping resources have been built ranging from phenotypic

    data of one species to multi-data types (Coppens et al.,

    2017). In 2010, PODD was developed for capturing, managing,

    annotating and distributing the data to support both Australian

    and international biological research communities (Li et al.,

    2010); Fabre et al. (2011) built a PHENOPSIS DB information

    system for Arabidopsis thaliana phenotypic data acquired by

    the PHENOPSIS phenotyping platform; Bisque is the ﬁrst web

    based, cross-platform, developed into a repository to store,

    visualize, organize and analyze images in the cloud (Kvilekval

    Das et al., 2010; Goﬀ et al., 2011). In 2014, ClearedLeaves

    DB, an on open online database, was built to store, manage

    and access leaf images and phenotypic data (Das et al., 2014);

    AraPheno1 was the ﬁrst comprehensive, central repository

    of population-scale phenotypes (it integrated more than 250

    publicly available phenotypes from six independent studies)

    for A. thaliana inbred lines (Seren et al., 2017); PhenoFront

    was a publicly available dataset of above-ground plant tissue

    to the LemnaTec Phenotyper platform (Fahlgren et al., 2015a);

    in 2016, the plant genomics and phenomics research data

    repository (PGP) were developed by the Leibniz institute

    of plant genetics and crop plant research and the German

    plant phenotyping network to comprehensively publish plant

    phenotypic and genotypic data (Arend et al., 2016a). Obviously,

    from the perspective of database data standardization and

    storage, the storage scheme based on “cloud technology” is

    becoming the trend for the development of plant phenotype data

    storage. Cloud storage system can optimize the design of the

    system architecture, ﬁle structure, high-speed cache, etc., for the

    plant phenotype platform. At present, all kinds of phenotypic

    data collection platforms are still relatively independent, and

    have not been established at the level of regions, countries

    or continents. Through the advanced technology of artiﬁcial

    intelligence, establishing a typical crop phenotype database

    based on the multi-layer phenotypic information, for example

    GDB Human Genome Database, will of interest to a range

    of stakeholders.

    Model-Assisted Phenotyping:

    Functional–Structural Plant Modeling

    Plants are highly plastic to genotypes, environment and

    management via changing morphological traits and adjusting

    1https://arapheno.1001genomes.org

    their physiological behavior. Complex interactions between

    genotypes, environment and managements at diﬀerent scales

    determine the development of plants, but their separate

    contribution to the phenotype remains unclear. Dynamic

    models have been proven to be an eﬃcient tool in dissection

    of abiotic and biotic eﬀects on plant phenotypes (Tardieu

    and Tuberosa, 2010). Functional–structural plant (FSP) (Vos

    et al., 2010) models simulate plant growth and development

    in

    time

    and

    three-dimension

    (3D)

    space,

    and

    quantify

    complex interactions between architecture and physiological

    processes. The combination of FSP modeling and phenotyping

    have been used as a facile technique to address research

    questions in two ways.

    Firstly, FSP models oﬀer a tool to dissect phenotype governed

    by a set of mechanisms. For example, Zhu et al. (2015) used

    a FSP model to dissect net biodiversity eﬀect into the eﬀect

    induced by interspeciﬁc trait diﬀerences, and the eﬀect induced

    by phenotypic plasticity by simulating whole-vegetation light

    capture for scenarios with and without phenotypic plasticity

    based on experimental plant trait data. The separate eﬀect of

    each architectural trait (leaf angle, leaf curvature and internode

    length etc.) on dry mass production and light interception

    was quantiﬁed by simulating canopy growth using a dynamic

    FSP model for tomato (Chen T.W. et al., 2014). Radiation

    interception and radiation use eﬃciency were dissected into an

    environmental and a genetic term via conducting virtual multi-

    genotype canopies, in which the FSP model for maize was applied

    to calculate light interception for each plant (Chen et al., 2018).

    FSP modeling has proven to be highly eﬀective for disentangling

    the relative contribution of each underlying process.

    Secondly, high-throughput phenotyping techniques facilitate

    the automate and precise calibration of FSP models. Image-

    based analysis was performed daily to reconstruct individual

    maize architecture, in order to calculate light interception using a

    FSP model and to estimate leaf area and the fresh plant weight

    of individual plants (Cabrera-Bosquet et al., 2016). Terrestrial

    LiDAR scanning was used to reconstruct complex tree canopy

    for predicting three-dimensional distribution of microclimate-

    related quantities in terms of net radiation, surface temperature

    and evapotranspiration (Bailey et al., 2016). OPENSIMROOT

    integrated a root model that can simulate growth of a root

    system with 3D phenotyping techniques, such as magnetic

    resonance imaging (MRI) and X-ray computed tomography (CT)

    (Postma et al., 2017). Phenotyping techniques not only provide an

    eﬃcient method to evaluate the ability of the model to simulate

    plant architecture and geometry but also help researchers to

    understand functional responses based on images.

    Phenotype–Genotype Association

    Analysis

    Although genomic data has a major role in crop genetic

    improvements and breeding programs, with the advent of the

    era of omics, considerable gain can only be achieved by tightly

    coupling genomic discovery to plant phenomics (Cobb et al.,

    2013; Bolger et al., 2017). In recent years, phenomic researches

    that combine genomic data with data on quantitative variation

    Frontiers in Plant Science | www.frontiersin.org

    11

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    in phenotypes have been initiated in many species, which

    rapidly decoded the function of a mass of unknown genes and

    improved understanding the G-P map (Campbell et al., 2015,

    2017; Neumann et al., 2017).

    Many agronomic traits are complex and controlled by many

    genes, each with a small eﬀect. Identifying the molecular basis

    of such complex traits requires genotyping and phenotyping

    of suitable mapping populations, enabling quantitative trait

    locus (QTL) mapping and genome-wide association studies

    (GWAS), which have been widely carried out in crop plants

    (Salvi and Tuberosa, 2015; Muraya et al., 2017). Busemeyer et al.

    (2013) associated phenotypic traits of small grain cereals with

    genome information to dissect the genetic architecture of biomass

    accumulation. In 2014, based on 13 traditional agronomic traits

    and 2 newly deﬁned traits of rice, Yang et al. (2014) identiﬁed

    141 associated loci by GWAS. In 2015, Combining GWAS with

    29 leaf traits at three growth stages using high-throughput leaf

    scoring (HLS), 73 new loci with leaf size, 123 of leaf color,

    and 177 of leaf shape were detected (Yang et al., 2015). In

    2017, large-scale quantitative trait locus (QTL) mapping was

    performed, combined with 106 agronomic traits of maize inbred

    line from seedling to tasselling stage, and a total of 988 QTLs were

    identiﬁed (Zhang et al., 2017). Also, plant sizes of 252 diverse

    maize inbred lines were monitored at 11 diﬀerent developmental

    time points, and 12 main-eﬀect marker-trait associations were

    identiﬁed (Muraya et al., 2017).

    Obviously, combining the high-throughput phenotyping

    technology

    and

    large-scale

    QTL

    or

    GWAS

    analysis

    not

    only greatly expanded our knowledge of the crop dynamic

    development process but also provided a novel tool for studies of

    crop genomics, gene characterization and breeding. We believe

    that with a complete system of genetic information, combined

    with crop high-throughput phenotyping technology, phenotypic-

    genomic analysis will revolutionize how we deal with complex

    traits and underpin a new era of crop improvement.

    FUTURE CHALLENGES AND

    PROSPECTS

    Phenomics is entering the era of ‘Big Data,’ thus the crop

    science community need to combine artiﬁcial intelligence

    technology and collaborative research at the national and

    international levels, to build a new theory for analyzing crop

    phenotypic

    information,

    construct

    an

    eﬀective

    technical

    system

    able

    to

    phenotype

    crops

    in

    a

    high-throughput,

    multi-dimensional,

    big-data,

    intelligent

    and

    automatically

    measuring

    manner,

    and

    create

    a

    tool

    comprehensively

    integrating big data achieved from a multi-modality, multi-

    scale, phenotypic + environmental + genotypic condition.

    There is no denying that there are many challenges that crop

    phenomics need to address in the next 5–10 years, such as:

    (1) Phenomics is entering the big-data era with high-

    throughput, multi-dimensionality, and multi-scale. We

    emphasize various phenotyping approaches for crop

    morphology,

    structure,

    and

    physiological

    data

    with

    three multi- characteristics: multi-domain (phenomics,

    genomics etc.), multi-level (traditional small to medium

    scale up to large-scale omics), and multi-scale (crop

    morphology, structure, and physiological data from cell

    to whole-plant). The single and individual phenotypic

    information cannot satisfy the association analysis in

    the new era called ‘-omics,’ and the systematic and

    complete phenomics information will be the foundation of

    future research.

    (2) In

    response

    to

    emerging

    challenges,

    new

    methods

    and techniques based on artiﬁcial intelligence shall be

    introduced to advance image-based phenotyping. An

    automated phenotyping system and platform result in

    lots of digital features, which need to prove their values

    throughout large sample statistics and relationship analysis

    with traditional agronomic traits. How to precisely and

    eﬃciently evaluate, understand and interpret these digital

    image-based features, and dig out valuable quantitative

    traits for functional genomes are key problems in the

    development and application of plant phenotyping.

    (3) With the multi-domain, multi-level, and multi-scale

    phenotypic information, we urgently need to make use

    of the latest achievements of artiﬁcial intelligence in

    depth learning, data fusion, hybrid intelligence and swarm

    intelligence to develop big-data management producers for

    supporting data integration, interoperability, ontologies,

    shareability and globality.

    (4) Modeling is a powerful tool to understand G × E × M

    interactions, identify key traits of interest for target

    environments. Nevertheless, several scientiﬁc and technical

    challenges need to be overcome. For example the

    validity and practicality of the models in terms of

    modeling processes and their interactions need further

    veriﬁcation, and the interaction and feedbacks of multi-

    scale phenotypes between modeling processes also need

    to be solved. Only then will we be able to streamline and

    speed up the tortuous gene-to-phenotype journey through

    modeling to develop the required agricultural outputs and

    sustainable environments for everybody.

    (5) Crop genotype (G) -phenotype (p) -envirotype (E)

    information comprehensive analysis and utilization. In

    short, as Coppens et al. (2017) said “the future of

    plant phenotyping lies in synergism at the national and

    international levels.” We need to seek novel solutions to the

    grand challenges of multi-omics data, such as intelligent

    data-mining analytics, which oﬀers a powerful tool to

    unravel the biological processes governing plant growth

    and development, and to advance plant breeding for much-

    needed climate-resilient and high-yielding crops.

    AUTHOR CONTRIBUTIONS

    CZ, XG, and YZ conducted the literature survey and drafted the

    article. JD, WW, SG, JW, and JF provided the data and tables

    about the crop phenotyping researches by NERCITA. All authors

    read and approved the ﬁnal manuscript.

    Frontiers in Plant Science | www.frontiersin.org

    12

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    FUNDING

    This study was supported by the Construction of Collaborative

    Innovation Center of Beijing Academy and Forestry Science

    (Collaborative

    Innovation

    Center

    of

    Crop

    Phenomics),

    the National Natural Science Foundation of China (Nos.

    31801254 and 31671577), Research Development Program of

    China (No. 2016YFD0300605-01), and Beijing Academy of

    Agricultural and Forestry Sciences Grant (Nos. KJCX20170404

    and KJCK20180423).

    REFERENCES

    Amanda, D., Doblin, M. S., Galletti, R., Bacic, A., Ingram, G. C., and Johnson,
    K. L.

    (2016). Defective kernel1 (DEK1) regulates cell walls in the leaf epidermis. Plant

    Physiol. 172, 2204–2218. doi: 10.1104/pp.16.01401

    Araus, J. L., and Cairns, J. E. (2014). Field high-throughput phenotyping: the
    new

    crop breeding frontier. Trends Plant Sci. 19, 52–61. doi: 10.1016/j.tplants.2013.

    09.008

    Arend, D., Junker, A., Scholz, U., Schüler, D., Wylie, J., and Lange, M. (2016a).
    PGP

    repository: a plant phenomics and genomics data publication infrastructure.

    Database 2016:baw033. doi: 10.1093/database/baw033

    Arend, D., Lange, M., Pape, J. M., Weigelt-Fischer, K., Arana-Ceballos, F., Mücke,

    I., et al. (2016b). Quantitative monitoring of Arabidopsis thaliana growth and

    development using high-throughput plant phenotyping. Sci. Data 3:160055.

    doi: 10.1038/sdata.2016.55

    Bailey, B. N., Stoll, R., Pardyjak, E. R., and Miller, N. E. (2016). A new three-

    dimensional energy balance model for complex plant canopy geometries: model

    development and improved validation strategies. Agric. For. Meteorol. 218,

    146–160. doi: 10.1016/j.agrformet.2015.11.021

    Berger, S., Benediktyová, Z., Matous, K., Bonﬁg, K., Mueller, M. J., Nedbal,

    L., et al. (2007). Visualization of dynamics of plant-pathogen interaction by

    novel combination of chlorophyll ﬂuorescence imaging and statistical analysis:

    diﬀerential eﬀects of virulent and avirulent strains of P. Syringae and of

    Oxylipins on A. thaliana. J. Exp. Bot. 58, 797–806. doi: 10.1093/jxb/erl208

    Berni, J. A. J., Zarco-Tejada, P. J., Suarez, L., and Fereres, E. (2009). Thermal

    and narrowband multispectral remote sensing for vegetation monitoring from

    an unmanned aerial vehicle. IEEE Trans. Geosci. Remote Sens. 47, 722–738.

    doi: 10.1109/tgrs.2008.2010457

    Bolger, M., Schwacke, R., Gundlach, H., Schmutzer, T., Chen, J., Arend, D., et
    al.

    (2017). From plant genomes to phenotypes. J. Biotechnol. 261, 46–52. doi:

    10.1016/j.jbiotec.2017.06.003

    Brichet, N., Fournier, C., Turc, O., Strauss, O., Artzet, S., Pradal, C., et al.
    (2017).

    A robot-assisted imaging pipeline for tracking the growths of maize ear and

    silks in a high-throughput phenotyping platform. Plant Methods 13:12. doi:

    10.1186/s13007-017-0246-7

    Burton, A. L., Williams, M., Lynch, J. P., and Brown, K. M. (2012). Rootscan:

    software for high-throughput analysis of root anatomical traits. Plant Soil 357,

    189–203. doi: 10.1007/s11104-012-1138-2

    Busemeyer, L., Ruckelshausen, A., Möller, K., Melchinger, A. E., Alheit, K. V.,

    Maurer, H. P., et al. (2013). Precision phenotyping of biomass accumulation

    in triticale reveals temporal genetic patterns of regulation. Sci. Rep. 3:2442.

    doi: 10.1038/srep02442

    Cabrera-Bosquet, L., Fournier, C., Brichet, N., Welcker, C., Suard, B., and Tardieu,

    F. (2016). High-throughput estimation of incident light, light interception and

    radiation-use eﬃciency of thousands of plants in a phenotyping platform. New

    Phytol. 212, 269–281. doi: 10.1111/nph.14027

    Cai, J., Okamoto, M., Atieno, J., Sutton, T., Li, Y., and Miklavcic, S. J. (2016).

    Quantifying the onset and progression of plant senescence by color image

    analysis for high throughput applications. PLoS One 11:e0157102. doi: 10.1371/

    journal.pone.0157102

    Camino, C., González-Dugo, V., Hernández, P., Sillero, J. C., and Zarco-Tejada,

    P. J. (2018). Improved nitrogen retrievals with airborne-derived ﬂuorescence

    and plant traits quantiﬁed from VNIR-SWIR hyperspectral imagery in the

    context of precision agriculture. Int. J. Appl. Earth Obs. Geoinf. 70, 105–117.

    doi: 10.1016/j.jag.2018.04.013

    Campbell, M. T., Du, Q., Liub, K., Brien, C. J., Berger, B., Zhang, C., et al.
    (2017).

    A comprehensive image-based phenomic analysis reveals the complex genetic

    architecture of shoot growth dynamics in rice (Oryza sativa). Plant Genome 10,

    1–14. doi: 10.3835/plantgenome2016.07.0064

    Campbell, M. T., Knecht, A. C., Berger, B., Brien, C. J., Wang, D., and Walia,
    H.

    (2015). Integrating image-based phenomics and association analysis to dissect

    the genetic architecture of temporal salinity responses in rice. Plant Physiol.
    168,

    1476–1489. doi: 10.1104/pp.15.00450

    Carrolla, A. A., Clarke, J., Fahlgrenc, N. A., Gehanc, M. J., Lawrence-Dilld,
    C., and

    Lorencee, L. (2019). NAPPN: who we ae, where we are going, and why you

    should join us! Plant Phenome J. 2:180006.

    Chaivivatrakul, S., Tang, L., Dailey, M. N., and Nakarmi, A. D. (2014). Automatic

    morphological trait characterization for corn plants via 3d holographic

    reconstruction. Comput. Electron. Agric. 109, 109–123. doi: 10.1016/j.compag.

    2014.09.005

    Chapman, S. C., Merz, T., Chan, A., Jackway, P., Hrabar, S., Dreccer, M. F.,

    et al. (2014). Pheno-copter: a low-altitude, autonomous remote-sensing robotic

    helicopter for high-throughput ﬁeld-based phenotyping. Agronomy 4, 279–301.

    doi: 10.3390/agronomy4020279

    Chen, D., Neumann, K., Friedel, S., Kilian, B., Chen, M., Altmann, T., et al.
    (2014).

    Dissecting the phenotypic components of crop plant growth and drought

    responses based on high-throughput image analysis. Plant Cell 26, 4636–4655.

    doi: 10.1105/tpc.114.129601

    Chen, T. W., Nguyen, T. M. N., Kahlen, K., and Stützel, H. (2014). Quantiﬁcation
    of

    the eﬀects of architectural traits on dry mass production and light interception

    of tomato canopy under diﬀerent temperature regimes using a dynamic

    functional–structural plant model. J. Exp. Bot. 65, 6399–6410. doi: 10.1093/jxb/

    eru356

    Chen, T.-W., Cabrera-Bosquet, L., Prado, S. A., Perez, R., Artzet, S., Pradal,
    C.,

    et al. (2018). Genetic and environmental dissection of biomass accumulation

    in multi-genotype maize canopies. J. Exp. Bot. 70, 2523–2534. doi: 10.1093/jxb/

    ery309

    Chimungu, J. G., Loades, K. W., and Lynch, J. P. (2015). Root anatomical phenes

    predict root penetration ability and biomechanical properties in maize. J Exp.

    Bot. 66, 3151–3162. doi: 10.1093/jxb/erv121

    Chopin, J., Laga, H., Huang, C. Y., Heuer, S., and Miklavcic, S. J. (2015).

    Rootanalyzer:

    a

    cross-section

    image

    analysis

    tool

    for

    automated

    characterization of root cells and tissues. PLoS One 10:e0137655. doi: 10.1371/

    journal.pone.0137655

    Cobb, J. N., DeClerck, G., Greenberg, A., Clark, R., and McCouch, S. (2013).

    Next-generation phenotyping: requirements and strategies for enhancing our

    understanding of genotype–phenotype relationships and its relevance to crop

    improvement. Theor. Appl. Genet. 126, 867–887. doi: 10.1007/s00122-013-

    2066-0

    Confalonieri, R., Paleari, L., Foi, M., Movedi, E., Vesely, F. M., Thoelke, W.,
    et al.

    (2017). Pocketplant3d: analysing canopy structure using a smartphone. Biosys.

    Eng. 164, 1–12. doi: 10.1016/j.biosystemseng.2017.09.014

    Coppens, F., Wuyts, N., Inzé, D., and Dhondt, S. (2017). Unlocking the potential
    of

    plant phenotyping data through integration and data-driven approaches. Curr.

    Opin. Syst. Biol. 4, 58–63. doi: 10.1016/j.coisb.2017.07.002

    Das, A., Bucksch, A., Price, C. A., and Weitz, J. S. (2014). Clearedleavesdb:
    an online

    database of cleared plant leaf images. Plant Methods 10:8. doi: 10.1186/1746-

    4811-10-8

    Deery, D. M., Rebetzke, G. J., Jimenez-Berni, J. A., James, R. A., Condon, A.
    G.,

    Bovill, W. D., et al. (2016). Methodology for high-throughput ﬁeld phenotyping

    of canopy temperature using airborne thermography. Front. Plant Sci. 7:e1808.

    doi: 10.3389/fpls.2016.01808

    Dhondt, S., Wuyts, N., and Inzé, D. (2013). Cell to whole-plant phenotyping: the

    best is yet to come. Trends Plant Sci. 18, 428–439. doi: 10.1016/j.tplants.2013.

    04.008

    Diaz-Varela, R. A., Zarco-Tejada, P. J., Angileri, V., and Loudjani, P. (2014).

    Automatic identiﬁcation of agricultural terraces through object-oriented

    analysis of very high resolution DSMs and multispectral imagery obtained from

    Frontiers in Plant Science | www.frontiersin.org

    13

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    an unmanned aerial vehicle. J. Environ. Manag. 134, 117–126. doi: 10.1016/j.

    jenvman.2014.01.006

    Du, J., Zhang, Y., Guo, X., Ma, L., Shao, M., Pan, X., et al. (2016). Micron-

    scale

    phenotyping

    quantiﬁcation

    and

    three-dimensional

    microstructure

    reconstruction of vascular bundles within maize stems based on micro-CT

    scanning. Funct. Plant Biol. 44, 10–22.

    Duan, T., Chapman, S. C., Holland, E., Rebetzke, G. J., Guo, Y., and Zheng, B.

    (2016). Dynamic quantiﬁcation of canopy structure to characterize early plant

    vigour in wheat genotypes. J. Exp. Bot. 67, 4523–4534. doi: 10.1093/jxb/erw227

    Fabre, J., Dauzat, M., Nègre, V., Wuyts, N., Tireau, A., Gennari, E., et al. (2011).

    Phenopsis DB: an information system for Arabidopsis thaliana phenotypic data

    in an environmental context. BMC Plant Biol. 11:77. doi: 10.1186/1471-2229-

    11-77

    Fahlgren, N., Feldman, M., Gehan, M. A., Wilson, M. S., Shyu, C., Bryant, D. W.,

    et al. (2015a). A versatile phenotyping system and analytics platform reveals

    diverse temporal responses to water availability in Setaria. Mol. Plant 8, 1520–

    1535. doi: 10.1016/j.molp.2015.06.005

    Fahlgren, N., Gehan, M. A., and Baxter, I. (2015b). Lights, camera, action: high-

    throughput plant phenotyping is ready for a close-up. Curr. Opin. Plant Biol.

    24:93. doi: 10.1016/j.pbi.2015.02.006

    Feng, H., Guo, Z., Yang, W., Huang, C., Chen, G., Fang, W., et al. (2017).

    An integrated hyperspectral imaging and genome-wide association analysis

    platform provides spectral and genetic insights into the natural variation in
    rice.

    Sci. Rep. 7:4401. doi: 10.1038/s41598-017-04668-8

    Fiorani, F., and Schurr, U. (2013). Future scenarios for plant phenotyping. Annu.

    Rev. Plant Biol. 64, 267–291. doi: 10.1146/annurev-arplant-050312-120137

    Fischer, R. A. T., and Edmeades, G. O. (2010). Breeding and cereal yield progress.

    Crop Sci. 50, S85–S98.

    Fritsche-Neto, R., and Borém, A. (2015). Phenomics How Next-Generation

    Phenotyping is Revolutionizing Plant Breeding. Switzerland: Springer Press.

    Furbank, R. T., Jimenez-Berni, J. A., George-Jaeggli, B., Potgieter, A. B., and

    Deery, D. M. (2019). Field crop phenomics: enabling breeding for radiation use

    eﬃciency and biomass in cereal crops. New Phytol. [Epub ahead of print]

    Furbank, R. T., and Tester, M. (2011). Phenomics-technologies to relieve the

    phenotyping bottleneck. Trends Plant Sci. 16, 635–644. doi: 10.1016/j.tplants.

    2011.09.005

    Gaudin, A. C. M., Henry, A., Sparks, A. H., and Slamet-Loedin, I. H. (2013).

    Taking transgenic rice drought screening to the ﬁeld. J. Exp. Bot. 64, 109–117.

    doi: 10.1093/jxb/ers313

    Gennaro, S. F. D., Rizza, F., Badeck, F. W., Berton, A., Delbono, S., Gioli, B.,

    et al. (2017). UAV-based high-throughput phenotyping to discriminate barley

    vigour with visible and near-infrared vegetation indices. Int. J. Remote Sens.
    39,

    5330–5344. doi: 10.1080/01431161.2017.1395974

    Gibbs, J. A., Pound, M., French, A. P., Wells, D. M., Murchie, E., and Pridmore,

    T. (2017). Approaches to three-dimensional reconstruction of plant shoot

    topology and geometry. Funct. Plant Biol. 44, 62–75.

    Gibbs, J. A., Pound, M., French, A. P., Wells, D. M., Murchie, E., and Pridmore,
    T.

    (2018). Plant phenotyping: an active vision cell for three-dimensional plant

    shoot reconstruction. Plant Physiol. 178, 524–534. doi: 10.1104/pp.18.00664

    Girshick, R. (2015). “Fast R-CNN,” in Proceedings of the IEEE International

    Conference on Computer Vision, Piscataway, NJ.

    Goﬀ, S. A., Vaughn, M., McKay, S., Lyons, E., Stapleton, A. E., Gessler, D., et
    al.

    (2011). The iPlant collaborative: cyberinfrastructure for plant biology. Front.

    Plant Sci. 2:34. doi: 10.3389/fpls.2011.00034

    Golzarian, M. R., Frick, R. A., Rajendran, K., Berger, B., Roy, S., Tester, M.,
    et al.

    (2011). Accurate inference of shoot biomass from high-throughput images of

    cereal plants. Plant Methods 7:2. doi: 10.1186/1746-4811-7-2

    Gómez-Candón, D., Virlet, N., Labbé, S., Jolivot, A., and Regnard, J. L. (2016).

    Field phenotyping of water stress at tree scale by UAV-sensed imagery: new

    insights for thermal acquisition and calibration. Precis. Agric. 17, 786–800.

    doi: 10.1007/s11119-016-9449-6

    Gonzalez-Dugo, V., Hernandez, P., Solis, I., and Zarco-Tejada, P. J. (2015).

    Using high-resolution hyperspectral and thermal airborne imagery to assess

    p-physiological condition in the context of wheat phenotyping. Remote Sens.

    7, 13586–13605. doi: 10.3390/rs71013586

    Gonzalez-Dugo, V., Zarco-Tejada, P. J., and Fereres, E. (2014). Applicability
    and

    limitations of using the crop water stress index as an indicator of water deﬁcits

    in citrus orchards. Agric. Forest Meteorol. 198, 94–104. doi: 10.1016/j.agrformet.

    2014.08.003

    Gonzalez-Dugo, V., Zarco-Tejada, P., Nicolás, E., Nortes, P. A., Alarcón, J. J.,

    Intrigliolo, D. S., et al. (2013). Using high resolution UAV thermal imagery

    to assess the variability in the water status of ﬁve fruit tree species within

    a commercial orchard. Precis. Agric. 14, 660–678. doi: 10.1007/s11119-013-

    9322-9

    Granier, C., Aguirrezabal, L., Chenu, K., Cookson, S. J., Dauzat, M., Hamard,

    P., et al. (2006). PHENOPSIS, an automated platform for reproducible

    phenotyping of plant responses to soil water deﬁcit in Arabidopsis thaliana

    permitted the identiﬁcation of an accession with low sensitivity to soil

    water deﬁcit. New Phytol. 169, 623–635.

    doi: 10.1111/j.1469-8137.2005.

    01609.x

    Guo, D., Juan, J., Chang, L., Zhang, J., and Huang, D. (2017). Discrimination
    of

    plant root zone water status in greenhouse production based on phenotyping

    and machine learning techniques. Sci. Rep. 7:8303. doi: 10.1038/s41598-017-

    08235-z

    Hall, H. C., Fakhrzadeh, A., Luengo Hendriks, C. L., and Fischer, U. (2016).

    Precision automation of cell type classiﬁcation and sub-cellular ﬂuorescence

    quantiﬁcation from laser scanning confocal images. Front. Plant Sci. 7:119.

    doi: 10.3389/fpls.2016.00119

    He, J. Q., Harrison, R. J., and Li, B. (2017). A novel 3d imaging system

    for strawberry phenotyping. Plant Methods 13:8. doi: 10.1186/s13007-017-

    0243-x

    He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep residual learning for image

    recognition. computer vision and pattern recognition (cs.CV). arXiv

    Heckwolf, S., Heckwolf, M., Kaeppler, S. M., de Leon, N., and Spalding, E. P.
    (2015).

    Image analysis of anatomical traits in stem transections of maize and other

    grasses. Plant Methods 11:26. doi: 10.1186/s13007-015-0070-x

    Herwitz, S. R., Johnson, L. F., Dunagan, S. E., Higgins, R. G., Sullivan, D. V.,

    Zheng, J., et al. (2004). Imaging from an unmanned aerial vehicle: agricultural

    surveillance and decision support. Comp. Electron. Agric. 44, 49–61. doi: 10.

    1016/j.compag.2004.02.006

    Houle, D., Govindaraju, D. R., and Omholt, S. (2010). Phenomics: the next

    challenge. Nat. Rev. Genet. 11, 855–866. doi: 10.1038/nrg2897

    Hu, Y., Wang, L., Xiang, L., Wu, Q., and Jiang, H. (2018). Automatic non-

    destructive growth measurement of leafy vegetables based on kinect. Sensors

    18:806. doi: 10.3390/s18030806

    Huang, H., Wu, S. H., Cohen-Or, D., Gong, M. L., Zhang, H., Li, G., et al. (2013).

    L1-medial skeleton of point cloud. ACM Trans. Graph. 32:8.

    Hui, F., Zhu, J., Hu, P., Meng, L., Zhu, B., Guo, Y., et al. (2018). Image-based

    dynamic quantiﬁcation and high-accuracy 3d evaluation of canopy structure

    of plant populations. Ann. Bot. 121, 1079–1088. doi: 10.1093/aob/mcy016

    Jannick, J. L., Lorenz, A. J., and Iwata, H. (2010). Genomic selection in plant

    breeding: from theory to practice. Brief. Funct. Genomics 9, 166–177. doi: 10.

    1093/bfgp/elq001

    Jimenez-Berni, J. A., Deery, D. M., Rozas-Larraondo, P., Condon, A. T. G.,

    Rebetzke, G. J., James, R. A., et al. (2018). High throughput determination of

    plant height, ground cover, and above-ground biomass in wheat with lidar.

    Front. Plant Sci. 9:237. doi: 10.3389/fpls.2018.00237

    Johannsen, W. (2014). The genotype conception of heredity. Int. J. Epidemiol.
    43,

    989–1000. doi: 10.1093/ije/dyu063

    Kirchgessner, N., Liebisch, F., Yu, K., Pfeifer, J., Friedli, M., Hund, A., et
    al. (2016).

    The ETH ﬁeld phenotyping platform FIP: a cable-suspended multi-sensor

    system. Funct. Plant Biol. 44, 154–168.

    Klukas, C., Chen, D. J., and Pape, J. M. (2014). Integrated analysis platform:
    an

    open-source information system for high-throughput plant phenotyping. Plant

    Physiol. 165, 506–518. doi: 10.1104/pp.113.233932

    Kolukisaoglu, U., and Thurow, K. (2010). Future and frontiers of automated

    screening in plant sciences. Plant Sci. 178, 476–484. doi: 10.1016/j.plantsci.

    2010.03.006

    Krajewski, P., Chen, D., Æwiek, H., van Dijk, A. D., Fiorani, F., Kersey, P.,
    et al.

    (2015). Towards recommendations for metadata and data handling in plant

    phenotyping. J. Exp. Bot. 66, 5417–5427. doi: 10.1093/jxb/erv271

    Kvilekval Das, K., Fedorov, D., Obara, B., Singh, A., and Manjunath, B. S. (2010).

    Bisque: a platform for bioimage analysis and management. Bioinformatics 26,

    544–552. doi: 10.1093/bioinformatics/btp699

    Lecun, Y. (1990). Handwritten digit recognition with a back-propagation network.

    Adv. Neural Inform. Process. Sys. 2, 396–404.

    Lecun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521:436.

    doi: 10.1038/nature14539

    Frontiers in Plant Science | www.frontiersin.org

    14

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    Legland, D., Devaux, M. F., and Guillon, F. (2014). Statistical mapping of maize

    bundle intensity at the stem scale using spatial normalisation of replicated

    images. PLoS One 9:e90673. doi: 10.1371/journal.pone.0090673

    Legland, D., El-Hage, F., Méchin, V., and Reymond, M. (2017). Histological

    quantiﬁcation of maize stem sections from FASGA-stained images. Plant

    Methods 13:84. doi: 10.1186/s13007-017-0225-z

    Leister, D., Varotto, C., Pesaresi, P., Niwergall, A., and Salamini, F. (1999).
    vLarge-

    scale evaluation of plant growth in Arabidopsis thaliana by non-invasive

    image analysis. Plant Physiol. Biochem. 37, 671–678. doi: 10.3389/fpls.2014.

    00770

    Li, M., Frank, M., Coneva, V., Mio, W., Chitwood, D. H., and Topp, C. N.

    (2018). The persistent homology mathematical framework provides enhanced

    genotype-to-phenotype associations for plant morphology. Plant Physiol. 177,

    1382–1395. doi: 10.1104/pp.18.00104

    Li, Y.-F., Kennedy, G., Davies, F., and Hunter, J. (2010). PODD: an ontology-driven

    data repository for collaborative phenomics research. Lect. Notes Comput. Sci.

    6102, 179–188. doi: 10.1007/978-3-642-13654-2_22

    Li, Z., Chen, Z., Wang, L., Liu, J., and Zhou, Q. (2014). Area extraction of maize

    lodging based on remote sensing by small unmanned aerial vehicle. Trans. Chin.

    Soc. Agric. Eng. 30, 207–213.

    Liang, Z., Pandey, P., Stoerger, V., Xu, Y., Qiu, Y., Ge, Y., et al. (2017). Conventional

    and hyperspectral time-series imaging of maize lines widely used in ﬁeld trials.

    Gigascience 7, 1–11. doi: 10.1093/gigascience/gix117

    Liebisch, F., Kirchgessner, N., Schneider, D., Walter, A., and Hund, A.

    (2015).

    Remote,

    aerial

    phenotyping

    of

    maize

    traits

    with

    a

    mobile

    multi-sensor

    approach.

    Plant

    Methods

    11:9.

    doi:

    10.1186/s13007-015-

    0048-8

    Lobos, G. A., Camargo, A. V., Del Pozo, A., Araus, J. L., Ortiz, R., and Doonan,
    J. H.

    (2017). Editorial: plant phenotyping and phenomics for plant breeding. Front.

    Plant Sci. 8:2181. doi: 10.3389/fpls.2017.02181

    Lorenz, A. J., Chao, S., Asoro, F. G., Heﬀner, E. L., Hayashi, T., Iwata, H.,
    et al.

    (2011). Genomic selection in plant breeding: knowledge and prospects. Adv.

    Agron. 110, 77–123.

    Majewsky, V., Scherr, C., Schneider, C., Sebastian, P. A., and Stephan, B. (2017).

    Reproducibility of the eﬀects of homeopathically potentised argentum nitricum

    on the growth of Lemna gibba L. in a randomised and blinded bioassay.

    Homeopathy. 106, 145–154. doi: 10.1016/j.homp.2017.04.001

    Mathews, A. J., and Jensen, J. L. R. (2013). Visualizing and quantifying vineyard

    canopy LAI using an unmanned aerial vehicle (UAV) collected high density

    structure from motion point cloud. Remote Sens. 5, 2164–2183. doi: 10.3390/

    rs5052164

    Meng, R., Saade, S., Kurtek, S., Berger, B., Brien, C., Pillen, K., et al. (2017).
    Growth

    curve registration for evaluating salinity tolerance in barley. Plant Methods

    13:18. doi: 10.1186/s13007-017-0165-7

    Mohanty, S. P., Hughes, D. P., and Salathe, M. (2016). Using deep learning for

    image-based plant disease detection. Front. Plant Sci. 7:1419. doi: 10.3389/fpls.

    2016.01419

    Montes, J. M., Technow, F., Dhillon, B., Mauch, F., and Melchinger, A. (2011).

    High-throughput non-destructive biomass determination during early plant

    development in maize under ﬁeld conditions. Field Crops Res. 121, 268–273.

    doi: 10.1016/j.fcr.2010.12.017

    Muraya, M. M., Chu, J., Zhao, Y., Junker, A., Klukas, C., Reif, J. C., et al.
    (2017).

    Genetic variation of growth dynamics in maize (Zea mays L.). revealed through

    automated non-invasive phenotyping. Plant J. 89, 366–380. doi: 10.1111/tpj.

    13390

    Nakhforoosh, A., Bodewein, T., Fiorani, F., and Bodner, G. (2016). Identiﬁcation

    of water use strategies at early growth stages in durum wheat from shoot

    phenotyping and physiological measurements. Front. Plant Sci. 7:1155. doi:

    10.3389/fpls.2016.01155

    Neilson, E. H., Edwards, A. M., Blomstedt, C. K., Berger, B., Moller, B. L., and

    Gleadow, R. M. (2015). Utilization of a high-throughput shoot imaging system

    to examine the dynamic phenotypic responses of a C-4 cereal crop plant to

    nitrogen and water deﬁciency over time. J. Exp. Bot. 66, 1817–1832.

    doi:

    10.1093/jxb/eru526

    Neumann, K., Zhao, Y., Chu, J., Keilwagen, J., Reif, J. C., Kilian, B., et al.
    (2017).

    Genetic architecture and temporal patterns of biomass accumulation in spring

    barley revealed by image analysis. BMC Plant Biol. 17:137. doi: 10.1186/s12870-

    017-1085-4

    Nigon, T. J., Mulla, D. J., Rosen, C. J., Cohen, Y., Alchanatis, V., Knight, J.,

    et al. (2015). Hyperspectral aerial imagery for detecting nitrogen stress in two

    potatocultivars. Comput. Electron. Agric. 112, 36–46. doi: 10.1016/j.compag.

    2014.12.018

    Overgaard, S. I., Isaksson, T., Kvaal, K., and Korsaeth, A. (2010). Comparisons

    of two hand-held, multispectral ﬁeld radiometers and a hyperspectral airborne

    imager in terms of predicting spring wheat grain yield and quality by means of

    powered partial least squares regression. J. Near Infrared Spectrosc. 18, 247–261.

    doi: 10.1255/jnirs.892

    Pan, X., Ma, L., Zhang, Y., Wang, J., Du, J., and Guo, X. (2018). Reconstruction

    of maize roots and quantitative analysis of metaxylem vessels based on X-ray

    micro-computed tomography. Can. J. Plant Sci. 98, 457–466.

    Pandey, P., Ge, Y., Stoerger, V., and Schnable, J. C. (2017). High throughput
    in vivo

    analysis of plant leaf chemical properties using hyperspectral imaging. Front.

    Plant Sci. 8:1348. doi: 10.3389/fpls.2017.01348

    Pape, J. M., and Klukas, C. (2015). 3-D histogram-based segmentation and

    leaf detection for rosette plants. Asia Pac. Conf. Concept. Model. 43,

    107–114.

    Parlati, A., Valkov, V. T., D’Apuzzo, E., Alves, L. M., Petrozza, A., Summerer,

    S., et al. (2017). Ectopic expression of PII induces stomatal closure in Lotus

    japonicus. Front. Plant Sci. 8:1299. doi: 10.3389/fpls.2017.01299

    Pawara, P., Okafor, E., Surinta, O., Schomaker, L., and Wiering, M. (2017).

    “Comparing local descriptors and bags of visual words to deep convolutional

    neural networks for plant recognition,” in Proceedings of the International

    Conference on Pattern Recognition Applications and Methods, Lisbon.

    Pereyra-Irujo, G. A., Gasco, E. D., Peirone, L. S., and Aguirrezábal, L. A. N.
    (2012).

    GlyPh: a low-cost platform for phenotyping plant growth and water use. Funct.

    Plant Biol. 39, 905–913.

    Postma, J. A., Kuppe, C., Owen, M. R., Mellor, N., Griﬃths, M., Bennett, M. J.,
    et al.

    (2017). OpenSimRoot: widening the scope and application of root architectural

    models. New Phytol. 215, 1274–1286. doi: 10.1111/nph.14641

    Potgieter, A. B., Watson, J., Eldridge, M., Laws, K., George-Jaeggli, B., Hunt,

    C., et al. (2018). “Determining crop growth dynamics in sorghum breeding

    trials through remote and proximal sensing technologies,” in Proceedings of

    the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing

    Symposium, Valencia.

    Ray, D. K., Mueller, N. D., West, P. C., and Foley, J. A. (2013). Yield trends
    are

    insuﬃcient to double global crop production by 2050. PLoS One 8:e66428.

    doi: 10.1371/journal.pone.0066428

    Reuzeau, C. (2007). TraitMill (TM): a high throughput functional genomics

    platform for the phenotypic analysis of cereals. Vitro Cell. Dev. Biol. Anim.

    43:S4.

    Reuzeau, C., Pen, J., Frankard, V., de Wolf, J., Peerbolte, R., Broekaert, W.,
    et al.

    (2005). TraitMill: a discovery engine for identifying yield-enhancement genes

    in cereals. Mol. Plant Breed. 1, 1–6.

    Reuzeau, C., Pen, J., Frankard, V., de Wolf, J., Peerbolte, R., Broekaert, W.,
    et al.

    (2006). TraitMill: a discovery engine for identifying yield-enhancement genes

    in cereals. Plant Genet. Res. 4, 20–24.

    Reuzeau, C., Pen, J., Frankard, V., Wolf, J., Peerbolte, R., Broekaert, W., et
    al.

    (2010). TraitMill: a discovery engine for identifying yield-enhancement genes

    in cereals. Plant Gene Trait. 1, 1–6. doi: 10.5376/pgt.2010.01.0001

    Rist, F., Herzog, K., Mack, J., Richter, R., Steinhage, V., and Töpfer, R. (2018).
    High-

    precision phenotyping of grape bunch architecture using fast 3d sensor and

    automation. Sensors 18:763. doi: 10.3390/s18030763

    Roitsch, T., Cabrera-Bosquet, L., Fournier, A., Ghamkhar, K., Jiménez-Berni, J.,

    Pinto, F., et al. (2019). Review: new sensors and data-driven approaches-A path

    to next generation phenomics. Plant Sci. 282, 2–10. doi: 10.1016/j.plantsci.2019.

    01.011

    Rose, J. C., Paulus, S., and Kuhlmann, H. (2015). Accuracy analysis of a multi-view

    stereo approach for phenotyping of tomato plants at the organ level. Sensors 15,

    9651–9665. doi: 10.3390/s150509651

    Rutkoski, J., Poland, J., Mondal, S., Autrique, E., Pérez, L. G., Crossa, J.,
    et al.

    (2016). Canopy temperature and vegetation indices from high-throughput

    phenotyping improve accuracy of pedigree and genomic selection for grain

    yield in wheat.Genes Genom. Genet. 6, 2799–2808. doi: 10.1534/g3.116.032888

    Sadeghi-Tehran, P., Sabermanesh, K., Virlet, N., and Hawkesford, M. J. (2017).

    Automated method to determine two critical growth stages of wheat: heading

    and ﬂowering. Front. Plant Sci. 8:252. doi: 10.3389/fpls.2017.00252

    Frontiers in Plant Science | www.frontiersin.org

    15

    June 2019 | Volume 10 | Article 714

    Zhao et al.

    Crop Phenomics Review

    Salvi, S., and Tuberosa, R. (2015). The crop QTLome comes of age. Curr. Opin.

    Biotechnol. 32, 179–185. doi: 10.1016/j.copbio.2015.01.001

    Schork, N. J. (1997). Genetics of complex disease: approaches, problem, and

    solutions. Am. J. Respir. Crit. Care Med. 156, S103–S109.

    Seren, Ü, Grimm, D., Fitz, I., Weigel, D., Nordborg, M., Borgwardt, K., et al.
    (2017).

    AraPheno: a public database for Arabidopsis thaliana phenotypes. Nucleic Acids

    Res. 4, 1054–1059. doi: 10.1093/nar/gkw986

    Silsbe, G. M., Oxborough, K., Suggett, D. J., Forster, R. M., Ihnken, S., Komárek,

    O., et al. (2015). Toward autonomous measurements of photosynthetic electron

    transport rates: an evaluation of active ﬂuorescence-based measurements of

    photochemistry. Limnol. Oceanogr. Methods 13, 138–155.

    Singh, A., Ganapathysubramanian, B., Singh, A. K., and Sarkar, S. (2016). Machine

    learning for high-throughput stress phenotyping in plants. Trends Plant Sci.

    21:110. doi: 10.1016/j.tplants.2015.10.015

    Sugiura, R., Noguchi, N., and Ishii, K. (2005). Remote-sensing technology for

    vegetation monitoring using an unmanned helicopter. Biosys. Eng. 90, 369–379.

    doi: 10.1016/j.biosystemseng.2004.12.011

    Swain, K. C., Thomson, S. J., and Jayasuriya, H. P. W. (2010). Adoption of an

    unmanned helicopter for low-altitude remote sensing to estimate yield and total

    biomass of a rice crop. Trans. Asabe. 53, 21–27. doi: 10.13031/2013.29493

    Tardieu, F., Cabrera-Bosquet, L., Pridmore, T., and Bennett, M. (2017). Plant

    phenomics, from sensors to knowledge. Curr. Biol. 27, R770–R783. doi: 10.

    1016/j.cub.2017.05.055

    Tardieu, F., and Tuberosa, R. (2010). Dissection and modelling of abiotic stress

    tolerance in plants. Curr. Opin. Plant Biol. 13, 206–212. doi: 10.1016/j.pbi.2009.

    12.012

    Tattaris, M., Reynolds, M. P., and Chapman, S. C. (2016). A direct comparison
    of

    remote sensing approaches for high-throughput phenotyping in plant breeding.

    Front. Plant Sci. 7:1131. doi: 10.3389/fpls.2016.01131

    Thapa, S., Zhu, F., Walia, H., Yu, H., and Ge, Y. (2018). A novel lidar-based

    instrument for high-throughput, 3d measurement of morphological traits in

    maize and sorghum. Sensors 18:1187. doi: 10.3390/s18041187

    Tomé, F., Jansseune, K., Saey, B., Grundy, J., Vandenbroucke, K., Hannah, M. A.,

    et al. (2017). rosettR: protocol and software for seedling area and growth

    analysis. Plant Methods. 13:13. doi: 10.1186/s13007-017-0163-9

    Tsaftaris, S., and Noutsos, C. (2009). Plant Phenotyping with Low Cost Digital

    Cameras and Image Analytics. Berlin: Springer Press.

    Tsaftaris, S. A., Minervini, M., and Scharr, H. (2016). Machine learning for plant

    phenotyping needs image processing. Trends Plant Sci. 21, 989–991. doi: 10.

    1016/j.tplants.2016.10.002

    Tuberosa, R. (2012). Phenotyping for drought tolerance of crops in the genomics

    era. Front. Physiol. 3:347. doi: 10.3389/fphys.2012.00347

    Ubbens, J. R., and Stavness, I. (2017). Deep plant phenomics: a deep learning

    platform for complex plant phenotyping tasks. Front. Plant Sci. 8:1190. doi:

    10.3389/fpls.2017.01190

    Vadez, V., Kholová, J., Hummel, G., Zhokhavets, U., Gupta, S. K., and Hash, C.
    T.

    (2015). LeasyScan: a novel concept combining 3D imaging and lysimetry for

    high-throughput phenotyping of traits controlling plant water budget. J. Exp.

    Bot. 66, 5581–5593. doi: 10.1093/jxb/erv251

    van de Velde, K., Chandler, P. M., van der Straeten, D., and Rohde, A. (2017).

    Diﬀerential coupling of gibberellin responses by Rht-B1c suppressor alleles

    and Rht-B1b in wheat highlights a unique role for the DELLA N-terminus in

    dormancy. J. Exp. Bot. 68, 443–455. doi: 10.1093/jxb/erw471

    Vazquez-Arellano, M., Griepentrog, H. W., Reiser, D., and Paraforos, D. S. (2016).

    3-d imaging systems for agricultural applications-a review. Sensors 16:24.

    Virlet, N., Sabermanesh, K., Sadeghi-Tehran, P., and Hawkesford, M. J. (2016).

    Field scanalyzer: an automated robotic ﬁeld phenotyping platform for detailed

    crop monitoring. Funct. Plant Biol. 44, 143–153.

    Vos, J., Evers, J. B., Buck-Sorlin, G. H., Andrieu, B., Chelle, M., and de Visser,

    P. H. (2010). Functional-structural plant modelling: a new versatile tool in crop

    science. J. Exp. Bot. 61, 2101–2115. doi: 10.1093/jxb/erp345

    Vylder, J. D., Vandenbussche, F., Hu, Y., Philips, W., and Straeten, D. V. D.
    (2012).

    Rosette tracker: an open source image analysis tool for automatic quantiﬁcation

    of genotype eﬀects. Plant Physiol. 160:1149. doi: 10.1104/pp.112.202762

    Wallace, L., Lucieer, A., Watson, C., and Turner, D. (2012). Development of a
    UAV-

    LiDAR system with application to forest inventory. Remote Sens. 4, 1519–1543.

    doi: 10.3390/rs4061519

    Walter, A., Liebisch, F., and Hund, A. (2015). Plant phenotyping: from bean

    weighing to image analysis. Plant Methods 11:14. doi: 10.1186/s13007-015-

    0056-8

    Wen, W., Guo, X., Wang, Y., Zhao, C., and Liao, W. (2017). Constructing a three-

    dimensional resource database of plants using measured in situ morphological

    data. Appl. Eng. Agric. 33, 747–756. doi: 10.13031/aea.12135

    Wen, W., Li, B., Li, B.-J., and Guo, X. (2018). A leaf modeling and multi-scale

    remeshing method for visual computation via hierarchical parametric vein and

    margin representation. Front. Plant Sci. 9:783. doi: 10.3389/fpls.2018.00783

    White, J. W., Andrade-Sanchez, P., Gore, M. A., Bronson, K. F., Coﬀelt, T. A.,

    Conley, M. M., et al. (2012). Field-based phenomics for plant genetics research.

    Field Crops Res. 133, 101–112. doi: 10.1016/j.fcr.2012.04.003

    Wilkinson, M. D., Dumonter, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak,

    A., et al. (2016). The FAIR guiding principles for scientiﬁc data management

    and stewardship. Sci. Data 3:160018. doi: 10.1038/sdata.2016.18

    Wu, C. (2011). Visualsfm: A Visual Structure from Motion System. Available at:

    http://ccwu.me/vsfm/ (accessed June 11, 2014).

    Wu, H., Jaeger, M., Wang, M., Li, B., and Zhang, B. G. (2011). Three-dimensional

    distribution of vessels, passage cells and lateral roots along the root axis of

    winter wheat (Triticum aestivum). Ann. Bot. 107, 843–853. doi: 10.1093/aob/

    mcr005

    Xu, Y. (2016). Envirotyping for deciphering environmental impacts on crop plants.

    Theor. Appl. Genet. 129, 653–673. doi: 10.1007/s00122-016-2691-5

    Yang, G., Liu, J., Zhao, C., Li, Z., Huang, Y., Yu, H., et al. (2017). Unmanned
    aerial

    vehicle remote sensing for ﬁeld-based crop phenotyping: current status and

    perspectives. Front. Plant Sci. 8:1111. doi: 10.3389/fpls.2017.01111

    Yang, W., Guo, Z., Huang, C., Duan, L., Chen, G., Jiang, N., et al. (2014).

    Combining high-throughput phenotyping and genome-wide association

    studies to reveal natural genetic variation in rice. Nat. Commun. 5:5087. doi:

    10.1038/ncomms6087

    Yang, W., Guo, Z., Huang, C., Wang, K., Jiang, N., Feng, H., et al. (2015). Genome-

    wide association study of rice (Oryza sativa L.) leaf traits with a high-throughput

    leaf scorer. J. Exp. Bot. 66, 5605–5615. doi: 10.1093/jxb/erv100

    Yin, K. X., Huang, H., Long, P. X., Gaissinski, A., Gong, M. L., Sharf, A., et
    al.

    (2016). Full 3d plant reconstruction via intrusive acquisition. Comput. Graph.

    Forum 35, 272–284. doi: 10.1111/cgf.12724

    Zarco-Tejada, P. J., Diaz-Varela, R., Angileri, V., and Loudjani, P. (2014). Tree

    height quantiﬁcation using very high resolution imagery acquired from an

    unmanned aerial vehicle (UAV) and automatic 3D photo-reconstruction

    methods. Eur. J. Agron. 55, 89–99. doi: 10.1016/j.eja.2014.01.004

    Zhang, C. H., and Kovacs, J. M. (2012). The application of small unmanned

    aerial systems for precision agriculture: a review. Precis. Agric. 13, 693–712.

    doi: 10.1007/s11119-012-9274-5

    Zhang, X., Huang, C., Wu, D., Qiao, F., Li, W., Duan, L., et al. (2017). High-

    throughput phenotyping and QTL mapping reveals the genetic architecture of

    maize plant growth. Plant Physiol. 173, 1554–1564. doi: 10.1104/pp.16.01516

    Zhang, Y., Legay, S., Barrière, Y., Méchin, V., and Legland, D. (2013). Color

    quantiﬁcation of stained maize stem section describes lignin spatial distribution

    within the whole stem. J. Sci. Food Agric. 61, 3186–3192. doi: 10.1021/jf400912s

    Zhang, Y., Ma, L., Pan, X., Wang, J., Guo, X., and Du, J. (2018). Micron-

    scale phenotyping techniques of maize vascular bundles based on X-ray

    microcomputed tomography. J. Vis. Exp. 140:e58501. doi: 10.3791/58501

    Zhu, J. Q., van der Werf, W., Anten, N. P. R., Vos, J., and Evers, J. B. (2015).
    The

    contribution of phenotypic plasticity to complementary light capture in plant

    mixtures. New Phytol. 207, 1213–1222. doi: 10.1111/nph.13416

    Conﬂict of Interest Statement: The authors declare that the research was

    conducted in the absence of any commercial or ﬁnancial relationships that could

    be construed as a potential conﬂict of interest.

    Copyright © 2019 Zhao, Zhang, Du, Guo, Wen, Gu, Wang and Fan. This is an

    open-access article distributed under the terms of the Creative Commons Attribution

    License (CC BY). The use, distribution or reproduction in other forums is permitted,

    provided the original author(s) and the copyright owner(s) are credited and that
    the

    original publication in this journal is cited, in accordance with accepted academic

    practice. No use, distribution or reproduction is permitted which does not comply

    with these terms.

    Frontiers in Plant Science | www.frontiersin.org

    16

    June 2019 | Volume 10 | Article 714

    '
  inline_citation: '>'
  journal: Frontiers in plant science
  limitations: '>'
  pdf_link: https://www.frontiersin.org/articles/10.3389/fpls.2019.00714/pdf
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Crop Phenomics: Current Status and Perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/rs13132486
  analysis: '>'
  authors:
  - Maryam Ouhami
  - Adel Hafiane
  - Youssef Es-Saady
  - Mohamed El Hajji
  - Raphaël Canals
  citation_count: 78
  full_citation: '>'
  full_text: ">\nremote sensing  \nReview\nComputer Vision, IoT and Data Fusion for\
    \ Crop Disease\nDetection Using Machine Learning: A Survey and\nOngoing Research\n\
    Maryam Ouhami 1,2,*\n, Adel Haﬁane 2\n, Youssef Es-Saady 1\n, Mohamed El Hajji\
    \ 1\nand Raphael Canals 2\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\
    \x05\x06\a\nCitation: Ouhami, M.; Haﬁane, A.;\nEs-Saady, Y.; El Hajji, M.; Canals,\
    \ R.\nComputer Vision, IoT and Data\nFusion for Crop Disease Detection\nUsing\
    \ Machine Learning: A Survey\nand Ongoing Research. Remote Sens.\n2021, 13, 2486.\
    \ https://doi.org/\n10.3390/rs13132486\nAcademic Editor: Clement Atzberger\nReceived:\
    \ 12 April 2021\nAccepted: 22 June 2021\nPublished: 25 June 2021\nPublisher’s\
    \ Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished\
    \ maps and institutional afﬁl-\niations.\nCopyright: © 2021 by the authors.\n\
    Licensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed\n\
    under\nthe\nterms\nand\nconditions of the Creative Commons\nAttribution (CC BY)\
    \ license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n1\nIRF-SIC Laboratory,\
    \ Ibn Zohr University, BP 8106—Cite Dakhla, 80000 Agadir, Morocco;\ny.essaady@uiz.ac.ma\
    \ (Y.E.-S.); m.elhajji@uiz.ac.ma (M.E.H.)\n2\nINSA CVL, University of Orléans,\
    \ PRISME Laboratory, EA 4229, F18022 Bourges, France;\nadel.haﬁane@insa-cvl.fr\
    \ (A.H.); raphael.canals@univ-orleans.fr (R.C.)\n*\nCorrespondence: maryam.ouhami1@insa-cvl.fr\n\
    Abstract: Crop diseases constitute a serious issue in agriculture, affecting both\
    \ quality and quan-\ntity of agriculture production. Disease control has been\
    \ a research object in many scientiﬁc and\ntechnologic domains. Technological\
    \ advances in sensors, data storage, computing resources and\nartiﬁcial intelligence\
    \ have shown enormous potential to control diseases effectively. A growing body\n\
    of literature recognizes the importance of using data from different types of\
    \ sensors and machine\nlearning approaches to build models for detection, prediction,\
    \ analysis, assessment, etc. However,\nthe increasing number and diversity of\
    \ research studies requires a literature review for further\ndevelopments and\
    \ contributions in this area. This paper reviews state-of-the-art machine learning\n\
    methods that use different data sources, applied to plant disease detection. It\
    \ lists traditional and\ndeep learning methods associated with the main data acquisition\
    \ modalities, namely IoT, ground\nimaging, unmanned aerial vehicle imaging and\
    \ satellite imaging. In addition, this study examines\nthe role of data fusion\
    \ for ongoing research in the context of disease detection. It highlights the\n\
    advantage of intelligent data fusion techniques, from heterogeneous data sources,\
    \ to improve plant\nhealth status prediction and presents the main challenges\
    \ facing this ﬁeld. The study concludes with\na discussion of several current\
    \ issues and research trends.\nKeywords: plant disease; machine learning; remote\
    \ sensing; intelligent sensors; data fusion\n1. Introduction\nAccording to the\
    \ FAO [1], pest attacks and plant diseases are considered as two\nof the main\
    \ causes of decreasing food availability and food hygiene. Plant diseases\nvary\
    \ seasonally depending on the presence of pathogen, environmental conditions and\n\
    the crop type. Crops can experience environmental stress due to the following\
    \ factors:\nabiotic (drought, water logging, salinity, etc.), biotic (insects,\
    \ pests, weeds, viruses,\netc.) or climate change [2]. On the other hand, pathogen\
    \ is the organism that causes\ndisease whether it is virus, a bacterium or a fungus.\
    \ Determined by the disease and the\ndevelopment stage, the damages on the crops\
    \ range from simple physiological defects\nto plant death. In addition to biological\
    \ agents, other physical agents such as abrupt\nclimate change [3] can cause diseases\
    \ and harm the plant. Reliance on pesticides is the\ncommon practice to limit\
    \ damages caused by these microorganisms [4]. In addition\nto their negative impacts\
    \ on nature, the unreasonable use of pesticides can lead to the\ndeath of auxiliary\
    \ insects used in biological control and/or the development of genetic\nresistance\
    \ [5]. Localization of infected areas in plantations can reduce chemical use.\n\
    Conventional methods for detecting and locating plant diseases include direct\
    \ visual\ndiagnosis by visual identification of disease symptoms appearing on\
    \ plant leaves or by\nchemical techniques that involve molecular tests on plant\
    \ leaves [6]. These methods are\ntime-consuming and require a large number of\
    \ people.\nRemote Sens. 2021, 13, 2486. https://doi.org/10.3390/rs13132486\nhttps://www.mdpi.com/journal/remotesensing\n\
    Remote Sens. 2021, 13, 2486\n2 of 24\nPromising approaches for detecting and locating\
    \ diseases were proposed in recent\nyears using automatic monitoring and recognition\
    \ systems. Advances in sensor technolo-\ngies and data processing have opened\
    \ new perspectives for the detection and diagnosis of\ncrop anomalies. Disease\
    \ surveillance can be performed by capturing data from the soil and\nplant cover\
    \ or using sensors, such as remote sensing (RS) or ground equipment, as well as\n\
    with developing and testing machine learning algorithms [7]. Implementing management\n\
    practices with smart algorithms optimizes proﬁtability, sustainability and protection\
    \ of\nland resources. Overall, it allows effective treatment to be delivered in\
    \ the right place, at\nthe right time, and with the right rate [8,9].\nFurthermore,\
    \ the agriculture ﬁeld can be supplied with multiple sensors measuring\nenvironmental\
    \ characteristics, plant canopy, leaves indices extracted from remote sensing\n\
    imagery and IoT sensors. Given a variety of data extracted, data fusion techniques\
    \ are\nrequired to assemble those types of data to better understand crop growing\
    \ conditions\nand disease symptoms development. In addition, machine learning-based\
    \ data fusion\nhas undergone important development, and when used on agriculture\
    \ data would have a\ngreat impact on plant protection ﬁeld, in particular, disease\
    \ and early disease detection.\nTherefore, several multi-sensors and remote sensing\
    \ based fusion techniques have been\nused in agriculture for this purpose [10,11].\n\
    The use of agriculture related data from different acquisition tools along with\
    \ machine\nlearning and fusion algorithms has led to extensive research in the\
    \ ﬁeld of digital agricul-\nture, especially for plant monitoring, control and\
    \ protection, generating a considerable\nscientiﬁc literature. Over the last decade,\
    \ several survey papers have been proposed on the\nuse of machine learning approaches\
    \ for agriculture mainly based on one of the different\nextracted data, such as\
    \ IoT data, ground imagery or remote sensing imagery. Table 1\nsummarizes the\
    \ review papers that have addressed different crop monitoring problems,\ncovering\
    \ acquisition techniques using wireless sensors, multispectral cameras, thermal\n\
    cameras, hyperspectral cameras and satellite sensors. Nonetheless, as far as we\
    \ know, there\nis no speciﬁc survey study covering IoT techniques, ground imagery,\
    \ unmanned aerial\nvehicle (UAV) imagery and satellite imagery for disease detection\
    \ using machine learning.\nThe literature still lacks comprehensive insight on\
    \ this ﬁeld of study regarding the\ndifferent data sources in the agriculture\
    \ ﬁeld and the fusion of these data for disease\ndetection. In this paper, we\
    \ review crop disease detection methods based on unimodal data\nsources (wireless\
    \ sensor networks, ground imagery, UAV imagery and satellite imagery)\nusing machine\
    \ learning algorithms. For each data source, these algorithms are broadly\ndivided\
    \ into two main categories: traditional machine learning and deep learning. We\n\
    highlight the combination of multi-source data for agricultural applications and\
    \ discuss\ndata fusion approaches. The recent data fusion advances are presented.\
    \ Current existing\nissues are also analyzed, particularly for disease detection.\n\
    Table 1. Recent review paper in the agricultural domain using machine learning.\n\
    Topics Covered\nYear\nReview\nIoT applications in agro-industrial and environmental\
    \ ﬁeld.\n2017\n[12]\nPrecision farming techniques in semi-arid West Africa for\
    \ labor productivity.\n2017\n[13]\nIOT\nIoT technologies in several smart farming\
    \ scenarios recognition, transport,\ncommunication and treatment.\n2018\n[14]\n\
    Crucial technologies of the internet of things in protected agriculture for plant\n\
    management, animal farming and food/agricultural product supply traceability.\n\
    2019\n[15]\nThe role of wireless sensor networks for greenhouses and the models\
    \ and techniques\nadopted for efﬁcient integration and management of WSN.\n2019\n\
    [16]\nCrop yield prediction using machine learning.\n2020\n[17]\nRemote Sens.\
    \ 2021, 13, 2486\n3 of 24\nTable 1. Cont.\nTopics Covered\nYear\nReview\nImaging\n\
    Hyperspectral image analysis techniques for the detection and classiﬁcation of\
    \ the\nearly onset of plant disease and stress.\n2017\n[18]\nData collection and\
    \ handling of plants close range hyperspectral imaging and\npresentation of recent\
    \ applications of plant assessment using those images.\n2017\n[19]\nUAV-Based\
    \ Sensors, data processing and applications for agriculture and forestry.\n2017\n\
    [20]\nApplied sensing systems and data analytics in agriculture.\n2018\n[21]\n\
    Hyperspectral Image Analysis for unmixing and classiﬁcation tasks.\n2018\n[22]\n\
    Deep learning in agriculture.\n2018\n[23]\nPlant disease detection applications\
    \ using neural networks and hyperspectral images.\n2018\n[8]\nUnmanned aerial\
    \ sensing solutions in precision agriculture.\n2019\n[24]\nImages and machine\
    \ learning techniques for nutrition deﬁciencies detection.\n2019\n[25]\nUnmanned\
    \ Aerial Vehicles and Imaging Sensors applications for monitoring and\nassessing\
    \ plant stresses.\n2019\n[26]\nMonitoring plant diseases and pests through remote\
    \ sensing technology.\n2019\n[9]\nApplications of remote sensing in precision\
    \ agriculture.\n2020\n[27]\nHigh-resolution satellite imagery applications in\
    \ crop phenotyping.\n2020\n[28]\nRemote sensing satellite missions for land monitoring.\n\
    2020\n[29]\nRemote sensing in agriculture.\n2020\n[30]\nPlant diseases identiﬁcation\
    \ and early disease detection using machine learning\ntechniques applied to crop\
    \ images.\n2020\n[31]\nHyperspectral imaging and 3D technologies for plant phenotyping\
    \ from satellite to\nclose-range sensing.\n2020\n[32]\nDeep learning for plant\
    \ diseases.\n2020\n[33]\nApplications of UAV thermal imagery in precision agriculture.\n\
    2020\n[34]\nRemote sensing and precision agriculture technologies for crop disease\
    \ detection and\nmanagement.\n2020\n[35]\nFusion\nMultisensory fusion applications\
    \ and consensus ﬁltering for sensor networks.\n2015\n[10]\nRemote sensing feature-level\
    \ fusion.\n2018\n[11]\nMultisource and multitemporal data fusion in remote Sensing.\n\
    2018\n[36]\nInternet of things applications using data fusion methods.\n2019\n\
    [37]\nMultilevel data fusion for the internet of things in smart agriculture.\n\
    2020\n[38]\nUtilization of multi-sensors and data fusion in precision agriculture.\n\
    2020\n[39]\nThe rest of the paper is organized as follows. Section 2 provides\
    \ an overview of crop\ndisease detection techniques for precision agriculture,\
    \ the use of monitoring systems in\nagriculture and the current state of the art\
    \ for crop disease detection techniques, from\nground to aerial imagery and satellites\
    \ imaging techniques. Section 3 analyzes fusion\napproach opportunities for agriculture.\
    \ The discussion and conclusion is presented in\nSection 4.\n2. Crop Disease Detection\n\
    Natural plant growth depends on multiple interactions with many environmental\n\
    characteristics: soil properties, cultivation techniques, weed growth and microclimatic\n\
    conditions. Unsuitable conditions such as sudden changes in temperature and humidity\n\
    can eventually cause plant diseases. Since early treatments are the most effective\
    \ in\nprotecting agricultural production, knowledge of the elements of infection\
    \ is essential\nto ultimately develop targeted control methods against pathogens.\
    \ Therefore, precision\nagriculture (PA) attempts to consider all these characteristics\
    \ to equip decision systems and\nagricultural machines with information provided\
    \ by sensors, since many recent sensors\nenable monitoring and mapping of various\
    \ ﬁeld parameters [40]. In the literature, several\nacquisition protocols have\
    \ been proposed to acquire crop spectral images. One of the\nmost used in this\
    \ domain is the protocol for imaging leaves of isolated plants. The choice\nof\
    \ the acquisition tools depends on the purpose of the study, it can vary from\
    \ a simple\nsmartphone camera to a highly sophisticated hyperspectral camera mounted\
    \ on an aerial\nRemote Sens. 2021, 13, 2486\n4 of 24\nvehicle. The images’ quality\
    \ and processing tools can vary dependently on the acquisition\nsystem and acquisition\
    \ conditions used.\nIn the next subsections we present different types of sensors\
    \ deployed for dis-\nease detection using machine learning, i.e., ground cameras,\
    \ UAV for aerial imaging,\nsatellites and pedoclimatic sensors. Note that for\
    \ the rest of the paper, IoT refers to\npedoclimatic sensors.\n2.1. Ground Imaging\n\
    Crop ground imaging is the technique of acquiring crops’ fruit and leave images\
    \ at\nground level using smartphones or digital cameras. Since visual symptoms\
    \ on the crops and\nplant leaves are important for disease detection, researchers\
    \ tried to capture plant leaves in\nﬁeld conditions [41,42], raising the challenge\
    \ of dealing with complex background, shadows\nand unstable luminosity. On the\
    \ other hand, other studies have examined the sensitivity\nof spectrometry to\
    \ chemical and organic characteristics of plants. When plants suffer\nfrom stress\
    \ the normal chlorophyll production decreases, thus absorption decreases which\n\
    causes a signiﬁcant growth of reﬂectance [43,44]. As a result, the spectral characteristics\
    \ of\nplants are affected by diseases, leading researchers to invest in the detection\
    \ of infected and\nuninfected leaves and the classiﬁcation of different disease\
    \ severity degrees with visual\nsymptoms and even before visual symptoms appearance\
    \ [45]. A modern approach for\ndisease detection relies on machine learning algorithms\
    \ to explore data from different\nacquisition systems:\nTraditional machine learning\
    \ algorithms were used for the purpose of disease detec-\ntion. Support vector\
    \ machine (SVM) models are commonly used for plant disease detection\ndue to their\
    \ prediction efﬁciency. In [45], SVM was used for early detection of drought\n\
    stress in barley with close range hyperspectral imaging. The model was trained\
    \ on the\nextracted information from labels and selected vegetation indices (Red\
    \ Edge Normalized\nDifference Vegetation Index (RENDVI) and Plant Senescence Reﬂectance\
    \ Index (PSRI)).\nSimilarly, manual extraction of lesion characteristics and combination\
    \ of multiple SVM\nclassiﬁers (color, texture and shape characteristics) for diseases\
    \ recognition on plant leaves\nhave been proposed in order to reduce misclassiﬁcation\
    \ [46–49]. Statistical analysis of some\nindices using the principal component\
    \ analysis (PCA) model successfully differentiated\nbetween healthy plants and\
    \ infected golden potato disease progression [50]. The authors\nﬁrst extracted\
    \ the vegetation indices, simple ratio (SR) and Normalized Difference Vege-\n\
    tation Index (NDVI) from acquired hyperspectral images of infected potatoes at\
    \ different\ndisease development stages. The analysis results demonstrated the\
    \ ability of spectral data\nto distinguish healthy from diseased plants. In the\
    \ same way, the authors in [51] used\nK-nearest neighbor (KNN) and the decision\
    \ tree-based classiﬁer C5.0 to classify grey mold\ninfection severities on tomato\
    \ leaves using hyperspectral images. Results indicate that the\nfull range model\
    \ can differentiate between healthy and infected leaves with an accuracy of\n\
    92.86% and 85.71% for KNN and C5.0, respectively. In [52], the authors aimed to\
    \ estimate\nthe severity of three diseases on wheat using an artiﬁcial neural\
    \ network composed of one\nhidden layer; the classiﬁcation accuracy reached 81%.\n\
    Deep learning models were then used to improve the prediction quality and address\n\
    larger types of diseases and crops. This subsection presents deep learning models\
    \ deployed\non RGB images, multispectral images and hyperspectral images for early\
    \ disease detection.\nIn [53], the authors tested several deep learning models\
    \ from scratch and with transfer\nlearning. The tests were carried out on the\
    \ PlantVillage dataset of plant leaves images [54].\nThe ResNet34 model outperformed\
    \ all other models by achieving an accuracy of 99.67%.\nIn [42], the authors proposed\
    \ a conditional convolutional neural network derived from\nthe ResNet50 topology.\
    \ To test their approach, they generated a dataset of ﬁve crops\nimages acquired\
    \ with a smartphone under real conditions. The approach outperformed the\nconventional\
    \ method using only visual information with an accuracy of 98%. Some authors\n\
    have tested EfﬁcientNet on the PlantVillage dataset [55]; the model outperformed\
    \ state-of-\nthe-art deep learning models achieving 99.91% and 99.97% in the original\
    \ and augmented\nRemote Sens. 2021, 13, 2486\n5 of 24\ndatasets, respectively.\
    \ Likewise, in [56], a smaller dataset of infected tomato plant leaves\nimages\
    \ was divided into different pest attacks and plant diseases. The detection method\n\
    achieved an accuracy of 95.65% using the DensNet161 with transfer learning.\n\
    For better practicability for farmers, researchers developed mobile applications\
    \ for\ndisease detection using deep learning adaptable models with mobile computing\
    \ capacity\nand energy. In [57], the authors proposed a model inspired by MobileNet\
    \ Google models for\ntomato disease detection. The model was able to distinguish\
    \ tomato leaves diseases through\nimage recognition with accuracy reaching 89.2%.\
    \ Similarly, MobileNet was deployed to\ndetect diseases from apple leaves [58].\
    \ Compared to the state-of-the-art model, ResNet,\nMobileNet was the most efﬁcient\
    \ with three times less computing time. In [59], MobileNet\nwas tested for citrus\
    \ disease detection and compared to another CNN model, such as\nSelf-Structured\
    \ (SSCNN) classiﬁers. The results showed that SSCNN was more accurate\nfor citrus\
    \ leaf disease classiﬁcation on mobile phone images.\nIn a pre-symptom disease\
    \ detection task exploiting hyperspectral images, the authors\nin [60] used the\
    \ extreme learning machine (ELM) classiﬁer model on full wavelengths of\nhyperspectral\
    \ tomato leaves images. The identiﬁcation results were very satisfying, with an\n\
    overall classiﬁcation accuracy of 100%, however, the classiﬁcation was time-consuming.\
    \ To\nsolve this issue, they re-established the ELM model based only on the effective\
    \ wavelengths\nselected by the successive projection algorithm (SPA). The model\
    \ performed the disease\nclassiﬁcation task with an accuracy of 77.7%. In a similar\
    \ way, the authors in [61] attempted\nto detect the tobacco mosaic virus (TMV)\
    \ on tobacco leaves using the ELM classiﬁer.\nThey selected the effective wavelengths\
    \ that contain much disease information in order\nto avoid instability of convergence\
    \ in predictive models (high correlation between bands).\nThe overall classiﬁcation\
    \ accuracy (healthy, 2 DPI (days post-infection), 4 DPI or 6 DPI)\nreached 98%\
    \ using input spectral data. In the same context, [41] developed a method to\n\
    detect fusarium head blight disease in wheat using hyperspectral images and a\
    \ speciﬁc\nacquisition protocol considering the ﬁeld conditions. The authors were\
    \ able to classify\ninfected and healthy wheat head crops using hyperspectral\
    \ images. The accuracy reached\n84.6% using a two-dimensional convolutional bidirectional\
    \ gated recurrent unit neural\nnetwork (2D-CNN-BidGRU) hybrid model.\nGround imagery\
    \ is an interesting technology in smart farming. Deep models using\nthis type\
    \ of acquisition guarantee high detection accuracy thanks to the close level leaf\n\
    imaging with high resolution, Table 2 summarizes the effective wavelengths used\
    \ for\ndisease detection for close range imaging presented in this section. However,\
    \ this strategy\nfails to monitor and diagnose plant diseases at a large scale.\
    \ Moreover, these techniques\nare time-consuming in a wide range study area.\n\
    Table 2. Effective wavelengths for disease detection.\nEffective Wavelengths\n\
    Indices\nRef.\n697.44, 639.04, 938.22, 719.15, 749.90, 874.91, 459.58 and 971.78\
    \ nm\n-\n[61]\nfull range 750–1350 nm\n700–1105 nm\n-\n[62]\n665 nm and 770 nm\n\
    SR\n[50]\n670, 695, 735 and 945 nm\nNDVI\n655, 746, and 759–761 nm\n-\n[51]\n\
    445 nm, 500 nm, 680 nm, 705 nm, 750 nm\nRENDVI, PSRI\n[45]\n442, 508, 573, 696\
    \ and 715 nm\n-\n[60]\nFull range 400–1000 nm\n-\n[41]\n2.2. UAV Imaging\nUAVs\
    \ are exploited also as a precision agriculture (PA) solution for monitoring and\n\
    controlling crops growth [25,63], eventual disease development and weed detection\
    \ [64,65],\nthanks to their ability to collect higher resolution images at lower\
    \ costs. It has an effective\nrole in agriculture considering the cost reduction\
    \ by avoiding the need for a ﬁeld expert\nto go through the whole culture several\
    \ times for monitoring. UAVs equipped with\nRemote Sens. 2021, 13, 2486\n6 of\
    \ 24\nembedding cameras and sensors perform efﬁcient ﬁeld data acquisition for\
    \ ﬁeld scale\nvisualization and analysis. Additional elements can help enhance\
    \ performances of crop\nmonitoring techniques, such as the choice of appropriate\
    \ sensors and intelligent recognition\nmodels. As spectrometry is sensitive to\
    \ diseases, multispectral cameras are more often\nused for disease detection studies.\
    \ The combination of cameras mounted on a low-altitude\nremote sensing platform\
    \ allows real-time image acquisition in precise location and with\ndifferent wavelengths.\n\
    For systems using these types of sensors, a large amount of data is ﬁrst stored\
    \ in large-\nscale databases provided by information systems such as the geographical\
    \ information\nsystem (GIS). In fact, the information system enables the visualization\
    \ and analysis this\ndata. Data collected provide information on soil and vegetation\
    \ cover characteristics,\nsuch as soil organic content and soil moisture, biomass\
    \ quantity, weed existence and early\ndetection of crop stress with eventual disease\
    \ stage evaluation.\nTraditional machine learning algorithms are used for plant\
    \ disease detection using\nUAV images. One of the ﬁrst models attempting to predict\
    \ infection severity on plants from\nimages is the Backpropagation NN (BPNN) [62],\
    \ in which the authors extracted spectral\ndata from remote sensing hyperspectral\
    \ images of tomato plants. Afterwards the authors\nrated the images according\
    \ to the light blight severity based on ﬁve stages and tested the\nBPNN on the\
    \ data extracted. The results showed that ANN with backpropagation could\nbe used\
    \ in spectral prediction for disease detection. In the same context, the authors\
    \ in [66]\nattempted to detect leafroll disease using the Classiﬁcation and Regression\
    \ Tree. Their\napproach was based on spectral and spatial signatures extracted\
    \ from UAV hyperspec-\ntral images of grapevine. Correspondingly, the authors\
    \ in [67] extracted spectral bands,\nvegetation indices and biophysical parameters\
    \ of diseased and healthy plants from UAV\nmultispectral images. The ROC analysis\
    \ was then exploited to estimate the capacity of\nthe selected variables for disease\
    \ detection. In [68], the authors adopted a segmentation\napproach based on the\
    \ Simple Linear Iterative Clustering (SLIC) for soybean foliar diseases\ndetection.\
    \ This method employs the k-means algorithm for superpixels. After segmenta-\n\
    tion, the images were classiﬁed using SVM achieving a precision of 98.34%. In\
    \ another\nstudy [69], authors covered the wheat yellow rust infection using UAV\
    \ multispectral im-\nages. The approach based on random forest classiﬁer was able\
    \ to discriminate the disease\nin different development stages with an accuracy\
    \ reaching 89.3%. In [70], UAV images were\nutilized for the detection of citrus\
    \ canker in several disease development stages. The au-\nthors used radial basis\
    \ function (RBF) for classiﬁcation; RBF is an artiﬁcial neural network\nthat performs\
    \ supervised machine learning. The classiﬁcation achieved a disease detection\n\
    accuracy of 92%. In another study [71], the authors extracted vegetation indices\
    \ (VIs) from\nmultispectral images to enhance information on plant characteristics.\
    \ The results showed\nthat the VIs feature compressed with PCA and combined with\
    \ the value of the original data\ngenerated an accuracy of 100% using AdaBoost\
    \ algorithm. Multilayer perceptrons (MLP)\nwere also applied for classiﬁcation\
    \ tasks using hyperspectral data collected on healthy\nand diseased avocado trees\
    \ [72]. Similarly, the SVM classiﬁer was used to detect a fungus\nattacking olive\
    \ trees based on hyperspectral and thermal images captured from a UAV [73].\n\
    The model achieved an accuracy of 80% using an optimal set of spectral bands.\n\
    To conclude, the performance of traditional machine learning approaches is limited\n\
    and can easily vary according to different growing periods and with different\
    \ acquisition\nequipment. In addition, the low performance can also be due to\
    \ the feature engineering\nprocess, which provokes important information loss.\n\
    Deep learning models have also been developed and used to tackle the limitations\n\
    of traditional machine learning for plant disease detection using UAV images.\
    \ In [74], a\nsliding window was used on each plot image, moving in small steps\
    \ along the image. The\nclassiﬁcation task was performed using a convolutional\
    \ neural network (CNN) architecture;\nthe results obtained had a mean absolute\
    \ error value of 11.72% and a relatively low variance.\nIn [75], the classiﬁcation\
    \ between healthy and diseased maize leaves was performed using\nResNet model,\
    \ achieving a test accuracy of 97.85%. Similarly, with the aim of detecting\n\
    Remote Sens. 2021, 13, 2486\n7 of 24\ndisease symptoms in grape leaves [76], the\
    \ authors used the CNN approach by performing\na relevant combination of image\
    \ features and color spaces. Images were converted into\ndifferent colorimetric\
    \ spaces to separate the intensity information from chrominance. The\nCNN model\
    \ Net-5 was tested on multiple combinations of input data and three patch\nsizes.\
    \ The best result using combination obtained an accuracy of 95.86%. In [77], the\n\
    authors proposed a novel deep learning architecture for the detection of yellow\
    \ rust in\nwinter wheat at different observation times across the wheat growing.\
    \ The architecture\nconsists of multiple Inception-ResNet blocks combining the\
    \ Inception and ResNet models\nfor deep feature extraction. The model reached\
    \ an overall accuracy of 85%. In another\nstudy [78], the authors attempted to\
    \ detect disease in pinus trees using UAV images.\nThe classiﬁcation model was\
    \ designed based on a combination of deep convolutional\ngenerative adversarial\
    \ networks (DCGANs), and an AdaBoost classiﬁer. The motivation\nbehind using Adaboost\
    \ classiﬁer is to create a reinforcement learning combining the other\nclassiﬁcation\
    \ models for better precision. The proposed approach achieved a recall value\n\
    of 95.7%. In [79], the authors developed a deep learning approach to combine visible\
    \ and\nnear-infrared images obtained from two different sensors in order to detect\
    \ the grapevine\nmildew symptoms. The ﬁrst step consisted of overlaying the two\
    \ types of images, using an\noptimized image registration, and resulting images\
    \ were used with semantic segmentation\napproach (SegNet architecture) to delineate\
    \ and detect the vine symptoms. Their approach\nachieved an accuracy of 92% for\
    \ detection at grapevine level. The same authors recently\nproposed a deep learning\
    \ architecture that combines multispectral and depth information\nfor vine symptom\
    \ detection [80].\n2.3. Satellite Imaging\nEven if UAV are available, the temporal\
    \ aspect in the historical monitoring task may be\nmissing. Conversely, satellites\
    \ covering wider land areas offer historical images of the study\narea depending\
    \ on satellite acquisition frequency. Plant health status can also be monitored\n\
    using satellite imaging [81]. Indeed, satellites can provide multispectral images\
    \ with very\nhigh spatial resolution that can range from 0.5 m to more than 30\
    \ m. For example, Landsat\nand Sentinel-2 satellite sensors provide the most widely\
    \ accessible medium-to-high spatial\nresolution multispectral data that can be\
    \ used for vegetation phenology. Table 3 details\ncommercial satellite sensors\
    \ collecting multispectral images with a spatial resolution from\n0.5 m to 30\
    \ m (Satellite Imaging Corporation (SIC)). Table 3 shows that satellites with\
    \ high\nspectral resolution have quite large revisit periods. Conversely, high\
    \ temporal resolution\nsatellites have very low spectral resolution [82]. For\
    \ instance, the MODIS sensor for the\nTerra/Aqua satellite collects daily images.\
    \ However, its images have a spatial resolution of\n250 m (band 1, 2), 500 m (bands\
    \ 3–7) and 1000 m (bands 8–36). A spatio-temporal fusion\ncan be useful to carry\
    \ out a vegetation monitoring study using these data [83].\nSeveral machine learning\
    \ methods have been used to perform land monitoring from\nsatellite images, for\
    \ instance: mapping of urban fabric [84–86], crop classiﬁcation and ﬁeld\nboundaries\
    \ [87,88] and pest detection [89].\nTraditional machine learning was used to test\
    \ the usage of satellite images for disease\ndetection. In [90], the authors developed\
    \ a detection application using SPOT-6 images\nwith a supervised classiﬁcation\
    \ algorithm called spectral angle mapper (SAM) to map\npowdery mildew of winter\
    \ wheat. The classiﬁcation was based on selected bands (green\nand red) and indices\
    \ of disease-sensitive vegetation. The approach achieved an overall\nmapping accuracy\
    \ of 78%. Similarly, the authors in [91] collected images from Sentinel\n2 for\
    \ stress detection in rice. The types of stress detected in this study were pests\
    \ and\ndisease stress, heavy metal stress or double stress combining the two ﬁrst\
    \ types. The\nstudy demonstrated the usefulness of satellite imagery to distinguish\
    \ the causes of stress in\ndifferent areas using the coefﬁcients of spatio-temporal\
    \ variation (CSTV) derived from the\nstress-sensitive VIs related to red edge\
    \ bands. In [92], the authors were able to discriminate\nseverity levels of yellow\
    \ rust infection (i.e., healthy, slight, and severe) in winter wheat\nusing multispectral\
    \ bands for the Sentinel 2 sensor and hyperspectral data acquired at\nRemote Sens.\
    \ 2021, 13, 2486\n8 of 24\ncanopy level. To achieve the classiﬁcation task, a\
    \ new multispectral index, the Red Edge\nDisease Stress Index (REDSI) was proposed.\
    \ This index was based on the sensitive bands\nB4 (Red), B5 (Re1), and B7 (Re3)\
    \ and validated with an overall identiﬁcation accuracy of\n85.2% using the optimal\
    \ threshold. SVM was deployed in [93] for disease detection in\nwinter wheat.\
    \ The proposed approach was based on growth indices and environmental\nfactors\
    \ calculated from Landsat-8 images. The model achieved an overall accuracy of\
    \ 80%.\nIn [94], the naive Bayes algorithm was tested on spectral signatures of\
    \ coffee berry necrosis\nissued from Landsat 8 OLI satellite images in the aim\
    \ of disease detection; the classiﬁcation\nreached an accuracy of 50%. Nevertheless,\
    \ accurate disease detection predictions require\nsmart data processing using\
    \ a smart method.\nTable 3. Satellite sensors collecting multispectral images\
    \ with spatial resolution (0.5–30 m) and spatial resolution.\nSatellite\nSensor\n\
    Spatial Resolution\nRevisit\nCycle\nLunched\nWorldView-2\nMultispectral sensor\n\
    0.46 m: 8 multispectral bands\n1.1 day\n8 October 2009\nWorldView-3\nMultispectral\
    \ sensor\n1.24 m: multispectral\nresolution\n3.7 m: SWIR\n<1.0 day\n13 August\
    \ 2014\nWorldView-4\nMultispectral sensor\n1.24 m: VNIR\n4.5 days\n7 January 2019\n\
    Pleiades-1A\nMultispectral sensor\n2 m: VNIR\n1 day\n16 December 2011\nQuickBird\n\
    Multispectral sensor\n2.62 m to 2.90 m VNIR\n1–3.5 days\n18 October 2001\nGaofen-2\n\
    Multispectral sensor\n3.2 m: B1, 2, 3, 4\n5 days\n19August 2014\nJiline-1\nOptical\
    \ Satellite\n2.88 m multispectral imagery\n3.3 days\n7 October, 2015\nHyperspectral\
    \ sensor\n5 m: 28 hyperspectral bands\n21 January 2019\nRapidEye\nMultispectral\
    \ sensor\n5 m: VNIR\n5.5 days\n29 August 2008\nTHEOS\nMultispectral sensor\n15\
    \ m: VNIR\n26 days\n1 October 2008\nSentinel 2\nMSI (Sentinel 2A and 2B)\n10 m:\
    \ (VNIR) B2, 3, 4, 8\n20 m: B5, 6, 7, 8A, 11, 12, 60\n30 m: B1, 9, 10\n10 days\n\
    23 June 2015 and 7 March 2017\nLandsat\nOLI+ (Landsat-8)\n30 m: VNIR + SWIR\n\
    16 days\n11 February 2013\nETM+(Landsat-7)\n30 m: VNIR\n60 m: TIR\n16 days\n15\
    \ April 1999\nHJ-1A/1B\nWVC\n30 m: VNIR\n4 days\n6 September 2008\nTH-01\nMultispectral\
    \ sensor\n10 m: VNIR\n5 days\n24 August 2010\nALOS\nAVNIR-2\n10 m: VNIR\n46 days\n\
    24 January 2006\nSPOT-7\nMultispectral sensor\n6 m: VNIR\n1 day\n30 June 2014\n\
    SPOT-6\nMultispectral sensor\n6 m: VNIR\n1 day\n9 September 2012\nSPOT-5\nMultispectral\
    \ sensor\n10 m: VNIR\n20 m: SWIR\n2–3 days\n4 May 2002\nASTER\nMultispectral sensor\n\
    15 m: VNIR\n30 m: SWIR\n16 days\n18 December 1999\nDeep learning has proven its\
    \ high performance for disease detection also using satellite\nimages. In [95],\
    \ the authors proposed a gated recurrent unit (GRU)-based model to predict\ndevelopment\
    \ of sudden death syndrome (SDS) disease in soybean quadrats. Twelve\nPlanetScope\
    \ satellite images were conducted in this study. The method incorporated\ntime-series\
    \ information for the classiﬁcation task in different scenarios, each scenario\n\
    having a different sequence size. Interestingly, the highest accuracy value was\
    \ reached\nin the fourth scenario with the highest sequence size, which means\
    \ that when enough\nhistorical images are available, precision improves. However,\
    \ the study suffers from a\nRemote Sens. 2021, 13, 2486\n9 of 24\nlimited dataset,\
    \ which has the effect of unbalancing the development of the SDS. The issue\n\
    was addressed by assigning weights to diseased samples. The high spatial resolution\
    \ is\nan important criterion for plant disease detection. In fact, the range of\
    \ 10 m resolution\nand above are barely enough for crop classiﬁcation task, which\
    \ becomes challenging for\ndisease detection [96]. To bridge the gap of lacking\
    \ data and improve the prediction,\nseveral analysts recommended incorporating\
    \ satellite images with aerial images and other\ndata sources such as wireless\
    \ sensor networks that capture environmental parameters for\ndisease detection\
    \ [97].\n2.4. Internet of Things Sensors\nIoT sensors are one of the most widely\
    \ used technologies in PA, due to their efﬁciency,\nease of installation and low\
    \ cost. A typical wireless monitoring system must contain\nmultiple sensors connected\
    \ in each zone to an installed node, with sensors and nodes\ncommunicating via\
    \ radio-frequency. In addition, a gateway is also needed to accomplish\nconnection\
    \ between sensors and the user [98–100]. Once connected to the Internet server,\n\
    the user can access the collected data. In case the WSN is unavailable, one of\
    \ the existing al-\nternative solutions is the weather station [101] which provides\
    \ different local measurements\nin real-time for various agricultural applications.\
    \ Several studies have been established to\ncollect wireless sensor network data\
    \ for disease detection. In [102], the authors developed\nan IoT monitoring system\
    \ that collects the environmental and soil information data using a\nwireless\
    \ sensor network. Collected data have been used for early detection of tomato\
    \ and\npotato disease. In [103], the authors developed a monitoring and prediction\
    \ system for\nmildew prediction in a vineyard. The approach was based on temperature,\
    \ humidity and\nrainfall observations and the Goldanich model for prediction and\
    \ alarming. Analyzing\nthe resulting data is an essential key to ensure phytosanitary\
    \ protection. Nevertheless, the\nclassic methods used for disease detection are\
    \ limited and it is more interesting to take\nadvantage of machine learning algorithms\
    \ to generate efﬁcient prediction models.\nTraditional machine learning: Many\
    \ research studies have been carried out to control\nand monitor plants, as well\
    \ as predict their health status based on speciﬁc physical sensors,\nsince abiotic\
    \ factors help to determine the health status of crops. In [104], the authors\n\
    developed a surveillance system to identify the risk of grape disease in its early\
    \ stages using\nthe Hidden Markov model. Sensors for temperature, relative humidity\
    \ and leaf humidity\nare placed in the vineyard to collect the necessary data.\
    \ These data were transferred\nto a server via Zig-Bee communication (standard\
    \ designed for low-power wireless data\ntransmission). For the classiﬁcation task,\
    \ the favorable conditions for those responsible for\nthe spread of diseases in\
    \ grapes were provided by the National Center for Research on\nGrapevines (CNRG),\
    \ as shown in Table 4. A naive Bayes kernel model was used in [105]\nfor disease\
    \ prediction based on environmental and soil information extracted using an IoT\n\
    monitoring system. A KNN model was deployed for the early detection of agricultural\n\
    diseases [106]. The prediction was based on multiparameters extracted from the\
    \ ﬁeld,\nnamely atmospheric temperature, atmospheric humidity, CO2 concentration,\
    \ illumination\nintensity, soil moisture, soil temperature and leaf wetness. The\
    \ model achieved promising\nresults, proving the validity of environmental data\
    \ for early disease detection. Similarly,\nin [107], the authors proposed a system\
    \ to predict the health status of tomato plants. Since\nabiotic factors such as\
    \ temperature, soil moisture and humidity help to determine whether\nthe plant\
    \ is growing in healthy conditions or not, the system used two sensors: a soil\n\
    moisture sensor and a temperature-humidity sensor. Two supervised learning algorithms\n\
    (SVM and Random Forest) and an unsupervised learning technique (K-means clustering)\n\
    were tested. The algorithms achieved test accuracies of 99.3% for SVM, 99.6% for\
    \ Random\nForest and 99.5% for K-means.\nDeep learning: In [108], the authors\
    \ developed an approach for prediction of cotton dis-\nease and pests occurrence.\
    \ The approach was designed based on weather and atmospheric\ncirculation time\
    \ series collected from six different zones in India. Bidirectional-LSTM\n(Bi-LSTM)\
    \ was then introduced for prediction; it achieved an accuracy of 87.84% and an\n\
    Remote Sens. 2021, 13, 2486\n10 of 24\noverall area under the curve (AUC) score\
    \ of 0.95. Nevertheless, we noticed that the amount\nof IoT papers established\
    \ for disease detection using machine learning is not sufﬁcient,\nwhich may be\
    \ due to the fact that these data are not efﬁcient in prediction crop health\n\
    status. Thus, these inputs coupled with other types of data can provide valuable\
    \ results\nby using appropriate fusion techniques and adequate AI models for good\
    \ adjustments to\nthese complex multivariate data.\nTable 4. Favorable conditions\
    \ for disease spread in grapevine. Reprinted with permission from\nref. [104].\
    \ Copyright 2016 Rajarambapu Institute of Technology.\nDisease\nTemperature (◦C)\n\
    Moisture (%)\nLeaf Wetness Duration\nBacterial Leaf Spot\n25–30\n80–90\n-\nPowdery\
    \ Mildew\n21–27\nMore than 48\n-\nDowny Mildew\n17–32.5\nMore than 48\n2–3\nAnthracnose\n\
    24–26\n-\n12\nBacterial Cancer\n25–30\n>80\n-\nRust\n24\n75\n-\n2.5. Summary\n\
    Some of the most innovative technologies in plant protection are connected sensor\n\
    networks, since there is a correlation between variations in microclimatic conditions\
    \ and\nplant stress. Numerous research studies were carried out to control and\
    \ monitor crops,\nand also predict plant health based on meteorological characteristics\
    \ [100,104,107]. In\naddition, images can be a better representation of crop health\
    \ state. This is due to the\nspectral signature of symptoms on the crops and plant\
    \ leaves. Ground images, UAV\nimages [74,76,109] and satellite images [91,92]\
    \ have proven effective in detecting plant\ndiseases. Table 5 is a summary of\
    \ the research studies presented previously.\nWe noticed that a new tendency in\
    \ disease detection application is spreading widely,\ncharacterized by the use\
    \ of deep learning. This may be due to the high performance of\ndeep learning\
    \ (DL) models compared to conventional machine learning models [60]. DL\neliminates\
    \ the manual feature extraction phase that can sometimes result in low prediction\n\
    performance and requires less effort for feature engineering [23]. In addition,\
    \ DL models\nhave been used to efﬁciently classify diseases in challenging environments\
    \ with complex\nbackgrounds and overlapping plant leaves. Conversely, traditional\
    \ machine learning\ncannot effectively distinguish symptoms of disease with similar\
    \ characteristics, nor can it\ntake advantage of a larger number of trainings\
    \ [42].\nNevertheless, there are still considerable drawbacks to DL regarding\
    \ training time\nthat can reach weeks depending on the processor capacity of the\
    \ computer used, dataset\nsize and model complexity. In the context of plant disease\
    \ detection, datasets are not\nsufﬁciently available or are inadequate, especially\
    \ when it concerns the task of early\ndisease detection. Prior knowledge of the\
    \ crop and the history of the parcels containing\ndiseases and pests that occur\
    \ is a preliminary task. To tackle this issue, researchers opt\nfor creating their\
    \ own dataset by monitoring and capturing the natural development\nof the infestation\
    \ if it occurs [54] or by inoculating the fungus causing the disease in\nan experimental\
    \ greenhouse [50,51]. Regarding acquisition, a hyperspectral image, for\nexample,\
    \ requires relatively expensive instruments and experts for the data collection\n\
    protocol [61]. In addition, annotation is a mandatory step for creating a new\
    \ dataset. The\ntask is time-consuming and involves agriculture experts for the\
    \ annotation of different\ndiseases since the task is not within the reach of\
    \ ordinary volunteers. As researchers\ntend to use data augmentation methods for\
    \ small datasets, these methods are not always\nefﬁcient and cannot exceed a certain\
    \ threshold to avoid overﬁtting. Once the dataset is\navailable, it can suffer\
    \ from imbalance, where samples of healthy plants are more important\nthan samples\
    \ of diseased plants, as well as seasonal and regional difﬁculties with various\n\
    categories of crop diseases [95,111].\nRemote Sens. 2021, 13, 2486\n11 of 24\n\
    Table 5. Summary of various crop disease detection using imaging techniques, IoT\
    \ and data fusion techniques, based on\ntraditional machine learning (TML) and\
    \ deep learning (DL) methods.\nTypes of Data\nMethod\nCrop\nData\nAccuracy\nRef.\n\
    Ground\nimaging\nTML\nSVM\nBarely\n204 images\n68%\n[45]\nSVM\nTomato\n284 images\n\
    93.90%\n[110]\nSVM\nRice\n120 images\n73.33%\n[48]\nPCA\nPotato\n120 images\n\
    -\n[50]\nKNN\nTomato\n212 images\n92.86%\n[51]\nANN\nwheat\n630 multispectral\
    \ images\n81%\n[52]\nDL\nELM\nTomato\n310 hyperspectral images\n100%\n[60]\nELM\n\
    Tobacco\n180 hyperspectral images\n98%\n[61]\nResNet\nMultiple\n55,038 images\n\
    99.67%.\n[53]\n2D-CNN-BidGRU\nWheat\n90 images\n84.6%\n[41]\nResNet-MC-1\nmultiple\n\
    121,955 images\n98%\n[42]\nAdapted\nMobileNet\ntomato\n7176 images\n89.2%\n[57]\n\
    SSCNN\ncitrus\n2939 images\n99%\n[59]\nMobileNet\napple\n334 images\n73.50%\n\
    [58]\nDensNet\nTomato\n666 images\n95.65%\n[56]\nEfﬁcientNet\nmultiple\n55,038\
    \ images\n99.97%\n[55]\nUAV imaging\nTML\nBPNN\nTomato\nHyperspectral images\n\
    -\n[62]\nCART\nVine grape\nHyperspectral images\n94.1%\n[66]\nROC analysis\nVine\
    \ grape\nMultispectral images\n-\n[67]\nSLIC + SVM\nSoybean\nRGB images\n98.34%.\n\
    [68]\nRandom forest\nWheat\nMultispectral images\n89.3%\n[69]\nRBF\nCitrus\nMultispectral\
    \ images\n96%\n[70]\nAdaBoost\nCitrus\nMultispectral images\n100%\n[71]\nSVM\n\
    Olive\nThermal and hyperspectral\nimages\n80%\n[73]\nMLP\nAvocado\nHyperspectral\
    \ images\n94%\n[72]\nDL\nResNet\nMaize\nRGB images\n97.85%\n[109]\nCNN\nPotato\n\
    Multispectral images\n-\n[74]\nNet-5\ngrapevine\nMultispectral images\n95.86%\n\
    [76]\nCNN\nMaize\nRGB images\n95.1%\n[75]\nDCNN\nWheat\nHyperspectral images\n\
    85%\n[77]\nDCGAN +\ninception\nPinus Tree\nRGB images\n-\n[78]\nSegNet\ngrapevine\n\
    Multispectral images\n-\n[79]\nVddNet\ngrapevine\nMultispectral images\n93.72%\n\
    [80]\nSatellite\nimagery\nTML\nSAM\nWheat\n(SPOT-6)\n78%\n[90]\nOptimal threshold\n\
    Wheat\n1 image (Sentinel-2)\n85.2%\n[92]\nSVM\nWheat\n3 images (Landsant-8)\n\
    80%\n[93]\nNaive Bayes\nCoffee\n3 images (landsat-8)\n50%\n[94]\nDL\nGRU\nSoybean\n\
    12 images (PlanetScrope)\n82.5%\n[95]\nIoT data\nTML\nHMM\nGrape\nTemperature,\
    \ relative humidity\nand leaf humidity\n90.9%\n[104]\nSVM\nRose\nTemperature,\
    \ humidity and\nbrightness\n-\n[100]\nNaive Bayes Kernel\nmultiple\nSoil and environmental\
    \ data\n-\n[105]\nKNN\nSoil and environmental data\n95.9%\n[106]\nGoidanich model\n\
    Vine\nTemperature, humidity and\nRainfall\n-\n[103]\nRandom forest\nTomato\nTemperature,\
    \ soil moisture and\nhumidity\n99.6%\n[107]\nDL\nBi-LSTM\nCotton\nWeather + atmospheric\
    \ circulation\nindexes\n87.84%\n[108]\nRemote Sens. 2021, 13, 2486\n12 of 24\n\
    3. Data Fusion Potential for Disease Detection\nAs the data comes from multiple\
    \ sources, it seems more judicious to combine them to\nachieve a better performance\
    \ in disease detection. Multimodal fusion for disease detection\nis still an ongoing\
    \ area of study. In fact, researchers have started to notice the importance\n\
    of merging heterogeneous types of data from different sensors [39,97]. Nevertheless,\n\
    important effort must be done in order to integrate sophisticated fusion techniques\
    \ on\nmultimodal data. This will allow a better understanding of crop behavior\
    \ and thus improve\nthe prediction quality. A deeper understanding of plant features\
    \ can be achieved by fusing\ndata from multiple sensors to provide more accurate\
    \ and efﬁcient predictions.\nData fusion is the combination and simultaneous use\
    \ of data and information from\nmultiple sources to achieve better performance\
    \ than using data sources separately. It is\noften related to the need to perceive\
    \ different environmental variables from sensors [112].\nMultimodal data fusion\
    \ is a challenging task because it deals with a combination of\nheterogenous data\
    \ from different modalities (images, signals, times series, etc.) [113].\nCompared\
    \ to classical probabilistic fusion methods, machine learning techniques have\n\
    proven their capacity to provide more accurate predictions for fusion [114]. In\
    \ this section,\nwe review data fusion techniques, namely measurement fusion,\
    \ feature fusion, decision\nfusion, hybrid fusion and tensor fusion, and explore\
    \ the data fusion applications using\nmachine learning for agriculture. Finally,\
    \ we will discuss the major challenges in applying\ndata fusion in agriculture.\n\
    3.1. Data Sources\nData sources can provide useful information about the studied\
    \ phenomena; for uni-\nmodal data source a simple data concatenation can be enough\
    \ for prediction purposes [104].\nOtherwise, when we have several types of sensors,\
    \ advanced data fusion is necessary [115].\nData from several sensors ﬁrst require\
    \ data analysis to characterize, order or correlate\nthe different available data\
    \ sources, and then to decide on the strategy or algorithm to\nbe used to merge\
    \ the data. Among the relationships that exist, we can ﬁnd distribution,\ncomplementarity,\
    \ heterogeneity, redundancy, contradiction, concordance, discordance,\nsynchronization\
    \ and difference in granularity [112].\n3.2. Data Fusion Categories\nIn literature,\
    \ data fusion methods are divided into three main categories: probability-\nbased\
    \ methods, evidence-based methods and knowledge-based methods. Probability-\n\
    based methods [37] such as the Kalman filter [116], the Bayesian fusion [117]\
    \ and the\nHidden Markov model [118] are limited to low-dimensional or homogeneous\
    \ data\nand suffer from high computational complexity. Therefore, they are not\
    \ adequate\nfor complex problems. Evidence-based methods [119], such as the Dempster\
    \ Shafer\ntheory [120,121], are used to deal with missing information, additional\
    \ assumptions and\nsolve the problem of uncertainty. Nevertheless, they present\
    \ estimation limitations that\nrestrict their applications.\nConversely, knowledge-based\
    \ methods have proven to be effective in feature extrac-\ntion, data reduction,\
    \ classiﬁcation and decision-making [122,123]. This type of method\nallows the\
    \ fusion center to extract useful information from large imprecise datasets.\n\
    3.3. Intelligent Multimodal Fusion\nMultimodal fusion based on machine learning\
    \ [124] is capable of learning represen-\ntations of different modalities at various\
    \ levels of abstraction [125], with signiﬁcantly im-\nproved performances [126].\
    \ Multimodal fusion can be split into two main categories [123]:\nmodel-based\
    \ approaches that explicitly address fusion in their construction, and model-\n\
    agnostic approaches which are general and ﬂexible and do not directly depend on\
    \ a\nspeciﬁc machine learning method. Depending on data abstraction level, different\
    \ fusion\narchitectures for the agnostic fusion [37,123,127] are possible.\nRemote\
    \ Sens. 2021, 13, 2486\n13 of 24\nMeasurement fusion (or early fusion), also known\
    \ as ﬁrst level data fusion, allows\nthe immediate integration and presentation\
    \ of sensor data using feature vectors. Data are\ngenerally concatenated [104],\
    \ which makes fusion limited when dealing with heterogeneous\ndata. This architecture\
    \ is the most widely used because of its simplicity: it is easy to align\ndata.\
    \ In [128], the authors tried to predict the rate of photosynthesis and calculate\
    \ the\noptimal CO2 concentration based on real-time environmental information\
    \ via a WSN\nsystem in greenhouses for tomato seedling stage cultivation. The\
    \ BPNN prediction model\ntakes as input parameters the environmental variables\
    \ (temperature, CO2 concentration,\nhumidity and luminosity) and the photosynthesis\
    \ rate of the individual leaf array as the\noutput parameter.\nFeature fusion\
    \ combines the results of early fusion and individual unimodal predictors\nby\
    \ merging feature vectors, allowing heterogeneous data from different data sources\
    \ to be\ncombined. In [129], deep fusion architectures were implemented to detect\
    \ defects in a plan-\netary gearbox using four types of signals as inputs. Deep\
    \ Convolutional Neuron Networks\n(DCNNs) were used on multiple levels of multimodal\
    \ data fusion. The feature level fusion\nwith feature learning from raw data was\
    \ performed after the raw data extraction phase.\nDCNNs were applied on each type\
    \ of data to learn features, and then the outputs were\nextracted as the learned\
    \ features. The learned features were ﬁnally combined and fed to\nanother DCNN\
    \ for feature-level fusion classiﬁcation.\nDecision fusion (or late fusion) involves\
    \ processing data from each sensor separately to\nobtain high-level inference\
    \ decisions, which are then combined in a second stage [130]. The\ndecision-level\
    \ fusion method combines information from different sensors after each sensor\n\
    has made a preliminary decision. The combination can be simple or weighted. In\
    \ [131],\na use case of weighted decision fusion architecture on multiple sensors\
    \ is presented. In\naddition to the sensor data, two classical characteristics\
    \ (power and median frequency)\nwere extracted from each signal corresponding\
    \ to each sensor and entered the individual\nchannel classiﬁer. Then, the method\
    \ of weighted majority voting (WMV) was used to\nmerge the resulting vectors,\
    \ with each sensor data being weighted by a conﬁdence measure\n(or weight).\n\
    Hybrid fusion merges information at two or more levels. In the hybrid approach\n\
    proposed in [115], the authors developed the merging technique of different CNN\
    \ classiﬁers\nfor object detection in changing environments. Three types of input\
    \ modalities were used:\nRGB, depth and optical ﬂow. The CifarNet architecture\
    \ was designed as the single expert\nmodel, and then the outputs of each expert\
    \ network model were fused with weights\ndetermined by an additional network called\
    \ gating network. This approach was called\nMixture of Deep Experts (MoDE).\n\
    Tensor Fusion (TFM) consists mainly of a tensor fusion layer that models unimodal,\n\
    bimodal and trimodal interactions using a three-fold Cartesian product from modality\
    \ inte-\ngration [132]. The architecture has been improved to lower the computational\
    \ complexity;\nthe resulting architecture is called low rank tensor fusion (LMF)\
    \ model [133]. LMF has been\nproposed to identify the emotions of speakers according\
    \ to their verbal and non-verbal\nbehaviors, based on visual, audio and language\
    \ data. Three YouTube videos databases\nwere used with annotation of feelings,\
    \ speaker traits and emotions. The learning network\nfor acoustic and visual modalities\
    \ was represented by a two-layer neural network, and\nfor linguistic modalities,\
    \ a Long Short-Term Memory (LSTM) network was used to extract\nthe representations.\
    \ The LMF model, compared to the tensor fusion model, performed\nsigniﬁcantly\
    \ better for all datasets and measurements. Moreover, the LMF signiﬁcantly\nreduced\
    \ the computational complexity from exponential to linear.\nMultimodal Search\
    \ Architecture Fusion (MFAS) is a generic architecture that creates\na large number\
    \ of possible fusion architectures, scans the neural architecture and choses\n\
    the best performing architectures [126]. The MFAS is inspired by the progressive\
    \ neural\narchitecture search (PAS) [134] where the search is efﬁciently guided\
    \ for architecture\nsampling using temperature-based sampling [135]. Testing three\
    \ datasets, the MFAS has\nproven its efﬁcacity against the state-of-the-art results\
    \ on those datasets.\nRemote Sens. 2021, 13, 2486\n14 of 24\nIn [136], the authors\
    \ performed a comparison between four types of fusion (late, MoE,\nLFM and MiD)\
    \ on image and signal modalities for automatic texture detection of objects.\n\
    Fusion methods provided latent vectors which were introduced in the corresponding\
    \ artiﬁ-\ncial neural networks ANNs. The most efﬁcient fusion method in the texture\
    \ classiﬁcation\ntask was the LMF which achieved an average test accuracy of 88.7%.\
    \ Tested on degra-\ndation scenarios, the Late, MoE and Mid fusion methods behaved\
    \ similarly. The fusion\narchitecture potentially allowed ANN to achieve good\
    \ results in the texture detection task.\nNevertheless, the performance without\
    \ the main modality (images) decreased signiﬁcantly.\nTo conclude, machine learning-based\
    \ multimodal fusion approaches have an impor-\ntant potential to solve open issues\
    \ in agriculture by merging different types of data. We\nbelieve that exploiting\
    \ these advanced techniques for disease detection issues can provide a\nbetter\
    \ understanding of the plant environment and thus improve prediction performance.\n\
    3.4. Data Fusion Applications in Agriculture\nEven if advanced fusion techniques\
    \ are a rapidly growing area in agriculture,\nliterature still lacks studies on\
    \ disease detection in this domain. Different applications\non data fusion in\
    \ agriculture are presented in literature, specifically data fusion for\nyield\
    \ prediction [63,137,138], crop identification [96,139], land monitoring [140,141]\n\
    and disease detection [111,142,143].\n3.4.1. Data Fusion for Yield Prediction\n\
    In [63], the authors investigated the relationship between canopy thermal information\n\
    and grain yield, using data fusion of data from different sensors. They extracted\
    \ spectral\n(VIs), structure (vegetation fraction (VF), canopy height (CH)), thermal\
    \ (normalized relative\ncanopy temperature (NRCT)) and texture information from\
    \ canopy using multi-sensors\ninstalled on a UAV. Two fusion models were used\
    \ in this study, input-level feature fusion\nDNN (DNN-F1) and intermediate-level\
    \ feature fusion DNN (DNN-F2). The DNN-F2\noutperformed DNN-F1 in terms of prediction\
    \ accuracy, spatial adaptability and robustness\nacross different types of models.\n\
    In [138], datasets of summer and winter rice yield, meteorology data and area\n\
    data from 81 counties in a Chinese region were used. To predict rice yield, the\
    \ authors\nproposed a deep learning fusion model named BBI model combining backpropagation\n\
    neural networks (BPNNs) with an independent recurrent neural network (IndRNN).\n\
    The model first captured deep spatial and temporal features from the input data,\
    \ then\ncombined these deep features by fusing the outputs and then learned the\
    \ relationship\nbetween these features and yield values. The proposed model accurately\
    \ predicted both\nsummer and winter rice yield.\nIn another study [137], extreme\
    \ learning regression machine (ELR) was used to phe-\nnotype estimation from several\
    \ sensors. First, the authors simultaneously collected RGB,\nmultispectral and\
    \ thermal images from sensors installed on a drone. Then, vegetation traits\n\
    (color, physical structure, spectral and thermal features) were extracted from\
    \ the images.\nThe features were combined and fed into the ELR prediction model.\
    \ Compared to other\nclassiﬁers, with multisensory combinations, ELR provided\
    \ relatively accurate results of\nplant features estimation.\n3.4.2. Data Fusion\
    \ for Crop Identiﬁcation\nIn this study [139], the authors exploited spatio-temporal\
    \ data to segment satellite\nimages of vegetation ﬁelds. The data used are images\
    \ captured by the Gaofen 1 and\n2 satellite. The authors developed a CNN 3D active\
    \ architecture to extract information for\nthe multi-temporal images. The 3D tensor\
    \ is composed of the spectral, spatial and temporal\ncharacteristics of each element\
    \ in each band. The convolution is done at spatial-spectral\nor spatial-temporal\
    \ scale. In the same context, some researchers tested the feasibility of\ntemporal\
    \ CNNs (TempCNNs) for satellite images classiﬁcation [96]. To do so, they collected\n\
    46 images during one year from the Formasat-2 satellite. Three bands (NIR, red\
    \ (R) and\nRemote Sens. 2021, 13, 2486\n15 of 24\ngreen (G)) and three VIs were\
    \ used (NDVI, Normalized Difference Water Index (NDWI)\nand Brilliance Index (IB)).\
    \ The proposed algorithm consisted of three convolutional ﬁlters\napplied consecutively.\
    \ The results obtained showed that the overall accuracy of the CNN\nmodels increased\
    \ when more features were added, regardless of their type. The proposed\nmodel\
    \ using the spectral bands in the feature vector outperformed all other combinations\n\
    by a variation between 1% and 3%, achieving an accuracy of 93.4%.\n3.4.3. Data\
    \ Fusion for Land Monitoring\nNearly all studies on satellite images agree that\
    \ a very high resolution is the key to\nachieving interesting results. Thus, the\
    \ required resolution is not frequently available.\nTherefore, some researchers\
    \ have attempted to develop resolution improvement techniques\nto solve this issue\
    \ using data fusion. In this context, the authors in [83] developed an\nextended\
    \ super-resolution convolutional neural network (ESRCNN) for data fusion frame-\n\
    work, speciﬁcally to blend Landsat-8 and Sentinel-2 images of 20 m and 10 m spatial\n\
    resolution, respectively. The study produced temporally dense observations of\
    \ land sur-\nfaces at short time intervals using Landsat-8 and Sentinel-2 data.\
    \ In the same context, the\nauthors in [140] proposed a method for exploiting\
    \ UAV remote sensing data by fusing high-\nand low-resolution images. The resulting\
    \ data were then transmitted to a deep semantic\nsegmentation module to provide\
    \ a useful reference for sunﬂower lodging assessment and\nmapping. The fusion\
    \ approach outperformed the no-fusion approach using the same mod-\nels, and the\
    \ best accuracies were achieved using the SegNet method reaching 84.4% and\n89.8%\
    \ without and with image fusion, respectively, on the test set. In another study\
    \ [141],\nthe authors suggested a satellite/UAV fusion technique for monitoring\
    \ soybean ﬁelds\nusing machine learning. They combined spectral canopy information\
    \ (vegetation indices)\nextracted from Worldview-2/3 data with canopy structure\
    \ features (canopy cover and\nheight) calculated from UAV RGB images. These features\
    \ were combined and fed into their\nELR dual activation prediction model for plant\
    \ characteristics predictions. Their results\nshowed that predictions based on\
    \ the combination of multi-sensors (Satellite/UAV) data\noutperformed those using\
    \ single-sensor features.\n3.4.4. Data Fusion for Disease Detection\nPlant monitoring\
    \ tools for disease identiﬁcation and classiﬁcation produce a huge\namount of\
    \ data. One way of dealing with these data is either to analyze each type of\n\
    modality separately to compare results and evaluate the method validity [144,145],\
    \ or to\nfuse and combine data for a better understanding of disease conditions.\
    \ One of the ﬁrst\nattempts to integrate multisource data for disease detection\
    \ was developed using both\nmeteorological data and satellite scenes [142]. The\
    \ classiﬁcation task was based on logistic\nregression and the effective characteristics\
    \ extracted from both modalities. Results showed\ngreat potential for the multimodal\
    \ data integration for disease detection. In [111], the\nauthors proposed a multi-context\
    \ fusion network for crop disease detection. The approach\nwas based on images\
    \ collected in the ﬁeld, in addition to contextual information (season,\ngeographical\
    \ location, temperature and humidity). The proposed model was composed\nof three\
    \ major parts: CNN backbone for visual features extraction, ContextNet for fusion\n\
    of the contextual features, and a fully connected network for fusion of all features\
    \ and\nﬁnal predictions. The proposed approach achieved an identiﬁcation accuracy\
    \ of 97.5%.\nNonetheless, their method suffers from imbalanced data due to seasonal\
    \ and regional\ndifﬁculty with various categories of crop diseases. In another\
    \ study [143], aiming to detect\ndiseases in mixed and complex African landscapes,\
    \ the researchers split the study into\nthree main parts: pixel-based banana classiﬁcation,\
    \ object-based banana localization and\ndisease detection. The classiﬁcation was\
    \ established using SVM in which the inputs were a\ncombination of multispectral\
    \ bands with vegetation indices extracted from the multi-level\nsatellite images\
    \ (Sentinel 2, PlanetScope and WorldView-2) and UAV (MicaSense RedEdge)\nimages.\
    \ For banana plant detection in the ﬁeld, they trained the object detection model\n\
    RetinaNet on UAV RGB images and developed a custom classiﬁer for simultaneous\
    \ banana\nRemote Sens. 2021, 13, 2486\n16 of 24\ntree localization and disease\
    \ classiﬁcation. Compared to the VGG model, the custom\nclassiﬁer provided the\
    \ best results with an accuracy reaching 92%. Although the approach\nhas good\
    \ classiﬁcation results, it suffers from considerably important training time.\n\
    3.4.5. Summary\nThe applications of data fusion in agriculture presented in this\
    \ section can be divided\ninto three types. Spatio-spectral fusion is a multi-band\
    \ fusion that constitutes a ﬁne-spatial\nand ﬁne-spectral fusion. Spatio-temporal\
    \ fusion is based on blending data with ﬁne spatial\nresolution and coarse temporal\
    \ resolution (temporal revisit frequency) with data that have\nﬁne temporal resolution,\
    \ but coarse spatial resolution, with the objective being to create a\nﬁne spatio-temporal\
    \ resolution. Finally, multimodal fusion corresponds to heterogeneous\nmultisensory\
    \ fusion. Table 6 summaries data fusion applications in agriculture presented\n\
    in this subsection.\nTable 6. Data fusion applications in agriculture.\nFusion\n\
    Type\nCrop\nSensor\nData Type\nModel Type\nModel Output\nRef.\nSpatio-\ntemporal\n\
    fusion\n-\nLandsat-8 and\nSentinel-2 satellites\nImages\nESRCNN\nHigh resolution\n\
    land image\n[83]\nMultiple\nFormasat-2 satellite\nImages\nTempCNNs\nCrop classiﬁcation\n\
    [96]\nMultiple\nGaofen 1, 2 satellite\nImages\nCNN 3D\nCrop classiﬁcation\n[139]\n\
    Spatio-\nSpectral\nfusion\nSoybean\nMulti-sensors UAV\nRGB, multispectral\nand\
    \ thermal images\nELR\nCrop phenotype\nestimation\n[137]\nSunﬂower\nMulti-sensors\
    \ UAV\nRGB and\nmultispectral\nimages\nSegNet\nLodging\nidentiﬁcation\n[140]\n\
    Multimodal\nfusion\nCanopy\nMulti-sensors UAV\nRGB, multispectral\nand thermal\
    \ images\nDNN-F2\nYield prediction\n[63]\nRice\nMulti-sensors\nYield and\nmeteorology\
    \ data\nBBI\nYield classiﬁcation\n[138]\nSoyben\nSatellite/UAV\nSatellite/UAV\n\
    ELR\nVegetation feature\nprediction\n[141]\nWheat\nSatellite/IoT sensors\nSatellite\
    \ +\nmeteorological data\nLogistic\nregression\nDisease detection\n[142]\nMultiple\n\
    Camera /IoT sensors\nimages +\nmeteorological data\nMCFN\nDisease detection\n\
    [111]\nBanana\nSatellite/UAV\nSatellite/UAV\nCustom model\nDisease detection\n\
    [143]\n3.5. Data Fusion Challenges for Agriculture\nData fusion is only worthwhile\
    \ if it increases the quality of predictions and relevance\nof decisions based\
    \ on the data combination. These data are likely to be insigniﬁcant,\nnoisy or\
    \ ﬂawed [113]. If the algorithm decisions are of poor quality, they may have a\n\
    negative impact on the expected results. Therefore, it is imperative to reduce\
    \ this noise and\neliminate these errors to improve the accuracy [10]. In addition\
    \ to noise, observed data\nmay be characterized by non-commensurability, different\
    \ resolutions and incompatible\nsizes or alignment, and consideration should be\
    \ given to exploit a pre-processing model to\nsolve this problem [79]. Furthermore,\
    \ different data sources may provide contradictory\ndata or missing values. A\
    \ data analysis step is therefore required. Once data are ready\nfor the learning\
    \ process, unbalanced data, which are basically unequal representation,\ncan also\
    \ affect the prediction rate. Thus, the biggest constraint of data fusion is the\n\
    multimodality with data from distinct types of sensors, different fusion architectures\
    \ can\nbe adopted [133,136]. However, the exploitation of these advanced models\
    \ in agriculture,\nprecisely in the disease detection area, is still budding.\n\
    Remote Sens. 2021, 13, 2486\n17 of 24\n4. Discussion and Conclusions\nThis review\
    \ enabled to map the various research works of disease detection using\nmachine\
    \ learning with different data modalities. Spectral imaging can be an essential\
    \ tool\nto assess crop health status. This is due to the different reﬂectance\
    \ of healthy and diseased\ncrops. Therefore, researchers exploited plant leaves\
    \ images using machine learning and\ndeep learning techniques for automatic disease\
    \ detection which has led to interesting\naccuracies. Multispectral and hyperspectral\
    \ images were very useful and provided higher\nprecision for disease detection.\
    \ That is tied to the spectral measurements sensitivity to\nstress and change\
    \ at different stages of crop growth and disease severity. Nevertheless,\nhyperspectral\
    \ data acquisition protocol is difﬁcult to apply in ﬁeld conditions. Moreover,\n\
    the spectral reﬂectance can also be inﬂuenced by several factors, such as technical\
    \ properties\n(resolution, brightness, etc.), sample preparation conditions (laboratory\
    \ or ﬁeld) and sample\nproperties (size, texture, humidity, etc.). Further analysis\
    \ of reﬂectance based on crop\nvegetation indices, during crop growth at all stages\
    \ of infection, is required. In addition\nto RGB and hyperspectral images, thermal\
    \ images have proven to be very useful in the\ndetection of plant diseases. The\
    \ main motivation is the fact that plant leaf temperature\ncan help predict plant\
    \ health status. Several researchers have explored this type of images\nfor disease\
    \ detection approaches at the leaf level, and others have combined these images\n\
    with multispectral data for effective early detection at the ground vehicle and\
    \ aerial vehicle\nlevel since plant leaves acquisition requires involving people\
    \ to drill down the whole\nﬁeld to acquire images, which is an energy and time-consuming\
    \ strategy. Indeed, the\nplant disease detection issue has strongly beneﬁted from\
    \ the aerial vehicles for the crop\nmonitoring process at plot scale. Several\
    \ types of cameras were used for that purpose and\nmounted on a drone. Based on\
    \ the acquired images, combined with machine learning\nmodels, researchers were\
    \ able to efﬁciently determine healthy and infected crops.\nSpectral imaging using\
    \ UAV provides important information on soil and the upper\npart of plants in\
    \ a large spectrum, therefore, UAVs are used more often. Nevertheless,\nthe difﬁculty\
    \ arises in evaluating the state of fruits in plants and lower to plant leaves.\n\
    Merging the two technologies can also broaden the spectrum of plants to be processed\
    \ while\nensuring early detection accuracy. However, UAVs suffer from environmental\
    \ and logistic\nconstraints such as high-speed wind and rain, battery capacity\
    \ and a fundamental need for\na trained person to start and manage ﬂights. Satellites\
    \ can be an excellent alternative to\nUAVs to monitor healthy plant growth depending\
    \ on the spatial and spectral resolution. A\npromising new ﬁeld in the detection\
    \ of plant disease on a larger scale is UAVs and satellites\nimaging which has\
    \ proved its usefulness in many agriculture applications.\nDespite the usefulness\
    \ of satellite images, this area of research faces several challenges.\nClouds\
    \ and their shadows represent a major obstacle when it comes to processing and\n\
    extracting disease signature from high-resolution satellite images; when clouds\
    \ cover\nvegetation, the acquired images become unexploitable. The main obstacles\
    \ to the crop\nmonitoring application and disease detection using satellites are\
    \ rapid changes in agricul-\ntural land cover in relatively short time intervals,\
    \ differences in seeding dates, atmospheric\nconditions and fertilization strategies;\
    \ since it is difﬁcult to predict whether the reﬂectance\nchanges are due to disease\
    \ or to those factors, an in-situ study is required to validate\npredictions.\
    \ High-resolution satellite images can be a key approach for very large-scale\n\
    disease detection.\nThe current technologies of imaging sensors have many limitations\
    \ for earlier disease\ndetection. The association of multiple sensors data can\
    \ provide a better understanding of\nthe growth and health status of the crop\
    \ and thus better prediction rates. This explains a\ngrowing interest in the scientiﬁc\
    \ community for the multimodal data fusion in the ﬁeld of\ncrop disease detection.\
    \ The most known meteorological sensors used for disease detection\nare temperature\
    \ [43], humidity [146], soil moisture and light intensity sensors [147]. One\n\
    can beneﬁt from the power of AI algorithms to process the multimodal data sources\
    \ and\npredict crop diseases in earlier stages.\nRemote Sens. 2021, 13, 2486\n\
    18 of 24\nIndeed, neural networks and deep neural network models demonstrated\
    \ a signiﬁcant\ncapacity in the agriculture ﬁeld to monitor the healthy crop growth\
    \ and capture anomalies\noutperforming traditional machine learning algorithms.\
    \ An example of the high perfor-\nmance of DL compared to conventional method\
    \ can be seen in [140]. The authors compared\nthe classiﬁcation results of SVM\
    \ with FCN (Fully Convolutional Network) and SegNet on\nmultispectral images.\
    \ SegNet and FCN outperformed SVM model in both experimental\nﬁelds and with different\
    \ combinations of image bands as shown in Table 7; where RGBMS\nand NIRMS are\
    \ respectively visual and near-infrared (NIR) bands of multispectral image,\n\
    FRGBMS and FNIRMS with high resolution are respectively fusion results of RGBMS\
    \ and\nNIRMS. In addition, from the table we can clearly see the impact of image\
    \ fusion on the\nrecognition results and the accuracies improved for all the models,\
    \ including the traditional\nmachine learning model.\nTable 7. Comparison of machine\
    \ learning, deep learning and image fusion performances. Adapted\nwith permission\
    \ from ref [140]. Copyright 2020 College of Mechanical and Electronic Engineering,\n\
    Northwest A&F University.\nUAV Images\nSVM\nFCN\nSegNet\nField 1\nField 2\nField\
    \ 1\nField 2\nField 1\nField 2\noriginal\nRGB (3 bands)\n66.8%\n55.3%\n83.8%\n\
    72.2%\n84.9%\n73.1%\nRGBMS + NIRMS (6 bands)\n68.2%\n56.0%\n82.4%\n72.5%\n83.7%\n\
    72.8%\nFusion\nFRGBMS + FNIRMS (6 bands)\n70.9%\n56.9%\n85.1%\n76.2%\n86.5%\n\
    76.8%\nRGBMS + FNIRMS (6 bands)\n69.0%\n57.7%\n86.7%\n76.4%\n87.1%\n78.2%\nThus,\
    \ the correct diagnosis depends on the choice of DL architecture and the type,\n\
    quantity and the quality of the data. Hence the fusion of all different important\
    \ components\ncan lead to an efﬁcient disease detection system. The application\
    \ of multimodal deep\nlearning involves the selection of a learning architecture\
    \ and algorithm. Lately, multimodal\nfusion has proven an inescapable potential\
    \ and is increasingly used in several domains such\nas healthcare, sentiment analysis,\
    \ human–robot interaction, human activity recognition or\nobject detection. In\
    \ the agriculture ﬁeld, several deep learning fusion approaches have been\nproposed,\
    \ such as applications in yield prediction, land monitoring, crop identiﬁcation\
    \ and\ndisease detection.\nThe most widely used type of fusion in agriculture\
    \ is the fusion of multi-sensors data\nfrom aerial vehicles, fusion of multi-resolution\
    \ satellites data and the fusion of satellite\nand UAV images. This fusion is\
    \ employed to improve the detection process for tasks such\nas crop monitoring\
    \ or plant classiﬁcation. Thus, for our speciﬁc task, the use of other\ndata sources\
    \ can enhance early disease detection performance. However, few multimodal\nfusion\
    \ studies have been conducted, particularly for disease detection. Promising results\n\
    of multimodal fusion were presented in this paper, demonstrating the high potential\
    \ of\ndeep learning fusion models for prediction when using multimodal data, which\
    \ creates an\nopportunity for further research works.\nAuthor Contributions: Writing—original\
    \ draft preparation, M.O. and A.H.; writing—review and\nediting, M.O., A.H., R.C.,\
    \ Y.E.-S. and M.E.H. All authors have read and agreed to the published\nversion\
    \ of the manuscript.\nFunding: This research was funded by Campus France, Eiffel\
    \ Scholarship Program of Excellence\n2020 grant number 96XXXF, and the PHC Toubkal/21/121.\n\
    Conﬂicts of Interest: The authors declare no conﬂict of interest.\nRemote Sens.\
    \ 2021, 13, 2486\n19 of 24\nReferences\n1.\nFAO; WHO. The Second Global Meeting\
    \ of the FAO/WHO International Food Safety Authorities Network; World Health Organization:\n\
    Geneva, Switzerland, 2019.\n2.\nVenkateswarlu, B.; Shanker, A.K.; Shanker, C.;\
    \ Maheswari, M. Crop Stress and Its Management: Perspectives and Strategies; Springer:\n\
    Dordrecht, Germany, 2013; pp. 1–18.\n3.\nJullien, P.; Alexandra, H. Agriculture\
    \ de precision. In Agricultures et Territoires; Éditions L’Harmattan: Paris, France,\
    \ 2005;\npp. 1–15.\n4.\nLamichhane, J.R.; Dachbrodt-Saaydeh, S.; Kudsk, P.; Messéan,\
    \ A. Toward a reduced reliance on conventional pesticides in\neuropean agriculture.\
    \ Plant Dis. 2016, 100, 10–24. [CrossRef]\n5.\nSánchez-Bayo, F.; Baskaran, S.;\
    \ Kennedy, I.R. Ecological relative risk (EcoRR): Another approach for risk assessment\
    \ of pesticides\nin agriculture. Agric. Ecosyst. Environ. 2002, 91, 37–57. [CrossRef]\n\
    6.\nRochon, D.A.; Kakani, K.; Robbins, M.; Reade, R. Molecular aspects of plant\
    \ virus transmission by olpidium and plasmodiophorid\nvectors. Annu. Rev. Phytopathol.\
    \ 2014, 42, 211–241. [CrossRef] [PubMed]\n7.\nLary, D.J.; Alavi, A.H.; Gandomi,\
    \ A.H.; Walker, A.L. Machine learning in geosciences and remote sensing. Geosci.\
    \ Front. 2016, 7,\n3–10. [CrossRef]\n8.\nGolhani, K.; Balasundram, S.K.; Vadamalai,\
    \ G.; Pradhan, B. A review of neural networks in plant disease detection using\n\
    hyperspectral data. Inf. Process. Agric. 2018, 5, 354–371. [CrossRef]\n9.\nZhang,\
    \ J.; Huang, Y.; Pu, R.; Gonzalez-Moreno, P.; Yuan, L.; Wu, K.; Huang, W. Monitoring\
    \ plant diseases and pests through\nremote sensing technology: A review. Comput.\
    \ Electron. Agric. 2019, 165, 104943. [CrossRef]\n10.\nLi, W.; Wang, Z.; Wei,\
    \ G.; Ma, L.; Hu, J.; Ding, D. A Survey on multisensor fusion and consensus ﬁltering\
    \ for sensor networks.\nDiscret. Dyn. Nat. Soc. 2015, 2015, 1–12. [CrossRef]\n\
    11.\nLiao, W.; Chanussot, J.; Philips, W. Remote sensing data fusion: Guided ﬁlter-based\
    \ hyperspectral pansharpening and graph-\nbased feature-level fusion. In Mathematical\
    \ Models for Remote Sensing Image Processing; Moser, G., Zerubia, J., Eds.; Springer:\n\
    Berlin/Heidelberg, Germany, 2017; pp. 243–275.\n12.\nTalavera, J.M.; Tobón, L.E.;\
    \ Gómez, J.A.; Culman, M.A.; Aranda, J.; Parra, D.T.; Quiroz, L.A.; Hoyos, A.;\
    \ Garreta, L.E. Review of\nIoT applications in agro-industrial and environmental\
    \ ﬁelds. Comput. Electron. Agric. 2017, 142, 283–297. [CrossRef]\n13.\nAune, J.B.;\
    \ Coulibaly, A.; Giller, K.E. Precision farming for increased land and labour\
    \ productivity in semi-arid West Africa. A\nreview. Agron. Sustain. Dev. 2017,\
    \ 37, 16. [CrossRef]\n14.\nBacco, M.; Berton, A.; Ferro, E.; Gennaro, C.; Gotta,\
    \ A.; Matteoli, S.; Paonessa, F.; Ruggeri, M.; Virone, G.; Zanella, A. Smart\n\
    farming: Opportunities, challenges and technology enablers. In Proceedings of\
    \ the 2018 IoT Vertical and Topical Summit on\nAgriculture—Tuscany (IOT Tuscany),\
    \ Tuscan, Italy, 8–9 May 2018; pp. 1–6.\n15.\nShi, X.; An, X.; Zhao, Q.; Liu,\
    \ H.; Xia, L.; Sun, X.; Guo, Y. State-of-the-art internet of things in protected\
    \ agriculture. Sensors 2019,\n19, 1833. [CrossRef]\n16.\nKochhar, A.; Kumar, N.\
    \ Wireless sensor networks for greenhouses: An end-to-end review. Comput. Electron.\
    \ Agric. 2019,\n163, 104877. [CrossRef]\n17.\nvan Klompenburg, T.; Kassahun, A.;\
    \ Catal, C. Crop yield prediction using machine learning: A systematic literature\
    \ review.\nComput. Electron. Agric. 2020, 177, 105709. [CrossRef]\n18.\nLowe,\
    \ A.; Harrison, N.; French, A.P. Hyperspectral image analysis techniques for the\
    \ detection and classiﬁcation of the early\nonset of plant disease and stress.\
    \ Plant Methods 2017, 13, 1–12. [CrossRef]\n19.\nMishra, P.; Asaari, M.S.M.; Herrero-Langreo,\
    \ A.; Lohumi, S.; Diezma, B.; Scheunders, P. Close range hyperspectral imaging\
    \ of\nplants: A review. Biosyst. Eng. 2017, 164, 49–67. [CrossRef]\n20.\nAdão,\
    \ T.; Hruška, J.; Pádua, L.; Bessa, J.; Peres, E.; Morais, R.; Sousa, J.J. Hyperspectral\
    \ Imaging: A review on uav-based sensors,\ndata processing and applications for\
    \ agriculture and forestry. Remote Sens. 2017, 9, 1110. [CrossRef]\n21.\nBasnet,\
    \ B.; Bang, J. The state-of-the-art of knowledge-intensive agriculture: A review\
    \ on applied sensing systems and data\nanalytics. J. Sensors 2018, 2018, 1–13.\
    \ [CrossRef]\n22.\nMaggiori, E.; Plaza, A.; Tarabalka, Y. Models for hyperspectral\
    \ image analysis: From unmixing to object-based classiﬁcation. In\nMathematical\
    \ Models for Remote Sensing Image Processing; Moser, G., Zerubia, J., Eds.; Springer:\
    \ Cham, Switzerland, 2017; pp. 37–80.\n23.\nKamilaris, A.; Prenafeta-Boldú, F.X.\
    \ Deep learning in agriculture: A survey. Comput. Electron. Agric. 2018, 147,\
    \ 70–90. [CrossRef]\n24.\nMukherjee, A.; Misra, S.; Raghuwanshi, N.S. A survey\
    \ of unmanned aerial sensing solutions in precision agriculture. J. Netw.\nComput.\
    \ Appl. 2019, 148, 102461. [CrossRef]\n25.\nBarbedo, J.G.A. Detection of nutrition\
    \ deﬁciencies in plants using proximal images and machine learning: A review.\
    \ Comput.\nElectron. Agric. 2019, 162, 482–492. [CrossRef]\n26.\nBarbedo, J. A\
    \ Review on the use of unmanned aerial vehicles and imaging sensors for monitoring\
    \ and assessing plant stresses.\nDrones 2019, 3, 40. [CrossRef]\n27.\nSishodia,\
    \ R.P.; Ray, R.L.; Singh, S.K. Applications of remote sensing in precision agriculture:\
    \ A review. Remote Sens. 2020, 12, 3136.\n[CrossRef]\n28.\nZhang, C.; Marzougui,\
    \ A.; Sankaran, S. High-resolution satellite imagery applications in crop phenotyping:\
    \ An over-view.\nComput. Electron. Agric. 2020, 175, 105584. [CrossRef]\nRemote\
    \ Sens. 2021, 13, 2486\n20 of 24\n29.\nRadoˇcaj, D.; Obho ¯daš, J.; Juriši´c,\
    \ M.; Gašparovi´c, M. Global open data remote sensing satellite missions for land\
    \ monitoring and\nconservation: A review. Land 2020, 9, 402. [CrossRef]\n30.\n\
    Khanal, S.; Kc, K.; Fulton, J.; Shearer, S.; Ozkan, E. Remote sensing in agriculture—Accomplishments,\
    \ limitations, and opportuni-\nties. Remote Sens. 2020, 12, 3783. [CrossRef]\n\
    31.\nSingh, V.; Sharma, N.; Singh, S. A review of imaging techniques for plant\
    \ disease detection. Artif. Intell. Agric. 2020, 4, 229–242.\n[CrossRef]\n32.\n\
    Liu, H.; Bruning, B.; Garnett, T.; Berger, B. Hyperspectral imaging and 3D technologies\
    \ for plant phenotyping: From satellite to\nclose-range sensing. Comput. Electron.\
    \ Agric. 2020, 175, 105621. [CrossRef]\n33.\nHasan, R.I.; Yusuf, S.M.; Alzubaidi,\
    \ L. Review of the state of the art of deep learning for plant diseases: A broad\
    \ analysis and\ndiscussion. Plants 2020, 9, 1302. [CrossRef]\n34.\nMessina, G.;\
    \ Modica, G. Applications of UAV thermal imagery in precision agriculture: State\
    \ of the art and future research\noutlook. Remote Sens. 2020, 12, 1491. [CrossRef]\n\
    35.\nYang, C. remote sensing and precision agriculture technologies for crop disease\
    \ detection and management with a practical\napplication example. Engineering\
    \ 2020, 6, 528–532. [CrossRef]\n36.\nGhamisi, P.; Gloaguen, R.; Atkinson, P.M.;\
    \ Benediktsson, J.A.; Rasti, B.; Yokoya, N.; Wang, Q.; Hoﬂe, B.; Bruzzone, L.;\
    \ Bovolo, F.;\net al. Multisource and multitemporal data fusion in remote sensing:\
    \ A comprehensive review of the state of the art. IEEE Geosci.\nRemote Sens. Mag.\
    \ 2019, 7, 6–39. [CrossRef]\n37.\nDing, W.; Jing, X.; Yan, Z.; Yang, L.T. A survey\
    \ on data fusion in internet of things: Towards secure and privacy-preserving\
    \ fusion.\nInf. Fusion 2019, 51, 129–144. [CrossRef]\n38.\nVisconti, P.; de Fazio,\
    \ R.; Velázquez, R.; Del-Valle-Soto, C.; Giannoccaro, N.I. Multilevel data fusion\
    \ for the internet of things in\nsmart agriculture. Comput. Electron. Agric. 2020,\
    \ 171, 105309.\n39.\nPantazi, X.E.; Moshou, D.; Bochtis, D. Intelligent Data Mining\
    \ and Fusion Systems in Agriculture; Academis Press: Cambridge, MA,\nUSA, 2020.\n\
    40.\nBogue, R. Sensors key to advances in precision agriculture. Sens. Rev. 2017,\
    \ 37, 1–6. [CrossRef]\n41.\nJin, X.; Jie, L.; Wang, S.; Qi, H.J.; Li, S.W. Classifying\
    \ wheat hyperspectral pixels of healthy heads and fusarium head blight disease\n\
    using a deep neural network in the wild Field. Remote Sens. 2018, 10, 395. [CrossRef]\n\
    42.\nPicon, A.; Seitz, M.; Alvarez-Gila, A.; Mohnke, P.; Ortiz-Barredo, A.; Echazarra,\
    \ J. Crop conditional Convolutional neural networks\nfor massive multi-crop plant\
    \ disease classiﬁcation over cell phone acquired images taken on real ﬁeld conditions.\
    \ Comput. Electron.\nAgric. 2019, 167, 105093. [CrossRef]\n43.\nPark, D.-H.; Kang,\
    \ B.-J.; Cho, K.-R.; Shin, C.-S.; Cho, S.-E.; Park, J.-W.; Yang, W.-M. A Study\
    \ on greenhouse automatic control\nsystem based on wireless sensor network. Wirel.\
    \ Pers. Commun. 2009, 56, 117–130. [CrossRef]\n44.\nBajwa, S.G.; Rupe, J.C.; Mason,\
    \ J. Soybean Disease monitoring with leaf reﬂectance. Remote Sens. 2017, 9, 127.\
    \ [CrossRef]\n45.\nBehmann, J.; Steinrücken, J.; Plümer, L. Detection of early\
    \ plant stress responses in hyperspectral images. ISPRS J. Photogramm.\nRemote\
    \ Sens. 2014, 93, 98–111. [CrossRef]\n46.\nEs-Saady, Y.; El Massi, I.; El Yassa,\
    \ M.; Mammass, D.; Benazoun, A. Automatic recognition of plant leaves diseases\
    \ based on\nserial combination of two SVM classiﬁers. In Proceedings of the 2016\
    \ International Conference on Electrical and Information\nTechnologies (ICEIT),\
    \ Tangiers, Morocco, 4–7 May 2016; pp. 561–566.\n47.\nEl Massi, I.; Es-Saady,\
    \ Y.; El Yassa, M.; Mammass, D.; Benazoun, A. Automatic recognition of the damages\
    \ and symptoms on plant\nleaves using parallel combination of two classiﬁers.\
    \ In Proceedings of the 13th Computer Graphics, Imaging and Visualization\n(CGiV\
    \ 2016), Beni Mellal, Morocco, 29 March–1 April 2016; pp. 131–136.\n48.\nPrajapati,\
    \ H.B.; Shah, J.P.; Dabhi, V.K. Detection and classiﬁcation of rice plant diseases.\
    \ Intell. Decis. Technol. 2017, 11, 357–373.\n[CrossRef]\n49.\nEl Massi, I.; Es-Saady,\
    \ Y.; El Yassa, M.; Mammass, D. Combination of multiple classiﬁers for automatic\
    \ recognition of diseases and\ndamages on plant leaves. Signal Image Video Process.\
    \ 2021, 15, 789–796. [CrossRef]\n50.\nAtherton, D.; Choudhary, R.; Watson, D.\
    \ Hyperspectral remote sensing for advanced detection of early blight (Alternaria\
    \ solani)\ndisease in potato (Solanum tuberosum) plants prior to visual disease\
    \ symptoms. In Proceedings of the 2017 ASABE Annual\nInternational Meeting, Washington,\
    \ DC, USA, 16–19 July 2017; pp. 1–10.\n51.\nXie, C.; Yang, C.; He, Y. Hyperspectral\
    \ imaging for classiﬁcation of healthy and gray mold diseased tomato leaves with\
    \ different\ninfection severities. Comput. Electron. Agric. 2017, 135, 154–162.\
    \ [CrossRef]\n52.\nBebronne, R.; Carlier, A.; Meurs, R.; Leemans, V.; Vermeulen,\
    \ P.; Dumont, B.; Mercatoris, B. In-ﬁeld proximal sensing of septoria\ntritici\
    \ blotch, stripe rust and brown rust in winter wheat by means of reﬂectance and\
    \ textural features from multispectral imagery.\nBiosyst. Eng. 2020, 197, 257–269.\
    \ [CrossRef]\n53.\nBrahimi, M.; Arsenovic, M.; Laraba, S.; Sladojevic, S.; Boukhalfa,\
    \ K.; Moussaoui, A. Deep Learning for Plant Diseases: Detection\nand Saliency\
    \ Map Visualisation. In Human and Machine Learning, Human–Computer Interaction\
    \ Series; Springer: Cham, Switzerland,\n2018; pp. 93–117.\n54.\nHughes, D.; Salathé,\
    \ M. An open access repository of images on plant health to enable the development\
    \ of mobile disease\ndiagnostics. arXiv 2015, arXiv:1511.08060v2.\n55.\nAtila,\
    \ Ü.; Uçar, M.; Akyol, K.; Uçar, E. Plant leaf disease classiﬁcation using EfﬁcientNet\
    \ deep learning model. Ecol. Inform. 2021,\n61, 101182. [CrossRef]\nRemote Sens.\
    \ 2021, 13, 2486\n21 of 24\n56.\nOuhami, M.; Es-Saady, Y.; El Hajji, M.; Haﬁane,\
    \ A.; Canals, R.; El Yassa, M. Deep transfer learning models for tomato disease\n\
    detection. Image Signal Process ICISP 2020, 12119, 65–73. [CrossRef]\n57.\nElhassouny,\
    \ A.; Smarandache, F. Smart mobile application to recognize tomato leaf diseases\
    \ using Convolutional Neural\nNetworks. In Proceedings of the 2019 International\
    \ Conference of Computer Science and Renewable Energies (ICCSRE), Agadir,\nMorocco,\
    \ 22–24 July 2019; pp. 1–4.\n58.\nBi, C.; Wang, J.; Duan, Y.; Fu, B.; Kang, J.-R.;\
    \ Shi, Y. MobileNet based apple leaf diseases identiﬁcation. Mob. Netw. Appl.\
    \ 2020, 1–9.\n[CrossRef]\n59.\nBarman, U.; Choudhury, R.D.; Sahu, D.; Barman,\
    \ G.G. Comparison of convolution neural networks for smartphone image based\n\
    real time classiﬁcation of citrus leaf disease. Comput. Electron. Agric. 2020,\
    \ 177, 105661. [CrossRef]\n60.\nXie, C.; Shao, Y.; Li, X.; He, Y. Detection of\
    \ early blight and late blight diseases on tomato leaves using hyperspectral imaging.\
    \ Sci.\nRep. 2015, 5, 16564. [CrossRef]\n61.\nZhu, H.; Chu, B.; Zhang, C.; Liu,\
    \ F.; Jiang, L.; He, Y. Hyperspectral imaging for presymptomatic detection of\
    \ tobacco disease with\nsuccessive projections algorithm and machine-learning\
    \ classiﬁers. Sci. Rep. 2017, 7, 1–12. [CrossRef] [PubMed]\n62.\nWang, X.; Zhang,\
    \ M.; Zhu, J.; Geng, S. Spectral prediction of Phytophthora infestans infection\
    \ on tomatoes using artiﬁcial neural\nnetwork (ANN). Int. J. Remote Sens. 2008,\
    \ 29, 1693–1706. [CrossRef]\n63.\nMaimaitijiang, M.; Sagan, V.; Sidike, P.; Hartling,\
    \ S.; Esposito, F.; Fritschi, F.B. Soybean yield prediction from UAV using\nmultimodal\
    \ data fusion and deep learning. Remote Sens. Environ. 2020, 237, 111599. [CrossRef]\n\
    64.\nMilioto, A.; Lottes, P.; Stachniss, C. Real-time blob-wise sugar beets vs\
    \ weeds classiﬁcation for monitoring ﬁelds using convolu-\ntional neural networks.\
    \ ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, 4, 41–48. [CrossRef]\n\
    65.\nBah, M.D.; Haﬁane, A.; Canals, R. Weeds detection in UAV imagery using SLIC\
    \ and the hough transform. In Proceedings of\nthe 2017 Seventh International Conference\
    \ on Image Processing Theory, Tools and Applications (IPTA), Montreal, QC, Canada,\n\
    28 November–1 December 2017; pp. 1–6.\n66.\nMacDonald, S.L.; Staid, M.; Staid,\
    \ M.; Cooper, M.L. Remote hyperspectral imaging of grapevine leafroll-associated\
    \ virus 3 in\ncabernet sauvignon vineyards. Comput. Electron. Agric. 2016, 130,\
    \ 109–117. [CrossRef]\n67.\nAlbetis, J.; Duthoit, S.; Guttler, F.; Jacquin, A.;\
    \ Goulard, M.; Poilvé, H.; Féret, J.-B.; Dedieu, G. Detection of Flavescence dorée\n\
    grapevine disease using unmanned aerial vehicle (uav) multispectral imagery. Remote\
    \ Sens. 2017, 9, 308. [CrossRef]\n68.\nTetila, E.C.; Machado, B.B.; Belete, N.A.D.S.;\
    \ Guimaraes, D.A.; Pistori, H. Identiﬁcation of soybean foliar diseases using\
    \ unmanned\naerial vehicle images. IEEE Geosci. Remote Sens. Lett. 2017, 14, 2190–2194.\
    \ [CrossRef]\n69.\nSu, J.; Liu, C.; Coombes, M.; Hu, X.; Wang, C.; Xu, X.; Li,\
    \ Q.; Guo, L.; Chen, W.-H. Wheat yellow rust monitoring by learning from\nmultispectral\
    \ UAV aerial imagery. Comput. Electron. Agric. 2018, 155, 157–166. [CrossRef]\n\
    70.\nAbdulridha, J.; Batuman, O.; Ampatzidis, Y. UAV-based remote sensing technique\
    \ to detect citrus canker disease uti-lizing\nhyperspectral imaging and machine\
    \ learning. Remote Sens. 2019, 11, 1373. [CrossRef]\n71.\nLan, Y.; Huang, Z.;\
    \ Deng, X.; Zhu, Z.; Huang, H.; Zheng, Z.; Lian, B.; Zeng, G.; Tong, Z. Comparison\
    \ of machine learning methods\nfor citrus greening detection on UAV multispectral\
    \ images. Comput. Electron. Agric. 2020, 171, 105234. [CrossRef]\n72.\nAbdulridha,\
    \ J.; Ampatzidis, Y.; Roberts, P.; Kakarla, S.C. Detecting powdery mildew disease\
    \ in squash at different stages using\nUAV-based hyperspectral imaging and artiﬁcial\
    \ intelligence. Biosyst. Eng. 2020, 197, 135–148. [CrossRef]\n73.\nPoblete, T.;\
    \ Camino, C.; Beck, P.S.A.; Hornero, A.; Kattenborn, T.; Saponari, M.; Boscia,\
    \ D.; Navas-Cortes, J.A.; Zarco-Tejada, P.J.\nDetection of Xylella fastidiosa\
    \ infection symptoms with airborne multispectral and thermal imagery: Assessing\
    \ bandset reduction\nperformance from hyperspectral analysis. ISPRS J. Photogramm.\
    \ Remote Sens. 2020, 162, 27–40. [CrossRef]\n74.\nDuarte-Carvajalino, J.M.; Alzate,\
    \ D.F.; Ramirez, A.A.; Santa-Sepulveda, J.D.; Fajardo-Rojas, A.E.; Soto-Suárez,\
    \ M. Evaluating late\nblight severity in potato crops using unmanned aerial vehicles\
    \ and machine learning algorithms. Remote Sens. 2018, 10, 1513.\n[CrossRef]\n\
    75.\nWu, H.; Wiesner-Hanks, T.; Stewart, E.L.; DeChant, C.; Kaczmar, N.; Gore,\
    \ M.A.; Nelson, R.J.; Lipson, H. Autonomous detection\nof plant disease symptoms\
    \ directly from aerial imagery. Plant Phenome J. 2019, 2, 1–9. [CrossRef]\n76.\n\
    Kerkech, M.; Haﬁane, A.; Canals, R. Deep leaning approach with colorimetric spaces\
    \ and vegetation indices for vine diseases\ndetection in UAV images. Comput. Electron.\
    \ Agric. 2018, 155, 237–243. [CrossRef]\n77.\nZhang, X.; Han, L.; Dong, Y.; Shi,\
    \ Y.; Huang, W.; Han, L.; González-Moreno, P.; Ma, H.; Ye, H.; Sobeih, T. A deep\
    \ learning-based\napproach for automated yellow rust disease detection from high-resolution\
    \ hyper-spectral UAV images. Remote Sens. 2019,\n13, 1554.\n78.\nHu, G.; Yin,\
    \ C.; Wan, M.; Zhang, Y.; Fang, Y. Recognition of diseased Pinus trees in UAV\
    \ images using deep learning and AdaBoost\nclassiﬁer. Biosyst. Eng. 2020, 194,\
    \ 138–151. [CrossRef]\n79.\nKerkech, M.; Haﬁane, A.; Canals, R. Vine disease detection\
    \ in UAV multispectral images using optimized image registration and\ndeep learning\
    \ segmentation approach. Comput. Electron. Agric. 2020, 174, 105446. [CrossRef]\n\
    80.\nKerkech, M.; Haﬁane, A.; Canals, R. VddNet: Vine disease detection network\
    \ based on multispectral images and depth map.\nRemote Sens. 2020, 12, 3305. [CrossRef]\n\
    81.\nSantoso, H.; Gunawan, T.; Jatmiko, R.H.; Darmosarkoro, W.; Minasny, B. Mapping\
    \ and identifying basal stem rot disease in oil\npalms in North Sumatra with QuickBird\
    \ imagery. Precis. Agric. 2011, 12, 233–248. [CrossRef]\n82.\nZhu, X.; Cai, F.;\
    \ Tian, J.; Williams, T.K.A. Spatiotemporal fusion of multisource remote sensing\
    \ data: Literature survey, taxonomy,\nprinciples, applications, and future directions.\
    \ Remote Sens. 2018, 10, 527. [CrossRef]\nRemote Sens. 2021, 13, 2486\n22 of 24\n\
    83.\nShao, Z.; Cai, J.; Fu, P.; Hu, L.; Liu, T. Deep learning-based fusion of\
    \ Landsat-8 and Sentinel-2 images for a harmonized surface\nreﬂectance product.\
    \ Remote Sens. Environ. 2019, 235, 111425. [CrossRef]\n84.\nMaggiori, E.; Tarabalka,\
    \ Y.; Charpiat, G.; Alliez, P. Convolutional neural networks for large-scale remote-sensing\
    \ image classiﬁca-\ntion. IEEE Trans. Geosci. Remote Sens. 2017, 55, 645–657.\
    \ [CrossRef]\n85.\nEl Mendili, L.; Puissant, A.; Chougrad, M.; Sebari, I. Towards\
    \ a multi-temporal deep learning approach for mapping urban fabric\nusing sentinel\
    \ 2 images. Remote Sens. 2020, 12, 423. [CrossRef]\n86.\nWang, Y.; Gu, L.; Li,\
    \ X.; Ren, R. building extraction in multitemporal high-resolution remote sensing\
    \ imagery using a multifeature\nlstm network. IEEE Geosci. Remote Sens. Lett.\
    \ 2020, 1–5. [CrossRef]\n87.\nWaldner, F.; Diakogiannis, F.I. Deep learning on\
    \ edge: Extracting ﬁeld boundaries from satellite images with a convolutional\n\
    neural network. Remote Sens. Environ. 2020, 245, 111741. [CrossRef]\n88.\nKarim,\
    \ Z.; Van Zyl, T. Deep Learning and Transfer Learning applied to Sentinel-1 DInSAR\
    \ and Sentinel-2 optical satellite imagery\nfor change detection. In Proceedings\
    \ of the 2020 International SAUPEC/RobMech/PRASA Conference 2020, Cape Town, South\n\
    Africa, 29–31 January 2020; pp. 1–7.\n89.\nDonovan, S.D.; MacLean, D.A.; Zhang,\
    \ Y.; Lavigne, M.B.; Kershaw, J.A. Evaluating annual spruce budworm defoliation\
    \ using\nchange detection of vegetation indices calculated from satellite hyperspectral\
    \ imagery. Remote Sens. Environ. 2020, 253, 112204.\n[CrossRef]\n90.\nYuan, L.;\
    \ Pu, R.; Zhang, J.; Wang, J.; Yang, H. Using high spatial resolution satellite\
    \ imagery for mapping powdery mildew at a\nregional scale. Precis. Agric. 2016,\
    \ 17, 332–348. [CrossRef]\n91.\nLiu, M.; Wang, T.; Skidmore, A.K.; Liu, X. Heavy\
    \ metal-induced stress in rice crops detected using multi-temporal Sentinel-2\n\
    satellite images. Sci. Total Environ. 2018, 637, 18–29. [CrossRef]\n92.\nZheng,\
    \ Q.; Huang, W.; Cui, X.; Shi, Y.; Liu, L. New spectral index for detecting wheat\
    \ yellow rust using sentinel-2 multispectral\nimagery. Sensors 2018, 18, 4040.\
    \ [CrossRef]\n93.\nMa, H.; Huang, W.; Jing, Y.; Yang, C.; Han, L.; Dong, Y.; Ye,\
    \ H.; Shi, Y.; Zheng, Q.; Liu, L.; et al. Integrating growth and\nenvironmental\
    \ parameters to discriminate powdery mildew and aphid of winter wheat using bi-temporal\
    \ Landsat-8 imagery.\nRemote Sens. 2019, 11, 846. [CrossRef]\n94.\nMiranda, J.D.R.;\
    \ Alves, M.D.C.; Pozza, E.A.; Neto, H.S. Detection of coffee berry necrosis by\
    \ digital image processing of landsat 8\noli satellite imagery. Int. J. Appl.\
    \ Earth Obs. Geoinf. 2020, 85, 101983. [CrossRef]\n95.\nBi, L.; Hu, G.; Raza,\
    \ M.; Kandel, Y.; Leandro, L.; Mueller, D. A gated recurrent units (gru)-based\
    \ model for early detection of\nsoybean sudden death syndrome through time-series\
    \ satellite imagery. Remote Sens. 2020, 12, 3621. [CrossRef]\n96.\nPelletier,\
    \ C.; Webb, G.I.; Petitjean, F. Temporal convolutional neural network for the\
    \ classiﬁcation of satellite image time series.\nRemote Sens. 2019, 11, 523. [CrossRef]\n\
    97.\nYashodha, G.; Shalini, D. An integrated approach for predicting and broadcasting\
    \ tea leaf disease at early stage using IoT with\nmachine learning—A review. Mater.\
    \ Today Proc. 2021, 37, 484–488. [CrossRef]\n98.\nDíaz, S.E.; Pérez, J.C.; Mateos,\
    \ A.C.; Marinescu, M.C.; Guerra, B.B. A novel methodology for the monitoring of\
    \ the agricultural\nproduction process based on wireless sensor networks. Comput.\
    \ Electron. Agric. 2011, 76, 252–265. [CrossRef]\n99.\nOjha, T.; Misra, S.; Raghuwanshi,\
    \ N.S. Wireless sensor networks for agriculture: The state-of-the-art in practice\
    \ and future\nchallenges. Comput. Electron. Agric. 2015, 118, 66–84. [CrossRef]\n\
    100. Rodríguez, S.; Gualotuña, T.; Grilo, C. A System for the monitoring and predicting\
    \ of data in precision. Procedia Comput. Sci. 2017,\n121, 306–313. [CrossRef]\n\
    101. Tripathy, A.K.; Adinarayana, J.; Merchant, S.N.; Desai, U.B.; Ninomiya, S.;\
    \ Hirafuji, M.; Kiura, T. Data mining and wireless sensor\nnetwork for groundnut\
    \ pest/disease interaction and predictions—A preliminary study. Int. J. Comput.\
    \ Inf. Syst. Ind. Manag. Appl.\n2013, 5, 427–436.\n102. Khattab, A.; Habib, S.E.;\
    \ Ismail, H.; Zayan, S.; Fahmy, Y.; Khairy, M.M. An IoT-based cognitive monitoring\
    \ system for early plant\ndisease forecast. Comput. Electron. Agric. 2019, 166,\
    \ 105028. [CrossRef]\n103. Trilles, S.; Torres-Sospedra, J.; Belmonte, Ó.; Zarazaga-Soria,\
    \ F.J.; González-Pérez, A.; Huerta, J. Development of an open\nsensorized platform\
    \ in a smart agriculture context: A vineyard support system for monitoring mildew\
    \ disease. Sustain. Comput.\nInform. Syst. 2020, 28, 100309. [CrossRef]\n104.\
    \ Patil, S.S.; Thorat, S.A. Early detection of grapes diseases using machine learning\
    \ and IoT. In Proceedings of the 2016 Second\nInternational Conference on Cognitive\
    \ Computing and Information Processing (CCIP), Mysuru, India, 12–13 August 2016;\n\
    pp. 1–5.\n105. Wani, H.; Ashtankar, N. An appropriate model predicting pest/diseases\
    \ of crops using machine learning algorithms. In Proceed-\nings of the 2017 4th\
    \ International Conference on Advanced Computing and Communication Systems (ICACCS),\
    \ Coimbatore,\nIndia, 6–7 January 2017; pp. 1–4.\n106. Materne, N.; Inoue, M.\
    \ IoT monitoring system for early detection of agricultural pests and diseases.\
    \ In Proceedings of the 12th\nSouth East Asian Technical University Consortium\
    \ (SEATUC), Yogyakarta, Indonesia, 12–13 March 2018; pp. 1–5.\n107. Khan, S.;\
    \ Narvekar, M. Disorder detection of tomato plant (Solanum lycopersicum) using\
    \ IoT and machine learning. J. Physics. Conf.\nSer. 2020, 1432. [CrossRef]\n108.\
    \ Chen, P.; Xiao, Q.; Zhang, J.; Xie, C.; Wang, B. Occurrence prediction of cotton\
    \ pests and diseases by bidirectional long short-term\nmemory networks with climate\
    \ and atmosphere circulation. Comput. Electron. Agric. 2020, 176, 105612. [CrossRef]\n\
    Remote Sens. 2021, 13, 2486\n23 of 24\n109. Wiesner-Hanks, T.; Stewart, E.L.;\
    \ Kaczmar, N.; DeChant, C.; Wu, H.; Nelson, R.J.; Lipson, H.; Gore, M.A. Image\
    \ set for deep\nlearning: Field images of maize annotated with disease symptoms.\
    \ BMC Res. Notes 2018, 11, 440. [CrossRef] [PubMed]\n110. El Massi, I.; Es-saady,\
    \ Y.; El Yassa, M.; Mammass, D.; Benazoun, A. Hybrid combination of multiple svm\
    \ classiﬁers for automatic\nrecognition of the damages and symptoms on plant leaves.\
    \ In Image and Signal Processing, ICISP 2016; Lecture Notes in Computer\nScience;\
    \ Springer: Cham, Switzerland; Volume 9680. [CrossRef]\n111. Zhao, Y.; Liu, L.;\
    \ Xie, C.; Wang, R.; Wang, F.; Bu, Y.; Zhang, S. An effective automatic system\
    \ deployed in agricultural internet of\nthings using multi-context fusion network\
    \ towards crop disease recognition in the wild. Appl. Soft Comput. 2020, 89, 106128.\n\
    [CrossRef]\n112. Bellot, D. Fusion de Données avec des Réseaux Bayésiens pour\
    \ la Modélisation des Systèmes Dynamiques et son Application en\nTélémédecine.\
    \ Ph.D. Thesis, Université Henri Poincaré, Nancy, France, 2002.\n113. Lahat, D.;\
    \ Adali, T.; Jutten, C. Multimodal Data Fusion: An Overview of Methods, Challenges,\
    \ and Prospects. Proc. IEEE 2015,\n103, 1449–1477. [CrossRef]\n114. Meng, T.;\
    \ Jing, X.; Yan, Z.; Pedrycz, W. A survey on machine learning for data fusion.\
    \ Inf. Fusion 2020, 57, 115–129. [CrossRef]\n115. Mees, O.; Eitel, A.; Burgard,\
    \ W. Choosing smartly: Adaptive multimodal fusion for object detection in changing\
    \ environments. In\nProceedings of the IEEE/RSJ International Conference on Intelligent\
    \ Robots and Systems (IROS), Daejeon, Korea, 9–14 October\n2016; pp. 151–156.\n\
    116. Liggins, M., II; Hall, D.; Llinas, J. Handbook of Multisensor Data Fusion:\
    \ Theory and Practice; CRC Press: Boca Raton, FL, USA, 2009.\n117. Pavlin, G.;\
    \ de Oude, P.; Maris, M.; Nunnink, J.; Hood, T. A multi-agent systems approach\
    \ to distributed bayesian in-formation\nfusion. Inf. Fusion 2010, 11, 267–282.\
    \ [CrossRef]\n118. Albeiruti, N.; Al Begain, K. Using hidden markov models to\
    \ build behavioural models to detect the onset of dementia. In\nProceedings of\
    \ the 2014 Sixth International Conference on Computational Intelligence, Communication\
    \ Systems and Networks,\nTetovo, Macedonia, 27–29 May 2014; pp. 18–26.\n119. Smith,\
    \ D.; Singh, S. Approaches to multisensor data fusion in target tracking: A Survey.\
    \ IEEE Trans. Knowl. Data Eng. 2006, 18,\n1696–1710. [CrossRef]\n120. Wu, H.;\
    \ Siegel, M.; Stiefelhagen, R.; Yang, J. Sensor fusion using dempster-shafer theory.\
    \ In Proceedings of the IMTC/2002.\nProceedings of the 19th IEEE Instrumentation\
    \ and Measurement Technology Conference (IEEE Cat. No.00CH37276), Anchorage,\n\
    AK, USA, 21–23 May 2002; pp. 7–11.\n121. Awogbami, G.; Agana, N.; Nazmi, S.; Yan,\
    \ X.; Homaifar, A. An Evidence theory based multi sensor data fusion for multiclass\n\
    classiﬁcation. In Proceedings of the 2018 IEEE International Conference on Systems,\
    \ Man, and Cybernetics (SMC), Miyazaki,\nJapan, 7–10 October 2018; pp. 1755–1760.\n\
    122. Brulin, D. Fusion de Données Multi-Capteurs Pour L’habitat Intelligent. Ph.D.\
    \ Thesis, Université d’Orléans, Orléans,\nFrance, 2010.\n123. Baltrusaitis, T.;\
    \ Ahuja, C.; Morency, L.P. Multimodal machine learning: A survey and taxonomy.\
    \ IEEE Trans. Pattern Anal. Mach.\nIntell. 2018, 41, 423–443. [CrossRef]\n124.\
    \ Abdelmoneem, R.M.; Shaaban, E.; Benslimane, A. A survey on multi-sensor fusion\
    \ techniques in iot for healthcare. In Proceedings\nof the 2018 13th International\
    \ Conference on Computer Engineering and Systems (ICCES), Cairo, Egypt, 18–19\
    \ December 2018;\npp. 157–162.\n125. Ramachandram, D.; Taylor, G.W. Deep Learning\
    \ for Visual understanding deep multimodal learning. IEEE Signal. Process. Mag.\n\
    2017, 34, 96–108. [CrossRef]\n126. Pérez-Rúa, J.M.; Vielzeuf, V.; Pateux, S.;\
    \ Baccouche, M.; Jurie, F. MFAS: Multimodal fusion architecture search. In Proceedings\n\
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\
    \ Long Beach, CA, USA, 16–20 June 2019;\npp. 6966–6975.\n127. Feron, O.; Mohammad-Djafari,\
    \ A. A Hidden Markov model for Bayesian data fusion of multivariate signals. J.\
    \ Electron. Imaging\n2004, 14, 1–14.\n128. Jiang, Y.; Li, T.; Zhang, M.; Sha,\
    \ S.; Ji, Y. WSN-based Control System of Co2 Concentration in Greenhouse. Intell.\
    \ Autom. Soft\nComput. 2015, 21, 285–294. [CrossRef]\n129. Jing, L.; Wang, T.;\
    \ Zhao, M.; Wang, P. An adaptive multi-sensor data fusion method based on deep\
    \ convolutional neural networks\nfor fault diagnosis of planetary gearbox. Sensors\
    \ 2017, 17, 414. [CrossRef] [PubMed]\n130. Joze, H.R.V.; Shaban, A.; Iuzzolino,\
    \ M.L.; Koishida, K. MMTM: Multimodal Transfer Module for CNN Fusion. In Proceedings\n\
    of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\
    \ Seattle, WA, USA, 14–19 June 2020;\npp. 13286–13296.\n131. Moslem, B.; Khalil,\
    \ M.; Diab, M.O.; Chkeir, A.; Marque, C.A. Multisensor data fusion approach for\
    \ improving the classiﬁcation\naccuracy of uterine EMG signals. In Proceedings\
    \ of the 18th IEEE International Conference Electronics Circuits, System ICECS,\n\
    Beirut, Lebanon, 11–14 December 2011; pp. 93–96.\n132. Zadeh, A.; Chen, M.; Poria,\
    \ S.; Cambria, E.; Morency, L.-P. Tensor Fusion Network for Multimodal Sentiment\
    \ Analysis. In\nProceedings of the 2017 Conference on Empirical Methods in Natural\
    \ Language Processing, Beijing, China, 17–20 September\n2017; pp. 1103–1114.\n\
    133. Liu, Z.; Shen, Y.; Lakshminarasimhan, V.B.; Liang, P.P.; Zadeh, A.B.; Morency,\
    \ L.-P. Efﬁcient low-rank multimodal fusion with\nmodality-speciﬁc factors. arXiv\
    \ 2018, arXiv:1806.00064.\nRemote Sens. 2021, 13, 2486\n24 of 24\n134. Liu, C.;\
    \ Zoph, B.; Neumann, M.; Shlens, J.; Hua, W.; Li, L.-J.; Fei-Fei, L.; Yuille,\
    \ A.; Huang, J.; Murphy, K. Progressive Neural\nArchitecture Search. In Transactions\
    \ on Petri Nets and Other Models of Concurrency XV; Kounty, M., Kordon, F., Pomello,\
    \ L., Eds.;\nSpringer Science and Business Media LLC: Berlin, Germany, 2018; Volume\
    \ 11205, pp. 19–35.\n135. Perez-Rua, J.M.; Baccouche, M.; Pateux, S. Efﬁcient\
    \ progressive neural architecture search. arXiv arXiv:1808.00391.\n136. Bednarek,\
    \ M.; Kicki, P.; Walas, K. On robustness of multi-modal fusion—Robotics perspective.\
    \ Electronics 2020, 9, 1152. [CrossRef]\n137. Maimaitijiang, M.; Ghulam, A.; Sidike,\
    \ P.; Hartling, S.; Maimaitiyiming, M.; Peterson, K.; Shavers, E.; Fishman, J.;\
    \ Peterson, J.;\nKadam, S.; et al. Unmanned Aerial System (UAS)-based phenotyping\
    \ of soybean using multi-sensor data fusion and extreme\nlearning machine. ISPRS\
    \ J. Photogramm. Remote Sens. 2017, 134, 43–58. [CrossRef]\n138. Chu, Z.; Yu,\
    \ J. An end-to-end model for rice yield prediction using deep learning fusion.\
    \ Comput. Electron. Agric. 2020,\n174, 105471. [CrossRef]\n139. Ji, S.; Zhang,\
    \ C.; Xu, A.; Shi, Y.; Duan, Y. 3D convolutional neural networks for crop classiﬁcation\
    \ with multi-temporal remote\nsensing images. Remote Sens. 2018, 10, 75. [CrossRef]\n\
    140. Song, Z.; Zhang, Z.; Yang, S.; Ding, D.; Ning, J. Identifying sunﬂower lodging\
    \ based on image fusion and deep semantic\nsegmentation with UAV remote sensing\
    \ imaging. Comput. Electron. Agric. 2020, 179, 105812. [CrossRef]\n141. Maimaitijiang,\
    \ M.; Sagan, V.; Sidike, P.; Daloye, A.M.; Erkbol, H.; Fritschi, F.B. Crop monitoring\
    \ using satellite/uav data fusion\nand machine learning. Remote Sens. 2020, 12,\
    \ 1357. [CrossRef]\n142. Zhang, J.; Pu, R.; Yuan, L.; Huang, W.; Nie, C.; Yang,\
    \ G. Integrating remotely sensed and meteorological observations to forecast\n\
    wheat powdery mildew at a regional scale. IEEE J. Sel. Top. Appl. Earth Obs. Remote\
    \ Sens. 2014, 7, 4328–4339. [CrossRef]\n143. Selvaraj, M.G.; Vergara, A.; Montenegro,\
    \ F.; Ruiz, H.A.; Safari, N.; Raymaekers, D.; Ocimati, W.; Ntamwira, J.; Tits,\
    \ L.; Omondi,\nA.B.; et al. Detection of banana plants and their major diseases\
    \ through aerial images and machine learning methods: A case\nstudy in DR Congo\
    \ and Republic of Benin. ISPRS J. Photogramm. Remote Sens. 2020, 169, 110–124.\
    \ [CrossRef]\n144. Yuan, L.; Bao, Z.; Zhang, H.; Zhang, Y.; Liang, X. Habitat\
    \ monitoring to evaluate crop disease and pest distributions based on\nmulti-source\
    \ satellite remote sensing imagery. Optik 2017, 145, 66–73. [CrossRef]\n145. Riskiawan,\
    \ R.H. SMARF: Smart farming framework based on big data, IoT and deep learning\
    \ model for plant disease detection\nand prevention. In Proceedings of the Applied\
    \ Computing to Support Industry: Innovation and Technology: First International\n\
    Conference, ACRIT 2019, Ramadi, Iraq, 15–16 September 2019; Revised Selected Papers.\
    \ Springer: Berlin/Heidelberg, Germany,\n2020; Volume 1174, p. 44.\n146. Huang,\
    \ Y.J.; Evans, N.; Li, Z.Q.; Eckert, M.; Chèvre, A.M.; Renard, M.; Fitt, B.D.\
    \ Temperature and leaf wetness duration affect\nphenotypic expression of Rlm6-mediated\
    \ resistance to Leptosphaeria maculans in Brassica napus. New Phytol. 2006, 170,\
    \ 129–141.\n[CrossRef] [PubMed]\n147. Azfar, S.; Nadeem, A.; Basit, A. Pest detection\
    \ and control techniques using wireless sensor network: A review. J. Entomol.\
    \ Zool.\nStud. 2015, 3, 92–99.\n"
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  pdf_link: https://www.mdpi.com/2072-4292/13/13/2486/pdf?version=1624616000
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Computer Vision, IoT and Data Fusion for Crop Disease Detection Using Machine
    Learning: A Survey and Ongoing Research'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.34133/2021/9840192
  analysis: '>'
  authors:
  - Wei Guo
  - Matthew E. Carroll
  - Arti Singh
  - Tyson L. Swetnam
  - Nirav Merchant
  - Soumik Sarkar
  - Baskar Ganapathysubramanian
  citation_count: 46
  full_citation: '>'
  full_text: '>

    ADVERTISEMENT About SPJ Author Services Journals Science.org Science Partner Journals
    Quick Search anywhere ENTER SEARCH TERM SEARCH ADVANCED SEARCH Featured Articles
    RESEARCH4 APR 2024 Harnessing Renewable Lignocellulosic Potential for Sustainable
    Wastewater Purification ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Multilayer
    MoS2 Photodetector with Broad Spectral Range and Multiband Response BY XIA-YAO
    CHEN DAN SU ET AL. ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Hepatocellular
    Carcinoma Detection by Cell Sensor Based on Anti-GPC3 Single-Chain Variable Fragment
    BY ZUPENG YAN ZIYUAN CHE ET AL. BIODESIGN RESEARCH3 APR 2024 High-Temperature
    Tolerance Protein Engineering through Deep Evolution BY HUANYU CHU ZHENYANG TIAN
    ET AL. BME FRONTIERS3 APR 2024 Multifunctional Ablative Gastrointestinal Imaging
    Capsule (MAGIC) for Esophagus Surveillance and Interventions BY HYEON-CHEOL PARK
    DAWEI LI ET AL. OCEAN-LAND-ATMOSPHERE RESEARCH3 APR 2024 Factors Modulating the
    Variability of Eddy Kinetic Energy in the Southern Ocean from Idealized Simulations
    BY YONGQING CAI DAKE CHEN ET AL. PLANT PHENOMICS3 APR 2024 Microfluidic Device
    for Simple Diagnosis of Plant Growth Condition by Detecting miRNAs from Filtered
    Plant Extracts BY YAICHI KAWAKATSU RYO OKADA ET AL. MORE ARTICLES ADVERTISEMENT
    Journals Advanced Devices & Instrumentation The Open Access journal Advanced Devices
    & Instrumentation, published in association with BIACD, is a forum to promote
    breakthroughs and application advances at all levels of electronics and photonics.
    BioDesign Research The Open Access journal BioDesign Research, published in association
    with NAU, publishes novel research in the interdisciplinary field of biosystems
    design. Biomaterials Research The Open Access journal Biomaterials Research, published
    in association with KSBM, covers the interdisciplinary fields of biomaterials
    research, including novel biomaterials, cutting-edge technologies of biomaterials
    synthesis and fabrication, and biomedical applications in clinics and industry.
    BMEF The Open Access journal BMEF (BME Frontiers), published in association with
    SIBET CAS, is a platform for the multidisciplinary community of biomedical engineering,
    publishing wide-ranging research in the field. Cyborg and Bionic Systems The Open
    Access journal Cyborg and Bionic Systems, published in association with BIT, promotes
    the knowledge interchange and hybrid system codesign between living beings and
    robotic systems. Ecosystem Health and Sustainability The Open Access journal Ecosystem
    Health and Sustainability, published in association with ESC, publishes research
    on advances in sustainability ecology and how global environmental change affects
    ecosystem health. Energy Material Advances The Open Access journal Energy Material
    Advances, published in association with BIT, is an interdisciplinary platform
    for research in multiple fields from cutting-edge material to energy science.
    Health Data Science The Open Access journal Health Data Science, published in
    association with PKU, publishes innovative, scientifically-rigorous research to
    advance health data science. Intelligent Computing Open Access journal Intelligent
    Computing, published in affiliation with Zhejiang Lab, publishes the latest research
    outcomes and technological breakthroughs in intelligent computing. Journal of
    Remote Sensing The Journal of Remote Sensing, an Open Access journal published
    in association with AIR-CAS, promotes the theory, science, and technology of remote
    sensing, as well as interdisciplinary research within earth and information science.
    Ocean-Land-Atmosphere Research The Open Access journal Ocean-Land-Atmosphere Research
    (OLAR), published in association with SML, publishes technologically innovative
    research in marine, terrestrial, and atmospheric studies and the interactions
    among them. Plant Phenomics The Open Access journal Plant Phenomics, published
    in association with NAU, publishes novel research that advances plant phenotyping
    and connects phenomics with other research domains. Research The Open Access journal
    Research, published in association with CAST, publishes innovative, wide-ranging
    research in life sciences, physical sciences, engineering and applied science.
    Space: Science & Technology Open Access journal Space: Science & Technology, published
    in association with BIT, promotes the interplay of science and technology for
    the benefit of all application domains of space activities. It particularly welcomes
    articles illustrating successful synergies in space programs and missions. Ultrafast
    Science The Open Access journal Ultrafast Science, published in association with
    Xi’an Institute of Optics and Precision Mechanics, is a platform for cutting-edge
    and emerging topics in ultrafast science with broad interest from scientific communities.
    BROWSE ALL JOURNALS About Us About SPJ About AAAS Science family of journals Work
    at AAAS Help FAQ Email Alerts and RSS Feeds Follow Us © 2024 American Association
    for the Advancement of Science. All rights Reserved. AAAS is a partner of HINARI,
    AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Terms of Service Privacy Policy
    Accessibility'
  inline_citation: '>'
  journal: Plant phenomics
  limitations: '>'
  pdf_link: https://downloads.spj.sciencemag.org/plantphenomics/2021/9840192.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: UAS-Based Plant Phenotyping for Research and Breeding Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.biosystemseng.2019.08.017
  analysis: '>'
  authors:
  - Jordi Gené-Mola
  - Eduard Gregorio
  - Javier Guevara
  - Fernando Auat
  - Ricardo Sanz
  - Alexandre Escolà
  - Jordi Llorens
  - Josep Ramon Morros
  - Javier Ruiz-Hidalgo
  - Verónica Vilaplana
  - Joan R. Rosell-Polo
  citation_count: 73
  full_citation: '>'
  full_text: '>

    Typesetting math: 100% Skip to main content Skip to article Journals & Books Search
    Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download
    full issue Outline Highlights Keywords Nomenclature 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusions Acknowledgements Appendix
    A. Parameter values and feature analysis References Show full outline Cited by
    (76) Figures (10) Show 4 more figures Tables (5) Table 1 Table 2 Table 3 Table
    4 Table A1 Biosystems Engineering Volume 187, November 2019, Pages 171-184 Research
    Paper Fruit detection in an apple orchard using a mobile terrestrial laser scanner
    Author links open overlay panel Jordi Gené-Mola a, Eduard Gregorio a, Javier Guevara
    b, Fernando Auat b, Ricardo Sanz-Cortiella a, Alexandre Escolà a, Jordi Llorens
    a, Josep-Ramon Morros c, Javier Ruiz-Hidalgo c, Verónica Vilaplana c, Joan R.
    Rosell-Polo a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.biosystemseng.2019.08.017
    Get rights and content Highlights • Apples present higher IR reflectance than
    leaves and trunks. • A new methodology for fruit detection using an MTLS has been
    developed. • Results show a success rate of 82.4%, with a 10.4% of false detections.
    • The methodology is not affected by lighting and it provides apple locations
    in 3D. The development of reliable fruit detection and localization systems provides
    an opportunity to improve the crop value and management by limiting fruit spoilage
    and optimised harvesting practices. Most proposed systems for fruit detection
    are based on RGB cameras and thus are affected by intrinsic constraints, such
    as variable lighting conditions. This work presents a new technique that uses
    a mobile terrestrial laser scanner (MTLS) to detect and localise Fuji apples.
    An experimental test focused on Fuji apple trees (Malus domestica Borkh. cv. Fuji)
    was carried out. A 3D point cloud of the scene was generated using an MTLS composed
    of a Velodyne VLP-16 LiDAR sensor synchronised with an RTK-GNSS satellite navigation
    receiver. A reflectance analysis of tree elements was performed, obtaining mean
    apparent reflectance values of 28.9%, 29.1%, and 44.3% for leaves, branches and
    trunks, and apples, respectively. These results suggest that the apparent reflectance
    parameter (at 905 nm wavelength) can be useful to detect apples. For that purpose,
    a four-step fruit detection algorithm was developed. By applying this algorithm,
    a localization success of 87.5%, an identification success of 82.4%, and an F1-score
    of 0.858 were obtained in relation to the total amount of fruits. These detection
    rates are similar to those obtained by RGB-based systems, but with the additional
    advantages of providing direct 3D fruit location information, which is not affected
    by sunlight variations. From the experimental results, it can be concluded that
    LiDAR-based technology and, particularly, its reflectance information, has potential
    for remote apple detection and 3D location. Previous article in issue Next article
    in issue Keywords LiDARMobile terrestrial laser scanningFruit detectionAgricultural
    robotics Nomenclature FDRID False detection rate identification FDRL False detection
    rate localization FoV Field of View [°] FPID False positive identification FPL
    False positive localization GTfield Number of fruits manually-counted in field
    GTlabels Number of fruits labelled IoDi Intersection over detection K Number of
    fruits in a cluster MTLS Mobile Terrestrial Laser Scanner n Number of clusters
    that detect the same fruit Nm Fruit multi-detections (n-1) P Number of points
    of a cluster Pkj Number of points threshold used to find clusters with j apples
    R Apparent reflectance [%] Rth Reflectance threshold [%] RTK-GNSS Real-Time Kinematics
    Global Navigation Satellite System Mean apparent reflectance of the points of
    a cluster [%] FP Mean apparent reflectance threshold used to find false positive
    clusters [%] kj Mean apparent reflectance threshold used to find clusters with
    j apples [%] SuccessID Identification success (recall) SuccessL Localization success
    SVD Singular Value Decomposition TOF Time of flight TPID True positive identification
    TPL True positive localization V Volume of a cluster [m3] VFP Volume threshold
    used to find false positive clusters [m3] Vkj Volume threshold used to find clusters
    with j apples [m3] [x, y, z] 3D point with UTM coordinates [m] α Sparse outlier
    removal tuning parameter λin Normalised principal value i λi Principal value i
    of a cluster Ψ Geometric parameter Geometric parameter value used to find false
    positive clusters Geometric parameter value used to find clusters with j apples
    1. Introduction Fruticulture is under constant pressure to increase fruit production
    and quality, as demanded by a growing world population. To this end, farmers need
    to find new ways to improve fruit productivity and, at the same time, reduce economic
    and environmental costs (Siegel, Ali, Srinivasiah, Nugent, & Narayan, 2014). Agricultural
    robotics takes advantage of new technologies to respond to this challenge (Bac
    et al., 2014, Bechar and Vigneault, 2017, Bechar and Vigneault, 2016, Gongal et
    al., 2015, Zhao et al., 2016a). The use of robotics in agricultural fields and
    orchards is increasing, particularly in tasks related to guidance (seeding or
    harvesting), detection (weed monitoring and control, extraction of biological
    features), and mapping (Auat Cheein and Carelli, 2013, Auat Cheein et al., 2017,
    Foglia and Reina, 2006). In general, the development of intelligent robots interacting
    with agricultural fields increases the accuracy of tasks and reduces the consumption
    of resources without decreasing yield, making it a reasonable option for repeatable
    tasks (Cariou et al., 2009, Foglia and Reina, 2006, Zhang and Pierce, 2016). Fruit
    detection and localization are complex tasks that can be handled by agricultural
    robotics, with applications related to yield prediction, yield mapping, and automated
    harvesting. Nowadays, yield prediction is done by manual counting of selected
    sample trees, leading to inaccurate predictions due to the high variability in
    orchards (Payne et al., 2014, Stein et al., 2016). Crop monitoring using new technologies
    could provide more accurate and efficient predictions (Bechar and Vigneault, 2017,
    Bechar and Vigneault, 2016). Another application of fruit detection is yield mapping.
    The fruit load of an orchard is influenced by in-field spatial variability (due
    to soil type variations), fertility, and water content, among other factors. In
    precision agriculture, yield mapping helps to determine the reasons for and find
    solutions to cope with this variability (Kurtulmus, Lee, & Vardar, 2014). Finally,
    fruit localization is the basis for future automated harvesting. Manual picking
    is a bottleneck in fruit production management, because it requires lots of resources
    in the context of decreasing farming labour force. In addition, hand harvesting
    exposes farmers to awkward postures on ladders and platforms with heavy loads,
    making manual harvesting dangerous and inefficient (De-An et al., 2011, Gongal
    et al., 2015). The detection of fruits can involve many fruit properties of different
    complexity, from the simplest, such as the presence/absence of a fruit, to properties
    that are more challenging to measure, including size, volume, diameter, maturation
    stage, sugar, and other substance contents, defects and disease/pest affectation,
    etc. There are multiple technologies available for fruit detection and localization,
    each with its advantages and disadvantages (Gongal et al., 2015). All approaches
    have to solve problems derived from occlusions (Stein et al., 2016, Wachs et al.,
    2010), clustering (Gong et al., 2013, Xiang et al., 2014), and variable lighting
    conditions (Gongal et al., 2016, Zhao et al., 2016b). The most commonly used sensors
    are RGB cameras (Linker, 2017, Maldonado and Barbosa, 2016, Zhao et al., 2016b).
    These are affordable sensors, which allow fruits to be distinguished from other
    elements by colour (Linker et al., 2012, Liu et al., 2016), geometric shape (Barnea
    et al., 2016, Lak et al., 2010), texture (Chaivivatrakul and Dailey, 2014, Qureshi
    et al., 2017), or by using machine learning techniques like, e.g., deep neural
    networks (Bargoti & Underwood, 2017). The two main drawbacks to RGB cameras are
    their sensitivity to lighting conditions and the fact that they only provide 2D
    information (unless using stereoscopic techniques). Other, more expensive, cameras
    include thermal cameras (Bulanon et al., 2008, Bulanon et al., 2009, Stajnko et
    al., 2004, Wachs et al., 2010), multispectral cameras (Sa et al., 2016, Zhang
    et al., 2015), and hyperspectral cameras (Okamoto and Lee, 2009, Safren et al.,
    2007). The former allows fruits to be distinguished from the background through
    their temperature, while the latter detect fruits from their reflectance at different
    wavelengths. Like RGB cameras, thermal, multispectral, and hyperspectral cameras
    do not provide 3D information, unless a stereoscopic approach is implemented.
    There are several solutions to obtain three-dimensional information. One of them
    is based on using two (stereovision) or more cameras (Font et al., 2014, Si et
    al., 2015, Xiang et al., 2014). By applying triangulation techniques, it is possible
    to obtain the depth of each pixel and reconstruct the 3D structure. The major
    advantage of this technique is that it allows us to obtain accurate 3D models
    with RGB information, while the main disadvantages are that 3D model generation
    is computationally expensive and the performance is affected by lighting conditions.
    Another more recent technique is the use of laser range finders and LiDAR-based
    (Light Detection and Ranging) systems. These are more expensive sensors that generally
    operate under the principle of time-of-flight (TOF) (Wehr & Lohr, 1999). This
    type of sensor typically also provides the amount of energy backscattered from
    the impacted object. Very few studies have used LiDAR-based systems in fruit detection
    and, to the best of the authors’ knowledge, none of them have been tested in a
    real orchard environment. For example, Jiménez et al., 2000, Jiménez et al., 1999
    developed a vision system based on a laser range-finder, with the aim of detecting
    spherical objects in non-structured environments. They report good detection performances,
    although the tests were carried out on a limited number of oranges suspended from
    an artificial tree. Finally, another technology derived from photogrammetry and
    LiDAR, and also used in fruit growing, are the RGB-D (depth) cameras, where each
    pixel of the image contains colour and depth data, generating 3D colour images
    (Barnea et al., 2016, Nguyen et al., 2016, Rosell-Polo et al., 2017, Rosell-Polo
    et al., 2015). These systems are based on the simultaneous combination of RGB
    cameras and depth sensors based on laser light (either through structured laser
    light or TOF flash-type LiDAR-based systems). This work presents a proof of concept
    of using LiDAR in detecting Fuji apples in producing orchard trees. The methodology
    is founded on the fact that apples have higher apparent reflectance than leaves
    and trunks at 905 nm laser wavelength. The main contributions of this paper are:
    (1) analysis of apple reflectivity on 3D point clouds from LiDAR sensors; (2)
    development of an apple detection and localization algorithm based on three stages
    (point cloud segmentation; fruit separation, and false positive removal); and
    (3) experimental validation of the proposed technique on a real Fuji apple orchard.
    The principal advantage of this technique over previously published efforts would
    be its capacity to provide direct 3D fruit localization information without being
    affected by illumination conditions. The paper is structured as follows. Section
    2 presents the experimental data set, the point cloud generation procedure, the
    reflectance analysis, and the developed apple detection algorithm. Section 3 shows
    the results of the first experimental tests performed on three Fuji apple trees
    of a commercial orchard. Finally, the conclusions are presented in Section 4.
    2. Materials and methods 2.1. Experimental set up A fruit detection experiment
    was carried out on September 28th of 2017 in Tarassó farm, a commercial apple
    orchard located in Agramunt, Catalonia, Spain (E: 336,297 m; N: 4,623,494 m; 312
    m a.s.l., UTM 31T - ETRS89). The trials were carried out in an 8-year-old Fuji
    apple orchard (Malus domestica Borkh. cv. Fuji), trained in a tall spindle system
    with a maximum tree height of 3.75 m. The three analysed trees were at BBCH (Biologische
    Bundesanstalt, Bundessortenamt und CHemische Industrie) growth stage 85 (Meier,
    2001), three weeks before harvesting. The measurement equipment consisted of a
    mobile Terrestrial Laser Scanner (MTLS), comprised of a LiDAR sensor and a real-time
    kinematics global navigation satellite system (RTK-GNSS), connected to a rugged
    laptop suitable for working in field conditions. The LiDAR sensor used was a Puck
    VLP-16 (Velodyne LIDAR Inc., San José, CA, USA), which generates a 3D point cloud
    (x-y-z positions) of the scanned scene, as well as calibrated apparent reflectance
    (R) of each point in the 3D point cloud. This calibration was carried out by sensor
    manufacturer using a set of calibration targets, and implies a conversion of the
    backscattered range-corrected intensity (digital numbers) into apparent reflectance
    values independently of laser power and distance (Velodyne, 2016). Note that the
    measured apparent reflectance (hereinafter referred to as reflectance) is an approximation
    of the actual hemispherical reflectance, considering that the measured objects
    are Lambertian (perfectly diffuse reflectors), and not considering the incidence
    angle (Kaasalainen et al., 2011, Kukko et al., 2008, Ray, 1994). The VLP-16 sensor
    emits 16 laser beams (905 nm wavelength) with a horizontal angular resolution
    of 2° (30° horizontal FoV) when mounted on a vertical plane as shown in Fig. 1.
    Although the vertical FoV can be set up to 360°, in this experiment it was set
    to 150°, since only one row of trees was scanned. The scanning frequency rate
    was set to 10 Hz, corresponding to a vertical angular resolution of 0.2°, so that
    a maximum of 12,000 points were obtained from each scan (acquisition speed of
    120,000 points/second). Even though this sensor has a range of 100 m, points further
    than 4 m where not considered for 3D point cloud generation, thus only the tree
    row of interest was modelled. The acquisition of Coordinated Universal Time (UTC)
    of each point was obtained via a GPS 18x LVC receiver (Garmin International Inc.,
    Olathe, KS, USA), connected to the VLP-16 sensor. The RTK-GNSS system used was
    the GPS1200+ (Leica Geosystems AG, Heerbrugg, Swizeland), which provides absolute
    coordinates and UTC time (synchronised with the LiDAR) with a frequency of 20
    Hz and a precision of approx. 20 mm. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 1. View of the MTLS equipment showing
    the GNSS antenna placement and the mounting orientation of the LiDAR sensor. Distance
    data are in mm. As shown in Fig. 1, the MTLS measurement system was mounted on
    the rear of an air-assisted sprayer by means of an aluminium structure. The sprayer
    was pulled at low gear by a farm tractor equipped with an electronic speedometer.
    The GNSS rover receiver antenna was installed on top of the mast, at a height
    of 3.5 m. The LiDAR sensor was mounted vertically (Fig. 1) and placed at a height
    of 1.8 m, that is about half the maximum height of studied trees. This position
    was selected to have similar detection performance along the tree height. The
    field test was performed by moving the MTLS along a rectilinear trajectory parallel
    to the tree row axis, at a distance of 2.4 m. Due to the fact that the system
    did not include an inertial measurement unit (IMU), moving the MTLS along a linear
    trajectory was important to improve the point cloud consistency. The forward speed
    was 0.125 m s−1, corresponding to a resolution of 12.5 mm between consecutive
    scans (∼53,600 points m−2 in a vertical plane at the distance of 2.4 m). The tree
    row was scanned from both sides in order to obtain a complete 3D model. 2.2. 3D
    point cloud model A rigid transformation was performed by applying a rotation
    and translation matrix to each point, in order to build the point cloud with absolute
    coordinates. The translation matrix was built using the Universal Transverse Mercator
    (UTM) coordinates of the RTK-GNSS system, by considering the relative distance
    between the optical centre of the LiDAR sensor and the GNSS receiver. The rotation
    matrix depends on the orientation of the MTLS at each time instant and was obtained
    by the forward direction computed from the measurements of the RTK-GNSS receiver.
    Given that the trials were performed with a short rectilinear trajectory, the
    tilt of the platform can be ignored, assuming a constant orientation along the
    path. An illustration of the 3D point cloud models generated is shown in Fig.
    2. Download : Download high-res image (792KB) Download : Download full-size image
    Fig. 2. 3D point cloud models obtained for trees 1, 2, and 3. First two trees
    were used as training dataset, while the third one was kept as test dataset. Ground
    truth bounding boxes of tree 3 are shown, while the zoom bounding box (red circle)
    shows its shape. The resulting 3D point cloud was manually labelled in order to
    generate ground truth of the apples locations. This enables a study of the features
    that characterise the apples, as well as the possibility to evaluate the performance
    of the developed apple detection techniques. The annotation was carried out using
    the software CloudCompare (Cloud Compare [GPL software] v2.9 Omnia), placing 3D
    rectangular bounding boxes on each apple, as can be seen in the third tree of
    Fig. 2. This annotation was supported by additional RGB images to localise the
    apples in the 3D point cloud. The actual number of apples counted in field (ground
    truth field or GT_field) were 139 in tree 1, 145 in tree 2, and 139 in tree 3,
    of which 133, 138 and 134, respectively, could be labelled in the 3D point cloud
    (ground truth labels or GT_labels) due to occlusions or in field counting errors.
    Trees 1 and 2 were used as the training dataset to select and tune the algorithm
    parameters, while tree 3 was used as the test dataset to evaluate the performance
    of the developed algorithm. From the labelled scene and the reflectance data extracted
    from the LiDAR for each point, a reflectance study of the different elements of
    the tree was carried out (Section 3.1). 2.3. Apple detection algorithm As shown
    in Fig. 3, the algorithm proposed in this paper is structured as follows: 1) Point
    cloud segmentation; 2) fruit separation; and 3) false positive removal. The segmentation
    is based on the reflectance of measured elements and aims at removing points corresponding
    to leaves, branches, and the trunk, and grouping the remaining points -likely
    to be an apple-in clusters. The fruit separation uses features of clusters in
    order to identify and split those that contain more than one apple. False positive
    removal is based on the geometry and reflectance of the clusters. All data processing
    was implemented in MATLAB (R2018a, Math Works Inc., Natick, Massachusetts, USA).
    The different implemented steps are detailed below. Download : Download high-res
    image (225KB) Download : Download full-size image Fig. 3. Apple detection algorithm
    flowchart. 2.3.1. Point cloud segmentation The objective of this step is to segment
    the 3D point cloud and obtain a set of clusters with points that could be apple
    candidates. Since some groups of apples could be touching, the clusters obtained
    in this first step could contain one or more apples. The 3D model acquired with
    the MTLS consists of a set of 3D points with UTM coordinates and their reflectance
    [x, y, z, R]. The reflectance analysis (Section 3.1) shows that apple reflectance
    at the 905 nm laser wavelength is higher than that for leaves and the trunk and,
    therefore, this parameter is used for apple detection. To remove the points that
    do not correspond to apples, a threshold, Rth, is applied. This is followed by
    Sparse Outlier Removal (Rusu, Marton, Blodow, Dolha, & Beetz, 2008) to reduce
    the noise; this approach removes the points which fall outside μ + α·σ, with μ
    and σ being the mean and standard deviation, respectively, of the k nearest neighbour
    distances, while α is a tuning parameter. The point cloud segmentation ends with
    a connected components labelling using a density-based scan algorithm, DBSCAN
    (Ester, Kriegel, Sander, & Xu, 1996), which clusters points that have more than
    minPts points closer than a distance, ε. Outlier Removal is first applied to delete
    noisy points that otherwise would connect clusters from different apples. All
    the parameters used in this step were selected through a hyperparameter optimization
    procedure, using the training data set to search for the combination of parameters
    that best suits our data. In this search, we found that the results were stable
    against small variations in the different parameter values, except the reflectance
    threshold, the behaviour of which is shown in Section 3.1, Fig. 7. The parameter
    values used are detailed in Appendix A. Download : Download high-res image (363KB)
    Download : Download full-size image Fig. 4. Method 1 - Cluster splitting by Gaussian
    smoothing. The aim of this method is to determine the number of apples, K, that
    are contained in a cluster. a) Actual data before applying method 1: the real
    scene is scanned and the resulting point cloud is segmented, obtaining clusters
    likely to contain apples. b) Cluster containing 2 apples. c) 2D projection in
    four planes: (1) frontal; (2) lateral, (3) top; and (4) plane defined by 2 principal
    axes. d) Gaussian smoothing. e) Local maxima identification. Download : Download
    high-res image (223KB) Download : Download full-size image Fig. 5. Method 2 -
    Decision tree used to predict the number of apples in a cluster. Download : Download
    high-res image (677KB) Download : Download full-size image Fig. 6. Localization
    and identification performance evaluation criteria. Intersection over detection
    (IoD) is given for different scenarios, while true positive and false positive
    are calculated for localization and identification assessment approaches. Red
    shapes are apple detections, while green squares correspond to the ground truth
    labels. Download : Download high-res image (244KB) Download : Download full-size
    image Fig. 7. a) Precision (green dashed line), recall (red dotted line), and
    F1-score (blue solid line) versus the applied reflectance threshold; b) Gaussian
    distributions obtained for each tree element in the reflectance analysis of the
    training dataset. Green solid line corresponds to leaves, blue dotted line to
    trunks and red dashed line to fruits. The vertical dash-dotted line indicates
    the reflectance threshold used for fruit detection. (For interpretation of the
    references to color in this figure legend, the reader is referred to the Web version
    of this article.) 2.3.2. Apple separation If apples are properly separated, the
    results obtained in the previous step would consist of a set of clusters of one
    apple in each. Nevertheless, it was found that groups of apples touching will
    result in clusters of more than one apple. The aim of this second step is to identify
    the clusters containing more than one apple and split them into sub-clusters,
    each containing one apple. First, the number of apples, K, that make up a cluster
    has to be predicted, and then the cluster is split using the K-means algorithm.
    This clustering method aims to partition the 3D points into K sub-clusters in
    which each 3D point belongs to the sub-cluster with nearest mean (Jain, 2010).
    To predict the number of apples contained in each cluster (the K number used in
    the K-means algorithm), three different methods were tested. The first one is
    inspired by a template matching technique (Brunelli, 2009). The second method
    applies a decision tree, based on cluster features such as volume, density of
    points, reflectance, and shape. Finally, the third method is a combination of
    the previous two approaches. These methods are explained in more detail below.
    2.3.2.1. Method 1 The first approach projects the 3D point clouds of each cluster
    (Fig. 4b) onto a 2D plane, obtaining an image of the cluster with reflectance
    data at a resolution of 4 × 4 mm per pixel (Fig. 4c). The cluster image then is
    convolved with a Gaussian filter of size 20 × 20 pixels and standard deviation
    of 3.5. These parameters correspond to the measured fruit size, so that the dimension
    of this filter is 80 × 80 mm, similar to the mean size of the tested apples. Since
    the apples have an approximately spherical shape, when the cluster image is convolved
    with a Gaussian filter, the local maxima of the obtained image correspond to the
    centres of the apples (Fig. 4d). The value K, to be used in the K-means algorithm,
    corresponds to the number of local maxima found in the convolved image (Fig. 4e).
    The result of this method could vary with the 2D projection plane used (e.g.,
    the projection may produce occlusions). The technique is applied in four different
    planes to prevent this projection-induced variability: frontal, lateral, top,
    and the plane defined by the first two principal axes of the cluster (Fig. 4b,c).
    The value of K will be the maximum obtained in these four planes. The principal
    axes are the directions where the variance of data is maximised and, therefore,
    where the points exhibit the largest range. The first two principal axes define
    the principal plane of the cluster and are obtained by applying singular value
    decomposition (SVD) to the set of points forming the cluster. 2.3.2.2. Method
    2 The second method applies a decision tree based on cluster features. The first
    step is to extract the following features for each cluster: volume (V), number
    of cluster points (P), mean reflectance of cluster points ( ), and a geometric
    parameter, Ψ, computed as the product of normalised eigenvalues [λ1n, λ2n, λ3n
    ]. The volume (V) was defined as the volume enclosed by the boundary points of
    the cluster. Clusters that contain more than one fruit are expected to have a
    larger volume (V) and more points (P). However, when a fruit is placed next to
    a leaf or trunk (not filtered in previous steps), the cluster volume (V) and the
    number of points (P) could increase as well. Due to this fact, the threshold,
    , is applied, as it was observed that the mean reflectance of this kind of trunk/leaf
    co-located cluster is lower than clusters containing grouped fruits. The last
    features used are the eigenvalues, which provide information about the cluster
    shape. Spherical shapes (clusters with only one apple) will have similar eigenvalues,
    while elongated shapes will have different eigenvalues. Eigenvalues are obtained
    with SVD, and their values depend on the variance of the points projected on the
    principal axes. In order to compare eigenvalues of different clusters, a normalization
    step is applied so that the eigenvalues sum to one. From that, the geometric parameter
    Ψ is defined as the product of eigenvalues and a normalization factor. The normalization
    factor of 27 allows the geometrical parameter, Ψ, to be bound between 0 and 1:
    (1) (2) The implemented decision tree is based on the analysed features in the
    training data set and is composed of the following steps (Fig. 5): • Feature extraction:
    Compute V,P, , and of the studied cluster. • Step 1: If V, P, and are higher than
    the corresponding thresholds Vk1, Pk1, k1, and is smaller than k1, it is concluded
    that the cluster contains more than one apple. Otherwise, K is assigned the value
    1. • Step 2: A cluster will have more than two apples if P is higher than Pk2
    and is lower than k2, or if V is higher than Vk2. Otherwise, K is assigned the
    value of 2. • Step 3: K = 4 when a cluster meets both previous conditions and
    has a volume (V) higher than Vk3. Otherwise, K is assigned the value 3. All threshold
    values used in the decision tree were empirically selected by the graphical representation
    of four analysed features using the training dataset. The values used and the
    graphical representation of these features are presented in Appendix A, Table
    A1 and Fig. A1. 2.3.2.3. Method 3 By applying method 1, some single-fruit clusters
    are split into multiple detections due to partial occlusions of apples by leaves.
    Method 3 addresses this concern by combining methods 1 and 2. First, step 1 of
    method 2 is applied to distinguish between clusters with single or multiple apples.
    For those clusters that contain more than one apple, method 1 is applied to determine
    the value of K. 2.3.3. False positive removal After implementing the first two
    steps of the algorithm (segmentation and apple separation), it was observed that
    some detections do not actually correspond to apples, i.e., these were false positive
    detections. That is because some leaves and trunks have a texture or shape that
    result in a high reflectance. It was found that some of these erroneous detections
    had a different geometric shape ( ), volume (V), and mean reflectance ( ) compared
    to the successful detections. In order to reduce these false positives, the clusters
    that met the condition (  <  FP) | (  <  FP) | (V > VFP) were removed. In the
    same manner as with method 2, the thresholds were empirically selected from a
    graphical representation of these three features using the training dataset. The
    values used and the graphical representation of these features are presented in
    Appendix A, Table A1 and Fig. A2. 2.4. Performance evaluation In this work, the
    results were evaluated using two different approaches: localization and identification.
    The localization evaluation aims to assess the system in the context of harvesting
    automation. This approach assumes that a robotic arm, when it gets close to a
    group of apples, is able to separate different apples that have been detected
    within the same cluster, or to unify the multi-detections that correspond to the
    same apple. Thus, a detection that contains K apples counts as K true positives
    (Fig. 6a), while multi-detections are counted as one true positive and no false
    positives (Fig. 6e). The identification evaluation aims to assess the system for
    use in yield prediction or mapping. This assessment is performed cluster-by-cluster,
    so that a single detection containing K apples counts as only one true positive
    (Fig. 6a), while a single apple detected n times (multi-detection) is counted
    as one true positive and Nm = n-1 false positives (Fig. 6e). To evaluate object
    detection in images, the metric intersection over union (IoU) is commonly used.
    This is possible when both bounding-box and object detection can be seen as a
    group of pixels. In this study, the detections are groups of 3D points, while
    ground truth bounding boxes are cube regions. The metric IoU has been substituted
    by the intersection over detection (IoD) for this reason; IoD is defined as the
    percentage of detected points that are placed inside ground truth bounding boxes.
    The following defines the metrics used for each approach, namely localization
    (subscript L) and identification (subscript ID). • Intersection over detection
    (IoDi): Percentage of points, Pi, of a detection, i, that are placed inside ground
    truth bounding-boxes (GT). • True positive localization (TPL): Number of ground
    truth apples that are detected with an IoDi ≥ 0.5. • False positive localization
    (FPL): Number of detections with an IoDi < 0.5. • Localization success (SuccessL):
    Quotient between TPL and the number of labelled apples (GTlabels). • False detection
    rate localization (FDRL): Ratio between FPL and the total positive (TPL + FPL).
    • True positive identification (TPID): Number of clusters with an IoDi ≥ 0.5,
    minus multi-detections ( . • False positive identification (FPID): Sum of the
    number of detections with an IoDi < 0.5 (FPL), plus multi-detections. . • Identification
    success (SuccessID or recall): Quotient between TPID and GTlabels. • False detection
    rate identification (FDRID): Ratio between FPID and the total positive. • Precision:
    Percentage of TPID with respect to the total positive (TPID + FPID). • F1-score:
    Harmonic mean of precision and recall. Selected examples of the evaluation criteria
    can be seen in Fig. 6. Intersection over detection (IoD) is given for different
    scenarios, while true positive and false positive rates are calculated for localization
    and identification assessment approaches. Red shapes are apple detections, while
    green squares correspond to the ground truth labels. Note that actual clusters
    and bounding-boxes are in 3D (as shown in Fig. 2), although for the sake of simplicity
    this figure shows the 2D projection. The examples shown are: a) One cluster with
    K = 2 apples and three GT bounding-boxes; b) One cluster with K = 1 apple and
    one GT bounding-box; c) Two clusters of K = 1 apple each and two GT bounding-boxes;
    d) One GT bounding-box not detected; e) Two clusters detecting the same GT bounding-box
    (multi-detection); f) One cluster that does not correspond to any GT object; and
    g) One cluster detecting an apple with an IoD<0.5. 3. Results and discussion 3.1.
    Reflectance analysis Table 1 shows the reflectance analysis results for both trees
    used in the training dataset. Mean apparent reflectance values of 28.9%, 29.1%,
    and 44.3% were obtained for leaves, trunks, and Fuji apples, respectively. These
    results indicate that the reflectance is higher than other tree elements. Hence,
    this characteristic will be used as a valuable feature for Fuji apple detection.
    Note that these results were obtained using a LiDAR system operating with a laser
    source at 905 nm wavelength. Further studies should be carried out to ensure that
    the present methodology could be extended to other laser systems (operating at
    different wavelengths) and other fruit varieties or branching structures. Table
    1. Reflectance analysis: The mean apparent reflectance and standard deviation
    of different elements in an apple orchard. Tree Elements mean (R) [%] std (R)
    [%] T1 Leaves 29.23 13.57 T2 Leaves 28.69 13.88 T1 Trunks 29.67 14.83 T2 Trunks
    28.52 15.41 T1 Apples 43.59 16.81 T2 Apples 45.10 16.78 The results of this analysis
    are the basis of the proposed detection algorithm, with reflectance being the
    principal feature used in the segmentation step. Although Fuji apples have higher
    reflectance than leaves and trunks at 905 nm, the standard deviation is high enough
    to create overlap between classes (Fig. 7b). In order to find the optimal threshold,
    Rth, that will remove the points corresponding to leaves, branches, and trunks,
    a performance evaluation of the detection algorithm (Section 2.3) was carried
    out using different reflectance thresholds. Figure 7a plots the evolution of precision,
    recall, and F1-score metrics, computed before applying the false positive removal
    step, under different reflectance thresholds, Rth. The best results were obtained
    with an Rth = 60%, resulting in an F1-score = 82.16% for the training dataset.
    Figure 7b shows the reflectance distributions for leaves, trunks, and apples.
    As can be seen, most of the 3D points belonging to apples were below the threshold
    value. This is because our restrictive threshold minimises the false positives
    of leaves and trunks being selected as apples. Furthermore, omitting points as
    apple is not as critical as having a few apple points in a cluster, which are
    sufficient for detection. 3.2. Step-by-step algorithm performance evaluation This
    section presents a qualitative and quantitative evaluation of the different steps
    and methods implemented in this paper. Regarding the qualitative evaluation, Fig.
    8 illustrates the evolution after each processing step. First, Fig. 8a shows an
    RGB image of one of the trees, which is incorporated to assist in visualization,
    but was not used in the algorithm. Figure 8b renders the 3D model obtained with
    the MTLS. The colour scale indicates the reflectance of each point, where blue
    corresponds to low values and red implies high reflectance. It is evident from
    this representation how Fuji apples exhibit higher reflectance than other tree
    elements. Figure 8c shows the results after applying the reflectance threshold,
    Rth, to the original point cloud. In this step, many of the leaf and trunk points
    were removed. Once the sparse outlier removal is applied (Fig. 8d), zones with
    low point density were removed, leaving only groups of points which are candidates
    for apple detection. Figure 8e illustrates the segmentation output, which terminates
    the clustering of connected points. This result has clusters with one apple (red,
    orange, blue, and purple), clusters with more than one apple (green), and false
    positives (grey). The apple detection algorithm ends by splitting clusters with
    more than one apple and removing false positives. The final result is presented
    in Fig. 8f. Download : Download high-res image (798KB) Download : Download full-size
    image Fig. 8. Illustration of the different processing steps (tree 2). a) RGB
    image. b) Point cloud obtained with the MTLS. c) Point cloud after applying the
    reflectance threshold. d) Sparse outlier removal. e) Connected component labelling
    (DBSCAN). f) Apple separation and false positive removal. For better visualization
    purposes, in b), scale ranges from 0 (blue) to 100 (red), while in c) and d) the
    scale ranges from 60 (blue) to 100 (red). Table 2 presents the results of the
    test dataset for each step and method implemented. The first row shows the results
    after point cloud segmentation (Section 2.3.1); rows 2–4 indicate the results
    obtained when applying the splitting techniques presented in Section 2.3.2; and
    the last three rows present the final results after removing the false positives
    detected (Section 2.3.3). Table 2. Performance assessment of the different implemented
    steps and methods: point cloud segmentation (S); apple separation methods 1, 2,
    and 3 (M1, M2, and M3, respectively); and false positive removal step (FPr). Results
    include information from the test dataset (tree 3). Method Localization Identification
    Processing Time [s] SuccessL [%] FDRL [%] SuccessID [%] FDRID [%] S 87.5 11.9
    73.5 13.8 11.0 S + M1 87.5 20.7 85.3 26.1 11.9 S + M2 87.5 15.6 82.4 17.0 11.0
    S + M3 87.5 16.8 84.6 20.7 12.0 S + M1 + FPr 87.5 12.5 85.3 18.3 12.1 S + M2 +
    FPr 87.5 9.8 82.4 10.4 11.1 S + M3 + FPr 86.8 11.3 83.8 14.9 12.4 The localization
    success values obtained after point cloud segmentation (before apple separation
    and false positive removal) are slightly higher than 87% (first row). These results
    are similar to other methodologies using colour cameras (Gongal et al., 2015).
    The identification success presents significantly lower results (∼73%), because
    of some detections containing more than one apple. Methods 1, 2, and 3 therefore
    were applied, in order to split these clusters, methods (Section 2.3.2). As a
    result, the identification success increased by more than 8% (rows 2 to 4), although
    the number of false positives also increased due to multi-detections. Method 1
    performed best in terms of increasing the identification success (+11%), but also
    generated more multi-detections. Method 2 increased identification success by
    more than 8%, while false positives only increased 3%. The results of method 3
    are a trade-off between the previous two methods. Since localization success performs
    an evaluation on a point-by-point basis, applying separation methods does not
    vary the results of this metric. When applying false positive removal (rows 5
    to 7), it is observed that the false detection rate fell by more than 5%, while
    the localization and identification successes were not affected (except for method
    3, with a decrease of less than 1%). The best results were obtained by combining
    method 2 with false positive removal, resulting in a lower number of false positives,
    without affecting the performance of the apple detection algorithm. The processing
    times indicated in Table 2 correspond to processing the data with a 64-bit operating
    system, with 8 GB of RAM and an Intel ® Core(TM) i7-4500U processor (1.80 GHz,
    boosted to 2.40 GHz). Although method 2 was slightly more efficient than the other
    two approaches, no significant differences were observed in the processing time.
    This is because the most computationally intensive operation is in the DBSCAN
    clustering algorithm (9.1 s), which is part of the segmentation step included
    in all methods. 3.3. Detection results Table 3 shows the apple detection algorithm,
    as evaluated individually for each tree. These results were generated by applying
    the point cloud segmentation, followed by an apple separation using method 2,
    and removing false positives using the condition expressed in Section 2.3. The
    detection rate is similar for processed trees despite being slightly better for
    tree 1 and 3. A localization success of 87.5% with a 9.8% of FDRL, an identification
    success of 82.4% with a 10.4% of FDRID, and an F1-score of 85.8% were obtained
    using the test dataset. These results are comparable with those obtained with
    other methodologies used in the state of the art. So far, the best detection rates
    have been reported with image processing, obtaining accuracies of between 80%
    and 85% using colour features (Gongal et al., 2015), and up to 86% of recall using
    deep learning (Bargoti & Underwood, 2017). However, the vision systems used in
    harvesting robots in orchard environments report a mean value of 80% in localization
    success and a mean value of 70% in identification success (Bac et al., 2014).
    Although it is difficult to compare the research found in the state of the art
    review, given that they are evaluated with different datasets, the methodology
    presented in this paper yields similar detection rates to previous work based
    on colour cameras, with the advantage that LiDAR-based measurements are not affected
    by illumination conditions. Furthermore, the location of each detected apple is
    obtained directly, which makes the presented system very interesting for autonomous
    harvesting or fruit load assessment for yield mapping applications. Table 3. Apple
    detection assessment using method 2. Trees 1 and 2 were used as training dataset
    and tree 3 as test dataset. GTfield corresponds to the number of apples hand-counted
    in field, while GTlabels corresponds to the number of apples labelled in data.
    Other metrics are defined in Section 2.4. Tree GTfield GTlabels Localization Identification
    F1-score TPL FPL SuccessL FDRL TPID FPID SuccessID FDRID Tree 1 139 133 116 12
    87.2% 9.4% 110 16 82.7% 12.7% 0.849 Tree 2 145 138 118 13 85.5% 9.9% 109 15 79.0%
    12.1% 0.832 Tree 3 139 136 119 13 87.5% 9.8% 112 13 82.4% 10.4% 0.858 Regarding
    the computational cost, Table 4 includes the inference time, processing each tree
    separately, and all trees combined. The number of points of each test is also
    reported. As expected, the computational time increases with the number of points
    processed. Results show that processing trees individually is much more efficient
    than processing all trees at once. This is because the average run time complexity
    of DBSCAN is not linear with the number of points (Ester et al., 1996), resulting
    in higher efficiency when processing small point clouds. Table 4. Computational
    cost according to the number of points in the point cloud. Tree Nº of points Processing
    Time [s] Tree 1 438.260 8.0 Tree 2 460.847 9.6 Tree 3 526.136 11.2 Tree 1 + 2+3
    1.425.243 68.8 4. Conclusions This work presents a new methodology for Fuji apple
    detection and localization in real commercial orchard environments using a LiDAR-based
    mobile terrestrial laser scanner (MTLS) with reflectance capabilities. A reflectance
    analysis of the different apple tree elements was carried out, which showed that
    apples exhibit a higher reflectance than leaves and trunks at the 905 nm laser
    wavelength; we therefore conclude that this characteristic is a valuable feature
    for apple detection. An apple detection algorithm, suitable for dealing with point
    clouds obtained with an MTLS, was subsequently developed and tested on three apple
    trees from a commercial apple orchard. The algorithm is divided into three steps:
    (1) removal of points corresponding to leaves and trunk and clustering the remaining
    points with a connected component labelling, (2) identification and splitting
    of clusters that contain more than one apple, and (3) false positive reduction.
    In order to predict the number of apples grouped in a cluster, three different
    methods were proposed: template matching, decision tree, and a combination of
    both approaches. The best results were achieved by applying a decision tree, resulting
    in a localization success of 87.5% with a 9.8% false detection rate, an identification
    success of 82.4% with a 10.4% false detection rate, and an F1-score of 85.8% in
    the test dataset. These outcomes represent an advance in the fruit detection field,
    since the results are comparable with those from colour (RGB) camera systems used
    in past efforts; however, the proposed LiDAR-based has the additional advantages
    that measurements are not affected by illumination conditions and that the method
    directly provides 3D fruit location information. An important limitation of this
    work is the small dataset. A larger dataset could allow the parameters to be learnt
    automatically (instead of being manually selected), thereby obtaining an algorithm
    that could better generalise with new data. Future efforts should include an analysis
    of fruit reflectance under different laser wavelengths, the extension of the dataset
    to other fruit varieties and species, and the application of machine learning
    algorithms in larger datasets. Acknowledgements This work was partly funded by
    the Secretaria d’Universitats i Recerca del Departament d’Empresa i Coneixement
    de la Generalitat de Catalunya (grant 2017 SGR 646), the Spanish Ministry of Economy
    and Competitiveness (projects AGL2013-48297-C2-2-R and MALEGRA, TEC2016-75976-R)
    and the Spanish Ministry of Science, Innovation and Universities (project RTI2018-094222-B-I00).
    The Spanish Ministry of Education is thanked for Mr. J. Gené’s pre-doctoral fellowships
    (FPU15/03355). The work of Jordi Llorens was supported by Spanish Ministry of
    Economy, Industry and Competitiveness through a postdoctoral position named Juan
    de la Cierva Incorporación (JDCI-2016-29464_N18003). We would also like to thank
    CONICYT/FONDECYT for grant 1171431 and CONICYT FB0008. Nufri (especially Santiago
    Salamero and Oriol Morreres) and Vicens Maquinària Agrícola S.A. are also thanked
    for their support during the data acquisition. Appendix A. Parameter values and
    feature analysis Table A1 presents the values set for each parameter used in the
    algorithm. Parameter Rth is used in the segmentation step. See more details about
    these parameters in Section 2.3.1. Parameters with sub-index kj refer to the thresholds
    used in Section 2.3.2 - method 2 and were selected after analysing the graphical
    representation of cluster features shown in Fig. A1. Parameters with sub-index
    FP correspond to the thresholds used to remove false positives (Section 2.3.3)
    and were selected after analysing the graphical representation of detection features
    shown in Fig. A2. Table A1. Parameter values used to detect apples in the presented
    dataset. The first five parameters were used during the point cloud segmentation
    step. Parameters sub-indexed with letter K correspond to thresholds used in the
    apple separation step. Parameters sub-indexed with letters FP were used in the
    false positive removal step. Symbol Value Units Rth 60 % k 20 Points α 0 – minPts
    15 Points ε 0.03 m Vk1 1.5·10-4 m3 Pk1 85 Points k1 67.5 % k1 0.8 – Pk2 400 Points
    k2 0.6 – Vk2 1.2·10-3 m3 Vk3 1.6·10-3 m3 FP 0.46 – FP 65.25 % VFP 10–3 m3 Download
    : Download high-res image (339KB) Download : Download full-size image Fig. A1.
    Graphical representation of cluster features. The features analysed are the geometric
    parameter, Ψ, and the number of points (left), and the mean reflectance and the
    cluster volume (right). Clusters with one apple are represented in green squares;
    clusters with two apples are represented in blue diamonds; clusters with three
    apples in magenta asterisks; and clusters with four apples or more in black crosses.
    Yellow, red and blue lines correspond to K1, K2 and K3 thresholds, respectively.
    This analysis was performed on the training data set (Trees 1 and 2) and was used
    to set the thresholds explained in Section 2.3.2 - method 2. Download : Download
    high-res image (371KB) Download : Download full-size image Fig. A2. Graphical
    representation of detection features. The features analysed are the geometric
    parameter, Ψ, the mean reflectance and the cluster volume. False positives (FP)
    are represented by red crosses; true positives are represented by blue circles.
    Horizontal and vertical lines show the thresholds used to remove FP. This analysis
    was performed on the training data set (Trees 1 and 2) and was used to set the
    thresholds explained in Section 2.3.3. References Auat Cheein and Carelli, 2013
    F.A. Auat Cheein, R. Carelli Agricultural robotics: Unmanned robotic service units
    in agricultural tasks IEEE Industrial Electronic Magazine, 7 (2013), pp. 48-58,
    10.1109/MIE.2013.2252957 View in ScopusGoogle Scholar Auat Cheein et al., 2017
    F. Auat Cheein, M. Torres-Torriti, N.B. Hopfenblatt, Á.J. Prado, D. Calabi Agricultural
    service unit motion planning under harvesting scheduling and terrain constraints
    Journal of Field Robotics, 34 (2017), pp. 1531-1542, 10.1002/rob.21738 View in
    ScopusGoogle Scholar Bac et al., 2014 C.W. Bac, E.J. Van Henten, J. Hemming, Y.
    Edan Harvesting robots for high-value crops: State-of-the-art review and challenges
    ahead Journal of Field Robotics, 31 (2014), 10.1002/rob.21525 Google Scholar Bargoti
    and Underwood, 2017 S. Bargoti, J. Underwood Deep fruit detection in orchards.
    2017 IEEE International Conference on Robotics and Automation (2017), pp. 3626-3633
    View in ScopusGoogle Scholar Barnea et al., 2016 E. Barnea, R. Mairon, O. Ben-Shahar
    Colour-agnostic shape-based 3D fruit detection for crop harvesting robots Biosystems
    Engineering, 146 (2016), pp. 57-70, 10.1016/j.biosystemseng.2016.01.013 View PDFView
    articleView in ScopusGoogle Scholar Bechar and Vigneault, 2016 A. Bechar, C. Vigneault
    Agricultural robots for field operations: Concepts and components Biosystems Engineering,
    149 (2016), pp. 94-111, 10.1016/j.biosystemseng.2016.06.014 View PDFView articleView
    in ScopusGoogle Scholar Bechar and Vigneault, 2017 A. Bechar, C. Vigneault Agricultural
    robots for field operations. Part 2: Operations and systems Biosystems Engineering,
    153 (2017), pp. 110-128, 10.1016/j.biosystemseng.2016.11.004 View PDFView articleView
    in ScopusGoogle Scholar Brunelli, 2009 R. Brunelli Template matching techniques
    in computer vision - theory and practice John Wiley & Sons (2009) Google Scholar
    Bulanon et al., 2008 D.M. Bulanon, T.F. Burks, V. Alchanatis Study on temporal
    variation in citrus canopy using thermal imaging for citrus fruit detection Biosystems
    Engineering, 101 (2008), pp. 161-171, 10.1016/j.biosystemseng.2008.08.002 View
    PDFView articleView in ScopusGoogle Scholar Bulanon et al., 2009 D.M. Bulanon,
    T.F. Burks, V. Alchanatis Image fusion of visible and thermal images for fruit
    detection Biosystems Engineering, 103 (2009), pp. 12-22, 10.1016/j.biosystemseng.2009.02.009
    View PDFView articleView in ScopusGoogle Scholar Cariou et al., 2009 C. Cariou,
    R. Lenain, B. Thuilot, M. Berducat Automatic guidance of a four-wheel-steering
    mobile robot for accurate field operations Journal of Field Robotics (2009), 10.1002/rob.20282
    Google Scholar Chaivivatrakul and Dailey, 2014 S. Chaivivatrakul, M.N. Dailey
    Texture-based fruit detection Precision Agriculture, 15 (2014), pp. 662-683, 10.1007/s11119-014-9361-x
    View in ScopusGoogle Scholar De-An et al., 2011 Z. De-An, L. Jidong, J. Wei, Z.
    Ying, C. Yu Design and control of an apple harvesting robot Biosystems Engineering,
    110 (2011), pp. 112-122, 10.1016/j.biosystemseng.2011.07.005 View PDFView articleView
    in ScopusGoogle Scholar Ester et al., 1996 M. Ester, H.P. Kriegel, J. Sander,
    X. Xu A density-based algorithm for discovering clusters in large spatial databases
    with noise Proceeding of 2nd International Conference Knowledge in Discovery Data
    Mining, 96 (34) (1996), pp. 226-231 doi:10.1.1.71.1980 Google Scholar Foglia and
    Reina, 2006 M.M. Foglia, G. Reina Agricultural robot for radicchio harvesting
    Journal of Field Robotics (2006), 10.1002/rob.20131 Google Scholar Font et al.,
    2014 D. Font, T. Pallejà, M. Tresanchez, D. Runcan, J. Moreno, D. Martínez, et
    al. A proposal for automatic fruit harvesting by combining a low cost stereovision
    camera and a robotic arm Sensors (Switzerland), 14 (2014), pp. 11557-11579, 10.3390/s140711557
    View in ScopusGoogle Scholar Gongal et al., 2015 A. Gongal, S. Amatya, M. Karkee,
    Q. Zhang, K. Lewis Sensors and systems for fruit detection and localization: A
    review Computers and Electronics in Agriculture, 116 (2015), pp. 8-19, 10.1016/j.compag.2015.05.021
    View PDFView articleView in ScopusGoogle Scholar Gongal et al., 2016 A. Gongal,
    A. Silwal, S. Amatya, M. Karkee, Q. Zhang, K. Lewis Apple crop-load estimation
    with over-the-row machine vision system Computers and Electronics in Agriculture,
    120 (2016), pp. 26-35, 10.1016/j.compag.2015.10.022 View PDFView articleView in
    ScopusGoogle Scholar Gong et al., 2013 A. Gong, J. Yu, Y. He, Z. Qiu Erratum to
    “Citrus yield estimation based on images processed by an Android mobile phone”
    Biosystems Engineering, 116 (2013), pp. 111-112, 10.1016/j.biosystemseng.2013.07.004
    View PDFView articleView in ScopusGoogle Scholar Jain, 2010 A.K. Jain Data clustering:
    50 years beyond K-means Pattern Recognition Letters (2010), 10.1016/j.patrec.2009.09.011
    Google Scholar Jiménez et al., 2000 A.R. Jiménez, R. Ceres, J.L. Pons A vision
    system based on a laser range-finder applied to robotic fruit harvesting Machine
    Vision and Applications, 11 (2000), pp. 321-329, 10.1007/s001380050117 View in
    ScopusGoogle Scholar Jiménez et al., 1999 A.R. Jiménez, A.K. Jain, R. Ceres, J.L.
    Pons Automatic fruit recognition: A survey and new results using range/attenuation
    images Pattern Recognition, 32 (1999), pp. 1719-1736, 10.1016/S0031-3203(98)00170-8
    View PDFView articleView in ScopusGoogle Scholar Kaasalainen et al., 2011 S. Kaasalainen,
    A. Jaakkola, M. Kaasalainen, A. Krooks, A. Kukko Analysis of incidence angle and
    distance effects on terrestrial laser scanner intensity: Search for correction
    methods Remote Sensing (2011), 10.3390/rs3102207 Google Scholar Kukko et al.,
    2008 A. Kukko, S. Kaasalainen, P. Litkey Effect of incidence angle on laser scanner
    intensity and surface data Applied Optics (2008), 10.1364/ao.47.000986 Google
    Scholar Kurtulmus et al., 2014 F. Kurtulmus, W.S. Lee, A. Vardar Immature peach
    detection in colour images acquired in natural illumination conditions using statistical
    classifiers and neural network Precision Agriculture, 15 (2014), pp. 57-79, 10.1007/s11119-013-9323-8
    View in ScopusGoogle Scholar Lak et al., 2010 M.B. Lak, S. Minaei, J. Amiriparian,
    B. Beheshti Apple fruits recognition under natural luminance using machine vision
    Advance Journal of Food Science and Technology, 2 (2010), pp. 325-327 View in
    ScopusGoogle Scholar Linker, 2017 R. Linker A procedure for estimating the number
    of green mature apples in night-time orchard images using light distribution and
    its application to yield estimation Precision Agriculture, 18 (2017), pp. 59-75,
    10.1007/s11119-016-9467-4 View in ScopusGoogle Scholar Linker et al., 2012 R.
    Linker, O. Cohen, A. Naor Determination of the number of green apples in RGB images
    recorded in orchards Computers and Electronics in Agriculture, 81 (2012), pp.
    45-57, 10.1016/j.compag.2011.11.007 View PDFView articleView in ScopusGoogle Scholar
    Liu et al., 2016 X. Liu, D. Zhao, W. Jia, C. Ruan, S. Tang, T. Shen A method of
    segmenting apples at night based on color and position information Computers and
    Electronics in Agriculture, 122 (2016), pp. 118-123, 10.1016/j.compag.2016.01.023
    View PDFView articleView in ScopusGoogle Scholar Maldonado and Barbosa, 2016 W.
    Maldonado, J.C. Barbosa Automatic green fruit counting in orange trees using digital
    images Computers and Electronics in Agriculture, 127 (2016), pp. 572-581, 10.1016/j.compag.2016.07.023
    View PDFView articleView in ScopusGoogle Scholar Meier, 2001 U. Meier Growth stages
    of mono- and dicotyledonous plants BBCH Monograph (2001), 10.5073/bbch0515 Google
    Scholar Nguyen et al., 2016 T.T. Nguyen, K. Vandevoorde, N. Wouters, E. Kayacan,
    J.G. De Baerdemaeker, W. Saeys Detection of red and bicoloured apples on tree
    with an RGB-D camera Biosystems Engineering, 146 (2016), pp. 33-44, 10.1016/j.biosystemseng.2016.01.007
    View PDFView articleView in ScopusGoogle Scholar Okamoto and Lee, 2009 H. Okamoto,
    W.S. Lee Green citrus detection using hyperspectral imaging Computers and Electronics
    in Agriculture, 66 (2009), pp. 201-208, 10.1016/j.compag.2009.02.004 View PDFView
    articleView in ScopusGoogle Scholar Payne et al., 2014 A. Payne, K. Walsh, P.
    Subedi, D. Jarvis Estimating mango crop yield using image analysis using fruit
    at “stone hardening” stage and night time imaging Computers and Electronics in
    Agriculture, 100 (2014), pp. 160-167, 10.1016/j.compag.2013.11.011 View PDFView
    articleView in ScopusGoogle Scholar Qureshi et al., 2017 W.S. Qureshi, A. Payne,
    K.B. Walsh, R. Linker, O. Cohen, M.N. Dailey Machine vision for counting fruit
    on mango tree canopies Precision Agriculture, 18 (2017), pp. 224-244, 10.1007/s11119-016-9458-5
    View in ScopusGoogle Scholar Ray, 1994 T.W. Ray A FAQ on vegetation in remote
    sensing Div. of Geological and Planetary Sciences California Institute of Technology,
    California (1994) Google Scholar Rosell-Polo et al., 2015 J.R. Rosell-Polo, F.A.
    Cheein, E. Gregorio, D. Andújar, L. Puigdomènech, J. Masip, et al. Advances in
    structured light sensors applications in precision agriculture and livestock farming
    Advances in Agronomy (2015), 10.1016/bs.agron.2015.05.002 Google Scholar Rosell-Polo
    et al., 2017 J.R. Rosell-Polo, E. Gregorio, J. Gene, J. Llorens, X. Torrent, J.
    Arno, et al. Kinect v2 sensor-based mobile terrestrial laser scanner for agricultural
    outdoor applications IEEE/ASME Transactions in Mechatronics (2017), 10.1109/TMECH.2017.2663436
    1–1 Google Scholar Rusu et al., 2008 R.B. Rusu, Z.C. Marton, N. Blodow, M. Dolha,
    M. Beetz Towards 3D Point cloud based object maps for household environments Robotics
    and Autonomous Systems, 56 (2008), pp. 927-941, 10.1016/j.robot.2008.08.005 View
    PDFView articleView in ScopusGoogle Scholar Safren et al., 2007 O. Safren, V.
    Alchanatis, V. Ostrovsky, O. Levi Detection of green apples in hyperspectral images
    of apple-tree foliage using machine Vision, 50 (2007), pp. 2303-2313 View in ScopusGoogle
    Scholar Sa et al., 2016 I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez, C. McCool
    DeepFruits: A fruit detection system using deep neural networks Sensors, 16 (2016),
    p. 1222, 10.3390/s16081222 View in ScopusGoogle Scholar Siegel et al., 2014 K.R.
    Siegel, M.K. Ali, A. Srinivasiah, R.A. Nugent, K.M.V. Narayan Do we produce enough
    fruits and vegetables to meet global health need? PLoS One, 9 (2014), 10.1371/journal.pone.0104059
    Google Scholar Si et al., 2015 Y. Si, G. Liu, J. Feng Location of apples in trees
    using stereoscopic vision Computers and Electronics in Agriculture, 112 (2015),
    pp. 68-74, 10.1016/j.compag.2015.01.010 View PDFView articleView in ScopusGoogle
    Scholar Stajnko et al., 2004 D. Stajnko, M. Lakota, M. Hocevar Estimation of number
    and diameter of apple fruits in an orchard during the growing season by thermal
    imaging Computers and Electronics in Agriculture, 42 (2004), pp. 31-42, 10.1016/S0168-1699(03)00086-3
    View PDFView articleView in ScopusGoogle Scholar Stein et al., 2016 M. Stein,
    S. Bargoti, J. Underwood Image based mango fruit detection, localisation and yield
    estimation using multiple view geometry Sensors, 16 (2016), p. 1915, 10.3390/s16111915
    View in ScopusGoogle Scholar Velodyne, 2016 L. Velodyne VLP-16 in VLP-16 manual:
    User''s manual and programming guide Velodyne LiDAR (2016) Google Scholar Wachs
    et al., 2010 J.P. Wachs, H.I. Stern, T. Burks, V. Alchanatis Low and high-level
    visual feature-based apple detection from multi-modal images Precision Agriculture,
    11 (2010), pp. 717-735, 10.1007/s11119-010-9198-x View in ScopusGoogle Scholar
    Wehr and Lohr, 1999 A. Wehr, U. Lohr Airborne laser scanning - an introduction
    and overview ISPRS Journal of Photogrammetry and Remote Sensing (1999), 10.1016/S0924-2716(99)00011-8
    Google Scholar Xiang et al., 2014 R. Xiang, H. Jiang, Y. Ying Recognition of clustered
    tomatoes based on binocular stereo vision Computers and Electronics in Agriculture,
    106 (2014), pp. 75-90, 10.1016/j.compag.2014.05.006 View PDFView articleView in
    ScopusGoogle Scholar Zhang et al., 2015 B. Zhang, W. Huang, C. Wang, L. Gong,
    C. Zhao, C. Liu, et al. Computer vision recognition of stem and calyx in apples
    using near-infrared linear-array structured light and 3D reconstruction Biosystems
    Engineering, 139 (2015), pp. 25-34, 10.1016/j.biosystemseng.2015.07.011 View PDFView
    articleView in ScopusGoogle Scholar Zhang and Pierce, 2016 Q. Zhang, F.J. Pierce
    Agricultural automation: Fundamentals and practices CRC Press (2016) Google Scholar
    Zhao et al., 2016a Y. Zhao, L. Gong, Y. Huang, C. Liu A review of key techniques
    of vision-based control for harvesting robot Computers and Electronics in Agriculture,
    127 (2016), pp. 311-323, 10.1016/j.compag.2016.06.022 View PDFView articleView
    in ScopusGoogle Scholar Zhao et al., 2016b C. Zhao, W.S. Lee, D. He Immature green
    citrus detection based on colour feature and sum of absolute transformed difference
    (SATD) using colour images in the citrus grove Computers and Electronics in Agriculture,
    124 (2016), pp. 243-253, 10.1016/j.compag.2016.04.009 View PDFView articleView
    in ScopusGoogle Scholar Cited by (76) Fruit sizing using AI: A review of methods
    and challenges 2023, Postharvest Biology and Technology Show abstract Three-dimensional
    quantification of apple phenotypic traits based on deep learning instance segmentation
    2023, Computers and Electronics in Agriculture Show abstract Robot-assisted mobile
    scanning for automated 3D reconstruction and point cloud semantic segmentation
    of building interiors 2023, Automation in Construction Show abstract Topological
    and spatial analysis of within-tree fruiting characteristics for walnut trees
    2023, Scientia Horticulturae Show abstract Effects of laser scanner quality and
    tractor speed to characterise apple tree canopies 2023, Smart Agricultural Technology
    Show abstract Real-time detection of street tree crowns using mobile laser scanning
    based on pointwise classification 2023, Biosystems Engineering Show abstract View
    all citing articles on Scopus View Abstract © 2019 IAgrE. Published by Elsevier
    Ltd. All rights reserved. Recommended articles Detection of red and bicoloured
    apples on tree with an RGB-D camera Biosystems Engineering, Volume 146, 2016,
    pp. 33-44 Tien Thanh Nguyen, …, Wouter Saeys View PDF Analyzing and overcoming
    the effects of GNSS error on LiDAR based orchard parameters estimation Computers
    and Electronics in Agriculture, Volume 170, 2020, Article 105255 Javier Guevara,
    …, Eduard Gregorio View PDF Automatic recognition of juicy peaches on trees based
    on 3D contour features and colour data Biosystems Engineering, Volume 188, 2019,
    pp. 1-13 Gang Wu, …, Jianwei Qin View PDF Show 3 more articles Article Metrics
    Citations Citation Indexes: 69 Captures Readers: 119 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Biosystems engineering
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Fruit detection in an apple orchard using a mobile terrestrial laser scanner
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3389/fpls.2021.611940
  analysis: '>'
  authors:
  - Abbas Atefi
  - Yufeng Ge
  - Santosh K. Pitla
  - James C. Schnable
  citation_count: 50
  full_citation: '>'
  full_text: '>

    REVIEW

    published: 25 June 2021

    doi: 10.3389/fpls.2021.611940

    Edited by:

    Ankush Prashar,

    Newcastle University, United Kingdom

    Reviewed by:

    Simon Pearson,

    University of Lincoln, United Kingdom

    Katja Herzog,

    Institut für Rebenzüchtung, Julius

    Kühn-Institut, Germany

    Rui Xu,

    University of Georgia, Georgia

    *Correspondence:

    Yufeng Ge

    yge2@unl.edu

    Specialty section:

    This article was submitted to

    Technical Advances in Plant Science,

    a section of the journal

    Frontiers in Plant Science

    Received: 29 September 2020

    Accepted: 14 May 2021

    Published: 25 June 2021

    Citation:

    Ateﬁ A, Ge Y, Pitla S and

    Schnable J (2021) Robotic

    Technologies for High-Throughput

    Plant Phenotyping: Contemporary

    Reviews and Future Perspectives.

    Front. Plant Sci. 12:611940.

    doi: 10.3389/fpls.2021.611940

    Robotic Technologies for

    High-Throughput Plant Phenotyping:

    Contemporary Reviews and Future

    Perspectives

    Abbas Ateﬁ1, Yufeng Ge1*, Santosh Pitla1 and James Schnable2

    1 Department of Biological Systems Engineering, University of Nebraska–Lincoln,
    Lincoln, NE, United States, 2 Department

    of Agronomy and Horticulture, University of Nebraska–Lincoln, Lincoln, NE, United
    States

    Phenotyping plants is an essential component of any effort to develop new crop

    varieties. As plant breeders seek to increase crop productivity and produce more

    food for the future, the amount of phenotype information they require will also

    increase. Traditional plant phenotyping relying on manual measurement is laborious,

    time-consuming, error-prone, and costly. Plant phenotyping robots have emerged
    as

    a high-throughput technology to measure morphological, chemical and physiological

    properties of large number of plants. Several robotic systems have been developed
    to

    fulﬁll different phenotyping missions. In particular, robotic phenotyping has
    the potential

    to enable efﬁcient monitoring of changes in plant traits over time in both controlled

    environments and in the ﬁeld. The operation of these robots can be challenging
    as a

    result of the dynamic nature of plants and the agricultural environments. Here
    we discuss

    developments in phenotyping robots, and the challenges which have been overcome

    and others which remain outstanding. In addition, some perspective applications
    of the

    phenotyping robots are also presented. We optimistically anticipate that autonomous

    and robotic systems will make great leaps forward in the next 10 years to advance
    the

    plant phenotyping research into a new era.

    Keywords: autonomous robotic technology, agricultural robotics, phenotyping robot,
    high-throughput plant

    phenotyping, computer vision

    INTRODUCTION: ROBOTIC TECHNOLOGY IS VITAL FOR

    HIGH-THROUGHPUT PLANT PHENOTYPING

    Agriculture must produce enough food, feed, ﬁber, fuel, and ﬁne chemicals in next
    century to meet

    the needs of a growing population worldwide. Agriculture will face multiple challenges
    to satisfy

    these growing human needs while at the same time dealing with the climate change,
    increased

    risk for drought and high temperatures, heavy rains, and degradation of arable
    land and depleting

    water resources. Plant breeders seek to address these challenges by developing
    high yielding and

    stress-tolerance crop varieties adapted to future climate conditions and resistant
    to new pests and

    diseases (Fischer, 2009; Furbank and Tester, 2011; Rahaman et al., 2015). However,
    the rate of crop

    productivity needs to be increased to meet projected future demands. Advances
    in DNA sequencing

    and genotyping technologies have relieved a major bottleneck in both marker assisted
    selection and

    Frontiers in Plant Science | www.frontiersin.org

    1

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    genomic prediction assisted plant breeding, the determination of

    genetic information for newly developed plant varieties. Dense

    genetic marker information can aid in the eﬃciency and speed

    of the breeding process (Wang et al., 2018; Happ et al., 2019;

    Moeinizade et al., 2019). However, large and high quality plant

    phenotypic datasets are also necessary to dissect the genetic

    basis of quantitative traits which are related to growth, yield

    and adaptation to stresses (McMullen et al., 2009; Jannink et al.,

    2010; Phillips, 2010; Fahlgren et al., 2015; Tripodi et al., 2018;

    Chawade et al., 2019).

    Plant

    phenotyping

    is

    the

    quantitative

    and

    qualitative

    assessment of the traits of a given plant or plant variety in a given

    environment. These traits include the biochemistry, physiology,

    morphology, structure, and performance of the plants at various

    organizational scales. Plant traits are determined by both genetic

    and environmental factors as well as non-additive interactions

    between the two. In addition, variation in one phenotypic trait

    (e.g., leaf characteristics) can result in variation in other plant

    traits (e.g., plant biomass or yield). Therefore, phenotyping large

    numbers of plant varieties for multiple traits across multiple

    environments is an essential task for plant breeders as they work

    to select desirable genotypes and identify genetic variants which

    provide optimal performance in diverse and changing target

    environments (Granier and Tardieu, 2009; Dhondt et al., 2013; Li

    et al., 2014; Foix et al., 2015; Walter et al., 2015; Costa et al., 2019;

    Pieruschka and Schurr, 2019).

    Traditionally plant traits are quantiﬁed using manual and

    destructive sampling methods. These methods are usually labor-

    intensive, time-consuming, and costly. In addition, manual

    sampling and analysis protocols generally involve many steps

    requiring human intervention, with each step increasing the

    chances of introducing mistakes. Often the plant and its organ

    is cut at ﬁxed time points or at particular phenological stages in

    order to measure its phenotypic traits. This method destroys or

    damages the plant at one time point, disallowing the temporal

    examination of the traits for individual plants during the growing

    season. For example, yield measurement (such as plant biomass

    and grain weight) is invasive and more labor intensive compare

    to the measurement of plant height and leaf chlorophyll content

    (measured by a handheld sensor). As a result of the labor and

    resource intensive nature of plant phenotyping, many plant

    breeders rely solely on a single measurement most critical to their

    eﬀorts: yield. However, yield is considered as one of the most

    weakly inherited phenotypes in crop breeding (Richards et al.,

    2010; Furbank and Tester, 2011). The measurement of other traits

    in addition to yield can increase the accuracy with which yield

    can be predicted across diverse environments. Enabling high-

    throughput and non-destructive measurements of plant traits

    from large numbers of plants in multiple environments would

    therefore lead to increases in breeding eﬃciency (McMullen

    et al., 2009; Andrade-Sanchez et al., 2013; Fahlgren et al., 2015;

    Foix et al., 2018; Vijayarangan et al., 2018; Ge et al., 2019;

    Hassanijalilian et al., 2020a).

    In recent years, high-throughput systems and workﬂows have

    been developed to monitor and measure large populations of

    plants rapidly in both greenhouse and ﬁeld environments. These

    systems combine modern sensing and imaging modalities with

    the sensor deployment technologies (including conveyor belts,

    ground and aerial vehicles, and ﬁeld gantries) to enable fast

    measurement and wide area coverage (Busemeyer et al., 2013;

    Ge et al., 2016; Virlet et al., 2017; Hassan et al., 2019). Although

    not fully autonomous, these systems represent the state of the art

    in modern plant phenotyping with several advantages over the

    traditional, manually collected phenotypic traits.

    Robotic systems have been playing a more signiﬁcant role

    in modern agriculture and considered as an integral part of

    precision agriculture or digital farming (Wolfert et al., 2017;

    Chlingaryan et al., 2018; Zhang et al., 2019; Hassanijalilian et al.,

    2020b; Jin et al., 2020; Pandey et al., 2021). The robots are

    fully autonomous and do not need experienced operators to

    accomplish farming tasks. This is the biggest advantage of the

    robots compared to tractor-based systems (White et al., 2012).

    Autonomous robots have taken over a wide range of farming

    operations including harvesting [Arad et al., 2020 (sweet pepper);

    Hemming et al., 2014 (sweet pepper); Lili et al., 2017 (tomato);

    van Henten et al., 2002 (cucumber); Hayashi et al., 2010; Xiong

    et al., 2020 (strawberry); Silwal et al., 2017 (apple)], pest and

    weed control [Raja et al., 2020 (tomato and lettuce); Oberti

    et al., 2016 (grape); Åstrand and Baerveldt, 2002 (sugar beet);

    Blasco et al., 2002 (lettuce)], spraying [Hejazipoor et al., 2021

    (Anthurium); Gonzalez-de-Soto et al., 2016 (wheat); Adamides

    et al., 2017 (grape)], and pruning [Zahid et al., 2020 (apple);

    Chonnaparamutt et al., 2009; Ishigure et al., 2013 (cedar and

    hinko trees)]. Together with imaging and sensing, autonomous

    robotic systems are also deemed essential and integral parts

    for high-throughput plant phenotyping, as they will enhance

    substantially the capacity, speed, coverage, repeatability, and cost-

    eﬀectiveness of plant trait measurements.

    In this paper, we reviewed the latest development of robotic

    technologies in high-throughput plant phenotyping. We deﬁne

    the robotic technologies as a system having three components:

    (1) a sensing module that senses the target (plants or crops)

    and its environment, (2) a computational module to interpret

    the sensed information and form adaptive (or context-speciﬁc)

    decisions, and (3) an actuation module to complete certain

    desired operations (e.g., robotic probing, trait measurements, and

    navigation). For example, the robot makes decision based on the

    existing status of environment, obstacles, and plant geometry to

    manipulate a robotic arm to locate an imaging system with less

    occlusion and collision free close to plant organs, ﬁnd appropriate

    target point on the leaf and control the end-eﬀector based on

    the leaf angle for eﬀective grasping, or accurately navigate the

    ground-based vehicles between crop rows. With this deﬁnition,

    systems like LemnaTec’s conveyor-based phenotyping platform

    (Fahlgren et al., 2015; Ge et al., 2016) was not considered in the

    review, because the plant movement usually follows a pre-deﬁned

    schedule and no adaptive decision is made during phenotyping.

    Also not considered in this review are self-propelled ground

    vehicles or unmanned aerial vehicles (Bai et al., 2016; Han et al.,

    2018) that are merely used as a sensor deployment platform with

    no automated path planning or navigation.

    Diﬀerent artiﬁcial intelligence (AI) technologies such as deep

    learning, fuzzy logic, and genetic algorithms are actively used

    for control of the phenotyping robots. In recent years, deep

    Frontiers in Plant Science | www.frontiersin.org

    2

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    learning techniques has gained increased interest to guide robotic

    manipulators and mobile platforms. In this regard, deep neural

    networks (DNNs) are commonly used to detect diﬀerent objects

    in images such as crop rows, plant organs, soil, and obstacles.

    DNNs are typically operate directly on raw images and actively

    learn a variety of ﬁlter parameters during the training of a model

    (Pound et al., 2017; Irshat et al., 2018). Aguiar et al. (2020)

    presented a DNN models to detect the vine trunks as reliable

    features and landmarks to navigate a mobile robot in a vineyard.

    Parhar et al. (2018) used variation of Generative Adversarial

    Network (GAN) to detect the stalk of sorghum in the ﬁeld and

    grasp it by a robotic manipulator.

    There are three motivations behind writing this review paper.

    Firstly, robotic technologies in agriculture have seen rapid

    advancement recently with many emerging applications in plant

    phenotyping. A timely review of the literature is warranted

    to summarize the newest development in the ﬁeld. Secondly,

    there is large and growing interest from the plant breeding and

    plant science communities in how these new technologies can

    be integrated into research and breeding programs to improve

    phenotyping throughput and capacity (Furbank and Tester, 2011;

    Fiorani and Schurr, 2013; Araus and Cairns, 2014). Thirdly,

    robotic phenotyping has advanced through cross-disciplinary

    collaborations between engineers and plant scientists. Outlining

    capabilities, goals and interests across these two very diﬀerent

    disciplines may help readers to identify research gaps and

    challenges as well as provide insight into the future directions of

    the plant phenotyping robotic technologies.

    REVIEW: MANY INDOOR AND

    OUTDOOR ROBOTS WERE DEVELOPED

    TO MEASURE A WIDE RANGE OF PLANT

    TRAITS

    Phenotyping

    robotic

    systems

    have

    emerged

    to

    automate

    the phenotyping process in diﬀerent aspects. The robotic

    manipulators and ground-based vehicles are used as platforms

    to attach diﬀerent sensors to collect data rapidly and with

    higher repeatability. Robotic systems are deployed to collect and

    measure the human-deﬁned phenotypic traits (such as plant

    height, and leaf area). Additionally, in some cases it is needed

    to collect repeated measurements of plant traits within large

    populations at several time points during a growing season.

    Robotic systems are highly desirable in this scenario as they

    provide the necessary speed and accuracy for this kind of

    phenotyping tasks.

    Robotic platforms for plant phenotyping applications can

    be divided into two categories: those developed for indoor or

    controlled environments (greenhouse or laboratory), and those

    for outdoor environments (ﬁeld) (Shaﬁekhani et al., 2017). In

    controlled environment, plants are either placed in a ﬁxed

    position and the robot moves around the facility to interact with

    the plants, or the plants are moved by conveyor belts or other

    automated systems to a ﬁxed location where the robot operates.

    Often the robotic system does not need to touch the plants. The

    robotic arm is equipped with RGB cameras or depth sensors

    [Time of Flight (TOF) cameras or 3D laser scanners] to acquire

    visible images or point cloud data. The morphological traits of

    the plants are then estimated from the reconstructed 3D model

    of the plants. Stem height and leaf length of corn seedlings were

    measured using a robotic arm at a ﬁxed position and a TOF

    camera (Lu et al., 2017). Chaudhury et al. (2017) developed a

    gantry robot system consisted of a 3D laser scanner installed on

    the end-eﬀector of a seven Degree of Freedom (DOF) robotic

    arm to compute the surface area and volume of Arabidopsis

    and barley. The settings of both robotic systems were unable

    to position the vision system to capture images from the leaves

    hidden by other leaves or the stem. This occlusion problem is

    common in image-based phenotyping (Das Choudhury et al.,

    2019). Even with imaging from multiple views (e.g., enabled by

    rotating plants during image acquisition), occlusion can still be

    substantial. The use of imaging systems carried by a robotic

    manipulator can provide viable solution to this issue, due to

    the ﬂexibility of the robotic manipulator to position and orient

    cameras at the best intended viewpoints. Wu et al. (2019)

    proposed an automated multi-robot system, which comprised of

    three robotic arms each equipped with a depth camera to obtain

    the point cloud data of the plant (Figure 1A). Deep learning

    based next-best view (NBV) planning pipeline was presented to

    evaluate and select the next-best viewpoints to maximize the

    information gain from the plant in data acquisition process. The

    robotic arms then were manipulated based on the determined

    optimal viewpoints. Their system was more eﬃcient and ﬂexible

    compared to other robotic systems to address the occlusion issue.

    The ability of the system to ﬁnd the optimal viewpoints, however,

    can be challenging, because its performance depends upon the

    predictions produced by the trained deep networks. This means

    that the best view-points may not be determined by the system if

    the deep networks can not generate accurate predictions.

    A second group of indoor plant phenotyping robots sought

    to touch or probe plants or plant organs, in order to

    extend the ability of robotic phenotyping from plant’s outward

    morphological traits to innate physiological and biochemical

    traits (Schulz and Baranska, 2007; Biskup et al., 2009). In this

    sense, the phenotyping robot was designed to mimic humans

    to manipulate plants and measure certain traits from targeted

    plant organs (Figure 2). This type of the robotic systems usually

    included a robotic gripper designed to attach specialized plant

    sensors, and a vision module to segment the plant from the

    background and ﬁnd an appropriate point on the organs for

    probing [Alenyà et al., 2011; Shah et al., 2016; Bao et al.,

    2017 (Ficus plant)] or grasping process [Alenya et al., 2013;

    Ahlin et al., 2016 (Anthurium, Pothos, and Dieﬀenbachia)].

    A sensor-equipped robot was presented to measure physiological

    parameters of the plant (Bao et al., 2019c). The sensor unit

    including RGB, hyperspectral, thermal, and TOF cameras, and a

    ﬂuorometer were attached to a robotic arm. The robot measured

    the reﬂectance spectra, temperature, and ﬂuorescence by imaging

    the leaf or placing probes with millimeter distance from the leaf

    surface (Figure 1B). Two diﬀerent plant phenotyping robotic

    systems were introduced to measure leaf and stem properties

    of maize and sorghum plants (Ateﬁ et al., 2019, 2020). The

    Frontiers in Plant Science | www.frontiersin.org

    3

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 1 | Plant phenotyping robotic systems for indoor environment: (A) A multi-robot
    system equipped with deep learning technique to determine optimal

    viewpoints for 3D model reconstruction (Wu et al., 2019), (B) Sensor-equipped
    robot to measure the reﬂectance spectra, temperature, and ﬂuorescence of leaf
    (Bao

    et al., 2019c), (C) Robotic system to measure leaf reﬂectance and leaf temperate
    (Ateﬁ et al., 2019), and (D) Robotic system for direct measurement of leaf

    chlorophyll concentrations (Alenyá et al., 2014).

    systems consisted of a TOF camera, a four DOF robotic

    manipulator, and custom-designed grippers to integrate the

    sensors to the robotic manipulator. Image-processing and deep-

    learning based algorithms were presented to ﬁnd the grasping

    point on leaves and stem. An optical ﬁber cable (attached to

    a spectrometer) and a thermistor were used to collect leaf

    hyperspectral reﬂectance and leaf temperature simultaneously.

    The stem diameter was measured by a linear potentiometer

    sensor. Leaf hyperspectral reﬂectance was used to build predictive

    models for leaf chlorophyll content, water content, N (nitrogen),

    P (phosphorus), and K (potassium) concentrations (Figure 1C).

    Alenyà Ribas et al. (2012) mounted a SPAD meter to a robotic

    arm to directly measure leaf chlorophyll concentrations of

    Anthurium White, Anthurium Red, and Pothus (Figure 1D).

    Quadratic surface models were applied to segment leaves from

    infrared-intensity images and depth maps captured by a TOF

    camera. The estimation issues of probing point caused by

    poor leaf-ﬁtting model reduced the probing success rate of the

    robotic system (82%).

    Although controlled environments can make it easier to grow

    plants and quantify their phenotypic traits, because environment

    plays a large role in determining plant traits, plants grown in

    controlled environments show many diﬀerences from plants

    grown in ﬁeld conditions. Therefore, with the exception of a

    growing range of horticultural crops where production occurs

    in control environments, for many crops the assessment of

    phenotypic responses in ﬁeld conditions provides more directly

    actionable information for crop improvement. A wide range of

    platforms have been developed for ﬁeld-based high-throughput

    plant phenotyping [Montes et al., 2007; White et al., 2012;

    Gao et al., 2018 (soybean); Weiss and Biber, 2011 (detection

    and mapping of maize plants); Jenkins and Kantor, 2017 (stalk

    detection of sorghum); Iqbal et al., 2020 (plant volume and

    height); Smitt et al., 2020 (fruit counting of sweet pepper and

    tomato)]. These robotic systems are guided between crop rows

    and moved toward plants. This creates several new challenges

    for both navigation and data collection which are absent when

    robotic phenotyping is conducted in control conditions. Factors

    like temperature, sunlight, wind, and unevenness of soil surface,

    can negatively impact the performance of the system. Therefore,

    the hardware and software of the robotic system must be designed

    to be resilient to the unique challenges of operating in ﬁeld

    conditions. In the ﬁeld plants are always stationary, necessitating

    that (1) phenotyping robots move to the plants rather than vice

    versa, (2) all components of the phenotyping robot including the

    vision system, robotic arm, and sensors as well as power supplies

    be carried by a robotic mobile platform, and (3) this platform

    be capable of navigation whether through global positioning

    system (GPS) data and/or employing sensors to perceive its local

    environment to guide navigation.

    Unmanned ground vehicle (UGV) robotic systems employ

    a range of sensor types including light detection and ranging

    (LIDAR) and cameras [RGB, TOF, near infrared (NIR), and

    stereo vision] for data collection. They can be installed on a ﬁxed

    stand within the overall mobile platform, or aﬃxed a robotic

    arm to increase the number of diversity of positions from which

    sensor data can be collected. Diﬀerent techniques such as 3D

    reconstruction, image processing, and machine learning are used

    for data analysis and quantify morphological traits. Existing UGV

    robotic systems have been employed to measure plant height,

    plant orientation, leaf angle, leaf area, leaf length, leaf and stem

    width, and stalk count of various species such as maize, and

    sorghum, sunﬂower, savoy cabbage, cauliﬂower, and Brussels

    sprout (Jay et al., 2015; Fernandez et al., 2017; Baweja et al.,

    2018; Choudhuri and Chowdhary, 2018; Vázquez-Arellano et al.,

    2018; Vijayarangan et al., 2018; Bao et al., 2019b; Breitzman et al.,

    2019; Qiu et al., 2019; Young et al., 2019; Zhang et al., 2020),

    count the cotton bolls (Xu et al., 2018), architectural traits and

    density of peanut canopy (Yuan et al., 2018), berry size and color

    of grape (Kicherer et al., 2015), and shape, volume, and yield

    estimation of vineyard (Lopes et al., 2016; Vidoni et al., 2017).

    A compact and autonomous TerraSentia rover equipped with

    three RGB cameras and a LIDAR was demonstrated to acquire

    in-ﬁeld LIDAR scans of maize plants to extract their Latent Space

    Phenotypes (LSPs) (Gage et al., 2019). They were inferred from

    the images using machine learning methods (Ubbens et al., 2020)

    and contained information about plant architecture and biomass

    distribution. Shaﬁekhani et al. (2017) introduced a robotic system

    Frontiers in Plant Science | www.frontiersin.org

    4

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 2 | Manual measurements of leaf reﬂectance (left), leaf temperature (middle),
    and chlorophyll content (right) (Ateﬁ et al., 2019).

    (Vinobot) including a six DOF robotic arm with a 3D imaging

    sensor mounted on a mobile platform. The Vinobot collected

    data in order to measure plant height and leaf area index (LAI) of

    maize and sorghum (Figure 3A). The authors reported that the

    use of a semi-autonomous approach created most of challenges

    for navigation of the system. In this approach, the alignment of

    the robot with the crop rows was required before autonomously

    moving between the rows and collecting data from the plants.

    Measurements

    of

    some

    biochemical

    and

    physiological

    properties require a direct contact between sensors and plants.

    Measuring these properties therefore requires a robot capable

    of grasping or touching plant organs. Grasping plant organs in

    turn requires a dexterous robotic arm as well as onboard sensors

    and algorithms capable of reconstructing the 3D geometry of the

    target plant. Robotanist, a UGV equipped with a custom stereo

    camera, was established to measure stalk strength of sorghum

    (Mueller-Sim et al., 2017; Figure 3B). A three DOF robotic arm

    along with a special end-eﬀector was mounted on Robotanist.

    The end-eﬀector consisted of a rind penetrometer that was

    modiﬁed by attaching a force gauge and a needle. When the

    stalk was grasped by the end-eﬀector, the needle and force gauge

    were pushed into the stalk to accomplish the measurement.

    The authors suggested to develop algorithms using laser scan

    and navigation camera data to improve the performance of the

    navigation system to reliably work under taller sorghum canopy

    and throughout the entire growing season. Abel (2018) attached

    a spectrometer to the robotic manipulator of Robotanist to

    capture spectral reﬂectance measurements of leaves and stems

    of sorghum. Random sample consensus (RANSAC) method was

    used for leaf and stem detection. A machine learning approach

    was applied to estimate the chlorophyll content of leaves, and

    moisture and starch contents of stems from reﬂectance spectra.

    Two factors reduced the grasping success rate of leaves (68%).

    First, the grasping process was failed because the wind moved

    the leaves and changed the position of the grasping point.

    Second, the occlusion and overlapping aﬀected the performance

    of the segmentation algorithms to detect more leaves in the

    images. Chen et al. (2021) developed a robotic system including

    LeafSpec (invented at Purdue University) attached to a robotic

    manipulator to collect hyperspectral images of maize leaves in

    the ﬁeld (Figure 3C). The robot slid the LeafSpec across the

    leaf from the beginning to tip to acquire hyperspectral images

    of entire leaf. The system predicted leaf nitrogen content with

    R2 = 0.73.

    Other autonomous ground-based systems were presented

    to measure both morphological and biochemical/physiological

    attributes. A visible and near infrared (VIS/NIR) multispectral

    camera was mounted on a mobile robot called “Thorvald I” to

    measure the normalized diﬀerence vegetation index (NDVI) of

    wheat from multispectral images (Burud et al., 2017). The robot

    then modiﬁed to a new version called “Thorvald II” to have better

    performance for phenotyping tasks (Grimstad and From, 2017;

    Figure 3D). BoniRob was proposed as an autonomous robot

    platform including spectral imaging and 3D TOF cameras which

    can be used to measure plant parameters such as plant height,

    stem thickness, biomass, and spectral reﬂection (Ruckelshausen

    et al., 2009; Biber et al., 2012; Figure 3E). Underwood et al.

    (2017) introduced a ground-based system (Ladybird) for row

    phenotyping of grain and legume crops (wheat, faba bean, lentil,

    barley, chickpea, and ﬁeld pea) (Figure 3F). Crop height, crop

    closure, and NDVI were determined after processing the data

    from the LIDAR and the hyperspectral camera. Flex-Ro, a multi-

    purpose ﬁeld robotic platform was used for high-throughput

    plant phenotyping to measure phenotyping traits of soybean

    (Murman, 2019; Figure 3G). Three sets of sensors were installed

    on Flex-Ro to collect data from crop rows. For each set, a passive

    ﬁber optic cable, a RGB camera, an ultrasonic distance sensor,

    and an infrared radiometer were used to measure NDVI, canopy

    coverage, canopy temperature, and height.

    Table 1 summarizes the indoor and outdoor robotic systems

    which could successfully measure plant traits for diﬀerent crops.

    Figure 4 gives summary statistics regarding the plant

    phenotyping robotic systems that is discussed in this section.

    It can be seen that the robotic phenotyping research targeted

    maize and sorghum more than other species (soybean, wheat,

    barley, chickpea, pea, faba bean, lentil, cabbage, cauliﬂower,

    cotton, peanut, sunﬂower, grape, tomato, sweet pepper, and

    Arabidopsis) (Figure 4A). Maize and sorghum are two of the

    most economically important and highly diverse cereal crops

    with vast numbers of accessions (Zhao et al., 2016; Bao et al.,

    2019b). Therefore, more attention was devoted to breed Maize

    and sorghum to produce food, animal fodder, and biofuel.

    Moreover, the available genetic resources for these crops required

    the phenotyping data to map their genotypes to phenotypes and

    Frontiers in Plant Science | www.frontiersin.org

    5

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 3 | Plant phenotyping systems for outdoor environment: (A) Vinobot: robotic
    system including six DOF robotic manipulator and a 3D imaging sensor

    mounting on a mobile platform to measure plant height and LAI (Shaﬁekhani et al.,
    2017), (B) Robotanist: UGV-based robotic system equipped with a three DOF

    robotic manipulator and a force gauge for stalk strength measurement (Mueller-Sim
    et al., 2017), (C) A robotic system to slide LeafSpec across entire leaf to collect

    its hyperspectral images (Chen et al., 2021), (D) Thorvald II: VIS/NIR multispectral
    camera mounted on a mobile robot to measure NDVI (Grimstad and From, 2017),

    (E) BoniRob: autonomous robot platform using spectral imaging and 3D TOF cameras
    to measure plant height, stem thickness, biomass, and spectral reﬂection

    (Biber et al., 2012), (F) Ladybird: ground-based system consisted of a hyperspectral
    camera, a stereo camera, a thermal camera, and LIDAR to measure crop

    height, crop closure, and NDVI (Underwood et al., 2017), and (G) Flex-Ro: high-throughput
    plant phenotyping system equipped with a passive ﬁber optic, a RGB

    camera, an ultrasonic distance sensor, and an infrared radiometer for the measurement
    of NDVI, canopy coverage, and canopy height (Murman, 2019).

    thus crop yield improvement. Accordingly, there has been an

    emerging need for phenotyping robots to automatically measure

    the phenotypic traits. Regarding the plant structure, maize, and

    sorghum have similar morphology. Their leaves are arranged

    alternately on each side of the stem that has cylindrical/elliptic-

    cylinder shape and is positioned in the middle part of the plant.

    This plant structure provides less complexity for the robotic

    system to distinguish between the stem and leaves and extract

    their features. Figure 4B shows that the height, width, and

    volume of plant/canopy are three main (morphological) traits

    that more frequently measured by the robotic systems than other

    traits, each of them being ≤ 5% (leaf length, leaf width, leaf angle,

    leaf area, leaf reﬂectance, leaf chlorophyll content, leaf/canopy

    temperature, LAI, plant/canopy NDVI, stem reﬂectance, stalk

    strength, stalk count, berry size, and fruit count). Two reasons

    can be considered for the frequent measurements of these

    phenotypic traits. Firstly, the plant architectural traits (such as

    plant height) are the most common and important parameters

    for ﬁeld plant phenotyping since they have signiﬁcant eﬀects

    on light interception for photosynthesis, nitrogen availability,

    and yield (Barbieri et al., 2000; Andrade et al., 2002; Tsubo and

    Walker, 2002). Consequently, by studying and then manipulation

    of the plant architecture, the crop productivity will be increased.

    Secondly, as it was discussed in this section, the robot just needs

    non-contact based sensors (RGB camera or depth sensor) to

    collect data from the plants. Then, by analyzing the 2D images

    or creating plant 3D models, the aforementioned plant traits can

    be estimated in either ways: (1) the correlation between the pixel

    counts in the images and the ground truth measurements, or

    (2) extracting the distance/volume in real world from the depth

    sensor data. Hence, the measurement of these morphological

    properties is less challenging for the phenotyping robots using

    simple sensors and algorithms. In addition to the more frequent

    measurements of stem height and width (of maize and sorghum),

    these properties were also measured more accurately by the

    robotic systems because they are less aﬀected by the plant

    morphology (Figure 4C). The ﬁrst step to extract the stem height

    and width is to detect the stem and segment it from other

    plant organs. The morphology of maize and sorghum (alternately

    arranged leaves, and cylindrical-shaped stem in the middle)

    provides more hints for stem detection and segmentation.

    Moreover, the height and width can be measured as linear

    measurements. Accordingly, these two stem properties can be

    measured with less complexity and higher accuracy. Figure 4D

    illustrates that non-contact based sensing systems (such as RGB,

    stereo vision, and multispectral cameras, and LIDAR) were

    Frontiers in Plant Science | www.frontiersin.org

    6

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    TABLE 1 | Summary of indoor and outdoor robotic systems that successfully measured
    plant properties for different crops.

    Robot type

    References

    Species

    Plant trait

    Performance

    Measurement method

    Software system

    Indoor

    Alenyà Ribas et al., 2012

    Anthurium Andreanum (White),

    Anthurium andreanum (Red),

    Epipremnum aureum (Pothos)

    Leaf chlorophyll content

    SR = 90%, 85%, 70%

    Contact based

    ROS

    Lu et al., 2017

    Maize

    Stem height, leaf length

    ER = 13.7%, 13.1%

    Non-contact based

    Qt development

    environment (C++)

    Ateﬁ et al., 2019

    Maize, Sorghum

    Leaf chlorophyll content, leaf

    potassium content, leaf water

    content, leaf temperature

    R2 = 0.52, 0.52, 0.61

    R2 = 0.58, 0.63

    Contact based

    MATLAB

    Ateﬁ et al., 2020

    Maize, Sorghum

    Stem diameter

    R2 = 0.98, 0.99

    Contact based

    MATLAB

    Outdoor

    Jay et al., 2015

    Sunﬂower, Savoy cabbage,

    cauliﬂower, Brussels sprout

    Plant height, leaf area

    R2 = 0.99, 0.94

    Non-contact based

    Not reported

    Shaﬁekhani et al., 2017 (Vinobot)

    Maize, sorghum

    Plant height

    R2 = 0.99

    Non-contact based

    ROS

    Abel, 2018 (Robotanist)

    Sorghum

    Stem starch content, stem

    moisture content, leaf

    chlorophyll content

    R = 0.81, 0.72, 0.92

    Contact based

    ROS

    Baweja et al., 2018 (Robotanist)

    Sorghum

    Stalk count, stalk width

    R2 = 0.88,

    MAE = 2.77 mm

    Non-contact based

    ROS

    Choudhuri and Chowdhary, 2018

    Sorghum

    Stem width

    Accuracy = 98.2%

    Non-contact based

    Python

    Vázquez-Arellano et al., 2018

    Maize

    Plant height

    AME = 8.7 mm

    SD = 35 mm

    Non-contact based

    Not reported

    Vijayarangan et al., 2018

    Sorghum

    Leaf area, leaf length, leaf width

    Relative RMSE = 26.15%,

    26.67%, 25.15%

    Non-contact based

    ROS

    Bao et al., 2019b

    Maize

    Plant height, leaf angle

    R2 = 0.96, 0.83

    Non-contact based

    Not reported

    Qiu et al., 2019

    Maize

    Plant height

    RMSE = 0.058 m

    Non-contact based

    ROS

    Young et al., 2019

    Sorghum

    Plant height, stem width

    AE = 15%, 13%

    Non-contact based

    Not reported

    Zhang et al., 2020

    Maize

    Stand counting

    R = 0.96

    SD = 6.76%

    Non-contact based

    Not reported

    ER, error ratio; SR, success rate, MAE, mean absolute error; RMSE, root mean square
    error; AME, absolute mean error; AE, absolute error; SD, standard deviation; ROS,
    robot operating system.

    Frontiers in Plant Science | www.frontiersin.org

    7

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 4 | Summary statistics of the phenotyping robotic systems: (A) Targeted
    plants, (B) Plant/canopy traits measured by the robots, (C) Average accuracy (R2)

    to measure the phenotypic traits, (D) Robot vision/sensing systems, and (E) Robot
    software systems.

    used more in phenotyping robots compared to contact-based

    sensors (chlorophyll meters, spectrometers, thermistors, and

    linear potentiometers). This can be explained by the fact that the

    majority of the robotic systems were developed to measure the

    morphological traits or some physiological properties. To achieve

    these goals, the phenotyping robots are required to use the 2D

    images/3D models of plants using non-contact based sensors.

    Among the non-contact sensors, the sensor-fusion based systems

    (including RGBD/stereo vision cameras, RGB camera + LIDAR,

    RGB + TOF cameras, spectral imaging + TOF camera) and

    depth sensors (TOF camera, LIDAR, laser scanner, and ultrasonic

    sensor) were commonly used as vision/sensing systems for the

    phenotyping robots. The key is to acquire depth information as

    a vital parameter to manipulate a robotic arm to grasp the plant

    organs, navigate a mobile robot between crop rows, and measure

    plant properties (such as height, width, and volume). Sensor-

    fusion based systems were employed by phenotyping robots more

    often than depth sensors. The reason would be that these sensors

    prepare the plant color/spectral information along with the depth

    information. Consequently, by acquiring more information, the

    plant can be eﬀectively segmented from the background and the

    plant properties can be eﬀectively measured. Regarding the robot

    software system, it can be found that Robot Operating System

    (ROS) is one the most popular systems to develop the software

    of the phenotyping robots (Figure 4E). ROS is an open source

    system that provides services, libraries, and tools for sensors-

    actuators interface, software components communication, and

    navigation and path planning1. Diﬀerent manufacturers of robot’s

    hardware provide ROS drivers for their products such as imagery

    systems, sensors, actuators, robotic manipulators, and mobile

    platforms. This allows the researchers to develop the phenotyping

    robotic systems more eﬃciently.

    1https://www.ros.org/

    Frontiers in Plant Science | www.frontiersin.org

    8

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Regarding the platform of the mobile phenotyping robots,

    most of the mobile platforms were developed by researchers

    (86%) and few oﬀ-the-shelf robots were used (14%). The custom-

    designed platform oﬀers potential to meet speciﬁc conditions

    regarding soil, weather, and plant which vary with experimental

    site and phenotyping task. Moreover, the researcher has more

    control on modifying the hardware and software. Based on

    the reviewed papers, most of the mobile platforms used four

    wheels/legged-wheels (88%) as their driving system compared

    with tracked mechanism (12%). In the wheeled-vehicles, the

    wheels can be independently steered (good maneuverability)

    which provides high ﬂexibility with respect to control and

    navigation (desired orientation angle and rotation speed) of the

    vehicles between the crop rows in the ﬁeld. Moreover, the vehicle

    can move faster using (legged) wheels and has high ground

    adaptability (crop height, and irregular and sloped terrain)

    using legged-wheels. However, the tracked-vehicles create more

    traction and less pressure on soil (work better in wet soil and less

    soil compaction), and can drive over rough terrain and obstacles

    easier than the wheeled-vehicles (Bruzzone and Quaglia, 2012).

    Accordingly, a hybrid locomotion system can be developed

    with the combination of legged-wheels and tracked systems.

    Therefore, this platform can use the advantages of the both

    driving systems to accomplish phenotyping tasks more eﬀectively

    and eﬃciently.

    PHENOTYPING ROBOTS FACE SEVERAL

    CHALLENGES

    There are several outstanding challenges in the development of

    robotic systems for plant phenotyping. Some of these challenges

    related to segmentation (vision systems) and grasping (robotic

    manipulators) are shared or at least similar for both indoor

    and outdoor phenotyping robots. Other challenges, particularly

    those related to navigation are speciﬁc to outdoor robotic

    phenotypic applications.

    Complex and Deformable Nature of

    Plants Represents a Major Issue for

    Robot’s Vision and Sensing System

    The UGV or robotic manipulator equipped with contact/non-

    contact based sensing systems oﬀer a great potential to measure

    plant phenotypic data compare to non-autonomous robotic

    sensing systems. For example, the UGV equipped with stereo

    vision camera can move between crop rows and collect images

    from the canopy or individual plant. The image data can be

    analyzed immediately or can be processed later to extract plant

    properties. The long-term measurement of the plant traits can

    provide useful knowledge for crop modeling purposes over time

    (Duckett et al., 2018). Another example would be the robotic

    manipulator equipped with a hyperspectral imaging system.

    The robotic arm can move around the plant to locate the

    sensor close to the plant organs. With this proximal sensing,

    more phenotyping information can be acquired about the

    organs. However, the robotic vision/sensing technologies for the

    phenotyping task encounter diﬀerent challenges.

    Various imaging technologies are utilized as vision systems of

    the robots. Visible imaging/RGBD camera are commonly used

    technologies that rely on the color/texture information of an

    object. Images are processed to segment plant organs and identify

    desirable targets for grasping. The identiﬁcation and localization

    of diﬀerent plant organs (such as leaves, stems, ﬂowers, and fruits)

    is one of the major problems in computer vision, due to complex

    structure and deformable nature of plants. The overlap between

    the adjacent leaves or leaf-stem causes occlusion; even though

    leaf and stem have diﬀerent architecture, they share similarities

    in color and texture. Accordingly, it is diﬃcult to distinguish

    occluded leaves or stem in the image. The morphology of

    plants (shape and size) varies dramatically across diﬀerent plant

    species and even within a single species diﬀerent varieties or

    the same variety grown in diﬀerent conditions may exhibit

    radically diﬀerent morphology. In this regard, the software of

    the robotic system should cover a wide range of scenarios and

    possibilities to be able to respond and adapt appropriately to day-

    to-day changes in the same plant or diﬀerences between plants

    within the same experiment. Additionally, non-uniform imaging

    conditions (lighting and background) make it more complex to

    ﬁnd an appropriate color space and optimal approach for the

    segmentation purposes (Zhang et al., 2016; Narvaez et al., 2017;

    Qiu et al., 2018; Bao et al., 2019a).

    Multispectral/hyperspectral and thermal imaging systems are

    sensitive to illumination since the reﬂectance from the plant

    organ is depend on its distance and orientation toward the

    light source/incident radiation and camera. Moreover, multiple

    reﬂectance and also shade will occur due to the curvature nature

    and complex geometry of plant (Li et al., 2014; Mishra et al., 2017;

    Qiu et al., 2018). To deal with these issues, researchers introduced

    diﬀerent technical solutions. Behmann et al. (2016) combined the

    hyperspectral image with 3D point cloud (using a laser scanner)

    of sugar beet to create hyperspectral 3D model. Then, it was used

    to quantify and model the eﬀects of plant geometry and sensor

    conﬁguration. Finally, the geometry eﬀects in hyperspectral

    images were weakened or removed using reﬂectance models.

    Shahrimie et al. (2016) used inverse square law and Lambert’s

    cosine law along with Standard Normal Variate (SNV) for maize

    plants to remove the distance and orientation eﬀects.

    Robotic Control System Needs to Deal

    With Dynamic and Unstructured

    Environment

    The size and orientation of the plant organs are constantly

    changing across their growth stages. Therefore, the lack of needed

    DOF or enough workspace of the robotic manipulator are the

    limitations for the robots to grasp the plant organs and sense their

    properties successfully. The robotic arm cannot reach the organs

    if they are out of its workspace. In addition, a robot arm with less

    ﬂexibility (DOF) might not able to properly adjust the angle of its

    end-eﬀector in grasping process.

    Field-based robots need to navigate between crop rows and

    then turned to the next row safely and autonomously. To achieve

    Frontiers in Plant Science | www.frontiersin.org

    9

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    this task, the crop rows and obstacles should be detected to

    build a map of the surrounding area. Then, their position and

    orientation relative to the vehicle will be found to compute an

    optimal path and avoid unexpected obstacles. Finally, adequate

    action will be determined to steer the wheels and guide the system

    around the ﬁeld. However, the uncontrolled and unstructured

    ﬁeld environment creates challenges for accurate navigation and

    control of the robot (De Baerdemaeker et al., 2001; Nof, 2009;

    Bechar and Vigneault, 2016; Shamshiri et al., 2018). GPS is a

    common method for robot navigation. However, the tall plant

    canopy aﬀects on the accuracy of GPS for navigation purposes

    as the canopy blocks the satellite signals to the GPS receiver.

    Hence, the information provided from other sensors along with

    GPS is also required to detect the obstacles, precisely guide

    the phenotyping robot, and minimize the damage to the robot

    and plants. The UGV based phenotyping robots can facilitate

    the data fusion of GPS and other sensors since the robot can

    equipped with various sensors (such as LIDAR, RGB/stereo

    vision cameras) and also precisely control their location (Duckett

    et al., 2018). Nonetheless, varying ambient light conditions,

    changing crop growth stages (size, shape, and color), and similar

    appearance between crops and weeds are common factors that

    fail visual navigation. In these situations, RGB sensor-based

    systems usually cannot ﬁnd a stable color-space or plant features

    to detect diﬀerent objects. Incomplete rows and missing plants

    can cause errors to compute distance between the robot and

    plants using range sensors. Diﬀerent soil properties (soil types

    and moisture) and terrain variation (even, uneven, ﬂat, slope) are

    other factors that inﬂuence robot dexterous manipulation, wheel-

    terrain interaction, wheel slip, and steering control algorithms

    (Åstrand and Baerveldt, 2005; Grift et al., 2008; Li et al., 2009;

    Shalal et al., 2013).

    After navigating the robot between the rows, a suitable

    path should be selected for the robotic manipulator with

    minimum collisions inside a plant or canopy to reach and

    grasp the targets delicately. However, robots operate in extremely

    complex, dynamic, uncertain, and heterogenous real world

    condition. In this situation, visual occlusion of a plant by others

    caused by high plant density should be taken into account

    for target identiﬁcation and segmentation. In addition, the

    target information will be aﬀected by sunlight and wind. For

    instance, TOF/RGBD cameras use infrared light to measure

    distance. Since the sunlight has infrared wavelengths and wind

    moves the targets, the location of the target in 3-dimensional

    space might not be accurately measured (Andújar et al., 2017;

    Narvaez et al., 2017; Qiu et al., 2018; Li et al., 2020a).

    Consequently, the obstacle-avoidance path-planning algorithm

    cannot be determined correctly. Another example would be

    when the targets are seen shinier or darker because of specular

    reﬂection or shade.

    Issues With Robot Software for

    Phenotyping Robotic System

    Development

    Two main drawbacks present in many robot software are: (1)

    the lack of support for certain functional packages (of open

    source software) and (2) real-time constraints (Barth et al., 2014;

    Park et al., 2020). For the ﬁrst issue, it can be supposed that a

    phenotyping robot is developed by researchers to accomplish a

    phenotyping task. They create the robot library and share their

    codes with the (open source) software community. However, by

    ending the project, there is no guarantee to ﬁx the bugs and

    update the codes. In the case of other researchers might start

    similar research using the shared codes, it might be problematic

    to make the research forward because of the lack of support

    for the robot library. The second challenge is the real-time

    constraints that causes system malfunction due to latency. One

    example would be when a UGV moves between crop rows to

    measure plant traits. If the robot cannot satisfy the real-time

    constraints, the robot will have delay to identify the obstacles

    or adjust its position relative to the crop rows. Accordingly,

    the robot could hit the obstacles and the plants and this

    causes the physical damage to the robot or plants. Regarding

    ROS, although ROS1 has real-time constraints, however the

    community is actively working on software improvement.

    For example, RT-ROS supports the real-time communication

    that leads to performance enhancement of ROS1 (Wei et al.,

    2016). It is obvious that by growing the ROS community,

    sophisticated libraries and packages will be developed for more

    plant phenotyping applications.

    Other Challenges: Managing Big Data,

    Reliable Power Source, Durability Under

    Harsh Environment, and High Cost

    The phenotyping robot collects massive volumes and various

    types of data (such as images, multi/hyperspectral data) taken

    by diﬀerent sensors from large population of plants. The robot

    needs to analyze large quantities of data in real-time for suitable

    action/decision-making process. In addition, the large-scale

    phenotypic data could be stored properly for the beneﬁt of future

    research. Therefore, managing and analyzing the big data as a

    result of high-throughput, robotically collected plant traits is an

    emerging issue for the phenotyping robot.

    The ﬁeld-based mobile robots need to be equipped with

    reliable power sources to provide energy for the vehicle carriage

    weight, distance traveled, and diﬀerent electrical components

    such as sensors for data collection. Batteries are commonly used

    for this purpose. The problems with batteries are: (1) limited

    operating time that prevents the robots to work for long time and

    accomplish large-scale missions, and (2) need to recharge which

    typically takes a long time.

    Another challenge is the durability and stability of these

    robotic systems under harsh outside environment caused by

    extreme temperature, high humidity, strong sunlight, and dust.

    These harsh conditions can cause damages for the components

    of the robotic system and accordingly will have negative eﬀects

    on the robot’s performance.

    The cost of phenotyping robots (in general agricultural robots)

    is still high and this makes limitations for wide-spread use

    of the robots. In most cases the phenotyping robotic systems

    are developed for research purposes and the robots are not

    commercially available yet. Both the hardware and software

    Frontiers in Plant Science | www.frontiersin.org

    10

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    systems were restricted to a very speciﬁc condition and could

    not be transferred to a diﬀerent scenario. This leads to high

    R&D (research and development) cost that can not be spread

    over multiple units. However, more general purpose phenotyping

    robots can be developed and commercialized in the future and

    their cost will be reduced substantially. Moreover, with the

    consistent trend of price reduction of electronics, sensors, and

    computers, the robotic systems will become cost-eﬀective enough

    to be more widely used for phenotyping tasks.

    POTENTIAL IMPROVEMENTS OF

    PHENOTYPING ROBOTS

    Sensors and Controllers Fusion

    Technique Can Improve the Performance

    of Robot

    Sensing-reasoning, and task planning-execution are two essential

    functions for autonomous phenotyping robots. They sense the

    environment, apply an appropriate control algorithms, make

    decision, and act in real-time to perform the phenotyping tasks

    (Grift et al., 2008; Bechar and Vigneault, 2016). The design

    of the phenotyping robot and its control algorithm needs to

    be optimized to achieve successful operation in continuously

    changing environment. To reach this purpose, the phenotyping

    robots need to employ advanced technology to cope with the

    dynamic and unstructured environment. The emerging sensor

    technologies such as sensor fusion increase the robot capabilities

    and yield better results (Grift et al., 2008). Sensor fusion

    allows the robot to combine information from a variety of

    sensing modules to form better decision for navigation and path

    planning, as well as increase the capacity of sensing to gather

    more information from the plants. For example, Choudhuri

    and Chowdhary (2018) measured the stem width of sorghum

    with 92.5% accuracy using RGB data. However, they achieved

    higher accuracy (98.2%) after combining RGB + LIDAR data.

    Kim et al. (2012) could successfully navigate an unmanned

    weeding robot using sensor fusion of a laser range ﬁnder

    (LRF) and an inertial measurement unit (IMU). The robot

    also needs more sophisticated and intelligent algorithms to

    accomplish diﬀerent subtasks such as sensing, navigation, path-

    planning, and control. Diﬀerent control strategies such as

    genetic algorithm (GA), fuzzy logic (FL), neural network (NN),

    reinforcement learning (RL), and transfer learning (TL) can

    be integrated to develop such robot algorithms (Shalal et al.,

    2013). Therefore, a robust controller will be provided for the

    phenotyping robot since the robot control system can use the

    merits of both technologies (combining two control strategies).

    Batti et al. (2020) studied the performance of fuzzy logic

    and neuro-fuzzy (NN + FL) approaches to guide a mobile

    robot moving between the static obstacles. The authors found

    that neuro-fuzzy controller provide better results for robot

    navigation compare to fuzzy logic controller. Although several

    diﬀerent autonomous phenotyping robots were developed,

    more research is needed to adapt and improve the advanced

    technologies to overcome the robot limitations to accomplish the

    phenotyping tasks, and also increase the autonomy level of the

    phenotyping robots.

    Internet of Robotic Things (IoRT):

    Technology to Manage Big Data for

    Phenotyping Robots

    Internet of Things (IoT) technologies are helpful to send lots

    of data collected by diﬀerent sensors over Internet in a real-

    time manner. The Internet-of-Robotic-Things (IoRT) is the

    conﬂuence of autonomous robotic systems with IoT which is

    an emerging paradigm that can be employed for phenotyping

    robots (Grieco et al., 2014; Ray, 2016; Batth et al., 2018;

    Saravanan et al., 2018; Afanasyev et al., 2019). Mobile robots

    can use IoT to transfer and store a large amount of phenotypic

    datasets to a central server. By sending the data via IoT,

    the robots do not need to frequently move to a place and

    physically upload the collected data to a local server/computer.

    Moreover, plant breeders/scientists can visualize the data using a

    mobile device (a tablet or a smartphone) or an oﬃce computer

    and therefore the performance of plants and changes in crop

    growth and development can be remotely inspected in diﬀerent

    regions of the ﬁeld in a real-time fashion. Another attractive

    aspect of using IoRT is to send commands to robots to

    accomplish phenotyping tasks. For instance, an operator can

    remotely control the greenhouse robotic manipulator systems

    via Internet any time from his home/oﬃce to collect phenotypic

    data. Another example is when the close inspection of an

    area in a ﬁeld is necessary after analyzing the drone-based

    image data; therefore, commands can be sent via Internet to

    deploy mobile robots in this regard. Several mobile robots

    can work together to operate more eﬃciently to achieve

    a speciﬁc task.

    Solar Panels and Hydrogen Fuel Cell:

    Renewable Power Sources for

    Phenotyping Robots

    Solar panels and hydrogen fuel cell are two technologies that

    produce clean, renewable, and sustainable energy. A solar panel

    consists of many small units called photovoltaic cells which

    convert sunlight into electricity. The maintenance cost of the

    solar panel is low since it does not have moving parts (no

    wear) and it just need to clean the cells. The hydrogen fuel cell

    comprised a pressurized container to store hydrogen. The fuel

    cell is an electrochemical device that takes oxygen from the air

    and combines hydrogen with oxygen to produce electricity. Re-

    fueling time of a hydrogen fuel cell is very short (5 min or less)

    and its cells are fairly durable.

    Based on the advantages of solar panels and hydrogen

    fuel

    cell,

    both

    technologies

    can

    be

    used

    as

    renewable

    power sources for diﬀerent components of the phenotyping

    robots

    (Underwood

    et

    al.,

    2017;

    Quaglia

    et

    al.,

    2020).

    However, there is not a wide range of application of these

    technologies for the phenotyping robots. The cost of both

    technologies is high. For solar panels, the eﬃciency of

    the system drops in cloudy and rainy days. In addition,

    more solar panels are needed to produce more electricity

    Frontiers in Plant Science | www.frontiersin.org

    11

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    which requires a lot of space. For hydrogen fuel cell, there

    are relatively few places to re-fuel the cell. Nevertheless,

    both technologies are constantly developing which can be

    assumed to reduce their cost and improve their eﬃciency to

    produce electricity.

    PERSPECTIVE APPLICATIONS OF

    ROBOTIC PHENOTYPING

    Phenotyping Robots Has Great Potential

    to Measure Other Plant Properties

    Section “Review: Many Indoor and Outdoor Robots Were

    Developed to Measure a Wide Range of Plant Traits” introduced

    the robotic systems for indoor and outdoor applications to

    measure several diﬀerent plant traits. However, other leaf/stem

    characteristics are also reliable indicators to detect the symptoms

    of biotic/abiotic stresses and monitor the plant health during

    a growing season. Stomatal conductance, gas exchange, and

    chlorophyll ﬂuorescence of leaves are indicative of their water

    status, photosynthesis, and chlorophyll content (Castrillo et al.,

    2001; Ohashi et al., 2006; Li et al., 2020b). Stem sap ﬂow

    and lodging resistance can provide useful information about

    plant water use and stem strength (Cohen et al., 1990; Kong

    et al., 2013). These aforementioned phenotypic traits are still

    measured manually. On the other hand, new clip-on sensor

    system can be presented to measure them automatically. The

    system includes a custom-designed gripper/clip combined with

    novel sensor(s) (Afzal et al., 2017; Palazzari et al., 2017).

    The design of these sensing systems is important since the

    accuracy and robustness of trait prediction models depend on

    the phenotypic data quality (Würschum, 2019). The design

    of the gripper and DOF of the robotic manipulator should

    allow a good and gentle contact between the sensing unit

    and the leaf/stem. Sometimes a vacuum mechanism attached

    to a soft gripper can hold the leaf/stem and help the sensing

    unit for eﬀective contact and collect accurate data with less

    damage to the plant organs (Hayashi et al., 2010; Hughes et al.,

    2016; Zhang et al., 2020). Moreover, autonomous robots should

    gather data with minimum error (high signal to noise ratio).

    Therefore, sensors with high signal to noise ratio should be

    selected and accurately calibrated. In addition to the accuracy,

    the robots should rapidly (short execution time) accomplish

    their missions. Deep reinforcement learning (DRL) technique is

    an accurate and reliable method to ﬁnd an optimal path with

    nearest and collision avoidance route. This technique can be

    adopted by phenotyping robots to manipulate a robotic arm

    for grasping process or to navigate a mobile robot between

    crop rows (Zhang et al., 2015; Zhang et al., 2019; Duguleana

    and Mogan, 2016; Franceschetti et al., 2018; Taghavifar et al.,

    2019). Although the robotic phenotyping is mainly focusing

    on leaf and stem, it can be utilized for other plant organs

    such as inﬂorescences (spike, panicle, and tassel), ﬂowers,

    fruits, and roots.

    The morphometric parameters of inﬂorescence are highly

    correlated with yield and grain quality (Leilah and Al-Khateeb,

    2005; Gegas et al., 2010). Several studies discussed about

    using image-based techniques (2D images/3D reconstruction)

    to extract architectural traits such as length and width of

    inﬂorescence, inﬂorescence volume (weight), grain shape and

    size, grain angle, and number of grains, and number of ﬂowers

    (Faroq et al., 2013; Crowell et al., 2014; Gage et al., 2017; Rudolph

    et al., 2019; Sandhu et al., 2019; Xiong et al., 2019; Zhou et al.,

    2019). In such applications to measure the morphological traits,

    a robot with LIDAR/camera can be useful to automatically take

    images/point cloud data from diﬀerent views of the inﬂorescence.

    The physiological traits are indicator for stress or disease.

    For instance, the temperature of the spikes was used for

    detecting the plant under the water stress (Panozzo et al., 1999).

    Conceivably, a robotic arm equipped with a temperature sensor

    can grasp the spike and insert the sensor into spikelets to record

    their temperature.

    Several properties of fruits such as water content, sugar

    content, chlorophyll, carotenoid, soluble solid, acidity, and

    ﬁrmness are measured for fruit quality assessment. The

    spectroscopy/spectral imagery are non-destructive and high-

    throughput methods to estimate these qualitative parameters

    (Berardo et al., 2004; ElMasry et al., 2007; Shao and He, 2008;

    Wu et al., 2008; Nishizawa et al., 2009; Penchaiya et al., 2009;

    Ecarnot et al., 2013; Guo et al., 2013; Dykes et al., 2014; Wang

    et al., 2015; Mancini et al., 2020). However, a robotic system

    can be presented to monitor the dynamics of these attributes

    for hundreds of growing fruits per day. For example, a portable

    spectrometer can be attached to the robot’s end-eﬀector. After

    detecting the fruit on the plant, the robot can grasp the fruit and

    gather its spectral data to further infer its quality parameters.

    Since the root has functional roles in resource acquisition,

    the characteristics of root provide valuable information about

    plant physiological and ecosystem functioning (Mishra et al.,

    2016). In traditional root phenotyping, two diﬀerent methods

    are used to acquire images from root (in the soil or soil-free

    or transparent media). In ﬁrst method, a camera is mounted

    on a tripod and moved by a human around the root, and

    in the second method camera(s)/sensor(s) are set in ﬁxed

    point(s) and root (plant) is rotated (Atkinson et al., 2019).

    This is a tedious task and some root information (such as

    ﬁne branches) might be lost due to less ﬂexibility of the

    system to take up close images from the complex architecture

    of root. Consequently, automated root phenotyping systems

    can facilitate and improve the traditional root phenotyping in

    terms of eﬃciency and eﬀectiveness with acquiring fast and

    precise measurements (Wu et al., 2019). Here, the “plant to

    sensor” system can be used to examine vast number of roots

    (or plants) without the need of huge space of greenhouse

    facility. In this system, the root (or plant) is moved toward a

    robotic manipulator (equipped with camera/sensor) and located

    on a rotation table. In each step angle of the table, the root

    is rotated and stopped in front of the robotic system. Then,

    the robotic manipulator moves the camera around the root

    and gather close proximity data from diﬀerent views (positions

    and angles). Therefore, more detailed information of root

    can be captured due to high resolution sensing oﬀered by

    the robotic system.

    Frontiers in Plant Science | www.frontiersin.org

    12

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 5 | (A) Mobile Agricultural Robot Swarms (MARS) for seeding process (The
    European Coordination Mobile Agricultural Robot Swarms (MARS). PDF ﬁle.

    November 11, 2016. http://echord.eu/public/wp-content/uploads/2018/01/Final-Report-MARS.pdf),
    (B) UAV-UGV cooperative system to measure environmental

    variables in greenhouse (Roldán et al., 2016).

    Robots in Greenhouses Complement the

    Image-Based Phenotyping

    Automatic greenhouses such as LemnaTec (LemnaTec GmbH,

    Aachen, Germany) monitor plants using image-based technique.

    While it has shown great potential to measure and predict the

    plant traits, many hurdles cannot be handled by this technology.

    It needs to eﬃciently manage “big data” problems and also

    postprocess images to characterize the plant traits. Moreover, this

    approach is not suﬃcient for early detection of stress/disease

    with internal symptoms. Furthermore, this method requires

    direct measurements using sensors to calibrate and validate

    of its models to extract the phenotypic traits from images

    (Madden, 2012; Mutka and Bart, 2015; Singh et al., 2016; Lee

    et al., 2018). Hence, several robotic arms with diﬀerent sensors

    can be integrated to the greenhouse for real-time and direct

    measurement of the chemical/physiological traits. Basically,

    plants are transported by an automatic conveyor belt and stopped

    in front of each robotic system. Then, the system uses “sensor-to-

    plant” concept (Lee et al., 2018) in which the robot moves toward

    the plant to take measurements before sending it through the

    imaging chambers. These stationary robotic systems are designed

    to operate in indoor environment. Moreover, several robots can

    be presented to collect data from a speciﬁc plant. It is diﬃcult

    to develop a general prototype that are broadly applicable for

    diﬀerent conditions (Mutka and Bart, 2015; Wu et al., 2019).

    However, the software and hardware of the robots should be

    adapted to other species and ﬁeld-phenotyping applications. The

    challenge for both type of robots (indoor/outdoor) would be

    continuously collect and save large amount of data.

    Swarm Robot Is a New Frontier to

    Efﬁciently Accomplish Complex

    Phenotyping Tasks

    Swarm robotics is a new frontier technology which has potential

    application for proximal sensing of plants, and data/sample

    collection in a large ﬁeld. A swarm robotics system composed of

    large numbers of autonomous robots that are coordinated with

    local sensing and communication, and a decentralized control

    system (Brambilla et al., 2013; Bayındır, 2016; Blender et al., 2016;

    Chamanbaz et al., 2017; Figure 5A). The application of swarm

    robots has some advantages which is suitable for large scale tasks.

    Since swarm robotics has large population size, the tasks can be

    decomposed using parallelism and can be completed eﬃciently

    and consequently it would save time signiﬁcantly. Moreover, the

    swarm robots can achieve the distributed sensing that means they

    can have a wide range of sensing in diﬀerent places at the same

    time (Navarro and Matía, 2012; Tan and Zheng, 2013).

    Both UAV and UGV by itself have been successfully employed

    in plant phenotyping tasks. The coordination between UAV and

    UGV enables a new breakthrough application of UAV/UGV

    cooperative systems to achieve a common goal more eﬀectively

    and eﬃciently (Arbanas et al., 2018; Vu et al., 2018). Both vehicles

    in this cooperative team share complementarities according to

    their capabilities that allow them to operate in the same ﬁeld and

    work together to fulﬁll phenotyping missions. In this manner, the

    UAV can ﬂy to quickly obtain overview of the ﬁelds beyond the

    obstacles; whereas the UGV can continuously patrols in the ﬁeld

    with large payload capabilities of diﬀerent sensors and robotic

    arms (Chen et al., 2016; Roldán et al., 2016; Figure 5B). In the

    context of UAV-UGV cooperation, an obstacle map of the ﬁeld

    will be provided by the UAV for UGV path planning. Based on

    their communication and the map, the UGV can move rapidly

    between the crop rows for up-close plant investigation.

    CONCLUDING REMARKS

    Autonomous

    robotic

    technologies

    have

    the

    potential

    to

    substantially increase the speed, capacity, repeatability, and

    accuracy of data collection in plant phenotyping tasks. Many

    robotic systems are successfully developed and deployed in both

    greenhouse and ﬁeld environments, tested on a variety of plant

    species (row crops, specialty crops, and vineyards), and capable

    of measuring many traits related to morphology, structure,

    development, and physiology. Many technical challenges remain

    to be addressed regarding sensing, localization, path planning,

    Frontiers in Plant Science | www.frontiersin.org

    13

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    object detection, and obstacle avoidance. Intensive research is

    needed to overcome these limitations of phenotyping robots

    and improve their speed, accuracy, safety, and reliability.

    Collaborations among diﬀerent disciplines (such as plant science,

    agricultural engineering, mechanical and electrical engineering,

    and computer science) are imperative. With this transdisciplinary

    research, more eﬃcient and robust sensing and control systems

    will be developed for intelligent plant phenotyping robots.

    Sophisticated sensor modules can be developed using sensor-

    fusion techniques. Regarding the control systems, multiple

    intelligent algorithms (such as diﬀerent AI algorithms) can

    be combined to design more powerful controllers. These

    developments can potentially overcome the issues caused by

    changing environmental parameters, and complex structure of

    plants. Moreover, the suitable sensing and control systems yield

    better performance for accurate object detection (mainly for

    plants and crops, but also for humans, animals and other

    obstacles coexisting in the environments), path planning, and

    navigation. Suﬃcient funding from the public and private sources

    is the key to fuel the high-risk research in intelligent phenotyping

    robots in a sustainable way. We are optimistic that, in the

    next 10 years, we will see great leaps forward in autonomous

    and robotic technologies in plant phenotyping, enabled by the

    conﬂuence of the rapid advancements in sensing, controllers, and

    intelligent algorithms (AIs).

    AUTHOR CONTRIBUTIONS

    AA and YG provided the conceptualization of the manuscript.

    AA drafted the manuscript. YG, SP, and JS substantially edited

    the manuscript. All the authors contributed to the article and

    approved the submitted version.

    FUNDING

    The funding for this work was provided by USDA-NIFA under

    Grant No. 2017-67007-25941 and National Science Foundation

    under Grant No. OIA-1557417.

    REFERENCES

    Abel, J. (2018). In-Field Robotic Leaf Grasping and Automated Crop Spectroscopy.

    Pittsburgh, PA: Carnegie Mellon University.

    Adamides, G., Katsanos, C., Parmet, Y., Christou, G., Xenos, M., Hadzilacos, T.,

    et al. (2017). HRI usability evaluation of interaction modes for a teleoperated

    agricultural robotic sprayer. Appl. Ergon. 62, 237–246. doi: 10.1016/j.apergo.

    2017.03.008

    Afanasyev, I., Mazzara, M., Chakraborty, S., Zhuchkov, N., Maksatbek, A., Kassab,

    M., et al. (2019). Towards the internet of robotic things: analysis, architecture,

    components and challenges. arXiv [Preprint]. Avaliable online at: https://arxiv.

    org/abs/1907.03817 (accessed April 23, 2020).

    Afzal, A., Duiker, S. W., Watson, J. E., and Luthe, D. (2017). Leaf thickness
    and

    electrical capacitance as measures of plant water status. Trans. ASABE 60,

    1063–1074.

    Aguiar, A. S., Dos Santos, F. N., De Sousa, A. J. M., Oliveira, P. M., and Santos,
    L. C.

    (2020). Visual trunk detection using transfer learning and a deep learning-based

    coprocessor. IEEE Access 8, 77308–77320.

    Ahlin, K., Joﬀe, B., Hu, A.-P., McMurray, G., and Sadegh, N. (2016). Autonomous

    leaf picking using deep learning and visual-servoing. IFAC PapersOnLine 49,

    177–183. doi: 10.1016/j.ifacol.2016.10.033

    Alenya, G., Dellen, B., Foix, S., and Torras, C. (2013). Robotized plant probing:
    leaf

    segmentation utilizing time-of-ﬂight data. IEEE Robot. Autom. Mag. 20, 50–59.

    doi: 10.1109/MRA.2012.2230118

    Alenyà, G., Dellen, B., and Torras, C. (2011). “3D modelling of leaves from

    color and ToF data for robotized plant measuring,” in 2011 IEEE International

    Conference on Robotics and Automation, Shanghai, 3408–3414. doi: 10.1109/

    ICRA.2011.5980092

    Alenyà Ribas, G., Dellen, B., Foix Salmerón, S., and Torras, C. (2012). “Robotic

    leaf probing via segmentation of range data into surface patches,” in Proceedings

    of the 2012 IROS Workshop on Agricultural Robotics: Enabling Safe, Eﬃcient,

    Aﬀordable Robots for Food Production, Vilamoura, 1–6.

    Alenyá, G., Foix, S., and Torras, C. (2014). ToF cameras for active vision in
    robotics.

    Sensors Actuators A Phys. 218, 10–22. doi: 10.1016/j.sna.2014.07.014

    Andrade, F. H., Calvino, P., Cirilo, A., and Barbieri, P. (2002). Yield responses
    to

    narrow rows depend on increased radiation interception. Agron. J. 94, 975–980.

    Andrade-Sanchez, P., Gore, M. A., Heun, J. T., Thorp, K. R., Carmo-Silva, A. E.,

    French, A. N., et al. (2013). Development and evaluation of a ﬁeld-based high-

    throughput phenotyping platform. Funct. Plant Biol. 41, 68–79. doi: 10.1071/

    FP13126

    Andújar, D., Dorado, J., Bengochea-Guevara, J. M., Conesa-Muñoz, J., Fernández-

    Quintanilla, C., and Ribeiro, Á (2017). Inﬂuence of wind speed on

    RGB-D images in tree plantations. Sensors 17:914.

    doi: 10.3390/s1704

    0914

    Arad, B., Balendonck, J., Barth, R., Ben-Shahar, O., Edan, Y., Hellström, T.,
    et al.

    (2020). Development of a sweet pepper harvesting robot. J. F. Robot. 37,

    1027–1039. doi: 10.3390/s16081222

    Araus, J. L., and Cairns, J. E. (2014). Field high-throughput phenotyping: the
    new

    crop breeding frontier. Trends Plant Sci. 19, 52–61. doi: 10.1016/j.tplants.2013.

    09.008

    Arbanas, B., Ivanovic, A., Car, M., Orsag, M., Petrovic, T., and Bogdan, S. (2018).

    Decentralized planning and control for UAV–UGV cooperative teams. Auton.

    Robots 42, 1601–1618.

    Åstrand, B., and Baerveldt, A.-J. (2002). An agricultural mobile robot with vision-

    based perception for mechanical weed control. Auton. Robots 13, 21–35. doi:

    10.1023/A:1015674004201

    Åstrand, B., and Baerveldt, A.-J. (2005). A vision based row-following system

    for agricultural ﬁeld machinery. Mechatronics 15, 251–269. doi: 10.1016/j.

    mechatronics.2004.05.005

    Ateﬁ, A., Ge, Y., Pitla, S., and Schnable, J. (2019). In vivo human-like robotic

    phenotyping of leaf traits in maize and sorghum in greenhouse. Comput.

    Electron. Agric. 163:104854. doi: 10.1016/j.compag.2019.104854

    Ateﬁ, A., Ge, Y., Pitla, S., and Schnable, J. (2020). Robotic detection and grasp
    of

    maize and sorghum: stem measurement with contact. Robotics 9:58.

    Atkinson, J. A., Pound, M. P., Bennett, M. J., and Wells, D. M. (2019). Uncovering

    the hidden half of plants using new advances in root phenotyping. Curr. Opin.

    Biotechnol. 55, 1–8. doi: 10.1016/j.copbio.2018.06.002

    Bai, G., Ge, Y., Hussain, W., Baenziger, P. S., and Graef, G. (2016). A multi-

    sensor system for high throughput ﬁeld phenotyping in soybean and wheat

    breeding. Comput. Electron. Agric. 128, 181–192. doi: 10.1016/j.compag.2016.

    08.021

    Bao, Y., Tang, L., Breitzman, M. W., Salas Fernandez, M. G., and Schnable, P.
    S.

    (2019a). Field-based robotic phenotyping of sorghum plant architecture using

    stereo vision. J. F. Robot. 36, 397–415.

    Bao, Y., Tang, L., Srinivasan, S., and Schnable, P. S. (2019b). Field-based

    architectural traits characterisation of maize plant using time-of-ﬂight 3D

    imaging. Biosyst. Eng. 178, 86–101. doi: 10.1016/j.biosystemseng.2018.11.005

    Bao, Y., Zarecor, S., Shah, D., Tuel, T., Campbell, D. A., Chapman, A. V. E.,

    et al. (2019c). Assessing plant performance in the Enviratron. Plant Methods

    15, 1–14. doi: 10.1186/s13007-019-0504-y

    Bao, Y., Tang, L., and Shah, D. (2017). “Robotic 3D plant perception and

    leaf probing with collision-free motion planning for automated indoor plant

    phenotyping,” in 2017 ASABE Annual International Meeting, Spokane, WA, 1.

    doi: 10.13031/aim.201700369

    Frontiers in Plant Science | www.frontiersin.org

    14

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Barbieri, P. A., Rozas, H. n. R. S., Andrade, F. H., and Echeverria, H. n. E.
    (2000).

    Row spacing eﬀects at diﬀerent levels of nitrogen availability in maize. Agron.
    J.

    92, 283–288.

    Barth, R., Baur, J., Buschmann, T., Edan, Y., Hellström, T., Nguyen, T.,

    et al. (2014). “Using ROS for agricultural robotics-design considerations and

    experiences,” in Proceedings of the Second International Conference on Robotics

    and Associated High-Technologies and Equipment for Agriculture and Forestry,

    509–518.

    Batth, R. S., Nayyar, A., and Nagpal, A. (2018). “Internet of robotic things:

    driving intelligent robotics of future-concept, architecture, applications and

    technologies,” in 2018 4th International Conference on Computing Sciences

    (ICCS), Jalandhar: IEEE, 151–160.

    Batti, H., Ben Jabeur, C., and Seddik, H. (2020). Autonomous smart robot for path

    predicting and ﬁnding in maze based on fuzzy and neuro-Fuzzy approaches.

    Asian J. Control 23:2345.

    Baweja, H. S., Parhar, T., Mirbod, O., and Nuske, S. (2018). in StalkNet: A Deep

    Learning Pipeline for High-Throughput Measurement of Plant Stalk Count and

    Stalk Width BT - Field and Service Robotics, eds M. Hutter and R. Siegwart

    (Cham: Springer International Publishing), 271–284.

    Bayındır, L. (2016). A review of swarm robotics tasks. Neurocomputing 172,

    292–321. doi: 10.1016/j.neucom.2015.05.116

    Bechar, A., and Vigneault, C. (2016). Agricultural robots for ﬁeld operations:

    concepts and components. Biosyst.

    Eng. 149, 94–111. doi: 10.1016/j.

    biosystemseng.2016.06.014

    Behmann, J., Mahlein, A.-K., Paulus, S., Dupuis, J., Kuhlmann, H., Oerke, E.-C.,

    et al. (2016). Generation and application of hyperspectral 3D plant models:

    methods and challenges. Mach. Vis. Appl. 27, 611–624.

    Berardo, N., Brenna, O. V., Amato, A., Valoti, P., Pisacane, V., and Motto, M.

    (2004). Carotenoids concentration among maize genotypes measured by near

    infrared reﬂectance spectroscopy (NIRS). Innov. food Sci. Emerg. Technol. 5,

    393–398.

    Biber, P., Weiss, U., Dorna, M., and Albert, A. (2012). “Navigation system of

    the autonomous agricultural robot Bonirob,” in in Workshop on Agricultural

    Robotics: Enabling Safe, Eﬃcient, and Aﬀordable Robots for Food Production

    (Collocated with IROS 2012), Vilamoura.

    Biskup, B., Scharr, H., Fischbach, A., Wiese-Klinkenberg, A., Schurr, U., and

    Walter, A. (2009). diel growth cycle of isolated leaf discs analyzed with a

    novel, high-throughput three-dimensional imaging method is identical to that

    of intact leaves. Plant Physiol. 149, 1452–1461. doi: 10.1104/pp.108.134486

    Blasco, J., Aleixos, N., Roger, J. M., Rabatel, G., and Moltó, E. (2002). AE—

    Automation and emerging technologies: robotic weed control using machine

    vision. Biosyst. Eng. 83, 149–157. doi: 10.1006/bioe.2002.0109

    Blender, T., Buchner, T., Fernandez, B., Pichlmaier, B., and Schlegel, C. (2016).

    “Managing a Mobile Agricultural Robot Swarm for a seeding task,” in IECON

    2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,

    Florence, 6879–6886. doi: 10.1109/IECON.2016.7793638

    Brambilla, M., Ferrante, E., Birattari, M., and Dorigo, M. (2013). Swarm robotics:
    a

    review from the swarm engineering perspective. Swarm Intell. 7, 1–41.

    Breitzman, M. W., Bao, Y., Tang, L., Schnable, P. S., and Salas-Fernandez, M.
    G.

    (2019). Linkage disequilibrium mapping of high-throughput image-derived

    descriptors of plant architecture traits under ﬁeld conditions. F. Crop. Res.

    244:107619. doi: 10.1016/j.fcr.2019.107619

    Bruzzone, L., and Quaglia, G. (2012). Locomotion systems for ground mobile

    robots in unstructured environments. Mech. Sci. 3, 49–62.

    Burud, I., Lange, G., Lillemo, M., Bleken, E., Grimstad, L., and Johan From, P.

    (2017). Exploring robots and UAVs as phenotyping tools in plant breeding.

    IFAC-PapersOnLine 50, 11479–11484. doi: 10.1016/j.ifacol.2017.08.1591

    Busemeyer, L., Mentrup, D., Möller, K., Wunder, E., Alheit, K., Hahn, V., et al.

    (2013). BreedVision — A multi-sensor platform for non-destructive ﬁeld-

    based phenotyping in plant breeding. Sensors 13, 2830–2847. doi: 10.3390/

    s130302830

    Castrillo, M., Fernandez, D., Calcagno, A. M., Trujillo, I., and Guenni, L.

    (2001). Responses of ribulose-1, 5-bisphosphate carboxylase, protein content,

    and stomatal conductance to water deﬁcit in maize, tomato, and bean.

    Photosynthetica 39, 221–226.

    Chamanbaz, M., Mateo, D., Zoss, B. M., Toki´c, G., Wilhelm, E., Bouﬀanais, R.,

    et al. (2017). Swarm-enabling technology for multi-robot systems. Front. Robot.

    AI 4:12. doi: 10.3389/frobt.2017.00012

    Chaudhury, A., Ward, C., Talasaz, A., Ivanov, A. G., Brophy, M., Grodzinski, B.,

    et al. (2017). Machine vision system for 3D plant phenotyping. arXiv [Preprint].

    arXiv1705.00540.

    Chawade, A., van Ham, J., Blomquist, H., Bagge, O., Alexandersson, E., and Ortiz,

    R. (2019). High-throughput ﬁeld-phenotyping tools for plant breeding and

    precision agriculture. Agronomy 9:258.

    Chen, J., Zhang, X., Xin, B., and Fang, H. (2016). Coordination between unmanned

    aerial and ground vehicles: a taxonomy and optimization perspective. IEEE

    Trans. Cybern. 46, 959–972. doi: 10.1109/TCYB.2015.2418337

    Chen, Z., Wang, J., Wang, T., Song, Z., Li, Y., Huang, Y., et al. (2021). Automated

    in-ﬁeld leaf-level hyperspectral imaging of corn plants using a Cartesian robotic

    platform. Comput. Electron. Agric. 183:105996.

    Chlingaryan, A., Sukkarieh, S., and Whelan, B. (2018). Machine learning

    approaches for crop yield prediction and nitrogen status estimation in precision

    agriculture: a review. Comput. Electron. Agric. 151, 61–69. doi: 10.1016/j.

    compag.2018.05.012

    Chonnaparamutt, W., Kawasaki, H., Ueki, S., Murakami, S., and Koganemaru, K.

    (2009). “Development of a timberjack-like pruning robot: climbing experiment

    and fuzzy velocity control,” in 2009 ICCAS-SICE, Fukuoka, 1195–1199.

    Choudhuri, A., and Chowdhary, G. (2018). “Crop stem width estimation in highly

    cluttered ﬁeld environment,” in Proc. Comput. Vis. Probl. Plant Phenotyping

    (CVPPP 2018), Newcastle, 6–13.

    Cohen, Y., Huck, M. G., Hesketh, J. D., and Frederick, J. R. (1990). Sap ﬂow in
    the

    stem of water stressed soybean and maize plants. Irrig. Sci. 11, 45–50.

    Costa, C., Schurr, U., Loreto, F., Menesatti, P., and Carpentier, S. (2019). Plant

    phenotyping research trends, a science mapping approach. Front. Plant Sci.

    9:1933.

    Crowell, S., Falcão, A. X., Shah, A., Wilson, Z., Greenberg, A. J., and McCouch,

    S. R. (2014). High-resolution inﬂorescence phenotyping using a novel image-

    analysis pipeline, PANorama. Plant Physiol. 165, 479–495. doi: 10.1104/pp.114.

    238626

    Das Choudhury, S., Samal, A., and Awada, T. (2019). Leveraging image analysis
    for

    high-throughput plant phenotyping. Front. Plant Sci. 10:508. doi: 10.3389/fpls.

    2019.00508

    De Baerdemaeker, J., Munack, A., Ramon, H., and Speckmann, H. (2001).

    Mechatronic systems, communication, and control in precision agriculture.

    IEEE Control Syst. Mag. 21, 48–70. doi: 10.1016/j.aca.2020.11.008

    Dhondt, S., Wuyts, N., and Inzé, D. (2013). Cell to whole-plant phenotyping: the

    best is yet to come. Trends Plant Sci. 18, 428–439. doi: 10.1016/j.tplants.2013.

    04.008

    Duckett, T., Pearson, S., Blackmore, S., Grieve, B., Chen, W. H., Cielniak, G.,
    et al.

    (2018). Agricultural robotics: The future of robotic agriculture. arXiv [Preprint].

    arXiv1806.06762.

    Duguleana, M., and Mogan, G. (2016). Neural networks based reinforcement

    learning for mobile robots obstacle avoidance. Exp. Syst. Appl. 62, 104–115.

    doi: 10.1016/j.eswa.2016.06.021

    Dykes, L., Hoﬀmann, L. Jr., Portillo-Rodriguez, O., Rooney, W. L., and

    Rooney, L. W. (2014). Prediction of total phenols, condensed tannins,

    and 3-deoxyanthocyanidins in sorghum grain using near-infrared (NIR)

    spectroscopy. J. Cereal Sci. 60, 138–142.

    Ecarnot, M., Ba¸czyk, P., Tessarotto, L., and Chervin, C. (2013). Rapid phenotyping

    of the tomato fruit model, Micro-Tom, with a portable VIS–NIR spectrometer.

    Plant Physiol. Biochem. 70, 159–163. doi: 10.1016/j.plaphy.2013.05.019

    ElMasry, G., Wang, N., ElSayed, A., and Ngadi, M. (2007). Hyperspectral imaging

    for nondestructive determination of some quality attributes for strawberry.

    J. Food Eng. 81, 98–107.

    Fahlgren, N., Feldman, M., Gehan, M. A., Wilson, M. S., Shyu, C., Bryant, D. W.,

    et al. (2015). A versatile phenotyping system and analytics platform reveals

    diverse temporal responses to water availability in setaria. Mol. Plant 8, 1520–

    1535. doi: 10.1016/j.molp.2015.06.005

    Faroq, A.-T., Adam, H., Dos Anjos, A., Lorieux, M., Larmande, P., Ghesquière,
    A.,

    et al. (2013). P-TRAP: a panicle trait phenotyping tool. BMC Plant Biol. 13:122.

    doi: 10.1186/1471-2229-13-122

    Fernandez, M. G. S., Bao, Y., Tang, L., and Schnable, P. S. (2017). A high-

    throughput, ﬁeld-based phenotyping technology for tall biomass crops. Plant

    Physiol. 174, 2008–2022. doi: 10.1104/pp.17.00707

    Fiorani, F., and Schurr, U. (2013). Future scenarios for plant phenotyping. Annu.

    Rev. Plant Biol. 64, 267–291. doi: 10.1146/annurev-arplant-050312-120137

    Frontiers in Plant Science | www.frontiersin.org

    15

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Fischer, G. (2009). “World food and agriculture to 2030/50,” in Technical Paper

    From the Expert Meeting on How to Feed the World in, Rome, 24–26.

    Foix, S., Alenyà, G., and Torras, C. (2015). “3D Sensor planning framework for
    leaf

    probing,” in 2015 IEEE/RSJ International Conference on Intelligent Robots and

    Systems (IROS), Hamburg, 6501–6506. doi: 10.1109/IROS.2015.7354306

    Foix, S., Alenyà, G., and Torras, C. (2018). Task-driven active sensing framework

    applied to leaf probing. Comput. Electron. Agric. 147, 166–175. doi: 10.1016/j.

    compag.2018.01.020

    Franceschetti, A., Tosello, E., Castaman, N., and Ghidoni, S. (2018). Robotic
    Arm

    Control and Task Training through Deep Reinforcement Learning.

    Furbank, R. T., and Tester, M. (2011). Phenomics – technologies to relieve the

    phenotyping bottleneck. Trends Plant Sci. 16, 635–644. doi: 10.1016/j.tplants.

    2011.09.005

    Gage, J. L., Miller, N. D., Spalding, E. P., Kaeppler, S. M., and de Leon, N.
    (2017).

    TIPS: a system for automated image-based phenotyping of maize tassels. Plant

    Methods 13:21. doi: 10.1186/s13007-017-0172-8

    Gage, J. L., Richards, E., Lepak, N., Kaczmar, N., Soman, C., Chowdhary, G., et
    al.

    (2019). In-ﬁeld whole plant maize architecture characterized by latent space

    phenotyping. bioRxiv [Preprint]. doi: 10.1101/763342

    Gao, T., Emadi, H., Saha, H., Zhang, J., Lofquist, A., Singh, A., et al. (2018).
    A

    novel multirobot system for plant phenotyping. Robotics 7:61. doi: 10.3390/

    robotics7040061

    Ge, Y., Ateﬁ, A., Zhang, H., Miao, C., Ramamurthy, R. K., Sigmon, B., et al.

    (2019). High-throughput analysis of leaf physiological and chemical traits with

    VIS–NIR–SWIR spectroscopy: a case study with a maize diversity panel. Plant

    Methods 15:66. doi: 10.1186/s13007-019-0450-8

    Ge, Y., Bai, G., Stoerger, V., and Schnable, J. C. (2016). Temporal dynamics of

    maize plant growth, water use, and leaf water content using automated high

    throughput RGB and hyperspectral imaging. Comput. Electron. Agric. 127,

    625–632. doi: 10.1016/j.compag.2016.07.028

    Gegas, V. C., Nazari, A., Griﬃths, S., Simmonds, J., Fish, L., Orford, S., et
    al. (2010).

    A genetic framework for grain size and shape variation in wheat. Plant Cell 22,

    1046–1056. doi: 10.1105/tpc.110.074153

    Gonzalez-de-Soto, M., Emmi, L., Perez-Ruiz, M., Aguera, J., and Gonzalez-de-

    Santos, P. (2016). Autonomous systems for precise spraying – Evaluation

    of a robotised patch sprayer. Biosyst. Eng. 146, 165–182. doi: 10.1016/j.

    biosystemseng.2015.12.018

    Granier, C., and Tardieu, F. (2009). Multi-scale phenotyping of leaf expansion
    in

    response to environmental changes: the whole is more than the sum of parts.

    Plant. Cell Environ. 32, 1175–1184. doi: 10.1111/j.1365-3040.2009.01955.x

    Grieco, L. A., Rizzo, A., Colucci, S., Sicari, S., Piro, G., Di Paola, D., et
    al. (2014).

    IoT-aided robotics applications: technological implications, target domains and

    open issues. Comput. Commun. 54, 32–47.

    Grift, T., Zhang, Q., Kondo, N., and Ting, K. C. (2008). A review of automation

    and robotics for the bio-industry. J. Biomechatronics Eng. 1, 37–54.

    Grimstad, L., and From, P. J. (2017). Thorvald II - a modular and re-conﬁgurable

    agricultural robot. IFAC PapersOnLine 50, 4588–4593. doi: 10.1016/j.ifacol.

    2017.08.1005

    Guo, Z., Huang, W., Chen, L., Wang, X., and Peng, Y. (2013). “Nondestructive

    evaluation of soluble solid content in strawberry by near infrared spectroscopy,”

    in Piageng 2013: Image Processing and Photonics for Agricultural Engineering,

    (Bellingham: International Society for Optics and Photonics), 87610O.

    Han, L., Yang, G., Yang, H., Xu, B., Li, Z., and Yang, X. (2018). Clustering ﬁeld-

    based maize phenotyping of plant-height growth and canopy spectral dynamics

    using a UAV remote-sensing approach. Front. Plant Sci. 9:1638. doi: 10.3389/

    fpls.2018.01638

    Happ, M. M., Wang, H., Graef, G. L., and Hyten, D. L. (2019). Generating high

    density, low cost genotype data in soybean [Glycine max (L.) Merr.]. G3 Genes

    Genomes Genet. 9, 2153–2160. doi: 10.1534/g3.119.400093

    Hassan, M. A., Yang, M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., et al.

    (2019). A rapid monitoring of NDVI across the wheat growth cycle for grain

    yield prediction using a multi-spectral UAV platform. Plant Sci. 282, 95–103.

    doi: 10.1016/j.plantsci.2018.10.022

    Hassanijalilian, O., Igathinathane, C., Bajwa, S., and Nowatzki, J. (2020a). Rating

    iron deﬁciency in soybean using image processing and decision-tree based

    models. Remote Sens. 12:4143.

    Hassanijalilian, O., Igathinathane, C., Doetkott, C., Bajwa, S., Nowatzki, J.,
    and Haji

    Esmaeili, S. A. (2020b). Chlorophyll estimation in soybean leaves inﬁeld with

    smartphone digital imaging and machine learning. Comput. Electron. Agric.

    174:105433. doi: 10.1016/j.compag.2020.10543

    Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J.,

    et al. (2010). Evaluation of a strawberry-harvesting robot in a ﬁeld test. Biosyst.

    Eng. 105, 160–171. doi: 10.1016/j.biosystemseng.2009.09.011

    Hejazipoor, H., Massah, J., Soryani, M., Vakilian, K. A., and Chegini, G. (2021).
    An

    intelligent spraying robot based on plant bulk volume. Comput. Electron. Agric.

    180:105859.

    Hemming, J., Bac, C. W., van Tuijl, B. A. J., Barth, R., Bontsema, J., Pekkeriet,
    E. J.,

    et al. (2014). “A robot for harvesting sweet-pepper in greenhouses,” in Paper

    Presented at AgEng 2014, Zurich.

    Hughes, J., Culha, U., Giardina, F., Guenther, F., Rosendo, A., and Iida, F. (2016).

    Soft manipulators and grippers: a review. Front. Robot. AI 3:69.

    Iqbal, J., Xu, R., Halloran, H., and Li, C. (2020). Development of a Multi-Purpose

    Autonomous Diﬀerential Drive Mobile Robot for Plant Phenotyping and Soil

    Sensing. Electronics 9:1550.

    Irshat, K., Petr, R., and Irina, R. (2018). “The selecting of artiﬁcial intelligence

    technology for control of mobile robots,” in 2018 International Multi-

    Conference on Industrial Engineering and Modern Technologies (FarEastCon),

    Vladivostok: IEEE, 1–4.

    Ishigure, Y., Hirai, K., and Kawasaki, H. (2013). “A pruning robot with a power-

    saving chainsaw drive,” in 2013 IEEE International Conference on Mechatronics

    and Automation, Takamatsu, 1223–1228. doi: 10.1109/ICMA.2013.6618088

    Jannink, J.-L., Lorenz, A. J., and Iwata, H. (2010). Genomic selection in plant

    breeding: from theory to practice. Brief. Funct. Genomics 9, 166–177. doi: 10.

    1093/bfgp/elq001

    Jay, S., Rabatel, G., Hadoux, X., Moura, D., and Gorretta, N. (2015). In-ﬁeld

    crop row phenotyping from 3D modeling performed using structure from

    Motion. Comput. Electron. Agric. 110, 70–77. doi: 10.1016/j.compag.2014.

    09.021

    Jenkins, M., and Kantor, G. (2017). “Online detection of occluded plant stalks
    for

    manipulation,” in Proceedings of the 2017 IEEE/RSJ International Conference

    on Intelligent Robots and Systems (IROS), 5162–5167. doi: 10.1109/IROS.2017.

    8206404

    Jin, X., Zarco-Tejada, P., Schmidhalter, U., Reynolds, M. P., Hawkesford, M. J.,

    Varshney, R. K., et al. (2020). High-throughput estimation of crop traits: a

    review of ground and aerial phenotyping platforms. IEEE Geosci. Remote Sens.

    Mag. 2, 1–33.

    Kicherer, A., Herzog, K., Pﬂanz, M., Wieland, M., Rüger, P., Kecke, S., et al.
    (2015).

    An automated ﬁeld phenotyping pipeline for application in grapevine research.

    Sensors 15, 4823–4836. doi: 10.3390/s150304823

    Kim, G.-H., Kim, S.-C., Hong, Y.-K., Han, K.-S., and Lee, S.-G. (2012). “A robot

    platform for unmanned weeding in a paddy ﬁeld using sensor fusion,” in

    Proceedings of the 2012 IEEE International Conference on Automation Science

    and Engineering (CASE), (Piscataway, NJ: IEEE), 904–907.

    Kong, E., Liu, D., Guo, X., Yang, W., Sun, J., Li, X., et al. (2013). Anatomical
    and

    chemical characteristics associated with lodging resistance in wheat. Crop J.
    1,

    43–49. doi: 10.1016/j.cj.2013.07.012

    Lee, U., Chang, S., Putra, G. A., Kim, H., and Kim, D. H. (2018). An automated,

    high-throughput plant phenotyping system using machine learning-based plant

    segmentation and image analysis. PLoS One 13:e0196615. doi: 10.1371/journal.

    pone.0196615

    Leilah, A. A., and Al-Khateeb, S. A. (2005). Statistical analysis of wheat yield
    under

    drought conditions. J. Arid Environ. 61, 483–496. doi: 10.1016/j.jaridenv.2004.

    10.011

    Li, L., Zhang, Q., and Huang, D. (2014). A review of imaging techniques for plant

    phenotyping. Sensors 14, 20078–20111. doi: 10.3390/s141120078

    Li, M., Imou, K., Wakabayashi, K., and Yokoyama, S. (2009). Review of research
    on

    agricultural vehicle autonomous guidance. Int. J. Agric. Biol. Eng. 2, 1–16.

    Li, Z., Guo, R., Li, M., Chen, Y., and Li, G. (2020a). A review of computer vision

    technologies for plant phenotyping. Comput. Electron. Agric. 176:105672.

    Li, Z., Zhang, Q., Li, J., Yang, X., Wu, Y., Zhang, Z., et al. (2020b). Solar-induced

    chlorophyll ﬂuorescence and its link to canopy photosynthesis in maize from

    continuous ground measurements. Remote Sens. Environ. 236:111420. doi: 10.

    1016/j.rse.2019.111420

    Lili, W., Bo, Z., Jinwei, F., Xiaoan, H., Shu, W., Yashuo, L., et al. (2017).

    Development of a tomato harvesting robot used in greenhouse. Int. J. Agric.

    Biol. Eng. 10, 140–149.

    Frontiers in Plant Science | www.frontiersin.org

    16

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Lopes, C. M., Graça, J., Sastre, J., Reyes, M., Guzmán, R., Braga, R., et al.
    (2016).

    “Vineyard yeld estimation by VINBOT robot-preliminary results with the white

    variety Viosinho,” in Proceedings 11th Int. Terroir Congress, eds G. Jones and
    N.

    Doran (Ashland, USA: Southern Oregon University), 458–463.

    Lu, H., Tang, L., Whitham, A. S., and Mei, Y. (2017). A robotic platform for corn

    seedling morphological traits characterization. Sensors 17:2082. doi: 10.3390/

    s17092082

    Madden, S. (2012). From databases to big data. IEEE Internet Comput. 16, 4–6.

    Mancini, M., Mazzoni, L., Gagliardi, F., Balducci, F., Duca, D., Toscano, G.,
    et al.

    (2020). Application of the non-destructive NIR technique for the evaluation of

    strawberry fruits quality parameters. Foods 9:441. doi: 10.3390/foods9040441

    McMullen, M. D., Kresovich, S., Villeda, H. S., Bradbury, P., Li, H., Sun, Q.,
    et al.

    (2009). Genetic properties of the maize nested association mapping population.

    Science 325, 737–740. doi: 10.1126/science.1174320

    Mishra, K. B., Mishra, A., Klem, K., and Govindjee. (2016). Plant phenotyping:

    a perspective. Indian J. Plant Physiol. 21, 514–527. doi: 10.1007/s40502-016-

    0271-y

    Mishra, P., Asaari, M. S. M., Herrero-Langreo, A., Lohumi, S., Diezma, B., and

    Scheunders, P. (2017). Close range hyperspectral imaging of plants: a review.

    Biosyst. Eng. 164, 49–67.

    Moeinizade, S., Hu, G., Wang, L., and Schnable, P. S. (2019). Optimizing selection

    and mating in genomic selection with a look-ahead approach: an operations

    research framework. G3 Genes, Genomes, Genet. 9, 2123–2133.

    Montes, J. M., Melchinger, A. E., and Reif, J. C. (2007). Novel throughput

    phenotyping platforms in plant genetic studies. Trends Plant Sci. 12, 433–436.

    doi: 10.1016/j.tplants.2007.08.006

    Mueller-Sim, T., Jenkins, M., Abel, J., and Kantor, G. (2017). “The robotanist:

    a ground-based agricultural robot for high-throughput crop phenotyping,”

    in 2017 IEEE International Conference on Robotics and Automation (ICRA),

    Singapore, 3634–3639. doi: 10.1109/ICRA.2017.7989418

    Murman, J. N. (2019). Flex-Ro: A Robotic High Throughput Field Phenotyping

    System.

    Mutka, A. M., and Bart, R. S. (2015). Image-based phenotyping of plant disease

    symptoms. Front. Plant Sci. 5:734. doi: 10.3389/fpls.2014.00734

    Narvaez, F. Y., Reina, G., Torres-Torriti, M., Kantor, G., and Cheein, F. A.

    (2017). A survey of ranging and imaging techniques for precision agriculture

    phenotyping. IEEE ASME Trans. Mechatronics 22, 2428–2439.

    Navarro, I., and Matía, F. (2012). An introduction to swarm robotics. Isrn Robot.

    2013:608164.

    Nishizawa, T., Mori, Y., Fukushima, S., Natsuga, M., and Maruyama, Y. (2009).

    Non-destructive analysis of soluble sugar components in strawberry fruits using

    near-infrared spectroscopy. Nippon Shokuhin Kagaku Kogaku KaishiJ. Japanese

    Soc. Food Sci. Technol. 56, 229–235.

    Nof, S. Y. (2009). Springer Handbook of Automation. Berlin: Springer Science &

    Business Media.

    Oberti, R., Marchi, M., Tirelli, P., Calcante, A., Iriti, M., Tona, E., et al.
    (2016).

    Selective spraying of grapevines for disease control using a modular agricultural

    robot.

    Biosyst.

    Eng.

    146,

    203–215.

    doi:

    10.1016/j.biosystemseng.2015.

    12.004

    Ohashi, Y., Nakayama, N., Saneoka, H., and Fujita, K. (2006). Eﬀects of drought

    stress on photosynthetic gas exchange, chlorophyll ﬂuorescence and stem

    diameter of soybean plants. Biol. Plant. 50, 138–141.

    Palazzari, V., Mezzanotte, P., Alimenti, F., Fratini, F., Orecchini, G., and Roselli,
    L.

    (2017). Leaf compatible “eco-friendly” temperature sensor clip for high density

    monitoring wireless networks. Wirel. Power Transf. 4, 55–60.

    Pandey, P., Dakshinamurthy, H. N., and Young, S. N. (2021). Autonomy in

    detection, actuation, and planning for robotic weeding systems. Trans. ASABE.

    Panozzo, J. F., Eagles, H. A., Cawood, R. J., and Wootton, M. (1999). Wheat spike

    temperatures in relation to varying environmental conditions. Aust. J. Agric.

    Res. 50, 997–1006. doi: 10.1371/journal.pone.0189594

    Park, J., Delgado, R., and Choi, B. W. (2020). Real-time characteristics of ROS
    2.0 in

    multiagent robot systems: an empirical study. IEEE Access 8, 154637–154651.

    Parhar, T., Baweja, H., Jenkins, M., and Kantor, G. (2018). “A deep learning-based

    stalk grasping pipeline,” in 2018 IEEE International Conference on Robotics and

    Automation (ICRA), Brisbane, QLD: IEEE, 6161–6167.

    Penchaiya, P., Bobelyn, E., Verlinden, B. E., Nicolaï, B. M., and Saeys, W. (2009).

    Non-destructive measurement of ﬁrmness and soluble solids content in bell

    pepper using NIR spectroscopy. J. Food Eng. 94, 267–273.

    Pieruschka, R., and Schurr, U. (2019). Plant phenotyping: past, present, and future.

    Plant Phenomics 2019, 1–6.

    Phillips, R. L. (2010). Mobilizing science to break yield barriers. Crop Sci.
    50, S–99.

    Pound, M. P., Atkinson, J. A., Townsend, A. J., Wilson, M. H., Griﬃths, M.,

    Jackson, A. S., et al. (2017). Deep machine learning provides state-of-the-art

    performance in image-based plant phenotyping. Gigascience 6:gix083.

    Qiu, Q., Sun, N., Bai, H., Wang, N., Fan, Z., Wang, Y., et al. (2019). Field-based

    high-throughput phenotyping for maize plant using 3D LiDAR point cloud

    generated with a “Phenomobile.”. Front. Plant Sci. 10:554. doi: 10.3389/fpls.

    2019.00554

    Qiu, R., Wei, S., Zhang, M., Li, H., Sun, H., Liu, G., et al. (2018). Sensors
    for

    measuring plant phenotyping: a review. Int. J. Agric. Biol. Eng. 11, 1–17.

    Quaglia, G., Visconte, C., Scimmi, L. S., Melchiorre, M., Cavallone, P., and

    Pastorelli, S. (2020). Design of a UGV powered by solar energy for precision

    agriculture. Robotics 9:13.

    Raja, R., Nguyen, T. T., Slaughter, D. C., and Fennimore, S. A. (2020). Real-time

    robotic weed knife control system for tomato and lettuce based on geometric

    appearance of plant labels. Biosyst. Eng. 194, 152–164.

    Rahaman, M. M., Chen, D., Gillani, Z., Klukas, C., and Chen, M. (2015). Advanced

    phenotyping and phenotype data analysis for the study of plant growth and

    development. Front. Plant Sci. 6:619. doi: 10.3389/fpls.2015.00619

    Ray, P. P. (2016). Internet of robotic things: concept, technologies, and challenges.

    IEEE Access 4, 9489–9500.

    Richards, R. A., Rebetzke, G. J., Watt, M., Condon, A. G., Spielmeyer, W., and

    Dolferus, R. (2010). Breeding for improved water productivity in temperate

    cereals: phenotyping, quantitative trait loci, markers and the selection

    environment. Funct. Plant Biol. 37, 85–97. doi: 10.1071/FP09219 (Tony),

    Roldán, J. J., Garcia-Aunon, P., Garzón, M., De León, J., Del Cerro, J.,

    and Barrientos, A. (2016). Heterogeneous multi-robot system for mapping

    environmental variables of greenhouses. Sensors 16:1018.

    doi: 10.3390/

    s16071018

    Ruckelshausen, A., Biber, P., Dorna, M., Gremmes, H., Klose, R., Linz, A., et
    al.

    (2009). BoniRob–an autonomous ﬁeld robot platform for individual plant

    phenotyping. Precis. Agric. 9, 841–847. doi: 10.3390/s17010214

    Rudolph, R., Herzog, K., Töpfer, R., and Steinhage, V. (2019). Eﬃcient

    identiﬁcation, localization and quantiﬁcation of grapevine inﬂorescences and

    ﬂowers in unprepared ﬁeld images using Fully Convolutional Networks. Vitis

    58, 95–104.

    Sandhu, J., Zhu, F., Paul, P., Gao, T., Dhatt, B. K., Ge, Y., et al. (2019). PI-Plat:

    a high-resolution image-based 3D reconstruction method to estimate growth

    dynamics of rice inﬂorescence traits. Plant Methods 15:162.

    doi: 10.1186/

    s13007-019-0545-2

    Saravanan, M., Perepu, S. K., and Sharma, A. (2018). “Exploring collective behavior

    of internet of robotic things for indoor plant health monitoring,” in 2018 IEEE

    International Conference on Internet of Things and Intelligence System (IOTAIS),

    Bali: IEEE, 148–154.

    Schulz, H., and Baranska, M. (2007). Identiﬁcation and quantiﬁcation of valuable

    plant substances by IR and Raman spectroscopy. Vib. Spectrosc. 43, 13–25.

    doi: 10.1016/j.vibspec.2006.06.001

    Shaﬁekhani, A., Kadam, S., Fritschi, B. F., and DeSouza, N. G. (2017). Vinobot

    and vinoculer: two robotic platforms for high-throughput ﬁeld phenotyping.

    Sensors 17:214. doi: 10.3390/s17010214

    Shah, D., Tang, L., Gai, J., and Putta-Venkata, R. (2016). Development of a mobile

    robotic phenotyping system for growth chamber-based studies of genotype

    x environment interactions. IFAC PapersOnLine 49, 248–253. doi: 10.1016/j.

    ifacol.2016.10.046

    Shahrimie, M. A. M., Mishra, P., Mertens, S., Dhondt, S., Wuyts, N., and

    Scheunders, P. (2016). “Modeling eﬀects of illumination and plant geometry

    on leaf reﬂectance spectra in close-range hyperspectral imaging,” in 2016 8th

    Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote

    Sensing (WHISPERS), Los Angeles, CA: IEEE, 1–4.

    Shalal, N., Low, T., McCarthy, C., and Hancock, N. (2013). A Review of

    Autonomous Navigation Systems in Agricultural Environments.

    Shamshiri, R. R., Weltzien, C., Hameed, I. A., Yule, I. J., Grift, T. E., Balasundram,

    S. K., et al. (2018). Research and development in agricultural robotics: a

    perspective of digital farming. Int. J. Agric. Biol. Eng. 11, 1–14.

    Shao, Y., and He, Y. (2008). Nondestructive measurement of acidity of strawberry

    using Vis/NIR spectroscopy. Int. J. Food Prop. 11, 102–111.

    Frontiers in Plant Science | www.frontiersin.org

    17

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Silwal, A., Davidson, J. R., Karkee, M., Mo, C., Zhang, Q., and Lewis, K. (2017).

    Design, integration, and ﬁeld evaluation of a robotic apple harvester. J. F. Robot.

    34, 1140–1159. doi: 10.1002/rob.21715

    Singh, A., Ganapathysubramanian, B., Singh, A. K., and Sarkar, S. (2016). Machine

    learning for high-throughput stress phenotyping in plants. Trends Plant Sci. 21,

    110–124.

    Smitt, C., Halstead, M., Zaenker, T., Bennewitz, M., and McCool, C. (2020).

    PATHoBot: A Robot for Glasshouse Crop Phenotyping and Intervention. arXiv

    [Preprint]. arXiv2010.16272.

    Taghavifar, H., Xu, B., Taghavifar, L., and Qin, Y. (2019). Optimal path-planning

    of nonholonomic terrain robots for dynamic obstacle avoidance using single-

    time velocity estimator and reinforcement learning approach. IEEE Access 7,

    159347–159356.

    Tan, Y., and Zheng, Z. (2013). Research advance in swarm robotics. Def. Technol.

    9, 18–39. doi: 10.1016/j.dt.2013.03.001

    Tripodi, P., Massa, D., Venezia, A., and Cardi, T. (2018). Sensing technologies
    for

    precision phenotyping in vegetable crops: current status and future challenges.

    Agronomy 8:57.

    Tsubo, M., and Walker, S. (2002). A model of radiation interception and use by
    a

    maize–bean intercrop canopy. Agric. For. Meteorol. 110, 203–215.

    Ubbens, J., Cieslak, M., Prusinkiewicz, P., Parkin, I., Ebersbach, J., and Stavness,

    I. (2020). Latent space phenotyping: automatic image-based phenotyping for

    treatment studies. Plant Phenomics 2020:5801869. doi: 10.34133/2020/5801869

    Underwood, J., Wendel, A., Schoﬁeld, B., McMurray, L., and Kimber, R. (2017).

    Eﬃcient in-ﬁeld plant phenomics for row-crops with an autonomous ground

    vehicle. J. F. Robot. 34, 1061–1083.

    van Henten, E. J., Hemming, J., van Tuijl, B. A. J., Kornet, J. G., Meuleman,
    J.,

    Bontsema, J., et al. (2002). An autonomous robot for harvesting cucumbers in

    greenhouses. Auton. Robots 13, 241–258. doi: 10.1023/A:1020568125418

    Vázquez-Arellano, M., Paraforos, D. S., Reiser, D., Garrido-Izard, M., and

    Griepentrog, H. W. (2018). Determination of stem position and height of

    reconstructed maize plants using a time-of-ﬂight camera. Comput. Electron.

    Agric. 154, 276–288.

    Vidoni, R., Gallo, R., Ristorto, G., Carabin, G., Mazzetto, F., Scalera, L., et
    al. (2017).

    “ByeLab: An agricultural mobile robot prototype for proximal sensing and

    precision farming,” in ASME International Mechanical Engineering Congress

    and Exposition, (New York, NY: American Society of Mechanical Engineers),

    V04AT05A057.

    Vijayarangan, S., Sodhi, P., Kini, P., Bourne, J., Du, S., Sun, H., et al. (2018).
    in

    High-Throughput Robotic Phenotyping of Energy Sorghum Crops BT - Field and

    Service Robotics, eds M. Hutter and R. Siegwart (Cham: Springer International

    Publishing), 99–113.

    Virlet, N., Sabermanesh, K., Sadeghi-Tehran, P., and Hawkesford, M. J. (2017).

    Field scanalyzer: an automated robotic ﬁeld phenotyping platform for detailed

    crop monitoring. Funct. Plant Biol. 44, 143–153. doi: 10.1071/FP16163

    Vu, Q., Rakovi´c, M., Delic, V., and Ronzhin, A. (2018). “Trends in development
    of

    UAV-UGV cooperation approaches,” in Precision Agriculture BT - Interactive

    Collaborative Robotics, eds A. Ronzhin, G. Rigoll, and R. Meshcheryakov

    (Cham: Springer International Publishing), 213–221.

    Walter, A., Liebisch, F., and Hund, A. (2015). Plant phenotyping: from bean

    weighing to image analysis. Plant Methods 11:14. doi: 10.1186/s13007-015-

    0056-8

    Wang, H., Peng, J., Xie, C., Bao, Y., and He, Y. (2015). Fruit quality evaluation
    using

    spectroscopy technology: a review. Sensors 15, 11889–11927.

    Wang, L., Zhu, G., Johnson, W., and Kher, M. (2018). Three new approaches to

    genomic selection. Plant Breed. 137, 673–681.

    Wei, H., Shao, Z., Huang, Z., Chen, R., Guan, Y., Tan, J., et al. (2016). RT-ROS:

    a real-time ROS architecture on multi-core processors. Futur. Gener. Comput.

    Syst. 56, 171–178. doi: 10.1016/j.future.2015.05.008

    Weiss, U., and Biber, P. (2011). Plant detection and mapping for agricultural
    robots

    using a 3D LIDAR sensor. Rob. Auton. Syst. 59, 265–273. doi: 10.1016/j.robot.

    2011.02.011

    White, J. W., Andrade-Sanchez, P., Gore, M. A., Bronson, K. F., Coﬀelt, T. A.,

    Conley, M. M., et al. (2012). Field-based phenomics for plant genetics research.

    F. Crop. Res. 133, 101–112. doi: 10.1016/j.fcr.2012.04.003

    Wolfert, S., Ge, L., Verdouw, C., and Bogaardt, M.-J. (2017). Big data in smart

    farming – a review. Agric. Syst. 153, 69–80. doi: 10.1016/j.agsy.2017.01.023

    Wu, C., Zeng, R., Pan, J., Wang, C. C. L., and Liu, Y. (2019). Plant phenotyping

    by deep-learning-based planner for multi-robots. IEEE Robot. Autom. Lett. 4,

    3113–3120. doi: 10.1109/LRA.2019.2924125

    Wu, G. F., Huang, L. X., and He, Y. (2008). Research on the sugar content

    measurement of grape and berries by using Vis/NIR spectroscopy technique.

    Guang pu xue yu Guang pu fen xiGuang pu 28, 2090–2093.

    Würschum, T. (2019). “Modern ﬁeld phenotyping opens new avenues for

    selection,” in Applications of Genetic and Genomic Research in Cereals, eds T.

    Miedaner and V. Korzun (Amsterdam: Elsevier), 233–250.

    Xiong, Y., Ge, Y., Grimstad, L., and From, P. J. (2020). An autonomous

    strawberry-harvesting robot: Design, development, integration, and ﬁeld

    evaluation. J. F. Robot. 37, 202–224.

    Xiong, B., Wang, B., Xiong, S., Lin, C., and Yuan, X. (2019). 3D morphological

    processing for wheat spike phenotypes using computed tomography images.

    Remote Sens. 11:1110.

    Xu, R., Li, C., and Mohammadpour Velni, J. (2018). Development of an

    autonomous ground robot for ﬁeld high throughput phenotyping. IFAC-

    PapersOnLine 51, 70–74. doi: 10.1016/j.ifacol.2018.08.063

    Young, S. N., Kayacan, E., and Peschel, J. M. (2019). Design and ﬁeld evaluation

    of a ground robot for high-throughput phenotyping of energy sorghum. Precis.

    Agric. 20, 697–722. doi: 10.1007/s11119-018-9601-6

    Yuan, H., Wang, N., Bennett, R., Burditt, D., Cannon, A., and Chamberlin,

    K. (2018). Development of a ground-based peanut canopy phenotyping

    system.

    IFAC

    PapersOnLine

    51,

    162–165.

    doi:

    10.1016/j.ifacol.2018.

    08.081

    Zahid, A., Mahmud, M. S., He, L., Choi, D., Heinemann, P., and Schupp, J. (2020).

    Development of an integrated 3R end-eﬀector with a cartesian manipulator for

    pruning apple trees. Comput. Electron. Agric. 179:105837.

    Zhang, F., Leitner, J., Milford, M., Upcroft, B., and Corke, P. (2015). Towards

    vision-based deep reinforcement learning for robotic motion control. arXiv

    [Preprint]. arXiv1511.03791.

    Zhang, J., Gong, L., Liu, C., Huang, Y., Zhang, D., and Yuan, Z. (2016).

    Field phenotyping robot design and validation for the crop breeding. IFAC

    PapersOnLine 49, 281–286. doi: 10.1016/j.ifacol.2016.10.052

    Zhang, Q., Karkee, M., and Tabb, A. (2019). The use of agricultural robots in

    orchard management. arXiv [Preprint]. arXiv1907.13114.

    Zhang, Z., Kayacan, E., Thompson, B., and Chowdhary, G. (2020). High

    precision control and deep learning-based corn stand counting algorithms for

    agricultural robot. Auton. Robots 44, 1289–1302.

    Zhao, J., Mantilla Perez, M. B., Hu, J., and Salas Fernandez, M. G. (2016).

    Genome-wide association study for nine plant architecture traits in sorghum.

    Plant Genome 9, 1–14. doi: 10.3835/plantgenome2015.06.0044

    Zhou, Y., Srinivasan, S., Mirnezami, S. V., Kusmec, A., Fu, Q., Attigala, L.,

    et al. (2019). Semiautomated feature extraction from RGB images for sorghum

    panicle architecture GWAS. Plant Physiol. 179, 24–37. doi: 10.1104/pp.18.

    00974

    Conﬂict of Interest: The authors declare that the research was conducted in the

    absence of any commercial or ﬁnancial relationships that could be construed as
    a

    potential conﬂict of interest.

    Copyright © 2021 Ateﬁ, Ge, Pitla and Schnable. This is an open-access article

    distributed under the terms of the Creative Commons Attribution License (CC BY).

    The use, distribution or reproduction in other forums is permitted, provided the

    original author(s) and the copyright owner(s) are credited and that the original

    publication in this journal is cited, in accordance with accepted academic practice.
    No

    use, distribution or reproduction is permitted which does not comply with these
    terms.

    Frontiers in Plant Science | www.frontiersin.org

    18

    June 2021 | Volume 12 | Article 611940

    '
  inline_citation: '>'
  journal: Frontiers in plant science
  limitations: '>'
  pdf_link: https://www.frontiersin.org/articles/10.3389/fpls.2021.611940/pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Robotic Technologies for High-Throughput Plant Phenotyping: Contemporary
    Reviews and Future Perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/f13060911
  analysis: '>'
  authors:
  - André Duarte
  - N. M. G. Borralho
  - Pedro Cabral
  - Mário Caetano
  citation_count: 36
  full_citation: '>'
  full_text: ">\nCitation: Duarte, A.; Borralho, N.;\nCabral, P.; Caetano, M. Recent\n\
    Advances in Forest Insect Pests and\nDiseases Monitoring Using\nUAV-Based Data:\
    \ A Systematic\nReview. Forests 2022, 13, 911.\nhttps://doi.org/10.3390/f13060911\n\
    Academic Editor: William\nW. Hargrove\nReceived: 4 May 2022\nAccepted: 9 June\
    \ 2022\nPublished: 10 June 2022\nPublisher’s Note: MDPI stays neutral\nwith regard\
    \ to jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\n\
    Copyright:\n© 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article\
    \ is an open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of\
    \ the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nReview\nRecent Advances in Forest Insect Pests and Diseases\nMonitoring\
    \ Using UAV-Based Data: A Systematic Review\nAndré Duarte 1,2,*\n, Nuno Borralho\
    \ 1\n, Pedro Cabral 2\nand Mário Caetano 2,3\n1\nRAIZ—Forest and Paper Research\
    \ Institute, Quinta de S. Francisco, Rua José Estevão (EN 230-1), Eixo,\n3800-783\
    \ Aveiro, Portugal; nuno.borralho@thenavigatorcompany.com\n2\nNOVA Information\
    \ Management School (NOVA IMS), Universidade NOVA de Lisboa, Campus de\nCampolide,\
    \ 1070-312 Lisboa, Portugal; pcabral@novaims.unl.pt (P.C.); mario@novaims.unl.pt\
    \ (M.C.)\n3\nDGT—Direção Geral do Território, 1099-052 Lisboa, Portugal\n*\nCorrespondence:\
    \ andre.duarte@thenavigatorcompany.com; Tel.: +351-234-920-130\nAbstract: Unmanned\
    \ aerial vehicles (UAVs) are platforms that have been increasingly used over\n\
    the last decade to collect data for forest insect pest and disease (FIPD) monitoring.\
    \ These machines\nprovide ﬂexibility, cost efﬁciency, and a high temporal and\
    \ spatial resolution of remotely sensed\ndata. The purpose of this review is to\
    \ summarize recent contributions and to identify knowledge\ngaps in UAV remote\
    \ sensing for FIPD monitoring. A systematic review was performed using the\npreferred\
    \ reporting items for systematic reviews and meta-analysis (PRISMA) protocol.\
    \ We reviewed\nthe full text of 49 studies published between 2015 and 2021. The\
    \ parameters examined were the\ntaxonomic characteristics, the type of UAV and\
    \ sensor, data collection and pre-processing, processing\nand analytical methods,\
    \ and software used. We found that the number of papers on this topic has\nincreased\
    \ in recent years, with most being studies located in China and Europe. The main\
    \ FIPDs\nstudied were pine wilt disease (PWD) and bark beetles (BB) using UAV\
    \ multirotor architectures.\nAmong the sensor types, multispectral and red–green–blue\
    \ (RGB) bands were preferred for the\nmonitoring tasks. Regarding the analytical\
    \ methods, random forest (RF) and deep learning (DL)\nclassiﬁers were the most\
    \ frequently applied in UAV imagery processing. This paper discusses the\nadvantages\
    \ and limitations associated with the use of UAVs and the processing methods for\
    \ FIPDs,\nand research gaps and challenges are presented.\nKeywords: insect pest\
    \ and disease monitoring; forest; unmanned aerial vehicles; remote sensing;\n\
    PRISMA protocol\n1. Introduction\nForests play a fundamental role in human well-being\
    \ [1]. They are crucial carbon\npools [2], contributing to mitigating the impacts\
    \ of climate change [3,4] while ensuring\nimportant economic and social beneﬁts,\
    \ providing soil and water protection, and many\nother relevant environmental\
    \ services [5].\nIn recent decades, changes in the frequency and severity of meteorological\
    \ events\nseem to be related to a concomitant drop in the vitality of forests,\
    \ namely with the outbreak\nof new insect pests and diseases [5–7]. These environmental\
    \ disturbances can facilitate a\nchange in the frequency of the occurrence of\
    \ forest pests [8], which undoubtedly impacts\nthe development, survival, reproduction,\
    \ and dissemination of the species [5]. Insects have\nbeen recognized as the ﬁrst\
    \ indicators of climate change [9]. Reducing forest degradation\nand increasing\
    \ its resilience involves managing and preventing these stressors and disturb-\n\
    ing agents [10]. In this context, accurate and timely forest health monitoring\
    \ is needed to\nmitigate climate change and support sustainable forest management\
    \ [11].\nField sampling and symptom observation on foliage and trunks are the\
    \ main methods\nto identify and register forest pests and diseases [11,12]. When\
    \ remotely sensed data with\nhigh spatial and spectral resolution are collected\
    \ at ideal times, we can differentiate canopy\nForests 2022, 13, 911. https://doi.org/10.3390/f13060911\n\
    https://www.mdpi.com/journal/forests\nForests 2022, 13, 911\n2 of 31\nreﬂectance\
    \ signals from noise in forests affected by pests and diseases [13,14]. Traditional\n\
    ﬁeld surveys based on forest inventories and observations are restricted by small\
    \ area\ncoverage and subjectivity [15]. However, when combined with unmanned aerial\
    \ vehicles\n(UAVs), spatial coverage can be expanded, response time minimized,\
    \ and the costs of\nmonitoring forested areas reduced. UAV systems provide images\
    \ of high spatial resolution\nand can obtain updated and timely data with different\
    \ sensors [16,17]. In addition, they can\ncomplement the already well-known and\
    \ explored satellites with airborne remote sensing\ncapabilities [16,18].\nUAVs\
    \ can also be a valuable ﬁeld data source to calibrate and validate remote sensing\n\
    monitoring systems [19]. UAVs offer automatic movement and navigation, support\
    \ dif-\nferent sensors, provide safe access to difﬁcult locations, and enable\
    \ data collection under\ncloudy conditions [20]. In addition, these systems can\
    \ be operated to monitor speciﬁc\nphenological phases of plants or during pest/disease\
    \ outbreaks [18,21]. In this sense, UAVs\nare versatile, ﬂexible, and adaptable\
    \ to different contexts [22]. Despite the relevant advanta-\ngeous characteristics\
    \ of UAVs, some limitations can also be identiﬁed, such as limited area\ncoverage,\
    \ battery duration, payload weight, and local regulations [23].\nSeveral reviews\
    \ have already provided critical aspects related to the application\nof UAVs to\
    \ forest insect pest and disease (FIPD) monitoring (Table 1). Some of them\nfocused\
    \ on using UAVs in generic subjects, their applications, capabilities, and European\n\
    regulations [24]. These authors highlighted only three studies related to forest\
    \ pests and\ndiseases. Adão et al. [25] provided another relevant review on UAV-based\
    \ hyperspectral\nsensors and data processing for agriculture and forestry applications.\
    \ These authors also\nincluded only three studies about FIPDs in their review.\
    \ Eugenio et al. [26] presented a\nglobal state of the art method for the development\
    \ and application of UAV technology\nin forestry. The authors addressed six studies\
    \ about forest health monitoring and other\nforestry applications. Focusing on\
    \ the data, processing, and potentialities, Guimarães\net al. [16] presented nine\
    \ studies related to FIPDs and other forestry applications. In 2021,\na systematic\
    \ review focusing on forest research applications was completed by Dainelli\n\
    et al. [27], highlighting 17 studies in which host–pathogen systems and causal\
    \ agents have\nbeen classiﬁed. The research question was about forest types, pests\
    \ and diseases, and\ntheir incidence. Torres et al. [28] also proposed a systematic\
    \ evidence synthesis to identify\nand analyze studies about forest health issues\
    \ by applying remote sensing techniques\nfrom multiple platforms. In their work,\
    \ 10 UAV studies were included. Recently, Eugenio\net al. [29] proposed a systematic\
    \ bibliometric literature review about the use of UAVs in\nforest pest and disease\
    \ research. These authors studied the temporal trends of the last\ndecade using\
    \ UAVs based on 33 scientiﬁc articles. The authors examined the monitored\npests\
    \ and diseases, focusing on the sensor types, technical ﬂight parameters, and\
    \ applied\nanalytical methods.\nTable 1. Review studies on unmanned aerial vehicle\
    \ (UAV) remote sensing for forest insect pests and\ndiseases.\nNo.\nRef.\nYear\n\
    Title\nJournal\nContents\n1\n[24]\n2017\nForestry applications of UAVs in\nEurope:\
    \ a review\nInternational Journal of\nRemote Sensing\nA review of UAV-based forestry\n\
    applications and aspects of regulations\nin Europe. Three studies about FIPDs\n\
    were reviewed.\n2\n[25]\n2017\nHyperspectral Imaging:\nA Review on UAV-Based Sensors,\n\
    Data Processing and Applications\nfor Agriculture and Forestry\nRemote Sensing\n\
    A review on UAV-based hyperspectral\nsensors, data processing, and\napplications\
    \ for agriculture and\nforestry. Three studies about FIPDs\nwere reviewed.\nForests\
    \ 2022, 13, 911\n3 of 31\nTable 1. Cont.\nNo.\nRef.\nYear\nTitle\nJournal\nContents\n\
    3\n[26]\n2020\nRemotely piloted aircraft systems\nand forests: a global state\
    \ of the\nart and future challenges\nCanadian Journal of\nForest Research\nA review\
    \ of UAV-based forestry\napplications. Six studies about FIPDs\nwere reviewed.\n\
    4\n[16]\n2020\nForestry Remote Sensing from\nUnmanned Aerial Vehicles:\nA Review\
    \ Focusing on the Data,\nProcessing and Potentialities\nRemote Sensing\nA review\
    \ focusing on data, processing,\nand potentialities. It covers all types of\n\
    procedures and provides examples.\nNine studies about FIPDs were\nreviewed.\n\
    5\n[27]\n2021\nRecent Advances in Unmanned\nAerial Vehicles Forest Remote\nSensing—A\
    \ Systematic Review.\nPart II: Research Applications\nForests\nA systematic review\
    \ of UAV system\nsolutions, technical advantages,\ndrawbacks of the technology,\
    \ and\nconsiderations on technology transfer.\nSeventeen studies about FIPDs were\n\
    reviewed.\n6\n[28]\n2021\nThe Role of Remote Sensing for\nthe Assessment and Monitoring\n\
    of Forest Health: A Systematic\nEvidence Synthesis\nForests\nA systematic evidence\
    \ synthesis about\nforest health issues with reference to\ndifferent remote sensing\
    \ platforms and\ntechniques. Ten studies about\nUAV–FIPDs were reviewed.\n7\n\
    [29]\n2021\nRemotely Piloted Aircraft\nSystems to Identify Pests and\nDiseases\
    \ in Forest Species:\nThe Global State of the Art and\nFuture Challenges\nIEEE\
    \ Geoscience and\nremote sensing magazine\nA literature review of UAV-based on\n\
    forest pest and disease monitoring.\nThirty-three studies about FIPDs\nwere reviewed.\n\
    Despite the diversity of UAV–FIPD reviews, the rapid growth of these technologies\n\
    and related computational advances have led to a need for the constant updating\
    \ of the\nliterature. On the other hand, the standards for mapping in the forestry\
    \ context are unclear,\nso it is necessary to aggregate available scientiﬁc studies\
    \ to improve the current UAV\nprocedures. In this context, we propose this review\
    \ to address these gaps to analyze the\ntrends, challenges, and future development\
    \ prospects for UAV–FIPD.\nThe main objective of this systematic review is to\
    \ provide readers with the current\npractices and techniques in use and identify\
    \ the knowledge gaps in UAV remote sens-\ning for FIPD monitoring. For this purpose,\
    \ we utilized the preferred reporting items for\nsystematic reviews and meta-analysis\
    \ (PRISMA) approach to review 49 peer-reviewed\narticles. A database was built\
    \ based on bibliometric data, the taxonomic characterization\nof FIPDs, UAVs and\
    \ sensor types, data collection and pre-processing, data processing\nand analytical\
    \ methods, and software used, in order to ﬁnd answers to these questions:\n(1)\
    \ Which platforms sensors are commonly used? (2) Which are the optimal ﬂight parame-\n\
    ters? (3) What are the main strategies for monitoring FIPDs? The quantitative\
    \ results of this\nsystematic review will allow ﬁnding new insights, trends, and\
    \ challenges for UAV–FIPD.\nThis systematic review is structured as follows: Section\
    \ 2 presents the method used\nto gather data using the main databases, the eligibility\
    \ criteria, bibliometric analysis, and\nquantitative analysis details. Section\
    \ 3 provides our results and discussions, identifying the\nmajor sources of information,\
    \ keyword co-occurrence, the taxonomic characterization of\neach pest or disease,\
    \ the frequency of UAV data collection procedures, and the analytical\nmethods\
    \ applied. Section 4 outlines the research gaps, challenges, and ideas for further\n\
    research. Finally, in Section 5, we present our conclusions and outline future\
    \ work.\n2. Methods\nWe reviewed studies using UAV-based data to detect and monitor\
    \ FIPDs published\non the major international journals of remote sensing, drones,\
    \ plant ecology, and forests\nForests 2022, 13, 911\n4 of 31\nindexed by the Scopus\
    \ and Web of Science (WoS) databases. The systematic review was con-\nducted by\
    \ adopting the PRISMA methodology [30]. A constructed search query (“Platform”\n\
    AND “Field” AND “Issue”) was applied on Scopus and WoS scientiﬁc databases (Figure\
    \ 1),\nmaking it possible to obtain the bibliographic resources used in this analysis.\n\
    Forests 2022, 13, x FOR PEER REVIEW \n4 of 32\n \nWe reviewed studies using UAV-based\
    \ data to detect and monitor FIPDs published\non the major international journals\
    \ of remote sensing, drones, plant ecology, and forest\nindexed by the Scopus\
    \ and Web of Science (WoS) databases. The systematic review was\nconducted by\
    \ adopting the PRISMA methodology [30]. A constructed search query (“Plat\nform”\
    \ AND “Field” AND “Issue”) was applied on Scopus and WoS scientific databases\n\
    (Figure 1), making it possible to obtain the bibliographic resources used in this\
    \ analysis. \n \nFigure 1. Search query design (“Platform” AND “Field” AND “Issue”)\
    \ used. \nThe papers were filtered on 31st December 2021 using the search engine\
    \ in both da\ntabases. The biennial conference UAV-g in Zurich, Switzerland, organized\
    \ by the interna\ntional photogrammetry community in 2011, was the base for the\
    \ search time period. Ac\ncording to Colomina and Molina [31], UAS-related conferences\
    \ and publications in\ncreased importantly in the referred period. \nOur analysis\
    \ considered only original articles and conference papers published in\nhigh-impact\
    \ journals. Therefore, we excluded review papers, reports, book chapters, and\n\
    Ph.D. theses. Furthermore, other search engines such as Google Scholar were utilized\
    \ to\nensure that no relevant studies were omitted. The eligibility criteria for\
    \ the studies selec\ntion were defined as follows: (1) studies of FIPDs using\
    \ UAV-based imagery; (2) studies\nproviding the type of equipment used and the\
    \ most critical flight plan parameters; (3\nstudies related to agroforestry systems;\
    \ (4) studies of FIPD monitoring using artificial sim\nulations. \nA total of\
    \ 471 records were returned by the query in the selected databases (Figure\n2).\
    \ This set was enriched with three additional studies found using a Google Schola\n\
    search. The subsequent analysis involved merging these studies and removing the\
    \ dupli\ncates using the bibliometrix package (University of Naples Federico II,\
    \ Naples, Italy) [32\nin R Studio (RStudio Team, Boston, MA, USA) [33]. Then,\
    \ through an abstract screening\nprocess, 277 articles were excluded that were\
    \ not within the scope of the research, such a\nUAV pest and disease mapping in\
    \ crops (e.g., citrus or olive trees). A total of 28 article\nwere excluded that\
    \ were related to other types of forestry damages or disturbances (e.g.\nabiotic\
    \ disturbances, such as windthrow and fire) or lacked the development of appropri\n\
    ate photogrammetric and remote sensing methods for UAV imagery. \nSubsequently,\
    \ we extracted the categories, parameters, and detailed description\nfrom each\
    \ article. To ensure all parameters were included, we considered the procedure\n\
    Figure 1. Search query design (“Platform” AND “Field” AND “Issue”) used.\nThe\
    \ papers were ﬁltered on 31 December 2021 using the search engine in both\ndatabases.\
    \ The biennial conference UAV-g in Zurich, Switzerland, organized by the in-\n\
    ternational photogrammetry community in 2011, was the base for the search time\
    \ period.\nAccording to Colomina and Molina [31], UAS-related conferences and\
    \ publications in-\ncreased importantly in the referred period.\nOur analysis\
    \ considered only original articles and conference papers published in high-\n\
    impact journals. Therefore, we excluded review papers, reports, book chapters,\
    \ and Ph.D.\ntheses. Furthermore, other search engines such as Google Scholar\
    \ were utilized to ensure\nthat no relevant studies were omitted. The eligibility\
    \ criteria for the studies selection were\ndeﬁned as follows: (1) studies of FIPDs\
    \ using UAV-based imagery; (2) studies providing\nthe type of equipment used and\
    \ the most critical ﬂight plan parameters; (3) studies related\nto agroforestry\
    \ systems; (4) studies of FIPD monitoring using artiﬁcial simulations.\nA total\
    \ of 471 records were returned by the query in the selected databases (Figure\
    \ 2).\nThis set was enriched with three additional studies found using a Google\
    \ Scholar search.\nThe subsequent analysis involved merging these studies and\
    \ removing the duplicates\nusing the bibliometrix package (University of Naples\
    \ Federico II, Naples, Italy) [32] in\nR Studio (RStudio Team, Boston, MA, USA)\
    \ [33]. Then, through an abstract screening\nprocess, 277 articles were excluded\
    \ that were not within the scope of the research, such as\nUAV pest and disease\
    \ mapping in crops (e.g., citrus or olive trees). A total of 28 articles\nwere\
    \ excluded that were related to other types of forestry damages or disturbances\
    \ (e.g.,\nabiotic disturbances, such as windthrow and ﬁre) or lacked the development\
    \ of appropriate\nphotogrammetric and remote sensing methods for UAV imagery.\n\
    Forests 2022, 13, 911\n5 of 31\n \nject, feature extraction and selection, analysis\
    \ type, algorithms, overall accuracy) and fi\nnally the software used to pre-process\
    \ imagery and to perform the analytical methods \n(Table 2). These categories\
    \ tried to reflect the vast number of procedures, techniques, and \nmethods commonly\
    \ used in forest insect pest and disease monitoring with UAVs. Accord-\ning to\
    \ the target categories, there was a revision of the full text of each of the\
    \ selected \narticles retained for the literature review. The dataset created\
    \ was analyzed using RStudio \n[33]. \n \nFigure 2. PRISMA flow diagram for the\
    \ selection of relevant papers (n = number of documents). \nFigure 2. PRISMA ﬂow\
    \ diagram for the selection of relevant papers (n = number of documents).\nSubsequently,\
    \ we extracted the categories, parameters, and detailed descriptions from\neach\
    \ article. To ensure all parameters were included, we considered the procedures\
    \ and\nparameters presented by Eskandari et al. [34] and Dash et al. [35] in their\
    \ works. Studies\nwere categorized according to the general characteristics (i.e.,\
    \ source of the study, year,\nauthors, study location), the taxonomy (i.e., host\
    \ species, pests, or disease species), UAV\nand sensor types (i.e., type of UAV,\
    \ active or passive sensor, manufacturer and model),\ndata collection and pre-processing\
    \ (i.e., study area size, ﬂight altitude, spatial resolution,\nfrontal and side\
    \ overlap, ﬁeld data collection, radiometric and geometric correction), data\n\
    processing and analytical methods (i.e., spatial analysis unit, segmentation single\
    \ tree object,\nfeature extraction and selection, analysis type, algorithms, overall\
    \ accuracy) and ﬁnally\nthe software used to pre-process imagery and to perform\
    \ the analytical methods (Table 2).\nThese categories tried to reﬂect the vast\
    \ number of procedures, techniques, and methods\ncommonly used in forest insect\
    \ pest and disease monitoring with UAVs. According to the\nForests 2022, 13, 911\n\
    6 of 31\ntarget categories, there was a revision of the full text of each of the\
    \ selected articles retained\nfor the literature review. The dataset created was\
    \ analyzed using RStudio [33].\nTable 2. Categories of the parameters extracted\
    \ from screened articles in the database.\nCategory\nParameter\nDescription\n\
    General\nSource\nRefereed journals and conference proceedings\nYear\n-\nAuthors\n\
    -\nStudy location\nThe geographic location of the study area\nTaxonomy\nSpecie\n\
    Name of the host tree specie\nPest or disease\nName of the pest or disease\nUAV\
    \ and sensor types\nUAV type\nType of the UAV (ﬁxed-wing, rotary-wing)\nSensor\
    \ type\nActive or passive sensor, manufacturer, model\nData collection and\npre-processing\n\
    Study area size\nArea coverage in hectares\nFlight altitude\nMeasured (m)\nSpatial\
    \ resolution\nMeasured centimeters (cm)\nImagery Overlap\nPercentage of frontal\
    \ and side overlap\nField data collection\nAncillary ﬁeld and laboratory data\
    \ about FIPD\nRadiometric calibration\nCalibrated panels\nGeometric calibration\n\
    Ground control points (GCPs)\nData processing and\nanalytical methods\nSpatial\
    \ unit analysis\nPixel-based, object-based\nSegmentation single tree\nManual,\
    \ raster-based, vector-based\nFeature extraction and selection\nNo feature extraction,\
    \ vegetation indices, textural or\ncontextual image, linear transformations, auxiliary\
    \ data\nAnalysis type\nClassiﬁcation, regression, other\nAlgorithms\nStatistical,\
    \ machine learning, deep learning, other\nAccuracy metrics\nMeasured in percentage\n\
    Software used\nSoftware brands\nSoftware used to process imagery and analytical\
    \ methods\nThe keyword clustering analysis was performed using Zotero (George\
    \ Mason Univer-\nsity, Fairfax, VA, USA) [36] to create the Research Information\
    \ Systems Citation (.ris) ﬁle and\nVosViewer software (Leiden University, Leiden,\
    \ The Netherlands) [37]. The quantitative\nanalysis, focused on acquiring the\
    \ frequencies of each parameter, was summarized using\ntables and ﬁgures.\n3.\
    \ Results and Discussion\n3.1. General Characterization of Selected Studies\n\
    Among the 49 publications selected, 45 were published in peer-reviewed journals,\
    \ and\n4 in conference proceedings. As shown in Table 3, most journals were Q1-quartile-ranked\n\
    (40), representing 89%, and the remaining were Q2 articles (5). The top publishers\
    \ identiﬁed\nwere Multidisciplinary Digital Publishing Institute (MDPI) (Switzerland)\
    \ (26), Elsevier\n(United States, The Netherlands and Germany) (8), Taylor & Francis\
    \ Ltd. (China and United\nKingdom) (3), and Springer (Germany) (3).\nThe main\
    \ journals were the Remote Sensing journal, which published 17 papers related\n\
    to FIPD, followed by the Forests journal, with 5 articles. The conference proceedings\n\
    identiﬁed in this analysis included the International Archives of the Photogrammetry\n\
    Remote Sensing and Spatial Information Sciences (ISPRS) Archives with three works,\
    \ and\nthe ISPRS Annals Photogrammetry Remote Sensing and Spatial Information\
    \ Sciences with\none, as shown in Table 4.\nForests 2022, 13, 911\n7 of 31\nTable\
    \ 3. Studies published by journal, quartile rank, and publisher. No. indicates\
    \ the number\nof papers.\nJournals\nNo.\nQuartile Rank\nPublisher\nRemote Sensing\n\
    17\nQ1\nMDPI\nForests\n5\nQ1\nMDPI\nForest Ecology and Management\n3\nQ1\nElsevier\
    \ Inc.\nDrones\n2\nQ1\nMDPI\nForest Ecosystems\n2\nQ1\nSpringer\nRemote Sensing\
    \ of Environment\n2\nQ1\nElsevier Inc.\nSensors\n2\nQ2\nMDPI\nAustralian Forestry\n\
    1\nQ1\nTaylor & Francis Ltd.\nEngineering\n1\nQ1\nElsevier Inc.\nGeo-Spatial Information\
    \ Science\n1\nQ1\nTaylor & Francis Ltd.\nIEEE Journal of selected topics in Applied\
    \ Earth\nObservation and Remote Sensing\n1\nQ2\nInstitute of Electrical and Electronics\n\
    Engineers Inc.\nInternational Journal of Applied Earth Observation and\nGeoinformation\n\
    1\nQ1\nElsevier Inc.\nInternational Journal of Remote Sensing\n1\nQ1\nTaylor &\
    \ Francis Ltd.\nISPRS Journal of Photogrammetry and Remote Sensing\n1\nQ1\nElsevier\
    \ Inc.\nJournal of Forestry Research\n1\nQ2\nNortheast Forestry University\nJournal\
    \ of Plant Diseases and Protection\n1\nQ2\nSpringer International Publishing AG\n\
    Plant Methods\n1\nQ1\nBioMed Central Ltd.\nPLoS One\n1\nQ1\nPublic Library of\
    \ Science\nUrban Forestry and Urban Greening\n1\nQ1\nUrban und Fischer Verlag\
    \ GmbH und Co. KG\nTable 4. Studies presented in conference proceedings by publisher\
    \ country and publisher. No.\nindicates the number of conference proceedings.\n\
    Conference Proceedings\nNo.\nPublisher\nInternational Archives of the Photogrammetry\
    \ Remote\nSensing and Spatial Information Sciences (ISPRS) Archives\n3\nInternational\
    \ Society for Photogrammetry\nand Remote Sensing\nISPRS Annals of the Photogrammetry\
    \ Remote Sensing and\nSpatial Information Sciences\n1\nCopernicus GmbH\nFigure\
    \ 3 illustrates how FIPD monitoring studies using UAV platforms have increased\n\
    over seven years. Out of the 49 studies, 18 were published in 2021, corresponding\
    \ to 37%,\nwhile 11 were published in 2020 (22%) and 7 in 2018 and 2019 (14%).\
    \ In recent years,\nthere has been a signiﬁcant increase in the number of publications,\
    \ corroborating the result\nobtained by Eugenio et al. [29]. The advances in UAV\
    \ capabilities and miniaturization is an\nessential factor contributing to this\
    \ study’s interest.\nWith the growing risks to forests worldwide, forest health\
    \ monitoring is critical to\nmaintaining forest sustainability [11,38]. Thus,\
    \ information obtained by UAV offers a\nvariety of monitoring possibilities. Such\
    \ opportunities include reaching otherwise inacces-\nsible areas using high spatiotemporal\
    \ resolution, which could complement or completely\nsubstitute time-consuming\
    \ ﬁeldwork [39,40].\nFigure 4 illustrates the worldwide distribution of the included\
    \ studies across four\ncontinents (Asia, Europe, Oceania, and North America).\
    \ As shown, the studies using\nUAV-based data were located in China (14), the\
    \ Czech Republic (6), Portugal (4), Spain (4),\nFinland (3), Scotland (2), South\
    \ Korea (2), New Zealand (2), the United States (2) and\nAustralia (2). This result\
    \ may be associated with the type of biome [41] (temperate and\nboreal forests)\
    \ and commercial coniferous and hardwood species in these areas [29].\nForests\
    \ 2022, 13, 911\n8 of 31\nForests 2022, 13, x FOR PEER REVIEW \n8 of 32 \n \n\
    \ \n \nFigure 3. Temporal distribution of published papers during the period included.\
    \ \nWith the growing risks to forests worldwide, forest health monitoring is critical\
    \ to \nmaintaining forest sustainability [11,38]. Thus, information obtained by\
    \ UAV offers a va-\nriety of monitoring possibilities. Such opportunities include\
    \ reaching otherwise inaccessi-\nble areas using high spatiotemporal resolution,\
    \ which could complement or completely \nsubstitute time-consuming fieldwork [39,40].\
    \ \nFigure 4 illustrates the worldwide distribution of the included studies across\
    \ four \ncontinents (Asia, Europe, Oceania, and North America). As shown, the\
    \ studies using \nUAV-based data were located in China (14), the Czech Republic\
    \ (6), Portugal (4), Spain \n(4), Finland (3), Scotland (2), South Korea (2),\
    \ New Zealand (2), the United States (2) and \nAustralia (2). This result may\
    \ be associated with the type of biome [41] (temperate and \nboreal forests) and\
    \ commercial coniferous and hardwood species in these areas [29]. \n \nFigure\
    \ 4. World distribution of papers published focusing on UAV-based data. \nThe\
    \ diversity of keywords used by authors and the number of clusters (3) can be\
    \ \nobserved in Figure 5. The size of the circle describes the number of occurrences\
    \ of the \nFigure 3. Temporal distribution of published papers during the period\
    \ included.\n \n \nFigure 3. Temporal distribution of published papers during\
    \ the period included. \nWith the growing risks to forests worldwide, forest health\
    \ monitoring is critical to \nmaintaining forest sustainability [11,38]. Thus,\
    \ information obtained by UAV offers a va-\nriety of monitoring possibilities.\
    \ Such opportunities include reaching otherwise inaccessi-\nble areas using high\
    \ spatiotemporal resolution, which could complement or completely \nsubstitute\
    \ time-consuming fieldwork [39,40]. \nFigure 4 illustrates the worldwide distribution\
    \ of the included studies across four \ncontinents (Asia, Europe, Oceania, and\
    \ North America). As shown, the studies using \nUAV-based data were located in\
    \ China (14), the Czech Republic (6), Portugal (4), Spain \n(4), Finland (3),\
    \ Scotland (2), South Korea (2), New Zealand (2), the United States (2) and \n\
    Australia (2). This result may be associated with the type of biome [41] (temperate\
    \ and \nboreal forests) and commercial coniferous and hardwood species in these\
    \ areas [29]. \n \nFigure 4. World distribution of papers published focusing on\
    \ UAV-based data. \nThe diversity of keywords used by authors and the number of\
    \ clusters (3) can be \nobserved in Figure 5. The size of the circle describes\
    \ the number of occurrences of the \nFigure 4. World distribution of papers published\
    \ focusing on UAV-based data.\nThe diversity of keywords used by authors and the\
    \ number of clusters (3) can be\nobserved in Figure 5. The size of the circle\
    \ describes the number of occurrences of the\nkeywords, and the color determines\
    \ which cluster it belongs to. The width of the link\nbetween two keywords determines\
    \ the strength of the connection. A keyword cluster\nanalysis (text mining) was\
    \ performed using VosViewer based on the frequency of the\nterms. We merged similar\
    \ terms and synonyms in a thesaurus ﬁle. The words were\nincluded in cluster analysis\
    \ if they occurred at least twice. We applied the node-repulsion\nLinLog modularity\
    \ as normalization. Out of 460 keywords, 28 met the threshold. According\nto Figure\
    \ 5, the most frequently used keywords were “Forestry”, “UAV”, “Remote sensing”,\n\
    “Airborne sensing”, “Forest health monitoring” and “multispectral”. Each cluster\
    \ represents\nthe different study approaches. For instance, the link between “Forestry”\
    \ and “UAV” is a\ndifferent approach than the link between “Random Forest” and\
    \ “UAV”. On the other hand,\nthe strength between “Forestry” and “UAV” is stronger,\
    \ because they belong to the same\nForests 2022, 13, 911\n9 of 31\ncluster. The\
    \ link between “Random Forest” and “UAV” is less robust because they belong\n\
    to different clusters.\n \nlong to the same cluster. The link between “Random\
    \ Forest” and “UAV” is less robust \nbecause they belong to different clusters.\
    \ \nThe keyword analysis revealed how UAV technology is used in forestry and forest\
    \ \nhealth monitoring, with various procedures and approaches for different purposes\
    \ [42]. \nFor instance, the first cluster (blue color) includes pine wild disease\
    \ (PWD) detection stud-\nies using deep learning techniques such as the convolutional\
    \ neural network (CNN). The \nsecond cluster (green color) contains all the studies\
    \ about bark beetle (BB) detection and \nthe classification process of insect\
    \ outbreaks. Hence, this analysis suggests the types of \nFIPDs studied and the\
    \ applied analysis types. \n \nFigure 5. Keyword co-occurrence diagram for the\
    \ selected papers. \nFigure 5. Keyword co-occurrence diagram for the selected\
    \ papers.\nThe keyword analysis revealed how UAV technology is used in forestry\
    \ and forest\nhealth monitoring, with various procedures and approaches for different\
    \ purposes [42].\nFor instance, the ﬁrst cluster (blue color) includes pine wild\
    \ disease (PWD) detection\nstudies using deep learning techniques such as the\
    \ convolutional neural network (CNN).\nThe second cluster (green color) contains\
    \ all the studies about bark beetle (BB) detection\nand the classiﬁcation process\
    \ of insect outbreaks. Hence, this analysis suggests the types of\nFIPDs studied\
    \ and the applied analysis types.\n3.2. Taxonomic Characterization\nThe aggregation\
    \ of the number of publications about pests, diseases and related hosts\nis shown\
    \ in Table 5. We separated the studies considering the taxonomy of the pests,\n\
    diseases and related host tree species. Regarding the forest pests, the BB was\
    \ the most\nfrequently studied (11), followed by the processionary moth (4), pine\
    \ shoot beetles (3), and\nthe Chinese pine caterpillar (2). The remaining studies\
    \ only mentioned one pest species.\nThe most frequently studied disease was the\
    \ PWD (13), followed by the red band needle\nblight (2) and pathogenic microorganisms\
    \ (2). According to this analysis, the remaining\nstudies only mentioned one pest\
    \ species.\nForests 2022, 13, 911\n10 of 31\nTable 5. Summary of common names\
    \ of pests or diseases and related host tree species in the\nstudies analyzed.\n\
    Common Name\nHost Tree Species\nStudies\nPests\nBark beetle\nAbies sibirica, Abies\
    \ mariesii, Picea abies, Pinus sylvestris, Pinus nigra\n[43–54]\nChinese pine\
    \ caterpillar\nPinus tabulaeformis\n[55,56]\nLonghorned borer\nEucalyptus globulus\n\
    [57]\nMosquito bugs\nEucalyptus pellita\n[58]\nMistletoe\nParrotia persica\n[59,60]\n\
    Oak splendor beetle\nQuercus robur\n[61]\nPine shoot beetle\nPinus yunnanensis\n\
    [62–64]\nProcessionary moth\nPinus Sylvestris, Pinus nigra, Pinus halepensis\n\
    [39,65–67]\nStem borer\nEucalyptus pellita\n[58]\nTortrix moth\nAbies mariesii\n\
    [53]\nDiseases\nArmillaria root rot\nPicea abies\n[12]\nAlder Phytophtora\nAlnus\
    \ glutinosa\n[68]\nChestnut ink disease\nCastanea sativa\n[69]\nMyrtle rust\n\
    Melaleuca quinquenervia\n[70]\nBacterial wild\nEucalyptus pellita\n[58,71]\nPine\
    \ wild disease\nPinus pinaster, P. desiﬂora, P. massoniana\n[72–83]\nRed band\
    \ needle blight\nPinus Sylvestris and P. contorta\n[84,85]\nWhite pine needle\
    \ cast\nPinus strobus and Pinus resinosa\n[86]\nSimulated\nPinus radiata\n[15,40]\n\
    The research provided by Briechle et al. [87] did not present a formal pest or\
    \ dis-\nease, only the host tree species (Pinus sylvestris), because this work\
    \ was performed in the\nChernobyl Exclusion Zone. Otherwise, Dash et al. [15,40]\
    \ conducted a simulated disease\noutbreak using herbicide on pinus radiata.\n\
    Due to the high number of hosts, the BB, PWD, and the processionary moth have\
    \ been\nwidely studied. Moreover, they have a tremendous economic impact worldwide.\
    \ The BB\nwas mainly studied in temperate forest ecosystems, while coniferous\
    \ defoliators such as\nprocessionary moths were mostly studied in boreal and Mediterranean\
    \ forests [41].\nDespite the most studied species in this ﬁeld being coniferous,\
    \ we veriﬁed an increase\nin the study of hardwood species studies (3), such as\
    \ Eucalyptus sp. [57,58,71]. The Eucalyp-\ntus genus is one of the most planted\
    \ worldwide [88,89], especially in temperate regions [90].\n3.3. UAV and Sensor\
    \ Types\n3.3.1. UAV Types\nFigure 6 shows the circular packing graph where each\
    \ circle is a group of UAV types\nconsidering the number of propellers and architecture.\
    \ The bubbles inside the circles\nrepresent the sub-groups. Each bubble’s size\
    \ is proportional to the UAV categories used\nin the studies. We extracted the\
    \ quantities of each UAV type considering the number of\npropellers and based\
    \ on commercial brands. Therefore, it was found that 84% of the studies\nused\
    \ multirotor drones. Fixed-wing drones represented 12%, and 4% did not indicate\n\
    the type, while the remaining 2% used both (ﬁxed-wing and multirotor). Quadcopters\n\
    were used in 58% of the studies, while hexacopters comprised 15%, octocopters\
    \ 15%, and\nﬁxed-wing drones represented 12%.\nRegarding the models used by the\
    \ number of propellers, the quadcopter model DJI\nPhantom 4 Pro was used in 30%\
    \ of the studies and DJI Phantom 3 in 14%. With regard to\noctocopters, the most\
    \ used models were the DJI S1000 (25%), Arealtronics (25%), and the\nMicroKopter\
    \ Droidwors AD-8 (25%). Thirteen percent made no distinction based on the\nmodel\
    \ used. The hexacopter DJI Matrice 600 model was used in 36% of the works. Finally,\n\
    in the ﬁxed-wing segment, the most popular was the eBee Sense Fly model with 71%\
    \ usage,\nfollowed by the Quest UAV Qpod (14%) and DB-2 (14%).\nForests 2022,\
    \ 13, 911\n11 of 31\nForests 2022, 13, x FOR PEER REVIEW \n12 of 32 \n \n \n \n\
    Figure 6. Summary of UAV types and model brands identified in the studies. \n\
    3.3.2. Sensor Types \nFigure 7a illustrates the number of remote sensing sensors,\
    \ and Figure 7b shows the \ntop 10 model camera brands coupled with UAVs. The\
    \ passive remote sensor quantities \nwere grouped into four categories: (i) RGB,\
    \ i.e., the simplification of multispectral red–\ngreen–blue (RGB); (ii) multispectral,\
    \ including RGB, near-infrared, and red-edge bands; \n(iii) hyperspectral; and\
    \ finally, (iv) thermal sensors. Light detection and ranging (LiDAR) \nwas the\
    \ only active sensor found in the studies. As shown in Figure 7a, RGB sensors\
    \ were \nused in 12 studies, multispectral cameras in 10, hyperspectral in 3,\
    \ and thermal sensors in \none study. The most widely applied remote sensing technology\
    \ combination was the RGB \nand multispectral combination in nine studies, followed\
    \ by RGB and hyperspectral in \nthree studies, and hyperspectral and LiDAR in\
    \ three. The remaining combinations of RGB \nand thermal multispectral and thermal,\
    \ and multispectral and LiDAR were used in one \nstudy. The most relevant sensors\
    \ found operated in visible light (RGB) and NIR regions, \nwhich may be related\
    \ to the low-cost acquisition and lesser complexity, size, and weight \nFigure\
    \ 6. Summary of UAV types and model brands identiﬁed in the studies.\nRegarding\
    \ the choice of platform, the most widely adopted was the rotary-wing, which\n\
    stands out due to its ﬂexibility, versatility, maneuverability, and its ability\
    \ to hover, offering\na much easier automated experience [20,91,92]. Fixed-wing\
    \ drones are more efﬁcient, stable\nin crosswind ﬂights, and have short ﬂight\
    \ times per unit of a mapped area [93]. However,\nthey are less versatile for\
    \ making small ﬂights when compared with rotary-wing drones.\nIn addition, rotary-wing\
    \ drones are more suitable for mapping small and complex sites,\nwhile ﬁxed-wing\
    \ drones are more appropriate for covering more extensive areas [94]. Con-\nversely,\
    \ a faster vehicle may have issues mapping small objects and insufﬁcient overlap\
    \ [92].\nIn spite of this, both UAV types offer the possibility to collect data\
    \ from short intervals and\nat a local scale, which is relevant for multitemporal\
    \ studies [15,40]. Notably, the preference\nfor quadcopters may be related to\
    \ the low-cost acquisition, the wide availability on the\nmarket, and the assessment\
    \ of FIPD in small areas [26]. For example, the DJI Phantom\nseries was the most\
    \ frequently used in this segment. The hexacopters and octocopters\nfrom the DJI\
    \ series choice were due to the payload capabilities in the remaining studies.\n\
    Finally, eBee Sense Fly stands out for its maturity in the market. The arguments\
    \ presented\nindicate that rotary-wing drones are the most suitable for FIPD monitoring.\
    \ However,\nmore comparative studies are needed to support the appropriate UAV\
    \ architecture for this\nforestry application. Despite these facts, platform choice\
    \ depends on the survey require-\nments, the budget, and the experience of the\
    \ researcher or practitioner. An important point\nto mention is the market offer\
    \ of hybrid VTOL (vertical take-off and landing), of which the\nonly disadvantage\
    \ is the complex system mechanism [95,96]. We anticipate that this UAV\ntype will\
    \ be used in FIPD studies in the near future.\nForests 2022, 13, 911\n12 of 31\n\
    3.3.2. Sensor Types\nFigure 7a illustrates the number of remote sensing sensors,\
    \ and Figure 7b shows the\ntop 10 model camera brands coupled with UAVs. The passive\
    \ remote sensor quantities\nwere grouped into four categories: (i) RGB, i.e.,\
    \ the simpliﬁcation of multispectral red–\ngreen–blue (RGB); (ii) multispectral,\
    \ including RGB, near-infrared, and red-edge bands;\n(iii) hyperspectral; and\
    \ ﬁnally, (iv) thermal sensors. Light detection and ranging (LiDAR)\nwas the only\
    \ active sensor found in the studies. As shown in Figure 7a, RGB sensors were\n\
    used in 12 studies, multispectral cameras in 10, hyperspectral in 3, and thermal\
    \ sensors\nin one study. The most widely applied remote sensing technology combination\
    \ was the\nRGB and multispectral combination in nine studies, followed by RGB\
    \ and hyperspectral\nin three studies, and hyperspectral and LiDAR in three. The\
    \ remaining combinations of\nRGB and thermal multispectral and thermal, and multispectral\
    \ and LiDAR were used\nin one study. The most relevant sensors found operated\
    \ in visible light (RGB) and NIR\nregions, which may be related to the low-cost\
    \ acquisition and lesser complexity, size, and\nweight [23,39]. Visible light\
    \ operates between 400 to 700 nm, while NIR is above 700 nm\nin terms of wavelength.\
    \ Most DJI consumer drones are equipped with RGB cameras with\nminimal features\
    \ and speciﬁcations to perform quality mappings. Besides, we found that\nresearchers\
    \ and practitioners couple multispectral cameras with consumer UAV types; for\n\
    instance, Cardil et al. [65] used a multispectral Parrot Sequoia coupled with\
    \ a Phantom 3\nUAV, and Iordache et al. [72] used a Micasense Red-Edge MX connected\
    \ to a Phantom 4\npro. On the other hand, the hyperspectral and LiDAR sensors\
    \ are more expensive, have\nmore complex speciﬁcations, and are commonly mounted\
    \ on drones with a higher payload\n(professional UAV), such as the Matrice 600\
    \ used by Lin et al. [63,64].\nForests 2022, 13, x FOR PEER REVIEW \n13 of 32\
    \ \n \n[23,39]. Visible light operates between 400 to 700 nm, while NIR is above\
    \ 700 nm in terms \nof wavelength. Most DJI consumer drones are equipped with\
    \ RGB cameras with minimal \nfeatures and specifications to perform quality mappings.\
    \ Besides, we found that research-\ners and practitioners couple multispectral\
    \ cameras with consumer UAV types; for in-\nstance, Cardil et al. [65] used a\
    \ multispectral Parrot Sequoia coupled with a Phantom 3 \nUAV, and Iordache et\
    \ al. [72] used a Micasense Red-Edge MX connected to a Phantom 4 \npro. On the\
    \ other hand, the hyperspectral and LiDAR sensors are more expensive, have \n\
    more complex specifications, and are commonly mounted on drones with a higher\
    \ pay-\nload (professional UAV), such as the Matrice 600 used by Lin et al. [63,64].\
    \ \n \n \n(a) \n(b) \nFigure 7. Summary of sensor types, including: (a) types\
    \ of remote sensing technology identified in \neach study; (b) top 10 model camera\
    \ brands. \nConcerning the sensor model brands coupled with different UAV architectures,\
    \ the \nmultispectral cameras Micasense Red-edge and Parrot Sequoia were the most\
    \ widely \nused, with nine and eight studies, respectively (Figure 7b). The Phantom\
    \ 4 Pro Camera \n(multispectral RGB) was applied in seven studies, followed by\
    \ the DJI Phantom 3 camera \nin four studies. Regarding the hyperspectral and\
    \ LiDAR sensors, the Nano-Hyperspec \nsensor was used in four studies and LiAir\
    \ 200 in two studies. \nThe preferred model brands of the cameras—related to the\
    \ type and payload of the \ndrones used in FIPD studies—were the DJI Phantom camera,\
    \ due to the discussed reasons, \nand the Sony camera, which is known for its\
    \ quality and specification [12,46,55,56,73,77]. \nThe Micasense series was the\
    \ leader of the multispectral cameras, containing five bands \nthat capture data\
    \ in the RGB, near-infrared, and red-edge regions (400–900 nm). The com-\npact\
    \ size and weight allow it to be used in a large variety of UAV types. Another\
    \ preferred \nmultispectral sensor is the Parrot Sequoia, which has a low price\
    \ when compared with the \nMicasense series. This camera collects four discrete\
    \ bands: green, red, red-edge, and NIR \n(530–810 nm). The interest in this type\
    \ of camera is due to its ability to obtain information \non the state of vegetation,\
    \ thereby offering the chance calculate vegetation indices, since \nvegetation\
    \ is more reflective in the infrared region [97] for disease detection [21]. On\
    \ the \nother hand, there is the possibility to transform RGB cameras into NIR\
    \ cameras by chang-\ning the filters [20,98,99]. For instance, Lehmann et al.\
    \ [60] removed the visible light filter, \nand a neutral glass filter was placed\
    \ to capture NIR radiation. A similar approach was \napplied by Brovkina et al.\
    \ [12], who used an infra-red filter after removing the visible \nfilter. \nAs\
    \ for the hyperspectral sensors—Nano-Hyperspect, the Pika L. imaging spectrom-\n\
    eter and the UHD S185 spectrometer\nthese were the most used because they are\
    \ adopted\nFigure 7. Summary of sensor types, including: (a) types of remote sensing\
    \ technology identiﬁed in\neach study; (b) top 10 model camera brands.\nConcerning\
    \ the sensor model brands coupled with different UAV architectures, the\nmultispectral\
    \ cameras Micasense Red-edge and Parrot Sequoia were the most widely\nused, with\
    \ nine and eight studies, respectively (Figure 7b). The Phantom 4 Pro Camera\n\
    (multispectral RGB) was applied in seven studies, followed by the DJI Phantom\
    \ 3 camera in\nfour studies. Regarding the hyperspectral and LiDAR sensors, the\
    \ Nano-Hyperspec sensor\nwas used in four studies and LiAir 200 in two studies.\n\
    The preferred model brands of the cameras—related to the type and payload of the\n\
    drones used in FIPD studies—were the DJI Phantom camera, due to the discussed\
    \ reasons,\nand the Sony camera, which is known for its quality and speciﬁcation\
    \ [12,46,55,56,73,77].\nThe Micasense series was the leader of the multispectral\
    \ cameras, containing ﬁve bands that\ncapture data in the RGB, near-infrared,\
    \ and red-edge regions (400–900 nm). The compact\nsize and weight allow it to\
    \ be used in a large variety of UAV types. Another preferred\nmultispectral sensor\
    \ is the Parrot Sequoia, which has a low price when compared with the\nForests\
    \ 2022, 13, 911\n13 of 31\nMicasense series. This camera collects four discrete\
    \ bands: green, red, red-edge, and NIR\n(530–810 nm). The interest in this type\
    \ of camera is due to its ability to obtain information\non the state of vegetation,\
    \ thereby offering the chance calculate vegetation indices, since\nvegetation\
    \ is more reﬂective in the infrared region [97] for disease detection [21]. On\
    \ the\nother hand, there is the possibility to transform RGB cameras into NIR\
    \ cameras by changing\nthe ﬁlters [20,98,99]. For instance, Lehmann et al. [60]\
    \ removed the visible light ﬁlter, and a\nneutral glass ﬁlter was placed to capture\
    \ NIR radiation. A similar approach was applied by\nBrovkina et al. [12], who\
    \ used an infra-red ﬁlter after removing the visible ﬁlter.\nAs for the hyperspectral\
    \ sensors—Nano-Hyperspect, the Pika L. imaging spectrometer,\nand the UHD S185\
    \ spectrometer—these were the most used because they are adopted on\na considerable\
    \ variety of professional drone types. These sensors have a much broader\nspectrum\
    \ than multispectral sensors, which allows the discrimination of small changes\
    \ in\npigmentation and minor anomalies [43], such as water content, and the structure\
    \ of the tree\ncrown [55]. For these reasons, their use is growing. Despite this,\
    \ the authors of [72] stress\nthat operational efforts, storage needed due to\
    \ the high dimensional data and noise, and\nweight [100] are the main constraints\
    \ of this type of sensor.\nWe found three studies that used thermal cameras. Smigaj\
    \ et al. [84,85] associated\nthe temperature of the vegetation, using a thermal\
    \ camera, with red band needle blight\nseverity. Using infrared thermography,\
    \ Maes et al. [60] studied the canopy temperatures of\nmistletoe plants and infected\
    \ and uninfected trees.\nActive sensors such as LiDAR were used in ﬁve studies,\
    \ mainly to extract structural\nfeatures of the forest such as tree segmentation,\
    \ tree crown delineation, and height per-\ncentiles for combination with passive\
    \ sensors [63,81]. LiAir 200 and LR1601-IRIS LiDAR\nmodel brands were the most\
    \ used in the studies analyzed. These models have compatible\ngimbals with the\
    \ DJI Matrice series.\nNotwithstanding the author’s preferences and costs, hyperspectral\
    \ sensors register\nmore precise spectral information and are more sensitive to\
    \ small changes than multispec-\ntral sensors [56]. Therefore, they are suitable\
    \ for identifying changes in vegetation at early\nstages [55,56,80], mid-term,\
    \ and post-disturbance. In spite of this, the spatial resolution is\nlower than\
    \ multispectral and RGB cameras, and they have a complicated process of imagery\n\
    registration [22]. According Tmuši´c et al. [92], multisensor combination for\
    \ FIPDs has been\nparticularly advantageous. For instance, the authors of [79]\
    \ used airborne hyperspectral\nand LiDAR to detect PWD.\n3.4. UAV Data Collection\n\
    3.4.1. Area Coverage\nWe identiﬁed 35 experimental studies with speciﬁed area\
    \ coverage and 14 without\nreferences. The largest mapped area was 16,043 ha,\
    \ distributed over four sections of 3397\nha, 3825 ha, 5283 ha, and 3537 ha. The\
    \ smallest area size mapped was 0.12 ha. Eighty\nseven percent of the studies\
    \ carried out mappings up to 200 ha, and the remaining were\nexclusively above\
    \ 200 ha. The median amount of covered area was 12.25 ha.\nThe parameters analyzed\
    \ indicate that most of the studies were carried out in rel-\natively small geographical\
    \ areas (median = 12.25 ha). However, Xia et al. [78] mapped\n16,043 ha in China,\
    \ distributed in four sections at 700 m altitude, using a ﬁxed-wing UAV\nto detect\
    \ dead or diseased trees with PWD. This remarkable mapping shows the high\ncapacity\
    \ of professional civil UAVs. However, despite UAVs’ technological improvements\n\
    and operational capabilities, there are barriers to research and development due\
    \ to the\nregulatory frameworks adopted by countries worldwide [101]. Due to the\
    \ recent European\nregulations of the Commission Implementing Regulation (EU)\
    \ 2019/947 [102], the factors\nof area coverage, ﬂight height, and UAV type are\
    \ complicated. Firstly, remote pilots need to\nhave a speciﬁc category course\
    \ for performing this type of ﬂight. Any UAV ﬂight above 120\nm and operating\
    \ beyond visual line of sight (BVLOS) is only possible through a declaration\n\
    of operational authorization. A risk analysis carried out through a Speciﬁc Operations\
    \ Risk\nAssessment (SORA) is also required. This harmonized legislation poses\
    \ a signiﬁcant chal-\nForests 2022, 13, 911\n14 of 31\nlenge to researchers, foresters,\
    \ and practitioners, since the bureaucracy around operation is\nvery complex.\n\
    3.4.2. Technical Flight Parameters\nTable 6 shows the ﬂight height and GSD descriptive\
    \ statistics by sensor type used in\nthe studies. GSD results from the combination\
    \ of ﬂight height, focal length, and sensor\nresolution [92]. According to this\
    \ study, it is crucial to deﬁne the camera settings to\ndetermine GSD, which corresponds\
    \ to the distance between pixel centers. The highest\nﬂight altitude was 700 m,\
    \ and the lowest was 20 m performed with a hyperspectral sensor.\nThe median of\
    \ ﬂight height for thermal sensor was 75 m, and the highest was 100 m using\n\
    multispectral sensors.\nTable 6. Flight height and GSD descriptive statistics\
    \ by sensor type.\nFlight Height (m)\nGSD (m)\nSensor Type\nNo.\nMax\nMin\nMedian\n\
    Max\nMin\nMedian\nRGB\n29\n700\n30\n90\n0.080\n0.015\n0.028\nMultispectral\n27\n\
    200\n50\n100\n0.170\n0.020\n0.070\nHyperspectral\n12\n140\n20\n95\n0.560\n0.047\n\
    0.200\nThermal\n4\n122\n60\n75\n0.980\n0.150\n0.211\nIn terms of GSD, the maximum\
    \ value was 0.98 m with the thermal sensor, and the\nminimum was 0.015 m, acquired\
    \ by an RGB sensor. The median ﬂight height for thermal\nsensors’ was 75 m, and\
    \ the highest was 100 m for the multispectral sensors. The RGB\nsensors’ median\
    \ GSD was 0.028 m, and the highest was 0.211 m with the thermal sensors.\nFigure\
    \ 8 illustrates GSD versus ﬂight height for different sensor types by remote\n\
    sensing sensor type. Thermal sensors were ruled out because of the low number\
    \ of studies.\nThere is a positive correlation between ﬂight height and GSD by\
    \ sensor type (hyperspectral,\nmultispectral, and RGB sensors).\nA positive correlation\
    \ between ﬂight height and spatial resolution by sensor type was\nfound, excluding\
    \ the thermal sensors due to the low number of samples. The increase\nin ﬂight\
    \ height decreased the GSD [103]. However, there was not always a proportional\n\
    relationship because GSD is comprised of a combination of ﬂight height, sensor\
    \ resolution,\nand focal length [92]. Thus, lower spatial resolution may affect\
    \ the feature delineation\nresulting from a high ﬂight height. Nevertheless, low\
    \ ﬂight height from an insufﬁcient ﬁeld\nof view might be detrimental to photogrammetric\
    \ products [31].\nImage overlapping is an essential component for structure for\
    \ motion (SfM) pho-\ntogrammetric reconstruction in order to produce digital surface\
    \ models, orthomosaics, and\n3D models [92,103,104]. SfM is a computer vision\
    \ technique that is used to construct models\nand composite orthomosaic images.\
    \ Figures 9a and 9b indicate that multispectral and visi-\nble sensors have the\
    \ same median of frontal–side overlap at 80.0% and 80.0%, respectively.\nForests\
    \ 2022, 13, x FOR PEER REVIEW \n15 of 32 \n \nmedian of flight height for thermal\
    \ sensor was 75 m, and the highest was 100 m using \nmultispectral sensors. \n\
    Table 6. Flight height and GSD descriptive statistics by sensor type. \n \n \n\
    Flight Height (m) \nGSD (m) \nSensor Type \nNo. \nMax \nMin \nMedian \nMax \n\
    Min \nMedian \nRGB \n29 \n700 \n30 \n90 \n0.080 \n0.015 \n0.028 \nMultispectral\
    \ \n27 \n200 \n50 \n100 \n0.170 \n0.020 \n0.070 \nHyperspectral \n12 \n140 \n\
    20 \n95 \n0.560 \n0.047 \n0.200 \nThermal \n4 \n122 \n60 \n75 \n0.980 \n0.150\
    \ \n0.211 \nIn terms of GSD, the maximum value was 0.98 m with the thermal sensor,\
    \ and the \nminimum was 0.015 m, acquired by an RGB sensor. The median flight\
    \ height for thermal \nsensors’ was 75 m, and the highest was 100 m for the multispectral\
    \ sensors. The RGB sen-\nsors’ median GSD was 0.028 m, and the highest was 0.211\
    \ m with the thermal sensors. \nFigure 8 illustrates GSD versus flight height\
    \ for different sensor types by remote sens-\ning sensor type. Thermal sensors\
    \ were ruled out because of the low number of studies. \nThere is a positive correlation\
    \ between flight height and GSD by sensor type (hyperspec-\ntral, multispectral,\
    \ and RGB sensors). \n(a) \n(b) \n(c) \nFigure 8. Ground control sampling distance\
    \ (GSD) versus flight height for different sensor types: \n(a) hyperspectral sensor\
    \ (R2 = 0.16); (b) multispectral sensors (R2 = 0.39); (c) RGB sensors (R2 = 0.44).\
    \ \nA positive correlation between flight height and spatial resolution by sensor\
    \ type was \nfound, excluding the thermal sensors due to the low number of samples.\
    \ The increase in \nflight height decreased the GSD [103]. However, there was\
    \ not always a proportional re-\nlationship because GSD is comprised of a combination\
    \ of flight height, sensor resolution, \nand focal length [92]. Thus, lower spatial\
    \ resolution may affect the feature delineation re-\nsulting from a high flight\
    \ height. Nevertheless, low flight height from an insufficient field \nFigure\
    \ 8. Ground control sampling distance (GSD) versus ﬂight height for different\
    \ sensor types:\n(a) hyperspectral sensor (R2 = 0.16); (b) multispectral sensors\
    \ (R2 = 0.39); (c) RGB sensors (R2 = 0.44).\nForests 2022, 13, 911\n15 of 31\n\
    Forests 2022, 13, x FOR PEER REVIEW \n16 of 32 \n \n \n(a) \n(b) \nFigure 9. Frontal\
    \ and side overlap distribution of UAV imagery included in every study. \nHyperspectral\
    \ and LiDAR sensors present the exact median of frontal–side overlap \nat about\
    \ 60.0% and 60.0%. In the studies that used a multispectral camera, the mean of\
    \ \nfrontal and side overlaps were 81.3% and 77.3%, respectively. In the field\
    \ of visible light \ncameras, the means of frontal and side overlap were 79.6%\
    \ and 72.0%, respectively, while \nfor hyperspectral cameras, the median was 75.0%\
    \ (frontal) and 55.1% (side). Finally, the \nLiDAR boxplot showed means of 57.0%\
    \ and 57.0%. \nThe inter-quartile range (IQR) in LiDAR presented a lower variability\
    \ than the other \ncameras at about 60.0% and 50.0% for the frontal and side overlaps,\
    \ respectively. In mul-\ntispectral cameras, the IQR achieved a higher variation\
    \ at about 90.0% for frontal and \n50.0% for the side overlap. Comparing the amplitude\
    \ range of frontal and side overlap, \nwe identified a much higher IQR in side\
    \ overlap, except in LiDAR cameras. \nAn essential component to planning a flight\
    \ mission is image overlapping (frontal \nand side lap), specifically for the\
    \ structure from motion (SfM) photogrammetric process. \nAppropriate image overlap\
    \ depends on the flight height and the type of forest texture, \nrepeated patterns,\
    \ and trees movement caused by the wind, which introduce more signif-\nicant uncertainty.\
    \ The 3D point cloud obtained in the SfM process allows the detection of \nsingle\
    \ trees, which may be combined with spectral data for crown segmentation to iden-\n\
    tify discolored trees [64]. In the studies analyzed, the image frontal–side overlap\
    \ in the \nvisible light and multispectral regions showed medians of 80% and 80%.\
    \ Although there \nis no standardized protocol, there is a tendency to use a high\
    \ overlap. The studies with \nthe most image frontal–side overlap were Dell et\
    \ al. [71] and Cessna et al. [54], using \n95%/95% and 90%/90%. Although a high\
    \ percentage increases the number of images, \nflight time, the volume of data,\
    \ and computational requirements [34], the authors used \ngeo-auxiliary structural\
    \ metrics to improve the process of tree detection. \nRegarding hyperspectral\
    \ and LiDAR sensors, the percentage of frontal and side over-\nlap was less than\
    \ the other sensors. This decision could have been due to saving the bat-\nteries,\
    \ decreasing the mapping time, or flying at a higher altitude to reduce the overlap\
    \ \npercentage [105]. As a result, the weight of the sensors has much influence\
    \ on battery \nconsumption. \nRadiometric calibration and correction to reduce\
    \ the atmospheric effects (for in-\nstance, cloud percentage and illumination)\
    \ were performed in 71.4% of the studies. An-\nother critical aspect is geometric\
    \ correction using GCPs, which consists of determining the \nabsolute vertical\
    \ and horizontal errors of the artificial or natural features with known lo-\n\
    cations [98]. However, only 53% of the studies performed geometric correction.\
    \ \nMost authors used empirical line methods such as Lambertian targets (calibration\
    \ \npanels) to avoid radiometric problems in multispectral and hyperspectral images.\
    \ This \nprocedure is fundamental to reducing noise and avoiding vignetting effects\
    \ and lens \nFigure 9. Frontal and side overlap distribution of UAV imagery included\
    \ in every study.\nHyperspectral and LiDAR sensors present the exact median of\
    \ frontal–side overlap\nat about 60.0% and 60.0%. In the studies that used a multispectral\
    \ camera, the mean of\nfrontal and side overlaps were 81.3% and 77.3%, respectively.\
    \ In the ﬁeld of visible light\ncameras, the means of frontal and side overlap\
    \ were 79.6% and 72.0%, respectively, while\nfor hyperspectral cameras, the median\
    \ was 75.0% (frontal) and 55.1% (side). Finally, the\nLiDAR boxplot showed means\
    \ of 57.0% and 57.0%.\nThe inter-quartile range (IQR) in LiDAR presented a lower\
    \ variability than the other\ncameras at about 60.0% and 50.0% for the frontal\
    \ and side overlaps, respectively. In mul-\ntispectral cameras, the IQR achieved\
    \ a higher variation at about 90.0% for frontal and\n50.0% for the side overlap.\
    \ Comparing the amplitude range of frontal and side overlap, we\nidentiﬁed a much\
    \ higher IQR in side overlap, except in LiDAR cameras.\nAn essential component\
    \ to planning a ﬂight mission is image overlapping (frontal\nand side lap), speciﬁcally\
    \ for the structure from motion (SfM) photogrammetric process.\nAppropriate image\
    \ overlap depends on the ﬂight height and the type of forest texture,\nrepeated\
    \ patterns, and trees movement caused by the wind, which introduce more signiﬁ-\n\
    cant uncertainty. The 3D point cloud obtained in the SfM process allows the detection\
    \ of\nsingle trees, which may be combined with spectral data for crown segmentation\
    \ to identify\ndiscolored trees [64]. In the studies analyzed, the image frontal–side\
    \ overlap in the visible\nlight and multispectral regions showed medians of 80%\
    \ and 80%. Although there is no\nstandardized protocol, there is a tendency to\
    \ use a high overlap. The studies with the\nmost image frontal–side overlap were\
    \ Dell et al. [71] and Cessna et al. [54], using 95%/95%\nand 90%/90%. Although\
    \ a high percentage increases the number of images, ﬂight time,\nthe volume of\
    \ data, and computational requirements [34], the authors used geo-auxiliary\n\
    structural metrics to improve the process of tree detection.\nRegarding hyperspectral\
    \ and LiDAR sensors, the percentage of frontal and side\noverlap was less than\
    \ the other sensors. This decision could have been due to saving\nthe batteries,\
    \ decreasing the mapping time, or ﬂying at a higher altitude to reduce the\noverlap\
    \ percentage [105]. As a result, the weight of the sensors has much inﬂuence on\n\
    battery consumption.\nRadiometric calibration and correction to reduce the atmospheric\
    \ effects (for instance,\ncloud percentage and illumination) were performed in\
    \ 71.4% of the studies. Another critical\naspect is geometric correction using\
    \ GCPs, which consists of determining the absolute\nvertical and horizontal errors\
    \ of the artiﬁcial or natural features with known locations [98].\nHowever, only\
    \ 53% of the studies performed geometric correction.\nMost authors used empirical\
    \ line methods such as Lambertian targets (calibration\npanels) to avoid radiometric\
    \ problems in multispectral and hyperspectral images. This\nprocedure is fundamental\
    \ to reducing noise and avoiding vignetting effects and lens\ncorrection. However,\
    \ in multitemporal studies, it is difﬁcult to avoid this issue due to\nthe imprecise\
    \ calibration of the imagery, as referenced by Fraser and Congalton [86].\nThe\
    \ illumination and atmospheric conditions are not the same.\nWith respect to thermal\
    \ sensors, the authors performed the calibration in laboratory\nconditions against\
    \ a thermally controlled blackbody radiation source [84,85]. The studies\nForests\
    \ 2022, 13, 911\n16 of 31\nthat used RGB sensors performed the gimbal calibration\
    \ and adjusted the parameters\naccording to the meteorological conditions. Finally,\
    \ we notice that the studies did not\nprovide the LiDAR calibration.\nThe authors\
    \ used the traditional method based on GCPs for georeferencing. Even\nthough it\
    \ is time-consuming, this still presents an accurate and low-cost solution.\n\
    3.4.3. Ancillary Field and Laboratory Data for UAV–FIPD\nFigure 10 shows the ancillary\
    \ ﬁeld and laboratory data for UAV–FIPD used in the\nstudies. Most of the studies\
    \ included ﬁeldwork (91.8%), and different strategies were\nemployed. For this\
    \ analysis, we grouped the ancillary ﬁeld and laboratory data into six\ncategories:\
    \ (i) no ﬁeldwork; (ii) ﬁeld visual assessment of the crown vigor or discoloration;\n\
    (iii) ﬁeld visual assessment and forest inventory; (iv) ﬁeld visual assessment,\
    \ spectroscopy,\nand laboratory analysis; (v) visual ﬁeld assessment, forest inventory,\
    \ and spectroscopy;\n(vi) visual ﬁeld assessment, forest inventory, spectroscopy,\
    \ and laboratory. The most applied\nassessment strategy used was category (ii)\
    \ with 67.4%, followed by categories (iii) and\n(vi), with 10.2%.\nAncillary data\
    \ for FIPDs are essential for fully understanding the spatio-temporal\nprocesses\
    \ and validation model procedures. The strategy is highly dependent on the goals\n\
    of the research. For example, the study of Briechle et al. [87] was conducted\
    \ using only\nthe interpretation of the imagery collected with UAVs due to the\
    \ danger of radiation in\nthe Chernobyl Exclusion Zone. The authors of [75,77,83]\
    \ performed their research through\nimagery interpretation, without in situ measurements\
    \ or laboratory data collection, to\ninvestigate the feasibility of using the\
    \ speciﬁc classiﬁcation algorithms.\nMost authors used a visual ﬁeld assessment\
    \ of the crown vigor or discoloration for\nmodel validation. For instance, Näsi\
    \ et al. [43] identiﬁed damaged trees using healthy,\ninfected, and dead classes.\
    \ Safonova et al. [47,52] assessed the damage to ﬁr trees caused by\nthe attacks\
    \ of bark beetles using four health classes: (a) completely healthy tree or recently\
    \ at-\ntacked by beetles; (b) tree colonized by beetles; (c) recently died tree;\
    \ (d) deadwood. The au-\nthors of [46,68] turned to experts in order to support\
    \ the damage or attack assessments.\nForests 2022, 13, x FOR PEER REVIEW \n17\
    \ of 32 \n \ncorrection. However, in multitemporal studies, it is difficult to\
    \ avoid this issue due to the \nimprecise calibration of the imagery, as referenced\
    \ by Fraser and Congalton [86]. The il-\nlumination and atmospheric conditions\
    \ are not the same. \nWith respect to thermal sensors, the authors performed the\
    \ calibration in laboratory \nconditions against a thermally controlled blackbody\
    \ radiation source [84,85]. The studies \nthat used RGB sensors performed the\
    \ gimbal calibration and adjusted the parameters ac-\ncording to the meteorological\
    \ conditions. Finally, we notice that the studies did not pro-\nvide the LiDAR\
    \ calibration. \nThe authors used the traditional method based on GCPs for georeferencing.\
    \ Even \nthough it is time-consuming, this still presents an accurate and low-cost\
    \ solution. \n3.4.3. Ancillary Field and Laboratory Data for UAV–FIPD \nFigure\
    \ 10 shows the ancillary field and laboratory data for UAV–FIPD used in the \n\
    studies. Most of the studies included fieldwork (91.8%), and different strategies\
    \ were em-\nployed. For this analysis, we grouped the ancillary field and laboratory\
    \ data into six cate-\ngories: (i) no fieldwork; (ii) field visual assessment\
    \ of the crown vigor or discoloration; \n(iii) field visual assessment and forest\
    \ inventory; (iv) field visual assessment, spectros-\ncopy, and laboratory analysis;\
    \ (v) visual field assessment, forest inventory, and spectros-\ncopy; (vi) visual\
    \ field assessment, forest inventory, spectroscopy, and laboratory. The \nmost\
    \ applied assessment strategy used was category (ii) with 67.4%, followed by catego-\n\
    ries (iii) and (vi), with 10.2%. \n \nFigure 10. Percentage of each category of\
    \ ancillary field and laboratory data for UAV–FIPD. (i) No \nfieldwork; (ii) field\
    \ visual assessment of the crown vigor or discoloration; (iii) field visual assess-\n\
    ment and forest inventory; (iv) field visual assessment, spectroscopy, and laboratory\
    \ analysis; (v) \nvisual field assessment, forest inventory, and spectroscopy;\
    \ (vi) visual field assessment, forest in-\nventory, spectroscopy, and laboratory.\
    \ \nAncillary data for FIPDs are essential for fully understanding the spatio-temporal\
    \ \nprocesses and validation model procedures. The strategy is highly dependent\
    \ on the goals \nof the research. For example, the study of Briechle et al. [87]\
    \ was conducted using only the \nFigure 10. Percentage of each category of ancillary\
    \ ﬁeld and laboratory data for UAV–FIPD. (i) No\nﬁeldwork; (ii) ﬁeld visual assessment\
    \ of the crown vigor or discoloration; (iii) ﬁeld visual assessment\nand forest\
    \ inventory; (iv) ﬁeld visual assessment, spectroscopy, and laboratory analysis;\
    \ (v) visual\nﬁeld assessment, forest inventory, and spectroscopy; (vi) visual\
    \ ﬁeld assessment, forest inventory,\nspectroscopy, and laboratory.\nForests 2022,\
    \ 13, 911\n17 of 31\nCombining a visual ﬁeld assessment with a forest inventory\
    \ allows us to determine\nthe defoliation rates for each sample tree [55] and\
    \ characterize the stand using fundamental\ndendrometric variables, such as diameter\
    \ at breast height (DBH), tree height, and the social\ncategorization of trees\
    \ [12]. In addition, a preexisting continuous forest inventory may be\nhelpful\
    \ to complement monitorization and generate collections of trees to use in studies\
    \ as\nground references [86].\nClassical damage identiﬁcation and sampling methods\
    \ are limited in detecting changes\nafter infections or pest attacks. In this\
    \ sense, other ﬁeld collection strategies can be applied,\nsuch as biochemical\
    \ parameters using spectrometers [80] to measure the leaf chlorophyll\ncontent\
    \ (Cab) and water content (WC) of each tree, spectral measurements, laboratory\n\
    analysis [72], and the assessment of leaf area index (LAI) using a plant canopy\
    \ analyzer [62].\n3.5. Data Processing and Analytical Methods\n3.5.1. Spatial\
    \ Unit Analysis\nRegarding spatial unit analysis, 67.4% (33 studies) used an object-based\
    \ approach\nand 22.4% used a pixel-based approach (11 studies), and both were\
    \ applied by 10.2%\n(5 studies). As a minimal unit in a digital image, pixels\
    \ may be used for every scale\nstudy. However, only spectral properties are considered\
    \ in analytical methods, while\nobject-based approaches are performed using segmentation\
    \ approaches that group objects\nbased on statistical or feature similarities.\
    \ This approach is mainly performed before\nfeature extraction and applying classiﬁers,\
    \ since these methods cannot add contextual\ninformation [28].\nThe authors preferred\
    \ the object-based approach due to its high-resolution imagery and\nsubmeter resolution\
    \ (<1 m), making it possible to perform an individual tree crown extraction\n\
    and delineation by substituting the traditional fieldwork [19,44,50]. Zhang et\
    \ al. [55] stress that\ntree crown extraction is a prerequisite for diseased detection\
    \ and mapping.\n3.5.2. Segmentation of Single Tree Objects\nTable 7 summarizes\
    \ the segmentation single tree methods used in the studies. Individ-\nual tree\
    \ crown delineation (ITCD) studies using photogrammetric or LiDAR point-cloud\n\
    utilize a canopy height model (CHM) or digital height model (DSM) to calculate\
    \ the local\nmaximum height value. For example, they ﬁnd treetops or locate trees\
    \ using algorithms\nsuch as local maxima ﬁltering, image binarization, scale analysis,\
    \ and template matching.\nTree delineations are grouped in valley following, region\
    \ growing, and watershed segmen-\ntation. Treetops are usually used as a seed\
    \ for region growing and watershed segmentation.\nTherefore, this process is required\
    \ prior to crown delineation [106]. Many studies combine\ntree detection and crown\
    \ delineation to extract the crown shape [107,108].\nForests 2022, 13, 911\n18\
    \ of 31\nTable 7. Summary of segmentation single tree methods in the studies.\n\
    Segmentation Single Tree\nMethod\nSynopsis\nStudies\nManually\nManually segmented\
    \ trees\nDigitalization of each tree crown above imagery using GIS software.\n\
    [15,39,40,50,54,56,60,64,68,79,80]\nLocal maxima ﬁlter and Buffer\nLocal maxima\
    \ ﬁlter within a rasterized CHM to detect the treetops, then a buffer applied\
    \ on\nthe treetop using GIS software.\n[39,46,48,84,85]\nRaster-based\nMean shift\
    \ algorithm\nGEOBIA method. Multispectral image segmentation using ArcGIS segment\
    \ mean shift tool.\n[66]\nMultiresolution segmentation\nGEOBIA method. Multispectral\
    \ image segmentation using eCognition software\nmultiresolution segmentation tool.\n\
    [12,61,83]\nLocal maxima ﬁlter and mean shift algorithm\nLocal maxima of a sliding\
    \ window using the brightness of the multispectral image. Then,\nthe select by\
    \ location tool is used between treetops and for large-scale mean shift algorithm\n\
    segments (GEOBIA).\n[57]\nSafonova et al. Wavelet-based local thresholding\nTree\
    \ crown delineation using RGB images. The steps are contrast enhancement, crown\n\
    segmentation based on wavelet transformation and morphological operations, and\n\
    boundary detection.\n[52]\nSafonova et al. Treetop detection\nRGB images are transformed\
    \ into one grey-scale band image; next, the grey-scale band\nimage is converted\
    \ into a blurred image; ﬁnally, the blurred image is converted into a\nbinary\
    \ image.\n[47]\nVoronoi Tesselations\nLocal maxima ﬁlter within a rasterized CHM\
    \ calculates the treetops and then uses a Voronoi\ntessellation algorithm [110].\n\
    [65]\nDalponte individual tree segmentation\nLocal maxima within a rasterized\
    \ CHM calculates the treetops and then uses a\nregion-growing algorithm for individual\
    \ segmentation [111,112].\n[50,59]\nWatershed segmentation\nVicent and Soille\
    \ original algorithm [113]. When the CHM is inverted, tree tops or\nvegetation\
    \ clusters look like “basins”.\n[49]\nMarker-controlled watershed [109]. Marker\
    \ and segmentation functions are used for\nmulti-tree identiﬁcation and segmentation\
    \ using rasterized CHM [114].\n[50,86]\nBinary watershed analysis and the Euclidean\
    \ distance using rasterized CHM or NIR band.\n[69,79]\nHyyppä et al. [115] methodology.\n\
    [43]\nNyguen Treetops in nDSM data\nBased on pixel intensity, an iterative sliding\
    \ window is passed over the nDSM. Finally, the\nreﬁnement is applied to eliminate\
    \ treetops that are too close to each other.\n[53]\nVector-based\n3D region-growing\
    \ algorithm\n3D region-growing algorithm applied in a point cloud (LiDAR or photogrammetric)\
    \ using a\nbuilt-in function for treetop detection [116].\n[50,63,79]\n3D segmentation\
    \ of single trees\nPoint cloud-based method with tree segmentation using a normalized\
    \ cut algorithm [117].\n[87]\nVoxel-based single tree\nLidar point cloud data\
    \ are converted into voxels in order to estimate the leaf area density\nand the\
    \ construction of the 3D forest scene.\n[63]\nForests 2022, 13, 911\n19 of 31\n\
    The data aggregation was performed using Zhen et al.’s [109] categories. The seg-\n\
    mentation of single tree classes established were manually digitalized, raster-based,\
    \ and\nvector-based.\nWe highlight the manual method that was used in 12 studies,\
    \ which consisted of man-\nual tree crown delineation using geographic information\
    \ system (GIS) software. The local\nmaxima ﬁlter with a posterior buffer was applied\
    \ in ﬁve studies.\nThe most frequently used raster-based method was watershed\
    \ segmentation in six\nstudies, with the original and marker-controlled variants.\
    \ Concerning the region-growing\nalgorithms used, the Dalponte individual tree\
    \ segmentation was applied in two studies and\nVoronoi tessellation in one. Subsequently,\
    \ geospatial object-based image analysis (GEOBIA)\nusing multispectral or RGB\
    \ image segmentation methods were used with multiresolution\nsegmentation (3)\
    \ and the mean shift algorithm (1). Original approaches such as wavelet-\nbased\
    \ local thresholding and treetops using normalized digital surface model (nDSM)\
    \ data\nwere also found in the studies.\nThe vector-based approaches were the\
    \ 3D region-growing algorithm (4) and the\nnormalized cut algorithm (1).\nThe\
    \ analysis of the summarized methods revealed that the manual approach was\npreferred\
    \ since it avoids background noise such as shadows and other vegetation types.\n\
    On the other hand, using the manual approach prevented missing tree crowns or\
    \ potential\nerrors from interpolation and smoothing procedures [109], which are\
    \ the disadvantages of\nraster-based methods. However, the manual approach could\
    \ be unsustainable when areas\nhave many trees.\nRaster-based methods are easy\
    \ to implement, despite the drawbacks mentioned earlier.\nVector-based techniques\
    \ could be useful for detecting small and understory trees, despite\nbeing harder\
    \ to implement [118].\n3.5.3. Feature Extraction and Selection\nTable 8 summarizes\
    \ the feature extraction techniques for UAV imagery applied in the\nstudies. The\
    \ investigated features used in the studies were catalogued according to the types\n\
    suggested by the authors in [119,120]. These features are obtainable attributes\
    \ or properties\nof objects and scenes [121]. They are computed from original\
    \ bands or a combination\nof bands [122]. They include spectral features, textural\
    \ features, linear transformations,\nmultisensors and multitemporal images. Geo-auxiliary\
    \ features extracted from LiDAR [118]\nor photogrammetric point clouds [103] include\
    \ digital surface models (DSM), canopy height\nmodels (CHM), individual tree detection,\
    \ and topographic features. Spectral features,\nincluding statistics of original\
    \ bands, ratios between bands, and vegetation indices, were the\nmost popular\
    \ feature type, followed geo-auxiliary features. Variables including multisensor\n\
    and multitemporal imagery were used in three studies. Textural and linear transformations\n\
    were also used in three studies.\nTable 8. Summary of feature extraction techniques\
    \ of UAV imagery applied in the studies.\nFeature Type\nDescription\nStudies\n\
    Spectral features\nStatistics of original bands, ratios between\nbands, vegetation\
    \ indices\n[12,15,39,40,43–46,48–51,54–64,66,68–72,75,77,79–87]\nTextural features\n\
    Gray level co-occurrence matrix (GLCM),\ngrey level difference vector (GLDV)\n\
    [48,68,86]\nLinear transformations\nHue, saturated and intensity (HSI), principal\n\
    component analysis (PCA)\n[55,61,79]\nGeo-auxiliary\nOriginal and normalized digital\
    \ surface\nmodels (DSM) such as digital elevation\nmodels (DEM), canopy height\
    \ models\n(CHM), slope, aspect, height percentiles\n[12,39,48,50,53,54,62,63,65,68,71,81,85–87]\n\
    Forests 2022, 13, 911\n20 of 31\nTable 8. Cont.\nFeature Type\nDescription\nStudies\n\
    Multisensor\nInclusion of data obtained from different\nsensors in analytical\
    \ methods\n[44,62,79,84,87]\nMultitemporal\nInclusion of multitemporal data classiﬁcation\n\
    in analytical methods\n[15,40,48,59,69]\nSpectral features are applied to explain\
    \ differences in particular symptoms of canopy [19,123].\nFor instance, Klouˇcek\
    \ et al. [46] calculated selected vegetation indices and evaluated\nthem based\
    \ on visual differences in the spectral curves of infested and healthy trees.\n\
    The classiﬁcation included Greenness Index (GI), Simple Ratio (SR), Green Ratio\
    \ Vegetation\nIndex (GRVI), Normalized Difference Vegetation Index (NDVI), and\
    \ Green Normalized\nDifference Vegetation Index (GNDVI).\nIn terms of geo-auxiliary\
    \ features used to improve the analytical methods, Minaˇrík\net al. [50] extracted\
    \ elevation features (crown area, height percentiles) and three vegetation\nindices\
    \ (NDVI, Normalized Difference Red-Edge Index (NDRE), and Enhanced Normalized\n\
    Difference Vegetation Index (ENDVI)) to detect a bark beetle disturbance in a\
    \ mixed urban\nforest. We highlight Nguyen et al. [53], who used nine orthomosaics\
    \ and normalized\ndigital surface models (nDSM) to detect and classify healthy\
    \ and declining ﬁr trees and\ndeciduous trees.\nConsidering the inclusion of multitemporal\
    \ features in the analytical methods, we\nnoticed that Abdollahnejad and Panagiotidis\
    \ [48] used a combination of bi-temporal in-\ntegrated spectral and textural information\
    \ to discriminate tree species and health status,\nachieving a satisfactory result.\
    \ Using multisensor features, Lin et al. [63] assessed the poten-\ntial of a hyperspectral\
    \ approach, a lidar approach, and a combined approach to characterize\nindividual\
    \ trees’ pine shoot beetle (PSB) damage.\nAlthough less applied, the authors used\
    \ textural features such as GLCM by Guerra-\nHernández [65] and linear transformation\
    \ through HSI [61].\nRegarding feature selection, 83.7% of the studies selected\
    \ variables without reduction\ntechniques. The authors of [52,57,83] used the\
    \ mean decrease in impurity (MDI) test to\nquantify the importance of features\
    \ and excluded the less critical features. For example,\nYu et al. [79,81] and\
    \ Zhang et al. [56] used principal component analysis (PCA) to reduce\nthe data\
    \ dimensionally. Recursive feature elimination (RFE) for each ﬂight campaign\n\
    was applied by Pádua et al. [69]. Finally, Yu et al. [80] calculated the Pearson’s\
    \ correla-\ntion coefﬁcient between the features and used a stepwise regression\
    \ method to test the\nmulticollinearity between them, excluding redundant variables.\n\
    The above studies show that feature extraction and selection may improve the analyti-\n\
    cal methods applied to discriminate between unhealthy and healthy canopies. However,\n\
    a small portion of the studies used a feature reduction or selection technique.\
    \ This result\ncan be explained by the high use of a stable image classiﬁcation\
    \ algorithm such as random\nforest, which is insensitive to the dimensionality\
    \ of data [124,125]. On the other hand, most\nauthors probably calculated a limited\
    \ number of features due to high correlation. Other\nreasons may have been to\
    \ avoid overﬁtting, a decrease in classiﬁcation accuracy, or high\ncomputational\
    \ costs [126].\n3.5.4. Analysis Type, Algorithms, and Overall Accuracy (OA)\n\
    Figure 11 summarizes the algorithms used in the studies by analysis method. We\
    \ have\nconsidered only the best performance of the algorithms in terms of accuracy\
    \ or shared\nmetric for the ease of describing the different algorithms used.\n\
    Forests 2022, 13, 911\n21 of 31\n \nindividual tree crown delineation (ITCD) algorithm.\
    \ We found three studies using\nregression (LR), one using logistic regression\
    \ (LOG), and two using RF regression m\nWe also highlighted the class “Others”,\
    \ which included the radiosity applicable to \nindividual objects (RAPID) model,\
    \ the ISIC-SPA-P-PLSR framework, histogram an\nand Getis Order GI among the different\
    \ analytical methods. \n \nFigure 11. Summary of the algorithms used in the studies:\
    \ CNN: convolutional neural n\nITCD: individual tree crown delineation; KNN: K-nearest\
    \ neighbor; LOGR: logistic regress\nlinear regression; MLC: maximum likelihood;\
    \ MSS: multiscale segmentation; PLS: part\nsquares; RF: random forest; SVM: support\
    \ vector machine; TA: thresholding analysis; XGBo\ntreme gradient boosting. \n\
    Figure 12. The overall accuracy of the different classifiers. \nFigure 11. Summary\
    \ of the algorithms used in the studies: CNN: convolutional neural network; ITCD:\n\
    individual tree crown delineation; KNN: K-nearest neighbor; LOGR: logistic regression;\
    \ LR: linear\nregression; MLC: maximum likelihood; MSS: multiscale segmentation;\
    \ PLS: partial least squares;\nRF: random forest; SVM: support vector machine;\
    \ TA: thresholding analysis; XGBoost: eXtreme\ngradient boosting.\nThe classiﬁcation\
    \ approach is broadly used for quantifying trees. Regarding the\nanalysis methods,\
    \ most of the studies (79.6%) used a classiﬁcation approach, 12.2% used\nregression\
    \ and other methods such as statistical analysis and histogram analysis, and 8.2%\n\
    used damage by stressors, different types of species, and the total area affected.\
    \ Regres-\nsion studies focus on a different level of damage and provide statistical\
    \ signiﬁcance for\nregression coefﬁcients and the relation between classes. Statistical\
    \ methods, physically\nbased models such as radiosity applicable to porous individual\
    \ objects to calculate dif-\nferent vegetation variables, and speciﬁc frameworks\
    \ were also used to estimate the level\nof damage.\nAs previously stated, most\
    \ of the studies used the classiﬁcation approach. The classi-\nﬁers most used\
    \ in the classiﬁcation approach were the random forest (RF) and convolutional\n\
    neural network (CNN), with 11 studies (Figure 12). Five studies applied the support\
    \ vector\nmachine (SVM) algorithm, and three applied K-Nearest Neighbor (KNN)\
    \ and the individ-\nual tree crown delineation (ITCD) algorithm. We found three\
    \ studies using linear regression\n(LR), one using logistic regression (LOG),\
    \ and two using RF regression models. We also\nhighlighted the class “Others”,\
    \ which included the radiosity applicable to porous individ-\nual objects (RAPID)\
    \ model, the ISIC-SPA-P-PLSR framework, histogram analysis, and Getis\nOrder GI\
    \ among the different analytical methods.\nForests 2022, 13, 911\n22 of 31\n \n\
    \ \nFigure 11. Summary of the algorithms used in the studies: CNN: convolutional\
    \ neural network; \nITCD: individual tree crown delineation; KNN: K-nearest neighbor;\
    \ LOGR: logistic regression; LR: \nlinear regression; MLC: maximum likelihood;\
    \ MSS: multiscale segmentation; PLS: partial least \nsquares; RF: random forest;\
    \ SVM: support vector machine; TA: thresholding analysis; XGBoost: eX-\ntreme\
    \ gradient boosting. \n \nFigure 12. The overall accuracy of the different classifiers.\
    \ \nFigure 12. The overall accuracy of the different classiﬁers.\nThe main CNN\
    \ architectures used were R-CNN [77], AlexNET [74], YOLOv3 [76],\nResNet50 [53,82],\
    \ DeepLabv3+ [78], PointNet [87], 3D-ResCNN [81], 3D-CNN [51],\nSCANet [75], and\
    \ other types [47].\nOut of 32 studies that calculated OA, we used only 28 for\
    \ this analysis (Figure 12).\nSome algorithms, such as logistic regression, linear\
    \ regression, the 3D rapid model, the\nISI-SPA-P-PLSR framework, and thresholding\
    \ analysis, were ruled out of the study because\nthey had only one sample. Figure\
    \ 12 shows that the median overall accuracy for all\nalgorithms used was higher\
    \ than 0.85. SVM achieved the highest median (0.93), followed\nby ITCD and CNN\
    \ with 0.90 and 0.88, respectively. The best algorithm performance was\nSVM (0.99)\
    \ and ITCD (0.99). The worst performance was RF (0.55).\nA wide range of non-parametric\
    \ and parametric algorithms have been applied in FIPD\nmonitoring. Non-parametric\
    \ machine learning algorithms that input data are not limited to\nnormal distribution,\
    \ such as RF, CNN, and SVM, and are preferred to quantify and detect\ndamages.\
    \ RF can be used for classiﬁcation and regression problems, allowing a straightfor-\n\
    ward interpretation of the model structure and determining the variable importance.\
    \ For\nexample, the authors of [72] used pixel-based RF to classify three levels\
    \ of PWD infection\n(infected, suspicious, and health) and achieved 95% accuracy.\
    \ Duarte et al. [57] performed\nlarge-scale mean shift segmentation (LSMSS) on\
    \ a single-date multispectral image to extract\ntree crowns with binary classiﬁcation\
    \ (healthy or dead trees) and achieved 98% overall\naccuracy using a RF algorithm.\n\
    CNN is a class of deep learning algorithm most used for spatial pattern analysis\
    \ in\nremote sensing imagery. Classiﬁcation and regression approaches could be\
    \ used in remotely\nsensed data in the following tasks: scene-wise classiﬁcation,\
    \ object detection, semantics,\nand instance segmentation [127,128]. We highlight\
    \ the work of Safonova et al. [47] which\nincludes two steps to detect bark beetle\
    \ damage in a ﬁr forest using an RGB image. First,\nthe authors applied a strategy\
    \ to ﬁnd the tree crowns, and in the second step, a new\nCNN network architecture\
    \ was used to classify each canopy. The authors using data\naugmentation achieved\
    \ 95% accuracy. Briechle et al. [87] performed classiﬁcation using 3D\nDeep Neural\
    \ Network PointNet ++ using UAV-based LiDAR and multispectral imagery of\nmultiple\
    \ species and standing dead trees. The overall accuracy was 90.2%.\nThe SVM method\
    \ based on statistical learning theory has been commonly used in\nFIPD detection\
    \ and monitoring [48]. An example is a study by Safonova et al. [52], which\n\
    automated individual tree crown delineation and Gaussian SVM to extract particular\n\
    species pixel by pixel and assess the tree canopy vitality. The authors achieved\
    \ 99% overall\naccuracy applying this methodology in a multispectral image.\n\
    In the ﬁrst stage, the strategies adopted in different classiﬁcation approaches—manual\n\
    or automated tree segmentation—were applied to process the tree crown delineation.\
    \ Next,\nForests 2022, 13, 911\n23 of 31\nthe authors used spectral indices, topographic\
    \ variables, texture, or contextual information\nbased on different image types.\n\
    3.6. Pre-Processing and Analysis Software\nMost of the studies used more than\
    \ one processing and analysis software. Therefore, to\ncount them, all the software\
    \ brands in each paper were considered. Figure 13a illustrates the\npreferred\
    \ processing software used in the studies. The photogrammetric software Agisoft\n\
    (Metashape or Photoscan) was used in 27 studies, followed by Pix4D Mapper in 6\
    \ studies.\nThe point cloud processing software LiDAR 360 was used in four studies.\
    \ For hyperspectral\nimagery, Spectronon processing was used in three studies\
    \ and Headwall Spectral View was\nalso used in three. As shown in Figure 13b,\
    \ ArcGIS was used in 17 studies, Python libraries\nin 15, and R software in 13\
    \ studies.\nForests 2022, 13, x FOR PEER REVIEW \n24 of 32 \n \nremote sensing.\
    \ Sophisticated computer vision algorithms are also available on Python \nlibraries\
    \ such as Tensorflow, Pytorch, and scikit-learn, which are easy to implement and\
    \ \nadaptable to other code languages. \n \n(a) \n(b) \nFigure 13. Processing\
    \ and analysis software applied in the studies. (a) Image processing software\
    \ \nbrands; (b) analysis software used. \n4. Research Gaps, Challenges, and Further\
    \ Research \nThe scientific literature analyzed shows serious concern regarding\
    \ improving the de-\ntection and monitoring of pests and diseases using UAV data.\
    \ Proof of this is the increased \nnumber of studies in recent years. However,\
    \ we found that most studies were carried out \nin small experimental areas that\
    \ did not always represent the reality of disturbances in \nforests. In addition,\
    \ the effects of climate change could promote the development of other \npests\
    \ and diseases. \nOur systematic review analysis highlights that the first research\
    \ gap is related to the \nlack of flight parameter standards in FIPD monitoring,\
    \ since each case is unique. Moreo-\nver, there is no base protocol for different\
    \ UAV systems or sensor types. Therefore, the \nUAV type, sensor type, flight\
    \ parameters, pre-processing and processing steps, weather \nconditions, and regulations\
    \ can affect the results. Hence, providing a detailed description \nof all flight\
    \ parameters and processing activities is essential to be taken as a reference\
    \ for \npractitioners, researchers, and forest professionals [29,92]. \nA second\
    \ research gap is how to take advantage of different UAV imagery and point \n\
    clouds to detect pests and diseases. Many features can be extracted from UAV-based\
    \ im-\nages and point clouds, such as spectral indices, gray level co-occurrence\
    \ matrices \n(GLCMs), digital surface models (DSMs), digital terrain models (DTMs),\
    \ and point cloud \nmetrics, to integrate classification or regression models.\
    \ We found a lack of studies using \ndata fusion between optical sensors and LiDAR.\
    \ Combining these technologies is advan-\ntageous for studying vegetation structure,\
    \ especially tree crown delineation [54,85]. One \nFigure 13. Processing and analysis\
    \ software applied in the studies. (a) Image processing software\nbrands; (b)\
    \ analysis software used.\nAgisoft was the most popular software due to its automatic\
    \ image quality assessment\nadvantages of excluding low-quality images and its\
    \ standardized workﬂow [23]. Pix4D\nmapper was the second preferred software due\
    \ to the dedicated and automated photogram-\nmetry workﬂow. The software for imagery\
    \ classiﬁcation with the most occurrences was\nArcGIS, which is user friendly\
    \ and offers a complete and standardized workﬂow for remote\nsensing. Sophisticated\
    \ computer vision algorithms are also available on Python libraries\nsuch as Tensorﬂow,\
    \ Pytorch, and scikit-learn, which are easy to implement and adaptable\nto other\
    \ code languages.\n4. Research Gaps, Challenges, and Further Research\nThe scientiﬁc\
    \ literature analyzed shows serious concern regarding improving the de-\ntection\
    \ and monitoring of pests and diseases using UAV data. Proof of this is the increased\n\
    number of studies in recent years. However, we found that most studies were carried\
    \ out\nin small experimental areas that did not always represent the reality of\
    \ disturbances in\nforests. In addition, the effects of climate change could promote\
    \ the development of other\npests and diseases.\nForests 2022, 13, 911\n24 of\
    \ 31\nOur systematic review analysis highlights that the ﬁrst research gap is\
    \ related to the\nlack of ﬂight parameter standards in FIPD monitoring, since\
    \ each case is unique. Moreover,\nthere is no base protocol for different UAV\
    \ systems or sensor types. Therefore, the UAV type,\nsensor type, ﬂight parameters,\
    \ pre-processing and processing steps, weather conditions,\nand regulations can\
    \ affect the results. Hence, providing a detailed description of all ﬂight\nparameters\
    \ and processing activities is essential to be taken as a reference for practitioners,\n\
    researchers, and forest professionals [29,92].\nA second research gap is how to\
    \ take advantage of different UAV imagery and point\nclouds to detect pests and\
    \ diseases. Many features can be extracted from UAV-based images\nand point clouds,\
    \ such as spectral indices, gray level co-occurrence matrices (GLCMs),\ndigital\
    \ surface models (DSMs), digital terrain models (DTMs), and point cloud metrics,\
    \ to\nintegrate classiﬁcation or regression models. We found a lack of studies\
    \ using data fusion\nbetween optical sensors and LiDAR. Combining these technologies\
    \ is advantageous for\nstudying vegetation structure, especially tree crown delineation\
    \ [54,85]. One obstacle is the\ndifﬁculty in performing image alignment due to\
    \ the repeated patterns in forests [129]. On\nthe other hand, the high price of\
    \ sensors is also an important constraint.\nIn terms of feature extraction, we\
    \ noticed that genetic programming (GP) was used\nin the studies to combine spectral\
    \ bands in satellite imagery [130], to improve the process\nof land use and land\
    \ cover (LULC) classiﬁcation [131], and to identify burned areas [132].\nAlthough\
    \ the work of Mejia-Zuluaga et al. [133] is outside of the time interval of this\n\
    study, we veriﬁed that the GP algorithm applied achieved an overall accuracy of\
    \ 96% for\nclassifying mistletoe. In this sense, this approach shows the ability\
    \ to extract features and\nimprove damage classiﬁcation. However, to the best\
    \ of our knowledge, multiclass genetic\nprogramming (M3GP) has not been used in\
    \ UAV images to classify different vegetation\nvitality.\nDeep learning approaches\
    \ such as CNN may be a robust option for segmentation tasks\nand identifying different\
    \ levels of tree crown vitality, as revealed in the studies performed\nin [47,51,53,74,78].\n\
    The third research gap is the limitation of UAV-based imagery to cover large scales.\n\
    This limitation was highlighted by Eugenio et al. [26,29], who noted that UAVs\
    \ can be\n“upscaled” for satellites for expansion without losing accuracy.\nThe\
    \ challenges of FIPD monitoring include the recently imposed UAV regulations.\n\
    Flight altitude is limited to 120 m and a maximum radius of 500 m, and BVLOS rules\
    \ are\nproblematic for forest surveys.\nEach year, UAV technologies are improved.\
    \ UAV and sensor miniaturization bring\nnew challenges. For example, the battery\
    \ duration issues are now a reality (for instance, the\nMatrice 300 battery has\
    \ a duration of 55 min); however, increasingly efﬁcient drones such\nas VTOLs\
    \ may be used to improve research (Wingtra drone series). However, the miniatur-\n\
    ization of hyperspectral cameras and their image collection process is being increasingly\n\
    improved so, in the short term, costs may drop importantly.\nOne of the biggest\
    \ challenges is the vulgarization of drones to complement or sub-\nstitute ﬁeld\
    \ data collection. Eugenio et al. [26,29] stress that breaking resistance and\n\
    disseminating UAVs in the forest community is essential to monitoring our forests.\n\
    Future research will beneﬁt from upscaling with satellite imagery to increase\
    \ area\ncoverage and improve early detection systems. Dash et al. [40] found that\
    \ RapidEye satellite\ndata can expand stress monitoring and be improved with UAV\
    \ sensor data. Therefore,\narea coverage can be increased by combining these different\
    \ platforms [39]. To this end,\nintelligent algorithms based on deep learning\
    \ and genetic programming are necessary for\ndetecting and monitoring disturbances\
    \ in forest contexts.\n5. Conclusions\nThis systematic literature review aimed\
    \ to identify the contribution of UAVs to forest in-\nsect pest and disease monitoring.\
    \ Using a PRISMA protocol, we reviewed 49 peer-reviewed\nForests 2022, 13, 911\n\
    25 of 31\narticles on FIPD monitoring to provide readers with the current practices\
    \ and techniques\nand to identify knowledge gaps and challenges.\nWe conclude\
    \ that the number of papers has increased in recent years, especially in\n2021\
    \ (18 articles). Based on our analysis, China and European Union (EU) countries\
    \ are the\nones with more studies about FIPD monitoring using UAV-based data.\
    \ The most studied\ndiseases were pine wilt disease (PWD) and the most common\
    \ pests were bark beetles (BB).\nPine, European spruce, and ﬁr were the conifers\
    \ most frequently studied, while the most\ncommon hardwoods were eucalypts.\n\
    Rotary-wing drones were the most frequently used due to the market and costs.\n\
    Our ﬁndings document that multispectral and visible light is preferred to monitor\
    \ FIPDs.\nRegarding RGB sensors, the DJI Phantom series camera was the most widely\
    \ used, while\nthe Micasense series was most used for multispectral segments.\
    \ In addition, we noticed an\nincrease in hyperspectral and LiDAR sensors in the\
    \ research of FIPDs.\nDespite the lack of standards for UAV data collection for\
    \ FIPDs, our ﬁndings may\nconstitute a reference for further research. We found\
    \ a positive correlation between GSD and\nﬂight altitude by sensor type and the\
    \ median of frontal and side overlap concerning visible,\nmultispectral, and hyperspectral\
    \ sensors. Most studies included ﬁeldwork to validate the\nresearch, and a signiﬁcant\
    \ number performed radiometric and geometric calibration.\nConcerning the methodological\
    \ approach of the studies, most works used an object-\nbased analysis unit. Due\
    \ to the high spatial resolution of the images, the authors of these\nstudies\
    \ applied several types of methods for tree crown delineation. Tree crown delineation\n\
    is an essential prerequisite for FIPD detection and monitoring. The spectral and\
    \ geo-\nauxiliary features were most used in feature extraction and selection.\
    \ Regarding analytical\nmethods, random forests (RF) and deep learning (DL) classiﬁers\
    \ were the most frequently\napplied in UAV imagery processing.\nOur literature\
    \ review suggests the lack of ﬂight parameter standards in FIPD mon-\nitoring.\
    \ Data fusion procedures for studying vegetation structure could potentially be\n\
    improved by combining optical and LiDAR technologies. Other possible improvements\n\
    for feature extraction include evolutionary algorithms, such as multiclass genetic\
    \ program-\nming. Deep learning algorithms can be fundamental for pattern recognition\
    \ and automatic\ndata processing regarding classiﬁcation or regression.\nFinally,\
    \ upscaling UAV data for satellites to expand data collection without losing\n\
    accuracy is essential for monitoring our forests.\nAuthor Contributions: Conceptualization,\
    \ A.D. and M.C.; methodology, A.D.; software, A.D.; formal\nanalysis, A.D.; data\
    \ curation, A.D.; writing—original draft preparation, A.D., P.C. and M.C.; writing—\n\
    review and editing, A.D., P.C., N.B. and M.C.; visualization, A.D.; supervision,\
    \ M.C.; funding\nacquisition, M.C. All authors have read and agreed to the published\
    \ version of the manuscript.\nFunding: This study was supported by national funds\
    \ through Fundação para a Ciência e a Tecnologia\n(FCT) under the project UIDB/04152/2020—Centro\
    \ de Investigação em Gestão de Informação (MagIC).\nInstitutional Review Board\
    \ Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData\
    \ Availability Statement: Not applicable.\nAcknowledgments: The authors would\
    \ like to thank Cindy Santos, Luís Acevedo-Muñoz, João\nRocha and Sérgio Fabres,\
    \ for all their valuable comments and support.\nConﬂicts of Interest: The authors\
    \ declare no conﬂict of interest.\nReferences\n1.\nAnderegg, W.R.L.; Trugman,\
    \ A.T.; Badgley, G.; Anderson, C.M.; Bartuska, A.; Ciais, P.; Cullenward, D.;\
    \ Field, C.B.; Freeman, J.;\nGoetz, S.J.; et al. Climate-Driven Risks to the Climate\
    \ Mitigation Potential of Forests. Science 2020, 368, eaaz7005. [CrossRef]\n[PubMed]\n\
    2.\nFAO. Assessing Forest Degradation: Towards the Development of Globally Applicable\
    \ Guidlines; Forest Resources Assessment Working\nPaper 177; Food and Agriculture\
    \ Organization of the United Nations: Rome, Italy, 2011.\nForests 2022, 13, 911\n\
    26 of 31\n3.\nCanadell, J.G.; Raupach, M.R. Managing Forests for Climate Change\
    \ Mitigation. Science 2008, 320, 1456–1457. [CrossRef]\n[PubMed]\n4.\nFAO. Climate\
    \ Change Guidelines for Forest Managers; FAO Forestry Paper 172; Food and Agriculture\
    \ Organization of the United\nNations: Rome, Italy, 2013; p. 123.\n5.\nFAO. Managing\
    \ Forests for Climate Change; Food and Agriculture Organization of the United\
    \ Nations: Rome, Italy, 2010; p. 20.\n6.\nDale, V.; JOYCE, L.; Mcnulty, S.; Neilson,\
    \ R.; Ayres, M.; Flannigan, M.; Hanson, P.; Irland, L.; Lugo, A.; PETERSON, C.;\
    \ et al.\nClimate Change and Forest Disturbances. BioScience 2001, 51, 723–734.\
    \ [CrossRef]\n7.\nSenf, C.; Buras, A.; Zang, C.S.; Rammig, A.; Seidl, R. Excess\
    \ Forest Mortality Is Consistently Linked to Drought across Europe.\nNat. Commun.\
    \ 2020, 11, 6200. [CrossRef]\n8.\nSeidl, R.; Spies, T.A.; Peterson, D.L.; Stephens,\
    \ S.L.; Hicke, J.A. Searching for Resilience: Addressing the Impacts of Changing\n\
    Disturbance Regimes on Forest Ecosystem Services. J. Appl. Ecol. 2016, 53, 120–129.\
    \ [CrossRef]\n9.\nKoricheva, J.; Castagneyrol, B. Science Direct Responses of\
    \ Forest Insect Pests to Climate Change: Not so Simple. Curr. Opin.\nInsect Sci.\
    \ 2019, 35, 103–108. [CrossRef]\n10.\nRaffa, K.F.; Aukema, B.H.; Bentz, B.J.;\
    \ Carroll, A.L.; Hicke, J.A.; Turner, M.G.; Romme, W.H. Cross-Scale Drivers of\
    \ Natural\nDisturbances Prone to Anthropogenic Ampliﬁcation: The Dynamics of Bark\
    \ Beetle Eruptions. BioScience 2008, 58, 501–517.\n[CrossRef]\n11.\nLausch, A.;\
    \ Borg, E.; Bumberger, J.; Dietrich, P.; Heurich, M.; Huth, A.; Jung, A.; Klenke,\
    \ R.; Knapp, S.; Mollenhauer, H.; et al.\nUnderstanding Forest Health with Remote\
    \ Sensing, Part III: Requirements for a Scalable Multi-Source Forest Health Monitoring\n\
    Network Based on Data Science Approaches. Remote Sens. 2018, 10, 1120. [CrossRef]\n\
    12.\nBrovkina, O.; Cienciala, E.; Surový, P.; Janata, P. Unmanned Aerial Vehicles\
    \ (UAV) for Assessment of Qualitative Classiﬁcation of\nNorway Spruce in Temperate\
    \ Forest Stands. Geo-Spat. Inf. Sci. 2018, 21, 12–20. [CrossRef]\n13.\nLausch,\
    \ A.; Heurich, M.; Gordalla, D.; Dobner, H.J.; Gwillym-Margianto, S.; Salbach,\
    \ C. Forecasting Potential Bark Beetle\nOutbreaks Based on Spruce Forest Vitality\
    \ Using Hyperspectral Remote-Sensing Techniques at Different Scales. For. Ecol.\
    \ Manag.\n2013, 308, 76–89. [CrossRef]\n14.\nGómez, C.; Alejandro, P.; Hermosilla,\
    \ T.; Montes, F.; Pascual, C.; Ruiz, L.Á.; Álvarez-Taboada, F.; Tanase, M.A.;\
    \ Valbuena, R.\nRemote Sensing for the Spanish Forests in the 21st century: A\
    \ Review of Advances, Needs, and Opportunities. For. Syst. 2019, 28,\neR001. [CrossRef]\n\
    15.\nDash, J.P.; Watt, M.S.; Pearse, G.D.; Heaphy, M.; Dungey, H.S. Assessing\
    \ Very High Resolution UAV Imagery for Monitoring\nForest Health during a Simulated\
    \ Disease Outbreak. ISPRS J. Photogramm. Remote Sens. 2017, 131, 1–14. [CrossRef]\n\
    16.\nGuimarães, N.; Pádua, L.; Marques, P.; Silva, N.; Peres, E.; Sousa, J.J.\
    \ Forestry Remote Sensing from Unmanned Aerial Vehicles:\nA Review Focusing on\
    \ the Data, Processing and Potentialities. Remote Sens. 2020, 12, 1046. [CrossRef]\n\
    17.\nPoley, L.G.; McDermid, G.J. A Systematic Review of the Factors Inﬂuencing\
    \ the Estimation of Vegetation Aboveground Biomass\nUsing Unmanned Aerial Systems.\
    \ Remote Sens. 2020, 12, 1052. [CrossRef]\n18.\nKlosterman, S.; Richardson, A.\
    \ Observing Spring and Fall Phenology in a Deciduous Forest with Aerial Drone\
    \ Imagery. Sensors\n2017, 17, 2852. [CrossRef]\n19.\nHall, R.J.; Castilla, G.;\
    \ White, J.C.; Cooke, B.J.; Skakun, R.S. Remote Sensing of Forest Pest Damage:\
    \ A Review and Lessons\nLearned from a Canadian Perspective. Can. Entomol. 2016,\
    \ 148, S296–S356. [CrossRef]\n20.\nPádua, L.; Vanko, J.; Hruška, J.; Adão, T.;\
    \ Sousa, J.J.; Peres, E.; Morais, R. UAS, Sensors, and Data Processing in Agroforestry:\n\
    A Review towards Practical Applications. Int. J. Remote Sens. 2017, 38, 2349–2391.\
    \ [CrossRef]\n21.\nRullan-Silva, C.D.; Olthoff, A.E.; Delgado de la Mata, J.A.;\
    \ Pajares-Alonso, J.A. Remote Monitoring of Forest Insect Defoliation.\nA Review.\
    \ For. Syst. 2013, 22, 377–391. [CrossRef]\n22.\nYao, H.; Qin, R.; Chen, X. Unmanned\
    \ Aerial Vehicle for Remote Sensing Applications—A Review. Remote Sens. 2019,\
    \ 11, 1443.\n[CrossRef]\n23.\nManfreda, S.; McCabe, M.; Miller, P.; Lucas, R.;\
    \ Pajuelo Madrigal, V.; Mallinis, G.; Ben Dor, E.; Helman, D.; Estes, L.; Ciraolo,\
    \ G.;\net al. On the Use of Unmanned Aerial Systems for Environmental Monitoring.\
    \ Remote Sens. 2018, 10, 641. [CrossRef]\n24.\nTorresan, C.; Berton, A.; Carotenuto,\
    \ F.; Di Gennaro, S.F.; Gioli, B.; Matese, A.; Miglietta, F.; Vagnoli, C.; Zaldei,\
    \ A.; Wallace, L.\nForestry Applications of UAVs in Europe: A Review. Int. J.\
    \ Remote Sens. 2017, 38, 2427–2447. [CrossRef]\n25.\nAdão, T.; Hruška, J.; Pádua,\
    \ L.; Bessa, J.; Peres, E.; Morais, R.; Sousa, J. Hyperspectral Imaging: A Review\
    \ on UAV-Based Sensors,\nData Processing and Applications for Agriculture and\
    \ Forestry. Remote Sens. 2017, 9, 1110. [CrossRef]\n26.\nEugenio, F.C.; Schons,\
    \ C.T.; Mallmann, C.L.; Schuh, M.S.; Fernandes, P.; Badin, T.L. Remotely Piloted\
    \ Aircraft Systems and Forests:\nA Global State of the Art and Future Challenges.\
    \ Can. J. For. Res. 2020, 50, 705–716. [CrossRef]\n27.\nDainelli, R.; Toscano,\
    \ P.; Di Gennaro, S.F.; Matese, A. Recent Advances in Unmanned Aerial Vehicles\
    \ Forest Remote Sensing—\nA Systematic Review. Part II: Research Applications.\
    \ Forests 2021, 12, 397. [CrossRef]\n28.\nTorres, P.; Rodes-Blanco, M.; Viana-Soto,\
    \ A.; Nieto, H.; García, M. The Role of Remote Sensing for the Assessment and\
    \ Monitoring\nof Forest Health: A Systematic Evidence Synthesis. Forests 2021,\
    \ 12, 1134. [CrossRef]\n29.\nEugenio, F.C.; PereiradaSilva, S.D.; Fantinel, R.A.;\
    \ de Souza, P.D.; Felippe, B.M.; Romua, C.L.; Elsenbach, E.M. Remotely Piloted\n\
    Aircraft Systems to Identify Pests and Diseases in Forest Species: The Global\
    \ State of the Art and Future Challenges. IEEE Geosci.\nRemote Sens. Mag. 2021,\
    \ 10, 2–15. [CrossRef]\nForests 2022, 13, 911\n27 of 31\n30.\nPage, M.J.; McKenzie,\
    \ J.E.; Bossuyt, P.M.; Boutron, I.; Hoffmann, T.C.; Mulrow, C.D.; Shamseer, L.;\
    \ Tetzlaff, J.M.; Akl, E.A.;\nBrennan, S.E.; et al. The PRISMA 2020 Statement:\
    \ An Updated Guideline for Reporting Systematic Reviews. Syst. Rev. 2021, 10,\n\
    89. [CrossRef]\n31.\nColomina, I.; Molina, P. Unmanned Aerial Systems for Photogrammetry\
    \ and Remote Sensing: A Review. ISPRS J. Photogramm.\nRemote Sens. 2014, 92, 79–97.\
    \ [CrossRef]\n32.\nAria, M.; Cuccurullo, C. Bibliometrix: An R-Tool for Comprehensive\
    \ Science Mapping Analysis. J. Informetr. 2017, 11, 959–975.\n[CrossRef]\n33.\n\
    RStudio Team RStudio: Integrated Development Environment for R; RStudio, PBC:\
    \ Boston, MA, USA, 2021.\n34.\nEskandari, R.; Mahdianpari, M.; Mohammadimanesh,\
    \ F.; Salehi, B.; Brisco, B.; Homayouni, S. Meta-Analysis of Unmanned Aerial\n\
    Vehicle (UAV) Imagery for Agro-Environmental Monitoring Using Machine Learning\
    \ and Statistical Models. Remote Sens. 2020,\n12, 3511. [CrossRef]\n35.\nDash,\
    \ J.P.; Watt, M.S.; Paul, T.S.H.; Morgenroth, J.; Hartley, R. Taking a Closer\
    \ Look at Invasive Alien Plant Research: A Review of\nthe Current State, Opportunities,\
    \ and Future Directions for UAVs. Methods Ecol. Evol. 2019, 10, 2020–2033. [CrossRef]\n\
    36.\nZotero; Center for History and New Media at George Mason University: Fairfax,\
    \ VA, USA, 2022.\n37.\nvan Eck, N.J.; Waltman, L. Software Survey: VOSviewer,\
    \ a Computer Program for Bibliometric Mapping. Scientometrics 2010, 84,\n523–538.\
    \ [CrossRef] [PubMed]\n38.\nLausch, A.; Erasmi, S.; King, D.; Magdon, P.; Heurich,\
    \ M. Understanding Forest Health with Remote Sensing-Part II—A Review\nof Approaches\
    \ and Data Models. Remote Sens. 2017, 9, 129. [CrossRef]\n39.\nCardil, A.; Vepakomma,\
    \ U.; Brotons, L. Assessing Pine Processionary Moth Defoliation Using Unmanned\
    \ Aerial Systems. Forests\n2017, 8, 402. [CrossRef]\n40.\nDash, J.; Pearse, G.;\
    \ Watt, M. UAV Multispectral Imagery Can Complement Satellite Data for Monitoring\
    \ Forest Health. Remote\nSens. 2018, 10, 1216. [CrossRef]\n41.\nSenf, C.; Seidl,\
    \ R.; Hostert, P. Remote Sensing of Forest Insect Disturbances: Current State\
    \ and Future Directions. Int. J. Appl.\nEarth Obs. Geoinf. 2017, 60, 49–60. [CrossRef]\n\
    42.\nAdamopoulos, E.; Rinaudo, F. UAS-Based Archaeological Remote Sensing: Review,\
    \ Meta-Analysis and State-of-the-Art. Drones\n2020, 4, 46. [CrossRef]\n43.\nNäsi,\
    \ R.; Honkavaara, E.; Lyytikäinen-Saarenmaa, P.; Blomqvist, M.; Litkey, P.; Hakala,\
    \ T.; Viljanen, N.; Kantola, T.; Tanhuanpää, T.;\nHolopainen, M. Using UAV-Based\
    \ Photogrammetry and Hyperspectral Imaging for Mapping Bark Beetle Damage at Tree-Level.\n\
    Remote Sens. 2015, 7, 15467–15493. [CrossRef]\n44.\nNäsi, R.; Honkavaara, E.;\
    \ Blomqvist, M.; Lyytikäinen-Saarenmaa, P.; Hakala, T.; Viljanen, N.; Kantola,\
    \ T.; Holopainen, M. Remote\nSensing of Bark Beetle Damage in Urban Forests at\
    \ Individual Tree Level Using a Novel Hyperspectral Camera from UAV and\nAircraft.\
    \ Urban For. Urban Green. 2018, 30, 72–83. [CrossRef]\n45.\nMinaˇrík, R.; Langhammer,\
    \ J. Use of a Multispectral UAV Photogrammetry for Detection and Tracking of Forest\
    \ Disturbance\nDynamics. In The International Archives of the Photogrammetry,\
    \ Remote Sensing & Spatial Information Sciences—ISPRS Archives;\nZdimal, V., Ramasamy,\
    \ S.M., Skidmore, A., Altan, O., Comiso, J., Thenkabail, P.S., Halounova, L.,\
    \ Safar, V., Planka, L., Raju, P.L.N.,\net al., Eds.; International Society for\
    \ Photogrammetry and Remote Sensing: Christian Heipke, Germany, 2016; Volume 41,\
    \ pp.\n711–718.\n46.\nKlouˇcek, T.; Komárek, J.; Surový, P.; Hrach, K.; Janata,\
    \ P.; Vašíˇcek, B. The Use of UAV Mounted Sensors for Precise Detection of\nBark\
    \ Beetle Infestation. Remote Sens. 2019, 11, 1561. [CrossRef]\n47.\nSafonova,\
    \ A.; Tabik, S.; Alcaraz-Segura, D.; Rubtsov, A.; Maglinets, Y.; Herrera, F. Detection\
    \ of Fir Trees (Abies Sibirica) Damaged\nby the Bark Beetle in Unmanned Aerial\
    \ Vehicle Images with Deep Learning. Remote Sens. 2019, 11, 643. [CrossRef]\n\
    48.\nAbdollahnejad, A.; Panagiotidis, D. Tree Species Classiﬁcation and Health\
    \ Status Assessment for a Mixed Broadleaf-Conifer\nForest with Uas Multispectral\
    \ Imaging. Remote Sens. 2020, 12, 3722. [CrossRef]\n49.\nHonkavaara, E.; Näsi,\
    \ R.; Oliveira, R.; Viljanen, N.; Suomalainen, J.; Khoramshahi, E.; Hakala, T.;\
    \ Nevalainen, O.; Markelin,\nL.; Vuorinen, M.; et al. Using Multitemporal Hyper-and\
    \ Multispectral UAV Imaging for Detecting Bark Beetle Infestation on\nNorway Spruce.\
    \ In The International Archives of the Photogrammetry, Remote Sensing and Spatial\
    \ Information Sciences—ISPRS Archives;\nPaparoditis, N., Mallet, C., Lafarge,\
    \ F., Jiang, J., Shaker, A., Zhang, H., Liang, X., Osmanoglu, B., Soergel, U.,\
    \ Honkavaara, E., et al.,\nEds.; International Society for Photogrammetry and\
    \ Remote Sensing: Christian Heipke, Germany, 2020; Volume 43, pp. 429–434.\n50.\n\
    Minaˇrík, R.; Langhammer, J.; Lendzioch, T. Automatic Tree Crown Extraction from\
    \ Uas Multispectral Imagery for the Detection\nof Bark Beetle Disturbance in Mixed\
    \ Forests. Remote Sens. 2020, 12, 4081. [CrossRef]\n51.\nMinaˇrík, R.; Langhammer,\
    \ J.; Lendzioch, T. Detection of Bark Beetle Disturbance at Tree Level Using UAS\
    \ Multispectral Imagery\nand Deep Learning. Remote Sens. 2021, 13, 4768. [CrossRef]\n\
    52.\nSafonova, A.; Hamad, Y.; Dmitriev, E.; Georgiev, G.; Trenkin, V.; Georgieva,\
    \ M.; Dimitrov, S.; Iliev, M. Individual Tree Crown\nDelineation for the Species\
    \ Classiﬁcation and Assessment of Vital Status of Forest Stands from UAV Images.\
    \ Drones 2021, 5, 77.\n[CrossRef]\n53.\nNguyen, H.T.; Caceres, M.L.L.; Moritake,\
    \ K.; Kentsch, S.; Shu, H.; Diez, Y. Individual Sick Fir Tree (Abies mariesii)\
    \ Identiﬁcation in\nInsect Infested Forests by Means of UAV Images and Deep Learning.\
    \ Remote Sens. 2021, 13, 260. [CrossRef]\n54.\nCessna, J.; Alonzo, M.G.; Foster,\
    \ A.C.; Cook, B.D. Mapping Boreal Forest Spruce Beetle Health Status at the Individual\
    \ Crown\nScale Using Fused Spectral and Structural Data. Forests 2021, 12, 1145.\
    \ [CrossRef]\nForests 2022, 13, 911\n28 of 31\n55.\nZhang, N.; Wang, Y.; Zhang,\
    \ X. Extraction of Tree Crowns Damaged by Dendrolimus Tabulaeformis Tsai et Liu\
    \ via Spectral-Spatial\nClassiﬁcation Using UAV-Based Hyperspectral Images. Plant\
    \ Methods 2020, 16, 1–9. [CrossRef]\n56.\nZhang, N.; Zhang, X.; Yang, G.; Zhu,\
    \ C.; Huo, L.; Feng, H. Assessment of Defoliation during the Dendrolimus Tabulaeformis\
    \ Tsai\net Liu Disaster Outbreak Using UAV-Based Hyperspectral Images. Remote\
    \ Sens. Environ. 2018, 217, 323–339. [CrossRef]\n57.\nDuarte, A.; Acevedo-Muñoz,\
    \ L.; Gonçalves, C.I.; Mota, L.; Sarmento, A.; Silva, M.; Fabres, S.; Borralho,\
    \ N.; Valente, C. Detection of\nLonghorned Borer Attack and Assessment in Eucalyptus\
    \ Plantations Using UAV Imagery. Remote Sens. 2020, 12, 3153. [CrossRef]\n58.\n\
    Megat Mohamed Nazir, M.N.; Terhem, R.; Norhisham, A.R.; Mohd Razali, S.; Meder,\
    \ R. Early Monitoring of Health Status of\nPlantation-Grown Eucalyptus Pellita\
    \ at Large Spatial Scale via Visible Spectrum Imaging of Canopy Foliage Using\
    \ Unmanned\nAerial Vehicles. Forests 2021, 12, 1393. [CrossRef]\n59.\nMiraki,\
    \ M.; Sohrabi, H.; Fatehi, P.; Kneubuehler, M. Detection of Mistletoe Infected\
    \ Trees Using UAV High Spatial Resolution\nImages. J. Plant Dis. Prot. 2021, 128,\
    \ 1679–1689. [CrossRef]\n60.\nMaes, W.; Huete, A.; Avino, M.; Boer, M.; Dehaan,\
    \ R.; Pendall, E.; Griebel, A.; Steppe, K. Can UAV-Based Infrared Thermography\n\
    Be Used to Study Plant-Parasite Interactions between Mistletoe and Eucalypt Trees?\
    \ Remote Sens. 2018, 10, 2062. [CrossRef]\n61.\nLehmann, J.R.K.; Nieberding, F.;\
    \ Prinz, T.; Knoth, C. Analysis of Unmanned Aerial System-Based CIR Images in\
    \ Forestry—A New\nPerspective to Monitor Pest Infestation Levels. Forests 2015,\
    \ 6, 594–612. [CrossRef]\n62.\nLin, Q.; Huang, H.; Chen, L.; Wang, J.; Huang,\
    \ K.; Liu, Y. Using the 3D Model RAPID to Invert the Shoot Dieback Ratio of\n\
    Vertically Heterogeneous Yunnan Pine Forests to Detect Beetle Damage. Remote Sens.\
    \ Environ. 2021, 260, 112475. [CrossRef]\n63.\nLin, Q.; Huang, H.; Wang, J.; Huang,\
    \ K.; Liu, Y. Detection of Pine Shoot Beetle (PSB) Stress on Pine Forests at Individual\
    \ Tree\nLevel Using UAV-Based Hyperspectral Imagery and Lidar. Remote Sens. 2019,\
    \ 11, 2540. [CrossRef]\n64.\nLiu, M.; Zhang, Z.; Liu, X.; Yao, J.; Du, T.; Ma,\
    \ Y.; Shi, L. Discriminant Analysis of the Damage Degree Caused by Pine Shoot\n\
    Beetle to Yunnan Pine Using UAV-Based Hyperspectral Images. Forests 2020, 11,\
    \ 1258. [CrossRef]\n65.\nCardil, A.; Otsu, K.; Pla, M.; Silva, C.A.; Brotons,\
    \ L. Quantifying Pine Processionary Moth Defoliation in a Pine-Oak Mixed Forest\n\
    Using Unmanned Aerial Systems and Multispectral Imagery. PLoS ONE 2019, 14, e0213027.\
    \ [CrossRef]\n66.\nOtsu, K.; Pla, M.; Duane, A.; Cardil, A.; Brotons, L. Estimating\
    \ the Threshold of Detection on Tree Crown Defoliation Using\nVegetation Indices\
    \ from Uas Multispectral Imagery. Drones 2019, 3, 80. [CrossRef]\n67.\nOtsu, K.;\
    \ Pla, M.; Vayreda, J.; Brotons, L. Calibrating the Severity of Forest Defoliation\
    \ by Pine Processionary Moth with Landsat\nand UAV Imagery. Sensors 2018, 18,\
    \ 3278. [CrossRef]\n68.\nGuerra-Hernández, J.; Díaz-Varela, R.A.; Ávarez-González,\
    \ J.G.; Rodríguez-González, P.M. Assessing a Novel Modelling\nApproach with High\
    \ Resolution UAV Imagery for Monitoring Health Status in Priority Riparian Forests.\
    \ For. Ecosyst. 2021, 8, 61.\n[CrossRef]\n69.\nPádua, L.; Marques, P.; Martins,\
    \ L.; Sousa, A.; Peres, E.; Sousa, J.J. Monitoring of Chestnut Trees Using Machine\
    \ Learning\nTechniques Applied to UAV-Based Multispectral Data. Remote Sens. 2020,\
    \ 12, 3032. [CrossRef]\n70.\nSandino, J.; Pegg, G.; Gonzalez, F.; Smith, G. Aerial\
    \ Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral\nSensors,\
    \ and Artiﬁcial Intelligence. Sensors 2018, 18, 944. [CrossRef] [PubMed]\n71.\n\
    Dell, M.; Stone, C.; Osborn, J.; Glen, M.; McCoull, C.; Rimbawanto, A.; Tjahyono,\
    \ B.; Mohammed, C. Detection of Necrotic Foliage\nin a Young Eucalyptus Pellita\
    \ Plantation Using Unmanned Aerial Vehicle RGB Photography—A Demonstration of\
    \ Concept. Aust.\nFor. 2019, 82, 79–88. [CrossRef]\n72.\nIordache, M.-D.; Mantas,\
    \ V.; Baltazar, E.; Pauly, K.; Lewyckyj, N. A Machine Learning Approach to Detecting\
    \ Pine Wilt Disease\nUsing Airborne Spectral Imagery. Remote Sens. 2020, 12, 2280.\
    \ [CrossRef]\n73.\nSyifa, M.; Park, S.-J.; Lee, C.-W. Detection of the Pine Wilt\
    \ Disease Tree Candidates for Drone Remote Sensing Using Artiﬁcial\nIntelligence\
    \ Techniques. Engineering 2020, 6, 919–926. [CrossRef]\n74.\nTao, H.; Li, C.;\
    \ Zhao, D.; Deng, S.; Hu, H.; Xu, X.; Jing, W. Deep Learning-Based Dead Pine Tree\
    \ Detection from Unmanned Aerial\nVehicle Images. Int. J. Remote Sens. 2020, 41,\
    \ 8238–8255. [CrossRef]\n75.\nQin, J.; Wang, B.; Wu, Y.; Lu, Q.; Zhu, H. Identifying\
    \ Pine Wood Nematode Disease Using Uav Images and Deep Learning\nAlgorithms. Remote\
    \ Sens. 2021, 13, 162. [CrossRef]\n76.\nWu, B.; Liang, A.; Zhang, H.; Zhu, T.;\
    \ Zou, Z.; Yang, D.; Tang, W.; Li, J.; Su, J. Application of Conventional UAV-Based\
    \ High-\nThroughput Object Detection to the Early Diagnosis of Pine Wilt Disease\
    \ by Deep Learning. For. Ecol. Manag. 2021, 486, 118986.\n[CrossRef]\n77.\nPark,\
    \ H.G.; Yun, J.P.; Kim, M.Y.; Jeong, S.H. Multichannel Object Detection for Detecting\
    \ Suspected Trees with Pine Wilt Disease\nUsing Multispectral Drone Imagery. IEEE\
    \ J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021, 14, 8350–8358. [CrossRef]\n\
    78.\nXia, L.; Zhang, R.; Chen, L.; Li, L.; Yi, T.; Wen, Y.; Ding, C.; Xie, C.\
    \ Evaluation of Deep Learning Segmentation Models for Detection\nof Pine Wilt\
    \ Disease in Unmanned Aerial Vehicle Images. Remote Sens. 2021, 13, 3594. [CrossRef]\n\
    79.\nYu, R.; Luo, Y.; Zhou, Q.; Zhang, X.; Wu, D.; Ren, L. A Machine Learning\
    \ Algorithm to Detect Pine Wilt Disease Using UAV-Based\nHyperspectral Imagery\
    \ and LiDAR Data at the Tree Level. Int. J. Appl. Earth Obs. Geoinf. 2021, 101,\
    \ 102363. [CrossRef]\n80.\nYu, R.; Ren, L.; Luo, Y. Early Detection of Pine Wilt\
    \ Disease in Pinus Tabuliformis in North China Using a Field Portable\nSpectrometer\
    \ and UAV-Based Hyperspectral Imagery. For. Ecosyst. 2021, 8, 44. [CrossRef]\n\
    81.\nYu, R.; Luo, Y.; Li, H.; Yang, L.; Huang, H.; Yu, L.; Ren, L. Three-Dimensional\
    \ Convolutional Neural Network Model for Early\nDetection of Pine Wilt Disease\
    \ Using Uav-Based Hyperspectral Images. Remote Sens. 2021, 13, 4065. [CrossRef]\n\
    Forests 2022, 13, 911\n29 of 31\n82.\nYu, R.; Luo, Y.; Zhou, Q.; Zhang, X.; Wu,\
    \ D.; Ren, L. Early Detection of Pine Wilt Disease Using Deep Learning Algorithms\
    \ and\nUAV-Based Multispectral Imagery. For. Ecol. Manag. 2021, 497, 119493. [CrossRef]\n\
    83.\nSun, Z.; Wang, Y.; Pan, L.; Xie, Y.; Zhang, B.; Liang, R.; Sun, Y. Pine Wilt\
    \ Disease Detection in High-Resolution UAV Images Using\nObject-Oriented Classiﬁcation.\
    \ J. For. Res. 2021, 577. [CrossRef]\n84.\nSmigaj, M.; Gaulton, R.; Barr, S.L.;\
    \ Suárez, J.C. UAV-Borne Thermal Imaging for Forest Health Monitoring: Detection\
    \ Of Disease-\nInduced Canopy Temperature Increase. In International Archives\
    \ of the Photogrammetry, Remote Sensing & Spatial Information\nSciences—ISPRS\
    \ Archives; Paparoditis, N., Raimond, A.-M., Sithole, G., Rabatel, G., Coltekin,\
    \ A., Rottensteiner, F., Briottet, X.,\nChristophe, S., Dowman, I., Elberink,\
    \ S.O., et al., Eds.; International Society for Photogrammetry and Remote Sensing:\
    \ Christian\nHeipke, Germany, 2015; Volume 40, pp. 349–354.\n85.\nSmigaj, M.;\
    \ Gaulton, R.; Suárez, J.C.; Barr, S.L. Canopy Temperature from an Unmanned Aerial\
    \ Vehicle as an Indicator of Tree\nStress Associated with Red Band Needle Blight\
    \ Severity. For. Ecol. Manag. 2019, 433, 699–708. [CrossRef]\n86.\nFraser, B.T.;\
    \ Congalton, R.G. Monitoring Fine-Scale Forest Health Using Unmanned Aerial Systems\
    \ (UAS) Multispectral Models.\nRemote Sens. 2021, 13, 4873. [CrossRef]\n87.\n\
    Briechle, S.; Krzystek, P.; Vosselman, G. Classiﬁcation of Tree Species and Standing\
    \ Dead Trees by Fusing Uav-Based Lidar Data\nand Multispectral Imagery in the\
    \ 3D Deep Neural Network Pointnet++. In ISPRS Annals of the Photogrammetry, Remote\
    \ Sensing\nand Spatial Information Sciences; Paparoditis, N., Mallet, C., Lafarge,\
    \ F., Remondino, F., Toschi, I., Fuse, T., Eds.; Copernicus GmbH:\nGöttingen,\
    \ Germany, 2020; Volume 5, pp. 203–210.\n88.\nElli, E.F.; Sentelhas, P.C.; Bender,\
    \ F.D. Impacts and Uncertainties of Climate Change Projections on Eucalyptus Plantations\n\
    Productivity across Brazil. For. Ecol. Manag. 2020, 474, 118365. [CrossRef]\n\
    89.\nZhang, Y.; Wang, X. Geographical Spatial Distribution and Productivity Dynamic\
    \ Change of Eucalyptus Plantations in China. Sci.\nRep. 2021, 11, 19764. [CrossRef]\n\
    90.\nPotts, B.M.; Vaillancourt, R.E.; Jordan, G.; Dutkowski, G.; Costa e Silva,\
    \ J.; Gay, M.; Steane, D.; Volker, P.; Lopez, G.; Apiolazza,\nL.; et al. Exploration\
    \ of the Eucalyptus Globulus Gene Pool. In Proceedings of the Eucalyptus in a\
    \ Changing World—IUFRO\nConference, Aveiro, Portugal, 1–15 October 2004; Borralho,\
    \ N., Pereira, J.S., Marques, C., Coutinho, J., Madeira, M., Tomé, M.,\nEds.;\
    \ IUFRO: Aveiro, Portugal, 2004; pp. 46–61.\n91.\nCromwell, C.; Giampaolo, J.;\
    \ Hupy, J.; Miller, Z.; Chandrasekaran, A. A Systematic Review of Best Practices\
    \ for UAS Data\nCollection in Forestry-Related Applications. Forests 2021, 12,\
    \ 957. [CrossRef]\n92.\nTmuši´c, G.; Manfreda, S.; Aasen, H.; James, M.R.; Gonçalves,\
    \ G.; Ben-Dor, E.; Brook, A.; Polinova, M.; Arranz, J.J.; Mészáros, J.;\net al.\
    \ Current Practices in UAS-Based Environmental Monitoring. Remote Sens. 2020,\
    \ 12, 1001. [CrossRef]\n93.\nShi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.;\
    \ Rooney, W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan, C.L.S.;\nNeely, H.L.;\
    \ et al. Unmanned Aerial Vehicles for High-Throughput Phenotyping and Agronomic\
    \ Research. PLoS ONE 2016, 11,\ne0159781. [CrossRef] [PubMed]\n94.\nGonzález-Jorge,\
    \ H.; Martínez-Sánchez, J.; Bueno, M.; Arias, P. Unmanned Aerial Systems for Civil\
    \ Applications: A Review. Drones\n2017, 1, 2. [CrossRef]\n95.\nWatts, A.C.; Ambrosia,\
    \ V.G.; Hinkley, E.A. Unmanned Aircraft Systems in Remote Sensing and Scientiﬁc\
    \ Research: Classiﬁcation\nand Considerations of Use. Remote Sens. 2012, 4, 1671–1692.\
    \ [CrossRef]\n96.\nZong, J.; Zhu, B.; Hou, Z.; Yang, X.; Zhai, J. Evaluation and\
    \ Comparison of Hybrid Wing VTOL UAV with Four Different Electric\nPropulsion\
    \ Systems. Aerospace 2021, 8, 256. [CrossRef]\n97.\nMüllerová, J.; Bartaloš, T.;\
    \ Br˚una, J.; Dvoˇrák, P.; Vítková, M. Unmanned Aircraft in Nature Conservation:\
    \ An Example from Plant\nInvasions. Int. J. Remote Sens. 2017, 38, 2177–2198.\
    \ [CrossRef]\n98.\nAssmann, J.J.; Kerby, J.T.; Cunliffe, A.M.; Myers-Smith, I.H.\
    \ Vegetation Monitoring Using Multispectral Sensors—Best Practices\nand Lessons\
    \ Learned from High Latitudes. J. Unmanned Veh. Sys. 2019, 7, 54–75. [CrossRef]\n\
    99.\nPajares, G. Overview and Current Status of Remote Sensing Applications Based\
    \ on Unmanned Aerial Vehicles (UAVs). Pho-\ntogramm. Eng. Remote Sens. 2015, 81,\
    \ 281–330. [CrossRef]\n100. Stuart, M.B.; McGonigle, A.J. Hyperspectral Imaging\
    \ in Environmental Monitoring: A Review of Recent Developments and\nTechnological\
    \ Advances in Compact Field Deployable Systems. Sensors 2019, 19, 3071. [CrossRef]\n\
    101. Stöcker, C.; Bennett, R.; Nex, F.; Gerke, M.; Zevenbergen, J. Review of the\
    \ Current State of UAV Regulations. Remote Sens. 2017, 9,\n459. [CrossRef]\n102.\
    \ EASA. Commission Implementing Regulation (EU) 2019/947 of 24 May 2019 on the\
    \ Rules and Procedures for the Operation of\nUnmanned Aircraft. Available online:\
    \ https://eur-lex.europa.eu/eli/reg_impl/2019/947/oj (accessed on 5 March 2021).\n\
    103. Iglhaut, J.; Cabo, C.; Puliti, S.; Piermattei, L.; O’Connor, J.; Rosette,\
    \ J. Structure from Motion Photogrammetry in Forestry:\nA Review. Curr. For. Rep.\
    \ 2019, 5, 155–168. [CrossRef]\n104. Whitehead, K.; Hugenholtz, C.H. Applying\
    \ ASPRS Accuracy Standards to Surveys from Small Unmanned Aircraft Systems\n(UAS).\
    \ Photogramm. Eng. Remote Sens. 2015, 81, 787–793. [CrossRef]\n105. Barbedo, J.\
    \ A Review on the Use of Unmanned Aerial Vehicles and Imaging Sensors for Monitoring\
    \ and Assessing Plant Stresses.\nDrones 2019, 3, 40. [CrossRef]\n106. Ke, Y.;\
    \ Quackenbush, L.J. A Review of Methods for Automatic Individual Tree-Crown Detection\
    \ and Delineation from Passive\nRemote Sensing. Int. J. Remote Sens. 2011, 32,\
    \ 4725–4747. [CrossRef]\nForests 2022, 13, 911\n30 of 31\n107. Koch, B.; Kattenborn,\
    \ T.; Straub, C.; Vauhkonen, J. Segmentation of Forest to Tree Objects. In Forestry\
    \ Application of Airborne\nLaser Scanning: Concept and Case Studies; Maltamo,\
    \ M., Naesset, E., Vauhkonen, J., Eds.; Springer Netherlands: Dordrecht,\nThe\
    \ Netherlands, 2014; pp. 89–112. ISBN 94-017-8662-3.\n108. Wang, L.; Gong, P.;\
    \ Biging, G.S. Individual Tree-Crown Delineation and Treetop Detection in High-Spatial-Resolution\
    \ Aerial\nImagery. Photogramm. Eng. Remote Sens. 2004, 70, 351–357. [CrossRef]\n\
    109. Zhen, Z.; Quackenbush, L.; Zhang, L. Trends in Automatic Individual Tree\
    \ Crown Detection and Delineation—Evolution of\nLiDAR Data. Remote Sens. 2016,\
    \ 8, 333. [CrossRef]\n110. Mohan, M.; Silva, C.; Klauberg, C.; Jat, P.; Catts,\
    \ G.; Cardil, A.; Hudak, A.; Dia, M. Individual Tree Detection from Unmanned\n\
    Aerial Vehicle (UAV) Derived Canopy Height Model in an Open Canopy Mixed Conifer\
    \ Forest. Forests 2017, 8, 340. [CrossRef]\n111. Dalponte, M.; Reyes, F.; Kandare,\
    \ K.; Gianelle, D. Delineation of Individual Tree Crowns from ALS and Hyperspectral\
    \ Data:\nA Comparison among Four Methods. Eur. J. Remote Sens. 2015, 48, 365–382.\
    \ [CrossRef]\n112. Dalponte, M.; Coomes, D.A. Tree-centric Mapping of Forest Carbon\
    \ Density from Airborne Laser Scanning and Hyperspectral\nData. Methods Ecol.\
    \ Evol. 2016, 7, 1236–1245. [CrossRef]\n113. Vincent, L.; Soille, P. Watersheds\
    \ in Digital Spaces: An Efﬁcient Algorithm Based on Immersion Simulations. IEEE\
    \ Trans. Pattern\nAnal. Mach. Intell. 1991, 13, 583–598. [CrossRef]\n114. Gu,\
    \ J.; Grybas, H.; Congalton, R.G. Individual Tree Crown Delineation from UAS Imagery\
    \ Based on Region Growing and Growth\nSpace Considerations. Remote Sens. 2020,\
    \ 12, 2363. [CrossRef]\n115. Hyyppa, J.; Kelle, O.; Lehikoinen, M.; Inkinen, M.\
    \ A Segmentation-Based Method to Retrieve Stem Volume Estimates from 3-D\nTree\
    \ Height Models Produced by Laser Scanners. IEEE Trans. Geosci. Remote Sens. 2001,\
    \ 39, 969–975. [CrossRef]\n116. Li, W.; Guo, Q.; Jakubowski, M.K.; Kelly, M. A\
    \ New Method for Segmenting Individual Trees from the Lidar Point Cloud.\nPhotogramm.\
    \ Eng. Remote Sens. 2012, 78, 75–84. [CrossRef]\n117. Reitberger, J.; Schnörr,\
    \ C.; Krzystek, P.; Stilla, U. 3D Segmentation of Single Trees Exploiting Full\
    \ Waveform LIDAR Data. ISPRS J.\nPhotogramm. Remote Sens. 2009, 64, 561–574. [CrossRef]\n\
    118. Hyyppä, J.; Hyyppä, H.; Leckie, D.; Gougeon, F.; Yu, X.; Maltamo, M. Review\
    \ of Methods of Small-footprint Airborne Laser\nScanning for Extracting Forest\
    \ Inventory Data in Boreal Forests. Int. J. Remote Sens. 2008, 29, 1339–1366.\
    \ [CrossRef]\n119. Zhou, Y.; Zhang, R.; Wang, S.; Wang, F. Feature Selection Method\
    \ Based on High-Resolution Remote Sensing Images and the\nEffect of Sensitive\
    \ Features on Classiﬁcation Accuracy. Sensors 2018, 18, 2013. [CrossRef]\n120.\
    \ Lu, D.; Weng, Q. A Survey of Image Classiﬁcation Methods and Techniques for\
    \ Improving Classiﬁcation Performance. Int. J.\nRemote Sens. 2007, 28, 823–870.\
    \ [CrossRef]\n121. Sowmya, A.; Trinder, J. Modelling and Representation Issues\
    \ in Automated Feature Extraction from Aerial and Satellite Images.\nISPRS J.\
    \ Photogramm. Remote Sens. 2000, 55, 34–47. [CrossRef]\n122. Fotso Kamga, G.A.;\
    \ Bitjoka, L.; Akram, T.; Mengue Mbom, A.; Rameez Naqvi, S.; Bouroubi, Y. Advancements\
    \ in Satellite Image\nClassiﬁcation: Methodologies, Techniques, Approaches and\
    \ Applications. Int. J. Remote Sens. 2021, 42, 7662–7722. [CrossRef]\n123. Oumar,\
    \ Z.; Mutanga, O.; Ismail, R. Predicting Thaumastocoris Peregrinus Damage Using\
    \ Narrow Band Normalized Indices and\nHyperspectral Indices Using Field Spectra\
    \ Resampled to the Hyperion Sensor. Int. J. Appl. Earth Obs. Geoinf. 2013, 21,\
    \ 113–121.\n[CrossRef]\n124. Ma, L.; Fu, T.; Blaschke, T.; Li, M.; Tiede, D.;\
    \ Zhou, Z.; Ma, X.; Chen, D. Evaluation of Feature Selection Methods for Object-Based\n\
    Land Cover Mapping of Unmanned Aerial Vehicle Imagery Using Random Forest and\
    \ Support Vector Machine Classiﬁers. Int. J.\nGeo-Inf. 2017, 6, 51. [CrossRef]\n\
    125. Ma, L.; Cheng, L.; Li, M.; Liu, Y.; Ma, X. Training Set Size, Scale, and\
    \ Features in Geographic Object-Based Image Analysis of Very\nHigh Resolution\
    \ Unmanned Aerial Vehicle Imagery. ISPRS J. Photogramm. Remote Sens. 2015, 102,\
    \ 14–27. [CrossRef]\n126. Ma, L.; Li, M.; Ma, X.; Cheng, L.; Du, P.; Liu, Y. A\
    \ Review of Supervised Object-Based Land-Cover Image Classiﬁcation. ISPRS J.\n\
    Photogramm. Remote Sens. 2017, 130, 277–293. [CrossRef]\n127. Ma, L.; Liu, Y.;\
    \ Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep Learning in Remote Sensing Applications:\
    \ A Meta-Analysis and\nReview. ISPRS J. Photogramm. Remote Sens. 2019, 152, 166–177.\
    \ [CrossRef]\n128. Osco, L.P.; Marcato Junior, J.; Marques Ramos, A.P.; de Castro\
    \ Jorge, L.A.; Fatholahi, S.N.; de Andrade Silva, J.; Matsubara, E.T.;\nPistori,\
    \ H.; Gonçalves, W.N.; Li, J. A Review on Deep Learning in UAV Remote Sensing.\
    \ Int. J. Appl. Earth Obs. Geoinf. 2021, 102,\n102456. [CrossRef]\n129. Nasiri,\
    \ V.; Darvishsefat, A.A.; Areﬁ, H.; Pierrot-Deseilligny, M.; Namiranian, M.; Le\
    \ Bris, A. Unmanned Aerial Vehicles (UAV)-\nBased Canopy Height Modeling under\
    \ Leaf-on and Leaf-off Conditions for Determining Tree Height and Crown Diameter\
    \ (Case\nStudy: Hyrcanian Mixed Forest). Can. J. For. Res. 2021, 51, 962–971.\
    \ [CrossRef]\n130. Puente, C.; Olague, G.; Smith, S.V.; Bullock, S.H.; Hinojosa-Corona,\
    \ A.; González-Botello, M.A. A Genetic Programming Approach\nto Estimate Vegetation\
    \ Cover in the Context of Soil Erosion Assessment. Photogramm. Eng. Remote Sens.\
    \ 2011, 77, 363–376.\n[CrossRef]\n131. Batista, J.E.; Cabral, A.I.R.; Vasconcelos,\
    \ M.J.P.; Vanneschi, L.; Silva, S. Improving Land Cover Classiﬁcation Using Genetic\n\
    Programming for Feature Construction. Remote Sens. 2021, 13, 1623. [CrossRef]\n\
    Forests 2022, 13, 911\n31 of 31\n132. Batista, J.E.; Silva, S. Improving the Detection\
    \ of Burnt Areas in Remote Sensing Using Hyper-Features Evolved by M3GP.\nIn Proceedings\
    \ of the 2020 IEEE Congress on Evolutionary Computation (CEC), Glasgow, UK, 19–24\
    \ July 2020.\n133. Mejia-Zuluaga, P.A.; Dozal, L.; Valdiviezo-N, J.C. Genetic\
    \ Programming Approach for the Detection of Mistletoe Based on UAV\nMultispectral\
    \ Imagery in the Conservation Area of Mexico City. Remote Sens. 2022, 14, 801.\
    \ [CrossRef]\n"
  inline_citation: '>'
  journal: Forests
  limitations: '>'
  pdf_link: https://www.mdpi.com/1999-4907/13/6/911/pdf?version=1654873793
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Recent Advances in Forest Insect Pests and Diseases Monitoring Using UAV-Based
    Data: A Systematic Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11119-019-09666-6
  analysis: '>'
  authors:
  - Shaun M. Sharpe
  - Arnold W. Schumann
  - Jialin Yu
  - Nathan S. Boyd
  citation_count: 39
  full_citation: '>'
  full_text: '>

    Your privacy, your choice We use essential cookies to make sure the site can function.
    We also use optional cookies for advertising, personalisation of content, usage
    analysis, and social media. By accepting optional cookies, you consent to the
    processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Precision Agriculture Article Vegetation
    detection and discrimination within vegetable plasticulture row-middles using
    a convolutional neural network Published: 08 May 2019 Volume 21, pages 264–277,
    (2020) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Precision Agriculture Aims and scope Submit manuscript Shaun M. Sharpe , Arnold
    W. Schumann, Jialin Yu & Nathan S. Boyd  1223 Accesses 36 Citations Explore all
    metrics Abstract Weed control between plastic covered, raised beds in Florida
    vegetable crops relies predominantly on herbicides. Broadcast applications of
    post-emergence herbicides are unnecessary due to the general patchy distribution
    of weed populations. Development of precision herbicide sprayers to apply herbicides
    where weeds occur would result in input reductions. The objective of the study
    was to test a state-of-the-art object detection convolutional neural network,
    You Only Look Once 3 (YOLOV3), to detect vegetation both indiscriminately (1-class
    network) and to detect and discriminate three classes of vegetation commonly found
    within Florida vegetable plasticulture row-middles (3-class network). Vegetation
    was discriminated into three categories: broadleaves, sedges and grasses. The
    3-class network (Fscore = 0.95) outperformed the 1-class network (Fscore = 0.93)
    in overall vegetation detection. The increase in target variability when combining
    classes increased and potentially negated benefits from pooling classes into a
    single target (and increasing the available data per class). The 3-class network
    Fscores for grasses, sedges and broadleaves were 0.96, 0.96 and 0.93 respectively.
    Recall was the limiting factor for all classes. With consideration to how much
    of the plant was identified (broadleaves and grasses), the 3-class network (Fscore = 0.93)
    outperformed the 1-class network (Fscore = 0.79). The 1-class network struggled
    to detect grassy weed species (recall = 0.59). Use of YOLOV3 as an object detector
    for discrimination of vegetation classes is a feasible option for incorporation
    into precision applicators. Similar content being viewed by others A deep learning-based
    framework for object recognition in ecological environments with dense focal loss
    and occlusion Article 07 March 2024 Convolutional Neural Networks for Planting
    System Detection of Olive Groves Chapter © 2023 A Cascaded Deep Learning Approach
    for Detection and Localization of Crop-Weeds in RGB Images Chapter © 2024 Introduction
    Fresh-market vegetable production is an economically important Florida agricultural
    sector. In 2017, the combined production value for bell peppers (Capsicum annum
    L.), strawberries [Fragaria × ananassa (Weston) Duchesne ex Rozier (pro sp.) [chiloensis × virginiana]],
    tomatoes (Solanum lycopersicum L.) and cucurbits (Cucurbitaceae) including watermelon
    (Citrullus lanatus (Thunb.) Matsum. and Nakai), cucumber (Cucumis sativus L.)
    and cantaloupe (Cucumis melo var. cantalupo Ser.) was $1087 million (USDA 2018a).
    Production of various crops in Florida is ongoing nearly year-round (from August
    to June) and predominately utilizes a plasticulture system. Weed management is
    challenging due to the limited number of registered herbicides and difficulty
    in utilizing cultivation during production (Boranno 1996). Except for purple (Cyperus
    rotundus L.) and yellow nutsedge (Cyperus esculentus L.), weed emergence is limited
    to planting-holes on the top of the bed and between the rows (row-middles). While
    weeds occurring in row-middles may have limited impact on yield (Gilreath and
    Santos 2004), they can harbor diseases (Freeman et al. 2001), insects (Bedford
    et al. 1998) and nematodes (Townshend and Davidson 1960) as well as interfere
    with harvest and end-of-season plastic removal. Weeds left uncontrolled also contribute
    to the seed bank which may interfere in subsequent production cycles. Herbicide
    applications in row-middles generally consist of mixtures of pre- and post-herbicides
    applied shortly after fumigation, followed by successive pre- or post-herbicides
    (or both) during the production cycle. Successive herbicide applications risk
    drift injury to the crop and if the mode-of-action is not rotated, risks herbicide
    resistance. Florida tomato production already has documented cases of paraquat-resistant
    goosegrass [Eleusine indica (L.) Gaertn.] (Buker et al. 2002) and American black
    nightshade (Solanum americanum Mill.) (Bewick et al. 1990). Herbicides are generally
    applied broadcast throughout fields, even though weed distributions are generally
    patchy. Reducing herbicide inputs through spot-spraying post-herbicides with precision
    application technology may reduce production costs. This may then reduce the risk
    for crop injury, which is a major obstacle for grower adoption of post-herbicides.
    Precision technology for spot-application of herbicides predominantly relies on
    machine vision-linked detectors for autonomous weed control applications (Fennimore
    et al. 2016). The most common sensor technologies are multispectral cameras (Vrindts
    et al. 2002), hyperspectral cameras (Zhang et al. 2012) and RGB cameras (dos Santos
    Ferreira et al. 2017, Sharpe et al. 2018). Consumer RGB cameras are a viable low-cost
    alternative sensor compared to hyperspectral technology (Fennimore et al. 2016).
    Deep learning convolutional neural networks (CNN) are robust networks which receive
    digital images and rely on pattern recognition to classify objects. They may use
    characteristics such as leaf vein morphology (Grinblat et al. 2016). Networks
    such as You Only Look Once (YOLO) have the potential to detect many classes (Redmon
    et al. 2016) which is an important network selection criterion for crop scouting
    and smart spraying applications in agriculture. Neural networks are inspired by
    the visual cortex and composed of several layers including those for feature extraction,
    convolution, pooling, non-linear activation functions and class label assignment
    (Ball et al. 2017) which have been described elsewhere in detail (Dyrmann et al.
    2016, Schmidhuber 2015). CNNs must be trained to refine their many parameters
    to produce the desired output (Schmidhuber 2015). Supervised learning is the most
    common form of machine learning, where large numbers of samples are compiled and
    the target identified and labeled (Lecun et al. 2015). Object detection-based
    CNNs have been applied to detect weeds within digital images from wheat (Triticum
    aestivum L.) (Dyrmann et al. 2017, 2018) and strawberry cropping systems (Sharpe
    et al. 2018). A segmentation-based CNN has been used to discriminate broadleaf
    and grass weeds from soybean [Glycine max (L.) Merr.] and bare-ground (dos Santos
    Ferreira et al. 2017). Weeds and sugar beet (Beta vulgaris L.) were discriminated
    on bare-ground using segmentation-CNNs with normalized difference vegetation index
    (NDVI) transformations on near-infrared and red–green–blue images (Milioto et
    al. 2017). Object detection CNNs also benefit from an ease of annotation and the
    ability to distinguish instances. Since the end application is for smart spraying
    technology, a network which prioritized inference time to accuracy was selected.
    This was primarily due to the need for rapid, in situ inference from a video camera
    near the nozzles. Therefore, the objective of the study was to test an inference
    time prioritized state-of-the-art object detection CNN to indiscriminately detect
    vegetation as well as detect and discriminate three weed classes in row-middles.
    Materials and methods Images of vegetation within Florida vegetable production
    row-middles were acquired using a digital single-lens reflex camera (D3400 camera
    with an AF-P DX NIKKOR 18–55 mm f3.5-5.6 G VR lens, Nikon Inc., Melville, NY,
    USA). Images were acquired at the Gulf Coast Research and Education Center (GCREC)
    at Balm, FL, USA (27.76 ºN, 82.22 ºW) in 2018. Image resolution was 6000 × 4000
    pixels. Camera height was 1.30 m from the soil surface. Two fields at GCREC in
    tomato production were used to collect training images, approximately 1-km apart.
    Fields were transplanted on March 7, 2018. Images were acquired at 20, 21, 22,
    30, 33, 37 and 40 days after transplanting (DATr). The soil type was a Seffner
    fine sand at site 1 and Malabar fine sand at site 2 (USDA 2018b). Species present
    included purple nutsedge (Cyperus rotundus L.), yellow nutsedge (Cyperus esculentus
    L.), goosegrass [Eleusine indica (L.) Gaertn.], smooth crabgrass [Digitaria ischaemum
    (Schreb.) Schreb. Ex Muhl.], bermudagrass [Cynodon dactylon (L.) Pers], carpetweed
    (Mollugo verticillate L.), livid amaranth [Amaranthus blitum L. var. emarginatus
    (Salzm. ex Uline & W.L. Bray) Lambinon], lambsquarters (Chenopodium album L.)
    and tievine (Ipomoea cordatotriloba Dennst). The compiled training dataset was
    1798 images. Validation images were taken from three additional fields at GCREC,
    two in tomato production and one in strawberry production. Soil type in the tomato
    fields varied from a Myakka fine sand to a Smyrna fine sand, whereas the strawberry
    field was a Zolfo fine sand (USDA 2018b). The first tomato field was transplanted
    on March 7, 2018 and images were acquired on 22 and 43 DATr. Species included
    C. rotundus, C. esculentus, E. indica, D. ischaemum, C. dactylon, A. blitum and
    tomato. Due to low broadleaf species populations emerging in the row middles of
    the validation site, a second validation site was selected, focusing on broadleaf
    species. These images were acquired on August 27, 2018, from the uncultivated
    borders of a strawberry field, 7 days after fumigation. Broadleaf weeds include
    Brazil pusley (Richardia brasiliensis Gomes), eclipta [Eclipta prostrata (L.)
    L], wild radish (Raphanus raphanistrum L.), M. verticillata and common ragweed
    (Ambrosia artemisiifolia L.). The compiled validation dataset contained 307 images
    within which were 325 individual grass plants, 792 sedges and 462 broadleaves.
    Images were resized 1280 × 853 then cropped to 1280 × 720 pixels (720p) using
    IrfanView (Version 4.50, Irfan Skijan, Jajce, Bosnia). This resolution was selected
    to later incorporate the neural networks into developed precision sprayer technology
    using 720p video as an input. The ground-sampling distance was 0.5 mm pixel−1.
    Labels for classification targets within each image were produced using custom
    software compiled with Lazarus (https://www.lazarus-ide.org/). Previous research
    demonstrated that smaller labels on the most prevalent, visible part of the plant
    were more effective than labeling the presence of whole plants (Sharpe et al.
    2018) and a similar approach was undertaken. Two sets of labels were produced,
    one per network. The first network was trained with only 1 class (1-class network)
    to indiscriminately detect green vegetation. All vegetation was labeled under
    a single category. Broad-spectrum non-selective herbicides including glyphosate,
    paraquat or carfentrazone are available in certain use patterns for control of
    most weeds within the crop production cycle. A second network was trained to detect
    three classes of weed (3-class network). Label designations were chosen based
    on available registered herbicides for weed control in row-middles. Weed Science
    Society of America (WSSA) group 2 herbicides such as halosulfuron or imazosulfuron
    for nutsedges (Cyperaceae), group 1 herbicides such as clethodim or sethoxydim
    for grasses (Poaceae) and group 14 herbicides including carfentrazone and lactofen
    for broadleaves (Eudicots). Typical bounding box labeling resembles the output
    demonstrated in Figs. 1, 2, 3, 4. Any presence of a weed in the image was labeled,
    even if it was not completely in the image. The presence of tomato leaves within
    row-middles was classified as broadleaves. The total number of labels per class
    in the training dataset was 747 for grasses, 622 for sedges and 735 for broadleaves.
    Annotation for nutsedges was entirely composed of a single bounding box over the
    central triangular stem only. For the purpose of evaluation, it is assumed that
    each stem is an isolated plant. Admittedly, this is likely untrue for later timings
    after rhizome expansion, but evaluation of interconnected shoots is impossible
    without disrupting the soil and destructive harvest. Annotation for broadleaves
    and grasses involved labeling entire plants for seedlings (up to approximately
    the 5-leaf stage) and then labeling leaf clusters on larger plants to maintain
    use of smaller labels. Fig. 1 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting grasses at Balm, FL, USA in 2018. Images demonstrate: a the original
    input image, b YOLOV3 trained to detect all vegetation indiscriminately and c
    YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses and
    sedges Full size image Fig. 2 Object detection of vegetation in Florida vegetable
    row middles at Balm, FL, USA in 2018 using the YOLOV3 convolutional neural network,
    highlighting limits to detection by the 1-class network. Images demonstrate: a
    the original input image, b YOLOV3 trained to detect all vegetation indiscriminately
    and c YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses
    and sedges Full size image Fig. 3 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting sedges at Balm, FL, USA in 2018. Images demonstrate: a the original
    input image, b YOLOV3 trained to detect all vegetation indiscriminately and c
    YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses and
    sedges Full size image Fig. 4 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting broadleaves at Balm, FL, USA in 2018. Images demonstrate: a the
    original input image, b YOLOV3 trained to detect all vegetation indiscriminately
    and c YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses
    and sedges Full size image The neural network selected was You Only Look Once
    Version 3 (YOLOV3) (Redmon and Farhadi 2018). YOLOV3 is a state-of-the-art network
    object detection convolutional neural network with regards to inference time while
    sacrificing accuracy. Inference time was determined a priority since the end application,
    incorporation into a precision sprayer, requires rapid, in situ processing from
    a video camera near the nozzles. Class prediction is multi-labeled, using independent
    logistic classifiers (Redmon and Farhadi 2018). This may be beneficial for weeds
    growing in very close proximity or intertwined, to occupy a single bounding box.
    YOLOV3 was trained and tested using the Darknet neural network framework (Redmon
    2016) and pretrained using the COCO dataset (Lin et al. 2014). YOLOV3 contained
    data augmentation parameters for altering the training set images including color
    alteration (saturation, exposure, hue), cropping, resizing and flipping. Augmentation
    parameters were used in network training to reduce overtraining on irrelevant
    features by increasing the variability of input images. Network training continued
    until either an increase in validation accuracy (recall or precision) ceased or
    the average loss error ceased to decrease. Training relied on the intersection
    of union (IoU) between ground truth labels and predicted bounding-boxes. While
    IoU works well for training, due to label selection (smaller parts of the plant),
    the ground truth identity of labels was no longer static. Therefore, validation
    results were evaluated by visually identifying ground truth vegetation using an
    IoU > 0, based on two criteria related to the network application of precision
    spraying. The first criterium was the ability of the network to detect any part
    of an individual plant, ignoring all subsequent detections. The underlying interest
    for the first criterion was to see if the precision sprayer could detect any part
    of the target weed to turn on the sprayer. The second criterion was the degree
    to which the network detected all visible vegetation. This criterion was to quantify
    the extent of the network’s ability to detect all of the visible vegetation within
    each image. While the IoU threshold is low, the bounding box size was also small
    and it is assumed that this minimized the amount of negative space on the image
    classified as the target. Network classification output for both the 1- and 3-class
    networks were pooled according to binary classification categories: true positive
    (tp), false positives (fp) and false negatives (fn). A tp is when the neural network
    correctly identifies the target. A fp is when the neural network falsely identifies
    a target, for example calling something a sedge that isn’t a sedge. A fn is when
    the neural network fails to identify a target. While true negative (tn) does complete
    the confusion matrix, this category is not a focal priority for the current application
    (implementation within precision spray application technology). Three measures
    were used to evaluate YOLOV3''s effectiveness at identifying targets: precision,
    recall and Fscore (Sokolova and Lapalme 2009). Precision is a measure of a network’s
    ability to accurately identify targets, calculated by: $$Precision = \frac{tp}{tp
    + fp}$$ (1) (Hoiem et al. 2012, Sokolova and Lapalme 2009). Recall is a measure
    of the network’s ability to detect its target, calculated by: $$Recall = \frac{tp}{tp
    + fn}$$ (2) (Hoiem et al. 2012, Sokolova and Lapalme 2009). The Fscore is the
    harmonic mean of the precision and recall and gives an overall measure of the
    network’s classifications, calculated by: $$Fscore = \frac{2*Precision*Recall}{Precision
    + Recall}$$ (3) (Sokolova and Lapalme 2009). Validation results for both 1- and
    3-class networks were considered in two ways. The first was if any part of the
    plant was identified by the network, indicating that the hypothetical sprayer
    would turn on at the detection. In such scenarios, a tp was assigned to the plant
    if any part of it was identified by the network. The second consideration was
    how much of the plant was identified, akin to percent coverage calculations for
    herbicide coverage. In this case, if a part of either the broadleaf or the grass
    species were not identified by the network, it was considered a miss. As such,
    each plant may have one to several boxes depending on plant size. Sedges were
    not graded in this fashion due to networks being trained to identify their presence
    by the characteristic 3-sided stem, as well as their lack of branching or prostrate
    growth habit. Although the 1-class network was trained to detect vegetation indiscriminately,
    results for tp and fp were scored based on the classes for the 3-class model for
    comparison. Since the 1-class network did not distinguish classes for fp, all
    fp were pooled and used to calculate precision. The default threshold setting
    for image validation was used (0.25). Results and discussion Both the 1- and 3-class
    YOLOV3 networks converged to provide acceptable target detection. Training YOLOV3
    to detect 3 classes of vegetation was more successful than training the network
    to detect vegetation indiscriminately (1-class) (Table 1). This was unexpected
    due to the larger number of within-class objects available to train a 1-class
    network. Targeting all vegetation increased the variability in the target, which
    likely negated the beneficial increase in available data. Table 1 YOLOV3 convolutional
    neural network object detection training results for networks trained to detect
    all vegetation indiscriminately (1-class) or to discriminate between broadleaves,
    sedges and grasses (3-class) at Balm, FL, USA in 2018 Full size table For overall
    total detection across all classes, the 3-class network outperformed the 1-class
    network at detecting individual plants (Table 2). Direct comparisons with previous
    work are difficult due to target variability, environmental variability, the infrastructure
    of the CNN and the number of available training images. Even so, both networks
    evaluated in this present study achieved higher overall precision and recall than
    object detection CNNs detecting weeds in a wheat field including SSD512 (precision = 0.82,
    recall = 0.60) (Dyrmann et al. 2018) and DetectNet (precision = 0.87, recall = 0.46)
    (Dyrmann et al. 2017). Results were slightly better than using DetectNet to detect
    Carolina geranium (Geranium carolinianum L.) leaves in a strawberry [Fragaria × ananassa
    (Weston) Duchesne ex Rozier (pro sp.) [chiloensis × virginiana]] canopy, when
    targeting individual leaves (Fscore = 0.94) (Sharpe et al. 2018). This is a promising
    result for utilizing object detection to target multiple weeds in situ using RGB
    camera-based sensors within precision herbicide application technology. Very high
    precision and recall (0.99) have been demonstrated using segmentation-based processing
    prior to feature extraction to detect weeds in soybeans (dos Santos Ferreira et
    al. 2017). Using preprocessing with such algorithms may be a way to improve detection
    if incorporation of such algorithms does not substantially impact the inference
    time of the precision sprayer. Table 2 Object detection accuracy for YOLOV3 convolutional
    neural network with consideration to the detection of individual broadleaf, grass
    and sedge plants growing in plasticulture row-middles at Balm, FL, USA in 2018
    Full size table Reductions in overall Fscore for both models primarily came from
    the influence of recall, specifically fn designations of broadleaves for the 3-class
    network and both broadleaves and grasses for the 1-class network. Precision was
    also reduced for the 1-class network, but that was a result of the over-compensation
    in fp attributed to all classes (Table 2). The overall precision of the network
    is close to the 3-class network. The grass classification demonstrated the most
    variability in what was being designated as the target (Figs. 1, 2). Eleusine
    indica, D. ischaemum and C. dactylon have very different growth habits and leaf
    morphologies which impacted the decision of what to identify as the repeatable
    unit. The network’s difficulty in grass detection and classification was demonstrated
    when considering the extent of detection across the entire plant habit (Figs.
    1, 2). Recall (Eq. 2) as considered gives an indication of coverage. The 1-class
    network struggled to detect grasses, only detecting approximately 59% of the visible
    vegetation. This likely comes as a consequence of grass leaf morphology and utilizing
    bounding boxes, which results in much background noise, even in smaller boxes.
    Focusing nutsedge annotation for training exclusively on the triangular stem (Fig.
    3) was quite successful, resulting in the highest inter-class Fscores for both
    networks (Table 2). The triangular shaped stem was consistent across both C. rotundus
    and R. esculentus. Issues with classification generally came when corms were young,
    resulting in thin leaves and a faint triangular stem outline against a more dominant
    background. Another, though less common misclassification case was when sand would
    bury emerged stems, leaving only leaves visible. Fitness and the possibility of
    survival of these corms were comparatively low. The 3-class network was very successful
    at identifying broadleaf plants (high precision) though the network struggled
    slightly with detecting them (moderate recall) (Table 2). Broadleaves were over
    half the listed common species in strawberry, cucurbit and fruiting vegetable
    production (Webster 2014). This was consistent with field observations in row
    middles for the current study. Considering that recall remained considerably high
    during validation for several weed species not present in the training set speaks
    to the robustness of the network (Fig. 4). These species include R. brasiliensis,
    E. prostrata, R. raphanistrum and A. artemissifolia. With regards to how much
    of the plant material was detected, the 3-class network was consistent for both
    broadleaf and grasses, demonstrating high recall (≥ 0.86) (Table 3) missing only
    12–14% of the plant material. The 1-class network retained a similar degree of
    detection for broadleaves as the 3-class network (Fig. 4). This deficit could
    be corrected using larger spray areas, both before and after the target when a
    target is detected but requires field testing. Table 3 Object detection accuracy
    for YOLOV3 convolutional neural network on broadleaf and grass species with consideration
    towards the extent of vegetation detection in vegetable row-middles in Balm, FL,
    USA in 2018 Full size table Overall, results demonstrate promising object detection
    in YOLOV3 for applications in vegetable row-middles. The 1-class network had difficulty
    detecting grasses. Additional photos will likely lead to increased recall and
    increase the coverage in which machine vision—controlled sprayers would apply
    post-herbicides. The 3-class network permits precision sprayers to potentially
    apply four herbicide application scenarios with a single network. These are: (1)
    application of group 1 herbicides such as clethodim or sethoxydim to grasses,
    (2) application of group 14 herbicides such as carfentrazone or lactofen to broadleaves,
    (3) application of group 2 herbicides such as halosulfuron or imazosulfuron to
    nutsedges and (4) application of the group 22 herbicides such as diquat and paraquat
    to all classes. Successful development of this spray technology will permit application
    of four modes of action to various targets, including sequential real-time applications
    from a bank of nozzles, while only passing through the field a single time. While
    this approach may not influence all herbicides used across all vegetable plasticulture
    crops (such as clomazone), it does impact the widely used post-row-middle herbicides.
    This does not account for the size or growth stage of the target. Future research
    may focus on CNNs which count the number of leaves, which have been previously
    demonstrated (Teimouri et al. 2018). This would permit sprayers to only spray
    known susceptible sizes of target weeds to reduce the risk of herbicide resistance.
    Upon field implementation, for broad-spectrum herbicides such as paraquat, the
    network could be compared to sensors such as WeedSeeker (Trimble Inc., Sunnyvale,
    CA, USA). Conclusions The 1-class YOLOV3 performed well at identifying any vegetation
    in row-middles during validation (Fscore = 0.93). Dividing the vegetation into
    relevant classes (broadleaves, sedges and grasses) for the 3-class network further
    increased the Fscore (0.95). Compared to other classes of interest, the 1-class
    network had difficulty in detecting grasses. This problem was overcome with the
    3-class network, relative to other classes. The developed YOLOV3 3-class network
    demonstrated acceptable detection levels to proceed with incorporation into smart
    spraying technology for field evaluation. References Ball, J. E., Anderson, D.
    T., & Chan, C. S. (2017). A comprehensive survey of deep learning in remote sensing:
    theories, tools and challenges for the community. Journal of Applied Remote Sensing,11(4),
    1–54. https://doi.org/10.1117/1.JRS.11.042601. Article   Google Scholar   Bedford,
    I. D., Kelly, A., Banks, G. K., Briddon, R. W., Cenis, J. L., & Markham, P. G.
    (1998). Solanum nigrum: An indigenous weed reservoir for a tomato yellow leaf
    curl geminivirus in southern Spain. European Journal of Plant Pathology,104, 221–222.
    https://doi.org/10.1023/A:1008627419450. Article   Google Scholar   Bewick, T.
    A., Kostewicz, S. R., Stall, W. M., Shilling, D. G., & Smith, K. (1990). Interaction
    of cupric hydroxide, paraquat, and biotype of American black nightshade (Solanum
    americanum). Weed Science,38(6), 634–638. https://doi.org/10.1017/S0043174500051626.
    Article   CAS   Google Scholar   Boranno, A. R. (1996). Weed management in plasticulture.
    HortTechnology,6(3), 186–189. Article   Google Scholar   Buker, R. S., Steed,
    S. T., & Stall, W. M. (2002). Confirmation and control of a paraquat-tolerant
    goosegrass (Eleusine indica) biotype. Weed Technology,16, 309–313. Article   CAS   Google
    Scholar   dos Santos Ferreira, A., Matte Freitas, D., Gonçalves da Silva, G.,
    Pistori, H., & Theophilo Folhes, M. (2017). Weed detection in soybean crops using
    ConvNets. Computers and Electronics in Agriculture,143, 314–324. https://doi.org/10.1016/j.compag.2017.10.027.
    Article   Google Scholar   Dyrmann, M., Karstoft, H., & Midtiby, H. S. (2016).
    Plant species classification using deep convolutional neural network. Biosystems
    Engineering,151, 72–80. https://doi.org/10.1016/j.biosystemseng.2016.08.024. Article   Google
    Scholar   Dyrmann, M., Jørgensen, R.N., Midtiby, H.S. (2017) RoboWeedSupport -
    Detection of weed locations in leaf occluded cereal crops using a fully convolutional
    neural network. In J A Taylor, D Cammarano, A Prashar, A Hamilton (Eds.) Proceedings
    of the 11th European Conference on Precision Agriculture. Advances in Animal Biosciences,
    8, 842–847. Dyrmann, M., Skovsen, S., Laursen, M.S., Jørgensen, R.N. (2018) Using
    a fully convolutional neural network for detecting locations of weeds in images
    from cereal fields. In The 14th International Conference on Precision Agriculture.
    Retrieved March 2019 from https://www.ispag.org/proceedings/?action=abstract&id=5081&search=years.
    Fennimore, S. A., Slaughter, D. C., Siemens, M. C., Leon, R. G., & Saber, M. N.
    (2016). Technology for automation of weed control in specialty crops. Weed Technology,30,
    823–837. https://doi.org/10.1614/WT-D-16-00070.1. Article   Google Scholar   Freeman,
    S., Horowitz, S., & Sharon, A. (2001). Pathogenic and nonpathogenic lifestyles
    in Colletotrichum acutatum from strawberry and other plants. Phytopathology,91,
    986–992. https://doi.org/10.1094/PHYTO.2001.91.10.986. Article   CAS   PubMed   Google
    Scholar   Gilreath, J. P., & Santos, B. M. (2004). Efficacy of methyl bromide
    alternatives on purple nutsedge (Cyperus rotundus) control in tomato and pepper.
    Weed Technology,18, 341–345. https://doi.org/10.1614/WT-03-086R2. Article   CAS   Google
    Scholar   Grinblat, G. L., Uzal, L. C., Larese, M. G., & Granitto, P. M. (2016).
    Deep learning for plant identification using vein morphological patterns. Computers
    and Electronics in Agriculture,127, 418–424. https://doi.org/10.1016/j.compag.2016.07.003.
    Article   Google Scholar   Hoiem, D., Chodpathumwan, Y., & Dai, Q. (2012). Diagnosing
    error in object detectors. In A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato,
    & C. Schmid (Eds.), Computer vision—ECCV 2012 (pp. 340–353). Berlin, Germany:
    Springer. Chapter   Google Scholar   Lecun, Y., Bengio, Y., & Hinton, G. (2015).
    Deep learning. Nature,521, 436–444. https://doi.org/10.1038/nature14539. Article   CAS   PubMed   Google
    Scholar   Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    et al. (2014). Microsoft COCO: Common objects in context. Accessed Sept 11, 2018,
    from https://arxiv.org/abs/1405.0312. Milioto, A., Lottes, P., Stachniss, C. (2017).
    Real-time blob-wise sugar beet vs weeds classification for monitoring fields using
    convolutional neural networks. In ISPRS Annals of the Photogrammetry, Remote Sensing
    and Spatial Information Sciences (pp. 41–48). Bonn, Germany: International Society
    for Photogrammetry and Remote Sensing. Article   Google Scholar   Redmon, J. (2016).
    Darknet: open source neural networks in C (2013-2016). Accessed Sept 10, 2018,
    from http://pjreddie.com/darknet/. Redmon, J., Divvala, S., Girshick, R., Farhadi,
    A. (2016). You only look once: Unified, real-time object detection. In Proceedings
    of the 29th IEEE Conference on Computer Vision and Pattern Recognition (pp. 779–788).
    Las Vegas, NV, USA: IEEE Computer Society. Redmon, J., Farhadi, A. (2018). YOLOv3:
    An incremental improvement. arXiv Preprint arXiv:1804.02767. Accessed Sept 10,
    2018, from https://arxiv.org/abs/1804.02767. Schmidhuber, J. (2015). Deep learning
    in neural networks: An overview. Neural Networks,61, 85–117. https://doi.org/10.1016/j.neunet.2014.09.003.
    Article   PubMed   Google Scholar   Sharpe, S. M., Schumann, A. W., & Boyd, N.
    S. (2018). Detection of Carolina geranium (Geranium carolinianum) growing in competition
    with strawberry using convolutional neural networks. Weed Science,67, 239–245.
    https://doi.org/10.1017/wsc.2018.66. Article   Google Scholar   Sokolova, M.,
    & Lapalme, G. (2009). A systematic analysis of performance measures for classification
    tasks. Information Processing and Management,45, 427–437. https://doi.org/10.1016/j.ipm.2009.03.002.
    Article   Google Scholar   Teimouri, N., Dyrmann, M., Nielsen, P., Mathiassen,
    S., Somerville, G., & Jørgensen, R. (2018). Weed growth stage estimator using
    deep convolutional neural networks. Sensors,18(1580), 1–13. https://doi.org/10.3390/s18051580.
    Article   Google Scholar   Townshend, J. L., & Davidson, T. R. (1960). Some weed
    hosts of pratylenchus penetrans in premier strawberry plantations. Canadian Journal
    of Botany,38, 267–273. https://doi.org/10.1139/b60-027. Article   Google Scholar   United
    States Department of Agriculture [USDA] (2018a) National Agricultural Statistics
    Service. Accessed Aug 22, 2018, from https://quickstats.nass.usda.gov/. United
    States Department of Agriculture [USDA] (2018b) Soil Survey Staff, Natural Resources
    Conservation Service, Web Soil Survey. Accessed Aug 30, 2018, from https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx.
    Vrindts, E., Baerdemaeker, J. D. E., & Ramon, H. (2002). Weed detection using
    canopy reflection. Precision Agriculture,3, 63–80. https://doi.org/10.1023/A:1013326304427.
    Article   Google Scholar   Webster, T.M. (2014). Weed survey—southern states 2014.
    Vegetable, fruit and nut crop subsection. In Proceedings of the Southern Weed
    Science Society 67th Annual Meeting (pp. 288). Westminster, CO, USA: Southern
    Weed Science Society. Zhang, Y., Staab, E. S., Slaughter, D. C., Giles, D. K.,
    & Downey, D. (2012). Automated weed control in organic row crops using hyperspectral
    species identification and thermal micro-dosing. Crop Protection,41, 96–105. https://doi.org/10.1016/j.cropro.2012.05.007.
    Article   CAS   Google Scholar   Download references Author information Authors
    and Affiliations Gulf Coast Research and Education Center, University of Florida,
    Wimauma, FL, USA Shaun M. Sharpe & Jialin Yu Citrus Research and Education Center,
    University of Florida, Lake Alfred, FL, USA Arnold W. Schumann Gulf Coast Research
    and Education Center, University of Florida, 14625 Count Road 672, Wimauma, FL,
    33598, USA Nathan S. Boyd Corresponding author Correspondence to Nathan S. Boyd.
    Ethics declarations Conflict of interest The authors declare that they have no
    conflict of interest. Additional information Publisher''s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Reprints and permissions About this article
    Cite this article Sharpe, S.M., Schumann, A.W., Yu, J. et al. Vegetation detection
    and discrimination within vegetable plasticulture row-middles using a convolutional
    neural network. Precision Agric 21, 264–277 (2020). https://doi.org/10.1007/s11119-019-09666-6
    Download citation Published 08 May 2019 Issue Date April 2020 DOI https://doi.org/10.1007/s11119-019-09666-6
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Broadleaves Class discrimination Grasses Object detection
    Sedges You Only Look Once Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Sections Figures References Abstract Introduction Materials
    and methods Results and discussion Conclusions References Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature'
  inline_citation: '>'
  journal: Precision agriculture (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Vegetation detection and discrimination within vegetable plasticulture row-middles
    using a convolutional neural network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1111/ele.14123
  analysis: '>'
  authors:
  - Marc Besson
  - Jamie Alison
  - Kim Bjerge
  - Thomas E. Gorochowski
  - Toke T. Høye
  - Tommaso Jucker
  - Hjalte M. R. Mann
  - Christopher F. Clements
  citation_count: 59
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register Ecology Letters
    SYNTHESIS Open Access Towards the fully automated monitoring of ecological communities
    Marc Besson,  Jamie Alison,  Kim Bjerge,  Thomas E. Gorochowski,  Toke T. Høye,  Tommaso
    Jucker,  Hjalte M. R. Mann,  Christopher F. Clements First published: 20 October
    2022 https://doi.org/10.1111/ele.14123Citations: 24 Editor: Jonathan Chase SECTIONS
    PDF 0COMMENTS TOOLS SHARE Abstract High-resolution monitoring is fundamental to
    understand ecosystems dynamics in an era of global change and biodiversity declines.
    While real-time and automated monitoring of abiotic components has been possible
    for some time, monitoring biotic components—for example, individual behaviours
    and traits, and species abundance and distribution—is far more challenging. Recent
    technological advancements offer potential solutions to achieve this through:
    (i) increasingly affordable high-throughput recording hardware, which can collect
    rich multidimensional data, and (ii) increasingly accessible artificial intelligence
    approaches, which can extract ecological knowledge from large datasets. However,
    automating the monitoring of facets of ecological communities via such technologies
    has primarily been achieved at low spatiotemporal resolutions within limited steps
    of the monitoring workflow. Here, we review existing technologies for data recording
    and processing that enable automated monitoring of ecological communities. We
    then present novel frameworks that combine such technologies, forming fully automated
    pipelines to detect, track, classify and count multiple species, and record behavioural
    and morphological traits, at resolutions which have previously been impossible
    to achieve. Based on these rapidly developing technologies, we illustrate a solution
    to one of the greatest challenges in ecology: the ability to rapidly generate
    high-resolution, multidimensional and standardised data across complex ecologies.
    INTRODUCTION Ecosystems are increasingly exposed to stressors, leading to unprecedented
    rates of biodiversity decline globally (Capdevila et al., 2022; Díaz et al., 2019).
    Our ability to reliably forecast ecosystems dynamics is limited by our capacity
    to understand what governs their composition, dynamics, function and structure
    (Dietze et al., 2018; Petchey et al., 2015). To drive predictive ecology forward
    and design appropriate conservation strategies, we therefore need access to long-term,
    high-resolution and standardised information about ecosystems’ abiotic and biotic
    components (Farley et al., 2018; Mccord et al., 2021). Indeed, short time-series
    and low-resolution monitoring of a limited number of biological and ecological
    metrics can be detrimental to our understanding of ecosystems dynamics (White,
    2019; White & Hastings, 2020; Wickham & Riitters, 2019) and are thus not recommended
    (Sparrow et al., 2020). In contrast, long-term, high-resolution and multidimensional
    data—from environmental parameters to individual morphological and behavioural
    traits, and up to species abundances, distributions and interaction—are key to
    holistically understand the mechanisms driving ecosystems dynamics (Naeem et al.,
    2016). The fine-scale patterns present in these multidimensional data are particularly
    useful to predict potential population collapses and manage ecosystems accordingly
    (Cerini et al., 2022; Dietze et al., 2018). However, acquiring such data has traditionally
    involved cost-prohibitive, labour-intensive and often invasive survey methods
    that have consequently limited historical ecological observations both spatially
    and temporally (Kays et al., 2015; Pimm et al., 2015). Recent technological advances
    in sensing technologies and their increasing accessibility have considerably improved
    our data collection capacity and are fundamentally changing how we sample ecological
    data (Allan et al., 2018). Using networked sensor arrays, environmental abiotic
    characteristics (e.g. humidity, light, pressure, temperature, pH) can already
    be monitored automatically, in real-time, and over large spatiotemporal scales
    (Pansch & Hiebenthal, 2019; Urrutia-Cordero et al., 2021). However, ecologists
    are also typically interested in complex biotic metrics such as the behaviours,
    locations and traits of individuals, as well as species abundances, distributions
    and interactions, which ultimately define ecological communities. Technologies
    such as acoustic sensors and camera traps can rapidly, remotely, non-invasively
    and automatically collect high-resolution sounds and images, thus replacing, augmenting
    and surpassing human sampling abilities (Cordier et al., 2018; Darras et al.,
    2019; Marcot et al., 2019; Wearn & Glover-Kapfer, 2019; Welbourne et al., 2015).
    Nevertheless, processing such data into meaningful ecological measurements remains
    a challenging task to automate and a critical operational bottleneck (Keitt &
    Abelson, 2021). Traditional approaches have required significant human effort
    to examine features and patterns in sounds and images that correlate with ecological
    reality (Pimm et al., 2015). Such manual procedures do not scale efficiently with
    ever-growing volumes of raw data produced by modern sensing technologies and are
    mostly inappropriate for large-scale monitoring of complex ecosystems (Kindsvater
    et al., 2018; Peters et al., 2014). To face this challenge, ecology increasingly
    relies on state-of-the-art computational methodologies that automate data processing
    and knowledge extraction from ecological records (Farley et al., 2018). Over the
    last decade, artificial intelligence has revolutionised the way we use computers
    to identify features and patterns in ecological datasets automatically, accurately
    and reliably (Christin et al., 2019). Using computer audition, computer vision
    and machine learning algorithms, ecologists can today automate complex tasks covering
    the detection, identification, counting and measurement of individuals from images
    and audio recordings (Brodrick et al., 2019; Lürig, 2022; Mcloughlin et al., 2019;
    Peters et al., 2014). Deep learning, a branch of machine learning based on multilayer
    artificial neural networks, has been particularly successful at performing these
    tasks (Christin et al., 2019; Scholl et al., 2021; Tuia et al., 2022). While these
    approaches are becoming increasingly popular in ecology, their use often requires
    expertise from multiple disciplines (e.g. ecology, computer science, and electronic
    engineering), such that their potential is generally not realised. Indeed, these
    technologies have primarily been used: (i) on single species systems (e.g. to
    track and quantify multiple traits of a single individual or a single species
    population; Panadeiro et al., 2021; Walter & Couzin, 2021); (ii) on multiple species
    but at suboptimal resolutions (e.g. on camera trap images with low frame rates
    or short temporal coverage [Høye et al., 2021; Weinstein, 2018], or on a limited
    number of image features [Norouzzadeh et al., 2018]); or (iii) asynchronously
    (e.g. by processing data offline rather that in real-time; Jarić et al., 2020).
    A more powerful approach would be to combine these data recording and processing
    technologies into accessible pipelines that could automatically and continuously
    monitor multiple species, in real-time, with high-resolution and multidimensional
    for long time periods (Christin et al., 2019). Here, we begin by reviewing these
    technologies before exploring how they can be incorporated into pipelines that
    can generate high-throughput multidimensional data for accurate, real-time and
    fully automated monitoring of multispecies systems. We use case studies from both
    laboratory-based and field-based experiments to demonstrate how data collection
    can be automated with sensor technologies and robotics, and how collected data
    can be directly analysed using computer vision and deep learning algorithms. Such
    frameworks offer the ability to automatically detect, track, count and classify
    multiple species, but also quantify their interactions, behaviours and morphological
    traits, at previously impossible resolutions. We then illustrate and discuss how
    modified versions of these automated frameworks can be operated on various ecological
    communities to revolutionise their monitoring. FROM AUTOMATED DATA COLLECTION
    TO ECOLOGICAL KNOWLEDGE The automation workflow Automated monitoring of ecological
    communities requires automating the collection, storage, transfer and processing
    of data to extract knowledge about the individuals, populations and communities
    (Figure 1). Among the key metrics that population and community ecologists aim
    at studying are species presences, abundances, functional traits, distributions
    and interactions, as well as individual''s morphology, behaviour and physiology
    (Jetz et al., 2019). Myriad automatic recorders can observe the environment non-invasively
    and collect data such as sounds and images from which it is then possible to detect,
    count, classify and measure unmarked organisms (Lahoz-Monfort & Magrath, 2021).
    Most of these recorders can be grouped into three main categories: (i) acoustic
    wave recorders (e.g. microphones, hydrophones, geophones and sonars); (ii) chemical
    recorders (e.g. environmental sample processors and DNA sequencers); and (iii)
    electromagnetic wave recorders (e.g. cameras and other optical sensors, LiDAR
    and radar systems) (Figure 2). Automating the extraction of ecological metrics
    from such recorders requires storing the data they collect before transferring
    it to computational platforms that can translate it into ecological knowledge
    (Figure 1). FIGURE 1 Open in figure viewer PowerPoint The automation workflow
    for monitoring populations and communities. From data collection to the extraction
    of ecological knowledge, a synthesis of the technologies that can automate the
    acquisition of information regarding individual traits and species abundances,
    distributions and interactions, which are key metrics for the monitoring of ecological
    communities. FIGURE 2 Open in figure viewer PowerPoint A diversity of automatic
    recorders to monitor ecological communities non-invasively and remotely. (1) Vocalising
    birds being monitored by microphones deployed on trees. (2) Stridulating and drumming
    fishes being recorded by hydrophones attached to moorings. (3) Walking elephants
    producing ground vibrations perceived by geophones. (4) Fish shoal being detected
    by a sonar. (5) Oceanic glider navigating an Environmental Sampling Processor
    (ESP) to sample eDNA. (6) Bear being detected by camera traps fixed on trees.
    (7) Hyperspectral camera mounted on a drone and monitoring tree composition in
    a forest. (8) LiDAR sensor mounted on an unmanned aerial vehicle monitoring multiple
    forest canopies. (9) Imaging flow cytometer attached to a mooring and recording
    planktonic communities. (10) Racoons being detected by thermal and IR cameras
    at night. (11) Stationary radar and a satellite radar, respectively, monitoring
    bird and large mammal populations. Recorder''s ability to detect the presence
    of living organisms, count their numbers, classify them at the species level and
    measure their traits (e.g. behavioural, functional and morphological traits) is
    evaluated from 1 to 3 levels as follows: 1 bar corresponds to ‘in corner-case
    situations only’, 2 bars corresponds to ‘in specific conditions and on specific
    organisms (for detecting, counting and classifying) or for a limited number of
    features (for measuring)’, and 3 bars corresponds to ‘in most cases and for most
    organisms (for detecting, counting and classifying) and for several features (for
    measuring)’. Automatic recorders of ecological information Acoustic wave recorders
    Microphones, hydrophones and geophones can record the mechanical pressure waves
    produced by living organisms, such as bird, fish and mammal vocalisations, but
    also the sounds produced by insect and invertebrate activities, and the ground
    vibrations generated by large terrestrial mammals (Bradbury & Vehrencamp, 1998)
    (Figure 2). The audio data obtained by these sensors are called soundscapes, from
    which one can extract ecological information about the present sound-producing
    organisms (i.e. acoustic fingerprints). Computer audition software and machine
    learning algorithms can be used to analyse the sound frequencies and their amplitudes
    to detect relevant audio features (Figure 3a), and to translate these features
    into ecological knowledge (Gibb et al., 2019; Mcloughlin et al., 2019). Indeed,
    this workflow can be used to automatically flag the presence of a sound-producing
    animal (Gervaise et al., 2021; Mac Aodha et al., 2018; Mankin & Benshemesh, 2006),
    its identity relative to other conspecifics (Favaro et al., 2017), and its behaviour
    (Ibrahim et al., 2019; Mortimer et al., 2018; Szymański et al., 2021). Acoustic
    features can also generate estimates of the total number of sound-producing individuals
    (Pieretti et al., 2011; Wrege et al., 2017) as well as determine their species
    identity (Acconcjaioco & Ntalampiras, 2020; Caruso et al., 2020; Kawakita & Ichikawa,
    2019; Mukundarajan et al., 2017; Roemer et al., 2021). While being essentially
    limited to sound-producing animals (but see Jung et al., 2018 for sound production
    in plants), passive acoustic recorders could record cryptic species in low-visibility
    conditions and over large spatial distances. Moreover, audio data can allow the
    identification of subpopulations that morphological phenotyping alone cannot discriminate,
    as evidenced in a damselfish species (Parmentier et al., 2021). However, the quantification
    of individual morphological traits using audio data is obviously limited. Morphological
    traits can only be roughly estimated when being directly correlated to an audio
    feature, for example, when some frequencies or intensities can only be produced
    by an animal of a certain age/size (Favaro et al., 2017). FIGURE 3 Open in figure
    viewer PowerPoint Deep neural networks and their application in monitoring ecological
    communities. (a) Schematic representation of a convolutional neural network (CNN)
    architecture and its application to classify multiple species based on sound or
    image data. (b) Typical example of CNN output when used to count the number of
    organisms present in an image such as in (Lu et al., 2019). (c) Typical example
    of CNN output when used to monitor plant status such as in (Mohanty et al., 2016).
    (d) and (e) represent the output from other types of deep neural networks (i.e.
    non-CNN) used to measure organism morphometrical traits such as in (Jung, 2021)
    and estimate animal pose such as in (Lauer et al., 2022; Mathis et al., 2018;
    Nath et al., 2019) respectively. Photo credits: Marc Besson. In contrast to their
    passive counterparts, active acoustic recorders first produce sound pulses before
    listening for the sound echoes being backscattered by the environment and organisms
    (Benoit-Bird & Lawson, 2016). Given the rapid and efficient transmission of sounds
    in water, active acoustic monitoring has almost exclusively been carried out in
    aquatic environments using sonar technologies (Figure 2). The acoustic features
    present in sonar echo data can be used to automatically detect small organisms
    like copepods and krill (Bernard & Steinberg, 2013), and track fish and squids
    at depths over 800 m (Dunlop et al., 2018; Kay et al., 2022). The taxonomic resolution
    of sonar remains low, and these technologies are unable to classify most organisms
    at the species level (Benoit-Bird & Lawson, 2016). Nevertheless, recent analysis
    methods based on deep learning have been able to successfully distinguish between
    echoes from two fish species (Marques et al., 2021) and two krill species (Fontana
    et al., 2021). Chemical recorders Living organisms continuously alter the biomolecular
    composition of their environment, for example through respiration and excretion
    of faeces, mucus and skin (Taberlet et al., 2018). Among these molecules, DNA
    contains information specific to species and individuals (i.e. genotypes). Sequencing
    nucleic acids can inform us about the presence and abundance of organisms in air
    (Clare et al., 2022; Lynggaard et al., 2022), freshwater (Li et al., 2021), marine
    (Agersnap et al., 2022; Boussarie et al., 2018) and terrestrial environments (Massey
    et al., 2022). Environmental DNA (eDNA) metabarcoding has traditionally required
    large sample volumes and long, labour-intensive and expensive laboratory operations
    (e.g. sample filtration, DNA extraction, purification, amplification, sequencing
    and sequence blasting and alignment) that were not suitable for automated remote
    monitoring of ecological communities. However, recent developments in miniaturised
    microfluidic technologies that automate the sampling and processing of eDNA samples
    (Dhar & Lee, 2018; Formel et al., 2021), and the advent of autonomous vehicles
    to carry such devices (Yamahara et al., 2019) have enabled the conception of environmental
    sample processors (ESPs) that can perform all these steps from sampling to DNA
    amplification and sample storage without human intervention (Hansen et al., 2020;
    Jacobsen, 2021) (Figure 2). While ESPs do not automate post-sampling procedures
    such as DNA sequencing, equipping these devices with modules composed of portable
    nanopore sequencing devices such as the MinION and SmidgION (Ames et al., 2021;
    Jain et al., 2016) could allow them to achieve fully automated status in the future
    (Huo et al., 2021). Species chemical fingerprinting is not restricted to nucleic
    acid and can also be operated on volatile and waterborne organic compounds such
    as carbohydrates, lipids and peptides. Mass spectrophotometry approaches have
    successfully been used to identify and classify micro-organism and plant species
    using their chemical fingerprint signatures (Emami et al., 2016; Lozano et al.,
    2022; Musah et al., 2015; Parveen et al., 2020). While being usually cheaper and
    faster than metabarcoding, these technologies require mass spectra reference libraries,
    and remain therefore primarily used on laboratory experimental communities (Mortier
    et al., 2021; Rossel et al., 2019). Electromagnetic wave recorders Electromagnetic
    wave sensors capture the electromagnetic energy radiated by the environment, either
    passively (e.g. digital cameras) or after emitting their own pulses (e.g. LiDAR
    and radar systems). The most common and affordable of these sensors are digital
    cameras, which can record images in the visible spectrum. Images produced by digital
    cameras comprise three matrices of red-green-blue (RGB) pixel intensities, from
    which it is possible to extract image features such as colours, shapes, contours,
    textures and relationships to surrounding pixels (Weinstein, 2018) (Figure 3a).
    Using computer vision approaches, including some that involve machine learning
    (Wäldchen & Mäder, 2018; Weinstein, 2018), these image features can be automatically
    detected and used to perform individual tracking (Lopez-Marcano et al., 2021),
    counting (Lu et al., 2019) and morphological measurements (Kühl & Burghardt, 2013;
    Mathis et al., 2018; Nath et al., 2019; Pennekamp & Schtickzelle, 2013; Walter
    & Couzin, 2021), as well as classifying multiple individuals, behaviours and species
    (Lürig et al., 2021; Weinstein, 2018; Zhou et al., 2021) (Figure 3a–e). Monitoring
    of larger flora and fauna using ground-level camera traps is already well-established
    (Norouzzadeh et al., 2018; Richardson, 2019; Richardson et al., 2018), and, as
    resolution and availability of satellite imagery increase, it becomes practical
    to detect and count megafauna from space (Guirado et al., 2019; Xue et al., 2017).
    Similarly, the potential for in situ camera-surveillance of small insects is now
    apparent, and such approaches may eventually be applicable at microscopic scales
    (Høye et al., 2020). When visibility is limited, thermal cameras, active infrared
    cameras and artificial illumination can help to monitor presence and activity
    of organisms (McCarthy et al., 2021; Zahoor et al., 2021). By capturing the contrast
    between the heat (infrared radiation) emitted by organisms and their surroundings
    to generate an image (Starosielski, 2019), thermal cameras offer the opportunity
    to detect and quantify the abundance of endotherms in low light conditions (Steen
    et al., 2012), even when being occluded by vegetation, smoke or fog (Corcoran
    et al., 2019; Corcoran, Winsen, et al., 2021). However, thermal cameras generally
    have lower spatial resolutions than standard digital cameras (Christiansen et
    al., 2014). In contrast to passive thermal cameras, active infrared cameras first
    pulse short wavelength infrared light before capturing the infrared energy reflected
    by the environment. Active infrared cameras are not limited to the monitoring
    of warm-blooded organisms (Teutsch et al., 2021), and their greater resolution
    than thermal cameras is useful for species classification (Mu et al., 2019). However,
    they are more sensitive than thermal cameras to visual noise caused by dust, haze
    and smoke, which can hide key image features and hamper subsequent image analyses
    (Soan et al., 2018). Hyperspectral cameras, often mounted on aerial vehicles,
    can measure dozens to hundreds of narrow spectral bands from the electromagnetic
    spectrum. Hyperspectral imagery comprises many matrices (one per spectral band)
    from which several image features can be extracted (ElMasry & Sun, 2010). This
    high spectral resolution can be used to predict the chemical composition of subjects
    observed, allowing, for example, the remote monitoring of plant health status
    at the individual level (Näsi et al., 2015) and functional traits such as growth,
    biomass and successional status (Asner, Martin, et al., 2015), with the objective
    of getting as close as possible to species identification (Dalponte et al., 2012)
    (Figure 2). The spatial resolution of hyperspectral imagery is usually limited
    (Feng et al., 2020) but remains well adapted to the monitoring of tree communities
    from an aircraft flying over large spatial ranges (Miyoshi et al., 2020; Nevalainen
    et al., 2017; Saarinen et al., 2018). The lack of spatial resolution from hyperspectral
    imaging can be compensated by a coupling with digital camera (Feng et al., 2020)
    and LiDAR (Light Detection and Ranging, or laser imaging) technologies (Cao et
    al., 2021; Eitel et al., 2016). LiDAR systems actively emit a pulsed laser light
    and measure its echo using an optical sensor to draw digital 3D representations
    of the targets based on laser signal return times and wavelengths (Melin et al.,
    2017). When scanning over a forest, LiDAR provides relatively fine morphological
    details about tree targets such as height and vertical structure (Vauhkonen et
    al., 2016). The canopy cover and leaf area index obtained from LiDAR data can
    be used to infer biomass, growth and assess tree condition (Korhonen et al., 2011;
    Melin et al., 2017). These morphological features can be combined with the chemical
    compositions obtained with hyperspectral imagery to build random forest and machine
    learning-based classifiers for the monitoring of tree communities at the species
    level (Cao et al., 2021; Scholl et al., 2021). At microscopic scales, flow cytometers
    also use laser imaging, but illuminate particles suspended in a liquid sample
    with light pulses of various wavelengths to record the phase and intensity of
    the illuminated particle, from which a hologram and an image can be reconstructed
    (Işll et al., 2021). Imaging flow cytometers can directly capture images of the
    passing particles and measure various of their morphological features (Işll et
    al., 2021). While this technology is primarily limited to the monitoring of microorganisms,
    it offers the possibility to detect, count, measure and classify high numbers
    of different particles, and has thus proven to be well suited to the monitoring
    of planktonic communities such as microalgae and dinoflagellates (Fischer et al.,
    2020; Pomati et al., 2011). Cytometers can also measure the physiological state
    of these organisms, such as algal photosynthetic and enzymatic activities (Furuya
    & Li, 1992; Hyka et al., 2013). At much larger scales, radars are active electromagnetic
    ray sensing devices, which scan their environment with micro and radio waves (beyond
    the infrared in the electromagnetic spectrum), and have been shown to be able
    to monitor wildlife (Baratchi et al., 2013; Hüppop et al., 2019; Lahoz-Monfort
    & Magrath, 2021). For example, by looking at the frequency shift from the transmitted
    signal to the received signal, pulse Doppler radars can provide information about
    the biomass, location, velocity and nature of moving birds (Zaugg et al., 2008),
    fishes (Benoit-Bird et al., 2003), insects (Hu et al., 2016), and marine mammals
    (DeProspo et al., 2004). Most radars operate on frequency/wavelength ranges that
    can penetrate the barriers affecting typical optical sensors. As such, radar technologies
    are particularly adapted to long-distance monitoring of flying animals or organisms
    in the open ocean, but often at low taxonomic resolution (Hüppop et al., 2019)
    (Figure 2). Automated monitoring at scale The sensing devices introduced so far
    can either: (i) be deployed locally and individually to monitor a single area;
    (ii) be assembled in a network to cover larger areas or (iii) be mounted on vehicles
    to navigate along larger scale transects. By transmitting their respective signals
    over long distances, single sonars and radars usually cover large areas. These
    technologies are often fixed, and single units can detect the presence of living
    organisms thousands of meters away for sonars and up to the continental scale
    for some radars (Benoit-Bird & Lawson, 2016; Hüppop et al., 2019). The spatial
    range of most microphones, hydrophones, geophones and cameras is more limited,
    from a few centimetres to a few kilometres. Generally, deploying these sensing
    devices in single and stationary recording stations is sufficient to monitor populations
    and communities of small body sizes such as ciliates in microcosms (Pennekamp
    & Schtickzelle, 2013), insect colonies (Tashakkori et al., 2017) or species with
    clumped distribution patterns such as demersal fishes and small range singing
    birds (Desjonquères et al., 2020; Frommolt, 2017). When aiming at monitoring communities
    at a specific place (e.g. wildlife crossing structures and fish aggregating devices),
    rather than exploring the whole species distribution, single stationary recording
    units are also appropriate (Brehmer et al., 2019; Fischer et al., 2020; Ford et
    al., 2009; Pomezanski & Bennett, 2018). In contrast, when aiming at monitoring
    living organisms with larger home ranges, sensing devices can be assembled in
    networks (e.g. along transects or within grid-frameworks). Sensor networks such
    as microphone arrays can locate vocalising animals by comparing the signal reception
    timing at different recorders (Sethi et al., 2020; Verreycken et al., 2021). In
    contrast, cameras have a greater directionality in their sensing, hence deploying
    numerous cameras in a network can increase the field-of-view coverage when being
    oriented in different directions or placed at different locations (Steenweg et
    al., 2017). Camera sensor networks can also improve object detection, identification
    and measurement accuracy when multiple cameras record the same environment from
    different perspectives (Zhu et al., 2021). Sensor networks can also deploy diverse
    types of recorders, such as the infrared sensors and digital cameras found in
    camera traps (Swann et al., 2004), and the multisensory devices used for bat monitoring
    (Gottwald et al., 2021). While being challenging to implement and automate, units
    that combine different sensing technologies—for example, combined digital cameras,
    hyperspectral cameras and acoustic recorders—often provide a more comprehensive
    and accurate picture of the studied system by capturing more species (e.g. visible
    and cryptic species) and more data types (e.g. behavioural, morphological, physiological
    and abundance data) (Chapuis et al., 2021; Frouin-Mouy et al., 2020; Ireland et
    al., 2019; Michez et al., 2021; Wägele et al., 2022). When the price of the automatic
    recorder is prohibitive (e.g. high definition and hyperspectral cameras), deploying
    multiple units of them in a network may not be feasible. Instead, mounting such
    recorders on autonomous vehicles such as drones (Corcoran, Denman, & Hamilton,
    2021; Gonzalez et al., 2016), oceanic gliders (Kowarski et al., 2020), satellites
    (Fretwell et al., 2017; LaRue et al., 2017), or terrestrial robots (Bietresato
    et al., 2016) offers opportunities to automate ecological monitoring over large
    spatial scales. A major challenge during automated ecosystem monitoring involves
    the temporal scales and resolutions over which they must be observed. Long time
    windows are particularly important when collecting data regarding the dynamic
    nature of population sizes or seasonal phenology. On the other hand, monitoring
    individual behaviours and interactions requires high temporal resolution during
    small time windows. By linking recording devices to power sources such as solar
    panels and small wind turbines, it is possible to extend their lifetime (Sethi
    et al., 2018), but these solutions are not applicable in every context, and can
    increase the operational costs and feasibility of the monitoring program. One
    way to expand the lifetime of automatic recorders with a limited and finite power
    supply (e.g. non-rechargeable battery) is to integrate on-board processing of
    data from low-energy sensors before deciding whether to trigger other power-hungry
    sensors. For example, most camera traps are equipped with power-efficient passive
    infrared sensors that only trigger high-resolution video recording when an animal
    is detected (Welbourne et al., 2015, 2016). The rise of portable electronics has
    seen the development of affordable and power-efficient microprocessors (e.g. Raspberry
    Pi) and microcontrollers (e.g. Arduino). Accessibility of these devices is revolutionising
    our ability to monitor ecosystems at low-cost and low-power consumption (although
    sleeping in very low power mode cannot be achieved yet for Raspberry Pi) for application
    in off-grid locations (Jolles, 2021). These solutions have inspired the conception
    of low-cost sensing devices such as AudioMoth (Hill et al., 2018, 2019), HydroMoth
    (Lamont et al., 2022), Aurita (Beason et al., 2019) and KiloCam (https://www.ecologisconsulting.com/),
    all of which can operate acoustic and visual sensors, on-battery and for extended
    periods of time. For systems installed without any internet access, data need
    to be collected on external drives, which may need replacing on a regular basis.
    Using on-board data processing approaches can minimise storage to only critical
    and informative components, extending battery life and storage capacity (Liu et
    al., 2019). For example, time-lapse wildlife camera systems powered by lithium
    AA batteries can run remotely for several months without human intervention, except
    for replacing SD cards (Mann et al., 2022). For systems with internet access,
    the introduction of 5G cellular networks and specialised networks for the Internet
    of Things (e.g. Low-Power Wide-Area Networks) has facilitated the high-bandwidth
    data transfer between recording devices and computational resources (Chettri &
    Bera, 2020). Such wireless sensors that directly send their recorded data to external
    servers have the advantage of not being limited by storage capacity and can allow
    for virtually unlimited continuous monitoring of a system (Sethi et al., 2018).
    Nevertheless, such frameworks require monitored sites to be equipped with antennas
    and/or relays, as well as with an energy source to power up data transmission,
    which are invasive additions to ecosystems (Levitt et al., 2021). Therefore, similarly
    to abiotic sensor networks, it is important to consider the best practices for
    network design and sensor data management to minimise impacts on ecosystems and
    management costs while optimising sensing quality and connectivity (ESIP EnviroSensing
    Cluster, 2014; Yu et al., 2020). Tools for automatic extraction of ecological
    knowledge Fully automated monitoring of ecological communities requires computational
    analysis pipelines that can process and extract knowledge from the large datasets
    generated by sensing technologies. Diverse computational methods exist for feature
    extraction, as well as classifying and measuring the characteristics of those
    features (Lürig et al., 2021). These have recently been dominated and greatly
    improved by deep learning approaches, based on models such as Convolutional Neural
    Networks (CNNs) (Brodrick et al., 2019) (Figure 3a–e). Excellent reviews about
    the usage of deep learning in bioacoustics and computer vision, as well as current
    trends and limitations already exist (Christin et al., 2019; Gibb et al., 2019;
    Høye et al., 2021; Mcloughlin et al., 2019; Stowell, 2022; Stowell et al., 2019;
    Tuia et al., 2022; Wäldchen & Mäder, 2018; Weinstein, 2018). Therefore, reviewing
    these methodologies and their technical characteristics is beyond the scope of
    this work. However, there are several key considerations when using machine learning
    methods in the context of automated analyses in ecology and the types of data
    captured by remote and distributed sensing systems. First, effective machine learning
    models typically require large amounts of training data where a ground truth is
    known. For imaging datasets, this would involve the annotation of large numbers
    of images with the specific features that need to be extracted (e.g. classification
    of individual pixels as ‘organism’ or ‘background’ if segmentation is the goal).
    While a single training set might allow for a model to accurately extract features
    for similar types of images, it is rare that a single model can generalise well
    to vastly different environments. Indeed, generalising animal detection and classification
    in new locations remain a great challenge, since many state-of-the-art algorithms
    only perform well on the same location where they were trained (Beery et al.,
    2018). As powerful as deep learning technologies are, they remain sensitive to
    distribution shifts between the training data and the data of the downstream use
    case. Therefore, separate models are often generated for specific use cases with
    training data sets required for each. In contrast, developing location invariant
    and robust deep learning classifiers requires infusing data subsets from each
    location (e.g. images from each camera trap) into the training (David et al.,
    2020; Shepley, Falzon, Meek, & Kwan, 2021), but can only be achieved by first
    collecting a larger number of images from numerous and diverse contexts (David,
    2021). Doing so for multiple species remains a major challenge that publicly available
    data sources (e.g. iNaturalist, Pl@ntNet) and easy-to-use tools to aid with the
    often-manual annotation process can help to address (Lauer et al., 2022; Mathis
    et al., 2018; Pereira et al., 2019; Shepley, Falzon, Lawson, et al., 2021). However,
    this step can hamper the application of deep learning approaches in specific areas
    where existing data is scarce and difficult to gather. Second, CNNs are well suited
    to general feature extraction from sensor data where spatial and temporal information
    is captured by the position of measurements in data matrices. To make sense of
    this data, CNNs exploit multi-resolution representations to capture generalised
    features that can be further combined. In the context of images, this might include
    at a low-level being able to distinguish edges by changes in contrast across nearby
    pixels, while at a high-level using these edge features to help capture shapes
    of relevance to specific types of object in a scene (e.g. different organisms).
    Beyond extraction of simple features like these, processed data can also form
    input to other analyses such as tracking and interaction mapping algorithms, as
    well as other machine learning models able to capture higher-level characteristics
    (e.g. behavioural traits). Recurrent Neural Networks (RNNs) and Transformer models
    have become commonplace in natural language processing and image analysis to aid
    in machine translation (Young et al., 2018) and the understanding of video content
    (Khan et al., 2022). While their use in ecology to date has been limited, it is
    likely their application will grow as large multidimensional datasets become available
    through automated sensing technologies. For example, Transformer models for species
    classification and distribution prediction from image and sound recordings in
    the field have already begun to emerge (Conde & Turgutlu, 2021; Elliott et al.,
    2021; Joly et al., 2021; Reedha et al., 2022). Third, a feature of deep learning
    models that is particularly interesting to remote monitoring applications is the
    efficiency with which data can be processed. Although the training of a deep learning
    model often requires extensive processing and memory resources, executing a trained
    model requires only a fraction of this computational power. Furthermore, specialised
    microprocessors and models are beginning to emerge to efficiently run in low power
    settings (Lou et al., 2020; Sanchez-Iborra & Skarmeta, 2020). This has brought
    machine learning at the place where data collection happens, enabling simultaneous
    collection and analysis of data and reducing the amount of data that needs to
    be stored and transmitted (Dutta & Bharali, 2021). Finally, it is important to
    recognise that no single machine learning method, nor computer audition/vision
    package can suit all automated monitoring purposes. Instead, various computational
    pipelines, each suited to dealing with a specific context or processing step,
    depending on data types and on the organisms being monitored, are likely to be
    needed. Contrary to intuition and similarly to sensor deployment and maintenance,
    the full automation of knowledge extraction from ecological dataset initially
    depends on people and labour. With labour ranging from data annotation and management
    to model development, training ecologists for achieving fully automated monitoring
    of ecological communities might trend towards literacy in the relevant data types,
    as well as collaborations with engineers and computer scientists. Therefore, we
    argue that there is a timely need for dedicated funding streams to both train
    ecologists in these methods and to develop coordinated research networks with
    such standardised data acquisition protocols. We therefore believe that developing
    easy-to-use systems, with workflows connecting existing machine learning and analysis
    methods, would help stimulate future research and funding in this domain. This
    would then allow greater effort to be placed on addressing specific challenges
    to fully automate ecological monitoring, such as how and when to trigger recordings,
    deal with data storage and pre-process data before feeding them into a fully automated
    analysis program. COMBINING TECHNOLOGIES TO FULLY AUTOMATE THE MONITORING OF MULTISPECIES
    SYSTEMS Fully automated monitoring of micro-organisms in experimental systems
    Experimental laboratory systems have been used for decades to examine how individual
    morphological traits, species abundances, distributions and interactions respond
    to various stressors. Collecting such data is often time consuming and labour-intensive,
    which limit data resolution and replication. Nevertheless, experimental systems
    ensure controlled environment (e.g. lighting conditions that guarantee species
    visibility), calibration of—and unlimited power-supply to—high-definition automatic
    recorders such as modern digital cameras. Therefore, laboratory systems are often
    the initial developmental space for automated technologies, and help pioneering
    technological advancements that can later be transferred into the field (Joska
    et al., 2021). Small-scale experimental systems offer a perfect opportunity to
    test and develop the concept of fully automated workflows (Alisch et al., 2018).
    Here, we detail a system developed to collect multidimensional data on freshwater
    protists, ciliates and rotifers to evaluate the resilience of these ecological
    communities in response to biotic and abiotic stressors (Box 1). BOX 1. Automated
    pipeline for monitoring freshwater protists in experimental microcosms System
    presentation The system is composed of a robotic gantry that controls the X and
    Y positions of a 6K-14fps camera mounted on a stereomicroscope, navigating over
    3D-printed experimental landscapes (i.e. microcosms, Figure 4a,b). This set up
    allows videos of each microcosm to be automatically collected and analysed to
    extract information about the abundance and distribution of species, and individuals''
    morphological and behavioural traits (e.g. size, velocity, turning rates) (Besson
    et al., 2021a, 2021b). Automated video acquisition The robotic gantry and the
    camera are controlled by an in-house-developed Python program that consists of
    the following steps (i.e. first part of the automation workflow): Parametrisation:
    A two-column data frame containing the X and Y locations where we want to move
    the gantry is loaded into the program (i.e. gantry location loop). A one-column
    data frame containing the GMT times for which the gantry will loop over the different
    locations is loaded into the program. Video duration is then selected, as well
    as a file path for where to save the video that will be recorded by the camera.
    Video acquisition loop: The gantry then starts its loop at the times indicated
    in a. The gantry moves the camera to the first X/Y location and starts the camera.
    A first check controlling whether the camera is well positioned over a microcosm
    is performed using another in-house-developed OpenCV algorithm (Bradski, 2000).
    If the position is not correct, the gantry moves the platform until the camera
    field of view matches with a microcosm. Once the position is correct, a second
    check is performed by reading a QRcode fixed to the microcosm. This QRcode contains
    information about the microcosm ID, treatment and replicate, which are stored
    as variables to properly name the video that is then recorded. Once the video
    is saved, the camera turns off and the gantry moves to the next location, repeating
    this same procedure. End of the loop: Once all locations have been navigated to,
    the gantry moves back to its home location, before starting the video acquisition
    loop again as many times as listed in the time data frame loaded during the parametrisation
    step. Automated video analysis The second part of the automation workflow consists
    of processing and extracting ecological knowledge from the videos that were automatically
    acquired. To achieve this, we firstly developed a computer vision methodology
    using OpenCV in Python (Bradski, 2000). Since the model species are in constant
    motion, we used background subtraction to segment objects corresponding to living
    organisms from the background (Figure 4c). The segmented objects are then measured
    (e.g. centroid location, length, width, surface area, orientation) using basic
    OpenCV functions, and tracked using a custom algorithm based on Kalman-Filtering
    (Patel & Thakore, 2013). Tracking objects allows us to calculate morphometric,
    velocity and trajectory metrics for each segmented object over the entire video
    (Figure 4d–g). Classifying objects by assigning them a species name is operated
    by sending each object''s images into a CNN based on MobilenetV2 and pretrained
    on ImageNet (Deng et al., 2009; Howard et al., 2017). This CNN was fine-tuned
    using automatically generated training protist image datasets that we obtained
    by recording videos of single species and using the same segmentation/tracking
    methodology described above. Classifying all objects in all frames allows the
    collection of multiple classification data: one per frame for each single object,
    increasing classification accuracy by looking at the classification time series
    of each object (Figure 4f). This workflow combines robotic, camera and deep learning
    technologies to fully automate the monitoring of protist communities over days
    to weeks, allowing the collection of multidimensional data (i.e. behavioural and
    morphological traits for each individual, and abundances and distributions for
    each species) at resolutions that would be impossible to achieve manually (one
    data per frame, at more than 10 frames per second). This pipeline allows to play
    with multi-patch landscapes (Figure 4b), thus exploring the effects of landscape
    fragmentation, patch connectivity and multiple stressors induced at the patch
    level, on protist community dynamics (Clements et al., 2014; Clements & Ozgul,
    2016). Ongoing upgrades of this system will equip every patch with miniaturised
    abiotic sensors such as temperature, oxygen and pH probes, to have a more comprehensive
    monitoring of each of these microscopic ecosystems. This workflow is generally
    well-suited to the monitoring of microorganisms (e.g. freshwater and marine phyto-
    and zooplankton), and can be easily adapted to larger experimental systems such
    as mesocosms, macrocosms and in the field (e.g. over water tanks, river streams,
    green houses, aviaries and fields). This would allow to scale at the community
    level the existing automated single species monitoring, such as those existing
    for ants and fish in the laboratory (Cao et al., 2020; Lopez-Marcano et al., 2021),
    and directly in the field (Francisco et al., 2020; Imirzian et al., 2019), by
    adding a species classification layer to these automated frameworks. Furthermore,
    such larger-scale systems would allow the use of multiple cameras without the
    need to mount them on a microscope and robot, reducing the cost of the recording
    part of the monitoring system while allowing to record of multiple landscapes
    simultaneously. FIGURE 4 Open in figure viewer PowerPoint Overview of a fully
    automated workflow towards the monitoring of multidimensional data from multispecies
    protist communities in experimental systems. (a) Robotic gantry navigating a microscope
    and camera over experimental microcosms. (b) Examples of other microcosm landscapes
    that can be used within this workflow. (c) Video analysis workflow, from raw frames
    to measurement and classification of moving objects using the CNN classifier.
    Red bounding boxes indicate the detected individuals and coloured overlay indicate
    different species. (d–g) Length and width, velocity, classification and trajectory
    measurements, respectively, obtained by this automated workflow for a single moving
    object (i.e. protist organism) over the duration of the video. Photo credits:
    Marc Besson. Fully automated monitoring of plant–pollinator interactions in field
    mesocosms Reports of drastic declines in insect diversity, abundance and biomass
    carry severe implications for ecosystem services such as pollination. However,
    insects are difficult to study, and traditional methods require substantial manual
    effort to collect data. Data are particularly sparse and patchy in logistically
    challenging areas. As a result, there are large geographic and taxonomic gaps
    in data about insects—including many pollinator—preventing thorough investigation
    of the drivers and severity of insect declines. Cameras and computer vision can
    help to solve data deficiencies in entomology and pollination, enabling remote
    data collection and automatic identification at unprecedented spatial and temporal
    resolutions (Høye et al., 2021). For some research questions related to plant–pollinator
    interactions, such as characterising plant phenology, recording must be hourly
    or even daily, while for questions related to pollination events or pollinator–pollinator
    interactions, the framerate must be in seconds or even milliseconds. Here, we
    detail a system to enable fully automated in situ monitoring of plants and pollinators
    at these resolutions, including inter- and intra-specific interactions (Box 2).
    BOX 2. Automated pipeline for monitoring plant–pollinator interactions in field
    mesocosms System presentation The system collects non-invasive, high-resolution
    data on flowers and pollinating insects. High-volume acquisition of images to
    train CNNs is achieved using a camera, mounted on a steel frame, with a power
    supply and a memory storage unit. By incorporating computers into on-site hardware,
    CNNs can then detect, classify and track insects and flowers in real-time. The
    system allows rigorous monitoring of abundance, diversity and phenology of plants
    and pollinators. By automatically generating entomological data at unprecedented
    spatial and temporal resolutions, real-time tracking can revolutionise our understanding
    of not only plant–pollinator interactions, but pollinator–pollinator interactions.
    Automated image acquisition Affordable webcams (e.g. Logitech C920 HD Pro Webcam
    ~$60) and wildlife time-lapse cameras (e.g. Wingscapes TimelapseCam Pro ~$150)
    have been successfully deployed in this system. Key image acquisition parameters
    include frame rate, recording periods, focal distance and resolution. These parameters
    are adjusted based on the study system and the mode of data collection. High-volume
    data collection generates the imagery needed to train a CNN, while real-time data
    collection leverages those CNNs to collect entomological data at extremely high
    frequency. High-volume data collection aims to generate a representative image
    library for off-site annotation and analysis. It is appropriate for pollination
    systems which lack trained CNNs, if data storage and labour are not strong limiting
    factors. Frame rate and recording periods are limited by storage capacity on-
    and off-site, as well how frequently storage and power can be replenished. Recording
    12 frames every hour to a 128 gb SD card at 4224 × 2376 pixel resolution, including
    LED flash at night, 20,000 images can easily be recorded over 70 days (Wingscapes
    TimelapseCam Pro with one set of eight AA lithium batteries). Real-time data collection,
    defined here as processing in parallel with image capture, builds upon resources
    generated by high-volume data collection. It involves rapid on-site analysis of
    images, retaining only text-based detection data and a subset of images for validation.
    Having massively reduced demand for memory, real-time data collection is very
    useful for remote sites, provided that trained CNNs are available for detection
    and classification. It also records flowers and insects at extremely high temporal
    resolution, allowing in-depth analysis of individual behaviours and species interactions.
    Automated image analysis Image analysis comprises two stages—detection and classification,
    followed by individual track identification. For detection and classification
    of flowers and pollinators, image-series spanning full growing seasons are processed
    by CNNs. Bjerge et al. (2021) demonstrate automated detection and classification
    of insects in an urban ecosystem with eight classes of arthropods, including species
    important for pollination. Using the CNN darknet53 (YOLOv3), they achieve real-time
    detection and classification at 0.33 frames per second. Tracking the movement
    of individuals within the frame can be achieved based on minimal displacement
    and size-change of objects between frames (Bjerge et al., 2021). As with automated
    monitoring of protists (Box 1), a tracking algorithm permits recording of behavioural
    metrics, but also improved classification. For example, a majority vote can be
    taken across consecutive classifications of an individual insect. A tracking algorithm
    can also be deployed to identify and separate individual flowers; this allows
    derivation of flower-level data on floral traits, phenology and visitation (Mann
    et al., 2022). Such real-time detection and classification present exciting opportunities
    to examine species interactions at unprecedented spatiotemporal resolutions. First,
    this approach can inform as to whether different pollinator taxa are active during
    different seasonal or diurnal periods, while accounting for multiple counting
    of the same individual by counting tracks rather than detections (Figure 5b,c).
    Similarly, this approach can provide a high spatial resolution view of how different
    floral resources are used by different insects (Figure 5d,e). Moreover, this approach
    reveals highly complex short-term patterns of co-occurrence, in which different
    pollinators that are generally active at similar times of day potentially exclude
    one another on timescales from minutes to seconds (Figure 5f). Such opportunities
    to quantify fine-scale pollinator–pollinator interactions are particularly relevant
    given mounting concerns about the impacts of managed honeybees on wild pollinators
    (Ropars et al., 2022; Thomson, 2016). Collection of sufficient real-time data
    will even allow individual tracks to be examined in relation to the presence and
    absence of other individuals or species. In this way, we may begin to grasp the
    behavioural mechanisms behind competitive exclusion as never before. FIGURE 5
    Open in figure viewer PowerPoint Insights from real-time, fully automated in situ
    monitoring of plants and pollinator interactions. (a) The automated pollinator
    monitoring system records a green roof comprising Sedum flowers. (b) Continuous
    surveillance allows the annual phenology of different pollinator groups to be
    quantified at fine temporal resolutions (blue = honeybees; dark purple = bumblebees;
    light orange = hoverflies; abundance = number of individual tracks). (c) Diurnal
    phenology can also be compared across groups, showing a relative preference of
    hoverflies for mornings and honeybees for evenings. (d) Image from day 234 of
    2020, a day of high pollinator activity. (e) Activity of different insect groups
    on day 234 can be mapped to inflorescences in (d) to quantify plant–pollinator
    interactions. (f) Real-time monitoring even allows exploration of pollinator-pollinator
    interactions; the activity (total detections) of honeybees, bumblebees and hoverflies
    is shown for 10-min intervals during day 234, where bumblebees are only active
    during a remarkably short period of the day. Using a similar pipeline, plant phenology
    is being recorded by cameras in environments such as the Arctic as well as along
    elevational gradients. For instance, cameras record arctic flowers such as Dryas
    integrifolia and Silene acaulis to characterise plant and pollinator phenology
    in extreme environments (Mann et al., 2022). Across Scandinavia, this system helps
    to understand effects of landscape composition and competition on pollination.
    In the UK, the system is being extended to monitor pest control in crop fields,
    showing clear potential to monitor ecological processes such as herbivory, predation
    and detritivory at hitherto impossible sites and resolutions. Ultimately, automated
    vehicles, such as drones, could enable in situ camera-based monitoring of a huge
    variety of smaller plant, animal and fungal communities. Still, the full potential
    of automated in situ pollinator monitoring has not yet been realised. Importantly,
    real-time data collection is contingent on reliable CNNs, which must be trained
    on huge numbers of annotated images from relevant ecological contexts. Automated
    monitoring is thus limited by (1) uptake of standardised, high-volume data collection
    with time-lapse cameras, (2) standardised annotation of massive image libraries
    and (3) robustness of detection and classification algorithms to novel insects
    and novel backgrounds. Annotation of insects and flowers can be increasingly outsourced
    to citizen science platforms such as eButterfly (https://www.e-butterfly.org/),
    the Global Biodiversity Information Diversity (https://www.gbif.org/), iNaturalist
    (https://www.inaturalist.org/), Pl@ntNet (https://plantnet.org/) and Zooniverse
    (https://www.zooniverse.org/). However, a major challenge is the generalisation
    of such automated solutions to a wide diversity of natural ecosystems. Specifically,
    even with an exceptionally large training dataset, the system will encounter unfamiliar
    species, including some that are inseparable within high-resolution imagery. Three
    emerging approaches will help this challenge to be overcome: (1) open-set classification
    can allow specimens, even those that are not present in training data, to be classified
    to the lowest possible taxonomic level (Lee et al., 2018); (2) synthetic image
    datasets can be generated using images of specimens with validated species-level
    identification (Skovsen et al., 2020) and (3) combination of data from multiple
    sources or sensors for species-level labelling and classification—for example,
    images may be complemented by DNA sequence data (Badirli et al., 2021). Towards
    the fully automated monitoring of any community in any ecosystem The previous
    examples of fully automated monitoring of multidimensional data from multispecies
    systems pose the question of whether such frameworks could be developed for almost
    any other ecosystem. A first limitation is obviously the requirement for large
    and properly labelled training datasets when implementing accurate and reliable
    deep learning classifiers, preventing the monitoring of communities for which
    such data is not available (McKibben & Frey, 2021). Second, the environmental
    complexity of natural systems could hamper such designs, generating data with
    a very low signal-to-noise ratio in comparison with experimental systems. Nevertheless,
    initiatives aiming at capturing and monitoring wildlife habitat complexity do
    exist and represent great research avenues to combine automated wildlife monitoring
    and habitat mapping within complex environments. For example, the 100 Island Challenge
    (https://100islandchallenge.org/) associates classical field surveys with innovative
    imaging and data technologies to reconstruct 100 m2 coral reefs digitally and
    in 3D, from which all corals are individually annotated and classified at the
    species level (Naughton et al., 2015). By combining this workflow and the vast
    amount of labelled 3D coral structures it has generated with approaches aiming
    at automatically classifying 3D objects such as MeshCNN (Hanocka et al., 2019)
    and Global point Signature Plus & Deep Wide Residual Network (Hoang et al., 2021),
    we could automate coral habitat mapping in the future. Moreover, the development
    of autonomous underwater vehicles would help achieving automated surveys (Modasshir
    & Rekleitis, 2020; Ordoñez Avila et al., 2021), while coupling these surveys with
    acoustic monitoring would scale up our understanding about how coral habitats
    promote surrounding biodiversity (Lin et al., 2021) (Figure 6a). FIGURE 6 Open
    in figure viewer PowerPoint Futurist examples of fully automated wildlife monitoring
    programs. (a) Autonomous and wireless underwater vehicle equipped with multiple
    high-resolution cameras and hydrophone array, together monitoring multidimensional
    data about coral reef communities such as habitat complexity, coral species distribution
    and fish functional diversity. (b) Autonomous and self-charging drones equipped
    with LiDAR and hyperspectral cameras for the monitoring of plant and tree flowering
    phenology. Another field of research where a combination of novel sensing and
    machine learning approaches is beginning to bear fruit is forest ecology. With
    growing access to high-resolution remote sensing imagery, we have witnessed rapid
    improvements in algorithms developed to reliably and accurately identify individual
    trees in both LiDAR point-clouds and RGB orthophotos (Brandt et al., 2020; Dalponte
    & Coomes, 2016; Ferraz et al., 2016; Weinstein et al., 2020). Particularly promising
    are recent efforts to use deep learning to delineate individual tree crowns in
    RGB imagery acquired from unmanned aerial vehicles (UAVs) and satellites, and
    then apply them at broad spatial scales (Brandt et al., 2020; Weinstein et al.,
    2020). For example, DeepForest was recently used to map the crowns of around 100
    million trees across the National Ecological Observatory Network (Weinstein et
    al., 2021), while Brandt et al. (2020) used a similar approach applied to sub-meter
    resolution satellite imagery to identify around 1.8 billion individual trees spread
    across 1.3 million km2 in the West African Sahara and Sahel. Delineating the crowns
    of individual trees not only allows us to count their numbers, but also measure
    key axes of their size that directly scale with their biomass—such as height and
    crown area (Jucker et al., 2017; Marconi et al., 2021). Moreover, by fusing individual
    tree maps with multi or hyperspectral imagery, one can also classify individuals
    to species and estimate several key traits related to plant growth and function
    (Asner, Anderson, et al., 2015; Asner, Martin, et al., 2015; Dalponte & Coomes,
    2016). Generating these baseline distribution maps is the first step towards developing
    automated routines for tracking forest phenology and dynamics at seasonal, inter-annual
    and even decadal time scale. For instance, daily 3-m resolution PlanetScope satellite
    imagery trained against data from in situ PhenoCam networks can detect the timing
    of key phenological stages in the canopy, such as bud burst, flowering and leaf
    drop (Dixon et al., 2021; Moon et al., 2021). Using RGB cameras mounted on UAVs,
    with repeated data acquisition over several years also allowed to track forest
    dynamics (e.g. canopy gap formation), phenology (e.g. leaves and flowering time)
    and the underlying mechanisms behind treefall rates that traditional survey could
    not capture (Araujo et al., 2021; Park et al., 2019). Similarly, the use of wireless
    sensor networks comprising multispectral cameras, LiDAR and abiotic sensors connected
    to solar-powered batteries allows to track in real-time changes in tree growth
    and key physiological parameters such as water use and local microclimate that
    are transforming our understanding of processes that constrain when and how fast
    trees grow (Etzold et al., 2022; Valentini et al., 2019; Zweifel et al., 2021).
    This information is critical for being able to predict how trees might respond
    to extreme climate events and for parameterising more realistic global vegetation
    models (Zuidema et al., 2018). Therefore, when coupled with data from camera traps
    and acoustic networks (Deere et al., 2020; Sethi et al., 2020) these novel data
    streams to study plant phenology and forest ecology would allow to (i) build a
    detailed picture of the interactions occurring in complex vegetated ecosystems
    across multiple trophic levels; (ii) elucidate how they shift from season to season
    and year to year and (iii) predict how they will change under novel climate (Figure
    6b). Towards new ecological knowledge and conservation challenges Overall, the
    different monitoring technologies presented here show clear advantages over traditional
    survey methods, including precise traits estimation, less disturbance (but see
    below), the ability to cover greater, more remote and potentially dangerous areas,
    in a repeatable, quantifiable, high-resolution and standardised way to measure
    myriad of biological and ecological metrics. Such systems, through their standardisation
    and the high-resolution multidimensional data they can acquire, have the potential
    to generate novel ecological insights (Tuia et al., 2022; van Klink et al., 2022).
    For example, 24-hour camera surveillance of Swiss alpine meadows recently revealed
    moth pollination of Trifolium pratense, a phenomenon overlooked during a century
    of research into that important wildflower and forage crop species (Alison et
    al., 2022). Furthermore, automation allows ecological interactions to be rigorously
    quantified at unprecedented spatiotemporal resolutions—ranging from ephemeral
    interactions between micro-organisms or insects, to drawn-out conversations between
    humpback whales (Cholewiak et al., 2018). Understanding interactions between species
    and individuals is crucial to predict ecosystem responses to anthropogenic drivers.
    The high-resolution and multidimensional data which can be generated using automated
    frameworks (e.g. behavioural and morphological traits, abundances and distributions
    across multiple species) offer the opportunity to develop new predictive frameworks,
    which for the first time can synthetise data across ecological scales (from individuals
    to populations) and help developing novel early warning signals that precede population
    and community collapses (Cerini et al., 2022). Indeed, ecological forecasting
    is an area where automated frameworks offer significant opportunity, as the resolution
    of data required to develop robust predictive tools is most often impossible to
    obtain with non-automated methods (Cordier et al., 2018; Darras et al., 2019;
    Lamprey et al., 2020; Marcot et al., 2019; Wearn & Glover-Kapfer, 2019; Welbourne
    et al., 2015). Moreover, automated methods allow the acquisition of these data
    in real-time, pushing ecological research from the post hoc era to one where forecasts
    about ecosystems fate are continually updated based on the current observed state,
    similar to weather forecasting (Deyle et al., 2016; Huang et al., 2019; Slingsby
    et al., 2020). The step change offered by such real-time data, in combination
    with cutting edge statistical methods such as Bayesian statistics and machine
    learning tools, which both leverage past state to improve predictive accuracy,
    offers perhaps the greatest opportunity for ecology to become a truly predictive
    science. These outstanding perspectives brought by the fully automated, high resolution
    and multidimensional monitoring of ecological communities should not eclipse the
    potential negative effects of these technologies on wildlife. For example, unmanned
    and self-navigating devices such as drones can affect animal physiology (Ditmer
    et al., 2015) and behaviour (Bennitt et al., 2019; Bevan et al., 2018; Mulero-Pázmány
    et al., 2017; Schroeder et al., 2020), although these disturbances may be less
    detrimental than those caused by traditional survey methods, with less impact
    per unit of data (Aubert et al., 2022; Christiansen et al., 2016; Gallego & Sarasola,
    2021) and some species becoming rapidly habituated to the presence of unmanned
    vehicles (Ditmer et al., 2019). Nevertheless, it is timely to (i) better quantify
    these impacts to avoid the generation of biased and unstandardised data; and (ii)
    aim to minimise these impacts to prevent animal stress. Ways to mitigate these
    impacts include the development of new unmanned aircraft systems, such as miniaturised
    drones and blimp-like aerostats, which eliminate noise disturbance to wildlife
    (Adams et al., 2020; Kuhlmann et al., 2022). When such devices are not available,
    disturbances can be avoided by using greater camera resolutions and obtaining
    the necessary permits to increase flying height (Scobie & Hugenholtz, 2016). For
    the autonomous monitoring of the canopy health state and plant phenology, most
    terrestrial robots are equipped with large wheels or wheel-chains (Bietresato
    et al., 2016), which can damage the vegetation (Stager et al., 2019). Legged robots
    would minimise these impacts, but their development for ecological monitoring
    is still in its infancy (Gonzalez-De-Santos et al., 2020). Similarly, underwater
    autonomous vehicles have potential to damage underwater vegetation, which, in
    turn, can clog and strangle propellers (Pedroso de Lima et al., 2020). In addition
    to the vehicles carrying them, sensor technologies themselves can negatively affect
    wildlife, as evidenced with sonar technologies on marine mammals (Harris et al.,
    2018; Southall et al., 2016), and artificial light on insects (Jonason et al.,
    2014; Kalinkat et al., 2021). Sensors themselves may also be perceived by subject
    organisms. For example, cameras based on their appearance, sound, flash and even
    active infrared emissions, may be recognised and consequently alter animal behaviours
    (Caravaggi et al., 2020). At end-of-life, and when being damaged by weather conditions
    and animas themselves, systems also have the potential to pollute the environment
    (e.g. via batteries) (Rysgaard et al., 2022). In this context, the development
    of biodegradable sensing systems represents a promising research avenue (Sethi
    et al., 2022). Thus, whilst the impacts of automated approaches are often localised,
    minimal and almost certainly sub-lethal, they will of course scale with the extent
    of the sensory network. As such, we propose a cautious rollout of automated monitoring
    over the coming decades, with concurrent studies aiming to minimise the disturbance
    caused by automated monitoring apparatus. CONCLUSION Technologies such as automatic
    recorders and deep learning have not reached their full potential to support modern
    ecological monitoring in a fully automated manner (Hampton et al., 2013; Tuia
    et al., 2022). In practice, automation of particular steps of a workflow and subsequent
    scaling up most often still require substantial labour input, for example, for
    maintenance and data retrieval. The development of fully automated frameworks
    may also be limited by challenges associated with building interdisciplinary collaborations
    among ecologists, electronic engineers and artificial intelligence specialists,
    to train the specialised staff needed to develop and maintain accessible automated
    systems (Pedroso de Lima et al., 2020). By synthesising the variety of existing
    automated technologies and describing real-world and futurist workflows that bring
    them together, we aim to stimulate such collaborations in the future—towards the
    development of new, user friendly and standardised pipelines that automatically
    monitor multiple components of multispecies systems with minimal disturbance exerted
    (Weinstein, 2018). The fully automated monitoring frameworks that we present here
    integrate novel hardware and software approaches allowing the rapid generation
    of high resolution, multidimensional data across complex ecological communities.
    In the current era of global change, such data will be critical to (i) reliably
    compare ecological communities globally and monitor their temporal dynamics, (ii)
    feed mechanistic models to better predict their fate, (iii) investigate potential
    signals preceding the changes in the functioning and structure of a system and
    (iv) examine which stressors are impacting wildlife the most, and which populations
    are the most at risk. AUTHOR CONTRIBUTIONS Marc Besson and Christopher F. Clements
    conceived the idea. Marc Besson wrote the paper with significant contributions
    from all authors. Marc Besson and Jamie Alison designed the figures. ACKNOWLEDGEMENTS
    MB is supported by NE/T003502/1 Natural Environment Research Council (NERC) grant.
    CC is supported by grants NE/T006579/1 and NE/T003502/1 from NERC, and RGS\R2\192033
    from The Royal Society. T.E.G. is supported by a Royal Society University Research
    Fellowship grant UF160357, a Turing Fellowship from The Alan Turing Institute
    under the EPSRC grant EP/N510129/1 and BrisEngBio, a UKRI-funded Engineering Biology
    Research Centre under grant BB/W013959/1. T.T.H. acknowledges funding from Independent
    Research Fund Denmark Grant 8021-00423B. T.J. was supported by grant NE/S01537X/1
    from UK NERC Independent Research Fellowship. Open Research REFERENCES Citing
    Literature Comments Volume25, Issue12 December 2022 Pages 2753-2775 Figures References
    Related Information Recommended Identification of 100 fundamental ecological questions
    William J. Sutherland,  Robert P. Freckleton,  H. Charles J. Godfray,  Steven
    R. Beissinger,  Tim Benton,  Duncan D. Cameron,  Yohay Carmel,  David A. Coomes,  Tim
    Coulson,  Mark C. Emmerson,  Rosemary S. Hails,  Graeme C. Hays,  Dave J. Hodgson,  Michael
    J. Hutchings,  David Johnson,  Julia P. G. Jones,  Matt J. Keeling,  Hanna Kokko,  William
    E. Kunin,  Xavier Lambin,  Owen T. Lewis,  Yadvinder Malhi,  Nova Mieszkowska,  E.
    J. Milner-Gulland,  Ken Norris,  Albert B. Phillimore,  Drew W. Purves,  Jane
    M. Reid,  Daniel C. Reuman,  Ken Thompson,  Justin M. J. Travis,  Lindsay A. Turnbull,  David
    A. Wardle,  Thorsten Wiegand Journal of Ecology Macroecology and consilience Brian
    A. Maurer Global Ecology and Biogeography Trade‐offs and Biological Diversity:
    Integrative Answers to Ecological Questions Paul R. Martin Integrative Organismal
    Biology, [1] Applications for deep learning in ecology Sylvain Christin,  Éric
    Hervet,  Nicolas Lecomte Methods in Ecology and Evolution Deep learning as a tool
    for ecology and evolution Marek L. Borowiec,  Rebecca B. Dikow,  Paul B. Frandsen,  Alexander
    McKeeken,  Gabriele Valentini,  Alexander E. White Methods in Ecology and Evolution
    Download PDF Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms
    of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies Developing World Access HELP & SUPPORT Contact Us Training
    and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies.'
  inline_citation: '>'
  journal: Ecology letters (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Towards the fully automated monitoring of ecological communities
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.knosys.2020.106723
  analysis: '>'
  authors:
  - Junfeng Gao
  - Jesper Cairo Westergaard
  - Ea Høegh Riis Sundmark
  - Merethe Bagge
  - Erland Liljeroth
  - Erik Alexandersson
  citation_count: 47
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Material and methods
    4. Results 5. Discussion 6. Conclusion and future work CRediT authorship contribution
    statement Declaration of Competing Interest Acknowledgments References Show full
    outline Cited by (56) Figures (18) Show 12 more figures Tables (4) Table 1 Table
    2 Table 3 Table 4 Knowledge-Based Systems Volume 214, 28 February 2021, 106723
    Automatic late blight lesion recognition and severity quantification based on
    field imagery of diverse potato genotypes by deep learning Author links open overlay
    panel Junfeng Gao a, Jesper Cairo Westergaard b, Ea Høegh Riis Sundmark c, Merethe
    Bagge c, Erland Liljeroth d, Erik Alexandersson d Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.knosys.2020.106723 Get rights and content Abstract
    The plant pathogen Phytophthora infestans causes the severe disease late blight
    in potato, which can result in huge yield loss for potato production. Automatic
    and accurate disease lesion segmentation enables fast evaluation of disease severity
    and assessment of disease progress. In tasks requiring computer vision, deep learning
    has recently gained tremendous success for image classification, object detection
    and semantic segmentation. To test whether we could extract late blight lesions
    from unstructured field environments based on high-resolution visual field images
    and deep learning algorithms, we collected 500 field RGB images in a set of diverse
    potato genotypes with different disease severity (0%–70%), resulting in 2100 cropped
    images. 1600 of these cropped images were used as the dataset for training deep
    neural networks and 250 cropped images were randomly selected as the validation
    dataset. Finally, the developed model was tested on the remaining 250 cropped
    images. The results show that the values for intersection over union (IoU) of
    the classes background (leaf and soil) and disease lesion in the test dataset
    were 0.996 and 0.386, respectively. Furthermore, we established a linear relationship
    (R .655) between manual visual scores of late blight and the number of lesions
    detected by deep learning at the canopy level. We also showed that imbalance weights
    of lesion and background classes improved segmentation performance, and that fused
    masks based on the majority voting of the multiple masks enhanced the correlation
    with the visual disease scores. This study demonstrates the feasibility of using
    deep learning algorithms for disease lesion segmentation and severity evaluation
    based on proximal imagery, which could aid breeding for crop resistance in field
    environments, and also benefit precision farming. Previous article in issue Next
    article in issue Keywords Plant diseaseResistance breedingConvolutional neural
    networksSemantic segmentationMulti-scale predictionMask fusionImage-based crop
    phenotyping 1. Introduction Crop diseases pose a threat to global food security
    [1]. Automated field phenotyping can become a powerful tool for future resistance
    breeding as well as for precision agriculture [2], [3], and can thus be a successful
    way to protect against crop disease. Potato is today the third most important
    food crop in the world and is an important part of many diets, especially in temperate
    climates. The oomycete Phytophthora infestans (Mont.) de Bary which causes potato
    late blight (PLB) and potato tuber blight (PTB) can be very destructive in potato
    cultivation if it is not managed [4]. In practice, the prevention of PLB in the
    field is highly relying on regular blanket spraying of fungicide during the growth
    season. As an example, in Sweden the potato production consumes around 20% of
    all fungicides used in agriculture, largely to combat P. infestans, in spite of
    occupying less than 1% of the area under cultivation [5]. In addition, PLB prevention
    requires frequent use of fungicides with sometimes more than 10 applications per
    growth season in Northern Europe to avoid significant yield loss. This management
    is effective in general and widely accepted by farmers, but also results in usage
    of large amounts of fungicide as well as fossil fuels, which hampers the sustainable
    development of agriculture. The loss caused by PLB can be reduced by breeding
    PLB resistant cultivars. To breed for high PLB resistance, plant breeders establish
    experimental plots to quantify the PLB severity of different potato genotypes
    and progeny lines. This is currently manually done by estimating visual scores
    based on the number and area of lesions on plants [6]. Visual scoring in the field
    provides a crucial metric to quantify disease severity and progression rate between
    cultivars and breeding lines under evaluation. However, this process is time consuming
    and prone to be biased leading to errors. Furthermore, it requires experienced
    raters and it is often not feasible to do daily scorings, which would otherwise
    advance the understanding of the progression rate. Therefore, the development
    of an automated disease evaluation system would be much welcomed to facilitate
    and accelerate the breeding processes. However, one of the main challenges in
    automating the system for disease scoring is to accurately segment the lesions
    under field conditions. Disease lesion segmentation is the process of segmenting
    pathological regions out of crop organs such as leaves and stems. These regions
    of interest typically occupy a very small fraction of the full images acquired
    from a crop field, particularly in early stages of plant development. In field
    environments, the captured images often contain soil background regions, sharing
    similar color and morphological features as disease lesions. To tackle these difficulties,
    many efforts have been made, including intensity threshold analysis, region growth
    algorithms, spectral indices and machine learning algorithms [7]. However, these
    approaches heavily rely on the hand-crafted features determined by experts. These
    features might also have limitations with regards to representation and generalization
    ability. Deep learning has proved to be an effective approach for traditional
    computer vision problems such as image classification, object detection and segmentation,
    since it is capable of extracting features hierarchically [8]. In addition, the
    applications with deep learning in agriculture also showed recent unprecedented
    advancements [9]. Specifically, in precision farming it has been deployed for
    weed detection [10], agricultural pest detection [11] and selective fruit harvesting
    [12], as well as leaf counting [13]. Furthermore, deep convolutional neural networks,
    one of the most used deep learning algorithms, combined with computer vision techniques
    have been exploited for crop disease classification and detection. Polder et al.
    [14] adapted a fully convolutional neural network (FCN) for potato virus Y (PVY)
    detection based on hyperspectral imagery. It demonstrated that the approach based
    on deep learning achieved good recall results and indicated the suitability of
    this method for field disease detection. Stewart et al. [15], [16], [17] developed
    deep neural networks for northern leaf blight (NLB) in maize from field RGB images
    collected from an unmanned aerial vehicle (UAV) platform. By contrast, the quantification
    and detection of PLB lesions have still been confined to images acquired in the
    laboratory or greenhouse. To the best of our knowledge, the use of deep convolutional
    neural networks has yet to be fully investigated for PLB lesion segmentation in
    diverse potato genotypes based on RGB imagery from the field. The specific objectives
    of this study are (1) to evaluate the performances of deep convolutional neural
    networks for PLB lesion segmentation; (2) to determine the optimal class weights
    for the classes PLB disease lesion and background (i.e., leaf and soil); (3) to
    fuse prediction masks at multiple scales for more accurate lesion prediction;
    (4) to determine the correlation between visual scoring and the number of lesions
    at the canopy level. The early pre-symptom PLB detection is outside the scope
    of this study as only RGB and no hyperspectral images were analyzed. 2. Related
    work Some studies have already explored lesion segmentation for detection of plant
    disease. For example, Abdu et al. [18] developed a pattern recognition approach
    to recognize early blight, caused by Alternaria solani, and PLB visual disease
    symptoms using soft computing and machine learning algorithms. Barbedo [19] and
    Camargo et al. [20] also carried out similar studies on plant disease detection
    and segmentation. However, the image datasets used in these studies were collected
    at foliage level under relatively clear and uniform backgrounds in controlled
    settings and the pipelines are not readily implemented into large scale field
    environments. Moreover, the majority of previous works have only investigated
    the symptom segmentation problems based on a single potato cultivar. These models
    might fail to segment other lesions due to variation in lesion morphology and
    leaf color between different potato cultivars. Attempts have also been made for
    early detection of PLB. Various sensors from imaging to non-imaging sensors have
    been employed in these applications. For example, Fernández et al. [21] investigated
    the classification accuracy changes of infested and healthy potato leaves over
    different days post-inoculation (DPI) with a spectroradiometer and a multispectral
    camera under structured environments. Appeltans et al. [22] discussed the setting
    of imaging parameters for hyperspectral and thermal proximal disease sensing in
    potato and leek fields with a ground-based vehicle. Other than ground-based platforms,
    an unmanned aerial vehicle (UAV) platform equipped with imaging sensors has also
    showed its feasibility on field PLB monitoring with the detection of spectral
    changes in crop traits, including multispectral imagery [23], [24], [25]. There
    are some previous publications for quantification of plant disease severity. Image-based
    analysis including images from RGB, multispectral and hyperspectral sensors under
    controlled conditions [26] or based on PLACL (Percentage of Leaf Area covered
    by Lesions) at single leaf level [27] exists, but is yet to demonstrate its full
    potential for accurate estimation in field environments [28]. Towards accurate
    PLB lesion segmentation under field conditions, our approach is based on deep
    learning algorithms. The main contributions of this study are three-fold: (1)
    We propose a SegNet-based network and determine the optimal class weight for training
    this network for accurate PLB lesion segmentation. (2) We develop an effective
    optimization strategy for lesion counting at a canopy level based on a majority
    voting mechanism. (3) We quantify the relationships between the number of predicted
    lesions and the visual scores estimated by experts. 3. Material and methods The
    research methodology of this study includes three main procedures: Image data
    collection, deep neural network model development, and lesion counting and severity
    quantification. A schematic overview of this study is displayed in Fig. 1. Download
    : Download high-res image (919KB) Download : Download full-size image Fig. 1.
    Schematic overview of study. The developed neural network was trained by the cropped
    images (512 × 512) obtained from raw field images and then cut using multiple
    scales. The final prediction was obtained by majority voting of all scales. 3.1.
    Image data collection The images were acquired with a hand-held RGB camera (Sony
    RX 100 iii) in nadir ( 5 degrees) at approximately 40 cm over each canopy. The
    ISO, aperture and FOV of the camera were set to 125, f/5.6 and 8.8 mm, respectively.
    Images were acquired in full cloudy, semi-cloudy or sunny light settings. No flash
    was used. Especially for the semi-cloudy and sunny acquisitions, consideration
    was taken to ensure no additional shade was being cast onto the canopy from person
    or camera. No post-processing regarding color correction was performed. The field
    location was outside Give, Denmark (N 55.859188, E 9.331065). Fig. 2 shows a single
    image of the field from 100 meters above ground, at 15 Days After Infection (DAI).
    It is clearly seen how a large part of the trial field is decimated by PLB. The
    trial was set up with guard rows of Oleva (cv.) around the trial area. Each plot
    within the trial consisted of 4 plants in a 2 × 2 formation with infector rows
    on each side and a 50 cm gap between plots. The total trial consisted of 775 genotypes:
    47 genotypes with three replicates, 217 genotypes with two replicates and 511
    genotypes represented by single plots. Infector rows consisted of alternating
    Bintje (cv.) and Oleva (cv.). Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 2. A bird’s-eye view of the experimental trial
    taken from a drone platform at 100 m above ground (a); an image example at 15
    DAI (b). 3.2. Manual visual scoring of PLB Manual visual scoring was done 2 times
    a week, starting 5 days after inoculation of infector rows and continuing until
    the standard cultivar Robijn had reached 50% infection. Infection of PLB at each
    time point was scored as percentage of leaf area infected according to the Euroblight
    protocol [29] with a single lesion per plot scored as 0.1%, 2–5 lesions scored
    as 0.5%, 5–10 lesions scored as 1%. The manual detection of lesions was done by
    walking between the two rows of each plot at a slow pace and looking at the plants
    at an angle. If a lesion was spotted, it was further investigated for PLB presence
    and the plot was investigated for presence of more lesions. The percentage of
    infection was used to create an area under disease progression curve (AUDPC) as
    follows: (1) where T is inoculation date and T  is evaluation dates (i 0, 1, 2,
    3). A is the percentage of infection at T . 3.3. Image preprocessing There are
    70 original images (5472 × 3648) which were all labeled manually with the annotation
    toolbox LabelMe [30]. The distribution of genotypes among these images is shown
    in Fig. 3. The majority of images are from the Bintje and Oleva potato cultivars.
    These two genotypes are susceptible to P. infestans, providing more disease lesions
    for training. The original image is unable to be directly fed to neural networks
    as the spatial resolution of original images is too large and requires intensive
    computation memories. It is also not advisable to shrink the whole image to a
    small size, making processing possible but heavily degrading the quality of small
    features in tiny lesion annotations. There are 70 original images and the same
    amount of corresponding ground truth images. The following procedures were applied
    to both. Each original image was cut into 6 × 5 (horizontal × vertical) sub-images
    (912 × 730) and then each sub-image was resized to 512 × 512 in order to be fed
    into the neural networks for training. Each original image contributes 30 sub-images
    leading to 2100 sub-images in total. 1600 sub-images were randomly selected for
    the training set and the remaining 500 sub-images were randomly separated equally
    as the validation set (250 sub-images) or the test set (250 sub-images). Download
    : Download high-res image (361KB) Download : Download full-size image Fig. 3.
    Histogram of raw image genotypes, the red bar represents the number of images
    belonging to that genotype. 3.4. Deep learning We adopted an encoder–decoder neural
    network architecture based on SegNet [31] for lesion segmentation. The proposed
    network operates on input images of 512 × 512 pixels and outputs segmentation
    masks in the same size as the input image. The architecture has an hourglass shape
    consisting of a bunch of convolutional and up-convolutional layers. A diagrammatic
    overview of the network is shown in Fig. 4. The encoder (Table 1) comprises a
    series of convolutional operations, activation and pooling operations. Semantic
    features from low-level to high-level were extracted at the end of the encoder
    process. Because of max-pooling layers, the spatial resolution output shrunk 2
    times compared to the previous convolution block in the encoder phase. The index
    of the maximum feature value in each pooling window was recorded for each encoder
    feature map (Fig. 5). In contrast, the decoder part (Table 1) upsamples its input
    feature maps using the recorded max-pooling indices learned from the corresponding
    layers in encoder part and finally generates the prediction results with same
    spatial size as original input images. The upsampling layers, such as deconvolution
    layers, in the decoder are also capable of learning a mapping from feature map
    to semantic segmentation results. Each convolutional layer is followed by several
    operational layers including a non-linear ReLU activation function (max(0, x))
    and batch normalization. The total parameters are 29,442,122 with 29,434,694 trainable
    parameters and 7428 non-trainable parameters. Download : Download high-res image
    (403KB) Download : Download full-size image Fig. 4. The neural network is based
    on an encoder and decoder architecture, followed by a final pixel-wise classification
    layer (red circles highlight disease lesions). Download : Download high-res image
    (203KB) Download : Download full-size image Fig. 5. Example of max-pooling and
    upsampling with index in SegNet. Table 1. Details of the proposed deep neural
    network architecture. Filter size is 3 × 3 and padding mode is set as ‘same’ with
    zero-filled. The final decoder output is fed to a softmax classifier to produce
    class probabilities for each pixel. Module Layers Dimensions Feature maps Block
    structure Input Input (RGB) 512 × 512 3 En-coder Convolution 1 block 256 × 256
    64 2 × convolution pooling Convolution 2 block 128 × 128 128 2 × convolution pooling
    Convolution 3 block 64 × 64 256 3 × convolution pooling Convolution 4 block 32
    × 32 512 3 × convolution pooling Convolution 5 block 16 × 16 512 3 × convolution
    pooling De-coder Up convolution 1 block 32 × 32 512 3 × convolution upsampling
    Up convolution 2 block 64 × 64 256 3 × convolution upsampling UP convolution 3
    block 128 × 128 128 3 × convolution upsampling UP convolution 4 block 256 × 256
    64 2 × convolution upsampling UP convolution 5 block 512 × 512 64 2 × convolution
    upsampling Output Softmax 512 × 512 2 Convolution sigmoid 3.5. Loss function The
    loss function is key for training a robust and high-performance network. The most
    commonly used loss function for semantic segmentation is pixel-wise cross-entropy
    loss. In our study, the frequency of appearance for lesion and background class
    is highly imbalanced. The number of disease lesion pixels is far less than other
    pixels such as healthy plant organs and soil backgrounds in field images at early
    infection stages. Using just a standard loss function without adaption would make
    a deep neural network model tend to only correctly classify dominant class pixels
    (backgrounds), ignoring the importance of lesion pixels. This is also called the
    accuracy paradox, which is a model that provides a very high overall accuracy
    but performs poorly over classes. One common way to mitigate this effect is the
    use of a class-balancing approach by assigning different weights over classes
    based on their median frequency [32]. In this study, we regard the weights for
    each class as a hyperparameter to tune. Other than weights calculated from Eq.
    (2), we also compared the prediction performance with different weight ratios
    from 1 to 9 to select the optimal weight. (2) where frequency(c) represents the
    frequency of occurrences of pixels of class c divided by the total number of pixels
    in any images containing that class, and median_frequency is the median of these
    frequencies overall all classes. The weighted loss function used in the network
    is shown in Eq. (3) below. (3) where N is the number of observations, C is the
    number of classes (background and lesion), is the weight for class c, y is a binary
    indicator (0 or 1) if a class label is correctly classified for observation o,
    and p is predicted probability of observation o being of class c. 3.6. Training
    The network was trained end-to-end from scratch with the Adam optimizer [33] using
    a stable learning rate of 0.0001 to minimize the loss values. The batch size was
    set to 18 with 500 epochs in total. We employed 3 Nvidia Tesla V100-SXM2 GPUs
    with around 32G memory each for training the network. Each epoch took around 171
    s to finish, accounting for 23.75 h of training time in total. Data augmentation
    was used to reduce the risk of overfitting in the training phase. Specifically,
    in each batch, cropping, horizontal or vertical flipping, and a zoom range from
    0.8 to 1.2 were randomly applied in the images and their corresponding ground
    truth masks. All network training and validation were done using the Tensorflow
    deep learning framework. The model was saved only with the decline of loss values
    in each epoch. The accuracy and loss values in the validation dataset were recorded
    as well in every epoch. 3.7. Model evaluation The model was evaluated with three
    standard metrics for semantic segmentation. The three metrics are overall average
    accuracy, class average accuracy and mean intersection over union (mIoU), respectively.
    The calculations are listed below. We also used confusion matrix to check how
    many pixels of each class are correctly classified. Overall average accuracy (Calculation
    (4)) measures the overall performance for all pixels. The high value means that
    the majority of pixels are correctly classified but it does not indicate good
    lesion segmentation as a majority of pixels in our image are of the background.
    Hence, this value is sometimes quite biased for evaluating model performances.
    Class average accuracy (Calculation (5)) averages the performance of each class.
    A high value represents good performance across all classes. IoU (intersection
    over union), also known as the Jaccard index, is a commonly used and effective
    metric in semantic segmentation. It measures the area of overlap between the predicted
    segmentation and the ground truth divided by the union area of the predicted segmentation
    and the ground truth in labeled images. mIoU is calculated by averaging the IoU
    of each class (Calculation (6)). (4) (5) (6) where is the total number of pixels
    of class i in ground truth image, is the number of pixels of class j predicted
    as belonging to class i and c is the total number of classes. Table 2. Pseudo-codes
    of fused mask generation based on a majority voting approach. Algorithm: Generic
    pseudo-code of lesion counting 1: Input: test dataset 2: For image in test dataset
    do: 3:  for i 3 to 9 do: 4:  cut image horizontally i and vertically i-1, i ×
    (i-1) sub-images generated in total; 5:  resize sub-images (512 × 512); 6:  predict
    each subimage with the optimal model, i × (i-1) sub-masks obtained in total; 7:  align
    i × (i-1) sub-masks/sub-images and reconstruct masks/images; 8:  resize masks/images
    in original size; 9:  save masks/images (each image has 7 corresponding masks);
    10:  end 11:  obtain class label in each pixel of an image by the majority vote
    of the class labels in its  corresponding pixels of its 7 masks; 12: Output: count
    the number of lesions in each image; 3.8. Post-processing Fully connected conditional
    random fields (FCCRFs) [34] were used for post-processing the predicted masks.
    It combines single pixel prediction and shared structure through unary and pairwise
    terms to improve smoothness and to maximize agreement between similar neighboring
    pixels. The FCCRFs establish pairwise potential by using a Gaussian function on
    all pixel pairs in an image. The main benefits of using FCCRFs are determining
    the optimal decision boundary at conflict regions of pixels, while not having
    notable negative effects on successfully segmented pixels. For post-processing
    of images, the prior knowledge of P. infestans disease lesion area is relatively
    small in an early infection stage was used. We found that some false positive
    areas likely represent a shadowed area between leaves (Fig. 10). But since these
    false lesion areas are far larger than normal lesions appeared at that early infection
    stage, a simple threshold algorithm could be operated to filter out some false
    positives in the test images. We set a reasonable lesion area range to be [50,10000]
    pixels to exclude the extreme false positives. Furthermore, the canopy heights
    and structures of potato plants vary, which means that the same lesion spots could
    be displayed differently in 2D images, which in turn can lead to failed predictions.
    For some failed lesions the network can successfully predict the lesions at a
    different scale. A majority voting approach for lesion counting was proposed based
    on multiple prediction masks from various scales. The generic pseudo-code is listed
    in Table 2. Each image for prediction was cropped into sub-images at 7 multiple
    scales from 3 × 2 to 9 × 8 (e.g. 3 × 2 scale represents splitting the image into
    6 parts by dividing the image into 3 parts horizontally and into 2 parts vertically).
    Specifically, each image was cropped at 3 × 2, 4 × 3, 5 × 4, 6 × 5, 7 × 6, 8 ×
    7, 9 × 8 scales in horizontal x vertical directions. The sub-images with the same
    scales were predicted separately by the model, resulting in 7 predictions for
    each image. The final prediction mask of an image is obtained based on the majority
    voting of its 7 prediction masks. 4. Results 4.1. Network training Overall, the
    training loss and validation loss decreased with the increment in training time
    (Fig. 6). The validation loss fluctuated much in the early training stage ( 150
    epochs) and then slowly converged at 0.0626 at the end of the training. By contrast,
    the training loss smoothly dropped until the end of training and finally converged
    at 0.0398, slightly lower than the final validation loss (0.0626). It can also
    be observed that both the training loss and validation loss were substantially
    stable after 450 epochs, indicating that the model stopped improving on a hold-out
    validation dataset. The model weights were saved at 450 epochs to prevent the
    risk of overfitting. At this epoch point, the overall accuracy values in the training
    and validation datasets were 0.9962 and 0.9945, respectively. The same procedures
    were followed when training other models with different hyperparameters (the weight
    ratio of lesion and background) for performance comparison in the test dataset.
    Download : Download high-res image (122KB) Download : Download full-size image
    Fig. 6. Loss curves of the network (1:7 weight ratio) in the training and validation
    datasets. 4.2. The weight ratio of two classes (lesion and background) The weight
    ratio of lesion and background classes is one of the important hyperparameters
    needed to be fine-tuned. In this study, we investigated 13 groups of weight ratios
    in order to determine the optimal weight ratio for lesion semantic segmentation.
    One of the weight ratios (1:2.5) was obtained based on the median frequency of
    two classes and the remaining weight ratios were ranging from 1 to 12. The metrics
    in the validation dataset are shown in Table 3. It shows that the imbalance weights
    can effectively improve the segmentation performance as all mIoU values exceed
    0.65 compared to the 0.551 obtained when the weight ratio was set to 1:1. Interestingly,
    the mIoU value does not continue to increase with a larger weight ratio ( 7).
    The maximum mIoU value was achieved with a 1:7 weight ratio. We selected the model
    with this weight ratio as the optimal model for lesion segmentation. Table 3.
    Metrics of the models with different class weight ratios in the validation dataset.
    Weight ratio Overall accuracy Class average accuracy IoU (background) IoU (lesion)
    mIoU 1:1 0.998 0.671 0.854 0.248 0.551 1:2 0.999 0.743 0.997 0.361 0.679 1:2.5
    0.999 0.789 0.997 0.371 0.684 1:3 0.998 0.802 0.996 0.345 0.671 1:4 0.999 0.810
    0.997 0.400 0.698 1:5 0.999 0.799 0.996 0.337 0.618 1:6 0.999 0.833 0.997 0.395
    0.696 1:7 0.998 0.804 0.997 0.401 0.699 1:8 0.998 0.817 0.997 0.397 0.697 1:9
    0.997 0.841 0.996 0.372 0.684 1:10 0.998 0.857 0.997 0.397 0.697 1:11 0.998 0.814
    0.997 0.397 0.697 1:12 0.998 0.816 0.997 0.376 0.687 4.3. Prediction in the test
    dataset Similar to the image process for training, the original test images (5472
    × 3648) were first cropped and then resized to sub-images (512 × 512) for prediction.
    We used the model with 1:7 weight ratio as the optimal model to test the images.
    Fig. 7 shows a confusion matrix in the validation dataset (a) and in the test
    dataset (b). The IoU values of background and lesion classes in the test dataset
    are 0.996 and 0.386, respectively. The metrics in the test dataset are lower than
    in the validation dataset. Most background pixels (99.8%) are correctly classified
    from the confusion matrix. As the majority of pixels are leaf, belonging to background
    class, they can be easily classified based on the color differences with lesion
    class. Around 40% of lesion pixels were classified as being background class in
    the test dataset. The prediction examples are illustrated in Fig. 8. Generally,
    most lesions, marked as the red areas in the images, can be correctly segmented.
    Some tiny lesions were not manually labeled on the ground truth images, but were
    successfully segmented by the model (shown in #2 and #4 columns in Fig. 8). The
    prediction masks of sub-images were reconstructed back to the predicted images
    of the original test images (5472 × 3648). Two examples of the predicted images
    are shown in Fig. 9. There are some examples of failed cases in some predictions.
    For example, no disease lesions were visually observed on potato leaves (Fig.
    10). But 3 lesion areas were predicted by the model. The three false positives
    are all from soil patches which largely have similar shape features and areas
    as seen in typical lesions. Moreover, these soil patches are surrounded by leaves,
    resulting in more confusion for inference. Also, other wrongly predicted cases
    are located in the image border (#1 column in Fig. 8). An entire lesion can be
    cut up into two pieces when cropping an image into multiple sub-images. In this
    case, the partial lesion significantly changes the morphological features and
    loses the important neighbor pixel information for the model to predict. This
    might lead to these failure cases. Download : Download high-res image (190KB)
    Download : Download full-size image Fig. 7. Confusion matrix in the validation
    dataset (a) and in the test dataset (b). Download : Download high-res image (2MB)
    Download : Download full-size image Fig. 8. Examples of sub-image predictions
    (512 × 512) in the test dataset (row #1: raw sub-images, row #2: ground truth,
    row #3: predicted images). (For interpretation of the references to color in this
    figure legend, the reader is referred to the web version of this article.) Download
    : Download high-res image (2MB) Download : Download full-size image Fig. 9. Examples
    of the predicted raw images (5472 × 3648) in the test dataset and ground truth
    images (Left column: ground truth images, Right column: predicted images). Download
    : Download high-res image (2MB) Download : Download full-size image Fig. 10. False
    positives in a test image (5472 × 3648). 4.4. Model comparison We compared our
    network model with other deep learning-based models. The results are listed in
    Table 4. The optimal class weight (1:7) obtained by the experiment above (Table
    3) was used for training the other networks as well. We also tested a few representative
    images from the test dataset with conventional K-Means clustering. To be consistent
    with deep learning-based algorithms (two classes), we assigned K to be 2 for comparison.
    The segmentation results are displayed in Fig. 11. It shows that this method failed
    to recognize lesions in field images with too many false positives from soil background
    and dark leaves. The major issue for the K-Means clustering algorithm is a lack
    of utilizing spatial information for lesion recognition in our case. The other
    three models all outperform the baseline model FCN. The proposed SegNet-based
    network with the optimal class weight (1:7) obtained slightly better results in
    mIoU compared to PSPNet and DeepLab. Table 4. Results on the test dataset. Methods
    Overall accuracy Class average accuracy IoU (background) IoU (lesion) mIoU FCN
    [35] 0.998 0.789 0.994 0.298 0.644 PSPNet [36] 0.999 0.799 0.996 0.384 0.690 DeepLab
    [37] 0.999 0.795 0.996 0.379 0.688 Proposed 0.999 0.800 0.996 0.386 0.691 Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 11. Segmentation
    results based on K-Means clustering (row #1: raw sub-images, row #2: ground truth,
    row #3: results based on K-Means clustering). 4.5. Model validation in negative
    examples We also tested the generalization ability and validity of the model with
    some difficult images (negative examples) without P. infestans lesions but with
    tissue damages caused by biotic or abiotic stresses. These images were collected
    from different potato fields in the summer of 2020. The model did not predict
    any lesions in these images in Fig. 12. These damages were from various sources
    such as fertilization, herbicide and pathogens other than P. infestans. Fig. 13
    shows that the model failed to recognize the P. infestans lesions. Specifically,
    some lesions from Alternaria solani (Early Blight) were recognized as being P.
    infestans lesions. However, the model did not recognize damages in stems caused
    by leaf mold as P. infestans lesions, though they have similar color features
    as P. infestans lesions. Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 12. Correct prediction samples with damages of burning by
    lime nitrate (a); with deformity and necrosis caused by herbicide damage (b);
    with damage caused by eutrophication with lime nitrate (c); with infestation of
    possible gray mold (d). Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 13. Failure cases of P. infestans lesion recognition where
    the infections are caused either by some abiotic stress or other fungi (Left);
    and in an image with severe damages caused by leaf mold infestation (Right). 4.6.
    Correlation between visual scores and the number of lesions at the canopy level
    As the visual scores were obtained based on the rating of the whole plant, we
    only selected the unseen images that covered the full crop canopy to avoid bias
    due to partial view. There were 43 images selected in total. The histogram of
    visual scores is shown in Fig. 14. The average value of visual scores is 3% ranging
    from 0 to 40%. In order to minimize the failure cases raised from cropping, each
    image was predicted with multiple scales. The number of lesions in each image
    was obtained based on the majority voting algorithm described in the section of
    post-processing. The histogram of the final detected lesions is displayed in Fig.
    15. There were 1063 lesions detected in these test images. The mean value of the
    detected lesion areas was 892 pixels. The maximum and minimum values are 7520
    and 52 pixels, respectively. More than 40% of lesion areas are below 500 pixels,
    and very few lesion areas exceed 6000 pixels. This is consistent with the visual
    scores where around 80% of the scores are below 5%. A linear model was fitted,
    to quantify the relationship between visual scoring by an experienced plant breeder
    from the Danespo company and the number of lesions that appeared at canopy level.
    Fig. 16, Fig. 17 show the fitted linear relationships between visual scores and
    the number of lesions predicted from the 3 × 2 and 5 × 4 partitioning scales,
    respectively. Fig. 18 illustrates the fitted linear relationship between visual
    scores and the number of lesions obtained from majority voting of all scales.
    Compared to prediction masks with only one scale, the fused masks based on majority
    voting achieved a better linear relationship by increasing the R2 value from around
    0.4 to 0.655. Download : Download high-res image (346KB) Download : Download full-size
    image Fig. 14. Histogram of visual scores; the red bar represents the number of
    visual scores in that x axis range (left y axis), and the blue line represents
    a cumulative percentage of visual scores (right y axis). Download : Download high-res
    image (334KB) Download : Download full-size image Fig. 15. Histogram of lesion
    areas; the red bar represents the number of lesion areas in that x axis range
    (left y axis), and the blue line represents a cumulative percentage of lesion
    areas (right y axis). Download : Download high-res image (189KB) Download : Download
    full-size image Fig. 16. Fitted linear relationship between visual scores and
    number of lesions assigned from images from the 3 × 2 partitioning scale. Download
    : Download high-res image (178KB) Download : Download full-size image Fig. 17.
    Fitted linear relationship between visual scores and number of lesions assigned
    from images from the 6 × 5 partitioning scale. Download : Download high-res image
    (179KB) Download : Download full-size image Fig. 18. Fitted linear relationship
    between visual scores and number of lesions from majority voting from all partitioning
    scales. 5. Discussion Deep learning has demonstrated its superior performance
    on disease detection in field settings compared to conventional machine learning
    methods [15], [17]. We also demonstrate that a deep learning-based approach (Fig.
    8) generates better results than K-Means clustering (Fig. 11) for lesion segmentation
    in fields. The proposed SegNet-based network outperforms the PSPNet and DeepLab
    networks in our test dataset (Table 4), even though the PSPNet is capable of utilizing
    global spatial information for semantic segmentation. One possible reason is that
    the class weight used for all network trainings is fine-tuned for the proposed
    SegNet-based network. The datasets used for training and testing are still limited,
    and the rank of networks might also be influenced when the dataset is expanded
    to larger numbers of samples. Imbalance classes are a common problem in deep learning
    and is widely represented in the agricultural field, and thus hampers applications
    in for example distinction between weed and crop [10], [38], pest detection [11]
    and disease segmentation [15]. In these cases, pixels of one class, generally
    soil or crop, are dominant in images. Training a network in an appropriate way
    is critical for delivering a good segmentation result for each class. Table 3
    shows that assigning imbalance weights in the loss function can effectively mitigate
    the issue with imbalance classes, which is consistent with the conclusion by Yasrab
    et al. (2019) [39]. The mIoU value is only 0.551 with same weights across classes,
    while it can be improved to nearly 0.7 with an imbalanced weight assignment. The
    IoU value ( 0.99) of the background class (soil and crop plant) is far higher
    than the IoU value of disease lesion class. Based on the confusion matrix, it
    is concluded that the majority of lesion pixels ( 60%) were correctly classified.
    The relatively low IoU of lesion class ( 0.5) indicates that a few cases of false
    positives (Fig. 10) were from soil patches, which have similar shapes to lesions
    that were predicted as lesions by the model. Stewart et al. [15] also found this
    kind of false positives for Northern Leaf Blight (NLB) lesion segmentation in
    maize fields. Besides, senesced leaves could also contribute to the false positives.
    Setting a threshold to filter out some false positives, based on lesion size,
    might improve the IoU value of lesion class. This threshold value can be estimated
    based on the prior knowledge of the maximum lesion area in certain developmental
    stages of PLB disease or plant development, related to the start of the outbreak
    or even disease prediction based on weather and cultivar. It should, however,
    be noted that a lesion size threshold is of course dependent on images that are
    collected at a fixed height and a lesion size threshold will only work in the
    early stages of the infection as lesions will merge into larger areas as the disease
    progresses. This could further influence the correlation of visual scores and
    number of lesions. It is expected that this correlation can be improved when the
    estimated visual scores of the infested plants range from 5% to 20%. We drew a
    group of connected key points to define the lesion area when manually labeling
    images with the tool LabelMe. This way of annotating speeds up the pixel-wise
    labeling process for segmentation. However, it is difficult to always draw a very
    accurate boundary line with those key points especially since a majority of PLB
    lesions in our images are tiny and have an irregular shape. As a consequence,
    the labeled lesion areas at times inevitably include pixels belonging to the background
    class (soil and leaf), leading to the relatively low IoU of lesion class. To overcome
    this problem, Wiesner-Hanks et al. [16] discussed using crowdsourced data for
    NLB lesion detection at millimeter-level based on aerial visual images and concluded
    that increasing the number of workers per image could improve the quality of annotation
    polygons. Image preprocessing is essential before feeding images to train a neural
    network. It is encouraged to randomly apply blur, contrast and brightness as data
    augmentation for the benefit of model robustness. The training dataset with 1600
    labeled images from 31 potato genotypes is still limited and unlikely to include
    all lesion variations. We tested the generalization ability and validity of the
    model in recognition of P. infestans lesions with some images from other fields
    and conditions (Fig. 12). The failure cases featured highly similar color and
    morphological characteristics as P. infestans lesions still are represented. Including
    such failed images in the training dataset again can improve the performance [40].
    The creation of further lesion variations in the training datasets by synthetic
    lesion images could be a good way to improve model performances . To this end,
    Sun et al. [41] developed a conventional image processing algorithm to optimize
    synthetic lesion images obtained from a generative adversarial network (GAN).
    Cap et al. [42] proposed a LeafGAN algorithm for lesion image generations, which
    improved the diagnostic performance by 7.4%. The use of a majority voting to generate
    accurate lesion masks from multiple prediction scales, inspired from random forest
    machine learning algorithm described in [43], proved its effectiveness to establish
    a linear relationship between visual scores and number of lesions at canopy level
    (Fig. 18). Very few studies have tried to automatize the visual scoring in field
    environments for plant breeding. In reality, visual scores are evaluated based
    on the number of lesions and their areas on single leaflets at early infection
    stages, which brings difficulties with analysis based on 2D images at the canopy
    level. The lesion recognition is suggested to be further explored by three-dimensional
    (3D) imaging to obtain full plant structures and by employing the state-of-the-art
    network architectures [44], [45] for video semantic segmentation for real-time
    evaluation. In precision farming, it is necessary to detect PLB disease as early
    as possible to bring in appropriate measurements to avoid yield loss. As only
    RGB images were used in this analysis, pre-symptomatic detection of PLB is inevitable
    missed. For pre-symptomatic crop disease detection, hyperspectral measurements
    from spectroradiometers or spectral imaging sensors are generally employed [46].
    For example, Anderegg et al. [47] used an ASD FieldSpec spectroradiometer (350–2500
    nm) to measure wheat plants at a canopy level for Septoria Tritici Blotch (STB)
    disease detection and quantification. Gold et al. [48] measured contact leaf reflectance
    with a field spectrometer for pre-symptomatic PLB detection. For many applications
    [49], [50], [51], spectral imaging sensors are more popular than non-imaging hyperspectral
    sensors due to the additional capability of providing spatial information on shape,
    texture and color. Partial least square discriminant analysis (PLSDA) is generally
    used to process full spectral data [52]. Our study also has the potential to monitor
    the development of PLB disease after lesion appearance, which could be useful
    to screen for high PLB resistance potato genotypes from a diverse germplasm in
    precision breeding. In terms of PLB management in potato production, it is useful
    to acknowledge how early PLB should be detected after the appearance of lesions.
    To address this question, Wiik et al. [53] carried out trials over two years in
    2018 and 2019 to test the need of first intervention after the first visual symptoms
    were detected. The preliminary results showed that it is acceptable for farmers
    to apply a first spray with curative systematic fungicides after the discovery
    of first symptoms, corresponding to a very low 0.01% infection provided that it
    is sprayed more or less immediately, as it was shown that a first spray, delayed
    by 5 days later than first symptoms appear, was too late to stop the disease.
    0.01% infection corresponds to only 300 spots/ha if the disease is evenly spread
    over the field. Thus it is clear that for precision farming protection against
    PLB using only an RGB sensor, instead of hyperspectral sensing, requires very
    early detection of the symptoms. 6. Conclusion and future work In this study,
    we demonstrated the feasibility of using a deep learning algorithm based on an
    encoder–decoder architecture for potato late blight disease lesion semantic segmentation
    based on field images. The results show that the intersection over union (IoU)
    values of background (soil and leaf) and lesion classes in the test dataset are
    0.996 and 0.386, respectively. Assigning different weights for the imbalance class
    could improve the performance of the model. This work also presents the possibility
    of accurate lesion counting at the plant canopy level with the use of image alignment.
    A linear relationship between visual scoring and the number of lesions was established.
    We can also conclude that the fused masks obtained from majority voting of the
    masks predicted with multiple scales achieved a higher R2 value (0.655) compared
    to prediction with a single scale. The proposed methodology has the potential
    to monitor the lesion development under field conditions and evaluate the resistance
    of genotypes against potato late blight enabling more precise and automated potato
    breeding. This study will be followed up by further field tests and the model
    will continue to be tested in terms of robustness and accuracy by adding new field
    image datasets. The updated model will also be used to test images collected from
    different time points to predict area under disease progression curve (AUDPC).
    In addition, we will continuously update the models with new labeled datasets
    and synthetic images to improve the generalization ability. Multiple imaging sensors
    like multispectral and hyperspectral cameras hold promise to also detect and maybe
    even quantify pre-symptomatic disease. Also, the sensor combinations, e.g., a
    spectroradiometer (early stage) and high-resolution RGB camera (late stage), can
    be considered for monitoring PLB progression. Multimodal data fusion and machine
    learning are suggested to be fully exploited for this application. Furthermore,
    it would be interesting to explore new vehicle and sensor techniques to build
    three-dimensional imaging to be able to detect disease lesions below the canopy.
    CRediT authorship contribution statement Junfeng Gao: Conceptualization, Methodology,
    Software, Writing - original draft, Writing - review & editing. Jesper Cairo Westergaard:
    Data curation, Writing - review & editing. Ea Høegh Riis Sundmark: Resources,
    Writing - review & editing. Merethe Bagge: Resources, Writing - review & editing.
    Erland Liljeroth: Data curation, Writing - review & editing. Erik Alexandersson:
    Supervision, Project administration, Funding acquisition, Writing - review & editing.
    Declaration of Competing Interest The authors declare that they have no known
    competing financial interests or personal relationships that could have appeared
    to influence the work reported in this paper. Acknowledgments We acknowledge the
    Flemish Supercomputer Center (VSC) for providing the GPU computational resources
    and services for this work. We thank Mathieu Gremillet for field assistance, Hanne
    Grethe Kirk at Danespo for visual scoring of disease, and Linnea Almqvist from
    SLU for providing image examples in Fig. 12, Fig. 13. This research was founded
    by Nordic Council of Ministers, Copenahgen, Denmark (PPP #6P2), NordForsk, Norway
    (#84597) and Vinnova, Sweden (#2016-04386). References [1] Savary S., Ficke A.,
    Aubertot J.N., Hollier C. Crop losses due to diseases and their implications for
    global food production losses and food security Food Secur., 4 (2012), pp. 519-537,
    10.1007/s12571-012-0200-5 View in ScopusGoogle Scholar [2] Chawade A., Van Ham
    J., Blomquist H., Bagge O., Alexandersson E., Ortiz R. High-throughput field-phenotyping
    tools for plant breeding and precision agriculture Agronomy, 9 (2019), 10.3390/agronomy9050258
    Google Scholar [3] Mahlein A.K. Plant disease detection by imaging sensors – Parallels
    and specific demands for precision agriculture and plant phenotyping Plant Dis.,
    100 (2016), pp. 241-254, 10.1094/PDIS-03-15-0340-FE View in ScopusGoogle Scholar
    [4] Wiik L., Rosenqvist H., Liljeroth E. Study on biological and economic considerations
    in the control of potato late blight and potato tuber blight J. Hortic., 05 (2018),
    10.4172/2376-0354.1000226 Google Scholar [5] Eriksson D., Carlson-Nilsson U.,
    Ortíz R., Andreasson E. Overview and breeding strategies of table potato production
    in Sweden and the Fennoscandian region Potato Res., 59 (2016), pp. 279-294, 10.1007/s11540-016-9328-6
    View in ScopusGoogle Scholar [6] Colon L., Nielsen B., Darsow U. Field test for
    foilage blight resistance (2004) Google Scholar [7] Mai X., Meng M.Q.H. Automatic
    lesion segmentation from rice leaf blast field images based on random forest 2016
    IEEE Int. Conf. Real-Time Comput. Robot. RCAR 2016 (2016), pp. 255-259, 10.1109/RCAR.2016.7784035
    View in ScopusGoogle Scholar [8] LeCun Y.A., Bengio Y., Hinton G.E. Deep learning
    Nature, 521 (2015), pp. 436-444, 10.1038/nature14539 View in ScopusGoogle Scholar
    [9] Li Z., Guo R., Li M., Chen Y., Li G. A review of computer vision technologies
    for plant phenotyping Comput. Electron. Agric., 176 (2020), 10.1016/j.compag.2020.105672
    Google Scholar [10] Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters
    J.G. Deep convolutional neural networks for image-based Convolvulus sepium detection
    in sugar beet fields Plant Methods, 16 (2020), 10.1186/s13007-020-00570-z Google
    Scholar [11] Liu Z., Gao J., Yang G., Zhang H., He Y. Localization and classification
    of paddy field pests using a saliency map and deep convolutional neural network
    Sci. Rep., 6 (2016), p. 20410, 10.1038/srep20410 View in ScopusGoogle Scholar
    [12] Barth R., Hemming J., Van Henten E.J. Angle estimation between plant parts
    for grasp optimisation in harvest robots Biosyst. Eng., 183 (2019), pp. 26-46,
    10.1016/j.biosystemseng.2019.04.006 View PDFView articleView in ScopusGoogle Scholar
    [13] Ubbens J., Cieslak M., Prusinkiewicz P., Stavness I. The use of plant models
    in deep learning: An application to leaf counting in rosette plants Plant Methods,
    14 (2018), 10.1186/s13007-018-0273-z Google Scholar [14] Polder G., Blok P.M.,
    de Villiers H.A.C., van der Wolf J.M., Kamp J. Potato virus Y detection in seed
    potatoes using deep learning on hyperspectral images Front. Plant Sci., 10 (2019),
    pp. 1-13, 10.3389/fpls.2019.00209 Google Scholar [15] Stewart E.L., Wiesner-Hanks
    T., Kaczmar N., DeChant C., Wu H., Lipson H., Nelson R.J., Gore M.A. Quantitative
    phenotyping of northern leaf blight in UAV images using deep learning Remote Sens.,
    11 (2019), 10.3390/rs11192209 Google Scholar [16] Wiesner-Hanks T., Wu H., Stewart
    E., DeChant C., Kaczmar N., Lipson H., Gore M.A., Nelson R.J. Millimeter-level
    plant disease detection from aerial photographs via deep learning and crowdsourced
    data Front. Plant Sci., 10 (2019), 10.3389/fpls.2019.01550 Google Scholar [17]
    Wu H., Wiesner-Hanks T., Stewart E.L., DeChant C., Kaczmar N., Gore M.A., Nelson
    R.J., Lipson H. Autonomous detection of plant disease symptoms directly from aerial
    imagery Plant Phenome J., 2 (2019), pp. 1-9, 10.2135/tppj2019.03.0006 Google Scholar
    [18] Abdu A.M., Mokji M.M., Sheikh U.U. A pattern analysis-based segmentation
    to localize early and late blight disease lesions in digital images of plant leaves
    (2020), pp. 116-121, 10.1109/icsipa45851.2019.8977798 Google Scholar [19] Barbedo
    J.G.A. A new automatic method for disease symptom segmentation in digital photographs
    of plant leaves Eur. J. Plant Pathol., 147 (2017), pp. 349-364, 10.1007/s10658-016-1007-6
    View in ScopusGoogle Scholar [20] Camargo A., Smith J.S. An image-processing based
    algorithm to automatically identify plant disease visual symptoms Biosyst. Eng.,
    102 (2009), pp. 9-21, 10.1016/j.biosystemseng.2008.09.030 View PDFView articleView
    in ScopusGoogle Scholar [21] Fernández C.I., Leblon B., Haddadi A., Wang K., Wang
    J. Potato late blight detection at the leaf and canopy levels based in the red
    and red-edge spectral regions Remote Sens., 12 (2020), 10.3390/RS12081292 Google
    Scholar [22] Appeltans S., Guerrero A., Nawar S., Pieters J., Mouazen A.M. Practical
    recommendations for hyperspectral and thermal proximal disease sensing in potato
    and leek fields Remote Sens., 12 (2020), p. 1939, 10.3390/rs12121939 View in ScopusGoogle
    Scholar [23] Sugiura R., Tsuda S., Tamiya S., Itoh A., Nishiwaki K., Murakami
    N., Shibuya Y., Hirafuji M., Nuske S. Field phenotyping system for the assessment
    of potato late blight resistance using RGB imagery from an unmanned aerial vehicle
    Biosyst. Eng., 148 (2016), pp. 1-10, 10.1016/j.biosystemseng.2016.04.010 View
    PDFView articleView in ScopusGoogle Scholar [24] Franceschini M.H.D., Bartholomeus
    H., van Apeldoorn D.F., Suomalainen J., Kooistra L. Feasibility of unmanned aerial
    vehicle optical imagery for early detection and severity assessment of late blight
    in Potato Remote Sens., 11 (2019), 10.3390/rs11030224 Google Scholar [25] Duarte-Carvajalino
    J.M., Alzate D.F., Ramirez A.A., Santa-Sepulveda J.D., Fajardo-Rojas A.E., Soto-Suárez
    M. Evaluating late blight severity in potato crops using unmanned aerial vehicles
    and machine learning algorithms Remote Sens., 10 (2018), 10.3390/rs10101513 Google
    Scholar [26] Laflamme B., Middleton M., Lo T., Desveaux D., Guttman D.S. Image-based
    quantification of plant immunity and disease Mol. Plant Microbe Interact., 29
    (2016), pp. 919-924, 10.1094/MPMI-07-16-0129-TA View in ScopusGoogle Scholar [27]
    Karisto P., Hund A., Yu K., Anderegg J., Walter A., Mascher F., McDonald B.A.,
    Mikaberidze A. Ranking quantitative resistance to septoria tritici blotch in elite
    wheat cultivars using automated image analysis Phytopathology, 108 (2018), pp.
    568-581, 10.1094/PHYTO-04-17-0163-R View in ScopusGoogle Scholar [28] Bock C.H.,
    Barbedo J.G.A., Del Ponte E.M., Bohnenkamp D., Mahlein A.-K. From visual estimates
    to fully automated sensor-based measurements of plant disease severity: status
    and challenges for improving accuracy Phytopathol. Res., 2 (2020), 10.1186/s42483-020-00049-8
    Google Scholar [29] Leontine Colon U.D., Nielsen Bent Field test for foliage blight
    resistance (2004) Google Scholar [30] Russell B.C., Torralba A., Murphy K.P.,
    Freeman W.T. LabelMe: A database and web-based tool for image annotation Int.
    J. Comput. Vis., 77 (2008), pp. 157-173, 10.1007/s11263-007-0090-8 View in ScopusGoogle
    Scholar [31] Badrinarayanan V., Kendall A., Cipolla R. SegNet: A deep convolutional
    encoder-decoder architecture for image segmentation IEEE Trans. Pattern Anal.
    Mach. Intell., 39 (2017), pp. 2481-2495, 10.1109/TPAMI.2016.2644615 View in ScopusGoogle
    Scholar [32] Eigen D., Fergus R. Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture Proc. IEEE Int. Conf.
    Comput. Vis. (2015), pp. 2650-2658, 10.1109/ICCV.2015.304 View in ScopusGoogle
    Scholar [33] Kingma D.P., Ba J. Adam: A method for stochastic optimization Int.
    Conf. Learn. Represent (2015), pp. 1-15 https://arxiv.org/abs/arXiv:1412.6980v9
    Google Scholar [34] Krähenbühl P., Koltun V. Efficient inference in fully connected
    CRFs with Gaussian edge potentials Shawe-Taylor J., Zemel R.S., Bartlett P.L.,
    Pereira F., Weinberger K.Q. (Eds.), Adv. Neural Inf. Process. Syst., Vol. 24,
    Curran Associates, Inc. (2011), pp. 109-117 http://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf
    Google Scholar [35] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks
    for semantic segmentation, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
    Recognit., 2015, pp. 431–440, http://dx.doi.org/10.1109/CVPR.2015.7298965. Google
    Scholar [36] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, Pyramid scene parsing network,
    in: Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, 2017,
    pp. 6230–6239, http://dx.doi.org/10.1109/CVPR.2017.660. Google Scholar [37] Chen
    L.-C., Zhu Y., Papandreou G., Schroff F., Aug C.V., Adam H. deeplabv3+: Encoder-decoder
    with atrous separable convolution for semantic image segmentation Proc. Eur. Conf.
    Comput. Vis. (2018), pp. 801-818 https://arxiv.org/pdf/1802.02611.pdf CrossRefGoogle
    Scholar [38] Gao J., Liao W., Nuyttens D., Lootens P., Vangeyte J., Pižurica A.,
    He Y., Pieters J.G. Fusion of pixel and object-based features for weed mapping
    using unmanned aerial vehicle imagery Int. J. Appl. Earth Obs. Geoinformation,
    67 (2018), pp. 43-53, 10.1016/j.jag.2017.12.012 View PDFView articleView in ScopusGoogle
    Scholar [39] Yasrab R., Atkinson J.A., Wells D.M., French A.P., Pridmore T.P.,
    Pound M.P. RootNav 2.0: Deep learning for automatic navigation of complex plant
    root architectures Gigascience, 8 (2019), pp. 1-16, 10.1093/gigascience/giz123
    Google Scholar [40] Huang Z., Sklar E., Parsons S. Design of automatic strawberry
    harvest robot suitable in complex environments ACM/IEEE Int. Conf. Human-Robot
    Interact. (2020), pp. 567-569, 10.1145/3371382.3377443 View in ScopusGoogle Scholar
    [41] Sun R., Zhang M., Yang K., Liu J. Data enhancement for plant disease classification
    using generated lesions Appl. Sci., 10 (2020), 10.3390/app10020466 Google Scholar
    [42] Cap Q.H., Uga H., Kagiwada S., Iyatomi H. LeafGAN: An effective data augmentation
    method for practical plant disease diagnosis (2020) arXiv Prepr. arXiv:2002.10100
    Google Scholar [43] Breiman L. Random forests Mach. Learn., 45 (2001), pp. 5-32,
    10.1023/A:1010933404324 Google Scholar [44] Miao J., Wei Y., Yang Y. Memory aggregation
    networks for efficient interactive video object segmentation CVPR (2020), pp.
    10363-10372, 10.1109/cvpr42600.2020.01038 View in ScopusGoogle Scholar [45] Yang
    Z., Wei Y., Yang Y. Collaborative video object segmentation by foreground-background
    integration (2020) arXiv Prepr. arXiv:2003.08333 Google Scholar [46] Mahlein A.-K.,
    Kuska M.T., Behmann J., Polder G., Walter A. Hyperspectral sensors and imaging
    technologies in phytopathology: State of the art Annu. Rev. Phytopathol., 56 (2018),
    pp. 535-558, 10.1146/annurev-phyto-080417-050100 View in ScopusGoogle Scholar
    [47] Anderegg J., Hund A., Karisto P., Mikaberidze A. In-field detection and quantification
    of septoria tritici blotch in diverse wheat germplasm using spectral–temporal
    features Front. Plant Sci., 10 (2019), 10.3389/fpls.2019.01355 Google Scholar
    [48] Gold K.M., Townsend P.A., Chlus A., Herrmann I., Couture J.J., Larson E.R.,
    Gevens A.J. Hyperspectral measurements enable pre-symptomatic detection and differentiation
    of contrasting physiological effects of late blight and early blight in potato
    Remote Sens., 12 (2020), pp. 1-21, 10.3390/rs12020286 Google Scholar [49] Gao
    J., Li X., Zhu F., He Y. Application of hyperspectral imaging technology to discriminate
    different geographical origins of Jatropha curcas L. seeds Comput. Electron. Agric.,
    99 (2013), pp. 186-193 View PDFView articleView in ScopusGoogle Scholar [50] Gao
    J., Nuyttens D., Lootens P., He Y., Pieters J.G. Recognising weeds in a maize
    crop using a random forest machine-learning algorithm and near-infrared snapshot
    mosaic hyperspectral imagery Biosyst. Eng., 170 (2018), pp. 39-50, 10.1016/j.biosystemseng.2018.03.006
    View PDFView articleView in ScopusGoogle Scholar [51] Appeltans S., Guerrero A.,
    Nawar S., Pieters J., Mouazen A.M. Practical recommendations for hyperspectral
    and thermal proximal disease sensing in potato and leek fields Remote Sens., 12
    (2020), p. 1939, 10.3390/rs12121939 View in ScopusGoogle Scholar [52] Yu K., Anderegg
    J., Mikaberidze A., Karisto P., Mascher F., McDonald B.A., Walter A., Hund A.
    Hyperspectral canopy sensing of wheat septoria tritici blotch disease Front. Plant
    Sci., 9 (2018), 10.3389/fpls.2018.01195 Google Scholar [53] Wiik L., Nilsson M.,
    Aldén L., Gerdtsson A., Didymus L.G.-B., Liljeroth E. Sweden Attempts Trial Report
    2019 (2019) https://sverigeforsoken.se/trialbook Google Scholar Cited by (56)
    Cross-domain transfer learning for weed segmentation and mapping in precision
    farming using ground and UAV images 2024, Expert Systems with Applications Show
    abstract Prediction performance and reliability evaluation of three ginsenosides
    in Panax ginseng using hyperspectral imaging combined with a novel ensemble chemometric
    model 2024, Food Chemistry Show abstract Deep convolutional feature aggregation
    for fine-grained cultivar recognition 2023, Knowledge-Based Systems Show abstract
    Towards robust registration of heterogeneous multispectral UAV imagery: A two-stage
    approach for cotton leaf lesion grading 2023, Computers and Electronics in Agriculture
    Show abstract Land use/land cover classification using hyperspectral soil reflectance
    features in the Eastern Himalayas, India 2023, Catena Show abstract Quaternion
    convolutional neural networks for hyperspectral image classification 2023, Engineering
    Applications of Artificial Intelligence Show abstract View all citing articles
    on Scopus View Abstract © 2021 Elsevier B.V. All rights reserved. Recommended
    articles VirLeafNet: Automatic analysis and viral disease diagnosis using deep-learning
    in plant Ecological Informatics, Volume 61, 2021, Article 101197 Rakesh Chandra
    Joshi, …, Nandlal Choudhary View PDF Cross-domain image translation with a novel
    style-guided diversity loss design Knowledge-Based Systems, Volume 255, 2022,
    Article 109731 Tingting Li, …, Keqin Li View PDF Field phenotyping system for
    the assessment of potato late blight resistance using RGB imagery from an unmanned
    aerial vehicle Biosystems Engineering, Volume 148, 2016, pp. 1-10 Ryo Sugiura,
    …, Stephen Nuske View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 45 Captures Readers: 117 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Knowledge-based systems
  limitations: '>'
  pdf_link: null
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: Automatic late blight lesion recognition and severity quantification based
    on field imagery of diverse potato genotypes by deep learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agronomy10010140
  analysis: '>'
  authors:
  - Deepak Gautam
  - Vinay Pagay
  citation_count: 36
  full_citation: '>'
  full_text: ">\nagronomy\nReview\nA Review of Current and Potential Applications\
    \ of\nRemote Sensing to Study the Water Status of\nHorticultural Crops\nDeepak\
    \ Gautam\nand Vinay Pagay *\nSchool of Agriculture, Food and Wine, The University\
    \ of Adelaide, PMB 1, Glen Osmond, SA 5064, Australia;\ndeepak.gautam@adelaide.edu.au\n\
    * Correspondence: vinay.pagay@adelaide.edu.au; Tel.: +61-8-83130773\nReceived:\
    \ 25 August 2019; Accepted: 9 January 2020; Published: 17 January 2020\n\x01\x02\
    \x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\nAbstract: With increasingly\
    \ advanced remote sensing systems, more accurate retrievals of crop water\nstatus\
    \ are being made at the individual crop level to aid in precision irrigation.\
    \ This paper summarises\nthe use of remote sensing for the estimation of water\
    \ status in horticultural crops. The remote\nmeasurements of the water potential,\
    \ soil moisture, evapotranspiration, canopy 3D structure, and\nvigour for water\
    \ status estimation are presented in this comprehensive review. These parameters\n\
    directly or indirectly provide estimates of crop water status, which is critically\
    \ important for irrigation\nmanagement in farms. The review is organised into\
    \ four main sections: (i) remote sensing platforms;\n(ii) the remote sensor suite;\
    \ (iii) techniques adopted for horticultural applications and indicators of\n\
    water status; and, (iv) case studies of the use of remote sensing in horticultural\
    \ crops. Finally, the\nauthors’ view is presented with regard to future prospects\
    \ and research gaps in the estimation of the\ncrop water status for precision\
    \ irrigation.\nKeywords: UAS; UAV; drone; unmanned; satellite; water stress; irrigation;\
    \ vegetation index\n1. Introduction\nUnderstanding the water status of crops is\
    \ important for optimal management and application\nof water to accommodate for\
    \ inter and intra-ﬁeld variability to achieve a speciﬁc target, such as\nmaximum\
    \ water use eﬃciency, yield, quality, or proﬁtability [1,2]. The importance of\
    \ optimal water\nmanagement in agriculture in semi-arid or arid regions has become\
    \ increasingly important in light\nof recent water scarcities through reduced\
    \ allocations, as well as increased demand due to greater\nareas under production\
    \ [3,4]. Climate change is expected to further intensify the situation due to\n\
    the increased frequency of heatwaves and drought episodes [5]. Climate change\
    \ coupled with the\nnecessity to increase food production due to an increase in\
    \ global population has placed pressure on\nhorticultural sector to improve eﬃciencies\
    \ in resources use, e.g., water, for sustainable farming [6–10].\nHorticultural\
    \ crops will have to produce more ‘crop-per-drop’ in the face of limited water\
    \ resources.\nInformed management of water resources whilst maintaining or increasing\
    \ crop quality and yield are\nthe primary goals of irrigation scheduling in horticulture.\
    \ These goals can be achieved by improving\nour understanding of the water status\
    \ of the crops at key phenological stages of development.\nTraditional decision-making\
    \ for irrigation of horticultural crops includes using information from\na combination\
    \ of sources such as historical regimes, soil moisture measurements, visual assessments\
    \ of\nsoil and/or crop, weather data including evapotranspiration (ET), and measurements\
    \ of crop water\nstatus using direct-, proximal- or remote-sensing techniques\
    \ [11–13]. Some growers undertake routine\nground-based measurements, e.g., pressure\
    \ chamber, for estimation of crop water status to make\ndecisions on irrigation\
    \ [14–16]. These ground-based measurements are robust; however, destructive,\n\
    Agronomy 2020, 10, 140; doi:10.3390/agronomy10010140\nwww.mdpi.com/journal/agronomy\n\
    Agronomy 2020, 10, 140\n2 of 35\ncumbersome, and expensive to acquire a reasonable\
    \ amount of data [14,16–18]. Consequently, the\nmeasured leaf is assumed to represent\
    \ the average population of leaves of the individual crop, and\na few crops are\
    \ assumed to represent the average population of the entire irrigation block.\
    \ As a\nresult, over- or under-watering can occur, which can lower yield and fruit\
    \ quality [19–22]. This is\nespecially evident for non-homogenous blocks where\
    \ spatial variability of soil and water status is\nexpected [23–25].\nTo address\
    \ some of the limitations of ground-based measurements, remote measurement\ntechniques\
    \ were introduced with capabilities to measure at higher spatial resolution, larger\
    \ area, and\non a regular basis [26–29]. Remote sensing, in particular, unmanned\
    \ aircraft systems (UAS), presents a\nﬂexible platform to deploy on-demand sensors\
    \ as a tool to eﬃciently and non-destructively measure\ncrop water status [30].\
    \ Using thermal and spectral signatures, remote sensing techniques can be used\n\
    to characterise a crop’s water status. Knowledge of crop water status allows growers\
    \ to more eﬃciently\nschedule irrigation (i.e., when and how much water to apply).\
    \ In this regard, UAS platforms provide a\nconvenient methodology to monitor the\
    \ water status across a farm, both spatially and temporally at\nthe canopy level\
    \ [31–33]. The spectral, spatial, and temporal ﬂexibility oﬀered by UAS-based\
    \ remote\nsensing may in future assist growers in irrigation decision-making [34,35].\n\
    This review provides an overview of the application of remote sensing to understand\
    \ the\ncrop’s water status (e.g., leaf/stem water potential, leaf/canopy conductance),\
    \ soil moisture, ET, and\nphysiological attributes, all of which can contribute\
    \ to understanding the crop’s water status to\nimplement precision irrigation.\
    \ Although the key focus of this review is UAS-based remote sensing,\na comparison\
    \ has been undertaken with other remote sensing platforms, such as earth observation\n\
    satellites, which are being increasingly used to acquire similar information.\
    \ In the following sections,\nwe provide an overview of the most common remote\
    \ sensing platforms in horticulture, various sensors\nused for remote sensing,\
    \ and several predictive indices of crop water status. Two case studies of remote\n\
    sensing in horticultural crops, grapevine and almond, are then presented followed\
    \ by an overview of\nthe current research gaps and future prospects.\n2. Remote\
    \ Sensing Platforms\nGround-based direct or proximal sensors acquire instantaneous\
    \ water status measurement from\na spatial location. For decision-making purposes,\
    \ the data is generally collected from multiple\nlocations across a ﬁeld, which\
    \ allows geospatial interpolation, such as kriging, to be applied [36–38].\nThis\
    \ scale of data collection is, however, cumbersome, ineﬃcient, and error-prone,\
    \ especially for water\nstatus measurements of large areas [17]. Monitoring and\
    \ observing farms at a larger spatial scale\nprompted the launch of several earth\
    \ observation satellite systems that typically operate at an altitude\nof 180–2000\
    \ km [39]. Manned high-altitude aircraft (operating within few km) and, more recently,\
    \ UAS\n(operating under 120 m) ﬁlled the spatial gap between high-resolution ground\
    \ measurements and\nrelatively low-resolution satellite measurements [40,41].\
    \ In the context of water status estimation for\nhorticultural crops, all the\
    \ aforementioned remote sensing platforms are utilised depending on the\nuser\
    \ requirements [23,42,43]. Each remote sensing platform has its own advantage\
    \ and shortcomings.\nThe decision to obtain remote sensing crop water status data\
    \ from one or more of these platforms will\ndepend on the spatial and temporal\
    \ resolution desired. Satellite and manned aircraft can be useful\nfor regional-scale\
    \ characterisation, whereas UAS can be more useful to map the intra-ﬁeld variability.\n\
    Vehicle-based ground systems also possess similar measurement capabilities, like\
    \ remote sensing,\nhowever, at a smaller scale [44,45]. These systems can move\
    \ within the horticultural rows obtaining\nwater status measurements of adjacent\
    \ plants while the vehicle is moving, enabling them to cover a\nrelatively larger\
    \ area as compared to ground-based direct measurements [46–48].\n2.1. Satellite\
    \ Systems\nThe use of satellite systems for remote sensing started with the launch\
    \ of Landsat-1 in 1972 [39,49].\nThe subsequent launch of SPOT-1 in 1986 and Ikonos\
    \ in 1999 opened the era of commercial satellite\nAgronomy 2020, 10, 140\n3 of\
    \ 35\nsystems that resulted in rapid improvement in imaging performance, including\
    \ spatial and spectral\nresolution [50]. Continued launch of satellites from the\
    \ same families, with newer sensor models\nand improved capability, resulted in\
    \ the formation of satellite constellations (e.g., Landsat, Sentinel,\nSPOT, RapidEye,\
    \ GeoEye/WorldView families). The satellite constellation substantially improved\
    \ the\nrevisit cycle of the satellite system [51]. Recently, the miniature form\
    \ of the satellite termed Nanosat or\nCubesat has been developed, which can be\
    \ deployed on the same orbit in a large number (20s–100s),\nenabling frequent\
    \ and high-resolution data acquisition (e.g., Dove satellite from Planet Labs)\
    \ [52].\nThe earth observation satellite system, such as Landsat, Sentinel, MODIS,\
    \ RapidEye, and GeoEye,\nhave been used to study horticultural crops (Table 1).\
    \ These satellite system oﬀer camera systems\nwith spectral bands readily available\
    \ in visible, near infrared (NIR), short-wave infrared (SWIR), and\nthermal infrared\
    \ (TIR). The measurement in these bands provides opportunities to study a crop’s\
    \ water\nstatus indirectly via, for example, calculation of the normalised diﬀerence\
    \ vegetation index (NDVI),\ncrop water stress index (CWSI), and ET [8–10] at the\
    \ ﬁeld- and regional-scales.\nTable 1. Some satellite systems that have been used\
    \ to study the water status of horticultural crops.\nSatellites\nBand Numbers:\
    \ Band Designation\nSpatial Resolution (m)\nRevisit Cycle\nLandsat 7\n8: V 3,\
    \ NIR 1, SWIR 2, TIR 1, Pan 1\n15–60\n16 days\nLandsat 8\n11: C 1, V 3, NIR 1,\
    \ SWIR 2, Pan 1, Ci 1, TIR 2\n15–100\n16 days\nSentinel-2\n13: C 1, V 3, RE 3,\
    \ NIR 2, WV 1, Ci 1, SWIR 2\n10–60\n5 days\nSpot-6 and-7\n5: Pan 1, V 3, NIR 1\n\
    1.5\n1 day\nRapidEye\n5: V 3, NIR 1, RE 1\n5\n5.5 days\nGeoEye-1\n5: Pan 1, V\
    \ 3, NIR 1\n0.41–2\n3 days\nNote: Superscript integers 1, 2, 3 represent the number\
    \ of bands; V = visible, NIR = near infrared, SWIR = short-wave\ninfrared, TIR\
    \ = thermal infrared, Pan = panchromatic, C = coastal, Ci = cirrus, RE = red edge,\
    \ WV = water vapour.\nThe reﬂected/emitted electromagnetic energy from the crop\
    \ reaching the sensor is recorded at a\nspeciﬁc wavelength. The width of the observed\
    \ wavelength expressed in full width at half maximum\n(FWHM) is called spectral\
    \ resolution. The number of observed bands and the spectral resolution\nindicates\
    \ the ability of the satellite to resolve spectral features on the earth’s surface.\
    \ Commonly used\nearth observation satellite systems possess between four and\
    \ 15 bands with approximately 20–200 nm\nFWHM spectral resolution. The bands are\
    \ generally designated for the visible and NIR region with\nextended capabilities\
    \ in SWIR, TIR, as well as red edge region (Table 1). The most widely used band\n\
    combinations to study the water status of vegetation are the visible, NIR and\
    \ TIR bands [23,25,53,54].\nWith the plethora of satellite systems currently available,\
    \ user requirements on band combination\nmay be achieved by using multiple satellites.\
    \ However, acquiring an extra or a narrower band to the\nexisting capabilities\
    \ is not possible.\nThe ground distance covered per pixel of the satellite image\
    \ is called the spatial resolution,\nwhereby, a higher spatial resolution indicates\
    \ a smaller ground distance. Existing satellite systems,\ndue to their lower spatial\
    \ resolution and large coverage, are suited to study larger regions [55]. For\
    \ a\nsmaller observation area, such as a farm block, an irrigation zone, a single\
    \ row of the horticultural crop,\nor a single canopy, this spatial resolution\
    \ is considered sub-optimal. Often, a pixel of the satellite image\ncomprises\
    \ of multiple rows and multiple canopies of horticultural crops [42,56]. Thus,\
    \ the spectral\nresponse on a single pixel of the satellite image includes a mixed\
    \ spectral signal from the canopy,\ninter-row vegetation and/or bare soil. The\
    \ mixed-pixel is particularly unavoidable in horticultural\ncrops with large inter-row\
    \ surfaces, introducing errors in satellite-based estimations [42,56]. Improving\n\
    the spatial resolution from freely available Landsat/Sentinel satellites (spatial\
    \ resolution 10–15 m) to\nsuch as WorldView-3 (spatial resolution 0.3 m), does\
    \ not necessarily resolve single canopies of many\nhorticultural crops.\nCurrent\
    \ satellite systems generally oﬀer a temporal resolution of about 1–2 weeks this\
    \ resolution\ncorresponds to the satellite’s revisit interval (Table 1). For example,\
    \ freely available Landsat-8 and\nSentinel-2 oﬀer revisit cycles of 16 and 5 days,\
    \ respectively. Although the MODIS sensor on NASA’s\nAgronomy 2020, 10, 140\n\
    4 of 35\nTerra and Aqua satellites oﬀer a greater temporal resolution (1–2 days),\
    \ its spatial resolution is relatively\ncoarse (250 m–1 km) to be valuable for\
    \ horticulture [25]. The revisit cycle of satellites does not alone\nrepresent\
    \ the timeframe on which the data can be interpreted. For instance, post-data\
    \ acquisition,\nthere are often delays in data transfer to the ground station,\
    \ handling, and delivery to the end user.\nThe end user then needs to process\
    \ the data before making an interpretation. Such processing can\nbe a combination\
    \ of atmospheric, radiometric, and geometric corrections, where applicable [57,58].\n\
    Furthermore, as the agricultural applications of the satellite imagery are illumination\
    \ sensitive and\nweather dependent, conditions have to be optimal on the satellite\
    \ revisit day to avoid data corruption\ndue to, for example, cloud cover [23,53].\
    \ Cloud corrupted data (~55% of the land area is covered by\ncloud at any one\
    \ time [59]) will require users to wait for the next revisit to attempt the data\
    \ acquisition.\nTime-series image fusion techniques, such as the spatial and temporal\
    \ adaptive reﬂectance fusion\nmodel, can improve the spatial and temporal resolution\
    \ of the satellite data [60,61]. These fusion\ntechniques blend the frequent (however\
    \ low-resolution) with higher-resolution (but infrequent) satellite\ndata [62,63].\
    \ The result combines the best aspects of multiple satellite systems to produce\
    \ frequent and\nhigher-resolution data, which can be useful for timely monitoring\
    \ of water status.\nThe clear advantage of the satellite system is the ability\
    \ to capture data at a large scale and at an\naﬀordable cost (e.g., the user can\
    \ download Landsat and Sentinel data for free). The compromise with\nthe satellite\
    \ data is in spatial resolution, as well as the relatively long revisit cycle\
    \ (in the order of days\nto weeks), making the data less than ideal for speciﬁc\
    \ applications, e.g., irrigation scheduling.\n2.2. Manned Aircraft System\nOperating\
    \ within few kilometres above ground level, manned aircraft have been used to\
    \ remotely\nacquire agricultural data at higher spatial detail (compared to the\
    \ satellites) and over a larger region\n(compared to UAS) [42,64]. Light ﬁxed-wing\
    \ aircraft and helicopters are the commonly used manned\naircraft employed in\
    \ agricultural remote sensing. The ﬁxed-wing aircraft generally ﬂies higher and\n\
    faster, enabling the coverage of a larger area, whereas the helicopters are traditionally\
    \ ﬂown lower\nand slower, enabling a spatially detailed observation. A signiﬁcant\
    \ advantage of the manned aircraft,\ncompared to UAS, lies in their ability to\
    \ carry heavier high-grade sensors, such as AVIRIS, HyPlant,\nHySpex SWIR-384,\
    \ Specim AisaFENIX, and Riegl LMS Q240i-60 [65–67]. The use of manned aircraft\
    \ is,\nhowever, limited by high operational complexity, safety regulations, scheduling\
    \ inﬂexibility, costs, and\nproduct turnaround time. As a result, these platforms\
    \ are barely used as compared to the recent surge\nin the use of UAS, speciﬁcally\
    \ for horticultural crops [68–70].\nIn horticulture, manned aircraft was used\
    \ to characterise olive and peach canopy temperature\nand water stress using speciﬁc\
    \ thermal bands (10.069 µm and 12.347 µm) of a wideband (0.43–12.5 µm)\nairborne\
    \ hyperspectral camera system [71,72]. This work found moderate correlations (R2\
    \ = 0.45–0.57)\nof ground vs. aerial olive canopy temperature measurements [72],\
    \ and high correlations (R2 = 0.94)\nof canopy temperature vs. peach fruit size\
    \ (diameter) [71]. The advantage of manned aircraft for\nremote sensing of a large\
    \ region was highlighted in recent work that characterised regional-scale\ngrapevine\
    \ (Vitis vinifera L.) water stress responses of two cultivars, Shiraz and Cabernet\
    \ Sauvignon, in\nAustralia [64]. Airborne thermal imaging was able to discriminate\
    \ between the two cultivars based on\ntheir water status responses to soil moisture\
    \ availability (Figure 1).\nAgronomy 2020, 10, 140\n5 of 35\nAgronomy 2020, 10,\
    \ 140 \n5 of 35 \n \n \nFigure 1. Water status of Shiraz and Cabernet Sauvignon\
    \ under similar soil moisture as captured from \nmanned aircraft [64]. \n2.3.\
    \ Unmanned Aircraft Systems \nBoth the fixed-wing and the rotary-wing variant\
    \ of UASs are used in agricultural remote \nsensing. Each variant has its advantages\
    \ and shortcomings vis-à-vis sensor payload, flexibility, and \ncoverage. In this\
    \ regard, the literature provides a list of state-of-the-art UAS [73], their categorisation\
    \ \n[74], and overview of structural characteristics, as well as flight parameters\
    \ [75], in the context of \nagricultural use. Depending on the number of rotors,\
    \ a rotary-wing UAS can be a helicopter, a \nquadcopter, a hexacopter, or an octocopter,\
    \ among others. Rotary-wing UAS are more agile and can \nfly with a higher degree\
    \ of freedom [76], while fixed-wing UAS needs to be moving forward at a \ncertain\
    \ speed to maintain thrust. As a result, rotary-wing UAS provides flexibility\
    \ and specific \ncapabilities, such as hovering, vertical take-off and landing,\
    \ vertical (up and down) motions, or return \nto the previous location. On the\
    \ contrary, fixed-wing UAS fly faster, carry heavier payloads, and have \ngreater\
    \ flying time enabling coverage of larger areas in a single flight [77]. Recently\
    \ developed fixed-\nwing UAS with vertical take-off and landing capabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse \nVTOL-PPK, and Carbonix Volanti, captures\
    \ the pros of both fixed-wing and rotary-wing, making \nthem a promising platform\
    \ for agricultural purposes. In the context of precision agriculture, the \napplication\
    \ of UAS, their future prospects, and knowledge gaps are discussed in [53,78–81].\
    \ While \nmany horticultural crops have been studied using UAS technology, the\
    \ most studied horticultural \ncrops are vineyards [31,82–84], citrus [85,86],\
    \ peach [32,33], olive [18,87,88], pistachio [89,90], and \nalmond [91–94], among\
    \ others [95–99]. Some of the UAS types used for water status studies of \nhorticultural\
    \ crops are shown in Figure 2. \n \n \n(a) \n(b) \nFigure 1. Water status of Shiraz\
    \ and Cabernet Sauvignon under similar soil moisture as captured from\nmanned\
    \ aircraft [64].\n2.3. Unmanned Aircraft Systems\nBoth the ﬁxed-wing and the rotary-wing\
    \ variant of UASs are used in agricultural remote sensing.\nEach variant has its\
    \ advantages and shortcomings vis-à-vis sensor payload, ﬂexibility, and coverage.\n\
    In this regard, the literature provides a list of state-of-the-art UAS [73], their\
    \ categorisation [74], and\noverview of structural characteristics, as well as\
    \ ﬂight parameters [75], in the context of agricultural use.\nDepending on the\
    \ number of rotors, a rotary-wing UAS can be a helicopter, a quadcopter, a hexacopter,\n\
    or an octocopter, among others. Rotary-wing UAS are more agile and can ﬂy with\
    \ a higher degree of\nfreedom [76], while ﬁxed-wing UAS needs to be moving forward\
    \ at a certain speed to maintain thrust.\nAs a result, rotary-wing UAS provides\
    \ ﬂexibility and speciﬁc capabilities, such as hovering, vertical\ntake-oﬀ and\
    \ landing, vertical (up and down) motions, or return to the previous location.\
    \ On the contrary,\nﬁxed-wing UAS ﬂy faster, carry heavier payloads, and have\
    \ greater ﬂying time enabling coverage of\nlarger areas in a single ﬂight [77].\
    \ Recently developed ﬁxed-wing UAS with vertical take-oﬀ and landing\ncapabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse VTOL-PPK, and Carbonix Volanti, captures\n\
    the pros of both ﬁxed-wing and rotary-wing, making them a promising platform for\
    \ agricultural\npurposes. In the context of precision agriculture, the application\
    \ of UAS, their future prospects,\nand knowledge gaps are discussed in [53,78–81].\
    \ While many horticultural crops have been studied\nusing UAS technology, the\
    \ most studied horticultural crops are vineyards [31,82–84], citrus [85,86],\n\
    peach [32,33], olive [18,87,88], pistachio [89,90], and almond [91–94], among\
    \ others [95–99]. Some of\nthe UAS types used for water status studies of horticultural\
    \ crops are shown in Figure 2.\nAgronomy 2020, 10, 140 \n5 of 35 \n \n \nFigure\
    \ 1. Water status of Shiraz and Cabernet Sauvignon under similar soil moisture\
    \ as captured from \nmanned aircraft [64]. \n2.3. Unmanned Aircraft Systems \n\
    Both the fixed-wing and the rotary-wing variant of UASs are used in agricultural\
    \ remote \nsensing. Each variant has its advantages and shortcomings vis-à-vis\
    \ sensor payload, flexibility, and \ncoverage. In this regard, the literature\
    \ provides a list of state-of-the-art UAS [73], their categorisation \n[74], and\
    \ overview of structural characteristics, as well as flight parameters [75], in\
    \ the context of \nagricultural use. Depending on the number of rotors, a rotary-wing\
    \ UAS can be a helicopter, a \nquadcopter, a hexacopter, or an octocopter, among\
    \ others. Rotary-wing UAS are more agile and can \nfly with a higher degree of\
    \ freedom [76], while fixed-wing UAS needs to be moving forward at a \ncertain\
    \ speed to maintain thrust. As a result, rotary-wing UAS provides flexibility\
    \ and specific \ncapabilities, such as hovering, vertical take-off and landing,\
    \ vertical (up and down) motions, or return \nto the previous location. On the\
    \ contrary, fixed-wing UAS fly faster, carry heavier payloads, and have \ngreater\
    \ flying time enabling coverage of larger areas in a single flight [77]. Recently\
    \ developed fixed-\nwing UAS with vertical take-off and landing capabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse \nVTOL-PPK, and Carbonix Volanti, captures\
    \ the pros of both fixed-wing and rotary-wing, making \nthem a promising platform\
    \ for agricultural purposes. In the context of precision agriculture, the \napplication\
    \ of UAS, their future prospects, and knowledge gaps are discussed in [53,78–81].\
    \ While \nmany horticultural crops have been studied using UAS technology, the\
    \ most studied horticultural \ncrops are vineyards [31,82–84], citrus [85,86],\
    \ peach [32,33], olive [18,87,88], pistachio [89,90], and \nalmond [91–94], among\
    \ others [95–99]. Some of the UAS types used for water status studies of \nhorticultural\
    \ crops are shown in Figure 2. \n \n \n(a) \n(b) \nFigure 2. Cont.\nAgronomy 2020,\
    \ 10, 140\n6 of 35\nAgronomy 2020, 10, 140 \n6 of 35 \n \n \n \n(c) \n(d) \nFigure\
    \ 2. Examples of unmanned aircraft systems (UAS) used to study water status in\
    \ horticulture \ncrops: (a) hexacopter equipped with RGB, multispectral and thermal\
    \ camera at The University of \nAdelaide, Adelaide, Australia (b) quadcopter equipped\
    \ with a thermal and multispectral camera \n[100], (c) fixed-wing aircraft used\
    \ for GRAPEX project to carry RGB, thermal and monochrome camera \nwith narrowband\
    \ filters [101], and (d) helicopter used for various studies of crop water status\
    \ \n[18,92,102]. \nUAS offers flexibility on spatial resolution, observation scale,\
    \ spectral bands, and temporal \nresolution to collect data on any good weather\
    \ day. However, like satellite and manned aircraft, the \nUAS is inoperable during\
    \ precipitation, high winds, and temperatures. By easily altering the flying \n\
    altitude, the UAS provides higher flexibility to observe a larger area with lower\
    \ spatial resolution or \nsmaller area with much greater detail [103]. Temporally,\
    \ the UAS can be scheduled at a user-defined \ntime at short notice, thus accommodating\
    \ applications that are time-sensitive, such as capturing vital \nphenological\
    \ stages of crop growth. Spectrally, UAS offer flexibility to carry on-demand\
    \ sensors and \ninterchangeability between sensor payloads; thus, any desired\
    \ combination of sensors and spectral \nbands can be incorporated to target specific\
    \ features. \nUAS-acquired image data requires post-processing before it can be\
    \ incorporated into the grower \ndecision-making process. Mosaicking of UAS images\
    \ currently has a turnaround time of \napproximately one day to one week, subject\
    \ to the size of the dataset, computational power, and \nspectral/spatial quality\
    \ of the product [104,105]. Spectral quality of the data is of optimal importance,\
    \ \nwhereas the spatial quality can be of less importance, such as for well-established\
    \ horticultural crops. \nHigher spectral quality demands calibration of the spectral\
    \ sensors and correction of atmospheric \neffects. Following post-processing of\
    \ aerial images, the UAS-based spectral data have shown to be \nhighly correlated\
    \ with ground-based data [82,102,106]. \nThe most common UAS-based sensor types\
    \ to study the crop water status are the thermal, \nmultispectral and RGB, while\
    \ hyperspectral and LiDAR (Light detection and ranging) sensors are \nused less\
    \ often [23,79,107]. Spectral sensors provide the capability to capture broader\
    \ physiological \nproperties of the crop, such as greenness (related to leaf chlorophyll\
    \ content and health) and biomass, \nthat generally correlate with crop water\
    \ status [82,108]. Narrower band spectral sensors provide \ndirect insight into\
    \ specific biophysical and biochemical properties of crops, such as via photochemical\
    \ \nreflectance index (PRI) and solar-induced chlorophyll fluorescence (SIF),\
    \ which reflects a plant’s \nphotosynthetic efficiency [109,110]. Thermal-based\
    \ sensors capture the temperature of the crop’s \nsurface, which indicates the\
    \ plant’s stress (both biotic and abiotic) [53]. Generally, digital RGB camera\
    \ \nand LiDAR can be used to quantify 3D metrics, such as the plant size and shape,\
    \ via 3D pointclouds \nwith sufficient accuracy for canopy level assessment [111–118].\
    \ \n3. Remote Sensor Types \n3.1. Digital Camera \nA digital camera typically\
    \ incorporates an RGB, modified RGB, and a monochrome digital \ncamera. The lens\
    \ quality of the camera determines the sharpness of the image, while the resolution\
    \ \nFigure 2. Examples of unmanned aircraft systems (UAS) used to study water\
    \ status in horticulture\ncrops: (a) hexacopter equipped with RGB, multispectral\
    \ and thermal camera at The University of\nAdelaide, Adelaide, Australia (b) quadcopter\
    \ equipped with a thermal and multispectral camera [100],\n(c) ﬁxed-wing aircraft\
    \ used for GRAPEX project to carry RGB, thermal and monochrome camera with\nnarrowband\
    \ ﬁlters [101], and (d) helicopter used for various studies of crop water status\
    \ [18,92,102].\nUAS oﬀers ﬂexibility on spatial resolution, observation scale,\
    \ spectral bands, and temporal\nresolution to collect data on any good weather\
    \ day. However, like satellite and manned aircraft, the\nUAS is inoperable during\
    \ precipitation, high winds, and temperatures. By easily altering the ﬂying\n\
    altitude, the UAS provides higher ﬂexibility to observe a larger area with lower\
    \ spatial resolution or\nsmaller area with much greater detail [103]. Temporally,\
    \ the UAS can be scheduled at a user-deﬁned\ntime at short notice, thus accommodating\
    \ applications that are time-sensitive, such as capturing vital\nphenological\
    \ stages of crop growth. Spectrally, UAS oﬀer ﬂexibility to carry on-demand sensors\
    \ and\ninterchangeability between sensor payloads; thus, any desired combination\
    \ of sensors and spectral\nbands can be incorporated to target speciﬁc features.\n\
    UAS-acquired image data requires post-processing before it can be incorporated\
    \ into the grower\ndecision-making process. Mosaicking of UAS images currently\
    \ has a turnaround time of approximately\none day to one week, subject to the\
    \ size of the dataset, computational power, and spectral/spatial\nquality of the\
    \ product [104,105]. Spectral quality of the data is of optimal importance, whereas\
    \ the\nspatial quality can be of less importance, such as for well-established\
    \ horticultural crops. Higher\nspectral quality demands calibration of the spectral\
    \ sensors and correction of atmospheric eﬀects.\nFollowing post-processing of\
    \ aerial images, the UAS-based spectral data have shown to be highly\ncorrelated\
    \ with ground-based data [82,102,106].\nThe most common UAS-based sensor types\
    \ to study the crop water status are the thermal,\nmultispectral and RGB, while\
    \ hyperspectral and LiDAR (Light detection and ranging) sensors are\nused less\
    \ often [23,79,107]. Spectral sensors provide the capability to capture broader\
    \ physiological\nproperties of the crop, such as greenness (related to leaf chlorophyll\
    \ content and health) and biomass,\nthat generally correlate with crop water status\
    \ [82,108]. Narrower band spectral sensors provide\ndirect insight into speciﬁc\
    \ biophysical and biochemical properties of crops, such as via photochemical\n\
    reﬂectance index (PRI) and solar-induced chlorophyll ﬂuorescence (SIF), which\
    \ reﬂects a plant’s\nphotosynthetic eﬃciency [109,110]. Thermal-based sensors\
    \ capture the temperature of the crop’s\nsurface, which indicates the plant’s\
    \ stress (both biotic and abiotic) [53]. Generally, digital RGB camera\nand LiDAR\
    \ can be used to quantify 3D metrics, such as the plant size and shape, via 3D\
    \ pointclouds\nwith suﬃcient accuracy for canopy level assessment [111–118].\n\
    3. Remote Sensor Types\n3.1. Digital Camera\nA digital camera typically incorporates\
    \ an RGB, modiﬁed RGB, and a monochrome digital camera.\nThe lens quality of the\
    \ camera determines the sharpness of the image, while the resolution of the\n\
    Agronomy 2020, 10, 140\n7 of 35\ncamera determines its spatial resolution and\
    \ details within an image. The RGB camera uses broad\nspectral bandwidth within\
    \ the blue, green and red spectral region to capture energy received at the\n\
    visible region of the electromagnetic spectrum. The images are used to retrieve\
    \ dimensional properties\nof the crop, terrain conﬁguration, macrostructure of\
    \ the ﬁeld, and the spatial information. Based on\nthe dimensional properties,\
    \ such as size, height, perimeter, and area of the crown, the resource need\n\
    practices can be estimated [119–121]. Generally, a larger crop is expected to\
    \ more quickly use available\nwater resources, resulting in crop water stress\
    \ at a later stage of the season if irrigation is not suﬃcient.\nThe evolution\
    \ of canopy structure within and between seasons can be useful to understand the\
    \ spatial\nvariability within the ﬁeld and corresponding water requirements. The\
    \ macro-structure of horticultural\ncrops, such as row height, width, spacing,\
    \ crop count, the fraction of ground cover, and missing\nplants, can be identiﬁed\
    \ remotely, which can aid in the allocation of resources [113,122]. The terrain\n\
    conﬁguration in the form of a digital elevation model (DEM) generated from a digital\
    \ camera can also\nenable understanding of the water status in relation to the\
    \ aspect and slope conﬁguration of the terrain.\n3.2. Multispectral Camera\nA\
    \ multispectral camera oﬀers multiple spectral bands across the electromagnetic\
    \ spectrum.\nMost common airborne multispectral cameras have 4–5 bands which include\
    \ rededge and NIR\nbands in addition to the visible bands, R-G-B (e.g., Figure\
    \ 3a,c). Conﬁgurable ﬁlter placement of\nthe spectral band is also available,\
    \ which can potentially target certain physiological responses of\nhorticultural\
    \ crops [102]. Spectrally, the airborne multispectral camera has been reported\
    \ to perform\nwith consistency, producing reliable measurements following radiometric\
    \ calibration and atmospheric\ncorrection [123–125].\nTheir spatial resolution\
    \ has been found to be suﬃcient for horticultural\napplications enabling canopy\
    \ level observation of the spectral response. For this reason, as well as\nrelatively\
    \ low cost, multispectral cameras are used more frequently in horticulture applications.\n\
    Agronomy 2020, 10, 140 \n7 of 35 \n \nof the camera determines its spatial resolution\
    \ and details within an image. The RGB camera uses \nbroad spectral bandwidth\
    \ within the blue, green and red spectral region to capture energy received \n\
    at the visible region of the electromagnetic spectrum. The images are used to\
    \ retrieve dimensional \nproperties of the crop, terrain configuration, macrostructure\
    \ of the field, and the spatial information. \nBased on the dimensional properties,\
    \ such as size, height, perimeter, and area of the crown, the \nresource need\
    \ practices can be estimated [119–121]. Generally, a larger crop is expected to\
    \ more \nquickly use available water resources, resulting in crop water stress\
    \ at a later stage of the season if \nirrigation is not sufficient. The evolution\
    \ of canopy structure within and between seasons can be \nuseful to understand\
    \ the spatial variability within the field and corresponding water requirements.\
    \ \nThe macro-structure of horticultural crops, such as row height, width, spacing,\
    \ crop count, the \nfraction of ground cover, and missing plants, can be identified\
    \ remotely, which can aid in the \nallocation of resources [113,122]. The terrain\
    \ configuration in the form of a digital elevation model \n(DEM) generated from\
    \ a digital camera can also enable understanding of the water status in relation\
    \ \nto the aspect and slope configuration of the terrain. \n3.2. Multispectral\
    \ Camera \nA multispectral camera offers multiple spectral bands across the electromagnetic\
    \ spectrum. Most \ncommon airborne multispectral cameras have 4–5 bands which\
    \ include rededge and NIR bands in \naddition to the visible bands, R-G-B (e.g.,\
    \ Figure 3a,c). Configurable filter placement of the spectral \nband is also available,\
    \ which can potentially target certain physiological responses of horticultural\
    \ \ncrops [102]. Spectrally, the airborne multispectral camera has been reported\
    \ to perform with \nconsistency, producing reliable measurements following radiometric\
    \ calibration and atmospheric \ncorrection [123–125]. Their spatial resolution\
    \ has been found to be sufficient for horticultural \napplications enabling canopy\
    \ level observation of the spectral response. For this reason, as well as \nrelatively\
    \ low cost, multispectral cameras are used more frequently in horticulture applications.\
    \ \n \n \n(a) \n(b) \n \n \n(c) \n(d) \nFigure 3. Some examples of sensors used\
    \ on a UAS platform to study water status of horticultural \ncrops: (a) A multispectral\
    \ camera (Tetracam Mini-MCA-6, Tetracam, Inc., Chatsworth, CA, USA) \n[126]. (b)\
    \ A thermal camera (FLIR TAU II, FLIR Systems, Inc., USA) [100,108]. (c) A multi-sensor\
    \ \ncamera setup with an RGB (Sony α7R III, Sony Electronics, Inc., Minato, Tokyo,\
    \ Japan), a multispectral \n(MicaSense RedEdge, MicaSense Inc., Seattle, WA, USA),\
    \ and a thermal (FLIR TAU II 640, FLIR \nSystems, Inc., USA) camera. (d) A micro-hyperspectral\
    \ camera (Micro-Hyperspec, Headwall \nPhotonics, MA, USA) [110]. \nChlorophyll\
    \ and cellular structures of vegetation absorb most of the visible light and reflect\
    \ \ninfrared light. The rise in reflectance between the red and NIR band is unique\
    \ to live green vegetation \nand is captured by vegetation spectral index called\
    \ NDVI (Table 2, Equation (3). Once the vegetation \nFigure 3. Some examples of\
    \ sensors used on a UAS platform to study water status of horticultural\ncrops:\
    \ (a) A multispectral camera (Tetracam Mini-MCA-6, Tetracam, Inc., Chatsworth,\
    \ CA, USA) [126].\n(b) A thermal camera (FLIR TAU II, FLIR Systems, Inc., USA)\
    \ [100,108]. (c) A multi-sensor camera setup\nwith an RGB (Sony α7R III, Sony\
    \ Electronics, Inc., Minato, Tokyo, Japan), a multispectral (MicaSense\nRedEdge,\
    \ MicaSense Inc., Seattle, WA, USA), and a thermal (FLIR TAU II 640, FLIR Systems,\
    \ Inc., USA)\ncamera. (d) A micro-hyperspectral camera (Micro-Hyperspec, Headwall\
    \ Photonics, MA, USA) [110].\nChlorophyll and cellular structures of vegetation\
    \ absorb most of the visible light and reﬂect\ninfrared light. The rise in reﬂectance\
    \ between the red and NIR band is unique to live green vegetation\nand is captured\
    \ by vegetation spectral index called NDVI (Table 2, Equation (3)). Once the vegetation\n\
    starts to experience stress (biotic and abiotic), its reﬂectance in the NIR region\
    \ is reduced, while the\nreﬂectance in the red band is increased. Thus, such stress\
    \ is reﬂected in the vegetation proﬁle and\nAgronomy 2020, 10, 140\n8 of 35\n\
    easily captured by indices, such as NDVI. For this reason, NDVI has shown correlations\
    \ with a wide\narray of crops response including vigour, chlorophyll content,\
    \ leaf area index (LAI), crop water stress,\nand occasionally yield [34,82–84,127].\n\
    The rededge band covers the portion of the electromagnetic spectrum between the\
    \ red and NIR\nbands where reﬂectance increases drastically. Studies have suggested\
    \ that the sharp transition between\nthe red absorbance and NIR reﬂection is able\
    \ to provide additional information about vegetation and\nits hydric characteristics\
    \ [128]. Using the normalised diﬀerence red edge (NDRE) index, the rededge\nband\
    \ was found to be useful in establishing a relative chlorophyll concentration\
    \ map [127]. Given the\nsensitivity of NDRE, it can be used for applications,\
    \ such as crops drought stress [107]. With regard to\nthe water use eﬃciency,\
    \ a combination of vegetation indices (VIs) along with structural physiological\n\
    indices were found to be useful to study water stress in horticultural crops [34,82,129].\n\
    3.3. Hyperspectral\nHyperspectral sensors have contiguous spectral bands sampled\
    \ at a narrower wavelength intervals\nspanning from visible to NIR spectrum at\
    \ a high to ultra-high spectral resolution (Figure 3d). Scanning\nat contiguous\
    \ narrow-band wavelengths, a hyperspectral sensor produces a three dimensional\
    \ (two\nspatial dimensions and one spectral dimension) data called hyperspectral\
    \ data cube. The hyperspectral\ndata cube is a hyperspectral image where each\
    \ pixel contain spatial information, as well as the entire\nspectral reﬂectance\
    \ curve [130]. Based on the operating principle and output data cube, hyperspectral\n\
    sensors for remote sensing can include a point spectrometer (aka spectroradiometer),\
    \ whiskbroom\nscanner, pushbroom scanner, and 2D imager (Figure 4) [130,131].\
    \ A point spectrometer, samples\nwithin its ﬁeld of view solid angle to produce\
    \ an ultra-high spectral resolution spectral data of a\npoint [130,132]. A whiskbroom\
    \ scanner deploys a single detector onboard to scan one single pixel at a\ntime.\
    \ As the scanner rotates across-track, successive scans form a row of the data\
    \ cube, and as the\nplatform moves forward along-track, successive rows form a\
    \ hyperspectral image [133]. A pushbroom\nscanner deploys a row of spatially contiguous\
    \ detectors arranged in the perpendicular direction of\ntravel and scans the entire\
    \ row of pixels at a time. As the platform moves forward, the successive\nrows\
    \ form a two-dimensional hyperspectral image [40,134]. The 2D imager using diﬀerent\
    \ scanning\ntechniques [130] captures hyperspectral data across the image scene\
    \ [135,136]. The point spectrometer\noﬀers the highest spectral resolution and\
    \ lowest signal-to-noise ratio (SNR) among the UAS-compatible\nhyperspectral sensors\
    \ [137,138].\nAgronomy 2020, 10, 140 \n8 of 35 \n \nstarts to experience stress\
    \ (biotic and abiotic), its reflectance in the NIR region is reduced, while the\
    \ \nreflectance in the red band is increased. Thus, such stress is reflected in\
    \ the vegetation profile and \neasily captured by indices, such as NDVI. For this\
    \ reason, NDVI has shown correlations with a wide \narray of crops response including\
    \ vigour, chlorophyll content, leaf area index (LAI), crop water stress, \nand\
    \ occasionally yield [34,82–84,127]. \nThe rededge band covers the portion of\
    \ the electromagnetic spectrum between the red and NIR \nbands where reflectance\
    \ increases drastically. Studies have suggested that the sharp transition \nbetween\
    \ the red absorbance and NIR reflection is able to provide additional information\
    \ about \nvegetation and its hydric characteristics [128]. Using the normalised\
    \ difference red edge (NDRE) \nindex, the rededge band was found to be useful\
    \ in establishing a relative chlorophyll concentration \nmap [127]. Given the\
    \ sensitivity of NDRE, it can be used for applications, such as crops drought\
    \ stress \n[107]. With regard to the water use efficiency, a combination of vegetation\
    \ indices (VIs) along with \nstructural physiological indices were found to be\
    \ useful to study water stress in horticultural crops \n[34,82,129]. \n3.3. Hyperspectral\
    \ \nHyperspectral sensors have contiguous spectral bands sampled at a narrower\
    \ wavelength \nintervals spanning from visible to NIR spectrum at a high to ultra-high\
    \ spectral resolution (Figure \n3d). Scanning at contiguous narrow-band wavelengths,\
    \ a hyperspectral sensor produces a three \ndimensional (two spatial dimensions\
    \ and one spectral dimension) data called hyperspectral data \ncube. The hyperspectral\
    \ data cube is a hyperspectral image where each pixel contain spatial \ninformation,\
    \ as well as the entire spectral reflectance curve [130]. Based on the operating\
    \ principle \nand output data cube, hyperspectral sensors for remote sensing can\
    \ include a point spectrometer (aka \nspectroradiometer), whiskbroom scanner,\
    \ pushbroom scanner, and 2D imager (Figure 4) [130,131]. A \npoint spectrometer,\
    \ samples within its field of view solid angle to produce an ultra-high spectral\
    \ \nresolution spectral data of a point [130,132]. A whiskbroom scanner deploys\
    \ a single detector onboard \nto scan one single pixel at a time. As the scanner\
    \ rotates across-track, successive scans form a row of \nthe data cube, and as\
    \ the platform moves forward along-track, successive rows form a hyperspectral\
    \ \nimage [133]. A pushbroom scanner deploys a row of spatially contiguous detectors\
    \ arranged in the \nperpendicular direction of travel and scans the entire row\
    \ of pixels at a time. As the platform moves \nforward, the successive rows form\
    \ a two-dimensional hyperspectral image [40,134]. The 2D imager \nusing different\
    \ scanning techniques [130] captures hyperspectral data across the image scene\
    \ \n[135,136]. The point spectrometer offers the highest spectral resolution and\
    \ lowest signal-to-noise \nratio (SNR) among the UAS-compatible hyperspectral\
    \ sensors [137,138]. \n \nFigure 4. The data cube structure of different spectral\
    \ sensors. The number of bands and resolution is \nshown as an example and does\
    \ not indicate true sensor capability (adapted from [130]). \nFigure 4. The data\
    \ cube structure of diﬀerent spectral sensors. The number of bands and resolution\
    \ is\nshown as an example and does not indicate true sensor capability (adapted\
    \ from [130]).\nIn horticultural applications, hyperspectral data, due to the\
    \ high resolution contiguous spectral\nsampling, possesses tremendous potential\
    \ to detect and monitor speciﬁc biotic and abiotic stresses [139].\nNarrowband\
    \ hyperspectral data was used to detect water stress using the measurement of\
    \ ﬂuorescence\nAgronomy 2020, 10, 140\n9 of 35\nand PRI over a citrus orchard\
    \ [110]. PRI was identiﬁed as one of the best predictors of water stress for a\n\
    vineyard in a study that investigated numerous VIs using hyperspectral imaging\
    \ [140]. High-resolution\nthermal imagery obtained from a hyperspectral scanner\
    \ was used to map canopy stomatal conductance\n(gs) and CWSI of olive orchards\
    \ where diﬀerent irrigation treatments were applied [18]. With the\nlarge volume\
    \ of spatial/spectral data extracted from the hyperspectral data cube, machine\
    \ learning\nwill likely be adopted more widely in the horticultural environment\
    \ to model water stress [141].\nSee Reference [54] for a comprehensive review\
    \ of hyperspectral and thermal remote sensing to detect\nplant water status.\n\
    3.4. Thermal\nThermal cameras use microbolometers to read passive thermal signals\
    \ in the spectral range of\napproximately 7–14 µm (Figure 3b). Small UAS are capable\
    \ of carrying a small form-factor thermal\ncamera with uncooled microbolometers,\
    \ which does not use an internal cooling mechanism and,\ntherefore, does not achieve\
    \ the high SNR that can be found in cooled microbolometer-based thermal\ncameras.\
    \ An array of microbolometer detectors in the thermal camera receives a thermal\
    \ radiation\nsignal and stores the signal on the corresponding image pixel as\
    \ raw data number (DN) values.\nThe result is a thermal image where each pixel\
    \ has an associated DN value, which can be converted to\nabsolute temperature.\
    \ A representative list of commercial thermal cameras used on UAS platforms\n\
    and their applications with regard to agricultural remote sensing is found in\
    \ the literature [23,53,73].\nThermal imagery enables the measurement of the foliar\
    \ temperature of plants. The foliar temperature\ndiﬀerence between well-watered\
    \ and water-stressed crops is the primary source of information for\nwater stress\
    \ prediction using a thermal sensor [142]. When mounted on a remote sensing platform,\
    \ the\ncanopy level assessment of crop water status can be performed on a large\
    \ scale.\nThermal cameras are limited by their resolution (e.g., 640 × 512 is\
    \ the maximum resolution of\nUAS compatible thermal cameras in the current market)\
    \ and high price-tag [53]. The small number of\npixels results in low spatial\
    \ resolution limiting either the ability to resolve a single canopy or ability\
    \ to\nﬂy higher and cover a larger area. If ﬂown at a higher altitude, the eﬀective\
    \ spatial resolution may\nbe inadequate for canopy level assessment of some horticultural\
    \ crops. For example, a FLIR Tau2\n640 thermal camera with a 13 mm focal length\
    \ when ﬂown at an altitude of approximately 120 m\nresults in a spatial resolution\
    \ of 15.7 cm. For relatively large horticultural crops, such as grapevine,\nalmond,\
    \ citrus, and avocado, the resolution at a maximum legal ﬂying altitude of 120\
    \ m in Australia\n(for small-sized UAS) oﬀers an adequate spatial resolution to\
    \ observe a single canopy.\nAnother challenge with the use of thermal cameras\
    \ is the temporal drift of the DN values\nwithin successive thermal images, especially\
    \ with uncooled thermal cameras [143]. Due to the lack\nof an internal cooling\
    \ mechanism for the microbolometer detectors, DN values registered by the\nmicrobolometers\
    \ experience temporal drift i.e., the registered DN values for the same temperature\n\
    target will drift temporally. Thus, the thermal image can be unreliable especially\
    \ when the internal\ntemperature of the camera is changing rapidly, such as during\
    \ camera warmup period or during the\nﬂight when a gust of cool wind results in\
    \ cooling of the camera. To overcome this challenge, the user\nmay need to provide\
    \ suﬃcient startup time before operation (preferably 30–60 min) [102,143–145],\n\
    shield the camera to minimize the change in the internal temperature of the camera\
    \ [142], calibrate the\ncamera [146–153], and perform frequent ﬂat-ﬁeld corrections.\n\
    3.5. Multi-Sensor\nTo carry multiple sensors, the total UAS payload needs to be\
    \ considered that includes, in\naddition to the sensors, an inertial measurement\
    \ unit (IMU) and global navigation satellite system\n(GNSS) for the georeferencing\
    \ purpose [40,154]. Higher accuracy sensors tend to be heavier, and in\na multi-sensor\
    \ scenario, the payload can quickly reach or even exceed the payload limit. This\
    \ has\nlimited contemporary measurements in earlier multirotor UAS requiring separate\
    \ ﬂights for each of\nsensor [126]. The use of ﬁxed-wing UAS has allowed carrying\
    \ higher payloads due to the much larger\nAgronomy 2020, 10, 140\n10 of 35\nthrust-to-weight\
    \ ratio as compared to a rotary-wing aircraft [155]. Similarly, recent advancement\
    \ in\nUAS technology and lightweight sensors have enabled multirotor (payload\
    \ 5–6 kg readily available) to\nonboard multi-sensors.\nWater status of crops\
    \ is a complex process inﬂuenced by a number of factors including the\nphysiology\
    \ of the crop, available soil moisture, the size and vigour of the crop, and meteorological\n\
    factors [30,108,116,156,157]. For this reason, a multi-sensor platform is used\
    \ to acquire measurements\nof the diﬀerent aspects of the crop for water status\
    \ assessment [34,102,108]. The most common\ncombination of sensors found in the\
    \ literature is the RGB, multispectral (including rededge and NIR\nbands) and\
    \ thermal. Together, these sensors can be used to investigate the water status\
    \ of the crop\nusing various indicators, such as PRI, CWSI, ﬂuorescence, and structural\
    \ properties, with the aim of\nimproving the water use eﬃciency [102,110,158–160].\n\
    4. Techniques of Remote Sensing in Horticulture\n4.1. Georeferencing of Remotely\
    \ Sensed Images\nGeoreferencing provides a spatial reference to the remotely sensed\
    \ images such that the pixels\nrepresenting crops or regions of interest on the\
    \ images are correctly associated with their position on\nEarth. The georeferencing\
    \ process generally uses surveyed coordinate points on the ground, known\nas ground\
    \ control points (GCPs), to determine and apply scaling and transformation to\
    \ the aerial\nimages [161]. Alternatively, instead of GCPs, the user can georeference\
    \ aerial images by using the\naccurate position of the camera, or by co-registration\
    \ with the existing georeferenced map [105,162].\nIn the case of UAS-based images,\
    \ the capture timing is scheduled to ensure a recommended\nforward overlap (>80%)\
    \ between successive images.\nThe ﬂight path is designed to ensure the\nrecommended\
    \ side overlap (>70%) between images from successive ﬂight strips. Thus, the captured\n\
    series of images are processed using the Structure-from-Motion (SfM) technique\
    \ to generate a 3D\npointcloud and orthomosaic [73,130] (see Figure 5). Commonly\
    \ used SfM software to process the\nremote sensing images are Agisoft PhotoScan\
    \ and Pix4D. The commonly retrieved outputs from the\nSfM software for assessment\
    \ of horticulture crops include the orthomosaic, digital surface model\n(DSM),\
    \ DEM, and 3D pointcloud [113,126,163]. This technique of georeferencing can be\
    \ applied to any\nsensor that produces images, e.g., RGB, thermal, or multispectral\
    \ cameras [126,164,165].\nAgronomy 2020, 10, 140 \n10 of 35 \n \nsensor [126].\
    \ The use of fixed-wing UAS has allowed carrying higher payloads due to the much\
    \ larger \nthrust-to-weight ratio as compared to a rotary-wing aircraft [155].\
    \ Similarly, recent advancement in \nUAS technology and lightweight sensors have\
    \ enabled multirotor (payload 5–6 kg readily available) \nto onboard multi-sensors.\
    \ \nWater status of crops is a complex process influenced by a number of factors\
    \ including the \nphysiology of the crop, available soil moisture, the size and\
    \ vigour of the crop, and meteorological \nfactors [30,108,116,156,157]. For this\
    \ reason, a multi-sensor platform is used to acquire measurements \nof the different\
    \ aspects of the crop for water status assessment [34,102,108]. The most common\
    \ \ncombination of sensors found in the literature is the RGB, multispectral (including\
    \ rededge and NIR \nbands) and thermal. Together, these sensors can be used to\
    \ investigate the water status of the crop \nusing various indicators, such as\
    \ PRI, CWSI, fluorescence, and structural properties, with the aim of \nimproving\
    \ the water use efficiency [102,110,158–160]. \n4. Techniques of Remote Sensing\
    \ in Horticulture \n4.1. Georeferencing of Remotely Sensed Images \nGeoreferencing\
    \ provides a spatial reference to the remotely sensed images such that the pixels\
    \ \nrepresenting crops or regions of interest on the images are correctly associated\
    \ with their position on \nEarth. The georeferencing process generally uses surveyed\
    \ coordinate points on the ground, known \nas ground control points (GCPs), to\
    \ determine and apply scaling and transformation to the aerial \nimages [161].\
    \ Alternatively, instead of GCPs, the user can georeference aerial images by using\
    \ the \naccurate position of the camera, or by co-registration with the existing\
    \ georeferenced map [105,162]. \nIn the case of UAS-based images, the capture\
    \ timing is scheduled to ensure a recommended \nforward overlap (>80%) between\
    \ successive images. The flight path is designed to ensure the \nrecommended side\
    \ overlap (>70%) between images from successive flight strips. Thus, the captured\
    \ \nseries of images are processed using the Structure-from-Motion (SfM) technique\
    \ to generate a 3D \npointcloud and orthomosaic [73,130] (see Figure 5). Commonly\
    \ used SfM software to process the \nremote sensing images are Agisoft PhotoScan\
    \ and Pix4D. The commonly retrieved outputs from the \nSfM software for assessment\
    \ of horticulture crops include the orthomosaic, digital surface model \n(DSM),\
    \ DEM, and 3D pointcloud [113,126,163]. This technique of georeferencing can be\
    \ applied to \nany sensor that produces images, e.g., RGB, thermal, or multispectral\
    \ cameras [126,164,165]. \n \nFigure 5. A typical workflow of structure-from-motion\
    \ (SfM) to produce georeferenced products from \nUAS-based image sets and ground\
    \ control points (adapted from [166,167]). SIFT = scale-invariant \nfeature transform;\
    \ ANN = approximate nearest neighbour; RANSAC = random sample consensus; \nFigure\
    \ 5. A typical workﬂow of structure-from-motion (SfM) to produce georeferenced\
    \ products from\nUAS-based image sets and ground control points (adapted from\
    \ [166,167]). SIFT = scale-invariant\nfeature transform; ANN = approximate nearest\
    \ neighbour; RANSAC = random sample consensus;\nCMVS = clustering views for multi-view\
    \ stereo; PMVS = patch-based multi-view stereo; GCP = ground\ncontrol points.\n\
    Agronomy 2020, 10, 140\n11 of 35\nThe complexity of georeferencing of hyperspectral\
    \ observations depends on the sensor type, i.e.,\nimaging or non-imaging. A non-imaging\
    \ spectroradiometer relies on the use of a GNSS antenna and\nan IMU for georeferencing\
    \ the point observation [130,132,138,168]. An imaging hyperspectral camera,\n\
    generally, in addition to GNSS and IMU measurement, uses the inter-pixel relation\
    \ in SfM to produce a\ngeoreferenced orthomosaic [40,134,135,169,170].\n4.2. Calibration\
    \ and Correction of Remotely Sensed Images\nEnsuring consistency, repeatability,\
    \ and quality of the spectral observation requires stringent\nradiometric, spectral,\
    \ and atmospheric corrections [123,171–177]. Spectral and radiometric calibration\n\
    is performed in the spectral calibration facility in darkroom settings. The sensor’s\
    \ optical properties\nand shift in spectral band position are corrected during\
    \ the spectral calibration process. Radiometric\ncalibration enables conversion\
    \ of the recorded digital values into physical units, such as radiance.\nInﬁeld\
    \ operation of the spectral sensor is inﬂuenced by variations in atmospheric transmittance\n\
    from thin clouds, invisible to the human observer. Changes in atmospheric transmittance\
    \ aﬀect the\nradiance incident on the plant. As a result, the change in acquired\
    \ spectral response by the sensor\nmay not represent the change in plants response\
    \ but the change in incident radiation on the plant.\nThe most common method to\
    \ convert the spectral data to reﬂectance is by generating an empirical line\n\
    relationship between sensor values and spectral targets, such as a Spectralon®\
    \ or calibration targets.\nThe use of downwelling sensors, such as a cosine corrector\
    \ [137], or the use of a ground-based PAR\nsensor enables absolute radiometric\
    \ calibration to generate radiance [130].\nThe calibration of the broad wavelength\
    \ multispectral sensor is generally less stringent than\nthe hyperspectral. Generally,\
    \ multispectral sensors are used to compute normalised indices such\nas NDVI.\
    \ The normalised indices are relatively less inﬂuenced, although signiﬁcant, by\
    \ the change\nin illumination conditions which aﬀect the entire spectrum proportionally\
    \ [29,101]. In this regard,\nradiometric calibration of the multispectral camera\
    \ has used a range of stringent to simpliﬁed, and\nvicarious approaches [123,125,171,173,178–180].\
    \ Some multispectral cameras are equipped with a\ndownwelling light sensor, which\
    \ is aimed at correcting for variations in atmospheric transmittance.\nHowever,\
    \ the performance of such downwelling sensors (without a cosine corrector) on\
    \ multispectral\ncameras have been reported to have directional variation resulting\
    \ in unstable correction, indicating\nthe inability of the sensor to incorporate\
    \ the entire hemisphere of diﬀused light [124,137].\nThe radiometric calibration\
    \ of the thermal images is typically based on the camera’s DN to object\ntemperature\
    \ curve, which provides the relationship between the DN of a pixel and a known\
    \ object\ntemperature, usually of a black body radiator. Measurement accuracy\
    \ and SNR of the camera under\nvarying ambient temperatures can be improved by\
    \ using calibration shutters, which are recently\navailable commercially. Furthermore,\
    \ for low measurement errors (under 1 ◦C), thermal data requires\nconsideration\
    \ to the atmospheric transmittance [18,102]. Flying over a few temperature reference\n\
    targets placed on the ground reduces the temporal drift of the camera [142,143,181].\
    \ Temperature\naccuracy within a few degrees was achieved by ﬂying over the targets\
    \ three times (at the start, middle\nand end of UAS operation) and using three\
    \ separate calibration equations for each overpass [142].\nAdditionally, using\
    \ the redundant information from multiple overlapping images, drift correction\n\
    models have been proposed, which lowered temperature error by 1 ◦C as compared\
    \ to uncorrected\northomosaic [152]. The manufacturer stated accuracies (generally\
    \ ±5 ◦C) can be suﬃcient to access the\nﬁeld variability and to detect “hotspots”\
    \ of water status. However, the aforementioned calibration and\ncorrection of\
    \ the thermal cameras are required for quantitative measurement as a goal [143].\
    \ In this\nregard, current challenges and best practices for the operation of\
    \ thermal cameras onboard a UAS is\nprovided in the literature [143].\n4.3. Canopy\
    \ Data Extraction\nA key challenge in remote sensing of horticultural as compared\
    \ to agricultural crops arises due\nto the proportion of inter-row ground/vegetation\
    \ cover and resulting mixed pixels. The proportion\nAgronomy 2020, 10, 140\n12\
    \ of 35\nof the mixed pixels increases with the decrease in spatial resolution\
    \ of the image. Most of the pixels\ntowards the edge of the canopy contain a blend\
    \ of information originating from the sun-lit canopy,\nshadowed leaves, and inter-row\
    \ bare soil/cover crop. A further challenge can arise for some crops,\nsuch as\
    \ grapevine, due to overlapping of adjacent plants.\nThe canopy data from orthomosaic\
    \ has been extracted using either a pixel-based or an object-based\napproach.\
    \ Earlier studies manually sampled from the centre of crop row which most likely\
    \ eliminated\nthe mixed pixels [182]. In the pixel-based approach, techniques,\
    \ such as applying global threshold\nand masking, have been used. Binary masks,\
    \ such as NDVI, eliminates non-canopy pixels from\nthe sampling [82,84]. Combining\
    \ the NDVI mask with the canopy height mask can exclude the\npixels associated\
    \ with non-vegetation, as well as vegetation that does not meet the height threshold.\n\
    The pixel-based approach, however, can result in inaccurate identiﬁcation of some\
    \ crops due to pixel\nheterogeneity, mixed pixels, spectral similarity, and crop\
    \ pattern variability.\nIn the object-based approach, using object detection techniques,\
    \ neighbouring pixels with\nhomogenous information, such as spectral, textural,\
    \ structural, and hierarchical features, are grouped\ninto “objects”. These objects\
    \ are used as the basis of object-based image analysis (OBIA) classiﬁcation\n\
    using classiﬁers, such as k-nearest neighbour, decision tree, support vector machine,\
    \ random forest,\nand maximum likelihood [122,183–185]. In the horticultural environment,\
    \ OBIA has been adopted\nto classify and sample from pure canopy pixels [119,122,186].\
    \ Consideration should be provided on\nthe number of features and their suitability\
    \ for a speciﬁc application to reduce the computational\nburden, as well as to\
    \ maintain the accuracies. The generalisation of these algorithms for transferability\n\
    between study sites usually penalises the achievable accuracy. For details in\
    \ object-based approach of\nsegmentation and classiﬁcation, readers are directed\
    \ to literatures [122,183,185,187–189].\nOther techniques found in the literature\
    \ include algorithms, such as ‘Watershed’, which has been\ndemonstrated in palm\
    \ orchards [82,190]. Vine rows and plants have been isolated and classiﬁed using\n\
    image processing techniques, such as clustering and skeletisation [188,191–193].\
    \ Similarly, the gridded\npolygon, available in common GIS software, such as ArcGIS\
    \ and QGIS, can be used in combination\nwith zonal statistics for this purpose.\
    \ When working with the low-resolution images, co-registration\nwith the high-resolution\
    \ images has been proposed, whereby, the high-resolution images enable better\n\
    delineation of the mixed pixels [194]. For this reason, spectral and thermal sensors,\
    \ which are usually\nlow in resolution, are generally employed along with high-resolution\
    \ digital cameras.\n4.4. Indicators of Crop Water Status\nA crop’s biophysical\
    \ and biochemical attributes can be approximated using diﬀerent indices and\n\
    quantitative products. For example, CWSI is used to proxy leaf water potential\
    \ (Ψleaf), stem water\npotential (Ψstem), gs, and net photosynthesis (Pn) [83,100,195].\
    \ With regard to horticultural crops, water\nstatus has been assessed using a\
    \ number of spectral and thermal indices (Table 2).\nTable 2. Commonly used vegetation\
    \ and thermal indices to study the water status of horticultural crops.\nIndicators\n\
    Sensor\nPurpose\nReferences\nTc, (Tc − Ta)\nThermal\nΨstem, gs, yield\n[34,82,85,99,110]\n\
    Ig, I3\nThermal\nΨstem, gs\n[82,196]\nCWSI\nThermal\nΨleaf, Ψstem, gs, Pn, yield\n\
    [18,31,33,85,90,97,99,100,182,194,197–199]\n(Tc − Ta)/NDVI\nThermal + multispectral\n\
    Ψstem, gs\n[82,200]\nNDVI\nMultispectral\nΨstem, gs, yield, LAI, vigour\n[34,56,82,86,182,201]\n\
    GNDVI\nMultispectral\nΨstem, gs, yield\n[34,82]\nRDVI\nMultispectral\nΨstem, gs\n\
    [82,86,182]\nPRI\nMultispectral\nΨleaf, gs\n[86,110,182]\nFluorescence\nHyperspectral\n\
    Ψleaf, gs\n[110]\nWBI\nHyperspectral\nΨleaf, gs\n[139,202,203]\nSIF\nHyperspectral\n\
    Water stress\n[204–206]\nNote the acronyms: Tc = Canopy temperature, Ta = ambient\
    \ temperature, Ig = conductance index, I3 = stomatal\nconductance index, CWSI\
    \ = crop water stress index, NDVI = normalised diﬀerence vegetation index, GNDVI\
    \ = green\nnormalised diﬀerence vegetation index, RDVI = renormalized diﬀerence\
    \ vegetation index, PRI = photochemical\nreﬂectance index, Fluorescence = chlorophyll\
    \ ﬂuorescence, WBI = water band index, SIF = solar-induced chlorophyll\nﬂuorescence,\
    \ LAI = leaf area index.\nAgronomy 2020, 10, 140\n13 of 35\n4.4.1. Canopy Temperature\n\
    A plant maintains its temperature by transpiring through the stomata to balance\
    \ the energy\nﬂuxes in and out of the canopy. As the plant experience stress (both\
    \ biotic and abiotic), the rate of\ntranspiration decreases, which results in\
    \ higher canopy temperature (Tc), which can be a proxy to\nunderstand the water\
    \ stress in the plant [207]. In this regard, crop water stress showed a correlation\n\
    with canopy temperature extracted from the thermal image [208], which enables\
    \ mapping the spatial\nvariability in water status [209]. Leaf/canopy temperature\
    \ alone, however, does not provide a complete\ncharacterisation of crop water\
    \ status, for instance, an equally stressed canopy can be 25 ◦C or 35 ◦C,\ndepending\
    \ on the current ambient temperature (Ta). Thus, canopy-to-air temperature diﬀerence\n\
    (Tc − Ta) was proposed, which showed a good correlation with the Ψstem, Ψleaf,\
    \ and gs in horticultural\ncrops [85,99,182].\n4.4.2. Normalised Thermal Indices\n\
    The CWSI, the conductance index (Ig) and the stomatal conductance index (I3) are\
    \ thermal\nindices most commonly used to estimate crop water status and gs [210–212].\
    \ These indices provide\nsimilar information, however, use a diﬀerent range of\
    \ numbers to represent the level of water stress.\nThe CWSI is normalised within\
    \ zero and one, whereas Ig and I3 represent stress using numbers\nbetween zero\
    \ and inﬁnity. CWSI has been adopted most widely in horticultural applications\
    \ to\nassess the water status of crops, such as the grapevines [100,213], almond\
    \ [91,198], citrus [85,110], and\nothers [18,87,99,214]. By normalising between\
    \ the lower and upper limits of (Tc − Ta), the CWSI of the\ncanopy presents quantiﬁable\
    \ relative water stress. The formula for CWSI computation is deﬁned as in\nEquation\
    \ (1) [208,212].\nCWSI =\n(Tc − Ta) − (Tc − Ta)LL\n(Tc − Ta)UL − (Tc − Ta)LL\n\
    (1)\nwhere (Tc − Ta)UL and (Tc − Ta)LL represent the upper and lower bound of\
    \ (Tc − Ta) which are found in\nthe water-stressed canopy and well-watered canopy\
    \ transpiring at the full potential (or maximum) rate,\nrespectively. Assuming\
    \ a constant ambient temperature, Equation (1) can be simpliﬁed to Equation (2),\n\
    which is the most widely reported formulation of CWSI with regard to the horticultural\
    \ remote sensing.\nCWSI = (Tc − Twet)\n\x10\nTdry − Twet\n\x11\n(2)\nwhere Twet\
    \ is the temperature of canopy transpiring at the maximum potential, and Tdry\
    \ is the\ntemperature of the non-transpiring canopy. CWSI has been shown to be\
    \ well-correlated with direct\nmeasurements of crop water status in the horticultural\
    \ environment [18,31,32,90,99]. In this regard,\na correlation of CWSI with various\
    \ ground measurements, such as Ψleaf [18,31,197], Ψstem [33,90,194],\nand gs [18,90,100],\
    \ have been established. Diurnal measurements of CWSI compared with Ψleaf showed\n\
    the best correlation at noon [89,197,209].\nCWSI is a normalised index, i.e.,\
    \ relative to a reference temperature range between Twet and Tdry,\nwhich is speciﬁc\
    \ to a region and crop type; thus, CWSI is not a universal quantitative indicator\
    \ of crop\nwater status. For instance, a CWSI of 0.5 for two diﬀerent varieties\
    \ of grapevines at diﬀerent locations\ndoes not conclusively inform that they\
    \ have equal or superior/inferior water status. Furthermore,\nthe degree of correlation\
    \ can change depending on the isohydric/anisohydric response of crop [214]\nwhere\
    \ early/late stomatal closure aﬀects the indicators of water stress [110]. Moreover,\
    \ phenological\nstage aﬀects the relationship between remotely sensed CWSI and\
    \ water stress [197]. Thus, water stress\nin a diﬀerent crop, at a diﬀerent location\
    \ and at a diﬀerent phenological stage, will have a unique\ncorrelation with CWSI\
    \ and, therefore, needs to be established independently.\nThere are multiple methods\
    \ to measure the two reference temperatures, Twet and Tdry, which\ncould result\
    \ in variable CWSI values depending on the method used. The ﬁrst method is to\
    \ measure the\ntwo reference temperatures on the crop of interest. Tdry can be\
    \ estimated by inducing stomatal closure,\nAgronomy 2020, 10, 140\n14 of 35\n\
    which is the leaf temperature approximately 30 min after applying a layer of petroleum\
    \ jelly e.g.,\nVaseline to both sides of a leaf. This eﬀectively blocks stomata\
    \ and, therefore, impedes leaf transpiration.\nTwet can be estimated by measuring\
    \ leaf temperature approximately 30 s after spraying water on the\nleaf, which\
    \ emulates maximum transpiration [23,83]. The advantage of this method is that\
    \ the stress\nlevels are normalised to actual plants response, whereas the necessity\
    \ to repeat the measurement for\nevery test site after each ﬂight can be cumbersome.\
    \ In an alternative (second) approach the range can\nbe established based on meteorological\
    \ data e.g., setting Tdry to 5 ◦C above air temperature and Twet\nmeasured from\
    \ an artiﬁcial surface. This method is also limited to local scale and presents\
    \ a problem\nregarding the choice of material, which ideally needs to have similar\
    \ to leaf emissivity, aerodynamic\nand optical properties [54,87]. The third method\
    \ uses the actual temperature measurement range\nof the remote sensing image [33,97].\
    \ This method is simple to implement, however, works on the\nassumption that the\
    \ ﬁeld contains enough variability to contain a representative Twet and Tdry.\
    \ Fourth,\nthe reference temperatures can be estimated by theoretically solving\
    \ for the leaf surface energy balance\nequations, however, are limited by the\
    \ necessity to compute the canopy aerodynamic resistance [87].\nStandard and robust\
    \ Twet and Tdry measurements are needed to characterize CWSI with accuracy,\n\
    especially for temporal analysis [85,87,211]. The level of uncertainty due to\
    \ the adaptation of diﬀerent\napproaches for Twet and Tdry determination in the\
    \ instantaneous and seasonal measurements of CWSI\nis not known. Nonetheless,\
    \ adopting a consistent approach, CWSI has been shown to be suitable for\nmonitoring\
    \ the water status and making irrigation decisions of horticultural crops [31,85].\n\
    4.4.3. Spectral Indices\nCrops reﬂectance properties convey information about\
    \ the crop, for instance, a healthier crop has\nhigher reﬂectance in the NIR band.\
    \ Most often, the bands are mathematically combined to form VIs,\nwhich provide\
    \ information on the crop’s health, growth stage, biophysical properties, leaf\
    \ biochemistry,\nand water stress [29,215–218]. Using multispectral or hyperspectral\
    \ data, several Vis, such as green\nnormalised diﬀerence vegetation index (GNDVI),\
    \ renormalised diﬀerence vegetation index (RDVI),\noptimized soil-adjusted vegetation\
    \ index (OSAVI), transformed chlorophyll absorption in reﬂectance\nindex (TCARI),\
    \ and TCARI/OSAVI, amongst others [34,79,82], can be calculated that correlate\
    \ with the\nwater stress of horticultural crops (see Table 2). The most widely\
    \ studied VI in horticulture, in this\nregard, is the NDVI (Equation (3)).\nNDVI\
    \ = Rnir − Rr\nRnir + Rr\n(3)\nwhere Rnir and Rr represent the spectral reﬂectance\
    \ acquired at the NIR and red spectral regions,\nrespectively. In horticulture,\
    \ NDVI has been used as a proxy to estimate the vigour, biomass, and water\nstatus\
    \ of the crop. A vigorous canopy with more leaves regulates more water, therefore\
    \ remaining\ncooler when irrigated [200] and experiencing early water stress when\
    \ unirrigated. With regard to\nirrigation, the broadband normalised spectral indices\
    \ (such as NDVI) are suitable to detect spatial\nvariability and to identify the\
    \ area that is most vulnerable to water stress. However, these indices are\nnot\
    \ expected to change rapidly to reﬂect the instantaneous water status of plants\
    \ that are needed to\nmake decisions on irrigation scheduling.\nThe multispectral\
    \ indices along with complementary information in thermal wavelengths have\nproven\
    \ to be well suited to monitoring vegetation, speciﬁcally in relation to water\
    \ stress [219]. The ratio\nof canopy surface temperature to NDVI, deﬁned as temperature-vegetation\
    \ dryness index (TVDI),\nwas found to be useful for the study of water status\
    \ in horticultural crops. TVDI exploits the fact that\nvegetation with larger\
    \ NDVI will have a lower surface temperature unless the vegetation is under\n\
    stress. As most vegetation normally remains green after an initial bout of water\
    \ stress, the TVDI is\nmore suited than NDVI for early detection of water stress\
    \ as the surface temperature can rise rapidly\neven during initial water stress\
    \ [200].\nSimilarly, narrowband VIs that have been studied in relation to remote\
    \ sensing of water status are\nPRI and chlorophyll ﬂuorescence, which have been\
    \ directly correlated to the crop Ψleaf, gs [110,182,204].\nAgronomy 2020, 10,\
    \ 140\n15 of 35\nSeveral hyperspectral indices to estimate water status have been\
    \ identiﬁed [139]; however, their\napplication in remote sensing of horticultural\
    \ crops is at its infancy. Hyperspectral indices speciﬁc to\nwater absorption\
    \ bands around 900 nm, 1200 nm, 1400 nm, and 1900 nm may be used to detect the\n\
    water status of horticultural crops. The absorption features were found to be\
    \ highly correlated with\nplant water status [139]. Water band index (WBI), as\
    \ deﬁned in Equation (4), has been shown to closely\ntrack the changes in the\
    \ plant water status of various crops [202,203].\nWBI = R970\nR900\n(4)\nOther\
    \ water-related hyperspectral indices with potential application for horticultural\
    \ crops can\nbe found in the literature [139,202,203]. Hyperspectral data possess\
    \ the capability to reﬂect the\ninstantaneous water status of the plant, which\
    \ can be useful for quantitative decision-making on\nirrigation scheduling.\n\
    4.4.4. Soil Moisture\nThe moisture status of the soil provides an indication of\
    \ the available water resource to the crop.\nSoil moisture is traditionally measured\
    \ indirectly using soil moisture sensors placed below the surface\nof the soil.\
    \ A key challenge with using soil moisture sensors are the spatial distribution\
    \ of moisture,\nboth vertically and horizontally, to account for inherent ﬁeld-scale\
    \ variability. For instance, the root\nsystem of some horticultural crops, such\
    \ as grapevine, is capable of accessing water up to 30 m deep,\nwhile customer-grade\
    \ soil moisture probes generally extend to 1.5 m in depth or less. Thus, soil\n\
    moisture probes do not capture all the water available to the crop as they are\
    \ point measures and\nnot necessarily where the roots are located. Moreover, estimation\
    \ of soil moisture across spatial and\ntemporal scales is of interest for various\
    \ agricultural and hydrological studies. Optical, thermal, and\nmicrowave remote\
    \ sensing with their advantages relating to high spatial scale and temporal resolutions\n\
    could potentially be used for soil moisture estimation [220–222]. L-band microwave\
    \ radiometry,\na component of synthetic aperture radar systems, has been shown\
    \ to be a reliable approach to estimate\nsoil moisture via satellite-based remote\
    \ sensing [223], such as using the ESA’s Soil Moisture and\nOcean Salinity (SMOS)\
    \ [224] and NASA’s Soil Moisture Active Passive (SMAP) satellites [225,226].\n\
    The limitation of the SMOS and SMAP missions, with regard to horticultural application,\
    \ is their\ndepth of retrieval (up to 5 cm) and spatial resolution (in the order\
    \ of tens of kilometre) [227–229].\nAs an airborne application, the volumetric\
    \ soil moisture has been estimated by analysing the SNR of\nthe GNSS interference\
    \ signal [230,231]. With aforementioned capabilities, a combination of satellite\n\
    and airborne remote sensing may, in the future, be a reliable tool to map soil\
    \ moisture across spatial,\ntemporal and depth scales.\n4.4.5. Physiological Attributes\n\
    Using the SfM on remotely-sensed images, 3D canopy structure, terrain conﬁgurations,\
    \ and canopy\nsurface models can be derived [113,114,119,186,232]. By employing\
    \ a delineation algorithm on the 3D\nmodels, the 3D attributes of the crops and\
    \ macrostructure are determined more accurately [120,122,233].\nCrop surface area\
    \ and terrain conﬁguration (e.g., slope and aspect) may help to develop an optimal\n\
    resource management strategy. For example, crops located at a higher elevation\
    \ within an irrigation\nzone may experience a level of water stress due to the\
    \ gravitational ﬂow of irrigated water.\nUsing the structural measurements, such\
    \ as the canopy height, canopy size, the envelope of each\nrow, LAI, and porosity,\
    \ among others, the water demand of the crop may be estimated. Generally,\nlarger\
    \ canopies tend to require more water than smaller canopies with less leaf area\
    \ [116,157]. Using\nthe temporal measurement of the plant’s 3D attributes, the\
    \ vigour can be computed. Monitoring\ncrop vigour over the season and over subsequent\
    \ years can provide an indication of its health and\nperformance, e.g., yield,\
    \ within an irrigation zone. Canopy structure metrics are closely related to\n\
    horticultural tree growth and provide strong indicators of water consumption,\
    \ whereby canopy size\nAgronomy 2020, 10, 140\n16 of 35\ncan be used to determine\
    \ its water requirements [234]. Other 3D attributes, such as the crown perimeter,\n\
    width, height, area, and leaf density, have been shown to enable improved pruning\
    \ of horticultural\nplants [116,119].\nLAI can be estimated using the 3D attributes\
    \ obtained from remote sensing [114,157,201], whereby,\nhigher LAI is equivalent\
    \ to more leaf layers, implying greater total leaf area and, consequently, canopy\n\
    transpiration. Leaf density, LAI, and exposed leaf area of a crop drive its water\
    \ requirement and\nproductivity [235–237]. Knowledge of ﬁeld attributes, such\
    \ as row and plant spacing, may assist in\ninter-row surface energy balance to\
    \ determine the irrigation need of the plant [238]. Combining the\nstructural\
    \ properties with spectral VIs provide an estimation of biomass [239], which can\
    \ serve as\nanother indicator of the plant’s water requirements. Although physiological\
    \ attributes have been used\nto understand plant water status and its spatial\
    \ variability, they have not been directly applied to make\nquantitative decisions\
    \ on irrigation.\n4.4.6. Evapotranspiration\nThe estimation of ET via remote sensing,\
    \ numerical modelling, and empirical methods have been\nextensively studied and\
    \ reviewed in the literature [240–247]. These models are based on either surface\n\
    energy balance (SEB), Penman-Monteith (PM), Maximum entropy production (MEP),\
    \ water balance,\nwater-carbon linkage, or empirical relationships.\nSEB models\
    \ are based on a surface energy budget in which the latent heat ﬂux is estimated\
    \ as a\nresidual of the net radiation, soil heat ﬂux, and sensible heat ﬂux. The\
    \ models are either one-source\n(canopy and soil treated as a single surface for\
    \ the estimation of sensible heat ﬂux) or two-source\n(canopy and soil surfaces\
    \ treated separately). Improvements over the original one-source SEB models\n\
    were in the form of Surface Energy Balance Algorithm for Land (SEBAL) algorithm\
    \ [248,249] and\nMapping EvapoTranspiration with high Resolution and Internalized\
    \ Calibration (METRIC) [249,250].\nSEBAL oﬀers a simpliﬁed approach to collect\
    \ ET data at both local and regional scales thereby increasing\nthe spatial scope,\
    \ while METRIC uses the same (SEBAL) technique but auto-calibrates the model using\n\
    hourly ground-based reference ET (ETr) data [251]. As such, these and other (e.g.,\
    \ MEP) models rely\non accurate measurements of surface (e.g., canopy) and air\
    \ temperatures, which can be erroneous\nunder non-ideal conditions, e.g., cloudy\
    \ days. There is also a reliance on ground-based sensors to\ncapture ambient air\
    \ temperatures required by the model.\nAmong the existing methods, FAO’s PM is\
    \ the most widely adopted model to estimate reference\nET (ETref or ET0) [252].\
    \ The PM method uses incident and reﬂected solar radiation, emitted thermal\n\
    radiation, air temperature, wind speed, and vapour pressure to calculate ET0 [253].\
    \ Remote sensing\nprovides a cost-eﬀective method to estimate the ET0 at regional\
    \ to global scales [241] by estimating\nreﬂected solar and emitted thermal radiation.\
    \ One of the advantages of using the PM approach is\nthat it is parametrised using\
    \ micrometeorological data easily obtained from ground-based automatic\nweather\
    \ stations. However, PM suﬀers from the drawback that canopy transpiration is\
    \ not dynamic\nas inﬂuenced by soil moisture availability via stomatal regulation\
    \ [241]. From a practical standpoint,\nPM-derived ET0 estimates are used in conjunction\
    \ with crop factors or crop coeﬃcients (kc), which are\nclosely related to the\
    \ light interception of the canopy [254].\nCrop evapotranspiration (ETc) is deﬁned\
    \ as the product of kc and ET0. In the absence of accurate\nETc measurements,\
    \ kc is an easy and practical means of getting reliable estimates of ETc using\
    \ ET0 [255].\nIn this regard, studies have focused on the use of remote sensing\
    \ to study spatial variability in kc and\nETc [101,256–258]. Thermal and NIR imagery\
    \ can be used to compute kc and ETc as transpiration\nrate is closely related\
    \ to canopy temperature [259–261] and kc has been shown to correlate with\ncanopy\
    \ reﬂectance [101,255]. Various thermal indices, such as CWSI, canopy temperature\
    \ ratio, canopy\ntemperature above non-stressed, and canopy temperature above\
    \ canopy threshold, can be used to\nestimate ETc, where CWSI- based ETc was found\
    \ to be the most accurate [24].\nET at a larger scale is typically estimated based\
    \ on satellite remote sensing. The temporal resolution\nof satellites is, however,\
    \ low and inadequate for horticultural applications, such as irrigation scheduling\n\
    Agronomy 2020, 10, 140\n17 of 35\n(e.g., Landsat has a 16-day revisit cycle).\
    \ In contrast, high temporal resolution satellites are coarse in\nspatial resolution\
    \ for ﬁeld-scale observations [25]. The daily or even instantaneous estimation\
    \ of ETc at\nthe ﬁeld scale is crucial for irrigation scheduling and is expected\
    \ to have great application prospects\nin the future [240,259,262,263]. In this\
    \ regard, the future direction of satellite-based ET estimates\nmay focus on temporal\
    \ downscaling either by extrapolation of instantaneous measurement [264],\ninterpolation\
    \ between two successive observations [201], data fusion of multiple satellites\
    \ [25,260], and\nspatial downscaling using multiple satellites [265–268]. An example\
    \ of early satellite-based remote\nsensing for ET is the MODIS Global Evapotranspiration\
    \ Project (MOD16), which was established in\n1999 to provide daily estimates of\
    \ global terrestrial evapotranspiration using data acquired from a\npair of NASA\
    \ satellites in conjunction with Algorithm Theoretical Based Documents (ATBDs)\
    \ [269].\nThese estimates correlated well with ground-based eddy covariance ﬂux\
    \ tower estimates of ET despite\ndiﬀerences in the uncertainties associated with\
    \ each of these techniques.\nUASs are being increasingly utilised to acquire multi-spectral\
    \ and thermal imagery to compute\nET at an unprecedented spatial resolution [270,271].\
    \ Using high-resolution images, ﬁltering the\nshadowed-pixel is possible, which\
    \ showed signiﬁcant improvement in the estimation of ET in\ngrapevine [101]. Using\
    \ high-resolution thermal and/or multispectral imagery, ET has been derived for\n\
    horticultural crops, such as grapevines [270] and olives [271]. The seasonal monitoring\
    \ of ETc at high\nspatial and temporal resolutions is of high importance for precision\
    \ irrigation of horticultural crops in\nthe future [259].\n5. Case Studies on\
    \ the Use of Remote Sensing for Crop Water Stress Detection\nThe increasing prevalence\
    \ of UAS along with low-cost camera systems has brought about much\ninterest in\
    \ the characterisation of crop water status/stress during the growing season to\
    \ inform orchard\nor farm management decisions, in particular, irrigation scheduling\
    \ [272,273]. Traditional methodologies\nto assess crop water stress are constrained\
    \ by limitations relating to large farm sizes and accompanying\nspatial variability,\
    \ high labour costs to collect data, and access to instrumentation that is both\
    \ inexpensive\nand portable [272]. The beneﬁts of precision agriculture [274],\
    \ including through precision irrigation\npractices [1], result in higher production\
    \ eﬃciencies and economic returns through site-speciﬁc crop\nmanagement [275,276].\
    \ This approach has motivated the use of high-resolution imagery acquired\nfrom\
    \ remote sensing to identify irrigation zones [99,277]. The ﬁrst horticultural\
    \ applications of UAS\nplatforms for crop water status measurement were in orange\
    \ and peach orchards where both thermal\nand multispectral-derived VIs, speciﬁcally\
    \ the PRI, were shown to be well-correlated to crop water\nstatus [102]. Here,\
    \ we explore the use of remote sensing and accompanying image acquisition platforms\n\
    to characterise the spatial and temporal patterns of the water status of two economically\
    \ important\nhorticultural crops, grapevine and almond.\n5.1. Grapevine (Vitis\
    \ spp.)\nThe characterisation of spatial variability in vine water status in a\
    \ vineyard provides valuable\nguidance on irrigation scheduling decisions [82],\
    \ and this spatial variability can be eﬃciently\ncharacterised by the use of remote\
    \ sensing platforms [29]. The ﬁrst use of remote sensing in vineyards\nfor crop\
    \ water stress detection was using manned aircraft ﬂown over an irrigated vineyard\
    \ in Hanwood\n(NSW) Australia where CWSI was mapped at a spatial resolution of\
    \ 10 cm [278]. Subsequently, UAS\nplatforms began to be used in vineyards for\
    \ vine water stress characterisation. Early work in this\ncrop used a fuel-based\
    \ helicopter with a 29 cc engine and equipped with thermal (Thermovision\nA40M)\
    \ and multispectral (Tetracam MCA-6) camera systems [102]. The study observed\
    \ strong (inverse)\nrelationships between (Tc − Ta) and gs. A related study showed\
    \ strong correlations between thermal\nand multispectral VIs, and traditional,\
    \ ground-based measures of water status, such as Ψleaf and\ngs [182]. In this\
    \ study, normalised PRI was shown to have correlation coeﬃcients exceeding 0.8\
    \ versus\nboth Ψleaf and gs, indicating that remotely-sensed VIs can be reliable\
    \ indicators of vine water status.\nThermal indices, such as (Tc − Ta) and CWSI,\
    \ were also well-correlated to Ψleaf and gs at speciﬁc times of\nAgronomy 2020,\
    \ 10, 140\n18 of 35\nthe day. The use of thermal indices, such as CWSI or Ig,\
    \ requires reference temperatures (Twet, Tdry) or\nnon-water stressed baselines\
    \ (NWSB) [279]. Due to the diﬃculty of obtaining reference temperatures or\nNWSB\
    \ using remote sensing, some authors have used the minimum temperature found from\
    \ all canopy\npixels as Twet [199], and Ta + 5 ◦C as Tdry [213,280]. NWSB is typically\
    \ obtained from well-watered\ncanopies, measuring (Tc − Ta) under a range of vapour\
    \ pressure deﬁcit conditions [279]. Thermal water\nstress indices have also shown\
    \ to be useful to distinguish between water use strategies of diﬀerent\ngrapevine\
    \ cultivars [83,281], which is useful for customising irrigation scheduling based\
    \ on the speciﬁc\nwater needs of a given cultivar. More recently, studies have\
    \ used UAS-based multispectral-based\nVIs to train an artiﬁcial neural network\
    \ (ANN) models to predict spatial patterns of Ψstem [84,282].\nUsing UAS-based\
    \ multispectral data, the authors showed that ANN estimated Ψstem with higher\n\
    accuracy (RMSE lower than 0.15 MPa) as compared to the conventional multispectral\
    \ indices based\nestimation (RMSE over 0.32 MPa).\n5.2. Almond (Prunus Dulcis)\n\
    Almonds are perennial nut trees grown in semi-arid climates and are reliant on\
    \ irrigation\napplications. Their water requirements are relatively high, with\
    \ seasonal ETc exceeding 1000 mm [283].\nThe requirement for prudent irrigation\
    \ management in the face of decreased water availability is\ncritical for maintaining\
    \ tree productivity, yield, and nut quality [284]. Towards this goal, UAS-based\n\
    remote sensing has been used to characterise the spatial patterns of tree water\
    \ status in almond\norchards. A UAS-based thermal camera was used to acquire tree\
    \ the crown temperature data from\na California almond orchard; this temperature\
    \ was used to determine the temperature diﬀerence\nbetween crown and air (Tc −\
    \ Ta) and compared to shaded leaf water potential (Ψsl) [92]. The study\nfound\
    \ a strong negative correlation (R2 = 0.72) between (Tc − Ta) and Ψsl. The same\
    \ authors conducted\na follow on study in Spain on several fruit tree species\
    \ including almond. The negative relationship\n(slope and oﬀset) between (Tc −\
    \ Ta) and Ψstem was observed to vary based on the time of observation;\nmorning\
    \ measurements had weak relationships, whereas afternoon measurements had stronger\n\
    relationships [99]. Their proposed methodology allowed for the spatial characterisation\
    \ of orchard\nwater status on a single-tree basis, demonstrating the utility of\
    \ UAS-based crop water stress data.\nBeyond the characterisation of crop water\
    \ stress for irrigation scheduling, there is an opportunity to\nuse this data\
    \ to quantify the economic impact at a spatial level.\n6. Future Prospective and\
    \ Gaps in the Knowledge\nPrecision irrigation is a promising approach to increase\
    \ farm water use eﬃciency for sustainable\nproduction, including for horticultural\
    \ crops [3,5,9,10,274,285]. It is envisioned that the future of\nprecision irrigation\
    \ will incorporate UAS, manned aircraft, and satellite-based remote sensing platforms\n\
    alongside ground-based proximal sensors coupled with wireless sensor networks.\
    \ The automation\nof UAS technology will continue to develop further to a point\
    \ that even novice users can adopt\nthe technology with ease. It is also expected\
    \ that the data processing pipeline of remote sensing\nimages will become automated\
    \ to be ‘ﬁt for purpose’ for crop water status measurements. The ideal\nsolution\
    \ may lie in the use of satellites (or sometimes manned aircraft) for regional\
    \ estimation and\nplanning [55,260], UAS for seasonal monitoring and zoning [32,100,197,286],\
    \ proximal sensors for\ncontinuous measurement [287], and artiﬁcial intelligence\
    \ to derive decision-ready products [84,282]\nthat can be used for making irrigation\
    \ scheduling decisions [31,288–295]. Continued technological\ndevelopments in\
    \ this space will enable growers to acquire actionable data with ease, and eventually\n\
    transition towards semi-automated or fully-automated irrigation applications.\n\
    Remote sensing and current irrigation application technologies are limited in\
    \ temporal and\nspatial resolution, respectively. Although UAS technology can\
    \ deliver sub-plant level spatially explicit\ninformation of water status, the\
    \ size of the management block is much coarser, typically over 10 m.\nHence, further\
    \ improvements in variable rate application technologies, e.g., boom sprayers,\
    \ or zoned\ndrip irrigation, are required to fully exploit high-resolution UAS\
    \ measurements. Nonetheless, the\nAgronomy 2020, 10, 140\n19 of 35\nrequired resolution\
    \ of remote sensing should be guided by the underlying spatial variability of\
    \ the crop.\nFor ﬁelds with relatively lower spatial variability, low/medium-resolution\
    \ remote sensing imagery\nmay suﬃce for crop water status assessment [278,296,297].\n\
    Remote sensing provides an indirect estimate of plant water status using the regression-based\n\
    approach through several calculated reﬂectance indices. In comparison, physical\
    \ and mechanistic\nmodels, e.g., radiative transfer models and energy balance\
    \ models, incorporate both direct and indirect\nmeasures of the canopy, therefore\
    \ establishing a basis for diﬀerences in plant water status. Using a\nsimilar\
    \ approach, predictions of crop water status using regression-based remote sensing\
    \ models can\nbe improved by incorporating some direct auxiliary variables.\n\
    Further developments in thermal remote sensing are also expected, speciﬁcally,\
    \ the advent of new\nthermal and hybrid thermal-multispectral water status/stress\
    \ indices that are more sensitive to canopy\ntranspiration. The most widely-adopted\
    \ thermal index, CWSI, is an instantaneous measure that is\nnormalised to local\
    \ weather conditions and inﬂuenced by genotype and phenotype. For example,\nthe\
    \ relationship between CWSI and crop water status is inﬂuenced by environmental\
    \ conditions\n(e.g., high incident radiation and low humidity vs low incident\
    \ radiation and high humidity) and\nphenological stage [197,214,298]. As a result,\
    \ corresponding ground-based measurements are required\nfor each temporal remote\
    \ measurement to determine the correlation with water status. Hence, temporal\n\
    assessments of water status using thermal cameras will require the incorporation\
    \ of meteorological\ndata along with the thermal response using novel indices.\n\
    In the area of satellite remote sensing, we foresee further developments on temporal\
    \ downscaling\nto achieve daily measurements. A higher temporal resolution may\
    \ be achieved by fusion of multiple\nsatellite observations, such as freely available\
    \ Landsat and Sentinel. Further reductions of temporal\nresolution will require\
    \ interpolation between two successive observations. Furthermore, temporal\nmodels\
    \ of water status could be developed to assist the interpolation to eventually\
    \ satisfy the\nrequirements for irrigation scheduling [25,201,263]. The continued\
    \ advancement and greater availability\nof Nanosat/Cubesat may provide an alternate\
    \ method to capture high-resolution data at a higher\na greater temporal resolution,\
    \ which can be suitable to study the water status of horticultural\ncrops [299–301].\n\
    Crop water status is a complex phenomenon, which can be interpreted with respect\
    \ to a\nnumber of variables. These variables can include spectral response, thermal\
    \ response, meteorological\ndata, 3D attributes of the canopy, and macrostructure\
    \ of the block (farm).\nClearly, there is\nan opportunity for a multi-disciplinary\
    \ approach, potentially incorporating artiﬁcial intelligence\ntechniques which\
    \ incorporate the aforementioned variables to provide a robust estimation of crop\n\
    water status [84,141,282,302,303]. Furthermore, with machine learning algorithms,\
    \ hyperspectral\nremote sensing will provide a wealth of data to estimate crop\
    \ water status. A quantitative product,\nsuch as SIF, derived from hyperspectral\
    \ data will have the potential for direct quantiﬁcation of water\nstress [204,205,304].\
    \ In this regard, the upcoming FLEX satellite mission [305,306] and recent advances\n\
    in aerial spectroradiometry [109,132,137,307–310] dedicated for observation of\
    \ SIF may be unique and\npowerful tools for high-value horticultural crops.\n\
    Multi-temporal images represent an excellent resource for seasonal monitoring\
    \ of changes in crop\nwater status. Five to six temporal points of data acquisition\
    \ at critical phenological stages of crop\ndevelopment have been recommended for\
    \ irrigation scheduling [31,32]. However, for semi-arid or arid\nregions, irrigation\
    \ is typically required multiple times per week. Acquisition and post-processing\
    \ of\nremote sensing data for actionable products multiple times a week is currently\
    \ logistically unfeasible.\nThe fusion of UAS-based remote sensing data, continuous\
    \ ground-based proximal or direct sensors,\nincluding weather station data, can\
    \ potentially inform daily estimates of water status at canopy level.\nThis approach\
    \ will require predictive models, such as those based on machine learning algorithms,\
    \ to\nestimate the current and future water status of the crop. Eventually, growers\
    \ would beneﬁt from the\nknowledge of crop water requirements for the determination\
    \ of seasonal irrigation requirements to\nsustainably farm into the future.\n\
    Agronomy 2020, 10, 140\n20 of 35\nOne vision for the future of precision irrigation\
    \ is in automated pipelines to explicitly manage\nirrigation water at the sub-block\
    \ level. This automated pipeline would likely include remote and\nproximal data\
    \ acquisition and processing, prediction and interpretation of crop water status\
    \ and\nrequirements, and subsequently, control of irrigation systems. Recent rapid\
    \ developments in cloud\ncomputing and wireless technology could assist in the\
    \ quasi-real-time processing of the remote sensing\ndata soon after acquisition\
    \ [311–313]. Eventually, automation and computational power will merge to\ndevelop\
    \ smart technology in which artiﬁcial intelligence uses real-time data analysis\
    \ for diagnosis\nand decision-making. Growers of the future will be able to take\
    \ advantage of precise irrigation\nrecommendations using information sourced from\
    \ a ﬂeet of UAS that map large farm blocks on a daily\nschedule, continuous ground-based\
    \ proximal and direct sensors, and weather stations. This data can be\nstored\
    \ on and accessed from the cloud almost instantaneously, used in conjunction with\
    \ post-processing\nalgorithms for decision-making on optimised irrigation applications\
    \ [311,314].\n7. Conclusions\nThis paper provides a comprehensive review of the\
    \ use of remote sensing to determine the water\nstatus of horticultural crops.\
    \ One of our objectives was to survey the range of remote sensing tools\navailable\
    \ for irrigation decision-making. Earth observation satellite systems possess\
    \ the required bands\nto study the water status of vegetation and soil. Satellites\
    \ are more suitable for scouting, planning,\nand management of irrigation applications\
    \ that involve large areas, and where data acquisition is\nnot time-constrained.\
    \ Manned aircraft are sparingly used in horticultural applications due to the\n\
    cost, logistics, and speciﬁc expertise needed for the operation of the platform.\
    \ UAS-based remote\nsensing provides ﬂexibility in spatial resolution (crop level\
    \ observation achievable), coverage (over\n25 ha achievable in a single ﬂight),\
    \ spectral bands, as well as temporal revisit. Routine monitoring of\nhorticultural\
    \ crops for water status characterisation is, therefore, best performed using\
    \ a UAS platform.\nWe envision a future for precision irrigation where satellites\
    \ are used for planning, and UAS used in\nconjunction with a network of ground-based\
    \ sensors to achieve actionable products on a timely basis.\nThe plant’s instantaneous\
    \ response to water stress can be captured using thermal cameras (via\nindices,\
    \ such as CWSI) and potentially narrow-band hyperspectral sensors (via, for example,\
    \ SIF),\nmaking them suitable to draw quantiﬁable decisions with regard to irrigation\
    \ scheduling. Broadband\nmultispectral and RGB cameras capture the non-instantaneous\
    \ water status of crops, making them\nsuitable for general assessment of crop\
    \ water status. Integrated use of thermal and multispectral\nimagery may be the\
    \ simplest yet eﬀective sensor combinations to capture the overall as well as\n\
    instantaneous water status of the plant. With regard to irrigation scheduling,\
    \ further developments\nare required to establish crop-speciﬁc thresholds of remotely-sensed\
    \ indices to decide when and how\nmuch to irrigate.\nAuthor Contributions: Performed\
    \ the article review and prepared the original draft, D.G.; contributed to write\n\
    the case studies, V.P., and together with D.G. contributed to review and edit\
    \ the manuscript. All authors have read\nand agreed to the published version of\
    \ the manuscript.\nFunding: This research and the APC was funded by Wine Australia\
    \ (Grant number: UA 1803-1.3).\nAcknowledgments: The authors would like to acknowledge\
    \ the funding body Wine Australia, The University of\nAdelaide, and anonymous\
    \ reviewers for their contribution.\nConﬂicts of Interest: The authors declare\
    \ no conﬂict of interest.\nReferences\n1.\nMonaghan, J.M.; Daccache, A.; Vickers,\
    \ L.H.; Hess, T.M.; Weatherhead, E.K.; Grove, I.G.; Knox, J.W. More\n‘crop per\
    \ drop’: constraints and opportunities for precision irrigation in European agriculture.\
    \ J. Sci.\nFood Agric. 2013, 93, 977–980. [CrossRef]\n2.\nSmith, R. Review of\
    \ Precision Irrigation Technologies and Their Applications; University of Southern\
    \ Queensland\nDarling Heights: Queensland, Australia, 2011.\nAgronomy 2020, 10,\
    \ 140\n21 of 35\n3.\nPiao, S.; Ciais, P.; Huang, Y.; Shen, Z.; Peng, S.; Li, J.;\
    \ Zhou, L.; Liu, H.; Ma, Y.; Ding, Y.; et al. The impacts of\nclimate change on\
    \ water resources and agriculture in China. Nature 2010, 467, 43. [CrossRef]\n\
    4.\nHowden, S.M.; Soussana, J.-F.; Tubiello, F.N.; Chhetri, N.; Dunlop, M.; Meinke,\
    \ H. Adapting agriculture to\nclimate change. Proc. Natl. Acad. Sci. USA 2007,\
    \ 104, 19691–19696. [CrossRef]\n5.\nWebb, L.; Whiting, J.; Watt, A.; Hill, T.;\
    \ Wigg, F.; Dunn, G.; Needs, S.; Barlow, E. Managing grapevines through\nsevere\
    \ heat: A survey of growers after the 2009 summer heatwave in south-eastern Australia.\
    \ J. Wine Res.\n2010, 21, 147–165. [CrossRef]\n6.\nDatta, S. Impact of climate\
    \ change in Indian horticulture-a review. Int. J. Sci. Environ. Technol. 2013,\
    \ 2,\n661–671.\n7.\nWebb, L.; Whetton, P.; Barlow, E. Modelled impact of future\
    \ climate change on the phenology of winegrapes\nin Australia. Aust. J. Grape\
    \ Wine Res. 2007, 13, 165–175. [CrossRef]\n8.\nWang, J.; Mendelsohn, R.; Dinar,\
    \ A.; Huang, J.; Rozelle, S.; Zhang, L. The impact of climate change on China’s\n\
    agriculture. Agric. Econ. 2009, 40, 323–337. [CrossRef]\n9.\nBeare, S.; Heaney,\
    \ A. Climate change and water resources in the Murray Darling Basin, Australia.\n\
    In Proceedings of the 2002 World Congress of Environmental and Resource Economists,\
    \ Monterey, CA, USA,\n24–27 June 2002.\n10.\nKhan, S.; Tariq, R.; Yuanlai, C.;\
    \ Blackwell, J. Can irrigation be sustainable? Agric. Water Manag. 2006, 80,\n\
    87–99. [CrossRef]\n11.\nDroogers, P.; Bastiaanssen, W. Irrigation performance\
    \ using hydrological and remote sensing modeling.\nJ. Irrig. Drain. Eng. 2002,\
    \ 128, 11–18. [CrossRef]\n12.\nRay, S.; Dadhwal, V. Estimation of crop evapotranspiration\
    \ of irrigation command area using remote sensing\nand GIS. Agric. Water Manag.\
    \ 2001, 49, 239–249. [CrossRef]\n13.\nKim, Y.; Evans, R.G.; Iversen, W.M. Remote\
    \ sensing and control of an irrigation system using a distributed\nwireless sensor\
    \ network. IEEE Trans. Instrum. Meas. 2008, 57, 1379–1387.\n14.\nRitchie, G.A.;\
    \ Hinckley, T.M. The pressure chamber as an instrument for ecological research.\
    \ In Advances in\nEcological Research; Elsevier: Amsterdam, The Netherlands, 1975;\
    \ Volume 9, pp. 165–254.\n15.\nSmart, R.; Barrs, H. The eﬀect of environment and\
    \ irrigation interval on leaf water potential of four\nhorticultural species.\
    \ Agric. Meteorol. 1973, 12, 337–346. [CrossRef]\n16.\nMeron, M.; Grimes, D.;\
    \ Phene, C.; Davis, K. Pressure chamber procedures for leaf water potential\n\
    measurements of cotton. Irrig. Sci. 1987, 8, 215–222. [CrossRef]\n17.\nSantos,\
    \ A.O.; Kaye, O. Grapevine leaf water potential based upon near infrared spectroscopy.\
    \ Sci. Agric.\n2009, 66, 287–292. [CrossRef]\n18.\nBerni, J.A.J.; Zarco-Tejada,\
    \ P.J.; Sepulcre-Cantó, G.; Fereres, E.; Villalobos, F. Mapping canopy conductance\n\
    and CWSI in olive orchards using high resolution thermal remote sensing imagery.\
    \ Remote Sens. Environ.\n2009, 113, 2380–2388. [CrossRef]\n19.\nChaves, M.M.;\
    \ Santos, T.P.; de Souza, C.; Ortuño, M.; Rodrigues, M.; Lopes, C.; Maroco, J.;\
    \ Pereira, J.S.\nDeﬁcit irrigation in grapevine improves water-use eﬃciency while\
    \ controlling vigour and production quality.\nAnn. Appl. Biol. 2007, 150, 237–252.\
    \ [CrossRef]\n20.\nBravdo, B.; Hepner, Y.; Loinger, C.; Cohen, S.; Tabacman, H.\
    \ Eﬀect of irrigation and crop level on growth,\nyield and wine quality of Cabernet\
    \ Sauvignon. Am. J. Enol. Vitic. 1985, 36, 132–139.\n21.\nMatthews, M.; Ishii,\
    \ R.; Anderson, M.; O’Mahony, M. Dependence of wine sensory attributes on vine\
    \ water\nstatus. J. Sci. Food Agric. 1990, 51, 321–335. [CrossRef]\n22.\nReynolds,\
    \ A.G.; Naylor, A.P. ‘Pinot noir’ and ‘Riesling’ grapevines respond to water stress\
    \ duration and soil\nwater-holding capacity. HortScience 1994, 29, 1505–1510.\
    \ [CrossRef]\n23.\nAlvino, A.; Marino, S. Remote sensing for irrigation of horticultural\
    \ crops. Horticulturae 2017, 3, 40. [CrossRef]\n24.\nKullberg, E.G.; DeJonge,\
    \ K.C.; Chávez, J.L. Evaluation of thermal remote sensing indices to estimate\
    \ crop\nevapotranspiration coeﬃcients. Agric. Water Manag. 2017, 179, 64–73. [CrossRef]\n\
    25.\nSemmens, K.A.; Anderson, M.C.; Kustas, W.P.; Gao, F.; Alﬁeri, J.G.; McKee,\
    \ L.; Prueger, J.H.; Hain, C.R.;\nCammalleri, C.; Yang, Y.; et al. Monitoring\
    \ daily evapotranspiration over two California vineyards using\nLandsat 8 in a\
    \ multi-sensor data fusion approach. Remote Sens. Environ. 2015, 185, 155–170.\
    \ [CrossRef]\n26.\nJackson, R.D. Remote sensing of biotic and abiotic plant stress.\
    \ Annu. Rev. Phytopathol. 1986, 24, 265–287.\n[CrossRef]\nAgronomy 2020, 10, 140\n\
    22 of 35\n27.\nMoran, M.; Clarke, T.; Inoue, Y.; Vidal, A. Estimating crop water\
    \ deﬁcit using the relation between surface-air\ntemperature and spectral vegetation\
    \ index. Remote Sens. Environ. 1994, 49, 246–263. [CrossRef]\n28.\nLamb, D.; Hall,\
    \ A.; Louis, J. Airborne remote sensing of vines for canopy variability and productivity.\n\
    Aust. Grapegrow. Winemak. 2001, 449a, 89–94.\n29.\nHall, A.; Lamb, D.; Holzapfel,\
    \ B.; Louis, J. Optical remote sensing applications in viticulture-a review. Aust.\
    \ J.\nGrape Wine Res. 2002, 8, 36–47. [CrossRef]\n30.\nDe Bei, R.; Cozzolino,\
    \ D.; Sullivan, W.; Cynkar, W.; Fuentes, S.; Dambergs, R.; Pech, J.; Tyerman,\
    \ S.\nNon-destructive measurement of grapevine water potential using near infrared\
    \ spectroscopy. Aust. J. Grape\nWine Res. 2011, 17, 62–71. [CrossRef]\n31.\nBellvert,\
    \ J.; Zarco-Tejada, P.J.; Marsal, J.; Girona, J.; González-Dugo, V.; Fereres,\
    \ E. Vineyard irrigation\nscheduling based on airborne thermal imagery and water\
    \ potential thresholds. Aust. J. Grape Wine Res. 2016,\n22, 307–315. [CrossRef]\n\
    32.\nBellvert, J.; Marsal, J.; Girona, J.; Gonzalez-Dugo, V.; Fereres, E.; Ustin,\
    \ S.; Zarco-Tejada, P. Airborne thermal\nimagery to detect the seasonal evolution\
    \ of crop water status in peach, nectarine and Saturn peach orchards.\nRemote\
    \ Sens. 2016, 8, 39. [CrossRef]\n33.\nPark, S.; Ryu, D.; Fuentes, S.; Chung, H.;\
    \ Hernández-Montes, E.; O’Connell, M. Adaptive estimation of crop\nwater stress\
    \ in nectarine and peach orchards using high-resolution imagery from an unmanned\
    \ aerial vehicle\n(UAV). Remote Sens. 2017, 9, 828. [CrossRef]\n34.\nEspinoza,\
    \ C.Z.; Khot, L.R.; Sankaran, S.; Jacoby, P.W. High resolution multispectral and\
    \ thermal remote\nsensing-based water stress assessment in subsurface irrigated\
    \ grapevines. Remote Sens. 2017, 9, 961.\n[CrossRef]\n35.\nEzenne, G.I.; Jupp,\
    \ L.; Mantel, S.K.; Tanner, J.L. Current and potential capabilities of UAS for\
    \ crop water\nproductivity in precision agriculture. Agric. Water Manag. 2019,\
    \ 218, 158–164. [CrossRef]\n36.\nOliver, M.A.; Webster, R. Kriging: A method of\
    \ interpolation for geographical information systems. Int. J.\nGeogr. Inf. Syst.\
    \ 1990, 4, 313–332. [CrossRef]\n37.\nHa, W.; Gowda, P.H.; Howell, T.A. A review\
    \ of downscaling methods for remote sensing-based irrigation\nmanagement: Part\
    \ I. Irrig. Sci. 2013, 31, 831–850. [CrossRef]\n38.\nHa, W.; Gowda, P.H.; Howell,\
    \ T.A. A review of potential image fusion methods for remote sensing-based\nirrigation\
    \ management: Part II. Irrig. Sci. 2013, 31, 851–869. [CrossRef]\n39.\nBelward,\
    \ A.S.; Skøien, J.O. Who launched what, when and why; trends in global land-cover\
    \ observation\ncapacity from civilian earth observation satellites. ISPRS J. Photogramm.\
    \ Remote Sens. 2015, 103, 115–128.\n[CrossRef]\n40.\nLucieer, A.; Malenovskỳ,\
    \ Z.; Veness, T.; Wallace, L. HyperUAS—Imaging spectroscopy from a multirotor\n\
    unmanned aircraft system. J. Field Robot. 2014, 31, 571–590. [CrossRef]\n41.\n\
    McCabe, M.F.; Rodell, M.; Alsdorf, D.E.; Miralles, D.G.; Uijlenhoet, R.; Wagner,\
    \ W.; Lucieer, A.; Houborg, R.;\nVerhoest, N.E.; Franz, T.E.; et al. The future\
    \ of Earth observation in hydrology. Hydrol. Earth Syst. Sci. 2017,\n21, 3879.\
    \ [CrossRef]\n42.\nMatese, A.; Toscano, P.; Di Gennaro, S.; Genesio, L.; Vaccari,\
    \ F.; Primicerio, J.; Belli, C.; Zaldei, A.; Bianconi, R.;\nGioli, B. Intercomparison\
    \ of UAV, aircraft and satellite remote sensing platforms for precision viticulture.\n\
    Remote Sens. 2015, 7, 2971–2990. [CrossRef]\n43.\nMancini, A.; Frontoni, E.; Zingaretti,\
    \ P. Satellite and UAV data for Precision Agriculture Applications.\nIn Proceedings\
    \ of the 2019 International Conference on Unmanned Aircraft Systems (ICUAS 2019),\
    \ Atlanta,\nGA, USA, 11–14 June 2019; pp. 491–497.\n44.\nDiago, M.P.; Bellincontro,\
    \ A.; Scheidweiler, M.; Tardáguila, J.; Tittmann, S.; Stoll, M. Future opportunities\n\
    of proximal near infrared spectroscopy approaches to determine the variability\
    \ of vineyard water status.\nAust. J. Grape Wine Res. 2017, 23, 409–414. [CrossRef]\n\
    45.\nGutierrez, S.; Diago, M.P.; Fernández-Novales, J.; Tardaguila, J. Vineyard\
    \ water status assessment using\non-the-go thermal imaging and machine learning.\
    \ PLoS ONE 2018, 13, e0192037. [CrossRef] [PubMed]\n46.\nFernández-Novales, J.;\
    \ Tardaguila, J.; Gutiérrez, S.; Marañón, M.; Diago, M.P. In ﬁeld quantiﬁcation\
    \ and\ndiscrimination of diﬀerent vineyard water regimes by on-the-go NIR spectroscopy.\
    \ Biosyst. Eng. 2018, 165,\n47–58. [CrossRef]\nAgronomy 2020, 10, 140\n23 of 35\n\
    47.\nDiago, M.P.; Fernández-Novales, J.; Gutiérrez, S.; Marañón, M.; Tardaguila,\
    \ J. Development and validation of a\nnew methodology to assess the vineyard water\
    \ status by on-the-go near infrared spectroscopy. Front. Plant Sci.\n2018, 9,\
    \ 59. [CrossRef]\n48.\nAquino, A.; Millan, B.; Diago, M.-P.; Tardaguila, J. Automated\
    \ early yield prediction in vineyards from\non-the-go image acquisition. Comput.\
    \ Electron. Agric. 2018, 144, 26–36. [CrossRef]\n49.\nMarkham, B.L.; Helder, D.L.\
    \ Forty-year calibrated record of earth-reﬂected radiance from Landsat: A review.\n\
    Remote Sens. Environ. 2012, 122, 30–40. [CrossRef]\n50.\nToth, C.; Jó´zków, G.\
    \ Remote sensing platforms and sensors: A survey. ISPRS J. Photogramm. Remote\
    \ Sens.\n2016, 115, 22–36. [CrossRef]\n51.\nTyc, G.; Tulip, J.; Schulten, D.;\
    \ Krischke, M.; Oxfort, M. The RapidEye mission design. Acta Astronaut. 2005,\n\
    56, 213–219. [CrossRef]\n52.\nSweeting, M.N. Modern small satellites-changing\
    \ the economics of space. Proc. IEEE 2018, 106, 343–361.\n[CrossRef]\n53.\nKhanal,\
    \ S.; Fulton, J.; Shearer, S. An overview of current and potential applications\
    \ of thermal remote sensing\nin precision agriculture. Comput. Electron. Agric.\
    \ 2017, 139, 22–32. [CrossRef]\n54.\nGerhards, M.;\nSchlerf, M.;\nMallick, K.;\n\
    Udelhoven, T. Challenges and Future Perspectives of\nMulti-/Hyperspectral Thermal\
    \ Infrared Remote Sensing for Crop Water-Stress Detection: A Review.\nRemote Sens.\
    \ 2019, 11, 1240. [CrossRef]\n55.\nRyan, S.; Lewis, M. Mapping soils using high\
    \ resolution airborne imagery, Barossa Valley, SA. In Proceedings\nof the Inaugural\
    \ Australian Geospatial Information and Agriculture Conference Incorporating Precision\n\
    Agriculture in Australasia 5th Annual Symposium, Orange, NSW, Australia, 17–19\
    \ July 2001.\n56.\nKhaliq, A.; Comba, L.; Biglia, A.; Ricauda Aimonino, D.; Chiaberge,\
    \ M.; Gay, P. Comparison of satellite and\nUAV-based multispectral imagery for\
    \ vineyard variability assessment. Remote Sens. 2019, 11, 436. [CrossRef]\n57.\n\
    Jones, H.G.; Vaughan, R.A. Remote Sensing of Vegetation: Principles, Techniques,\
    \ and Applications; Oxford\nUniversity Press: Oxford, UK, 2010.\n58.\nThenkabail,\
    \ P.S.; Lyon, J.G. Hyperspectral Remote Sensing of Vegetation; CRC Press: Boco\
    \ Raton, FL, USA, 2016.\n59.\nKing, M.D.; Platnick, S.; Menzel, W.P.; Ackerman,\
    \ S.A.; Hubanks, P.A. Spatial and temporal distribution of\nclouds observed by\
    \ MODIS onboard the Terra and Aqua satellites. IEEE Trans. Geosci. Remote Sens.\
    \ 2013, 51,\n3826–3852. [CrossRef]\n60.\nChen, X.; Liu, M.; Zhu, X.; Chen, J.;\
    \ Zhong, Y.; Cao, X. “Blend-then-Index” or “Index-then-Blend”:\nA Theoretical\
    \ Analysis for Generating High-resolution NDVI Time Series by STARFM. Photogramm.\
    \ Eng.\nRemote Sens. 2018, 84, 65–73. [CrossRef]\n61.\nYin, T.; Inglada, J.; Osman,\
    \ J. Time series image fusion: Application and improvement of STARFM for land\n\
    cover map and production. In Proceedings of the 2012 IEEE International Geoscience\
    \ and Remote Sensing\nSymposium, Munich, Germany, 22–27 July 2012; pp. 378–381.\n\
    62.\nGevaert, C.M.; García-Haro, F.J. A comparison of STARFM and an unmixing-based\
    \ algorithm for Landsat\nand MODIS data fusion. Remote Sens. Environ. 2015, 156,\
    \ 34–44. [CrossRef]\n63.\nLi, L.; Wang, X.; Li, M. Study on the fusion of MODIS\
    \ and TM images using the spectral response function\nand STARFM algorithm. In\
    \ Proceedings of the 2011 International Conference on Image Analysis and Signal\n\
    Processing, Wuhan, China, 21–23 October 2011; pp. 171–176.\n64.\nPagay, V.; Kidman,\
    \ C.M. Evaluating Remotely-Sensed Grapevine (Vitis vinifera L.) Water Stress Responses\n\
    Across a Viticultural Region. Agronomy 2019, 9, 682. [CrossRef]\n65.\nRascher,\
    \ U.; Alonso, L.; Burkart, A.; Cilia, C.; Cogliati, S.; Colombo, R.; Damm, A.;\
    \ Drusch, M.; Guanter, L.;\nHanus, J.; et al. Sun-induced ﬂuorescence—A new probe\
    \ of photosynthesis: First maps from the imaging\nspectrometer HyPlant. Glob.\
    \ Chang. Biol. 2015, 21, 4673–4684. [CrossRef]\n66.\nBuckley, S.; Vallet, J.;\
    \ Braathen, A.; Wheeler, W. Oblique helicopter-based laser scanning for digital\
    \ terrain\nmodelling and visualisation of geological outcrops. Int. Arch. Photogramm.\
    \ Remote Sens. Spat. Inf. Sci. 2008,\n37, 1–6.\n67.\nPullanagari, R.; Kereszturi,\
    \ G.; Yule, I. Mapping of macro and micro nutrients of mixed pastures using\n\
    airborne AisaFENIX hyperspectral imagery. ISPRS J. Photogramm. Remote Sens. 2016,\
    \ 117, 1–10. [CrossRef]\n68.\nHaboudane, D.; Miller, J.R.; Tremblay, N.; Zarco-Tejada,\
    \ P.J.; Dextraze, L. Integrated narrow-band vegetation\nindices for prediction\
    \ of crop chlorophyll content for application to precision agriculture. Remote\
    \ Sens.\nEnviron. 2002, 81, 416–426. [CrossRef]\nAgronomy 2020, 10, 140\n24 of\
    \ 35\n69.\nMiao, Y.; Mulla, D.J.; Randall, G.W.; Vetsch, J.A.; Vintila, R. Predicting\
    \ chlorophyll meter readings with aerial\nhyperspectral remote sensing for in-season\
    \ site-speciﬁc nitrogen management of corn. Precis. Agric. 2007, 7,\n635–641.\n\
    70.\nHaboudane, D.; Miller, J.R.; Pattey, E.; Zarco-Tejada, P.J.; Strachan, I.B.\
    \ Hyperspectral vegetation indices\nand novel algorithms for predicting green\
    \ LAI of crop canopies: Modeling and validation in the context of\nprecision agriculture.\
    \ Remote Sens. Environ. 2004, 90, 337–352. [CrossRef]\n71.\nSepulcre-Cantó, G.;\
    \ Zarco-Tejada, P.J.; Jiménez-Muñoz, J.; Sobrino, J.; Soriano, M.; Fereres, E.;\
    \ Vega, V.;\nPastor, M. Monitoring yield and fruit quality parameters in open-canopy\
    \ tree crops under water stress.\nImplications for ASTER. Remote Sens. Environ.\
    \ 2007, 107, 455–470. [CrossRef]\n72.\nSepulcre-Cantó, G.; Zarco-Tejada, P.J.;\
    \ Jiménez-Muñoz, J.; Sobrino, J.; De Miguel, E.; Villalobos, F.J. Detection\n\
    of water stress in an olive orchard with thermal remote sensing imagery. Agric.\
    \ For. Meteorol. 2006, 136,\n31–44. [CrossRef]\n73.\nColomina, I.; Molina, P.\
    \ Unmanned aerial systems for photogrammetry and remote sensing: A review.\nISPRS\
    \ J. Photogramm. Remote Sens. 2014, 92, 79–97. [CrossRef]\n74.\nZecha, C.; Link,\
    \ J.; Claupein, W. Mobile sensor platforms: Categorisation and research applications\
    \ in\nprecision farming. J. Sens. Sens. Syst. 2013, 2, 51–72. [CrossRef]\n75.\n\
    Urbahs, A.; Jonaite, I. Features of the use of unmanned aerial vehicles for agriculture\
    \ applications. Aviation\n2013, 17, 170–175. [CrossRef]\n76.\nGautam, D.; Ha,\
    \ C. Control of a quadrotor using a smart self-tuning fuzzy PID controller. Int.\
    \ J. Adv. Robot.\nSyst. 2013, 10, 380. [CrossRef]\n77.\nShi, Y.; Thomasson, J.A.;\
    \ Murray, S.C.; Pugh, N.A.; Rooney, W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan,\
    \ C.L.;\nNeely, H.L.; et al. Unmanned aerial vehicles for high-throughput phenotyping\
    \ and agronomic research.\nPLoS ONE 2016, 11, e0159781. [CrossRef]\n78.\nZhang,\
    \ C.; Kovacs, J.M. The application of small unmanned aerial systems for precision\
    \ agriculture: a review.\nPrecis. Agric. 2012, 13, 693–712. [CrossRef]\n79.\n\
    Mulla, D.J. Twenty ﬁve years of remote sensing in precision agriculture: Key advances\
    \ and remaining\nknowledge gaps. Biosyst. Eng. 2013, 114, 358–371. [CrossRef]\n\
    80.\nHuang, Y.; Thomson, S.J.; Hoﬀmann, W.C.; Lan, Y.; Fritz, B.K. Development\
    \ and prospect of unmanned aerial\nvehicle technologies for agricultural production\
    \ management. Int. J. Agric. Biol. Eng. 2013, 6, 1–10.\n81.\nZude-Sasse, M.; Fountas,\
    \ S.; Gemtos, T.A.; Abu-Khalaf, N. Applications of precision agriculture in horticultural\n\
    crops. Eur. J. Hortic. Sci. 2016, 81, 78–90. [CrossRef]\n82.\nBaluja, J.; Diago,\
    \ M.P.; Balda, P.; Zorer, R.; Meggio, F.; Morales, F.; Tardaguila, J. Assessment\
    \ of vineyard\nwater status variability by thermal and multispectral imagery using\
    \ an unmanned aerial vehicle (UAV).\nIrrig. Sci. 2012, 30, 511–522. [CrossRef]\n\
    83.\nMatese, A.; Baraldi, R.; Berton, A.; Cesaraccio, C.; Di Gennaro, S.F.; Duce,\
    \ P.; Facini, O.; Mameli, M.G.;\nPiga, A.; Zaldei, A. Estimation of water stress\
    \ in grapevines using proximal and remote sensing methods.\nRemote Sens. 2018,\
    \ 10, 114. [CrossRef]\n84.\nPoblete, T.; Ortega-Farías, S.; Moreno, M.; Bardeen,\
    \ M. Artiﬁcial neural network to predict vine water status\nspatial variability\
    \ using multispectral information obtained from an unmanned aerial vehicle (UAV).\
    \ Sensors\n2017, 17, 2488. [CrossRef]\n85.\nGonzalez-Dugo, V.; Zarco-Tejada, P.J.;\
    \ Fereres, E. Applicability and limitations of using the crop water stress\nindex\
    \ as an indicator of water deﬁcits in citrus orchards. Agric. For. Meteorol. 2014,\
    \ 198, 94–104. [CrossRef]\n86.\nStagakis, S.; González-Dugo, V.; Cid, P.; Guillén-Climent,\
    \ M.L.; Zarco-Tejada, P.J. Monitoring water stress\nand fruit quality in an orange\
    \ orchard under regulated deﬁcit irrigation using narrow-band structural and\n\
    physiological remote sensing indices. ISPRS J. Photogramm. Remote Sens. 2012,\
    \ 71, 47–61. [CrossRef]\n87.\nAgam, N.; Cohen, Y.; Berni, J.A.J.; Alchanatis,\
    \ V.; Kool, D.; Dag, A.; Yermiyahu, U.; Ben-Gal, A. An insight to\nthe performance\
    \ of crop water stress index for olive trees. Agric. Water Manag. 2013, 118, 79–86.\
    \ [CrossRef]\n88.\nPoblete-Echeverría, C.; Sepulveda-Reyes, D.; Ortega-Farias,\
    \ S.; Zuñiga, M.; Fuentes, S. Plant water stress\ndetection based on aerial and\
    \ terrestrial infrared thermography: A study case from vineyard and olive\norchard.\
    \ In Proceedings of the XXIX International Horticultural congress on Horticulture:\
    \ Sustaining Lives,\nLivelihoods and Landscapes (IHC2014): International Symposia\
    \ on Water, Eco-Eﬃciency and Transformation\nof Organic Waste in Horticultural\
    \ Production, Brisbane, Australia, 25 October 2016; pp. 141–146.\nAgronomy 2020,\
    \ 10, 140\n25 of 35\n89.\nTesti, L.; Goldhamer, D.; Iniesta, F.; Salinas, M. Crop\
    \ water stress index is a sensitive water stress indicator in\npistachio trees.\
    \ Irrig. Sci. 2008, 26, 395–405. [CrossRef]\n90.\nGonzalez-Dugo, V.; Goldhamer,\
    \ D.; Zarco-Tejada, P.J.; Fereres, E. Improving the precision of irrigation in\
    \ a\npistachio farm using an unmanned airborne thermal system. Irrig. Sci. 2015,\
    \ 33, 43–52. [CrossRef]\n91.\nGarcía-Tejero, I.F.; Rubio, A.E.; Viñuela, I.; Hernández,\
    \ A.; Gutiérrez-Gordillo, S.; Rodríguez-Pleguezuelo, C.R.;\nDurán-Zuazo, V.H.\
    \ Thermal imaging at plant level to assess the crop-water status in almond trees\
    \ (cv. Guara)\nunder deﬁcit irrigation strategies. Agric. Water Manag. 2018, 208,\
    \ 176–186. [CrossRef]\n92.\nGonzalez-Dugo, V.; Zarco-Tejada, P.; Berni, J.A.;\
    \ Suárez, L.; Goldhamer, D.; Fereres, E. Almond tree canopy\ntemperature reveals\
    \ intra-crown variability that is water stress-dependent. Agric. For. Meteorol.\
    \ 2012, 154,\n156–165. [CrossRef]\n93.\nZhao, T.; Stark, B.; Chen, Y.; Ray, A.L.;\
    \ Doll, D. Challenges in water stress quantiﬁcation using small\nunmanned aerial\
    \ system (sUAS): Lessons from a growing season of almond. J. Intell. Robot. Syst.\
    \ 2017, 88,\n721–735. [CrossRef]\n94.\nZhao, T.; Doll, D.; Wang, D.; Chen, Y.\
    \ A new framework for UAV-based remote sensing data processing and\nits application\
    \ in almond water stress quantiﬁcation. In Proceedings of the 2017 International\
    \ Conference on\nUnmanned Aircraft Systems (ICUAS 2017), Miami, FL, USA, 13–16\
    \ June 2017; pp. 1794–1799.\n95.\nHerwitz, S.; Johnson, L.; Dunagan, S.; Higgins,\
    \ R.; Sullivan, D.; Zheng, J.; Lobitz, B.; Leung, J.; Gallmeyer, B.;\nAoyagi,\
    \ M.; et al. Imaging from an unmanned aerial vehicle: agricultural surveillance\
    \ and decision support.\nComput. Electron. Agric. 2004, 44, 49–61. [CrossRef]\n\
    96.\nFurfaro, R.; Ganapol, B.D.; Johnson, L.; Herwitz, S. Model-based neural network\
    \ algorithm for coﬀee ripeness\nprediction using Helios UAV aerial images. In\
    \ Remote Sensing for Agriculture, Ecosystems, and Hydrology VII;\nInternational\
    \ Society for Optics and Photonics: Bruges, Belgium, 2005; Volume 5976, p. 59760X.\n\
    97.\nPark, S.; Nolan, A.; Ryu, D.; Fuentes, S.; Hernandez, E.; Chung, H.; O’connell,\
    \ M. Estimation of crop\nwater stress in a nectarine orchard using high-resolution\
    \ imagery from unmanned aerial vehicle (UAV).\nIn Proceedings of the 21st International\
    \ Congress on Modelling and Simulation, Gold Coast, QLD, Australia,\n29 November–4\
    \ December 2015; pp. 1413–1419.\n98.\nBulanon, D.M.; Lonai, J.; Skovgard, H.;\
    \ Fallahi, E. Evaluation of diﬀerent irrigation methods for an apple\norchard\
    \ using an aerial imaging system. ISPRS Int. J. Geo-Inf. 2016, 5, 79.\n99.\nGonzalez-Dugo,\
    \ V.; Zarco-Tejada, P.; Nicolás, E.; Nortes, P.A.; Alarcón, J.; Intrigliolo, D.S.;\
    \ Fereres, E. Using\nhigh resolution UAV thermal imagery to assess the variability\
    \ in the water status of ﬁve fruit tree species\nwithin a commercial orchard.\
    \ Precis. Agric. 2013, 14, 660–678. [CrossRef]\n100. Santesteban, L.G.; Di Gennaro,\
    \ S.F.; Herrero-Langreo, A.; Miranda, C.; Royo, J.B.; Matese, A. High-resolution\n\
    UAV-based thermal imaging to estimate the instantaneous and seasonal variability\
    \ of plant water status\nwithin a vineyard. Agric. Water Manag. 2017, 183, 49–59.\
    \ [CrossRef]\n101. Aboutalebi, M.; Torres-Rua, A.F.; Kustas, W.P.; Nieto, H.;\
    \ Coopmans, C.; McKee, M. Assessment of diﬀerent\nmethods for shadow detection\
    \ in high-resolution optical imagery and evaluation of shadow impact on\ncalculation\
    \ of NDVI, and evapotranspiration. Irrig. Sci. 2018, 1, 1–23. [CrossRef]\n102.\
    \ Berni, J.A.; Zarco-Tejada, P.J.; Suárez, L.; Fereres, E. Thermal and narrowband\
    \ multispectral remote sensing\nfor vegetation monitoring from an unmanned aerial\
    \ vehicle. IEEE Trans. Geosci. Remote Sens. 2009, 47,\n722–738. [CrossRef]\n103.\
    \ Candiago, S.; Remondino, F.; De Giglio, M.; Dubbini, M.; Gattelli, M. Evaluating\
    \ multispectral images and\nvegetation indices for precision farming applications\
    \ from UAV images. Remote Sens. 2015, 7, 4026–4047.\n[CrossRef]\n104. Thomasson,\
    \ J.A.; Shi, Y.; Olsenholler, J.; Valasek, J.; Murray, S.C.; Bishop, M.P. Comprehensive\
    \ UAV\nagricultural remote-sensing research at Texas AM University. In Autonomous\
    \ Air and Ground Sensing Systems\nfor Agricultural Optimization and Phenotyping;\
    \ International Society for Optics and Photonics: Baltimore, MD,\nUSA, 2016; Volume\
    \ 9866, p. 986602.\n105. Turner, D.; Lucieer, A.; Wallace, L. Direct georeferencing\
    \ of ultrahigh-resolution UAV imagery. IEEE Trans.\nGeosci. Remote Sens. 2014,\
    \ 52, 2738–2745. [CrossRef]\n106. Primicerio, J.; Di Gennaro, S.F.; Fiorillo,\
    \ E.; Genesio, L.; Lugato, E.; Matese, A.; Vaccari, F.P. A ﬂexible\nunmanned aerial\
    \ vehicle for precision agriculture. Precis. Agric. 2012, 13, 517–523. [CrossRef]\n\
    107. Pajares, G. Overview and current status of remote sensing applications based\
    \ on unmanned aerial vehicles\n(UAVs). Photogramm. Eng. Remote Sens. 2015, 81,\
    \ 281–330. [CrossRef]\nAgronomy 2020, 10, 140\n26 of 35\n108. Di Gennaro, S.F.;\
    \ Matese, A.; Gioli, B.; Toscano, P.; Zaldei, A.; Palliotti, A.; Genesio, L. Multisensor\
    \ approach\nto assess vineyard thermal dynamics combining high-resolution unmanned\
    \ aerial vehicle (UAV) remote\nsensing and wireless sensor network (WSN) proximal\
    \ sensing. Sci. Hortic. 2017, 221, 83–87. [CrossRef]\n109. Cendrero-Mateo, M.P.;\
    \ Wieneke, S.; Damm, A.; Alonso, L.; Pinto, F.; Moreno, J.; Guanter, L.; Celesti,\
    \ M.;\nRossini, M.; Sabater, N.; et al. Sun-induced chlorophyll ﬂuorescence III:\
    \ Benchmarking retrieval methods\nand sensor characteristics for proximal sensing.\
    \ Remote Sens. 2019, 11, 962. [CrossRef]\n110. Zarco-Tejada, P.J.; González-Dugo,\
    \ V.; Berni, J.A.J. Fluorescence, temperature and narrow-band indices\nacquired\
    \ from a UAV platform for water stress detection using a micro-hyperspectral imager\
    \ and a thermal\ncamera. Remote Sens. Environ. 2012, 117, 322–337. [CrossRef]\n\
    111. Harwin, S.; Lucieer, A. Assessing the accuracy of georeferenced point clouds\
    \ produced via multi-view\nstereopsis from Unmanned Aerial Vehicle (UAV) imagery.\
    \ Remote Sens. 2012, 4, 1573–1599. [CrossRef]\n112. Wallace, L.; Lucieer, A.;\
    \ Malenovskỳ, Z.; Turner, D.; Vopˇenka, P. Assessment of forest structure using\
    \ two\nUAV techniques: A comparison of airborne laser scanning and structure from\
    \ motion (SfM) point clouds.\nForests 2016, 7, 62. [CrossRef]\n113. Weiss, M.;\
    \ Baret, F. Using 3D point clouds derived from UAV RGB imagery to describe vineyard\
    \ 3D\nmacro-structure. Remote Sens. 2017, 9, 111. [CrossRef]\n114. Mathews, A.;\
    \ Jensen, J. Visualizing and quantifying vineyard canopy LAI using an unmanned\
    \ aerial vehicle\n(UAV) collected high density structure from motion point cloud.\
    \ Remote Sens. 2013, 5, 2164–2183. [CrossRef]\n115. Stone, C.; Webster, M.; Osborn,\
    \ J.; Iqbal, I. Alternatives to LiDAR-derived canopy height models for softwood\n\
    plantations: a review and example using photogrammetry. Aust. For. 2016, 79, 271–282.\
    \ [CrossRef]\n116. Wu, D.; Phinn, S.; Johansen, K.; Robson, A.; Muir, J.; Searle,\
    \ C. Estimating changes in leaf area, leaf area\ndensity, and vertical leaf area\
    \ proﬁle for mango, avocado, and macadamia tree crowns using terrestrial laser\n\
    scanning. Remote Sens. 2018, 10, 1750. [CrossRef]\n117. Rosell, J.R.; Llorens,\
    \ J.; Sanz, R.; Arno, J.; Ribes-Dasi, M.; Masip, J.; Escolà, A.; Camp, F.; Solanelles,\
    \ F.;\nGràcia, F.; et al. Obtaining the three-dimensional structure of tree orchards\
    \ from remote 2D terrestrial LIDAR\nscanning. Agric. For. Meteorol. 2009, 149,\
    \ 1505–1515. [CrossRef]\n118. Matese, A.; Di Gennaro, S.F. Technology in precision\
    \ viticulture: A state of the art review. Int. J. Wine Res.\n2015, 7, 69–81. [CrossRef]\n\
    119. Johansen, K.; Raharjo, T.; McCabe, M.F. Using multi-spectral UAV imagery\
    \ to extract tree crop structural\nproperties and assess pruning eﬀects. Remote\
    \ Sens. 2018, 10, 854. [CrossRef]\n120. Tu, Y.-H.; Johansen, K.; Phinn, S.; Robson,\
    \ A. Measuring canopy structure and condition using multi-spectral\nUAS imagery\
    \ in a horticultural environment. Remote Sens. 2019, 11, 269. [CrossRef]\n121.\
    \ Mu, Y.; Fujii, Y.; Takata, D.; Zheng, B.; Noshita, K.; Honda, K.; Ninomiya,\
    \ S.; Guo, W. Characterization of\npeach tree crown by using high-resolution images\
    \ from an unmanned aerial vehicle. Hortic. Res. 2018, 5, 74.\n[CrossRef]\n122.\
    \ De Castro, A.I.; Jiménez-Brenes, F.M.; Torres-Sánchez, J.; Peña, J.M.; Borra-Serrano,\
    \ I.; López-Granados, F. 3-D\ncharacterization of vineyards using a novel UAV\
    \ imagery-based OBIA procedure for precision viticulture\napplications. Remote\
    \ Sens. 2018, 10, 584. [CrossRef]\n123. Del Pozo, S.; Rodríguez-Gonzálvez, P.;\
    \ Hernández-López, D.; Felipe-García, B. Vicarious radiometric\ncalibration of\
    \ a multispectral camera on board an unmanned aerial system. Remote Sens. 2014,\
    \ 6, 1918–1937.\n[CrossRef]\n124. Tu, Y.-H.; Phinn, S.; Johansen, K.; Robson,\
    \ A. Assessing radiometric correction approaches for multi-spectral\nUAS imagery\
    \ for horticultural applications. Remote Sens. 2018, 10, 1684. [CrossRef]\n125.\
    \ Stow, D.; Nichol, C.J.; Wade, T.; Assmann, J.J.; Simpson, G.; Helfter, C. Illumination\
    \ geometry and ﬂying\nheight inﬂuence surface reﬂectance and NDVI derived from\
    \ multispectral UAS imagery. Drones 2019, 3, 55.\n[CrossRef]\n126. Turner, D.;\
    \ Lucieer, A.; Watson, C. Development of an unmanned aerial vehicle (UAV) for\
    \ hyper resolution\nvineyard mapping based on visible, multispectral, and thermal\
    \ imagery.\nIn Proceedings of the 34th\nInternational Symposium on Remote Sensing\
    \ of Environment, Sydney, Australia, 10–15 April 2011; p. 4.\n127. Jorge, J.;\
    \ Vallbé, M.; Soler, J.A. Detection of irrigation inhomogeneities in an olive\
    \ grove using the NDRE\nvegetation index obtained from UAV images. Eur. J. Remote\
    \ Sens. 2019, 52, 169–177. [CrossRef]\n128. Filella, I.; Penuelas, J. The red\
    \ edge position and shape as indicators of plant chlorophyll content, biomass\n\
    and hydric status. Int. J. Remote Sens. 1994, 15, 1459–1470. [CrossRef]\nAgronomy\
    \ 2020, 10, 140\n27 of 35\n129. Zúñiga, C.E.; Khot, L.R.; Jacoby, P.; Sankaran,\
    \ S. Remote sensing based water-use eﬃciency evaluation\nin sub-surface irrigated\
    \ wine grape vines. In Autonomous Air and Ground Sensing Systems for Agricultural\n\
    Optimization and Phenotyping; International Society for Optics and Photonics:\
    \ Baltimore, MD, USA, 2016;\nVolume 9866, p. 98660O.\n130. Aasen, H.; Honkavaara,\
    \ E.; Lucieer, A.; Zarco-Tejada, P. Quantitative remote sensing at ultra-high\
    \ resolution\nwith uav spectroscopy: A review of sensor technology, measurement\
    \ procedures, and data correction\nworkﬂows. Remote Sens. 2018, 10, 1091. [CrossRef]\n\
    131. Adão, T.; Hruška, J.; Pádua, L.; Bessa, J.; Peres, E.; Morais, R.; Sousa,\
    \ J. Hyperspectral imaging: A review on\nUAV-based sensors, data processing and\
    \ applications for agriculture and forestry. Remote Sens. 2017, 9, 1110.\n[CrossRef]\n\
    132. Gautam, D.; Watson, C.; Lucieer, A.; Malenovský, Z. Error budget for geolocation\
    \ of spectroradiometer point\nobservations from an unmanned aircraft system. Sens.\
    \ Switz. 2018, 18, 3465. [CrossRef] [PubMed]\n133. Uto, K.; Seki, H.; Saito, G.;\
    \ Kosugi, Y.; Komatsu, T. Development of a low-cost hyperspectral whiskbroom\n\
    imager using an optical ﬁber bundle, a swing mirror, and compact spectrometers.\
    \ IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens. 2016, 9, 3909–3925. [CrossRef]\n\
    134. Suomalainen, J.; Anders, N.; Iqbal, S.; Roerink, G.; Franke, J.; Wenting,\
    \ P.; Hünniger, D.; Bartholomeus, H.;\nBecker, R.; Kooistra, L. A lightweight\
    \ hyperspectral mapping system and photogrammetric processing chain\nfor unmanned\
    \ aerial vehicles. Remote Sens. 2014, 6, 11013–11030. [CrossRef]\n135. Iseli,\
    \ C.; Lucieer, A. Tree species classiﬁcation based on 3d spectral point clouds\
    \ and orthomosaics acquired\nby snapshot hyperspectral UAS sensor. ISPRS-Int.\
    \ Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2019, 4213,\n379–384. [CrossRef]\n\
    136. Hagen, N.A.; Kudenov, M.W. Review of snapshot spectral imaging technologies.\
    \ Opt. Eng. 2013, 52, 090901.\n[CrossRef]\n137. Bendig, J.; Gautam, D.; Malenovsky,\
    \ Z.; Lucieer, A. Inﬂuence of Cosine Corrector and UAS Platform Dynamics\non Airborne\
    \ Spectral Irradiance Measurements. In Proceedings of the 2018 IEEE International\
    \ Geoscience\nand Remote Sensing Symposium (IGARSS 2018), Valencia, Spain, 22–27\
    \ July 2018; pp. 8822–8825.\n138. Gautam, D. Direct Georeferencing and Footprint\
    \ Characterisation of a Non-Imaging Spectroradiometer\nMounted on an Unmanned\
    \ Aircraft System. Ph.D. Thesis, University of Tasmania, Hobart, Tasmania,\nAustralia,\
    \ 2019.\n139. Rodríguez-Pérez, J.R.; Riaño, D.; Carlisle, E.; Ustin, S.; Smart,\
    \ D.R. Evaluation of hyperspectral reﬂectance\nindexes to detect grapevine water\
    \ status in vineyards. Am. J. Enol. Vitic. 2007, 58, 302–317.\n140. Hurley, S.P.;\
    \ Horney, M.; Drake, A. Using hyperspectral imagery to detect water stress in\
    \ vineyards.\nIn Autonomous Air and Ground Sensing Systems for Agricultural Optimization\
    \ and Phenotyping IV; International\nSociety for Optics and Photonics: Baltimore,\
    \ MD, USA, 2019; Volume 11008, p. 1100807.\n141. Loggenberg, K.; Strever, A.;\
    \ Greyling, B.; Poona, N. Modelling water stress in a shiraz vineyard using\n\
    hyperspectral imaging and machine learning. Remote Sens. 2018, 10, 202. [CrossRef]\n\
    142. Gómez-Candón, D.; Virlet, N.; Labbé, S.; Jolivot, A.; Regnard, J.-L. Field\
    \ phenotyping of water stress at tree\nscale by UAV-sensed imagery: new insights\
    \ for thermal acquisition and calibration. Precis. Agric. 2016, 17,\n786–800.\
    \ [CrossRef]\n143. Kelly, J.; Kljun, N.; Olsson, P.-O.; Mihai, L.; Liljeblad,\
    \ B.; Weslien, P.; Klemedtsson, L.; Eklundh, L. Challenges\nand best practices\
    \ for deriving temperature data from an uncalibrated UAV thermal infrared camera.\n\
    Remote Sens. 2019, 11, 567. [CrossRef]\n144. Smigaj, M.; Gaulton, R.; Suarez,\
    \ J.; Barr, S. Use of miniature thermal cameras for detection of physiological\n\
    stress in conifers. Remote Sens. 2017, 9, 957. [CrossRef]\n145. Clarke, I. Thermal\
    \ Infrared Remote Sensing from Unmanned Aircraft Systems (UAS) for Precision Viticulture.\n\
    Master’s Thesis, University of Tasmania, Hobart, Tasmania, Australia, 2014.\n\
    146. Daakir, M.; Zhou, Y.; Pierrot Deseilligny, M.; Thom, C.; Martin, O.; Rupnik,\
    \ E. Improvement of\nphotogrammetric accuracy by modeling and correcting the thermal\
    \ eﬀect on camera calibration. ISPRS J.\nPhotogramm. Remote Sens. 2019, 148, 142–155.\
    \ [CrossRef]\n147. Nugent, P.W.; Shaw, J.A. Calibration of uncooled LWIR microbolometer\
    \ imagers to enable long-term ﬁeld\ndeployment. In Infrared Imaging Systems: Design,\
    \ Analysis, Modeling, and Testing XXV; International Society\nfor Optics and Photonics:\
    \ Baltimore, MD, USA, 2014; Volume 9071, p. 90710V.\nAgronomy 2020, 10, 140\n\
    28 of 35\n148. Budzier, H.; Gerlach, G. Calibration of uncooled thermal infrared\
    \ cameras. J. Sens. Sens. Syst. 2015, 4,\n187–197. [CrossRef]\n149. Lin, D.; Maas,\
    \ H.-G.; Westfeld, P.; Budzier, H.; Gerlach, G. An advanced radiometric calibration\
    \ approach for\nuncooled thermal cameras. Photogramm. Rec. 2018, 33, 30–48. [CrossRef]\n\
    150. Ribeiro-Gomes, K.; Hernández-López, D.; Ortega, J.F.; Ballesteros, R.; Poblete,\
    \ T.; Moreno, M.A. Uncooled\nthermal camera calibration and optimization of the\
    \ photogrammetry process for UAV applications in\nagriculture. Sensors 2017, 17,\
    \ 173. [CrossRef]\n151. Lin, D.; Westfeld, P.; Maas, H.G. Shutter-less temperature-dependent\
    \ correction for uncooled thermal camera\nunder fast changing FPA temperature.\
    \ Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.—ISPRS Arch. 2017,\n42, 619–625.\
    \ [CrossRef]\n152. Mesas-Carrascosa, F.J.; Pérez-Porras, F.; de Larriva, J.E.M.;\
    \ Frau, C.M.; Agüera-Vega, F.; Carvajal-Ramírez, F.;\nMartínez-Carricondo, P.;\
    \ García-Ferrer, A. Drift correction of lightweight microbolometer thermal sensors\n\
    on-board unmanned aerial vehicles. Remote Sens. 2018, 10, 615. [CrossRef]\n153.\
    \ Torres-Rua, A. Vicarious calibration of sUAS microbolometer temperature imagery\
    \ for estimation of\nradiometric land surface temperature. Sensors 2017, 17, 1499.\
    \ [CrossRef] [PubMed]\n154. Bendig, J.; Bolten, A.; Bareth, G. Introducing a low-cost\
    \ mini-UAV for thermal-and multispectral-imaging.\nInt. Arch. Photogramm Remote\
    \ Sens. Spat. Inf. Sci. 2012, 39, 345–349. [CrossRef]\n155. Raymer, D. Aircraft\
    \ Design: A Conceptual Approach; American Institute of Aeronautics and Astronautics,\
    \ Inc.:\nReston, VA, USA, 2018.\n156. Tardieu, F.; Simonneau, T. Variability among\
    \ species of stomatal control under ﬂuctuating soil water status\nand evaporative\
    \ demand: modelling isohydric and anisohydric behaviours. J. Exp. Bot. 1998, 49,\
    \ 419–432.\n[CrossRef]\n157. White, W.A.; Alsina, M.M.; Nieto, H.; McKee, L.G.;\
    \ Gao, F.; Kustas, W.P. Determining a robust indirect\nmeasurement of leaf area\
    \ index in California vineyards for validating remote sensing-based retrievals.\n\
    Irrig. Sci. 2019, 37, 269–280. [CrossRef]\n158. Zarco-Tejada, P.J.; Berni, J.A.;\
    \ Suárez, L.; Sepulcre-Cantó, G.; Morales, F.; Miller, J.R. Imaging chlorophyll\n\
    ﬂuorescence with an airborne narrow-band multispectral camera for vegetation stress\
    \ detection.\nRemote Sens. Environ. 2009, 113, 1262–1275. [CrossRef]\n159. Gago,\
    \ J.; Douthe, C.; Florez-Sarasa, I.; Escalona, J.M.; Galmes, J.; Fernie, A.R.;\
    \ Flexas, J.; Medrano, H.\nOpportunities for improving leaf water use eﬃciency\
    \ under climate change conditions. Plant Sci. 2014, 226,\n108–119. [CrossRef]\n\
    160. Suárez, L.; Zarco-Tejada, P.J.; Sepulcre-Cantó, G.; Pérez-Priego, O.; Miller,\
    \ J.; Jiménez-Muñoz, J.; Sobrino, J.\nAssessing canopy PRI for water stress detection\
    \ with diurnal airborne imagery. Remote Sens. Environ. 2008,\n112, 560–575. [CrossRef]\n\
    161. Eugenio, F.; Marqués, F. Automatic satellite image georeferencing using a\
    \ contour-matching approach.\nIEEE Trans. Geosci. Remote Sens. 2003, 41, 2869–2880.\
    \ [CrossRef]\n162. Hugenholtz, C.; Brown, O.; Walker, J.; Barchyn, T.; Nesbit,\
    \ P.; Kucharczyk, M.; Myshak, S. Spatial accuracy of\nUAV-derived orthoimagery\
    \ and topography: Comparing photogrammetric models processed with direct\ngeo-referencing\
    \ and ground control points. Geomatica 2016, 70, 21–30. [CrossRef]\n163. Matese,\
    \ A.; Di Gennaro, S.F.; Berton, A. Assessment of a canopy height model (CHM) in\
    \ a vineyard using\nUAV-based multispectral imaging. Int. J. Remote Sens. 2017,\
    \ 38, 2150–2160. [CrossRef]\n164. Yahyanejad, S.; Misiorny, J.; Rinner, B. Lens\
    \ distortion correction for thermal cameras to improve aerial\nimaging with small-scale\
    \ UAVs. In Proceedings of the 2011 IEEE International Symposium on Robotic and\n\
    Sensors Environments (ROSE 2011), Montreal, QC, Canada, 17–18 September 2011;\
    \ pp. 231–236.\n165. Maes, W.; Huete, A.; Steppe, K. Optimizing the processing\
    \ of UAV-based thermal imagery. Remote Sens. 2017,\n9, 476. [CrossRef]\n166. Smith,\
    \ M.; Carrivick, J.; Quincey, D. Structure from motion photogrammetry in physical\
    \ geography.\nProg. Phys. Geogr. 2016, 40, 247–275. [CrossRef]\n167. Westoby,\
    \ M.J.; Brasington, J.; Glasser, N.F.; Hambrey, M.J.; Reynolds, J.M. ‘Structure-from-Motion’\n\
    photogrammetry: A low-cost, eﬀective tool for geoscience applications. Geomorphology\
    \ 2012, 179, 300–314.\n[CrossRef]\n168. Gautam, D.; Lucieer, A.; Malenovský, Z.;\
    \ Watson, C. Comparison of MEMS-based and FOG-based IMUs to\ndetermine sensor\
    \ pose on an unmanned aircraft system. J. Surv. Eng. 2017, 143. [CrossRef]\nAgronomy\
    \ 2020, 10, 140\n29 of 35\n169. Turner, D.; Lucieer, A.; McCabe, M.; Parkes, S.;\
    \ Clarke, I. Pushbroom hyperspectral imaging from an\nunmanned aircraft system\
    \ (UAS)–geometric processingworkﬂow and accuracy assessment. ISPRS-Int. Arch.\n\
    Photogramm. Remote Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 379–384. [CrossRef]\n\
    170. Fang, J.; Wang, X.; Zhu, T.; Liu, X.; Zhang, X.; Zhao, D. A Novel Mosaic\
    \ Method for UAV-Based Hyperspectral\nImages. In Proceedings of the 2019 IEEE\
    \ International Geoscience and Remote Sensing Symposium (IGARSS\n2019), Yokohama,\
    \ Japan, 28 July–2 August 2019; pp. 9220–9223.\n171. Tagle, X. Study of Radiometric\
    \ Variations in Unmanned Aerial Vehicle Remote Sensing Imagery for Vegetation\n\
    Mapping. Master’s Thesis, Lund University, Lund, Sweden, 2017.\n172. Kedzierski,\
    \ M.; Wierzbicki, D.; Sekrecka, A.; Fryskowska, A.; Walczykowski, P.; Siewert,\
    \ J. Inﬂuence of lower\natmosphere on the radiometric quality of unmanned aerial\
    \ vehicle imagery. Remote Sens. 2019, 11, 1214.\n[CrossRef]\n173. Kelcey, J.;\
    \ Lucieer, A. Sensor correction of a 6-band multispectral imaging sensor for UAV\
    \ remote sensing.\nRemote Sens. 2012, 4, 1462–1493. [CrossRef]\n174. Maes, W.H.;\
    \ Steppe, K. Perspectives for remote sensing with unmanned aerial vehicles in\
    \ precision agriculture.\nTrends Plant Sci. 2019, 24, 152–164. [CrossRef]\n175.\
    \ McCabe, M.F.; Houborg, R.; Lucieer, A. High-resolution sensing for precision\
    \ agriculture:\nfrom\nEarth-observing satellites to unmanned aerial vehicles.\n\
    In Remote Sensing for Agriculture, Ecosystems,\nand Hydrology XVIII; International\
    \ Society for Optics and Photonics: Edinbrugh, UK, 2016; Volume 9998,\np. 999811.\n\
    176. Dinguirard, M.; Slater, P.N. Calibration of space-multispectral imaging sensors:\
    \ A review. Remote Sens. Environ.\n1999, 68, 194–205. [CrossRef]\n177. Geladi,\
    \ P.; Burger, J.; Lestander, T. Hyperspectral imaging: calibration problems and\
    \ solutions. Chemom. Intell.\nLab. Syst. 2004, 72, 209–217. [CrossRef]\n178. Iqbal,\
    \ F.; Lucieer, A.; Barry, K. Simpliﬁed radiometric calibration for UAS-mounted\
    \ multispectral sensor.\nEur. J. Remote Sens. 2018, 51, 301–313. [CrossRef]\n\
    179. Mamaghani, B.; Salvaggio, C. Multispectral Sensor Calibration and Characterization\
    \ for sUAS Remote\nSensing. Sensors 2019, 19, 4453. [CrossRef] [PubMed]\n180.\
    \ Mamaghani, B.; Salvaggio, C. Comparative study of panel and panelless-based\
    \ reﬂectance conversion\ntechniques for agricultural remote sensing. arXiv 2019,\
    \ arXiv:191003734.\n181. Jensen, A.M.; McKee, M.; Chen, Y. Calibrating thermal\
    \ imagery from an unmanned aerial system-AggieAir.\nIn Proceedings of the 2013\
    \ IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2013),\n\
    Melbourne, Australia, 21–26 July 2013; pp. 542–545.\n182. Zarco-Tejada, P.J.;\
    \ Victoria, G.-D.; Williams, L.; Suárez, L.; Berni, J.A.; Goldhamer, D.; Fereres,\
    \ E. A PRI-based\nwater stress index combining structural and chlorophyll eﬀects:\
    \ Assessment using diurnal narrow-band\nairborne imagery and the CWSI thermal\
    \ index. Remote Sens. Environ. 2013, 138, 38–50. [CrossRef]\n183. Blaschke, T.\
    \ Object based image analysis for remote sensing. ISPRS J. Photogramm. Remote\
    \ Sens. 2010, 65,\n2–16. [CrossRef]\n184. De Castro, A.; Torres-Sánchez, J.; Peña,\
    \ J.; Jiménez-Brenes, F.; Csillik, O.; López-Granados, F. An automatic\nrandom\
    \ forest-OBIA algorithm for early weed mapping between and within crop rows using\
    \ UAV imagery.\nRemote Sens. 2018, 10, 285. [CrossRef]\n185. Peña-Barragán, J.M.;\
    \ Ngugi, M.K.; Plant, R.E.; Six, J. Object-based crop identiﬁcation using multiple\
    \ vegetation\nindices, textural features and crop phenology. Remote Sens. Environ.\
    \ 2011, 115, 1301–1316. [CrossRef]\n186. Johansen, K.; Raharjo, T. Multi-temporal\
    \ assessment of lychee tree crop structure using multi-spectral RPAS\nimagery.\
    \ ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 165–170.\
    \ [CrossRef]\n187. Ma, L.; Li, M.; Ma, X.; Cheng, L.; Du, P.; Liu, Y. A review\
    \ of supervised object-based land-cover image\nclassiﬁcation. ISPRS J. Photogramm.\
    \ Remote Sens. 2017, 130, 277–293. [CrossRef]\n188. Pádua, L.; Vanko, J.; Hruška,\
    \ J.; Adão, T.; Sousa, J.J.; Peres, E.; Morais, R. UAS, sensors, and data processing\n\
    in agroforestry: a review towards practical applications. Int. J. Remote Sens.\
    \ 2017, 38, 2349–2391. [CrossRef]\n189. Torres-Sánchez, J.; López-Granados, F.;\
    \ Peña, J.M. An automatic object-based method for optimal thresholding\nin UAV\
    \ images: Application for vegetation detection in herbaceous crops. Comput. Electron.\
    \ Agric. 2015, 114,\n43–52. [CrossRef]\n190. Cohen, Y.; Alchanatis, V.; Prigojin,\
    \ A.; Levi, A.; Soroker, V. Use of aerial thermal imaging to estimate water\n\
    status of palm trees. Precis. Agric. 2012, 13, 123–140. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n30 of 35\n191. Comba, L.; Gay, P.; Primicerio, J.; Aimonino, D.R. Vineyard\
    \ detection from unmanned aerial systems images.\nComput. Electron. Agric. 2015,\
    \ 114, 78–87. [CrossRef]\n192. Nolan, A.; Park, S.; Fuentes, S.; Ryu, D.; Chung,\
    \ H. Automated detection and segmentation of vine rows using\nhigh resolution\
    \ UAS imagery in a commercial vineyard. In Proceedings of the 21st International\
    \ Congress\non Modelling and Simulation, Gold Coast, QLD, Australia, 29 November–4\
    \ December 2015; Volume 29,\npp. 1406–1412.\n193. Bobillet, W.; Da Costa, J.-P.;\
    \ Germain, C.; Lavialle, O.; Grenier, G. Row detection in high resolution remote\n\
    sensing images of vine ﬁelds. In Proceedings of the 4th European Conference on\
    \ Precision Agriculture,\nBerlin, Germany, 15–19 June 2003; pp. 81–87.\n194. Poblete,\
    \ T.; Ortega-Farías, S.; Ryu, D. Automatic coregistration algorithm to remove\
    \ canopy shaded pixels in\nUAV-borne thermal images to improve the estimation\
    \ of crop water stress index of a drip-irrigated cabernet\nsauvignon vineyard.\
    \ Sensors 2018, 18, 397. [CrossRef]\n195. Ihuoma, S.O.; Madramootoo, C.A. Recent\
    \ advances in crop water stress detection. Comput. Electron. Agric.\n2017, 141,\
    \ 267–275. [CrossRef]\n196. Jones, H.G. Use of infrared thermometry for estimation\
    \ of stomatal conductance as a possible aid to irrigation\nscheduling. Agric.\
    \ For. Meteorol. 1999, 95, 139–149. [CrossRef]\n197. Bellvert, J.; Marsal, J.;\
    \ Girona, J.; Zarco-Tejada, P.J. Seasonal evolution of crop water stress index\
    \ in grapevine\nvarieties determined with high-resolution remote sensing thermal\
    \ imagery. Irrig. Sci. 2015, 33, 81–93.\n[CrossRef]\n198. García-Tejero, I.F.;\
    \ Gutiérrez-Gordillo, S.; Ortega-Arévalo, C.; Iglesias-Contreras, M.; Moreno,\
    \ J.M.;\nSouza-Ferreira, L.; Durán-Zuazo, V.H. Thermal imaging to monitor the\
    \ crop-water status in almonds\nby using the non-water stress baselines. Sci.\
    \ Hortic. 2018, 238, 91–97. [CrossRef]\n199. Alchanatis, V.; Cohen, Y.; Cohen,\
    \ S.; Moller, M.; Sprinstin, M.; Meron, M.; Tsipris, J.; Saranga, Y.; Sela, E.\n\
    Evaluation of diﬀerent approaches for estimating and mapping crop water status\
    \ in cotton with thermal\nimaging. Precis. Agric. 2010, 11, 27–41. [CrossRef]\n\
    200. Goetz, S. Multi-sensor analysis of NDVI, surface temperature and biophysical\
    \ variables at a mixed grassland\nsite. Int. J. Remote Sens. 1997, 18, 71–94.\
    \ [CrossRef]\n201. Sun, L.; Gao, F.; Anderson, M.; Kustas, W.; Alsina, M.; Sanchez,\
    \ L.; Sams, B.; McKee, L.; Dulaney, W.;\nWhite, W.; et al. Daily mapping of 30\
    \ m LAI and NDVI for grape yield prediction in California Vineyards.\nRemote Sens.\
    \ 2017, 9, 317. [CrossRef]\n202. Peñuelas, J.; Filella, I.; Biel, C.; Serrano,\
    \ L.; Save, R. The reﬂectance at the 950–970 nm region as an indicator\nof plant\
    \ water status. Int. J. Remote Sens. 1993, 14, 1887–1905. [CrossRef]\n203. Jones,\
    \ C.L.; Weckler, P.R.; Maness, N.O.; Stone, M.L.; Jayasekara, R. Estimating water\
    \ stress in plants\nusing hyperspectral sensing. In Proceedings of the 2004 ASAE\
    \ Annual Meeting, Ottawa, ON, Canada,\n1–4 August 2004; p. 1.\n204. Aˇc, A.; Malenovskỳ,\
    \ Z.; Olejníˇcková, J.; Gallé, A.; Rascher, U.; Mohammed, G. Meta-analysis assessing\n\
    potential of steady-state chlorophyll ﬂuorescence for remote sensing detection\
    \ of plant water, temperature\nand nitrogen stress. Remote Sens. Environ. 2015,\
    \ 168, 420–436. [CrossRef]\n205. Mohammed, G.H.; Colombo, R.; Middleton, E.M.;\
    \ Rascher, U.; van der Tol, C.; Nedbal, L.; Goulas, Y.;\nPérez-Priego, O.; Damm,\
    \ A.; Meroni, M.; et al. Remote sensing of solar-induced chlorophyll ﬂuorescence\n\
    (SIF) in vegetation: 50 years of progress. Remote Sens. Environ. 2019, 231, 111177.\
    \ [CrossRef]\n206. Panigada, C.; Rossini, M.; Meroni, M.; Cilia, C.; Busetto,\
    \ L.; Amaducci, S.; Boschetti, M.; Cogliati, S.; Picchi, V.;\nPinto, F.; et al.\
    \ Fluorescence, PRI and canopy temperature for water stress detection in cereal\
    \ crops. Int. J.\nAppl. Earth Obs. Geoinformation 2014, 30, 167–178. [CrossRef]\n\
    207. Jones, H.G.; Stoll, M.; Santos, T.; Sousa, C.D.; Chaves, M.M.; Grant, O.M.\
    \ Use of infrared thermography for\nmonitoring stomatal closure in the ﬁeld: application\
    \ to grapevine. J. Exp. Bot. 2002, 53, 2249–2260. [CrossRef]\n208. Jackson, R.D.;\
    \ Idso, S.B.; Reginato, R.J.; Pinter, P.J. Canopy temperature as a crop water\
    \ stress indicator.\nWater Resour. Res. 1981, 17, 1133–1138. [CrossRef]\n209.\
    \ Bellvert, J.; Zarco-Tejada, P.J.; Girona, J.; Fereres, E. Mapping crop water\
    \ stress index in a ‘Pinot-noir’vineyard:\ncomparing ground measurements with\
    \ thermal remote sensing imagery from an unmanned aerial vehicle.\nPrecis. Agric.\
    \ 2014, 15, 361–376. [CrossRef]\n210. Fuentes, S.; De Bei, R.; Pech, J.; Tyerman,\
    \ S. Computational water stress indices obtained from thermal image\nanalysis\
    \ of grapevine canopies. Irrig. Sci. 2012, 30, 523–536. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n31 of 35\n211. Jackson, R.D. Canopy temperature and crop water stress.\
    \ In Advances in Irrigation; Elsevier: Amsterdam,\nThe Netherlands, 1982; Volume\
    \ 1, pp. 43–85.\n212. Idso, S.; Jackson, R.; Pinter, P., Jr.; Reginato, R.; Hatﬁeld,\
    \ J. Normalizing the stress-degree-day parameter for\nenvironmental variability.\
    \ Agric. Meteorol. 1981, 24, 45–55. [CrossRef]\n213. Möller, M.; Alchanatis, V.;\
    \ Cohen, Y.; Meron, M.; Tsipris, J.; Naor, A.; Ostrovsky, V.; Sprintsin, M.; Cohen,\
    \ S.\nUse of thermal and visible imagery for estimating crop water status of irrigated\
    \ grapevine. J. Exp. Bot. 2006,\n58, 827–838. [CrossRef] [PubMed]\n214. Egea,\
    \ G.; Padilla-Díaz, C.M.; Martinez-Guanter, J.; Fernández, J.E.; Pérez-Ruiz, M.\
    \ Assessing a crop water\nstress index derived from aerial thermal imaging and\
    \ infrared thermometry in super-high density olive\norchards. Agric. Water Manag.\
    \ 2017, 187, 210–221. [CrossRef]\n215. Bannari, A.; Morin, D.; Bonn, F.; Huete,\
    \ A. A review of vegetation indices. Remote Sens. Rev. 1995, 13, 95–120.\n[CrossRef]\n\
    216. Ballester, C.; Zarco-Tejada, P.; Nicolas, E.; Alarcon, J.; Fereres, E.; Intrigliolo,\
    \ D.; Gonzalez-Dugo, V. Evaluating\nthe performance of xanthophyll, chlorophyll\
    \ and structure-sensitive spectral indices to detect water stress in\nﬁve fruit\
    \ tree species. Precis. Agric. 2018, 19, 178–193. [CrossRef]\n217. Romero-Trigueros,\
    \ C.; Nortes, P.A.; Alarcón, J.J.; Hunink, J.E.; Parra, M.; Contreras, S.; Droogers,\
    \ P.; Nicolás, E.\nEﬀects of saline reclaimed waters and deﬁcit irrigation on\
    \ Citrus physiology assessed by UAV remote sensing.\nAgric. Water Manag. 2017,\
    \ 183, 60–69. [CrossRef]\n218. Zhao, T.; Stark, B.; Chen, Y.; Ray, A.; Doll, D.\
    \ More reliable crop water stress quantiﬁcation using small\nunmanned aerial systems\
    \ (sUAS). IFAC-PapersOnLine 2016, 49, 409–414. [CrossRef]\n219. Sandholt, I.;\
    \ Rasmussen, K.; Andersen, J. A simple interpretation of the surface temperature/vegetation\
    \ index\nspace for assessment of surface moisture status. Remote Sens. Environ.\
    \ 2002, 79, 213–224. [CrossRef]\n220. Wang, L.; Qu, J.J. Satellite remote sensing\
    \ applications for surface soil moisture monitoring: A review.\nFront. Earth Sci.\
    \ China 2009, 3, 237–247. [CrossRef]\n221. Colaizzi, P.D.; Barnes, E.M.; Clarke,\
    \ T.R.; Choi, C.Y.; Waller, P.M. Estimating soil moisture under low frequency\n\
    surface irrigation using crop water stress index. J. Irrig. Drain. Eng. 2003,\
    \ 129, 27–35. [CrossRef]\n222. Ahmed, A.; Zhang, Y.; Nichols, S. Review and evaluation\
    \ of remote sensing methods for soil-moisture\nestimation. SPIE Rev. 2011, 2,\
    \ 028001.\n223. Kerr, Y.H. Soil moisture from space: Where are we? Hydrogeol.\
    \ J. 2007, 15, 117–120. [CrossRef]\n224. Kerr, Y.H.; Waldteufel, P.; Wigneron,\
    \ J.-P.; Delwart, S.; Cabot, F.; Boutin, J.; Escorihuela, M.-J.; Font, J.; Reul,\
    \ N.;\nGruhier, C.; et al. The SMOS mission: New tool for monitoring key elements\
    \ ofthe global water cycle.\nProc. IEEE 2010, 98, 666–687. [CrossRef]\n225. Entekhabi,\
    \ D.; Njoku, E.G.; O’Neill, P.E.; Kellogg, K.H.; Crow, W.T.; Edelstein, W.N.;\
    \ Entin, J.K.; Goodman, S.D.;\nJackson, T.J.; Johnson, J.; et al. The soil moisture\
    \ active passive (SMAP) mission. Proc. IEEE 2010, 98, 704–716.\n[CrossRef]\n226.\
    \ Yueh, S.; Entekhabi, D.; O’Neill, P.; Njoku, E.; Entin, J. NASA soil moisture\
    \ active passive mission status\nand science performance. In Proceedings of the\
    \ 2016 IEEE International Geoscience and Remote Sensing\nSymposium (IGARSS 2016),\
    \ Beijing, China, 10–16 July 2016; pp. 116–119.\n227. Piles, M.; Sánchez, N.;\
    \ Vall-llossera, M.; Camps, A.; Martínez-Fernández, J.; Martínez, J.; González-Gambau,\
    \ V.\nA Downscaling Approach for SMOS Land Observations: Evaluation of High-Resolution\
    \ Soil Moisture Maps\nOver the Iberian Peninsula. IEEE J. Sel. Top. Appl. Earth\
    \ Obs. Remote Sens. 2014, 7, 3845–3857. [CrossRef]\n228. Cui, C.; Xu, J.; Zeng,\
    \ J.; Chen, K.-S.; Bai, X.; Lu, H.; Chen, Q.; Zhao, T. Soil moisture mapping from\
    \ satellites:\nAn intercomparison of SMAP, SMOS, FY3B, AMSR2, and ESA CCI over\
    \ two dense network regions at\ndiﬀerent spatial scales. Remote Sens. 2018, 10,\
    \ 33. [CrossRef]\n229. Peng, J.; Loew, A.; Merlin, O.; Verhoest, N.E. A review\
    \ of spatial downscaling of satellite remotely sensed soil\nmoisture. Rev. Geophys.\
    \ 2017, 55, 341–366. [CrossRef]\n230. Roussel, N.; Darrozes, J.; Ha, C.; Boniface,\
    \ K.; Frappart, F.; Ramillien, G.; Gavart, M.; Van de Vyvere, L.;\nDesenfans,\
    \ O.; Baup, F. Multi-scale volumetric soil moisture detection from GNSS SNR data:\
    \ Ground-based\nand airborne applications. In Proceedings of the 2016 IEEE Metrology\
    \ for Aerospace (MetroAeroSpace),\nFlorence, Italy, 22–23 June 2016; pp. 573–578.\n\
    231. Yan, S.; Zhang, N.; Chen, N.; Gong, J. Feasibility of using signal strength\
    \ indicator data to estimate soil\nmoisture based on GNSS interference signal\
    \ analysis. Remote Sens. Lett. 2018, 9, 61–70. [CrossRef]\nAgronomy 2020, 10,\
    \ 140\n32 of 35\n232. Johansen, K.; Sohlbach, M.; Sullivan, B.; Stringer, S.;\
    \ Peasley, D.; Phinn, S. Mapping banana plants from\nhigh spatial resolution orthophotos\
    \ to facilitate plant health assessment. Remote Sens. 2014, 6, 8261–8286.\n[CrossRef]\n\
    233. Hall, A.; Louis, J.; Lamb, D.W. Low-resolution remotely sensed images of\
    \ winegrape vineyards map spatial\nvariability in planimetric canopy area instead\
    \ of leaf area index. Aust. J. Grape Wine Res. 2008, 14, 9–17.\n[CrossRef]\n234.\
    \ Furness, G.; Magarey, P.; Miller, P.; Drew, H. Fruit tree and vine sprayer calibration\
    \ based on canopy size and\nlength of row: unit canopy row method. Crop Prot.\
    \ 1998, 17, 639–644. [CrossRef]\n235. Rosell, J.; Sanz, R. A review of methods\
    \ and applications of the geometric characterization of tree crops in\nagricultural\
    \ activities. Comput. Electron. Agric. 2012, 81, 124–141. [CrossRef]\n236. Lee,\
    \ K.; Ehsani, R. A laser scanner based measurement system for quantiﬁcation of\
    \ citrus tree geometric\ncharacteristics. Appl. Eng. Agric. 2009, 25, 777–788.\
    \ [CrossRef]\n237. Li, F.; Cohen, S.; Naor, A.; Shaozong, K.; Erez, A. Studies\
    \ of canopy structure and water use of apple trees on\nthree rootstocks. Agric.\
    \ Water Manag. 2002, 55, 1–14. [CrossRef]\n238. Kustas, W.; Agam, N.; Alﬁeri,\
    \ J.; McKee, L.; Prueger, J.; Hipps, L.; Howard, A.; Heitman, J. Below canopy\n\
    radiation divergence in a vineyard: Implications on interrow surface energy balance.\
    \ Irrig. Sci. 2019, 37,\n227–237. [CrossRef]\n239. Bendig, J.V. Unmanned aerial\
    \ vehicles (UAVs) for multi-temporal crop surface modelling. A new method for\n\
    plant height and biomass estimation based on RGB-imaging. Ph.D. Thesis, University\
    \ of Cologne, Cologne,\nGermany, 2015.\n240. Gowda, P.H.; Chavez, J.L.; Colaizzi,\
    \ P.D.; Evett, S.R.; Howell, T.A.; Tolk, J.A. ET mapping for agricultural\nwater\
    \ management: present status and challenges. Irrig. Sci. 2008, 26, 223–237. [CrossRef]\n\
    241. Zhang, K.; Kimball, J.S.; Running, S.W. A review of remote sensing based\
    \ actual evapotranspiration estimation.\nWiley Interdiscip. Rev. Water 2016, 3,\
    \ 834–853. [CrossRef]\n242. Liou, Y.-A.; Kar, S. Evapotranspiration estimation\
    \ with remote sensing and various surface energy balance\nalgorithms—A review.\
    \ Energies 2014, 7, 2821–2849. [CrossRef]\n243. Courault, D.; Seguin, B.; Olioso,\
    \ A. Review on estimation of evapotranspiration from remote sensing data:\nFrom\
    \ empirical to numerical modeling approaches. Irrig. Drain. Syst. 2005, 19, 223–249.\
    \ [CrossRef]\n244. Kalma, J.D.; McVicar, T.R.; McCabe, M.F. Estimating land surface\
    \ evaporation: A review of methods using\nremotely sensed surface temperature\
    \ data. Surv. Geophys. 2008, 29, 421–469. [CrossRef]\n245. Li, Z.-L.; Tang, R.;\
    \ Wan, Z.; Bi, Y.; Zhou, C.; Tang, B.; Yan, G.; Zhang, X. A review of current\
    \ methodologies\nfor regional evapotranspiration estimation from remotely sensed\
    \ data. Sensors 2009, 9, 3801–3853. [CrossRef]\n246. Marshall, M.; Thenkabail,\
    \ P.; Biggs, T.; Post, K. Hyperspectral narrowband and multispectral broadband\n\
    indices for remote sensing of crop evapotranspiration and its components (transpiration\
    \ and soil evaporation).\nAgric. For. Meteorol. 2016, 218, 122–134. [CrossRef]\n\
    247. Maes, W.; Steppe, K. Estimating evapotranspiration and drought stress with\
    \ ground-based thermal remote\nsensing in agriculture: a review. J. Exp. Bot.\
    \ 2012, 63, 4671–4712. [CrossRef]\n248. Bastiaanssen, W.G.; Menenti, M.; Feddes,\
    \ R.; Holtslag, A. A remote sensing surface energy balance algorithm\nfor land\
    \ (SEBAL). 1. Formulation. J. Hydrol. 1998, 212, 198–212. [CrossRef]\n249. Allen,\
    \ R.; Irmak, A.; Trezza, R.; Hendrickx, J.M.; Bastiaanssen, W.; Kjaersgaard, J.\
    \ Satellite-based ET estimation\nin agriculture using SEBAL and METRIC. Hydrol.\
    \ Process. 2011, 25, 4011–4027. [CrossRef]\n250. Allen, R.G.; Tasumi, M.; Trezza,\
    \ R. Satellite-based energy balance for mapping evapotranspiration with\ninternalized\
    \ calibration (METRIC)—Model. J. Irrig. Drain. Eng. 2007, 133, 380–394. [CrossRef]\n\
    251. Allen, R.G.; Tasumi, M.; Morse, A.; Trezza, R.; Wright, J.L.; Bastiaanssen,\
    \ W.; Kramber, W.; Lorite, I.;\nRobison, C.W. Satellite-Based Energy Balance for\
    \ Mapping Evapotranspiration with Internalized Calibration\n(METRIC)-Applications.\
    \ J. Irrig. Drain. Eng. 2007, 133, 395–406. [CrossRef]\n252. Allen, R.G.; Pereira,\
    \ L.S.; Raes, D.; Smith, M. FAO Irrigation and drainage paper No. 56. Rome Food\
    \ Agric.\nOrgan. U. N. 1998, 56, e156.\n253. Jackson, R.D.; Moran, M.S.; Gay,\
    \ L.W.; Raymond, L.H. Evaluating evaporation from ﬁeld crops using airborne\n\
    radiometry and ground-based meteorological data. Irrig. Sci. 1987, 8, 81–90. [CrossRef]\n\
    254. Williams, L.; Ayars, J. Grapevine water use and the crop coeﬃcient are linear\
    \ functions of the shaded area\nmeasured beneath the canopy. Agric. For. Meteorol.\
    \ 2005, 132, 201–211. [CrossRef]\nAgronomy 2020, 10, 140\n33 of 35\n255. Jayanthi,\
    \ H.; Neale, C.M.; Wright, J.L. Development and validation of canopy reﬂectance-based\
    \ crop\ncoeﬃcient for potato. Agric. Water Manag. 2007, 88, 235–246. [CrossRef]\n\
    256. Samani, Z.; Bawazir, A.S.; Bleiweiss, M.; Skaggs, R.; Longworth, J.; Tran,\
    \ V.D.; Pinon, A. Using remote sensing\nto evaluate the spatial variability of\
    \ evapotranspiration and crop coeﬃcient in the lower Rio Grande Valley,\nNew Mexico.\
    \ Irrig. Sci. 2009, 28, 93–100. [CrossRef]\n257. Kustas, W.P.; Anderson, M.C.;\
    \ Alﬁeri, J.G.; Knipper, K.; Torres-Rua, A.; Parry, C.K.; Nieto, H.; Agam, N.;\n\
    White, W.A.; Gao, F.; et al. The grape remote sensing atmospheric proﬁle and evapotranspiration\
    \ experiment.\nBull. Am. Meteorol. Soc. 2018, 99, 1791–1812. [CrossRef]\n258.\
    \ Kamble, B.; Kilic, A.; Hubbard, K. Estimating crop coeﬃcients using remote sensing-based\
    \ vegetation index.\nRemote Sens. 2013, 5, 1588–1602. [CrossRef]\n259. Hou, M.;\
    \ Tian, F.; Zhang, L.; Li, S.; Du, T.; Huang, M.; Yuan, Y. Estimating crop transpiration\
    \ of soybean\nunder diﬀerent irrigation treatments using thermal infrared remote\
    \ sensing imagery. Agronomy 2019, 9, 8.\n[CrossRef]\n260. Knipper, K.R.; Kustas,\
    \ W.P.; Anderson, M.C.; Alﬁeri, J.G.; Prueger, J.H.; Hain, C.R.; Gao, F.; Yang,\
    \ Y.;\nMcKee, L.G.; Nieto, H.; et al. Evapotranspiration estimates derived using\
    \ thermal-based satellite remote\nsensing and data fusion for irrigation management\
    \ in California vineyards. Irrig. Sci. 2019, 37, 431–449.\n[CrossRef]\n261. Hoﬀmann,\
    \ H.; Nieto, H.; Jensen, R.; Guzinski, R.; Zarco-Tejada, P.; Friborg, T. Estimating\
    \ evapotranspiration\nwith thermal UAV data and two source energy balance models.\
    \ Hydrol. Earth Syst. Sci. Discuss. 2016, 20,\n697–713. [CrossRef]\n262. Cammalleri,\
    \ C.; Anderson, M.; Kustas, W. Upscaling of evapotranspiration ﬂuxes from instantaneous\
    \ to\ndaytime scales for thermal remote sensing applications. Hydrol. Earth Syst.\
    \ Sci. 2014, 18, 1885–1894.\n[CrossRef]\n263. Biggs, T.W.; Marshall, M.; Messina,\
    \ A. Mapping daily and seasonal evapotranspiration from irrigated crops\nusing\
    \ global climate grids and satellite imagery: Automation and methods comparison.\
    \ Water Resour. Res.\n2016, 52, 7311–7326. [CrossRef]\n264. Chávez, J.L.; Neale,\
    \ C.M.; Prueger, J.H.; Kustas, W.P. Daily evapotranspiration estimates from extrapolating\n\
    instantaneous airborne remote sensing ET values. Irrig. Sci. 2008, 27, 67–81.\
    \ [CrossRef]\n265. McCabe, M.F.; Wood, E.F. Scale inﬂuences on the remote estimation\
    \ of evapotranspiration using multiple\nsatellite sensors. Remote Sens. Environ.\
    \ 2006, 105, 271–285. [CrossRef]\n266. Kustas, W.; Li, F.; Jackson, T.; Prueger,\
    \ J.; MacPherson, J.; Wolde, M. Eﬀects of remote sensing pixel resolution\non\
    \ modeled energy ﬂux variability of croplands in Iowa. Remote Sens. Environ. 2004,\
    \ 92, 535–547. [CrossRef]\n267. Hong, S.; Hendrickx, J.M.; Borchers, B. Eﬀect\
    \ of scaling transfer between evapotranspiration maps derived\nfrom LandSat 7\
    \ and MODIS images. In Targets and Backgrounds XI: Characterization and Representation;\n\
    International Society for Optics and Photonics: Orlando, FL, USA, 2005; Volume\
    \ 5811, pp. 147–159.\n268. Abiodun, O.O.; Guan, H.; Post, V.E.; Batelaan, O. Comparison\
    \ of MODIS and SWAT evapotranspiration over\na complex terrain at diﬀerent spatial\
    \ scales. Hydrol. Earth Syst. Sci. 2018, 22, 2775–2794. [CrossRef]\n269. Justice,\
    \ C.; Townshend, J.; Vermote, E.; Masuoka, E.; Wolfe, R.; Saleous, N.; Roy, D.;\
    \ Morisette, J. An overview\nof MODIS Land data processing and product status.\
    \ Remote Sens. Environ. 2002, 83, 3–15. [CrossRef]\n270. Nieto, H.; Bellvert,\
    \ J.; Kustas, W.P.; Alﬁeri, J.G.; Gao, F.; Prueger, J.; Torres-Rua, A.; Hipps,\
    \ L.E.; Elarab, M.;\nSong, L. Unmanned airborne thermal and mutilspectral imagery\
    \ for estimating evapotranspiration in irrigated\nvineyards. In Proceedings of\
    \ the 2017 IEEE International Geoscience and Remote Sensing Symposium\n(IGARSS\
    \ 2017), Fort Worth, TX, USA, 23–28 July 2017; pp. 5510–5513.\n271. Ortega-Farías,\
    \ S.; Ortega-Salazar, S.; Poblete, T.; Poblete-Echeverría, C.; Zúñiga, M.; Sepúlveda-Reyes,\
    \ D.;\nKilic, A.; Allen, R. Estimation of olive evapotranspiration using multispectral\
    \ and thermal sensors placed\naboard an unmanned aerial vehicle. Acta Hortic.\
    \ 2017, 1150, 1–8. [CrossRef]\n272. Gago, J.; Douthe, C.; Coopman, R.E.; Gallego,\
    \ P.P.; Ribas-Carbo, M.; Flexas, J.; Escalona, J.; Medrano, H. UAVs\nchallenge\
    \ to assess water stress for sustainable agriculture. Agric. Water Manag. 2015,\
    \ 153, 9–19. [CrossRef]\n273. Sepúlveda-Reyes, D.; Ingram, B.; Bardeen, M.; Zúñiga,\
    \ M.; Ortega-Farías, S.; Poblete-Echeverría, C. Selecting\ncanopy zones and thresholding\
    \ approaches to assess grapevine water status by using aerial and ground-based\n\
    thermal imaging. Remote Sens. 2016, 8, 822. [CrossRef]\n274. McBratney, A.; Whelan,\
    \ B.; Ancev, T.; Bouma, J. Future directions of precision agriculture. Precis.\
    \ Agric. 2005,\n6, 7–23. [CrossRef]\nAgronomy 2020, 10, 140\n34 of 35\n275. Ferguson,\
    \ R.; Rundquist, D. Remote sensing for site-speciﬁc crop management. In Precision\
    \ Agriculture Basics;\nShannon, D.K., Clay, D.E., Kitchen, N.R., Eds.; American\
    \ Society of Agronomy: Madison, WI, USA; Crop\nScience Society of America: Madison,\
    \ WI, USA; Soil Science Society of America: Madison, WI, USA, 2018;\npp. 103–118.\n\
    276. Florin, M.J.; McBratney, A.B.; Whelan, B.M. Extending site-speciﬁc crop management\
    \ from individual ﬁelds\nto an entire farm. In Proceedings of the Precision agriculture\
    \ ’05, Proceedings of the 5th European Conference\non Precision Agriculture, Uppsala,\
    \ Sweden, 9–12 June 2005; pp. 857–863.\n277. Perea-Moreno, A.J.; Aguilera-Urena,\
    \ M.J.; Merono-de Larriva, J.E.; Manzano-Agugliaro, F. Assessment of\nthe potential\
    \ of UAV video image analysis for planning irrigation needs of golf courses. Water\
    \ 2016, 8, 584.\n[CrossRef]\n278. Meron, M.; Tsipris, J.; Charitt, D. Remote mapping\
    \ of crop water status to assess spatial variability of crop\nstress. Precis.\
    \ Agric. 2003, 405–410.\n279. Idso, S.B. Non-water-stressed baselines: A key to\
    \ measuring and interpreting plant water stress. Agric. Meteorol.\n1982, 27, 59–70.\
    \ [CrossRef]\n280. Cohen, Y.; Alchanatis, V.; Meron, M.; Saranga, Y.; Tsipris,\
    \ J. Estimation of leaf water potential by thermal\nimagery and spatial analysis.\
    \ J. Exp. Bot. 2005, 56, 1843–1852. [CrossRef] [PubMed]\n281. Pagay, V.; Kidman,\
    \ C.; Jenkins, A. Proximal and remote sensing tools for regional-scale characterisation\
    \ of\ngrapevine water and nitrogen status in Coonawarra. Wine Vitic. J. 2016,\
    \ 31, 42–47.\n282. Romero, M.; Luo, Y.; Su, B.; Fuentes, S. Vineyard water status\
    \ estimation using multispectral imagery from an\nUAV platform and machine learning\
    \ algorithms for irrigation scheduling management. Comput. Electron. Agric.\n\
    2018, 147, 109–117. [CrossRef]\n283. Goldhamer, D.A.; Viveros, M.; Salinas, M.\
    \ Regulated deﬁcit irrigation in almonds: eﬀects of variations in\napplied water\
    \ and stress timing on yield and yield components. Irrig. Sci. 2006, 24, 101–114.\
    \ [CrossRef]\n284. Girona, J.; Marsal, J.; Cohen, M.; Mata, M.; Miravete, C. Physiological,\
    \ growth and yield responses of almond\n(Prunus dulcis L ) to diﬀerent irrigation\
    \ regimes. Acta Hortic. 1993, 335, 389–398. [CrossRef]\n285. Sadler, E.; Evans,\
    \ R.; Stone, K.; Camp, C. Opportunities for conservation with precision irrigation.\
    \ J. Soil\nWater Conserv. 2005, 60, 371–378.\n286. Corbane, C.; Jacob, F.; Raclot,\
    \ D.; Albergel, J.; Andrieux, P. Multitemporal analysis of hydrological soil surface\n\
    characteristics using aerial photos: A case study on a Mediterranean vineyard.\
    \ Int. J. Appl. Earth Obs. Geoinf.\n2012, 18, 356–367. [CrossRef]\n287. Osroosh,\
    \ Y.; Peters, R.T.; Campbell, C.S. Daylight crop water stress index for continuous\
    \ monitoring of water\nstatus in apple trees. Irrig. Sci. 2016, 34, 209–219. [CrossRef]\n\
    288. Osroosh, Y.; Peters, R.T.; Campbell, C.S.; Zhang, Q. Comparison of irrigation\
    \ automation algorithms for\ndrip-irrigated apple trees. Comput. Electron. Agric.\
    \ 2016, 128, 87–99. [CrossRef]\n289. Lamm, F.R.; Aiken, R.M. Comparison of temperature-time\
    \ threshold-and ET-based irrigation scheduling for\ncorn production. In Proceedings\
    \ of the 2008 ASABE Annual International Meeting, Providence, RI, USA,\n29 June–2\
    \ July 2008; p. 1.\n290. O’Shaughnessy, S.A.; Evett, S.R.; Colaizzi, P.D.; Howell,\
    \ T.A. A crop water stress index and time threshold\nfor automatic irrigation\
    \ scheduling of grain sorghum. Agric. Water Manag. 2012, 107, 122–132. [CrossRef]\n\
    291. Bellvert, J.; Zarco-Tejada, P.; Gonzalez-Dugo, V.; Girona, J.; Fereres, E.\
    \ Scheduling vineyard irrigation based\non mapping leaf water potential from airborne\
    \ thermal imagery. In Precision Agriculture’13; Staﬀord, J.V., Ed.;\nSpringer:\
    \ Cham, Switzerland, 2013; pp. 699–704.\n292. Bellvert, J.; Girona, J. The use\
    \ of multispectral and thermal images as a tool for irrigation scheduling in\n\
    vineyards. In The Use of Remote Sensing and Geographic Information Systems for\
    \ Irrigation Management in\nSouthwest Europe; Erena, M., López-Francos, A., Montesinos,\
    \ S., Berthoumieu, J.-P., Eds.; CIHEAM: Zaragoza,\nSpain, 2012; pp. 131–137.\n\
    293. Erdem, Y.; ¸Sehirali, S.; Erdem, T.; Kenar, D. Determination of crop water\
    \ stress index for irrigation scheduling\nof bean (Phaseolus vulgaris L.). Turk.\
    \ J. Agric. For. 2006, 30, 195–202.\n294. Osroosh, Y.; Troy Peters, R.; Campbell,\
    \ C.S.; Zhang, Q. Automatic irrigation scheduling of apple trees using\ntheoretical\
    \ crop water stress index with an innovative dynamic threshold. Comput. Electron.\
    \ Agric. 2015, 118,\n193–203. [CrossRef]\n295. Irmak, S.; Haman, D.Z.; Bastug,\
    \ R. Determination of crop water stress index for irrigation timing and yield\n\
    estimation of corn. Agron. J. 2000, 92, 1221–1227. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n35 of 35\n296. Acevedo-Opazo, C.; Tisseyre, B.; Ojeda, H.; Ortega-Farias,\
    \ S.; Guillaume, S. Is it possible to assess the spatial\nvariability of vine\
    \ water status? OENO One 2008, 42, 203–219. [CrossRef]\n297. Acevedo-Opazo, C.;\
    \ Tisseyre, B.; Guillaume, S.; Ojeda, H. The potential of high spatial resolution\
    \ information\nto deﬁne within-vineyard zones related to vine water status. Precis.\
    \ Agric. 2008, 9, 285–302. [CrossRef]\n298. Petrie, P.R.; Wang, Y.; Liu, S.; Lam,\
    \ S.; Whitty, M.A.; Skewes, M.A. The accuracy and utility of a low cost\nthermal\
    \ camera and smartphone-based system to assess grapevine water status. Biosyst.\
    \ Eng. 2019, 179,\n126–139. [CrossRef]\n299. Woellert, K.; Ehrenfreund, P.; Ricco,\
    \ A.J.; Hertzfeld, H. Cubesats: Cost-eﬀective science and technology\nplatforms\
    \ for emerging and developing nations. Adv. Space Res. 2011, 47, 663–684. [CrossRef]\n\
    300. Kramer, H.J.; Cracknell, A.P. An overview of small satellites in remote sensing.\
    \ Int. J. Remote Sens. 2008, 29,\n4285–4337. [CrossRef]\n301. McCabe, M.; Aragon,\
    \ B.; Houborg, R.; Mascaro, J. CubeSats in Hydrology: Ultrahigh-Resolution Insights\n\
    Into Vegetation Dynamics and Terrestrial Evaporation. Water Resour. Res. 2017,\
    \ 53, 10017–10024. [CrossRef]\n302. Trombetti, M.; Riaño, D.; Rubio, M.; Cheng,\
    \ Y.; Ustin, S. Multi-temporal vegetation canopy water content\nretrieval and\
    \ interpretation using artiﬁcial neural networks for the continental USA. Remote\
    \ Sens. Environ.\n2008, 112, 203–215. [CrossRef]\n303. King, B.; Shellie, K. Evaluation\
    \ of neural network modeling to predict non-water-stressed leaf temperature in\n\
    wine grape for calculation of crop water stress index. Agric. Water Manag. 2016,\
    \ 167, 38–52. [CrossRef]\n304. Shan, N.; Ju, W.; Migliavacca, M.; Martini, D.;\
    \ Guanter, L.; Chen, J.; Goulas, Y.; Zhang, Y. Modeling canopy\nconductance and\
    \ transpiration from solar-induced chlorophyll ﬂuorescence. Agric. For. Meteorol.\
    \ 2019, 268,\n189–201. [CrossRef]\n305. Moreno, J.; Goulas, Y.; Huth, A.; Middelton,\
    \ E.; Miglietta, F.; Mohammed, G.; Nebdal, L.; Rascher, U.;\nVerhof, W. Report\
    \ for mission selection: CarbonSat ﬂex–An earth explorer to observe vegetation\
    \ ﬂuorescence.\nEur. Space Agency 2015, 1330/2, 179–185.\n306. Drusch, M.; Moreno,\
    \ J.; Del Bello, U.; Franco, R.; Goulas, Y.; Huth, A.; Kraft, S.; Middleton, E.M.;\
    \ Miglietta, F.;\nMohammed, G.; et al. The ﬂuorescence explorer mission concept-ESA’s\
    \ Earth explorer 8. IEEE Trans. Geosci.\nRemote Sens. 2017, 55, 1273–1284. [CrossRef]\n\
    307. Gautam, D.; Lucieer, A.; Watson, C.; McCoull, C. Lever-arm and boresight\
    \ correction, and ﬁeld of view\ndetermination of a spectroradiometer mounted on\
    \ an unmanned aircraft system. ISPRS J. Photogramm.\nRemote Sens. 2019, 155, 25–36.\
    \ [CrossRef]\n308. Garzonio, R.; Di Mauro, B.; Colombo, R.; Cogliati, S. Surface\
    \ reﬂectance and sun-induced ﬂuorescence\nspectroscopy measurements using a small\
    \ hyperspectral UAS. Remote Sens. 2017, 9, 472. [CrossRef]\n309. Gautam, D.; Lucieer,\
    \ A.; Bendig, J.; Malenovský, Z. Footprint Determination of a Spectroradiometer\
    \ Mounted\non an Unmanned Aircraft System. IEEE Trans. Geosci. Remote Sens. 2019,\
    \ 1–12. [CrossRef]\n310. Bendig, J.; Malenovskỳ, Z.; Gautam, D.; Lucieer, A. Solar-Induced\
    \ Chlorophyll Fluorescence Measured\nFrom an Unmanned Aircraft System: Sensor\
    \ Etaloning and Platform Motion Correction. IEEE Trans. Geosci.\nRemote Sens.\
    \ 2019, 1–8. [CrossRef]\n311. TongKe, F. Smart agriculture based on cloud computing\
    \ and IOT. J. Converg. Inf. Technol. 2013, 8, 210–216.\n312. Ojha, T.; Misra,\
    \ S.; Raghuwanshi, N.S. Wireless sensor networks for agriculture: The state-of-the-art\
    \ in\npractice and future challenges. Comput. Electron. Agric. 2015, 118, 66–84.\
    \ [CrossRef]\n313. Hori, M.; Kawashima, E.; Yamazaki, T. Application of cloud\
    \ computing to agriculture and prospects in other\nﬁelds. Fujitsu Sci. Tech. J.\
    \ 2010, 46, 446–454.\n314. Goap, A.; Sharma, D.; Shukla, A.; Krishna, C.R. An\
    \ IoT based smart irrigation management system using\nMachine learning and open\
    \ source technologies. Comput. Electron. Agric. 2018, 155, 41–49. [CrossRef]\n\
    © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open\
    \ access\narticle distributed under the terms and conditions of the Creative Commons\
    \ Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Agronomy (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2073-4395/10/1/140/pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: A Review of Current and Potential Applications of Remote Sensing to Study
    the Water Status of Horticultural Crops
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s21134363
  analysis: '>'
  authors:
  - Shona Nabwire
  - Hyun Kwon Suh
  - Moon S. Kim
  - Insuck Baek
  - Byoung‐Kwan Cho
  citation_count: 30
  full_citation: '>'
  full_text: ">\nsensors\nReview\nReview: Application of Artiﬁcial Intelligence in\
    \ Phenomics\nShona Nabwire 1, Hyun-Kwon Suh 2,*\n, Moon S. Kim 3, Insuck Baek\
    \ 3\nand Byoung-Kwan Cho 1,4,*\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\
    \x03\x04\x05\x06\a\nCitation: Nabwire, S.; Suh, H.-K.;\nKim, M.S.; Baek, I.; Cho,\
    \ B.-K. Review:\nApplication of Artiﬁcial Intelligence\nin Phenomics. Sensors\
    \ 2021, 21, 4363.\nhttps://doi.org/10.3390/s21134363\nAcademic Editor: Sindhuja\
    \ Sankaran\nReceived: 13 May 2021\nAccepted: 22 June 2021\nPublished: 25 June\
    \ 2021\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims\
    \ in\npublished maps and institutional afﬁl-\niations.\nCopyright: © 2021 by the\
    \ authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access\
    \ article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\n\
    Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n\
    1\nDepartment of Biosystems Engineering, Chungnam National University, Daejeon\
    \ 34134, Korea;\nnabwireshona@o.cnu.ac.kr\n2\nDepartment of Life Resources Industry,\
    \ Dong-A University, Busan 49315, Korea\n3\nEnvironmental Microbial and Food Safety\
    \ Laboratory, Agricultural Research Service, United States\nDepartment of Agriculture,\
    \ Powder Mill Road, BARC-East, Bldg 303, Beltsville, MD 20705, USA;\nmoon.kim@usda.gov\
    \ (M.S.K.); insuck.baek@usda.gov (I.B.)\n4\nDepartment of Smart Agriculture System,\
    \ Chungnam National University, Daejeon 34134, Korea\n*\nCorrespondence: davidsuh79@dau.ac.kr\
    \ (H.-K.S.); chobk@cnu.ac.kr (B.-K.C.)\nAbstract: Plant phenomics has been rapidly\
    \ advancing over the past few years. This advancement\nis attributed to the increased\
    \ innovation and availability of new technologies which can enable the\nhigh-throughput\
    \ phenotyping of complex plant traits. The application of artiﬁcial intelligence\
    \ in\nvarious domains of science has also grown exponentially in recent years.\
    \ Notably, the computer\nvision, machine learning, and deep learning aspects of\
    \ artiﬁcial intelligence have been successfully\nintegrated into non-invasive\
    \ imaging techniques. This integration is gradually improving the\nefﬁciency of\
    \ data collection and analysis through the application of machine and deep learning\
    \ for\nrobust image analysis. In addition, artiﬁcial intelligence has fostered\
    \ the development of software\nand tools applied in ﬁeld phenotyping for data\
    \ collection and management. These include open-\nsource devices and tools which\
    \ are enabling community driven research and data-sharing, thereby\navailing the\
    \ large amounts of data required for the accurate study of phenotypes. This paper\
    \ reviews\nmore than one hundred current state-of-the-art papers concerning AI-applied\
    \ plant phenotyping\npublished between 2010 and 2020. It provides an overview\
    \ of current phenotyping technologies and\nthe ongoing integration of artiﬁcial\
    \ intelligence into plant phenotyping. Lastly, the limitations of the\ncurrent\
    \ approaches/methods and future directions are discussed.\nKeywords: artiﬁcial\
    \ intelligence; deep learning; plant phenomics; ﬁeld phenotyping; high through-\n\
    put phenotyping; image-based phenotyping\n1. Introduction\nAccording to data from\
    \ the United Nations, the world population is expected to grow\nto nine billion\
    \ by 2050 [1]. With the increasing need for food production to match the\nprojected\
    \ population growth in order to prevent food insecurity, plant phenotyping is\n\
    now at the forefront of plant breeding as compared to genotyping [2]. Plant phenotyping\n\
    is deﬁned as the assessment of complex traits such as growth, development, tolerance,\n\
    resistance, architecture, physiology, ecology, yield, and the basic measurement\
    \ of individual\nquantitative parameters that form the basis for complex trait\
    \ assessment [2]. Scientists\nare increasingly interested in the use of phenomic-level\
    \ data to aid in the correlation\nbetween genomics and the variation in crop yields\
    \ and plant health [3–5]. In this way, plant\nphenotyping has become an important\
    \ aspect of crop improvement, availing data to assess\ntraits for variety selection\
    \ in order to identify desirable traits and eliminate undesirable\ntraits during\
    \ the evaluation of plant populations [6]. Plant phenotyping has improved\nprogressively\
    \ over the past 30 years, although obtaining satisfactory phenotypic data for\n\
    complex traits such as stress tolerance and yield potential remains challenging\
    \ [7]. The\nlarge data required for the effective study of phenotypes has led\
    \ to the development and\nuse of high-throughput phenotyping technologies to enable\
    \ the characterization of large\nnumbers of plants at a fraction of the time,\
    \ cost, and labor of previously used traditional\nSensors 2021, 21, 4363. https://doi.org/10.3390/s21134363\n\
    https://www.mdpi.com/journal/sensors\nSensors 2021, 21, 4363\n2 of 19\ntechniques.\
    \ Traditional techniques previously required destructive measurements whereby\n\
    crops were harvested at particular growth stages in order to carry out genetic\
    \ testing and\nthe mapping of plant traits [8]. Since crop breeding programs require\
    \ repeated experimental\ntrials in order to ascertain which traits are of interest,\
    \ the process was slow, costly and\nsigniﬁcantly lagging behind the DNA sequencing\
    \ technologies which are necessary for\ncrop improvement [9].\nHigh-throughput\
    \ phenotyping has been fostered by non-invasive imaging techniques,\nwhich have\
    \ enabled the visualization of plant cell structures on a wider scale. As these\n\
    imaging technologies develop, images carry more useful extractable information\
    \ that sup-\nports biological interpretations of plant growth [10,11]. These techniques\
    \ include thermal\nimaging [12], chlorophyll ﬂuorescence [13,14], digital imaging\
    \ [15], and spectroscopic\nimaging [16].\nHigh-throughput phenotyping techniques\
    \ are currently being used to enable data\nacquisition in both laboratory and\
    \ ﬁeld settings. They are being employed at the levels of\ndata collection, data\
    \ management, and analysis. They include imaging sensors, growth\nchambers, data\
    \ management and analysis software, etc. [7,17–20]. The integration of\nartiﬁcial\
    \ intelligence into these technologies has contributed to the development of the\
    \ non-\ninvasive imaging aspect of phenomics. Artiﬁcial intelligence (AI) technologies\
    \ in the form\nof computer vision and machine learning are increasingly being\
    \ used to acquire and analyze\nplant image data. Computer vision systems process\
    \ digital images of plants to detect\nspeciﬁc attributes for object recognition\
    \ purposes [21]. Machine learning employs various\ntools and approaches to ‘learn’\
    \ from large collections of crop phenotypes in order to classify\nunique data,\
    \ identify new patterns and features, and predict novel trends [22,23]. Recent\n\
    advancements in deep learning, a subset of machine learning, have provided promising\n\
    results for real-time image analysis. Deep learning is a machine learning approach\
    \ that\ntakes advantage of the large plant datasets available and uses them to\
    \ carry out image\nanalysis using convolutional neural networks [24]. In ﬁeld\
    \ phenotyping, AI is being applied\nin ﬁeld equipment for ground and obstacle\
    \ detection, the detection of plants and weeds\nduring data collection, and the\
    \ stable remote control of the equipment [25]. Although\nthe ﬁeld phenotyping\
    \ applications of AI are in relative infancy compared to laboratory\nphenotyping,\
    \ their growth is notable because they provide the phenotypic data of plants in\n\
    their natural environment.\nIn addition to image analysis, AI applications that\
    \ are widely used in other domains\nof science are now being integrated into the\
    \ phenomics data management pipeline. Cy-\nberinfrastructure (CI), a research\
    \ environment that provides linkages between researchers,\ndata storage, and computing\
    \ systems using high-performance networks has been applied\nwidely in the environmental\
    \ sciences [26–28]. CI is now being applied to phenomics in\norder to facilitate\
    \ collaboration among researchers [29]. Open-source devices and tools\nrepresent\
    \ another fast developing application of AI technologies [30]. In phenomics, these\n\
    tools are addressing the challenges of expensive phenotyping equipment and proprietary\n\
    or incompatible data formats. The growth of these applications of AI has expanded\
    \ the\nﬁeld of phenomics with industry companies investing in the manufacture\
    \ and distribution\nof phenotyping technologies alongside government-funded agricultural\
    \ institutions.\nThis review begins by considering the broader area of artiﬁcial\
    \ intelligence and its inte-\ngration and application in phenomics through machine\
    \ learning and deep learning. Digital,\nﬂuorescence, spectroscopic, thermography,\
    \ and tomography imaging, and the integration\nof artiﬁcial intelligence into\
    \ their individual data management are highlighted. Thereafter,\nadditional applications\
    \ of AI such as cyberinfrastructure and open-source devices and tools\nare discussed.\
    \ The current utilization of phenotyping technologies for ﬁeld phenotyping,\n\
    which is increasingly gaining ground over phenotyping in controlled environments,\
    \ is then\ndiscussed brieﬂy, highlighting their cross applicability with artiﬁcial\
    \ intelligence.\nSensors 2021, 21, 4363\n3 of 19\n2. Artiﬁcial Intelligence\n\
    Artiﬁcial Intelligence (AI) is widely referred to as the simulation of human intelligence\n\
    in machines which are programmed to think like humans and mimic their actions.\
    \ It is\napplied when referring to machines that exhibit traits associated with\
    \ the human mind [31].\nThe recent emergence and growth of AI in academia has\
    \ presented an opportunity, as well\nas a threat, for various domains in the sciences.\
    \ Whereas some predict that the application\nof AI through robotics could lead\
    \ to technological unemployment, it has enabled science to\nextend into areas\
    \ previously unexplored and provided ease of execution, for example, in\nmedical\
    \ diagnostics [32].\nIn order to effectively program machines for desired tasks,\
    \ AI methods call for large\nrepositories of data. The algorithms used in AI methods\
    \ need large sets of data for train-\ning to facilitate decision support by enhancing\
    \ early detection and thereby improving\ndecision-making [33]. The data acquisition\
    \ process in non-destructive phenomics involves\nintegrating the data from instruments/sensors\
    \ (i.e., digital cameras and spectrometers),\nusually equipped with their individual,\
    \ proprietary communication protocols, into the AI\nalgorithms. The sensor outputs\
    \ often require conversion to compatible digital formats be-\nfore analysis [7].\
    \ Phenomic data management therefore involves three critical components\nwhere\
    \ artiﬁcial intelligence is applied: algorithms and programs to convert the sensory\
    \ data\ninto phenotypic information; model development to understand the genotype–phenotype\n\
    relationships with environmental interactions; and the management of databases\
    \ to allow\nfor the sharing of information and resources [6]. The main aspects\
    \ of AI, machine learning,\ndeep learning, and computer vision have been applied\
    \ thus far to a recognizable extent in\nphenomics (illustrated in Figure 1). Other\
    \ areas of application are cyberinfrastructure and\nopen-source devices and tools,\
    \ which will subsequently be discussed in detail.\nSensors 2021, 21, x FOR PEER\
    \ REVIEW \n3 of 19 \n \n \n2. Artificial Intelligence \nArtificial Intelligence\
    \ (AI) is widely referred to as the simulation of human intelli-\ngence in machines\
    \ which are programmed to think like humans and mimic their actions. \nIt is applied\
    \ when referring to machines that exhibit traits associated with the human \n\
    mind [31]. The recent emergence and growth of AI in academia has presented an\
    \ oppor-\ntunity, as well as a threat, for various domains in the sciences. Whereas\
    \ some predict that \nthe application of AI through robotics could lead to technological\
    \ unemployment, it has \nenabled science to extend into areas previously unexplored\
    \ and provided ease of execu-\ntion, for example, in medical diagnostics [32].\
    \ \nIn order to effectively program machines for desired tasks, AI methods call\
    \ for large \nrepositories of data. The algorithms used in AI methods need large\
    \ sets of data for training \nto facilitate decision support by enhancing early\
    \ detection and thereby improving deci-\nsion-making [33]. The data acquisition\
    \ process in non-destructive phenomics involves in-\ntegrating the data from instruments/sensors\
    \ (i.e., digital cameras and spectrometers), usu-\nally equipped with their individual,\
    \ proprietary communication protocols, into the AI al-\ngorithms. The sensor outputs\
    \ often require conversion to compatible digital formats be-\nfore analysis [7].\
    \ Phenomic data management therefore involves three critical components \nwhere\
    \ artificial intelligence is applied: algorithms and programs to convert the sensory\
    \ \ndata into phenotypic information; model development to understand the genotype–phe-\n\
    notype relationships with environmental interactions; and the management of databases\
    \ \nto allow for the sharing of information and resources [6]. The main aspects\
    \ of AI, machine \nlearning, deep learning, and computer vision have been applied\
    \ thus far to a recognizable \nextent in phenomics (illustrated in Figure 1).\
    \ Other areas of application are cyberinfra-\nstructure and open-source devices\
    \ and tools, which will subsequently be discussed in de-\ntail. \n \nFigure 1.\
    \ Workflow illustrating application of AI in phenomics. \n \n \nFigure 1. Workﬂow\
    \ illustrating application of AI in phenomics.\n2.1. Machine Learning\nSince the\
    \ 1970s, AI research has been focused on machine learning. Statistical ma-\nchine\
    \ learning frameworks and models such as Perceptron, support vector machines,\
    \ and\nBayesian networks have been designed. However, no single model works best\
    \ for all tasks.\nIt is still challenging to determine the best model for a given\
    \ problem [34]. According\nSensors 2021, 21, 4363\n4 of 19\nto Roscher et al.\
    \ 2020 [35], the rise and success of neural networks, coupled with the\nabundance\
    \ of data and high-level computational and data processing infrastructure, has\n\
    led to the comprehensive utilization of machine learning (ML) models and algorithms\n\
    despite the challenge of model determinations for a given task.\nThe utilization\
    \ of a range of imaging techniques for nondestructive phenotyping has\ninﬂuenced\
    \ the development of high-throughput phenotyping (HTP), whereby multiple\nimaging\
    \ sensors collect plant data in near-real-time platforms. These sensors have the\n\
    capacity to collect large volumes of data which has, in turn, made phenomics a\
    \ big data\nproblem well suited for the application of ML. The analysis and interpretation\
    \ of these large\ndatasets are quite challenging, but ML algorithms provide an\
    \ approach for faster, efﬁcient,\nand better data analytics than traditional processing\
    \ methods. Traditional processing\nmethods include probability theory, decision\
    \ theory, optimization, and statistics. ML\ntools leverage these processing methods\
    \ to extract patterns and features from these large\namounts of data in order\
    \ to enable feature identiﬁcation in particular complex tasks such\nas stress\
    \ phenotyping [36].\nAccording to Rahaman et al. 2019 [37], one of the advantages\
    \ of using ML approaches\nin plant phenotyping is their ability to search large\
    \ datasets and discover patterns by\nsimultaneously looking at a combination of\
    \ features (compared to analyzing each feature\nseparately). This was previously\
    \ a challenge because of the high dimensionality of plant\nimages and their large\
    \ quantity, making them difﬁcult to analyze through traditional\nprocessing methods\
    \ [36]. Machine learning methods have thus far been successfully\napplied in the\
    \ identiﬁcation and classiﬁcation of plant diseases [38,39] and plant organ\n\
    segmentation, among other tasks as shown in Table 1 below. This has been achieved\
    \ by\nsupervised learning, where the algorithms can identify diseased plants after\
    \ being trained\nwith sample images from large datasets. However, this approach\
    \ prevents the search for\nnovel and unexpected phenotypic traits that would otherwise\
    \ be discovered by the less\naccurate unsupervised ML [40].\nTable 1. Examples\
    \ of ML-based approaches that have been applied in phenotyping tasks.\nML-Based\
    \ Approach\nApplication\nPlant\nReference\nBag-of-keypoints,\nSIFT\nIdentiﬁcation\
    \ of plant\ngrowth stage\nWheat\n[41]\nDecision tree\nPlant image segmentation\n\
    Maize\n[42]\nSIFT, SVM\nTaxonomic classiﬁcation of\nleaf images\nA group of varied\n\
    genera and species\n[43]\nMLP, ANFIS\nClassiﬁcation\nWheat\n[44,45]\nkNN, SVM\n\
    Classiﬁcation\nRice\n[46]\nAbbreviations: SIFT, Scale Invariant Features Transforms;\
    \ kNN, k-nearest neighbor; SVM, Support Vector Machine;\nMLP, Multilayer Perceptron;\
    \ ANFIS, Adaptive Neuro-fuzzy Inference System.\n2.2. Deep Learning\nDeep learning\
    \ is a rapidly advancing subset of machine learning tools that has created\na\
    \ paradigm shift in image-based plant phenotyping. It is efﬁcient in the discovery\
    \ of\ncomplex structures in high-dimensional data and is thus applicable to a\
    \ range of scientiﬁc\nresearch tasks [24]. The plant images collected using the\
    \ various sensors have a wide\nvariability, making the use of some machine learning\
    \ techniques challenging [47]. While\ntraditional machine learning involves trial-and-error\
    \ steps in the feature extraction process\nof images, deep learning tools have\
    \ enabled the creation of more reliable workﬂows for\nfeature identiﬁcation. They\
    \ employ an automatic hierarchical feature extraction process\nusing a large bank\
    \ of non-linear ﬁlters before carrying out decision-making, such as\nclassiﬁcation.\
    \ Deep learning approaches have multiple hidden layers in the network, with\n\
    each layer performing a simple operation on the images in succession which increases\
    \ their\ndiscrimination and prediction ability [48]. A wide range of deep learning\
    \ architectures has\nSensors 2021, 21, 4363\n5 of 19\nbeen used in plant phenotyping\
    \ by a process called transfer learning. This involves the use\nof a network that\
    \ was pre-trained on a large dataset somewhat similar to the one under\ninvestigation\
    \ and retraining it with weights for the new dataset. Table 2 details some deep\n\
    learning architectures that have been applied using transfer learning for phenotyping.\n\
    Table 2. Examples of deep learning architectures applied in plant phenotyping\
    \ using transfer learning.\nDeep Learning Architecture\nApplication\nPlant\nReference\n\
    AlexNet, ZFNet, VGG-16,\nGoogLeNet, ResNet-50,\nResNet-101, ResNetXt-101\nIdentiﬁcation\
    \ of biotic\nand abiotic stress\nTomato\n[49]\nVGG-16, VGG-19\nSemantic\nsegmentation\
    \ of crops\nand weeds\nOilseed rape\n[50]\nXception net,\nInception-ResNet, DenseNet\n\
    Weed identiﬁcation\nBlack Nightshade\n[51]\nGoogLeNet\nPlant disease\nclassiﬁcation\n\
    A group of 12 plant\nspecies\n[52]\nVGG-16, VGG-19,\nInception-v3, ResNt50\nClassiﬁcation\
    \ of biotic\nstress\nApple\n[53]\nYOLOv3\nLeaf counting\nArabidopsis\n[54]\nEven\
    \ with the current inﬂux of deep learning architectures, they still face a few\
    \ chal-\nlenges in their integration with agricultural applications. There is\
    \ still limited availability\nof publicly available annotated agricultural data,\
    \ which reduces the possibility of obtain-\ning high-performance feature extraction\
    \ models through transfer learning. In addition,\nmany agricultural image data\
    \ have high levels of occlusion (especially plant leaves and\nbackground noise),\
    \ leading to higher likelihoods of error from confusing objects of interest\n\
    with the background. This is partly due to the environmental variations (e.g.,\
    \ cloudy sky,\nwindy weather for ﬁeld data collection) that signiﬁcantly impact\
    \ the images and make\nthem harder to work with. Similarly, data samples are also\
    \ sensitive to imaging angles,\nﬁeld terrain and conditions, and variations within\
    \ plant genotypes. Hence, the robustness\nand adaptability requirements are signiﬁcantly\
    \ high for the deep learning models built for\nagricultural applications [55].\n\
    Thus far, deep learning architectures in phenotyping have been used in leaf count-\n\
    ing [56], the classiﬁcation of plant morphology [57], plant recognition and identiﬁcation\
    \ [55],\nroot and shoot feature identiﬁcation [48], and plant stress identiﬁcation\
    \ and classiﬁca-\ntion [58]. A few reported applications of machine learning or\
    \ deep learning for stress\nprediction and quantiﬁcation provide great opportunities\
    \ for new research efforts of plant\nscientists. A key challenge to overcome is\
    \ that the underlying processes for linking the\ninputs to the outputs are too\
    \ complex to model mathematically.\n3. Application of Artiﬁcial Intelligence in\
    \ Phenotyping Technologies\n3.1. Imaging Techniques\nTraditionally, the measurement\
    \ of observable plant traits has been conducted by\ndestructive sampling followed\
    \ by laboratory determinations to characterize phenotypes\nbased on their genetic\
    \ functions. Due to technological advancement in AI, imaging tech-\nniques (overview\
    \ in Table 3) have emerged as important tools for non-destructive sampling,\n\
    allowing image capture, data processing, and analysis to determine observable\
    \ plant traits.\nAccording to Houle et al. 2010 [5], “imaging is ideal for phenomic\
    \ studies because of the\navailability of many technologies that span molecular\
    \ to organismal spatial scales, the\nintensive nature of the characterization,\
    \ and the applicability of generic segmentation\ntechniques to data.” Spatial\
    \ or temporal data of many phenotype classes such as mor-\nphology and geometric\
    \ features, behavior, physiological state, and locations of proteins\nSensors\
    \ 2021, 21, 4363\n6 of 19\nand metabolites can be captured in intensive detail\
    \ by imaging. For that reason, imaging\ntechniques have allowed for high-throughput\
    \ screening and real-time image analysis of\nphysiological changes in plant populations.\
    \ At the laboratory scale, the different imaging\nmethods are tested individually.\
    \ One, or a combination of the best-suited methods for crop\nsurveillance, are\
    \ then used both in controlled and ﬁeld environments [5,59].\nTable 3. Visualization\
    \ techniques and applications [60].\nImaging Technique\nApplications\nReference\n\
    Fluorescence\nPhotosynthesis features\nMetabolite composition\nPathogen infection\n\
    [14,61–63]\nRGB Imaging\nPhotosynthesis characteristics\nPathogen infection\n\
    Nutritional deﬁciencies\n[15,22,64,65]\nThermography\nIrrigation management\n\
    Transpirational characteristics\n[13,66–68]\nTomography\nTissue structure and\
    \ metabolites\nMonitoring physiological and biochemical\nprocesses that occur\
    \ in vivo\n[69–71]\nSpectroscopy\nIdentiﬁcation of physiological responses,\n\
    pathogens, and pests\nSurface structure growth and movements,\npigment content\n\
    [16,72–74]\nComputer vision is the major aspect of artiﬁcial intelligence that\
    \ is applied in these\nimaging techniques. Zhuang et al. 2017 [34], comprehensively\
    \ state that “computer\nvision aims to bring together factors derived from multiple\
    \ research areas such as image\nprocessing and statistical learning to simulate\
    \ human perception capability using the\npower of computational modeling of the\
    \ visual domain.” Computer vision uses machine\nlearning to recognize patterns\
    \ and extract information from the images. The computer\nvision workﬂow (shown\
    \ in Figure 2) carries out visual tasks ranging from pre-processing\n(e.g., conversion\
    \ of image formats, color space conversions etc.) to object detection applied\n\
    in ML algorithms, resulting in the comprehension of image understanding in a human-like\n\
    way [34]. Computer vision transforms the images so that they can be applied to\
    \ an AI\nsystem. This process has high computational demands, especially when\
    \ working with\nimages of varying formats from a range of sensors, as is the case\
    \ in phenotyping imaging\ntechniques [32]. A few of the different imaging techniques\
    \ are discussed below.\nSensors 2021, 21, x FOR PEER REVIEW \n7 of 19 \n \n \n\
    Figure 2. AI workflow for image analysis. \n3.1.1. Digital/RGB Imaging \nDigital\
    \ imaging is the lowest costing and easiest to use imaging technique. Its images\
    \ \ncomprise pixels from a combination of the red, green, and blue (RGB) color\
    \ channels. RGB \ncamera sensors are sensitive to light in the visible spectral\
    \ range (400–700 nm). Within this \nrange, they are able to extract images that\
    \ can be used to depict some significant physio-\nlogical changes in a biological\
    \ sample. RGB/digital imaging depends on the color variation \nof different biological\
    \ samples and has significantly contributed to various plant pheno-\ntyping aspects\
    \ [75,76]. It tracks the color changes and directly helps in monitoring the \n\
    status of plant developmental stage, morphology, biomass, health, yield traits,\
    \ and stress \nresponse mechanisms. These mechanisms can be measured rapidly and\
    \ accurately for \nFigure 2. AI workﬂow for image analysis.\n3.1.1. Digital/RGB\
    \ Imaging\nDigital imaging is the lowest costing and easiest to use imaging technique.\
    \ Its images\ncomprise pixels from a combination of the red, green, and blue (RGB)\
    \ color channels. RGB\ncamera sensors are sensitive to light in the visible spectral\
    \ range (400–700 nm). Within\nthis range, they are able to extract images that\
    \ can be used to depict some signiﬁcant\nphysiological changes in a biological\
    \ sample. RGB/digital imaging depends on the color\nSensors 2021, 21, 4363\n7\
    \ of 19\nvariation of different biological samples and has signiﬁcantly contributed\
    \ to various plant\nphenotyping aspects [75,76]. It tracks the color changes and\
    \ directly helps in monitoring\nthe status of plant developmental stage, morphology,\
    \ biomass, health, yield traits, and\nstress response mechanisms. These mechanisms\
    \ can be measured rapidly and accurately\nfor large populations. Digital imaging\
    \ also provides information on the size and color of\nplants, which enables the\
    \ quantiﬁcation of plant deterioration arising from, for example,\nnutrient deﬁciencies\
    \ or pathogen infections, etc. Using a combination of careful image\ncapture,\
    \ image analysis, and color classiﬁcation, it is possible to follow the progression\
    \ of\nlesions from infections and deﬁciencies over time quantitatively [8].\n\
    Advances in hardware and software for digital image processing have motivated\
    \ the\ndevelopment of machine vision systems providing for the expeditious analysis\
    \ of RGB\nimages. Being relatively the simplest and most widely used technique\
    \ has served as an\nadvantage, as many machine learning techniques and deep learning\
    \ architecture can be\napplied effectively to RGB images. These techniques have\
    \ been applied in the identiﬁcation\nof plant growth stage [41], classiﬁcation\
    \ of plant images [43–46], disease detection and\nclassiﬁcation [38,52,53,65],\
    \ identiﬁcation of biotic and abiotic stress [49,58], weed detec-\ntion [50,51],\
    \ detection of ﬂowering times [57], and leaf counting [54–56]. Although the\n\
    extraction of useful phenotyping features from 2D digital images has been successful,\
    \ the\nexpansion in computer vision has led to an exploration of the applications\
    \ of 3D imaging.\nThis has been achieved using stereo-vision, where two identical\
    \ RGB cameras are used\nto capture images in a setup similar to the operation\
    \ of the human eyes. These images\nare then used to reconstruct a 3D model of\
    \ the plant for analysis using stereo-matching\nalgorithms [60,77]. Approaches\
    \ using multiple images from more than two RGB cameras\nplaced at different viewing\
    \ angles have been successfully used for larger plants with a\nhigher degree of\
    \ occlusion thereby expanding the applications of digital imaging [78].\n3.1.2.\
    \ Spectroscopy\nSpectroscopic imaging is a widely used imaging technique that\
    \ has been used to\npredict many properties of large plant populations [5]. It\
    \ consists of multispectral and\nhyperspectral imaging. In multispectral imaging,\
    \ the images are captured in wavelengths\nbetween visible and near-infrared, consisting\
    \ of up to ﬁfteen spectral bands, whereas in\nhyperspectral imaging, hundreds\
    \ of continuous spectral wavebands are available. Pre-\nvious phenotyping studies\
    \ have shown that spectroscopy can be used to monitor plant\nphotosynthetic pigment\
    \ composition, assess water status, and detect abiotic or biotic plant\nstresses\
    \ [79]. During plant development, varying growth conditions induce changes in\n\
    surface and internal leaf structure, modifying the reﬂection of light from plant\
    \ leaves or\ncanopies. These changes can be visualized by spectroscopy, either\
    \ in the visible spectrum\nor near-infrared wavelengths undetectable by the human\
    \ eye (0.7–1.3 mm) [18]. The appli-\ncation of spectroscopy is therefore important\
    \ for the ﬁeld monitoring of plant traits arising\nfrom gene expression in response\
    \ to environmental factors [20].\nHyperspectral imaging has thus far been successfully\
    \ applied in both controlled envi-\nronments (i.e., greenhouses and growth chambers)\
    \ and ﬁeld environments [80]. However,\na major limitation to the utility of hyperspectral\
    \ data in ﬁeld phenotyping, besides the\ncost of the equipment, is the variability\
    \ in environmental conditions during measurements.\nSpectrometers are highly sensitive\
    \ and rely on solar radiation as a light source in the ﬁeld,\nand this leads to\
    \ difﬁculty in the analysis of images due to cloud cover, shadows caused\nby phenotyping\
    \ platforms, and changes in solar angle during the photo period [8,80].\nAnother\
    \ challenge in hyperspectral data analysis is data redundancy due to the continuous\n\
    nature of wavelengths and their similarity. This has been alleviated by the selection\
    \ of\neffective wavelengths using algorithms such as the successive projections\
    \ algorithm (SPA),\ngenetic algorithm (GA), the Monte-Carlo uninformative variable\
    \ elimination (MC-UVE),\nand boosted regression tree (BRT) which is also a ML\
    \ technique [81–83].\nAlthough these difﬁculties can potentially be remedied by\
    \ applying robust com-\nputer vision algorithms, hyperspectral images have only\
    \ been successfully applied to\nSensors 2021, 21, 4363\n8 of 19\nmachine learning\
    \ algorithms and not to faster and more advanced deep learning algo-\nrithms.\
    \ Nonetheless, hyperspectral imaging allows for a wide variety of stresses to\
    \ be\ndetected and continues to be a promising way to detect speciﬁc signatures\
    \ for a particular\nstressor [18].\n3.1.3. Thermography\nThermography, also known\
    \ as thermal imaging, is a technique that detects infra-\nred radiation from an\
    \ object and creates an image based on it. Thermographic cameras\ndetect infrared\
    \ radiation (9000−14,000 nanometers) in the electromagnetic spectrum and\ncreate\
    \ images based off of it [84]. Thermography has been used in plant research to\n\
    monitor transpiration and canopy temperature. Transpiration is linked with nutrient\n\
    uptake by the roots and, ultimately, with crop productivity. However, it also\
    \ reﬂects\nwater use efﬁciency. Canopy temperature has been widely used to infer\
    \ crop water use,\nphotosynthesis, and, in some cases, to predict yield. In breeding\
    \ programs aimed at\nselecting plants based on water use efﬁciency, thermography\
    \ improves the speed and\neffectiveness of monitoring transpiration [18]. It has\
    \ also been used in the ﬁeld as a\nremote sensing tool to capture canopy temperature\
    \ data for a large number of plots using\nmicrobolometer-based thermal imaging\
    \ mounted on ﬁeld phenotyping platforms above the\ncrop using helium balloons\
    \ or manned aircraft [8]. Despite the inability of thermography to\ndetect pre-symptomatic\
    \ changes in leaves, it can detect changes in leaf thickness [18]. This\nallows\
    \ for the visualization and monitoring of internal structural heterogeneity resulting\n\
    from stresses or infections.\nIn phenotyping, thermography is used in combination\
    \ with other imaging techniques\nfor effective diagnostics [85]. Photogrammetry\
    \ algorithms such as structure-from-motion\nhave been applied to thermographic\
    \ images [86] collected in ﬁeld environments without\nmuch success. Currently,\
    \ the data collected from the images is analyzed using standard\nequations and\
    \ ML statistical methods such as probability theory, decision theory and\nclassiﬁers\
    \ [87]. Thermography has a range of applications from medical diagnostics to\n\
    metal defect detection in industries where deep learning algorithms have been\
    \ applied to\nthermal images [88].\n3.1.4. Fluorescence\nFluorescence imaging,\
    \ also known as ﬂuorescence spectroscopy, is used as a measure-\nment technique\
    \ for photosynthetic function under stresses such as drought and infections\n\
    by detecting light emitted after the plant has been exposed to a speciﬁc wavelength\
    \ of light.\nThese stresses have adverse effects that lead to a decrease in photosynthesis\
    \ which, in turn,\nlimits crop yield. Chlorophyll ﬂuorescence imaging has enabled\
    \ the early visualization\nof viral and fungal infections due to its ability to\
    \ achieve high resolutions. It has also\nbeen used in studies to determine plant\
    \ leaf area [18]. For the rapid screening of plant\npopulations, portable ﬂuorometers\
    \ are being used to obtain average measurements of\nwhole plants or leaves at\
    \ the same developmental stage. There is potential for portable ﬂu-\norescence\
    \ imaging to be used for the ﬁeld-scale assessment of infections, even for those\
    \ that\nleave no visible trace [8,18,67]. Fluorescence imaging is usually used\
    \ in combination with\nhyperspectral imaging, and image data extracted using this\
    \ technique has been successfully\napplied to algorithms based on AI methods,\
    \ such as neural networks, for analysis [89].\n3.1.5. Tomography\nX-ray computed\
    \ tomography (X-ray CT) is a technology that uses computer-processed\nX-rays to\
    \ produce tomographic images of speciﬁc areas of scanned objects. It can generate\
    \ a\n3D image of the inside of an object from an extensive series of 2D radiographic\
    \ images taken\naround a single axis of rotation [76]. X-ray CT imaging technology\
    \ has been used for several\napplications in plant phenotyping. It has been applied\
    \ in the observation of root growth\nbecause of its ability to capture the intricacies\
    \ of the edaphic environment with high spatial\nresolutions [90,91]. X-ray CT\
    \ has also been used in the high-throughput measurement of\nSensors 2021, 21,\
    \ 4363\n9 of 19\nrice tillers to determine grain yield. It was applied as the\
    \ preferred imaging technique for\nthe rice tiller study because of the tendency\
    \ of rice tillers to overlap and, hence, not be easily\ndetectable by digital\
    \ imaging [92]. According to Li et al. 2014 [76], however, “tomographic\nimaging\
    \ remains low throughput, and its image segmentation and reconstruction need to\
    \ be\nfurther improved to enable high throughput plant phenotyping.” Although\
    \ this technology\nis effective in the early detection of plant stress symptoms,\
    \ its effectiveness is further\nimproved by combined use with other imaging techniques.\
    \ The simultaneous use of CT and\npositron emission tomography (PET) has the potential\
    \ to be used to provide insight into the\neffect of abiotic stress in particular\
    \ [71,76]. Additionally, to provide satisfactory resolutions,\nX-ray CT requires\
    \ small pot sizes and controlled environments, making it unsuitable for\nﬁeld\
    \ applications. For morphological root phenotyping tasks, X-ray CT has been applied\
    \ in\nthe identiﬁcation of root tips and root-soil segmentation tasks using machine\
    \ learning [93].\nDespite the minimal use of tomography in phenotyping, its application\
    \ in the medical ﬁeld\npositions it as a powerful technique that, coupled with\
    \ AI algorithms (and particularly\nCNNs), is beneﬁcial in diagnostics [94].\n\
    3.2. Cyberinfrastructure\nCyberinfrastructure (CI) is described by Atkins et al.\
    \ 2003 [95] as a “research environ-\nment that supports advanced data acquisition,\
    \ storage, management, integration, mining,\nvisualization and other computing\
    \ and processing services distributed over the internet\nbeyond the scope of a\
    \ single institution.” It consists of computing systems, data storage\nsystems,\
    \ advanced instruments and data repositories, visualization environments, and\
    \ peo-\nple (shown in Figure 3) linked together by software and high-performance\
    \ networks [27].\nThis enhances the efﬁciency of research and productivity in\
    \ the use of resources. Some CI\nsystems can provide for in-ﬁeld data analysis\
    \ to point out errors in data collection that can\nbe rectiﬁed and identify further\
    \ areas of interest for data collection.\nSensors 2021, 21, x FOR PEER REVIEW\
    \ \n10 of 19 \n \n \nFigure 3. Simplified schematic of cyberinfrastructure. \n\
    CI has been applied in scientific disciplines ranging from biomedical to geospatial\
    \ \nand environmental sciences. One such project is the distributed CI called\
    \ the Function \nBiomedical Informatics Research Network (FBIRN), a large-scale\
    \ project in the area of bi-\nomedical research funded by the U.S. National Institutes\
    \ of Health (NIH) [96]. CI has also \nbeen applied in geospatial research with\
    \ a range of initiatives under the National Spatial \nData Infrastructure (NSDI).\
    \ The NSDI focuses on spatial data collection, sharing, and ser-\nvice, and its\
    \ geodata.gov provides geospatial data services. Data.gov provides all publicly\
    \ \navailable US government data, with their geospatial aspects supplemented by\
    \ geo-\nFigure 3. Simpliﬁed schematic of cyberinfrastructure.\nCI has gained more\
    \ interest in recent years because of the growth in quantities of data\ncollected\
    \ in science, interdisciplinarity in research, the establishment of a range of\
    \ locations\naround the world where cutting-edge research is performed, and the\
    \ spread of advanced\nSensors 2021, 21, 4363\n10 of 19\ntechnologies [96]. Due\
    \ to the various components required, CI systems are expensive, with\nthe cost\
    \ of a supercomputer alone being upwards of US$90 million. For this reason, some\n\
    organizations that can invest in this infrastructure offer it as a service at\
    \ a fee to researchers.\nFor example, the University of Illinois at Chicago provides\
    \ various cost models for access\nto their infrastructure [97].\nCI has been applied\
    \ in scientiﬁc disciplines ranging from biomedical to geospatial\nand environmental\
    \ sciences. One such project is the distributed CI called the Function\nBiomedical\
    \ Informatics Research Network (FBIRN), a large-scale project in the area of\n\
    biomedical research funded by the U.S. National Institutes of Health (NIH) [96].\
    \ CI has\nalso been applied in geospatial research with a range of initiatives\
    \ under the National\nSpatial Data Infrastructure (NSDI). The NSDI focuses on\
    \ spatial data collection from\ngovernment and private sources, integration and\
    \ sharing through its GeoPlatform (https:\n//www.geo-platform.gov/, accessed on\
    \ 26 June 2021) [98]. In the geospatial domain, one\nexample is the Data Observation\
    \ Network for Earth (DataONE), which is a CI platform for\nintegrative biological\
    \ and environmental research. It is designed to provide an underlying\ninfrastructure\
    \ that facilitates data preservation and re-use for research with an initial focus\n\
    on remote-sensed data for the biological and environmental sciences [99]. Because\
    \ of the\ncomplexities in developing and setting up a CI, Wang et al. 2013 [100]\
    \ put forward the\nCyberaide Creative service, which uses virtual machine technologies\
    \ to create a common\nplatform separate from the hardware and software and then\
    \ deploys a cyberinfrastructure\nfor its users. This allows for end-users to specify\
    \ the necessary resource requirements and\nhave them immediately deployed without\
    \ needing to understand their conﬁguration and\nbasic infrastructures.\nIn plant\
    \ phenotyping, a case for the use of CI has been made similarly in that non-\n\
    invasive high throughput phenotyping technologies collect large amounts of plant\
    \ data.\nAnalysis methods for this data using AI are being developed but face\
    \ the challenge of\nintegrating datasets and the poor scalability of these tools\
    \ [29]. The large amounts of data\ngenerated by HTP platforms need to be efﬁciently\
    \ archived and retrieved for analysis.\nResearchers afﬁliated with the United\
    \ States National Science Foundation (NSF) have\ndeveloped a form of CI called\
    \ iPlant that incorporates artiﬁcial intelligence technologies to\nstore and process\
    \ plant data gathered from the various HTP platforms. This CI platform\nprovides\
    \ tools for data analysis and storage with high-performance computing to access\n\
    and analyze the data. It also has methods for the integration of tools and datasets\
    \ [29].\nIn order to support both genotyping and phenotyping, iPlant uses the\
    \ BISQUE (Bio-\nImage Semantic Query User Environment) [101] software system.\
    \ Its main functionality is\nimage analysis which it supports using its ﬁve core\
    \ services: image storage and manage-\nment, metadata management and query, analysis\
    \ execution, and client presentation. Its\ndesign is ﬂexible enough to support\
    \ the range of variability in image analysis workﬂows\nbetween research labs.\
    \ The plant-oriented version of BISQUE, PhytoBISQUE, provides\nan application\
    \ programming interface integrated with iPlant to develop and deploy new\nalgorithms,\
    \ facilitating collaboration among researchers [29].\n3.3. Open-Source Devices\
    \ and Tools\nOpen-source is a term generally used to refer to tools or software\
    \ that, as stated by\nAksulu & Wade, 2010 [30], “allows for the modiﬁcation of\
    \ source code, is freely distributed,\nis technologically neutral, and grants\
    \ free subsidiary licensing rights.” Characteristically,\nOpen-Source Systems\
    \ (OSS) are voluntary and collaborative in nature and their lifespan\nlasts as\
    \ long as there is an individual willing and able to maintain the system [102].\
    \ Few\ntraditional operation constraints such as scope, time, and cost factors\
    \ affect these systems,\nand they have the added advantage of enhancing the skills\
    \ of the people involved while\nproducing tangible cost-effective technology output\
    \ [30]. Some OSS development teams\ntake advantage of crowdsourcing which widens\
    \ the scope and quality of ideas and reduces\nproject cycle time [103].\nSensors\
    \ 2021, 21, 4363\n11 of 19\nIn phenomics, new crop management strategies require\
    \ the co-analyses of both sensor\ndata on crop status and related environmental\
    \ and genetic metadata. Unfortunately, this\ndata is mostly restricted to larger\
    \ well-funded agricultural institutions since the instruments\nfor data collection\
    \ are expensive. The available phenotyping instruments output data that is\nchallenging\
    \ to interpret because of proprietary or incompatible formats. Many techniques\n\
    that are being applied are ready-made off-the-shelf software packages that do\
    \ not have\nspeciﬁc algorithms for this data interpretation. Phenotyping researchers,\
    \ therefore, have to\naddress the challenges of data interpretation and data sharing\
    \ alongside limited access to\ninstrumentation (especially that which is well\
    \ suited for ﬁeld phenotyping). Open-source\ntools and devices represent a promising\
    \ approach for addressing these challenges. Those be-\ning applied in phenomics\
    \ are more accessible and easy to use while providing a connection\nto a community\
    \ of users with broader support and continuous improvement [104,105]. One\nsuch\
    \ open-source device is the MultispeQ device (PhotosynQ, East Lansing, MI, USA),\
    \ an\ninexpensive device linked through the PhotosynQ platform (https://www.photosynq.com,\n\
    accessed on 1 June 2021) to communities of researchers, providing useful data\
    \ on plant\nperformance. The MultispeQ device is rugged and ﬁeld deployable, open-source,\
    \ and\nexpandable to incorporate new sensors and techniques. The PhotosynQ platform\
    \ connects\nthe MultispeQ instrument to the community of researchers, breeders,\
    \ and citizen scientists\nto foster ﬁeld-based and community-driven phenotyping\
    \ [105].\nAn open-source prediction approach called Dirichlet-aggregation regression\
    \ (DAR)\nwas put forward by Bauckhage and Kersting, 2013 [104], to address the\
    \ challenge of manual\ndata labeling and running supervised classiﬁcation algorithms\
    \ on hyperspectral data.\nHyperspectral cameras record a spectrum of several hundred\
    \ wave-lengths ranging from\napproximately 300 nm to 2500 nm, which poses a signiﬁcant\
    \ challenge of data handling\nin hyperspectral image analysis. Therefore, working\
    \ with hyperspectral data requires\nalgorithms and architecture that can cope\
    \ with massive amounts of data. Their research\nshows that DAR can predict the\
    \ level of drought stress of plants effectively and before it\nbecomes visible\
    \ to the human eye.\nOpen-source software and platforms have also been developed\
    \ that simplify the com-\nputer vision image management pipeline. One such tool\
    \ is the PlantCV image analysis\nsoftware package. It is used to build workﬂows\
    \ that can be used to extract data from images\nand sensors, and it employs various\
    \ computational tools in python that are extendable,\ndepending on the required\
    \ image analysis task, in order to provide data scientists and\nbiologists with\
    \ a common interface [106]. It deploys additional open-source tools such as\n\
    LabelImg for image annotation [107]. PlantCV’s image processing library has been\
    \ applied\nin Deep Plant Phenomics which is an open-source software platform that\
    \ implements deep\nconvolutional neural networks for plant phenotyping [108].\
    \ Deep Plant Phenomics pro-\nvides an image processing pipeline that has been\
    \ used for complex non-linear phenotyping\ntasks such as leaf counting, mutant\
    \ classiﬁcation, and age regression in Arabidopsis.\nAnother group of open-source\
    \ tools that have been applied in phenomics are the\nMobileNet deep learning architectures.\
    \ MobileNet architectures are convolutional neural\nnetworks (CNNs) with reduced\
    \ complexity and model size and are suited to devices with\nlow computational\
    \ power such as mobile phones. MobileNets optimize for latency resulting\nfrom\
    \ low computing power by providing small networks with substantial accuracy that\n\
    can be used in real-world applications [109]. One example is the MobileNetV2 which\
    \ is\nbuilt to be used for classiﬁcation, detection, and segmentation of images.\
    \ It uses ReLU6\nnon-linearity, which is suited to low-precision computation [110].\
    \ These are promising for\nuse on mobile phones which are widely accessible and\
    \ commonplace to be potentially used\nfor ﬁeld phenotyping. MobileNets employ\
    \ TensorFlow Lite, an open-source deep learning\nframework used to deploy machine\
    \ learning models on mobile devices [111].\nLighting problems in outdoor settings\
    \ have the potential to affect open-source tools\nand the performance of CNNs.\
    \ There is, therefore, a need to test and train the networks\non plant images\
    \ collected in the ﬁeld. A mobile-based CNN model was used for plant\ndisease\
    \ diagnosis, and the problem of inconsistent light conditions was solved in this\n\
    Sensors 2021, 21, 4363\n12 of 19\nstudy [112] by the use of an umbrella. This\
    \ model was employed ofﬂine and displayed\ndecreased performance because of differing\
    \ training datasets from the data collected in\nthe ﬁeld and, therefore, highlighted\
    \ the need to capture more images using mobile devices\nin typical ﬁeld settings\
    \ and to use those very images for the training of the models in\norder to improve\
    \ the accuracy. In deploying open-source platforms and CNNs, there is an\nadditional\
    \ challenge in developing tools that can analyze and determine a wide variety\n\
    of phenotypes from various crops, and it has not been possible to develop a one-size-\n\
    ﬁts-all platform for analysis. However, various platforms are thus far available\
    \ for some\nphenotyping tasks.\n4. Artiﬁcial Intelligence and Field Phenotyping\n\
    In order to screen plants for valuable traits (such as grain size, abiotic stress\
    \ toler-\nance, product quality, or yield potential), experiments with repeated\
    \ trials are required\nin different environments based on the objectives of the\
    \ study. Much of the discussion of\nphenotyping has focused on the measurement\
    \ of individual plants in controlled environ-\nments. However, controlled environments\
    \ do not provide an accurate representation of\nplant growth in open-air conditions\
    \ [7]. Field-based phenotyping (FBP) is now increasingly\nwidely recognized as\
    \ the only approach that gives accurate depictions of the traits in\nactual cropping\
    \ systems. Currently, sensor systems suitable for high-throughput ﬁeld\nphenotyping\
    \ can simultaneously measure multiple plots and fuse a multitude of traits in\n\
    different data formats [17]. Through the use of vehicles carrying multiple sets\
    \ of sensors,\nFBP platforms are transforming the characterization of plant populations\
    \ for genetic re-\nsearch and crop improvement. Accomplishing FBP in a timely\
    \ and cost-effective manner\nhas led to the use of unmanned aircraft, wheeled\
    \ vehicles, or agricultural robots to deploy\nmultiple sensors that can measure\
    \ plant traits in brief time intervals [7]. Therefore, pheno-\ntyping in many\
    \ crop breeding programs is now being conducted by combining instruments\nwith\
    \ novel technologies such as non-invasive imaging, robotics, and high-performance\n\
    computing on the FBP platforms [8].\nUnmanned aircraft are particularly attractive\
    \ for data acquisition because they enable\nsensing with a high spatial and spectral\
    \ resolution for a relatively low cost. Unmanned\nhelicopters (such as the one\
    \ shown in Figure 4) can carry various sensors and have the ac-\ncommodation to\
    \ carry larger sensors. In windy conditions, helicopters enable precise ﬂight\n\
    control and operations in cluttered environments because of their maneuverability\
    \ and\nability to ﬂy at low speeds. Replicated collection of sensor data can be\
    \ achieved through au-\ntomatic ﬂight control when the helicopter is equipped\
    \ with algorithms for ground detection,\nobstacle detection and avoidance, and\
    \ stable effective control [113]. Modern unmanned\naerial systems (UAS) are better\
    \ equipped to manage the harsh environmental conditions\nand obstacles due to\
    \ rapid advances in technology such as collision technology, optical\nsensors\
    \ for machine vision, GPS, accelerometers, gyroscopes, and compasses. However,\n\
    they still face the challenge of limited battery power, with electric batteries\
    \ providing\nbetween 10 to 30 min of battery power [25].\nSensors 2021, 21, x\
    \ FOR PEER REVIEW \n13 of 19 \n \nthrough automatic flight control when the helicopter\
    \ is equipped with algorithms for \nground detection, obstacle detection and avoidance,\
    \ and stable effective control [113]. \nModern unmanned aerial systems (UAS) are\
    \ better equipped to manage the harsh envi-\nronmental conditions and obstacles\
    \ due to rapid advances in technology such as collision \ntechnology, optical\
    \ sensors for machine vision, GPS, accelerometers, gyroscopes, and \ncompasses.\
    \ However, they still face the challenge of limited battery power, with electric\
    \ \nbatteries providing between 10 to 30 min of battery power [25]. \n \nFigure\
    \ 4. The CSIRO autonomous helicopter system. Adapted from Merz & Chapman, 2012\
    \ [113]. \nAlthough not commonly used for phenotyping, wheeled vehicles are sometimes\
    \ \nused in phenotyping systems for some research projects since they also provide\
    \ for prox-\nimal phenotyping. They have the advantage of being able to cover\
    \ large areas and operate \nfor longer periods, along with the disadvantage of\
    \ compacting and damaging the soil and \nA\nFigure 4. The CSIRO autonomous helicopter\
    \ system. Adapted from Merz & Chapman, 2012 [113].\nSensors 2021, 21, 4363\n13\
    \ of 19\nAlthough not commonly used for phenotyping, wheeled vehicles are sometimes\
    \ used\nin phenotyping systems for some research projects since they also provide\
    \ for proximal\nphenotyping. They have the advantage of being able to cover large\
    \ areas and operate for\nlonger periods, along with the disadvantage of compacting\
    \ and damaging the soil and\nbeing costly because of the human labor required\
    \ to operate the vehicle. According to\nWhite et al. 2012 [7], high-clearance\
    \ tractors were expected to play a more central role in\nFBP as the wheeled vehicles\
    \ of choice due to their high vertical clearance, availability, and\nease of use.\
    \ They can be used for continuous measurements at different stages of the crop\n\
    growth process and can operate for longer periods compared to the UAVs. A variation\
    \ of\nhigh clearance tractors in the form of mobile motorized platforms, which\
    \ eliminate the\nneed for human labor in the ﬁeld, have been developed and tested\
    \ in various phenotyping\napplications [114,115].\nPhenotyping data collected\
    \ using these FBP systems faces the challenge of instability\nduring motion and\
    \ weather changes, which cause occlusion and mal-alignment in the\nimages. This\
    \ is partially addressed by using proximal sensing at slower speeds in order to\n\
    improve image resolution (although this limits the areal coverage in one ﬂight\
    \ and does\nnot fully solve the misalignment). Data processing for this data has\
    \ signiﬁcantly improved\nin recent years with AI-enabled snapshot and line scanning\
    \ imaging software, optimized\nspeciﬁcally for unmanned aircraft such as structure-from-motion\
    \ photogrammetry [87,116].\nOne approach that has been proposed to enhance the\
    \ application of these platforms for\nsensing in ﬁeld conditions is adapting ﬁeld\
    \ conditions to align with the HTP ﬁeld tech-\nniques (rather than the traditional\
    \ approach of adapting the instruments for the ﬁeld),\ndepending on the crop of\
    \ interest without compromising the realistic crop evaluation in\nﬁeld conditions\
    \ [6].\n5. Phenotyping Communities and Facilities\nThe growth in plant phenotyping\
    \ research coupled with the integration of AI tech-\nnology has fostered the development\
    \ of laboratories and centers equipped with high-\nthroughput phenotyping technologies.\
    \ Some of these plant phenotyping centers are\nmembers of the International Plant\
    \ Phenotyping Network (IPPN), which works with\nvarious member organizations in\
    \ academia and industry to distribute relevant information\nabout plant phenotyping\
    \ and increase its visibility [117]. Partner facilities such as the\nAustralian\
    \ Plant Phenomics Facility, a government-funded national facility, provide access\n\
    to infrastructure such as glass-house automation technologies, digital imaging\
    \ technologies,\nlong-term data storage, etc. [118]. Such facilities have and\
    \ continue to provide subsidized\naccess to advanced AI-phenotyping technologies\
    \ that would otherwise be inaccessible due\nto the costs of operation and maintenance.\
    \ In addition, as high-throughput phenotyping\nbecoming more common, there has\
    \ come the issue of data-merging with many laboratories\ngathering phenotypic\
    \ data that rarely enter the public domain where it could be accessed\nby other\
    \ institutions to foster interdisciplinary research [8]. Networks such as the\
    \ IPPN\ncontinue to provide access to phenotyping information generated by member\
    \ organiza-\ntions which is key in enabling cooperation between the organizations\
    \ and advancing the\nphenomics agenda through collaborative research.\nBesides\
    \ the academic institutions and government organizations, private industry\ncompanies\
    \ (a few of which have been highlighted here) are establishing themselves as\n\
    key providers and facilitators of plant phenotyping AI technology around the world.\
    \ Bio-\npute technology provides high-end research instruments such as multispectral\
    \ cameras\nfor ﬁeld phenotyping, drones for aerial photography, and provides after-sales\
    \ support\nservices to their customers. In partnership with universities and research\
    \ institutes, Bio-\npute provides innovations that are contributing to the progress\
    \ of plant phenotyping\nin China (http://www.bjbiopute.cn, accessed on 1 June\
    \ 2021). KeyGene is an agricul-\ntural biotechnology company providing tools for\
    \ precision breeding and digital pheno-\ntyping investing in deep learning-based\
    \ algorithms and virtual reality for data visual-\nization (https://www.keygene.com,\
    \ accessed on 1 June 2021). PhenoTrait Technology\nSensors 2021, 21, 4363\n14\
    \ of 19\nCo., Ltd. mainly focuses on plant phenotyping using the photosynthesis\
    \ characteris-\ntics of plants and promoting the use of phenotyping technologies\
    \ to improve crop qual-\nity, crop yield, and environmental conditions in China.\
    \ Some of their products include\nhigh-throughput phenotyping instruments, chlorophyll\
    \ ﬂuorescence imaging systems, etc.\n(http://www.phenotrait.com, accessed on 1\
    \ June 2021). Photon Systems Instruments (PSI)\nis a company in the Czech Republic\
    \ that also supplies a range of phenotyping systems,\nboth ﬁeld and laboratory-based,\
    \ including root system phenotyping. They have also in-\ncorporated machine learning\
    \ to integrate robotics into the systems they develop to better\nautomate the\
    \ processes (https://psi.cz, accessed 1 June 2021).\n6. Conclusions\nRecent advancements\
    \ in high-throughput phenotyping technologies have led to signif-\nicant strides\
    \ in plant phenomics. The on-going integration of artiﬁcial intelligence into\
    \ these\ntechnologies promises progression into smarter and much faster technologies\
    \ with signiﬁ-\ncantly lower input costs. In the area of phenotyping image data\
    \ analysis, the integration\nof AI into the data management pipeline of tomography\
    \ and thermography is on a lower\nscale in comparison to the other imaging techniques.\
    \ The application of deep learning in\nthe data analysis of these techniques is\
    \ promising, as it has been successfully implemented\nin analysis of composite\
    \ materials [88] and medical diagnostics [94]. As much as ﬁeld\nphenotyping is\
    \ the most effective way to collect phenotypic data, it is still being conducted\n\
    on a relatively lower scale than is possible. Artiﬁcial intelligence technologies\
    \ also require\nlarge amounts of data from various sources to improve their accuracy.\
    \ This provides an\nopportunity to invest more into the tailoring of current technologies\
    \ for ﬁeld data collection\nand the utilization of already existing AI adaptable\
    \ technologies, such as smartphones, to\nincrease the quantity of quality data.\
    \ Smartphones have become widespread consumer\nproducts, and the simplicity and\
    \ ease of use of their sensors suggest that their use can be\nexplored in agriculture\
    \ [104]. Some of the challenges that would need to be addressed are\nthat advanced\
    \ signal processing on smartphones has to cope with constraints such as low\n\
    battery life, restricted computational power, or limited bandwidth [104]. The\
    \ use of citizen\nscience alongside professional researchers [119] in data collection\
    \ also has the potential\nto aid in increasing the amount of data collected. The\
    \ overall goal of employing these\napproaches and technologies is to provide the\
    \ infrastructure that allows for tracking how\nplant traits progress throughout\
    \ the growing season and facilitate the coordination of data\nanalysis, management,\
    \ and utilization of results using AI methods.\nAuthor Contributions: Conceptualization,\
    \ S.N. and B.-K.C.; investigation, M.S.K. and I.B.; writing—\noriginal draft preparation,\
    \ S.N.; writing—review and editing, H.-K.S. and M.S.K.; visualization, S.N.\n\
    and I.B.; supervision, B.-K.C. and H.-K.S.; funding acquisition, B.-K.C. All authors\
    \ have read and\nagreed to the published version of the manuscript.\nFunding:\
    \ This research was funded by the National Institute of Food Science and Technology\
    \ (Project\nNo.: PJ0156892021) of the Rural Development Administration, Korea.\n\
    Institutional Review Board Statement: Not applicable.\nInformed Consent Statement:\
    \ Not applicable.\nData Availability Statement: No new data were created in this\
    \ study. Data sharing is not applicable\nto this article.\nConﬂicts of Interest:\
    \ The authors declare no conﬂict of interest.\nReferences\n1.\nUN. United Nations|Population\
    \ Division. Available online: https://www.un.org/development/desa/pd/ (accessed\
    \ on 10\nSeptember 2020).\n2.\nCosta, C.; Schurr, U.; Loreto, F.; Menesatti, P.;\
    \ Carpentier, S. Plant phenotyping research trends, a science mapping approach.\n\
    Front. Plant Sci. 2019, 9, 1–11. [CrossRef]\nSensors 2021, 21, 4363\n15 of 19\n\
    3.\nArvidsson, S.; Pérez-Rodríguez, P.; Mueller-Roeber, B. A growth phenotyping\
    \ pipeline for Arabidopsis thaliana integrating image\nanalysis and rosette area\
    \ modeling for robust quantiﬁcation of genotype effects. New Phytol. 2011, 191,\
    \ 895–907. [CrossRef]\n[PubMed]\n4.\nFurbank, R.T. Plant phenomics: From gene\
    \ to form and function. Funct. Plant Biol. 2009, 36, v–vi.\n5.\nHoule, D.; Govindaraju,\
    \ D.R.; Omholt, S. Phenomics: The next challenge. Nat. Rev. Genet. 2010, 11, 855–866.\
    \ [CrossRef]\n6.\nPauli, D. High-throughput phenotyping technologies in cotton\
    \ and beyond. In Proceedings of the Advances in Field-Based\nHigh-Throughput Phenotyping\
    \ and Data Management: Grains and Specialty Crops, Spokane, WA, USA, 9–10 November\
    \ 2015;\npp. 1–11.\n7.\nWhite, J.W.; Andrade-Sanchez, P.; Gore, M.A.; Bronson,\
    \ K.F.; Coffelt, T.A.; Conley, M.M.; Feldmann, K.A.; French, A.N.; Heun,\nJ.T.;\
    \ Hunsaker, D.J.; et al. Field-based phenomics for plant genetics research. Field\
    \ Crops Res. 2012, 133, 101–112. [CrossRef]\n8.\nFurbank, R.T.; Tester, M. Phenomics—Technologies\
    \ to relieve the phenotyping bottleneck. Trends Plant Sci. 2011, 16, 635–644.\n\
    [CrossRef] [PubMed]\n9.\nFahlgren, N.; Gehan, M.A.; Baxter, I. Lights, camera,\
    \ action: High-throughput plant phenotyping is ready for a close-up. Curr.\nOpin.\
    \ Plant Biol. 2015, 24, 93–99. [CrossRef]\n10.\nChen, D.; Neumann, K.; Friedel,\
    \ S.; Kilian, B.; Chen, M.; Altmann, T.; Klukas, C. Dissecting the phenotypic\
    \ components of crop\nplant growthand drought responses based on high-throughput\
    \ image analysis w open. Plant Cell 2014, 26, 4636–4655. [CrossRef]\n[PubMed]\n\
    11.\nWalter, T.; Shattuck, D.W.; Baldock, R.; Bastin, M.E.; Carpenter, A.E.; Duce,\
    \ S.; Ellenberg, J.; Fraser, A.; Hamilton, N.; Pieper, S.;\net al. Visualization\
    \ of image data from cells to organisms. Nat. Methods 2010, 7, S26–S41. [CrossRef]\
    \ [PubMed]\n12.\nOerke, E.C.; Steiner, U.; Dehne, H.W.; Lindenthal, M. Thermal\
    \ imaging of cucumber leaves affected by downy mildew and\nenvironmental conditions.\
    \ J. Exp. Bot. 2006, 57, 2121–2132. [CrossRef]\n13.\nChaerle, L.; Pineda, M.;\
    \ Romero-Aranda, R.; Van Der Straeten, D.; Barón, M. Robotized thermal and chlorophyll\
    \ ﬂuorescence\nimaging of pepper mild mottle virus infection in Nicotiana benthamiana.\
    \ Plant Cell Physiol. 2006, 47, 1323–1336. [CrossRef]\n14.\nZarco-Tejada, P.J.;\
    \ Berni, J.A.J.; Suárez, L.; Sepulcre-Cantó, G.; Morales, F.; Miller, J.R. Imaging\
    \ chlorophyll ﬂuorescence with an\nairborne narrow-band multispectral camera for\
    \ vegetation stress detection. Remote Sens. Environ. 2009, 113, 1262–1275. [CrossRef]\n\
    15.\nJensen, T.; Apan, A.; Young, F.; Zeller, L. Detecting the attributes of a\
    \ wheat crop using digital imagery acquired from a\nlow-altitude platform. Comput.\
    \ Electron. Agric. 2007, 59, 66–77. [CrossRef]\n16.\nMontes, J.M.; Utz, H.F.;\
    \ Schipprack, W.; Kusterer, B.; Muminovic, J.; Paul, C.; Melchinger, A.E. Near-infrared\
    \ spectroscopy on\ncombine harvesters to measure maize grain dry matter content\
    \ and quality parameters. Plant Breed. 2006, 125, 591–595. [CrossRef]\n17.\nBai,\
    \ G.; Ge, Y.; Hussain, W.; Baenziger, P.S.; Graef, G. A multi-sensor system for\
    \ high throughput ﬁeld phenotyping in soybean\nand wheat breeding. Comput. Electron.\
    \ Agric. 2016, 128, 181–192. [CrossRef]\n18.\nChaerle, L.; Van Der Straeten, D.\
    \ Imaging techniques and the early detection of plant stress. Trends Plant Sci.\
    \ 2000, 5, 495–501.\n[CrossRef]\n19.\nGupta, S.; Ibaraki, Y.; Trivedi, P. Applications\
    \ of RGB color imaging in plants. Plant Image Anal. 2014, 41–62. [CrossRef]\n\
    20.\nMontes, J.M.; Melchinger, A.E.; Reif, J.C. Novel throughput phenotyping platforms\
    \ in plant genetic studies. Trends Plant Sci. 2007,\n12, 433–436. [CrossRef] [PubMed]\n\
    21.\nCasanova, J.J.; O’Shaughnessy, S.A.; Evett, S.R.; Rush, C.M. Development\
    \ of a wireless computer vision instrument to detect\nbiotic stress in wheat.\
    \ Sensors 2014, 14, 17753–17769. [CrossRef] [PubMed]\n22.\nKruse, O.M.O.; Prats-Montalbán,\
    \ J.M.; Indahl, U.G.; Kvaal, K.; Ferrer, A.; Futsaether, C.M. Pixel classiﬁcation\
    \ methods for\nidentifying and quantifying leaf surface injury from digital images.\
    \ Comput. Electron. Agric. 2014, 108, 155–165. [CrossRef]\n23.\nShakoor, N.; Lee,\
    \ S.; Mockler, T.C. High throughput phenotyping to accelerate crop breeding and\
    \ monitoring of diseases in the\nﬁeld. Curr. Opin. Plant Biol. 2017, 38, 184–192.\
    \ [CrossRef] [PubMed]\n24.\nLecun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature\
    \ 2015, 521, 436–444. [CrossRef]\n25.\nHardin, P.J.; Lulla, V.; Jensen, R.R.;\
    \ Jensen, J.R. Small Unmanned Aerial Systems (sUAS) for environmental remote sensing:\n\
    Challenges and opportunities revisited. GIScience Remote Sens. 2019, 56, 309–322.\
    \ [CrossRef]\n26.\nMookerjee, M.; Vieira, D.; Chan, M.A.; Gil, Y.; Goodwin, C.;\
    \ Shipley, T.F.; Tikoff, B. We need to talk: Facilitating communication\nbetween\
    \ ﬁeld-based geoscience and cyberinfrastructure communities. GSA Today 2015, 34–35.\
    \ [CrossRef]\n27.\nStewart, C.A.; Simms, S.; Plale, B.; Link, M.; Hancock, D.Y.;\
    \ Fox, G.C. What is cyberinfrastructure? In Proceedings of the\nProceedings of\
    \ the 38th Annual ACM SIGUCCS Fall Conference: Navigation and Discovery, Norfolk,\
    \ VA, USA, 24–27 October\n2010; pp. 37–44. [CrossRef]\n28.\nMadhavan, K.; Elmqvist,\
    \ N.; Vorvoreanu, M.; Chen, X.; Wong, Y.; Xian, H.; Dong, Z.; Johri, A. DIA2:\
    \ Web-based cyberinfrastructure\nfor visual analysis of funding portfolios. IEEE\
    \ Trans. Vis. Comput. Graph. 2014, 20, 1823–1832. [CrossRef]\n29.\nGoff, S.A.;\
    \ Vaughn, M.; McKay, S.; Lyons, E.; Stapleton, A.E.; Gessler, D.; Matasci, N.;\
    \ Wang, L.; Hanlon, M.; Lenards, A.; et al.\nThe iPlant collaborative: Cyberinfrastructure\
    \ for plant biology. Front. Plant Sci. 2011, 2, 1–16. [CrossRef]\n30.\nAksulu,\
    \ A.; Wade, M. A comprehensive review and synthesis of open source research. J.\
    \ Assoc. Inf. Syst. 2010, 11, 576–656.\n[CrossRef]\n31.\nFrankenﬁeld, J. Artiﬁcial\
    \ Intelligence (AI). Available online: https://www.investopedia.com/terms/a/artiﬁcial-intelligence-ai.\n\
    asp (accessed on 9 February 2021).\nSensors 2021, 21, 4363\n16 of 19\n32.\nPaschen,\
    \ U.; Pitt, C.; Kietzmann, J. Artiﬁcial intelligence: Building blocks and an innovation\
    \ typology. Bus. Horiz. 2020, 63,\n147–155. [CrossRef]\n33.\nFrey, L.J. Artiﬁcial\
    \ intelligence and integrated genotype–Phenotype identiﬁcation. Genes 2019, 10,\
    \ 18. [CrossRef]\n34.\nZhuang, Y.T.; Wu, F.; Chen, C.; Pan, Y. He Challenges and\
    \ opportunities: From big data to knowledge in AI 2.0. Front. Inf. Technol.\n\
    Electron. Eng. 2017, 18, 3–14. [CrossRef]\n35.\nRoscher, R.; Bohn, B.; Duarte,\
    \ M.F.; Garcke, J. Explainable Machine Learning for Scientiﬁc Insights and Discoveries.\
    \ IEEE Access\n2020, 8, 42200–42216. [CrossRef]\n36.\nSingh, A.; Ganapathysubramanian,\
    \ B.; Singh, A.K.; Sarkar, S. Machine Learning for High-Throughput Stress Phenotyping\
    \ in\nPlants. Trends Plant Sci. 2016, 21, 110–124. [CrossRef]\n37.\nRahaman, M.M.;\
    \ Ahsan, M.A.; Chen, M. Data-Mining Techniques for Image-based Plant Phenotypic\
    \ Traits Identiﬁcation and\nClassiﬁcation. Sci. Rep. 2019, 9, 1–11. [CrossRef]\
    \ [PubMed]\n38.\nHuang, K.Y. Application of artiﬁcial neural network for detecting\
    \ Phalaenopsis seedling diseases using color and texture features.\nComput. Electron.\
    \ Agric. 2007, 57, 3–11. [CrossRef]\n39.\nWetterich, C.B.; Kumar, R.; Sankaran,\
    \ S.; Belasque, J.; Ehsani, R.; Marcassa, L.G. A comparative study on application\
    \ of computer\nvision and ﬂuorescence imaging spectroscopy for detection of citrus\
    \ huanglongbing disease in USA and Brazil. Opt. InfoBase\nConf. Pap. 2013, 2013.\
    \ [CrossRef]\n40.\nSommer, C.; Gerlich, D.W. Machine learning in cell biology-teaching\
    \ computers to recognize phenotypes. J. Cell Sci. 2013, 126,\n5529–5539. [CrossRef]\n\
    41.\nSadeghi-Tehran, P.; Sabermanesh, K.; Virlet, N.; Hawkesford, M.J. Automated\
    \ method to determine two critical growth stages of\nwheat: Heading and ﬂowering.\
    \ Front. Plant Sci. 2017, 8, 1–14. [CrossRef]\n42.\nBrichet, N.; Fournier, C.;\
    \ Turc, O.; Strauss, O.; Artzet, S.; Pradal, C.; Welcker, C.; Tardieu, F.; Cabrera-Bosquet,\
    \ L. A robot-assisted\nimaging pipeline for tracking the growths of maize ear\
    \ and silks in a high-throughput phenotyping platform. Plant Methods 2017,\n13,\
    \ 1–12. [CrossRef]\n43.\nWilf, P.; Zhang, S.; Chikkerur, S.; Little, S.A.; Wing,\
    \ S.L.; Serre, T. Computer vision cracks the leaf code. Proc. Natl. Acad. Sci.\
    \ USA\n2016, 113, 3305–3310. [CrossRef]\n44.\nSabanci, K.; Toktas, A.; Kayabasi,\
    \ A. Grain classiﬁer with computer vision usingadaptive neuro-fuzzy inference\
    \ system.pdf. J. Sci.\nFood Agric. 2017, 97, 3994–4000. [CrossRef]\n45.\nSabanci,\
    \ K.; Kayabasi, A.; Toktas, A. Computer vision-based method for classiﬁcation\
    \ of wheat grains using artiﬁcial neural\nnetwork. J. Sci. Food Agric. 2017, 97,\
    \ 2588–2593. [CrossRef] [PubMed]\n46.\nLin, P.; Li, X.L.; Chen, Y.M.; He, Y. A\
    \ Deep Convolutional Neural Network Architecture for Boosting Image Discrimination\n\
    Accuracy of Rice Species. Food Bioprocess Technol. 2018, 11, 765–773. [CrossRef]\n\
    47.\nSingh, A.K.; Ganapathysubramanian, B.; Sarkar, S.; Singh, A. Deep Learning\
    \ for Plant Stress Phenotyping: Trends and Future\nPerspectives. Trends Plant\
    \ Sci. 2018, 23, 883–898. [CrossRef]\n48.\nPound, M.P.; Atkinson, J.A.; Townsend,\
    \ A.J.; Wilson, M.H.; Grifﬁths, M.; Jackson, A.S.; Bulat, A.; Tzimiropoulos, G.;\
    \ Wells, D.M.;\nMurchie, E.H.; et al. Deep machine learning provides state-of-the-art\
    \ performance in image-based plant phenotyping. GigaScience\n2017, 6, 1–10. [CrossRef]\
    \ [PubMed]\n49.\nFuentes, A.; Yoon, S.; Kim, S.C.; Park, D.S. A robust deep-learning-based\
    \ detector for real-time tomato plant diseases and pests\nrecognition. Sensors\
    \ 2017, 17, 2022. [CrossRef] [PubMed]\n50.\nAbdalla, A.; Cen, H.; Wan, L.; Rashid,\
    \ R.; Weng, H.; Zhou, W.; He, Y. Fine-tuning convolutional neural network with\
    \ transfer\nlearning for semantic segmentation of ground-level oilseed rape images\
    \ in a ﬁeld with high weed pressure. Comput. Electron.\nAgric. 2019, 167, 105091.\
    \ [CrossRef]\n51.\nEspejo-Garcia, B.; Mylonas, N.; Athanasakos, L.; Vali, E.;\
    \ Fountas, S. Combining generative adversarial networks and agricultural\ntransfer\
    \ learning for weeds identiﬁcation. Biosyst. Eng. 2021, 204, 79–89. [CrossRef]\n\
    52.\nBarbedo, J.G.A. Impact of dataset size and variety on the effectiveness of\
    \ deep learning and transfer learning for plant disease\nclassiﬁcation. Comput.\
    \ Electron. Agric. 2018, 153, 46–53. [CrossRef]\n53.\nWang, G.; Sun, Y.; Wang,\
    \ J. Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning.\
    \ Comput. Intell.\nNeurosci. 2017, 2017. [CrossRef]\n54.\nBuzzy, M.; Thesma, V.;\
    \ Davoodi, M.; Velni, J.M. Real-time plant leaf counting using deep object detection\
    \ networks. Sensors 2020,\n20, 6896. [CrossRef]\n55.\nGhosal, S.; Zheng, B.; Chapman,\
    \ S.C.; Potgieter, A.B.; Jordan, D.R.; Wang, X.; Singh, A.K.; Singh, A.; Hirafuji,\
    \ M.; Ninomiya, S.;\net al. A Weakly Supervised Deep Learning Framework for Sorghum\
    \ Head Detection and Counting. Plant Phenomics 2019, 2019,\n1–14. [CrossRef] [PubMed]\n\
    56.\nAich, S.; Stavness, I. Leaf counting with deep convolutional and deconvolutional\
    \ networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision\
    \ (Workshops), Venice, Italy, 22–29 October 2017; pp. 2080–2089. [CrossRef]\n\
    57.\nWang, X.; Xuan, H.; Evers, B.; Shrestha, S.; Pless, R.; Poland, J. High-throughput\
    \ phenotyping with deep learning gives insight\ninto the genetic architecture\
    \ of ﬂowering time in wheat. GigaScience 2019, 8, 1–11. [CrossRef]\n58.\nGhosal,\
    \ S.; Blystone, D.; Singh, A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar,\
    \ S. An explainable deep machine vision\nframework for plant stress phenotyping.\
    \ Proc. Natl. Acad. Sci. USA 2018, 115, 4613–4618. [CrossRef]\nSensors 2021, 21,\
    \ 4363\n17 of 19\n59.\nChaerle, L.; Van Der Straeten, D. Seeing is believing:\
    \ Imaging techniques to monitor plant health. Biochim. Biophys. Acta Gene\nStruct.\
    \ Expr. 2001, 1519, 153–166. [CrossRef]\n60.\nPerez-Sanz, F.; Navarro, P.J.; Egea-Cortines,\
    \ M. Plant phenomics: An overview of image acquisition technologies and image\
    \ data\nanalysis algorithms. GigaScience 2017, 6, 1–18. [CrossRef]\n61.\nCen,\
    \ H.; Weng, H.; Yao, J.; He, M.; Lv, J.; Hua, S.; Li, H.; He, Y. Chlorophyll ﬂuorescence\
    \ imaging uncovers photosynthetic\nﬁngerprint of citrus Huanglongbing. Front.\
    \ Plant Sci. 2017, 8, 1–11. [CrossRef]\n62.\nLichtenthaler, H.K.; Langsdorf, G.;\
    \ Lenk, S.; Buschmann, C. Chlorophyll ﬂuorescence imaging of photosynthetic activity\
    \ with the\nﬂash-lamp ﬂuorescence imaging system. Photosynthetica 2005, 43, 355–369.\
    \ [CrossRef]\n63.\nEhlert, B.; Hincha, D.K. Chlorophyll ﬂuorescence imaging accurately\
    \ quantiﬁes freezing damage and cold acclimation responses\nin Arabidopsis leaves.\
    \ Plant Methods 2008, 4, 1–7. [CrossRef] [PubMed]\n64.\nZheng, H.; Zhou, X.; He,\
    \ J.; Yao, X.; Cheng, T.; Zhu, Y.; Cao, W.; Tian, Y. Early season detection of\
    \ rice plants using RGB, NIR-G-B\nand multispectral images from unmanned aerial\
    \ vehicle (UAV). Comput. Electron. Agric. 2020, 169, 105223. [CrossRef]\n65.\n\
    Padmavathi, K.; Thangadurai, K. Implementation of RGB and grayscale images in\
    \ plant leaves disease detection—Comparative\nstudy. Indian J. Sci. Technol. 2016,\
    \ 9, 4–9. [CrossRef]\n66.\nWang, X.; Yang, W.; Wheaton, A.; Cooley, N.; Moran,\
    \ B. Automated canopy temperature estimation via infrared thermography: A\nﬁrst\
    \ step towards automated plant water stress monitoring. Comput. Electron. Agric.\
    \ 2010, 73, 74–83. [CrossRef]\n67.\nMunns, R.; James, R.A.; Sirault, X.R.R.; Furbank,\
    \ R.T.; Jones, H.G. New phenotyping methods for screening wheat and barley for\n\
    beneﬁcial responses to water deﬁcit. J. Exp. Bot. 2010, 61, 3499–3507. [CrossRef]\
    \ [PubMed]\n68.\nUrrestarazu, M. Infrared thermography used to diagnose the effects\
    \ of salinity in a soilless culture. Quant. InfraRed Thermogr. J.\n2013, 10, 1–8.\
    \ [CrossRef]\n69.\nFittschen, U.E.A.; Kunz, H.H.; Höhner, R.; Tyssebotn, I.M.B.;\
    \ Fittschen, A. A new micro X-ray ﬂuorescence spectrometer for\nin vivo elemental\
    \ analysis in plants. X-ray Spectrom. 2017, 46, 374–381. [CrossRef]\n70.\nChow,\
    \ T.H.; Tan, K.M.; Ng, B.K.; Razul, S.G.; Tay, C.M.; Chia, T.F.; Poh, W.T. Diagnosis\
    \ of virus infection in orchid plants with\nhigh-resolution optical coherence\
    \ tomography. J. Biomed. Opt. 2009, 14, 014006. [CrossRef]\n71.\nGarbout, A.;\
    \ Munkholm, L.J.; Hansen, S.B.; Petersen, B.M.; Munk, O.L.; Pajor, R. The use\
    \ of PET/CT scanning technique for 3D\nvisualization and quantiﬁcation of real-time\
    \ soil/plant interactions. Plant Soil 2012, 352, 113–127. [CrossRef]\n72.\nAˇc,\
    \ A.; Malenovský, Z.; Hanuš, J.; Tomášková, I.; Urban, O.; Marek, M.V. Near-distance\
    \ imaging spectroscopy investigating\nchlorophyll ﬂuorescence and photosynthetic\
    \ activity of grassland in the daily course. Funct. Plant Biol. 2009, 36, 1006–1015.\n\
    [CrossRef]\n73.\nVigneau, N.; Ecarnot, M.; Rabatel, G.; Roumet, P. Potential of\
    \ ﬁeld hyperspectral imaging as a non destructive method to assess\nleaf nitrogen\
    \ content in Wheat. Field Crops Res. 2011, 122, 25–31. [CrossRef]\n74.\nBehmann,\
    \ J.; Steinrücken, J.; Plümer, L. Detection of early plant stress responses in\
    \ hyperspectral images. ISPRS J. Photogramm.\nRemote Sens. 2014, 93, 98–111. [CrossRef]\n\
    75.\nPrey, L.; von Bloh, M.; Schmidhalter, U. Evaluating RGB imaging and multispectral\
    \ active and hyperspectral passive sensing for\nassessing early plant vigor in\
    \ winter wheat. Sensors 2018, 18, 2931. [CrossRef]\n76.\nLi, L.; Zhang, Q.; Huang,\
    \ D. A review of imaging techniques for plant phenotyping. Sensors 2014, 14, 20078–20111.\
    \ [CrossRef]\n77.\nHan, X.F.; Laga, H.; Bennamoun, M. Image-based 3D Object Reconstruction:\
    \ State-of-the-Art and Trends in the Deep Learning\nEra. IEEE Trans. Pattern Anal.\
    \ Mach. Intell. 2019, 43, 1578–1604. [CrossRef] [PubMed]\n78.\nNguyen, C.V.; Fripp,\
    \ J.; Lovell, D.R.; Furbank, R.; Kuffner, P.; Daily, H.; Sirault, X. 3D scanning\
    \ system for automatic high-\nresolution plant phenotyping. In Proceedings of\
    \ the 2016 International Conference on Digital Image Computing: Techniques and\n\
    Applications (DICTA), Gold Coast, Australia, 30 November–2 December 2016.\n79.\n\
    Matovic, M.D. Biomass: Detection, Production and Usage; BoD—Books on Demand: Norderstedt,\
    \ Germany, 2011; ISBN 9533074922.\n80.\nLiu, H.; Bruning, B.; Garnett, T.; Berger,\
    \ B. Hyperspectral imaging and 3D technologies for plant phenotyping: From satellite\
    \ to\nclose-range sensing. Comput. Electron. Agric. 2020, 175, 105621. [CrossRef]\n\
    81.\nZhu, H.; Chu, B.; Fan, Y.; Tao, X.; Yin, W.; He, Y. Hyperspectral Imaging\
    \ for Predicting the Internal Quality of Kiwifruits Based on\nVariable Selection\
    \ Algorithms and Chemometric Models. Sci. Rep. 2017, 7, 1–13. [CrossRef] [PubMed]\n\
    82.\nZhang, M.; Li, G. Visual detection of apple bruises using AdaBoost algorithm\
    \ and hyperspectral imaging. Int. J. Food Prop. 2018,\n21, 1598–1607. [CrossRef]\n\
    83.\nGu, Q.; Sheng, L.; Zhang, T.; Lu, Y.; Zhang, Z.; Zheng, K.; Hu, H.; Zhou,\
    \ H. Early detection of tomato spotted wilt virus infection\nin tobacco using\
    \ the hyperspectral imaging technique and machine learning algorithms. Comput.\
    \ Electron. Agric. 2019, 167,\n105066. [CrossRef]\n84.\nRamesh, V. A Review on\
    \ the Application of Deep Learning in Thermography. Int. J. Eng. Manag. Res. 2017,\
    \ 7, 489–493.\n85.\nPineda, M.; Barón, M.; Pérez-Bueno, M.L. Thermal imaging for\
    \ plant stress detection and phenotyping. Remote Sens. 2021, 13, 68.\n[CrossRef]\n\
    86.\nMessina, G.; Modica, G. Applications of UAV thermal imagery in precision\
    \ agriculture: State of the art and future research\noutlook. Remote Sens. 2020,\
    \ 12, 1491. [CrossRef]\n87.\nMaes, W.H.; Huete, A.R.; Steppe, K. Optimizing the\
    \ processing of UAV-based thermal imagery. Remote Sens. 2017, 9, 476.\n[CrossRef]\n\
    Sensors 2021, 21, 4363\n18 of 19\n88.\nBang, H.T.; Park, S.; Jeon, H. Defect identiﬁcation\
    \ in composite materials via thermography and deep learning techniques. Compos.\n\
    Struct. 2020, 246, 112405. [CrossRef]\n89.\nMoshou, D.; Bravo, C.; West, J.; Wahlen,\
    \ S.; McCartney, A.; Ramon, H. Automatic detection of “yellow rust” in wheat using\n\
    reﬂectance measurements and neural networks. Comput. Electron. Agric. 2004, 44,\
    \ 173–188. [CrossRef]\n90.\nFlavel, R.J.; Guppy, C.N.; Tighe, M.; Watt, M.; McNeill,\
    \ A.; Young, I.M. Non-destructive quantiﬁcation of cereal roots in soil using\n\
    high-resolution X-ray tomography. J. Exp. Bot. 2012, 63, 2503–2511. [CrossRef]\n\
    91.\nGregory, P.J.; Hutchison, D.J.; Read, D.B.; Jenneson, P.M.; Gilboy, W.B.;\
    \ Morton, E.J. Non-invasive imaging of roots with high\nresolution X-ray micro-tomography.\
    \ Plant Soil 2003, 255, 351–359. [CrossRef]\n92.\nYang, W.; Xu, X.; Duan, L.;\
    \ Luo, Q.; Chen, S.; Zeng, S.; Liu, Q. High-throughput measurement of rice tillers\
    \ using a conveyor\nequipped with X-ray computed tomography. Rev. Sci. Instrum.\
    \ 2011, 82, 1–8. [CrossRef]\n93.\nAtkinson, J.A.; Pound, M.P.; Bennett, M.J.;\
    \ Wells, D.M. Uncovering the hidden half of plants using new advances in root\n\
    phenotyping. Curr. Opin. Biotechnol. 2019, 55, 1–8. [CrossRef]\n94.\nShi, F.;\
    \ Wang, J.; Shi, J.; Wu, Z.; Wang, Q.; Tang, Z.; He, K.; Shi, Y.; Shen, D. Review\
    \ of artiﬁcial intelligence techniques in imaging\ndata acquisition, segmentation\
    \ and diagnosis for COVID-19. IEEE Rev. Biomed. Eng. 2020, 14, 4–15. [CrossRef]\n\
    95.\nAtkins, D.E.; Droegemeier, K.K.; Feldman, S.I.; García Molina, H.; Klein,\
    \ M.L.; Messerschmitt, D.G.; Messina, P.; Ostriker, J.P.;\nWright, M.H.; Garcia-molina,\
    \ H.; et al. Revolutionizing Science and Engineering through Cyberinfrastructure.\
    \ Science 2003, 84.\n96.\nLee, C.P.; Dourish, P.; Mark, G. The human infrastructure\
    \ of cyberinfrastructure. In Proceedings of the 2006 20th Anniversary\nConference\
    \ on Computer Supported Cooperative Work, Banff, AB, Canada, 4–8 November 2006;\
    \ pp. 483–492. [CrossRef]\n97.\nUIC Advanced Cyberinfrastructure for Education\
    \ and Research. Available online: https://acer.uic.edu/get-started/resource-\n\
    pricing/ (accessed on 4 September 2020).\n98.\nYang, C.; Raskin, R.; Goodchild,\
    \ M.; Gahegan, M. Geospatial Cyberinfrastructure: Past, present and future. Comput.\
    \ Environ.\nUrban Syst. 2010, 34, 264–277. [CrossRef]\n99.\nMichener, W.K.; Allard,\
    \ S.; Budden, A.; Cook, R.B.; Douglass, K.; Frame, M.; Kelling, S.; Koskela, R.;\
    \ Tenopir, C.; Vieglais, D.A.\nParticipatory design of DataONE-Enabling cyberinfrastructure\
    \ for the biological and environmental sciences. Ecol. Inform. 2012,\n11, 5–15.\
    \ [CrossRef]\n100. Wang, L.; Chen, D.; Hu, Y.; Ma, Y.; Wang, J. Towards enabling\
    \ Cyberinfrastructure as a Service in Clouds. Comput. Electr. Eng.\n2013, 39,\
    \ 3–14. [CrossRef]\n101. Kvilekval, K.; Fedorov, D.; Obara, B.; Singh, A.; Manjunath,\
    \ B.S. Bisque: A platform for bioimage analysis and management.\nBioinformatics\
    \ 2009, 26, 544–552. [CrossRef] [PubMed]\n102. Shah, S.K. Motivation, governance,\
    \ and the viability of hybrid forms in open source software development. Manag.\
    \ Sci. 2006, 52,\n1000–1014. [CrossRef]\n103. Olson, D.L.; Rosacker, K. Crowdsourcing\
    \ and open source software participation. Serv. Bus. 2013, 7, 499–511. [CrossRef]\n\
    104. Bauckhage, C.; Kersting, K. Data Mining and Pattern Recognition in Agriculture.\
    \ KI Künstl. Intell. 2013, 27, 313–324. [CrossRef]\n105. Kuhlgert, S.; Austic,\
    \ G.; Zegarac, R.; Osei-Bonsu, I.; Hoh, D.; Chilvers, M.I.; Roth, M.G.; Bi, K.;\
    \ TerAvest, D.; Weebadde, P.; et al.\nMultispeQ Beta: A tool for large-scale plant\
    \ phenotyping connected to the open photosynQ network. R. Soc. Open Sci. 2016,\
    \ 3.\n[CrossRef] [PubMed]\n106. Gehan, M.A.; Fahlgren, N.; Abbasi, A.; Berry,\
    \ J.C.; Callen, S.T.; Chavez, L.; Doust, A.N.; Feldman, M.J.; Gilbert, K.B.; Hodge,\
    \ J.G.;\net al. PlantCV v2: Image analysis software for high-throughput plant\
    \ phenotyping. PeerJ 2017, 2017, 1–23. [CrossRef] [PubMed]\n107. Tzutalin LabelImg.\
    \ Available online: https://github.com/tzutalin/labelImg (accessed on 14 September\
    \ 2020).\n108. Ubbens, J.R.; Stavness, I. Deep plant phenomics: A deep learning\
    \ platform for complex plant phenotyping tasks. Front. Plant Sci.\n2017, 8. [CrossRef]\n\
    109. Howard, A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.;\
    \ Andreetto, M.; Adam, H. MobileNets: Efﬁcient\nConvolutional Neural Networks\
    \ for Mobile Vision Applications. arXiv 2017, arXiv:1704.04861.\n110. Sandler,\
    \ M.; Howard, A.; Zhu, M.; Zhmoginov, A.; Chen, L.C. MobileNetV2: Inverted Residuals\
    \ and Linear Bottlenecks. In\nProceedings of the 2018 IEEE/CVF Conference on Computer\
    \ Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–23 June\n2018; pp.\
    \ 4510–4520. [CrossRef]\n111. Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro,\
    \ C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; et al.\nTensorFlow:\
    \ Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv 2015,\
    \ arXiv:1603.04467.\n112. Ramcharan, A.; McCloskey, P.; Baranowski, K.; Mbilinyi,\
    \ N.; Mrisho, L.; Ndalahwa, M.; Legg, J.; Hughes, D.P. A mobile-based\ndeep learning\
    \ model for cassava disease diagnosis. Front. Plant Sci. 2019, 10, 1–8. [CrossRef]\n\
    113. Merz, T.; Chapman, S. Autonomous Unmanned Helicopter System for Remote Sensing\
    \ Missions in Unknown Environments.\nISPRS Int. Arch. Photogramm. Remote Sens.\
    \ Spat. Inf. Sci. 2012, XXXVIII-1, 143–148. [CrossRef]\n114. Andrade-Sanchez,\
    \ P.; Gore, M.A.; Heun, J.T.; Thorp, K.R.; Carmo-Silva, A.E.; French, A.N.; Salvucci,\
    \ M.E.; White, J.W. Devel-\nopment and evaluation of a ﬁeld-based high-throughput\
    \ phenotyping platform. Funct. Plant Biol. 2014, 41, 68–79. [CrossRef]\n[PubMed]\n\
    115. Chawade, A.; Van Ham, J.; Blomquist, H.; Bagge, O.; Alexandersson, E.; Ortiz,\
    \ R. High-throughput ﬁeld-phenotyping tools for\nplant breeding and precision\
    \ agriculture. Agronomy 2019, 9, 258. [CrossRef]\nSensors 2021, 21, 4363\n19 of\
    \ 19\n116. Virlet, N.; Sabermanesh, K.; Sadeghi-Tehran, P.; Hawkesford, M.J. Field\
    \ Scanalyzer: An automated robotic ﬁeld phenotyping\nplatform for detailed crop\
    \ monitoring. Funct. Plant Biol. 2017, 44, 143–153. [CrossRef]\n117. IPPN International\
    \ Plant Phenotyping Network.\nAvailable online: https://www.plant-phenotyping.org/\
    \ (accessed on\n13 April 2020).\n118. APPF Australian Plant Phenomics Facility.\
    \ Available online: https://www.plantphenomics.org.au/ (accessed on 13 April 2020).\n\
    119. Cooper, C.B.; Shirk, J.; Zuckerberg, B. The Invisible Prevalence of Citizen\
    \ Science in Global Research: Migratory The Invisible\nPrevalence of Citizen Science\
    \ in Global Research: Migratory Birds and Climate Change. PLoS ONE 2014, 9, e106508.\
    \ [CrossRef]\n[PubMed]\n"
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  pdf_link: https://www.mdpi.com/1424-8220/21/13/4363/pdf?version=1624868958
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Review: Application of Artificial Intelligence in Phenomics'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tpami.2018.2827049
  analysis: '>'
  authors:
  - Kunfu Zhu
  - Yujia Xue
  - Qiang Fu
  - Sing Bing Kang
  - Xilin Chen
  - Jingyi Yu
  citation_count: 19
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading web-font TeX/Size2/Regular
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Pattern
    ... >Volume: 41 Issue: 5 Hyperspectral Light Field Stereo Matching Publisher:
    IEEE Cite This PDF Kang Zhu; Yujia Xue; Qiang Fu; Sing Bing Kang; Xilin Chen;
    Jingyi Yu All Authors 21 Cites in Papers 1091 Full Text Views Abstract Document
    Sections 1 Introduction 2 Related Work 3 Hyperspectral Light Field Imager (HLFI)
    4 Two-View Spectral-Aware Matching 5 H-LF Stereo Matching Scheme Show Full Outline
    Authors Figures References Citations Keywords Metrics Footnotes Abstract: In this
    paper, we describe how scene depth can be extracted using a hyperspectral light
    field capture (H-LF) system. Our H-LF system consists of a 5×6 array of cameras,
    with each camera sampling a different narrow band in the visible spectrum. There
    are two parts to extracting scene depth. The first part is our novel cross-spectral
    pairwise matching technique, which involves a new spectral-invariant feature descriptor
    and its companion matching metric we call bidirectional weighted normalized cross
    correlation (BWNCC). The second part, namely, H-LF stereo matching, uses a combination
    of spectral-dependent correspondence and defocus cues. These two new cost terms
    are integrated into a Markov Random Field (MRF) for disparity estimation. Experiments
    on synthetic and real H-LF data show that our approach can produce high-quality
    disparity maps. We also show that these results can be used to produce the complete
    plenoptic cube in addition to synthesizing all-focus and defocused color images
    under different sensor spectral responses. Published in: IEEE Transactions on
    Pattern Analysis and Machine Intelligence ( Volume: 41, Issue: 5, 01 May 2019)
    Page(s): 1131 - 1143 Date of Publication: 16 April 2018 ISSN Information: PubMed
    ID: 29993926 DOI: 10.1109/TPAMI.2018.2827049 Publisher: IEEE Funding Agency: SECTION
    1 Introduction The availability of commodity light field (LF) cameras such as
    Lytro [1] and Raytrix [2] makes it easy to capture light fields. Dense stereo
    matching solutions have exploited unique properties, e.g., spatial and angular
    coherence [3], ray geometric constraints [4], [5], [6], focal symmetry [7], and
    defocus blurs [8]. In addition to 3D reconstruction, LF stereo matching can also
    address traditionally challenging problems, e.g., transparent object reconstruction
    [9], saliency detection [10] and scene classification [11]. Since an LF consists
    of densely sampled rays within a specific range of location and angle, it can
    be thought of as representing geometry and surface reflectance as well. However,
    the original plenoptic function [12] includes an additional dimension of spectra,
    which has been largely ignored in most previous LF systems. What is required is
    hyperspectral imaging, which refers to the dense spectral sampling of a scene,
    as opposed to the regular RGB three-band sampling for color cameras. Holloway
    et al. [13] acquire a multispectral1 LF using generalized assorted camera arrays.
    More recently, Xiong et al. [14] adopts a hybrid sensing technique that combines
    an LF with a hyperspectral camera. These solutions assume small camera baselines
    for reliable image registration. In contrast, we present a wide baseline hyperspectral
    light field (H-LF) imaging technique based on novel cross-spectral LF stereo matching.
    Direct adoption of existing LF stereo matching solutions (e.g., [4], [7], [8],
    [15], [16], [17]) for H-LF would be ineffective, since images at different spectral
    bands may be very visually different. Instead, we introduce a new spectral-invariant
    feature descriptor and its companion matching metric (which we call bidirectional
    weighted normalized cross correlation or BWNCC). BWNCC measures gradient inconsistencies
    between cross-spectral images; it significantly outperforms other state-of-the-art
    metrics (e.g., sum of squared differences (SSD) [18], normalized cross correlation
    (NCC) [19], histogram of oriented gradient (HOG) [20], scale-invariant feature
    transform (SIFT) [21]), in both robustness and accuracy. Our spectral-dependent
    H-LF stereo matching technique combines correspondence and defocus cues that are
    based on BWNCC. For visual coherency, we calculate the correspondence cost using
    local subsets of views, since views that are farther away may be less reliable.
    The entire H-LF is used to compute our new spectral-aware defocus cost. Previous
    approaches use color or intensity variance to measure focusness. However, for
    H-LF, the same 3D point will map to different intensities; as a result, such variance
    measures would be unreliable. We instead synthesize the RGB color from H-LF samples,
    then use the CIE Color Space to map the estimated hue of color to its spectral
    band. Consistency is then measured using the actual captured band as the focusness
    measure. Finally, we integrate the new correspondence and defocus costs with occlusion
    and smoothness terms in an energy function, and solve it as a Markov Random Field
    (MRF). We validate our approach on both synthetic and real H-LFs. To capture real
    H-LFs, we construct an H-LF camera array, with each camera equipped with a different
    narrow 10\;\mathrm{nm} -wide bandpass filter (Fig. 1). The union of all the cameras
    covers the visible spectrum from 410\;\mathrm{nm} to 700\;\mathrm{nm} . The baseline
    is 36\;\mathrm{mm} , which is large enough that parallax for the scenes used would
    be significant. We show our H-LF stereo matching technique can produce high-quality
    disparity maps for both synthetic and real datasets. The disparity maps can be
    used to produce the complete plenoptic cube. These maps can also be used for image
    warping, which allows color image synthesis, hyperspectral refocusing, and emulation
    of different color sensors. Fig. 1. System overview. Our hyperspectral light field
    (H-LF) imager (HLFI, top left) consists of a 5\times 6 array of cameras, each
    with a narrow bandpass filter centered at a specific wavelength. The HLFI samples
    the visible spectrum from 410\;\mathrm{nm} to 700\;\mathrm{nm} with an 10\;\mathrm{nm}
    interval. We propose a new spectral-dependent H-LF stereo matching technique (middle),
    which involves novel correspondence cost (top) and spectral-aware defocus cost
    (bottom). The correspondence cost is based on a new spectral-invariant feature
    descriptor called BWNCC with local view selection. The generated disparity map
    (top right) can be used for complete plenoptic reconstruction (bottom right).
    Show All The contributions of this paper are: We designed a snapshot hyperspectral
    light field imager (HLFI) that samples only a subset of H-LFs, avoiding demosaicking
    artifacts. In principle, our HLFI can be expanded both in the spectral resolution
    (e.g., a 5\;\mathrm{nm} interval or lower) and range (e.g., additional infrared
    and ultraviolet bands). We propose a new spectral-invariant feature descriptor
    to effectively represent the visually-varying spectral images. We also propose
    a matching metric, BWNCC, to measure the similarity of multi-dimensional features.
    This feature descriptor and BWNCC are used in H-LF stereo matching. We propose
    a novel spectral-dependent H-LF stereo matching technique that combines a local
    view selection strategy with spectral-aware defocus. We show that our matching
    technique produces high-quality disparity maps. We show three applications using
    our H-LF results: reconstruction of the complete plenoptic cube, generation of
    all-focus H-LF, and synthesis of defocused color images under different spectral
    profiles. We expect these applications would be useful for 3D reconstruction,
    object detection and identification, and material analysis. The rest of this paper
    is organized as follows. We review related work in feature descriptors, LF stereo
    matching, and multi-spectral imaging in Section 2. Our design of HLFI is described
    in Section 3. Section 4 details our feature descriptor and matching metric; these
    are used in our H-LF stereo matching technique (Section 5). The task of complete
    plenoptic cube reconstruction is described in Section 6. Section 7 presents the
    experimental results and applications, with discussion of limitations in Section
    8 and concluding remarks in Section 9. SECTION 2 Related Work In this section,
    we review relevant approaches in the areas of multispectral imaging, feature descriptors,
    and light field stereo matching. 2.1 Multispectral Imaging Spectral imaging has
    long been driven by the need of high quality remote sensing [22], with applications
    in agriculture, military, astronomy, surveillance, etc. (e.g., [23], [24], [25],
    [26]). The commonly adopted techniques include coupling bandpass filters with
    spatial and temporal multiplexing to acquire both the spatial and spectral information.
    In satellite imaging, spectral-coded pushbroom cameras is capable of acquiring
    the full spectra [27]. Tunable filters (e.g., LCTF, AOTF [28]) provide an alternative
    single camera solution. Such solutions require the camera be fixed under different
    shots and cannot provide scene parallax. More expensive snapshot imaging spectrometry
    involving diffraction grating, dispersing prism, multi-aperture spectral filter,
    Lyot filter or generalized Bayer filter (e.g., [29], [30]), requires extremely
    accurate calibration. Alternative approaches mostly rely on hybrid sensing, i.e.,
    using sensors with different modalities. For example, Xiong et al. [14] combine
    an LF camera with a hyperspectral camera to obtain the angular and spectral dimensions
    to recover the hyperspectral LF. Their approach for band-wise recovery is based
    on correlations across angular and spectral dimensions of the RGB LF data. These
    correlations are extracted through self-learned dictionaries, which only approximate
    the cross-spectral mappings. In our approach, we reconstruct a plenoptic cube
    through hyperspectral stereo matching; in principle, the reconstruction is physically
    correct if matching is perfect. In addition, their built-in LF camera is of low
    resolution (380 \times 380 for single view) with small baselines (1-2 \;\mathrm{mm}
    ), which significantly limits angular information. By comparison, our camera resolution
    is 1292\times 964 , with the nearest baseline being 36 \;\mathrm{mm} . Ye and
    Imai [31] describe a plenoptic multispectral camera whose microlens array has
    a spectrally-coded mask. Using a sparse representation, the spectral samples are
    used to reconstruct high resolution multispectral images. Closely to related to
    our work is that of generalized assorted cameras [13], where a camera array is
    used for multispectral imaging. This system is a custom-built camera array (ProFUSION
    from PTGrey) that is modified by mounting broad band-pass filters. The camera
    baseline is rather small, and the filter being broad band-pass makes it easier
    to correspond images using existing color features. Our system uses a 2D array
    of monochrome cameras with narrow band-pass filters to avoid the demosaicking
    artifacts caused by the de-mulplexing procedure used in [13]. More importantly,
    the camera baselines in our system are significantly larger relative to scene
    depth; this allows more reliable depth estimation and enables synthetic refocusing.
    Finally, our system is extensible: in principle, more cameras can be added to
    increase the synthetic aperture (with wider extents) or spectral sampling resolution
    (with narrower band-pass filters). 2.2 Feature Descriptors Feature descriptors
    (e.g., [32], [13], [18], [19], [33], [34], [35], [36]) play a critical role in
    stereo matching and image registration. SSD (e.g., [32], [18]) is widely used
    in stereo matching as a data cost (e.g., [33]). NCC [19] is a highly popular as
    well for matching contrast-varying images. To take into account local radiometric
    variabilities, adaptive normalized cross correlation (ANCC) [34] is introduced
    for matching. Hirschmuller [35] uses mutual information (MI) with correlation-based
    method to resolve radiometric inconsistencies in images matching. Other matching
    features used include robust selective normalized cross correlation (RSNCC) [36]
    for multi-modal and multispectral image registration, and cross-channel normalized
    gradient (CCNG) [13] for multispectral image registration. However, in cross-spectral
    stereo matching, the crucial problem is the spectral difference. Techniques such
    as [18], [19], [33] do not work well because of the intensity consistency assumption.
    Although radiometric inconsistencies that are handled in [34], [35] are related
    to spectral difference, they have very different properties. Radiometric changes
    (e.g., caused by the varying exposure or lighting) mostly preserve the relative
    ordering of local scene point intensities. In contrast, in multispectral imaging,
    the relative ordering of local intensities can change arbitrarily, including order
    reversal. This is because different materials tend to have different responses
    at different wavelengths. Both [13], [36] are applied to multispectral imaging.
    Unfortunately, in [36], errors occur in regions with uncorrelated textures. Meanwhile,
    [13] describes a technique that operates on broad band-pass RGB color channels;
    it is not expected to handle the single narrow band-pass channel images in our
    H-LF as well using [34]. 2.3 Light Field Stereo Matching Many stereo techniques
    have been proposed [37], including local methods [19], semi-global methods [35],
    and global methods [33]. More recently, these techniques have been adapted for
    LFs. For exmaple, Wanner and Goldlucke[15] extract the direction field in the
    Epipolar Image to estimate disparity. Yu et al. [4] use geometric structures of
    3D lines in ray space to improve depth with encoded line constraints. Tao et al.
    [8] introduce the defocus cue combined with correspondence for depth estimation.
    Chen et al. [16] propose a bilateral consistency metric to handle occluding and
    non-occluding pixels, while Lin et al. [7] make use of the LF focal stack to recover
    depth. Wang et al. [17] handle occlusion through edge detection. Again, these
    solutions cannot be directly applied for H-LF stereo matching; under spectral
    variations, regular data consistency measures (such as focusness) are no longer
    effective. Our spectral-dependent H-LF stereo matching technique addresses the
    cross-spectral inconsistency problem by using a spectral-invariant feature descriptor,
    applying local selection of views, and using spectral-aware defocus cues. We also
    handle occlusion in a manner similar to [17]. SECTION 3 Hyperspectral Light Field
    Imager (HLFI) To simultaneously acquire spatial, angular, and spectral samples
    of the plenoptic function, we build a hyperspectral light field imager. The left
    of Fig. 1 shows our HLFI setup: we use an array of 5 \times 6 monochrome cameras,
    each equipped with a narrow band-pass filter centered at a different wavelength.
    The spectral responses of the filters are shown in Fig. 2. These filters sample
    the visible spectrum, centered from 410\;\mathrm{nm} to 700\;\mathrm{nm} with
    an 10\;\mathrm{nm} interval. The bandwidth of each filter is 10 nm (i.e., \pm
    5\; \mathrm{nm} ) with \pm 2\;\mathrm{nm} uncertainty. Due to the uncertainty,
    neighboring filters have responses that overlap. Fortunately, the response drop-off
    for each narrow band-pass filter is steep. As shown in Fig. 2, the overlaps occur
    below 35 percent quantum efficiency, where drop-off is rapid. We treat each filter
    response as a Dirac delta function F_{\lambda _{i}}(\lambda)=\delta (\lambda -
    \lambda _{i}) , where \lambda _i is the center wavelength. Fig. 2. The spectral
    profile of narrow band-pass filters. In our setup, we mount 30 filters on camera
    array (Fig. 1). These filters sample the visible spectrum, centered from 410\;\mathrm{nm}
    to 700\;\mathrm{nm} with an 10\;\mathrm{nm} interval as this figure shows. Each
    bandwidth is 10\;\mathrm{nm} (i.e., \pm 5\;\mathrm{nm} about the central wavelength)
    with \pm 2\;\mathrm{nm} uncertainty. The overlaps occur near 35 percent quantum
    efficiency with rapid drop-off. Show All To accommodate the extra spectral dimension,
    we modify the two-plane LF representation [1], [38] to L(u, v, s, t, \lambda)
    for the sampled hyperspectral light field (H-LF). (u, v) and (s, t) represent
    the ray intersection with the aperture and sensor planes (respectively) at wavelength
    \lambda . The image I(s,t,\lambda _{i}) on (s,t) corresponding to narrow band-pass
    spectral profile F_{\lambda _{i}}(\lambda) centered at wavelength \lambda _{i}
    is modeled as: \begin{align} \nonumber I(s,t,\lambda _{i})& = \int\int\int L(u,v,s,t,\lambda)A(u,v)
    C(\lambda) \\ & \quad \cdot F_{\lambda _{i}}(\lambda) \cos ^4{\theta } d\lambda
    du dv, \tag{(1)} \end{align} View Source where A(u,v) is the aperture function,
    \theta is incident angle of the ray, and C(\lambda) is the camera spectral response
    function. We ignore \cos ^4{\theta } using the paraxial assumption. Equation (1)
    simplifies to: \begin{align} \nonumber I(s,t,\lambda _{i})&= C(\lambda _{i}) \int\int
    L(u,v,s,t,\lambda _{i})A(u,v)du dv\\ &= C(\lambda _{i})S(s, t, \lambda _{i}),
    \tag{(2)} \end{align} View Source where S(\lambda _{i}) is the latent radiance
    image at spectrum \lambda _{i} while C(\lambda _{i}) is the spectral response
    function. SECTION 4 Two-View Spectral-Aware Matching In this section, we describe
    our new approach to matching two views \mathcal {L} and \mathcal {R} corresponding
    to narrow band spectra centered at two different wavelengths \lambda _L and \lambda
    _R (respectively). 4.1 Spectral-Invariant Feature Descriptor Traditional measures
    for correspondence assume either brightness constancy or preservation of brightness
    ordering. As mentioned earlier, such measures (including direct gradient-based
    measures) fail because cross-spectral images violate these assumptions. Fig. 3
    shows an example from the Middlebury dataset [37]. The red channel of \mathcal
    {L} (Fig. 3a) is markedly different from the blue channel of \mathcal {R} (Fig.
    3b); for example, edge pixels around the lamp exhibit significant inconsistencies
    across the image pair. This demonstrates that we need to devise a new feature
    descriptor for cross-spectral images. Fig. 3. Cross-channel stereo imaging on
    the Tsukuba image pair. (a) and (b): Red channel of \mathcal {L} and blue channel
    of \mathcal {R} , respectively. (c) and (d): respective gradient magnitudes. (e)
    and (f): respective gradient directions. Section 4.1 describes how we match boundary
    (e.g., \mathbf {p}_1 -\mathbf {q}_1 ) and non-boundary (e.g., \mathbf {p}_2 -\mathbf
    {q}_2 ) pixels. The pixels denoted with primes (\mathbf {p}_1^{\prime } , etc.)
    are neighboring pixels. Show All We first eliminate the effect caused by the camera
    spectral response. From Equation (2), for two corresponding pixels \mathbf {p}
    and \mathbf {q} (\mathbf {p},\mathbf {q}\in N^2 ), we have I_L({\mathbf {p}})
    = C(\lambda _L)S_{\mathbf {p}}(\lambda _L) and I_R({\mathbf {q}}) = C(\lambda
    _R)S_{\mathbf {q}}(\lambda _R) . We normalize them to yield: \begin{equation}
    \left\lbrace \begin{array}{ll}\widetilde{I}_L({\mathbf {p}}) &= \frac{I_L({\mathbf
    {p}})}{\bar{I}_L}=\frac{S_{\mathbf {p}}(\lambda _L)}{\bar{S}(\lambda _L)}\\ \widetilde{I}_R({\mathbf
    {q}}) &= \frac{I_R({\mathbf {q}})}{\bar{I}_R}=\frac{S_{\mathbf {q}}(\lambda _R)}{\bar{S}(\lambda
    _R)} \end{array} \right., \tag{(3)} \end{equation} View Source where \bar{I}_L
    and \bar{I}_R are the mean intensities, and \bar{S}(\lambda _L) and \bar{S}(\lambda
    _R) are the average radiances in the corresponding views. For the remainder of
    the paper, we use \widetilde{I}_L({\mathbf {p}}) and \widetilde{I}_R({\mathbf
    {q}}) as inputs, eliminating the effect of the camera spectral response while
    still depending on the spectrum. We exploit the gradient of image as the feature
    descriptor. M(\mathbf {p}) and \Theta (\mathbf {p}) represent the magnitude and
    direction of the gradient at \mathbf {p} , respectively: M({\mathbf {p}}) = \sqrt{\nabla
    _x \widetilde{I}(\mathbf {p})^2 + \nabla _y \widetilde{I}(\mathbf {p})^2} and
    \Theta ({\mathbf {p}}) = \mathbf {atan} (\nabla _y {\widetilde{I}(\mathbf {p})}
    / {\nabla _x \widetilde{I}(\mathbf {p})}) . In Figs. 3c and 3d show the magnitudes
    of gradient for (a) and (b); (e) and (f) shows the directions of gradient for
    (a) and (b) quantized within [0,\pi ] . We consider two cases, based on proximity
    to edges. Case 1: Suppose corresponding pixels \mathbf {p}, \mathbf {q} and their
    respective neighbors \mathbf {p^{\prime }}, \mathbf {q^{\prime }} are all part
    of the same object (e.g., \mathbf {p}_2, \mathbf {q}_2 are adjacent to \mathbf
    {p}^{\prime }_2,\mathbf {q}^{\prime }_2 , respectively, in Fig. 3). Then, |\widetilde{I}_L(\mathbf
    {p})-\widetilde{I}_L(\mathbf {p^{\prime }})| \simeq |\widetilde{I}_R(\mathbf {q})-\widetilde{I}_R(\mathbf
    {q^{\prime }})| , implying that the gradient magnitude and direction should be
    approximately the same, i.e., M_L({\mathbf {p}})\simeq M_R({\mathbf {q}}) and
    \Theta _L({\mathbf {p}})\simeq \Theta _R({\mathbf {q}}) . Case 2: Suppose the
    pixels lie near an edge (e.g., \mathbf {p}_1, \mathbf {q}_1 are adjacent to \mathbf
    {p}^{\prime }_1,\mathbf {q}^{\prime }_1 , respectively, in Fig. 3). The foreground
    and background correspond to objects with different spectral responses and the
    magnitude measure is no longer consistent. However, note that the gradient directions
    should still be similar. We design a feature descriptor that incorporates both
    edge and non-edge features at each pixel. The non-edge features couple the gradient
    magnitude and direction histograms, whereas the edge features are an extension
    of HOG we call Overlapping HOG or O-HOG. Unlike traditional histograms, where
    every bin represent a separate range of values, in O-HOG, adjacent bins have overlapping
    values (i.e., they share some range of values). This is to more robustly handle
    view and spectral variations that exist in cross-spectral matching. By comparison,
    even a slight change in perspective or spectrum may lead to misalignment in regular
    HOG [20]. To find correspondence, we first calculate the gradient magnitude and
    direction histograms (with K_1 and K_2 bins, respectively). Given a local window
    \mathbf {U}({\mathbf {p}},w) \in \mathcal {N}^{w^2 \times 2} centered at \mathbf
    {p} with size w \times w for a stack of magnitude and direction images, we count
    weighted votes for bins in the magnitude histogram \mathbf {h}_1(\mathbf {p},w,K_1)
    and direction histogram \mathbf {h}_2(\mathbf {p},w,K_2) . Specifically, the k
    -th bin b_i^{(k)}(\mathbf {p},w) of \mathbf {h}_i (i = 1, 2 ;k\in [0,K_i -1))
    is aggregated as \begin{equation} b_i^{(k)}(\mathbf {p},w) = \frac{\sum \nolimits
    _{\mathbf {u}_t \in \mathbf {U}(\mathbf {p},w)} G(\mathbf {p},\mathbf {u}_t,\sigma
    _g) f(\mathbf {u}_t)}{\sum \nolimits _{j \in [0, K_i-1] }b_i^{(j)}}, \tag{(4)}
    \end{equation} View Source where G(\mathbf {p},\mathbf {u_t},\sigma _g) = \exp
    ({-||\mathbf {p}-\mathbf {u}_t||^2_2}/{2\sigma _g}^2) is a spatial weight kernel,
    and f(\mathbf {u}_t) is a truncation function defined as \begin{equation} f(\mathbf
    {u}_t) = \left\lbrace \begin{array}{ll}1& \text{if } Q(\mathbf {u}_t) \in [k(1-o)s,k(1-o)s+s)\\
    0& \text{otherwise} \end{array} \right. . \tag{(5)} \end{equation} View Source
    Here o is the overlapping portion between the neighboring bins and s is the bin
    width. For \mathbf {h}_1 , Q(\mathbf {u}_t) = M(\mathbf {u}_t) ; for \mathbf {h}_2
    , Q(\mathbf {u}_t) = \Theta (\mathbf {u}_t) . Similarly, for the O-HOG histogram
    \mathbf {h}_3(\mathbf {p},w,K_3) , the k -th bin b_3^{(k)}(\mathbf {p},w) is computed
    as \begin{equation} b_3^{(k)}(\mathbf {p},w) = \frac{\sum \nolimits _{\mathbf
    {u}_t \in \mathbf {U}(\mathbf {p},w)} G(\mathbf {p},\mathbf {u}_t,\sigma _g) M(\mathbf
    {u}_t) f(\mathbf {u}_t)}{\sum \nolimits _{j \in [0, K_3-1] }b_3^{(j)}} . \tag{(6)}
    \end{equation} View Source Note that for \mathbf {h}_3 , Q(\mathbf {u}_t) = \Theta
    (\mathbf {u}_t) in Equation (5). We set s=1/64 and o=1/16 for both \mathbf {h}_1
    , \mathbf {h}_2 and \mathbf {h}_3 . K_1=K_2=K_3 , all rounded up to 68. For each
    pixel \mathbf {p} , we define descriptor \mathbf {D_p} = \left[ \alpha _1 \mathbf
    {h}_1^T, \alpha _2 \mathbf {h}_2^T, \alpha _3 \mathbf {h}_3^T \right]^T , with
    \alpha _1 , \alpha _2 , and \alpha _3 being weights. Recall that \mathbf {h}_1
    and \mathbf {h}_2 represent non-edge features of \mathbf {p} defined by Equation
    (4), while \mathbf {h}_3 represents the edge feature of \mathbf {p} defined by
    Equation (6). Since M(\mathbf {p}) is the edge strength of \mathbf {p} , we simply
    reuse M(\mathbf {p}) to get \alpha _1=\alpha _2=\beta \exp ({-M^2(\mathbf {p})}/{\sigma
    _w}) and \alpha _3=1-\alpha _1-\alpha _2 . In our work, \beta =1/2 and \sigma
    _w=0.16 . For robustness, we build a 3-level pyramid structure with different
    patch widths \mathbf {w}=[w_1,w_2,w_3]^T to obtain the final descriptor \mathbf
    {H_p} = [ \mathbf {D_p^T}(w_1), \mathbf {D_p^T}(w_2), \mathbf {D_p^T}(w_3) ]^T
    with K elements, where K=3(K_1+K_2+K_3) . In all our experiments, \mathbf {w}=[3,5,9]^T
    . Fig. 4 shows the structure of our feature descriptor. Fig. 4. Our spectral-invariant
    feature descriptor \mathbf {H} is based on weighted histograms for 3-level pyramids
    of the gradient magnitude and direction maps. \mathbf {h}_1 and \mathbf {h}_2
    are the histograms for gradient magnitude and direction, while \mathbf {h}_3 represents
    O-HOG. Show All 4.2 Spectral-Invariant Similarity Metric A popular similarity
    metric for stereo matching is the normalized cross correlation (NCC) [19]: \begin{align}
    \xi (I) = \frac{\sum \nolimits _{\mbox{$\begin{array}{c}\mathbf {u}_i \in \mathbf
    {U}_{L}\\ \mathbf {u}_j\in \mathbf {U}_{R}\end{array}$}} (I_L(\mathbf {u}_i)-\bar{I}_L)(I_R(\mathbf
    {u}_j)-\bar{I}_R) }{ \sqrt{ \sum \nolimits _{\mathbf {u}_i \in \mathbf {U}_{L}}
    (I_L(\mathbf {u}_i)-\bar{I}_L)^2 \sum \nolimits _{\mathbf {u}_j \in \mathbf {U}_{R}}
    (I_R(\mathbf {u}_j)-\bar{I}_R)^2 } }, \tag{(7)} \end{align} View Source where
    \bar{I}_L and \bar{I}_R are the mean values of \mathbf {U}_{L}(\mathbf {p},w)
    and \mathbf {U}_{R}(\mathbf {q},w) , respectively, in domain I (e.g., intensity).
    Unfortunately, NCC cannot be directly used to match multi-dimensional features.
    Note that each element h^{(i)} in \mathbf {H} is independent of any other element
    h^{(j)} (j \not=i ), and represents a unique attribute of \mathbf {H} (as shown
    in Fig. 4). We define our similarity metric as \xi (\mathbf {H})=\sum \limits
    _{i=0}^{K-1} \omega _i \xi (h^{(i)}) , where \omega _i is a similarity weight
    of h^{(i)} . In principle, we can simply use h^{(i)} as w_i . In practice, for
    robustness to noise, we use the mean \bar{h}^{(i)} instead of h^{(i)} as weights.
    Since h_{\mathbf {p}}^{(i)} and h_{\mathbf {q}}^{(i)} play equally important roles
    in computing \xi {(\mathbf {H})} , the final metric we use incorporates both,
    leading to the Bidirectional Weighted Normalized Cross Correlation (BWNCC). The
    forward component weighted by \bar{h}_{\mathbf {p}}^{(i)} represents the similarity
    between \mathbf {p} and \mathbf {q} , while the backward component weighted by
    \bar{h}_{\mathbf {q}}^{(i)} represents the similarity between \mathbf {q} and
    \mathbf {p} . BWNCC is thus defined as \begin{equation} \xi _{bwncc}(\mathbf {H})
    = \sqrt{\sum \limits _{i=0}^{K-1} \xi {(h^{(i)})} \bar{h}_{\mathbf {p}}^{(i)}
    \sum \limits _{j=0}^{K-1} \xi {(h^{(j)})} \bar{h}_{\mathbf {q}}^{(j)}} . \tag{(8)}
    \end{equation} View Source SECTION 5 H-LF Stereo Matching Scheme Our new feature
    descriptor and metric enable more reliable feature selection and matching. Compared
    with binocular stereo, LF stereo matching has two different properties: use of
    many views and refocusing. When modeled as a disparity labeling problem, the correspondence
    cost makes use of the multiple views while defocus cost is based on refocusing
    (e.g., [4], [7], [8], [17]). We denote \Omega as all LF views (s,t) and estimate
    the disparity map for the central view (s_o,t_o) . For simplicity, we use I_{\mathbf
    {p}}(s,t) to represent \widetilde{I}(u_{\mathbf {p}},v_{\mathbf {p}},s,t,\lambda
    _{(s,t)}) in Equation (3). The correspondence cost is typically cast as ([4],
    [7], [8], [17]): \begin{equation} C(\mathbf {p},f(\mathbf {p})) \propto \frac{1}{|\Omega
    |} \sum _{(s,t)\in \Omega }|I_{\mathbf {p}}(s,t)-I_{\mathbf {p}}(s_o,t_o)|_2^2
    . \tag{(9)} \end{equation} View Source For H-LF, we find a proper subset \Omega
    ^* (\Omega ^* \subseteq \Omega ) and use that in conjunction with our feature
    descriptor and metric to maximize spectral consistency. The defocus cost in [17]
    is based on the depth-from-defocus formulation for non-occlusion regions: \begin{equation}
    D(\mathbf {p},f(\mathbf {p})) \propto \nabla _{(x,y)}\bar{I}_{\mathbf {p}} . \tag{(10)}
    \end{equation} View Source For occlusion regions the defocus cost is \begin{equation}
    D(\mathbf {p}, f(\mathbf {p}))\propto \frac{1}{|\Omega |}\sum _{(s,t)\in \Omega
    } |I_\mathbf {p}(s,t)-\bar{I}_{\mathbf {p}}|_2^2, \tag{(11)} \end{equation} View
    Source where \bar{I}_{\mathbf {p}}=1/|\Omega |\cdot \sum _{(s,t)\in \Omega } I_{\mathbf
    {p}}(s,t) . However, direct use of the defocus measure in H-LF would fail due
    to spectral variance. We instead propose a new defocus cost based on hue-spectrum
    matching. After extracting two initial disparity maps f^*_c (based on correspondence
    cost) and f^*_d (based on defocus cost), we then impose regularization to generate
    the refined result f^{\dagger } . 5.1 Correspondence Cost Recall that the correspondence
    cost measures similarity of corresponding pixels. For a hypothesized disparity
    f({\mathbf {p}}) , we compute this cost using our spectral-invariant feature descriptor
    and BWNCC metric: \begin{equation} C(\mathbf {p},f(\mathbf {p}))=\frac{1}{|\Omega
    ^*|}\sum _{(s,t)\in \Omega ^*} - \log (\xi _{bwncc}(\mathbf {H})) . \tag{(12)}
    \end{equation} View Source Instead of matching \mathbf {p} in (s_o,t_o) with pixel
    \mathbf {q} across all LF views, we use only a subset of views \Omega ^* that
    share a coherent appearance (response). To do so, we first compute the arithmetic
    mean gradient magnitude over all \mathbf {q} . Next, we determine if the gradient
    magnitude of \mathbf {p} is above or below the mean value. If it is above, then
    it is likely that \mathbf {p} is an edge pixel; we use only pixels \mathbf {q}
    in the H-LF views with a higher gradient magnitude. Similarly, if it is below,
    it is likely that \mathbf {p} is a non-edge point, and we use only the ones with
    lower gradient magnitudes. In addition, we treat occluding and non-occluding pixels
    differently using the technique described in [17] to extract an initial disparity
    map f^*_c based on correspondence cost. If \mathbf {p} is non-occluding, f^*_c(\mathbf
    {p})=\min _f \lbrace C\rbrace . If \mathbf {p} is occluding, we partition \Omega
    ^* into occluder and occluded regions \Omega ^*_1 and \Omega ^*_2 (analogous to
    [17]), then compute C_1 and C_2 using Equation (12). This yields f^*_c (\mathbf
    {p})=\min _f\lbrace C_1,C_2\rbrace . 5.2 Defocus Cost A unique property in LF
    stereo matching is the availability of a synthetic focal stack, synthesized via
    LF rendering. Conceptually, if the disparity hypothesis is correct, the color
    variance over correspondences in all (non-occluding) views should be very small.
    If it is incorrect, the variance would be large, causing aliasing. In [17], the
    defocus cost measures the occlusion and non-occlusion regions separately in terms
    of color consistency. However, the traditional defocus cost cannot be used in
    our work because we cannot measure color consistency under different spectral
    responses. We adapted this cost to be spectral-aware. As Fig. 5 shows, given a
    hypothesized disparity f(\mathbf {p}) , we estimate RGB color of \mathbf {p} for
    a reference camera. To do this, we first form a spectral profile of \mathbf {p}
    as P_{\mathbf {p}}(\lambda) by indexing \lambda _{(s,t)} using I_{\mathbf {p}}(s,t)
    into respective views. Next, we use the spectral profile to synthesize its RGB
    value. In our experiments, we use the spectral response function of the PTGrey
    FL3-U3-20E4C-C camera (reference camera) as \mathbf {P}_{c} (\lambda)=[P_r({\lambda
    }),P_g({\lambda }),P_b({\lambda })]^T and compute RGB values \mathbf {V}=[R,G,B]^T
    by summing P_{\mathbf {p}}(\lambda _{(s,t)}), \mathbf {P}_{c}(\lambda _{(s,t)})
    over the respective bandwidths: \begin{equation} \mathbf {V}=\frac{\sum _{(s,t)\in
    \Omega } P_{\mathbf {p}}(\lambda _{(s,t)}) \mathbf {P}_{c}(\lambda _{(s,t)})}{\mathbf
    {P}_{c}(\lambda _{(s,t)})} . \tag{(13)} \end{equation} View Source Fig. 5. Spectral-aware
    defocus cue. Given a disparity hypothesis, we combine corresponding pixels from
    H-LF to form its spectral profile \mathbf {P}_p(\lambda) . Next, we use the camera
    (PTGrey FL3-U3-20E4C-C) spectral response curves \mathbf {P}_c(\lambda) to map
    this profile to RGB color. We then convert the RGB color to its hypothesized wavelength
    \lambda _r using the CIE 1931 Color Space. Finally, we match the observed profile
    with a Gaussian profile \mathbf {P}_g(\lambda) centered at \lambda _r via K-L
    divergence. Show All Finally, we map the RGB color back to spectra \lambda _r
    by first converting it to hue before using a table to map hue to \lambda _r based
    on CIE 1931 Color Space [39]. If the disparity hypothesis is correct, P_{\mathbf
    {p}}(\lambda) and the final RGB values estimation should be accurate. The captured
    spectra should then approximately form a Gaussian distribution centered at \lambda
    _r , with the probability density function \begin{equation} P_{g}(\lambda)=\frac{1}{\sigma
    _d\sqrt{2\pi }} \cdot \exp {\left(-\frac{(\lambda -\lambda _r)^2}{2\sigma _d^2}
    \right)} . \tag{(14)} \end{equation} View Source In our implementation, we use
    the special case of \lambda _r=550\;\mathrm{nm} (middle of [410\;\mathrm{nm},700\;\mathrm{nm}]
    ) to set \sigma _d=96.5 . This is to ensure that P_g(\lambda) have at least 30
    percent response in overlapping the visible spectrum throughout (especially in
    the corner cases of \lambda = 400\;\mathrm{nm} and \lambda = 700\;\mathrm{nm}
    ). We subsequently normalize P_{\mathbf {p}}(\lambda) to P_{\mathbf {p}}^*(\lambda)=P_{\mathbf
    {p}}(\lambda)/\sum _{(s,t)\in \Omega }P_{\mathbf {p}}(\lambda _{(s,t)}) , and
    measure the Kullback–Leibler divergence [40], [41] from P_{\mathbf {p}}^*(\lambda)
    to P_g(\lambda) . This results in our defocus cost \begin{equation} D(\mathbf
    {p},f(\mathbf {p}))=\sum _{(s,t)\in \Omega } P_g(\lambda _{(s,t)})\log {\frac{P_g(\lambda
    _{(s,t)})}{P_{\mathbf {p}}^*(\lambda _{(s,t)})}} . \tag{(15)} \end{equation} View
    Source Finally, we have f^*_d(\mathbf {p})=\min _f\lbrace D\rbrace . 5.3 Regularization
    The energy function for disparity hypothesis f that is typically used in an MRF
    is ([8], [17]) \begin{equation} E(f)=E_{unary}(f)+E_{binary}(f) . \tag{(16)} \end{equation}
    View Source We adopt the binary term similar to Wang et al. [17] for smoothness
    and to handle occlusion. The major difference is that we use spectral-aware defocus
    cues (described in Section 5.2). Our unary term is defined as \begin{align} \nonumber
    E_{unary}(f)&=\sum _{\mathbf {p}} \gamma _c |C(f(\mathbf {p}))-C(f^*_c(\mathbf
    {p}))|\\ &\quad +|D(f(\mathbf {p}))-D(f^*_d(\mathbf {p}))|, \tag{(17)} \end{align}
    View Source where \gamma _c adjusts the weight between defocus and correspondence
    cost. (Its value is 0.45 for synthetic data and 0.6 for real data.) Using a method
    similar to that of Wang et al. [17], we minimize this function to generate the
    desired disparity map f^{\dagger } . SECTION 6 Plenoptic Cube Completion The results
    of our H-LF stereo matching technique can be used to complete the missing dimensions.
    The direct approach would be to use the disparity map of the central view to warp
    images to propagate the missing information. The problem, however, is that the
    warped images will contain holes due to occlusion. While it is possible to perform
    independent pairwise stereo matching between all views, this approach does not
    fully exploit the properties of LFs. We instead present a technique for cross-spectral
    joint binocular stereo. 6.1 Disparity Initialization We first warp the disparity
    map of the central view, f^{\dagger }_{(s_0,t_0)} (using the technique described
    in Section 5) to individual LF views as their initial disparities: \begin{equation}
    f^*_{(s,t)}(u+d(s-s_0),v+d(t-t_0))=d, \tag{(18)} \end{equation} View Source where
    d=f_{(s_0,t_0)}^{\dagger }(u,v) . At this point, at each view, f_{(s,t)}^*(u,v)
    is an incomplete disparity map. There are pixels with invalid depth due to occlusion,
    being outside the field-of-view of the central view, and mismatches. However,
    regions that are valid can be used to guide and refine correspondences between
    cross-spectral image pairs. 6.2 Disparity Estimate Using our BWNCC metric described
    in Section 4, we extract the disparity map for an image pair using Graph Cuts
    [33]. The energy function in [33] is \begin{equation} E(f)=E_{data}(f)+E_{occlu}(f)+E_{smooth}(f)+E_{unique}(f)
    . \tag{(19)} \end{equation} View Source E_{data}(f) is the data term that calculates
    similarity between corresponding pixels: \begin{equation} E_{data}(f)=\sum _{\mathbf
    {p}}|C(f(\mathbf {p}))-C(f^*(\mathbf {p}))|, \tag{(20)} \end{equation} View Source
    where C(f(\mathbf {p})) is defined in Equation 12 but applied to two views (i.e.,
    left and right or top and bottom). E_{occlu}(f) is the occlusion term to minimize
    the number of occluded pixels while E_{smooth}(f) is the smoothness term that
    favors piecewise constant maps. E_{unique} enforces uniqueness of disparities
    between image pairs. The last three terms are same as those in [33]. The disparity
    maps for all image pairs (with vertical or horizontal neighbors) are computed
    using Graph Cuts [33]. These disparity maps are then merged to produce a single
    one denoted as f_{(s,t)} . 6.3 Disparity Refinement As mentioned in Section 6.1,
    f^*_{(s,t)} has regions of invalid depth. Furthermore, f_{(s,t)} is likely to
    have unreliable depths due to occlusion in neighboring views. Park et al. [42]
    propose an optimization technique using RGB-D images to acquire a high-quality
    depth map. This technique uses confidence weighting in terms of color similarities,
    segmentation, and edge saliency. We use a similar approach to refine f_{(s,t)}
    , with the difference being the confidence weighting is adapted to our single
    channel spectral images. This results in the improved disparity map f^{\dagger
    }_{(s,t)} for each view. 6.4 Image Registration We use f^{\dagger }_{(s,t)} to
    warp images. First, all pixels \mathbf {p} on left (or top) view are mapped to
    \mathbf {q} on right (or bottom) view. We then register all images currently on
    right (or bottom) to left (or top) view. This is iterated for all neighboring
    pairs until the plenoptic cube is completed, i.e., when all the missing spectra
    are propagated across all the views. For an H-LF imager with M \times N views,
    the number of hyperspectral images in the completed plenoptic cube is M \times
    N \times MN , with each view having MN images corresponding to MN different spectra.
    SECTION 7 Experiments and Applications In this section, we report the results
    of our technique and how they compare with competing state-of-the-art. We also
    describe two applications (namely, color sensor emulation and H-LF refocusing)
    that are made possible using the depth information generated using our technique.
    7.1 Experimental Setup Our prototype HLFI consists of a 5 \times 6 monochrome
    camera array (Fig. 1). The cameras are MER-132-30GM from Daheng ImaVision, with
    a resolution of 1292\times 964 ; they are synchronized via GenLock. The lens are
    M0814-MP2 from Computar, with a focal length of 8\;\mathrm{mm} . We mount 30 narrow
    bandpass filters (from Rayan Technology) on cameras centered wavelengths between
    410\;\mathrm{nm} to 700\;\mathrm{nm} at a 10\;\mathrm{nm} interval. Data collection
    and processing are done on a Lenovo ThinkStation P500 with a Intel(R) Xeon(R)
    4-core CPU E5-1630 running at 3.70 GHz. We calibrate our cameras using Zhang''s
    algorithm [43] to extract the intrinsic and extrinsic parameters. Since we use
    a black-and-white checkerboard and all the filters are within the visible spectra,
    the calibration images have sufficient contrast for corner detection. Once the
    cameras are calibrated, the views are rectified to simplify stereo matching. As
    mentioned in Section 4.1, throughout all our experiments, we set s=1/64 , o=1/16
    , and \mathbf {w}=[3,5,9]^T to generate our hierarchical feature descriptor \mathbf
    {H} . 7.2 Validation for Feature Descriptor with BWNCC We compared results of
    pairwise stereo matching using Graph Cuts [33] with SSD [18], NCC [19], and the
    recent RSNCC [36] measures against those for our spectral-invariant descriptor
    with BWNCC measure. We first ran experiments involving synthetic data adapted
    from the Middlebury stereo vision datasets [37]. To emulate spectrum inconsistency,
    we treat the red channel of \mathcal {L} and the blue channel of \mathcal {R}
    as the pseudo cross-spectral pair. Fig. 6 compares the visual quality of the results
    using different methods and Table 1 shows the quantitative comparisons in terms
    of bad5.0 (percentage of “bad” pixels whose error is greater than 5 pixels [37]).
    Our approach significantly reduces error in stereo matching. Fig. 6. Cross-channel
    stereo matching results for three Middlebury datasets. From left to right: red
    channel of the left image, blue channel of right image, ground truth disparity
    map, estimated disparity map by SSD, NCC, RSNCC methods with graph cuts, and our
    proposed feature descriptor with BWNCC metric. Our method can estimate much better
    disparity maps compared with these state-of-the-art methods. Show All TABLE 1
    Comparison of bad5.0 Error Metric (Smaller Values Are Better) We also ran experiments
    on datasets captured using our HLFI on real scenes, again comparing our method
    with other competing techniques. Fig. 7 shows results for two scenes. Visually,
    our approach outperforms the other techniques; for example, as can be seen at
    the bottom row, our technique is able to recover the guitar edge where other techniques
    fail due to spectral inconsistencies. Fig. 7. Cross-spectral stereo matching results
    on real scenes captured using our HLFI. The first and second columns are left
    and right images captured by two adjacent cameras at different spectra. The other
    columns show extracted disparity maps using SSD, NCC, RSNCC and our technique.
    Qualitatively, our results outperform the other competing techniques. Show All
    7.3 H-LF Stereo Matching Results In one set of experiments, we generate synthetic
    H-LF scenes from regular LFs used in Wanner et al. [15]. For each scene, we choose
    5 \times 6 views with uniform baseline. For each view, we add a synthetic tunable
    filter. The synthetic filter is actually a series of coefficients which are the
    transmittance in Red, Green, and Blue channels associated with the filter''s wavelength.
    The coefficients are based on the approximate relationship between RGB color and
    rendering spectra as provided in [44]. Finally, we render 5 \times 6 spectral
    images from original images by adjusting the filter transmittance in the RGB channels
    and converting them to gray scale. Because this synthesized spectral profile is
    different from that for our HLFI, we choose different values of \gamma _c in Equation
    (17) (0.45 for synthetic data and 0.6 for real data). In another set of experiments,
    we compare our H-LF stereo matching results with techniques by Tao et al. [8],
    Lin et al. [7] and Wang et al. [17], on synthetic and real data. Fig. 8 compares
    the disparity maps on the synthetic dataset. The close-up regions (red and green
    boxes) show how well our technique works compared to the others. As Table 2 shows,
    our technique has the lowest RMSE. Fig. 8. H-LF results for two synthetic scenes
    from Wanner et al. [15], with each view having a different spectral response.
    We show our result as well as those of previous LF stereo matching methods (Tao
    et al. [8], Lin et al. [7], and Wang et al. [17]). The two close-ups show the
    relative quality of our result. Show All TABLE 2 Comparison of RMSE (Smaller Values
    Are Better) for H-LF Stereo Matching on Synthetic Data Shown in Fig. 8 Fig. 9
    shows H-LF stereo matching results for three real scenes. The overall visual quality
    of our results is better than that for the other competing techniques. In particular,
    our technique is better able to handle scene detail; see, for example, the mug
    in the top scene and guitar''s neck in the bottom scene. Fig. 9. H-LF stereo matching
    results for three real scenes captured by our HLFI. We show our results as well
    as those of previous LF stereo matching methods (Tao et al. [8], Lin et al. [7],
    and Wang et al. [17]). The two close-ups show how well our technique can recover
    scene detail. Show All These results show our approach outperforms the state-of-the
    art in visual quality, accuracy, and robustness on both synthetic and real data.
    They validate our design decisions on handling cross-spectral variation. 7.4 H-LF
    Reconstruction Results In another experiment, we use our HLFI to capture a room
    scene, processed the data using our technique, and completed its plenoptic cube
    representation. Results are shown in Fig. 10. The raw data are shown in Fig. 10a;
    the scene has colorful objects made with different materials and placed at different
    depths. Fig. 10b shows the completed plenoptic cube. Reconstructed hyperspectral
    datacubes at viewpoints (2, 2), (3, 4), and (5, 6) are shown in Fig. 10c. Selected
    close-ups in Fig. 10d demonstrate that our technique can robustly align occlusion
    and texture boundaries under spectral variation. Fig. 10e shows the spectral profiles
    of three scene points (captured and recovered): a point on the guitar, a cyan
    point surrounded by white letters, and a depth boundary. These results show that
    our reconstruction scheme can robustly align occlusion and texture boundaries
    under spectral variations and recover high fidelity H-LFs. Fig. 10. H-LF reconstruction
    results for a real scene. (a) Raw data acquired by our HLFI, (b) completed plenoptic
    cube, (c) reconstructed hyperspectral datacubes at viewpoints (2, 2), (3, 4),
    and (5, 6), (d) close-ups of representative boundary and textured areas, (e) spectral
    profiles of three scene points: a point on the guitar, a cyan point surrounded
    by white letters, and a depth boundary. Show All Fig. 11 shows the spectral profiles
    (captured/ground truth and recovered) for selected areas on four different objects
    (guitar, toy, book, and bottle) in three sampled views. Given that the selected
    areas are uniform, we merge and average the measurements across the different
    filters to generate ground truth. The recovered spectral profile for each area
    is the average of the spectral profiles of the constituent pixels. The recovered
    spectral profiles mostly align with the ground truth; where they differ (e.g.,
    in the range of 630\;\mathrm{nm}-700\;\mathrm{nm} for area 2) is caused by uncertainty
    due to some texture within the area. In addition, notice that area 4 is at the
    image periphery in view (2,2). The missing spectra in its vicinity in that view
    has a negative impact on the recovered spectral profile. Fig. 11. Comparison of
    H-LF reconstruction results for four representative areas. The top row is a series
    of reconstructed hyperspectral datacubes at viewpoints (2, 2), (3, 4), and (5,
    6). Areas 1-4 are on different materials (guitar, toy, book and bottle); the curves
    are spectral profile of these materials, both recovered in different views and
    compared with the ground truth. Show All Table 3 lists the Relative Average Spectral
    Error (RASE) for these four areas. RASE is a global quality metric of the recovered
    spectra [45] with respect to the ground truth; it is between 0-100 percent, and
    defined as \begin{equation} RASE=\frac{100}{\sum _{l=1}^{L}\mu (l)}\sqrt{L\sum
    _{l=1}^{L}RMSE(l)^2}, \tag{(21)} \end{equation} View Source where \mu (l) is the
    mean intensity of band l and L is the number of bands. The lower RASE is, the
    better the quality of spectra is considered to be. (Please note that ground truth
    is created by manually segmenting the scene and aligning the segments among 30
    bands.) TABLE 3 RASE Values (Smaller Is Better) for Representative Areas and Views
    for H-LF Reconstruction Shown in Fig. 11 Results for another real scene are shown
    in Fig. 12: raw data at view (3,3) in (a), reconstructed results in (b), and error
    maps for representative views in (c). We use the Bhattacharyya distance (BD) to
    measure the dispersion between the recovered spectra and ground truth. The Bhattacharyya
    distance BD(\mathbf {p}) at pixel \mathbf {p} is the negative log of the Bhattacharyya
    coefficient BC(\mathbf {p}) : \begin{align} \nonumber BC(\mathbf {p})&={\sum _{l\in
    L}\sqrt{\frac{p(l)}{\sum _{l\in L} p(l)} *\frac{q(l)}{\sum _{l \in L} q(l)}}}\\
    BD(\mathbf {p})&=-\ln {BC(\mathbf {p})}, \tag{(22)} \end{align} View Source where
    p(l) and q(l) are recovered and ground truth values for band l in pixel \mathbf
    {p} , respectively. BC(\mathbf {p}) is Bhattacharyya coefficient which measures
    similarity between p(\cdot) and q(\cdot) in range [0,1] . Lower BD(\mathbf {p})
    values are better. Fig. 12. H-LF reconstruction results for a real scene. (a)
    Raw data at view (3,3), (b) recovered hyperspectral datacube at view (3,3), (c)
    heat maps of BD({\mathbf {p}}) for reconstructed spectra relative to ground truth
    for views (1,1), (1,3), (1,6), (3,1), (3,3), (3,6), (5,1), (5,3), and (5,6); smaller
    values are better. The regions within black and white rectangles respectively
    are invalid and incorrect due to non-overlapping views (being outside the FOV
    of some cameras, and occlusions, respectively). Show All As Figs. 12b, 12c show,
    the visual quality of spectral reconstruction is good for most of the scene. The
    top and left margins (within black rectangles) are considered invalid because
    those areas are not seen by all the cameras; regions in the scene are considered
    valid only if they are seen by all cameras. As a result, only part of the full
    spectrum is available for reconstruction. Regions within white rectangles are
    incorrect since they exemplify areas with depth discontinuities that cause occlusion.
    Reconstruction errors in these regions are caused by mismatches. Table 4 provides
    quantitative evaluation of valid areas for every view of the scene shown in Fig.
    12a. The RASE values are between 13.3-23.9 percent; unsurprisingly, the highest
    errors occur at the peripheral (extreme) views, where overlap with the farthest
    views is less. Table 5 shows statistics on the percentage of pixels where BD(\mathbf
    {p})\leq 0.05 (similarity between the recovered spectra and ground truth is greater
    than 0.95, or BC(\mathbf {p})\geq 0.95 ). The range of results is 92.8-96.5 percent
    across all views. Again, the values are worse at peripheral views. TABLE 4 RASE
    Values (Smaller Is Better) at All (5 \times 6 5×6) Views in Valid Areas Relative
    to Ground Truth, for the Scene Shown in Fig. 12a TABLE 5 Percentage of Pixels
    with BD(\mathbf {p})\leq 0.05 BD(p)≤0.05 in Valid Areas in the Scene Shown in
    Fig. 12a 7.5 Applications: Color Sensor Emulation and H-LF Refocusing We can use
    the recovered H-LF data to emulate a synthetic camera with a specific spectral
    profile. This allows us to reproduce color images unique to that camera. Fig.
    13 shows two pairs of real images captured by PTGrey FL3-U3-20E4C-C alongside
    our synthesized color images (whose original spectral profile is shown in Fig.
    5). The top pair includes original images (without cropping and alignment) of
    one scene. The red boxes show the incorrect color of the table cloth in the synthesized
    one (right), which is caused by missing spectra due to limited field of view.
    Notice that the top rows of Fig. 10b do not include most of the table cloth; as
    a result, no information on a specific range of the spectrum is available for
    propagation, causing incorrect color synthesis. After removing the region of red
    box and aligning images, we get PSNR of right image is 22.6, given the left image
    as reference. The bottom pair includes cropped and aligned images of another scene,
    and PSNR of right image is 23.1. Both the images and PSNR values show that our
    synthesized color images are reasonable reproductions of the actual versions.
    Fig. 13. Comparison of real and synthetic color images. Left: real images captured
    by a PTGrey FL3-U3-20E4C-C camera (the profile is same as that shown in Fig. 5).
    Right: synthesized images using our acquired H-LF and the camera profile. Given
    left images as references, PSNR of right image on top pair is 22.6 after removing
    the region of red box and alignment, whereas PSNR is 23.1 on bottom pair. Show
    All Fig. 14 shows results of synthetic refocusing for different spectral profiles.
    These results demonstrate that our dynamic H-LF refocusing is different from regular
    LF refocusing; it can focus at any depth layer at any sampled spectrum. Note that
    the banding artifacts are due to the discrete view sampling of our HLFI. Fig.
    14. H-LF refocusing results. Top: spectral profiles of three cameras. Bottom:
    synthetic refocusing results at different depths for the three profiles. Results
    in different rows are at different depths (near, middle and far). Results in different
    columns are synthesized from different profiles respectively. Show All SECTION
    8 Discussion Because our HLFI is fundamentally a multi-view camera system, it
    has the same issues associated with length of baseline versus accuracy and ease
    of correspondence. Our HLFI has two main problems that are specific to multi-spectral
    matching. The first is the computational complexity of our feature descriptor
    and metric. In order to acquire accurate depth, we need to consider both edge
    and non-edge regions hierarchically, and compute the distance using descriptors
    over local patches at different levels. These operations are more computationally
    expensive compared to traditional methods. Our GPU implementation produces depth
    results in about two minutes for datasets shown in Fig. 9 (each dataset has a
    5 \times 6 array of images, with each image having a resolution of 1200 \times
    900 ). More specifically, the GPU is used mainly to extract the hierarchical feature
    descriptor and calculate pairwise BWNCC. Another problem is the incomplete spectral
    reconstruction due to missing views. Each camera samples a narrow band of the
    visible spectrum and a different view of the scene. As a result, different parts
    of the scene would visible to a different subset of cameras in the HLFI. This
    results in incomplete propagation of the missing spectra, as can be seen in Figs.
    10 and 13. More specifically, the table cloth has incorrect colors because cameras
    at the top few rows are not able to capture its appearance, resulting in absence
    of certain spectral bands. There is also the interesting issue of filter arrangement.
    Currently, the filter wavelengths in our HLFI are arranged in raster order. As
    a result, as can be seen on the left of Fig. 1, horizontal neighbors are much
    more similar in appearance than vertical neighbors. This arrangement has implications
    on the H-LF stereo matching and reconstruction. There may be a better way of arranging
    these filters so as to reduce appearance changes in both vertical and horizontal
    directions. While it is possible to redesign with different cameras having the
    same filters, this reduces the spectral sampling density (for the same number
    of cameras and overall visible spectral extent). SECTION 9 Concluding Remarks
    We proposed a hyperspectral light field (H-LF) stereo matching technique. Our
    approach is based on a new robust spectral-invariant feature descriptor to address
    intensity inconsistency across different spectra and a novel cross-spectral multi-view
    stereo matching algorithm. For increased robustness in matching, we show how to
    perform view selection in addition to measuring focusness in an H-LF. We have
    conducted comprehensive experiments by constructing an H-LF camera array to validate
    our claims. Finally, we show how our results can be used for plenoptic cube completion,
    emulation of cameras with known spectral profiles, and spectral refocusing. An
    immediate future direction is to capture and process H-LF video. This will require
    temporal regularization techniques, in addition to requiring efficient compression
    to save bandwidth. In our current setup, the band-pass filters were sequentially
    assigned to the cameras, i.e., the neighboring cameras will have close spectral
    responses. The advantage of this setup is that we can more reliably conduct stereo
    matching and hence warping between adjacent images. Despite this, the baseline
    of the cameras cannot be too large, because we still require good visual overlap
    between images for effective spectral propagation. It would be interesting to
    investigate other camera designs with different spectral distributions to handle
    current limitations. ACKNOWLEDGMENTS This work is partially supported by the programs
    of STCSM (17XD1402900, 17JC1403800). Authors Figures References Citations Keywords
    Metrics Footnotes More Like This 1CCD and 3CCD Color Cameras Performance Comparison
    Applied to Hyperspectral Image Reconstruction IEEE Latin America Transactions
    Published: 2015 Endmember-Assisted Camera Response Function Learning, Toward Improving
    Hyperspectral Image Super-Resolution Performance IEEE Transactions on Geoscience
    and Remote Sensing Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Transactions on Pattern Analysis and Machine Intelligence
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Hyperspectral Light Field Stereo Matching
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2022.3154709
  analysis: '>'
  authors:
  - Sergey Nesteruk
  - Svetlana Illarionova
  - Timur Akhtyamov
  - Dmitrii Shadrin
  - Andrey Somov
  - Mariia Pukalchik
  - Ivan Oseledets
  citation_count: 13
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading [MathJax]/extensions/MathMenu.js
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Personal Sign In * Required *Email Address *Password
    Forgot Password? Sign In Don''t have a Personal Account? Create an IEEE Account
    now. Create Account Learn more about personalization features. IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/9668973/09721254.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'XtremeAugment: Getting More From Your Data Through Combination of Image
    Collection and Image Augmentation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.34133/2021/9871989
  analysis: '>'
  authors:
  - David M. Deery
  - Hamlyn G. Jones
  citation_count: 22
  full_citation: '>'
  full_text: '>

    ADVERTISEMENT About SPJ Author Services Journals Science.org Science Partner Journals
    Quick Search anywhere ENTER SEARCH TERM SEARCH ADVANCED SEARCH Featured Articles
    RESEARCH4 APR 2024 Harnessing Renewable Lignocellulosic Potential for Sustainable
    Wastewater Purification ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Multilayer
    MoS2 Photodetector with Broad Spectral Range and Multiband Response BY XIA-YAO
    CHEN DAN SU ET AL. ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Hepatocellular
    Carcinoma Detection by Cell Sensor Based on Anti-GPC3 Single-Chain Variable Fragment
    BY ZUPENG YAN ZIYUAN CHE ET AL. BIODESIGN RESEARCH3 APR 2024 High-Temperature
    Tolerance Protein Engineering through Deep Evolution BY HUANYU CHU ZHENYANG TIAN
    ET AL. BME FRONTIERS3 APR 2024 Multifunctional Ablative Gastrointestinal Imaging
    Capsule (MAGIC) for Esophagus Surveillance and Interventions BY HYEON-CHEOL PARK
    DAWEI LI ET AL. OCEAN-LAND-ATMOSPHERE RESEARCH3 APR 2024 Factors Modulating the
    Variability of Eddy Kinetic Energy in the Southern Ocean from Idealized Simulations
    BY YONGQING CAI DAKE CHEN ET AL. PLANT PHENOMICS3 APR 2024 Microfluidic Device
    for Simple Diagnosis of Plant Growth Condition by Detecting miRNAs from Filtered
    Plant Extracts BY YAICHI KAWAKATSU RYO OKADA ET AL. MORE ARTICLES ADVERTISEMENT
    Journals Advanced Devices & Instrumentation The Open Access journal Advanced Devices
    & Instrumentation, published in association with BIACD, is a forum to promote
    breakthroughs and application advances at all levels of electronics and photonics.
    BioDesign Research The Open Access journal BioDesign Research, published in association
    with NAU, publishes novel research in the interdisciplinary field of biosystems
    design. Biomaterials Research The Open Access journal Biomaterials Research, published
    in association with KSBM, covers the interdisciplinary fields of biomaterials
    research, including novel biomaterials, cutting-edge technologies of biomaterials
    synthesis and fabrication, and biomedical applications in clinics and industry.
    BMEF The Open Access journal BMEF (BME Frontiers), published in association with
    SIBET CAS, is a platform for the multidisciplinary community of biomedical engineering,
    publishing wide-ranging research in the field. Cyborg and Bionic Systems The Open
    Access journal Cyborg and Bionic Systems, published in association with BIT, promotes
    the knowledge interchange and hybrid system codesign between living beings and
    robotic systems. Ecosystem Health and Sustainability The Open Access journal Ecosystem
    Health and Sustainability, published in association with ESC, publishes research
    on advances in sustainability ecology and how global environmental change affects
    ecosystem health. Energy Material Advances The Open Access journal Energy Material
    Advances, published in association with BIT, is an interdisciplinary platform
    for research in multiple fields from cutting-edge material to energy science.
    Health Data Science The Open Access journal Health Data Science, published in
    association with PKU, publishes innovative, scientifically-rigorous research to
    advance health data science. Intelligent Computing Open Access journal Intelligent
    Computing, published in affiliation with Zhejiang Lab, publishes the latest research
    outcomes and technological breakthroughs in intelligent computing. Journal of
    Remote Sensing The Journal of Remote Sensing, an Open Access journal published
    in association with AIR-CAS, promotes the theory, science, and technology of remote
    sensing, as well as interdisciplinary research within earth and information science.
    Ocean-Land-Atmosphere Research The Open Access journal Ocean-Land-Atmosphere Research
    (OLAR), published in association with SML, publishes technologically innovative
    research in marine, terrestrial, and atmospheric studies and the interactions
    among them. Plant Phenomics The Open Access journal Plant Phenomics, published
    in association with NAU, publishes novel research that advances plant phenotyping
    and connects phenomics with other research domains. Research The Open Access journal
    Research, published in association with CAST, publishes innovative, wide-ranging
    research in life sciences, physical sciences, engineering and applied science.
    Space: Science & Technology Open Access journal Space: Science & Technology, published
    in association with BIT, promotes the interplay of science and technology for
    the benefit of all application domains of space activities. It particularly welcomes
    articles illustrating successful synergies in space programs and missions. Ultrafast
    Science The Open Access journal Ultrafast Science, published in association with
    Xi’an Institute of Optics and Precision Mechanics, is a platform for cutting-edge
    and emerging topics in ultrafast science with broad interest from scientific communities.
    BROWSE ALL JOURNALS About Us About SPJ About AAAS Science family of journals Work
    at AAAS Help FAQ Email Alerts and RSS Feeds Follow Us © 2024 American Association
    for the Advancement of Science. All rights Reserved. AAAS is a partner of HINARI,
    AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Terms of Service Privacy Policy
    Accessibility'
  inline_citation: '>'
  journal: Plant phenomics
  limitations: '>'
  pdf_link: https://downloads.spj.sciencemag.org/plantphenomics/2021/9871989.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Field Phenomics: Will It Enable Crop Improvement?'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1017/s1466252321000177
  analysis: '>'
  authors:
  - Sigfredo Fuentes
  - Claudia Gonzalez Viejo
  - Eden Tongson
  - Frank R. Dunshea
  citation_count: 16
  full_citation: '>'
  full_text: '>

    Animal Health Research

    Reviews

    cambridge.org/ahr

    Review

    Cite this article: Fuentes S, Gonzalez Viejo C,

    Tongson E, Dunshea FR (2022). The livestock

    farming digital transformation:

    implementation of new and emerging

    technologies using artificial intelligence.

    Animal Health Research Reviews 23, 59–71.

    https://doi.org/10.1017/S1466252321000177

    Received: 5 June 2021

    Revised: 29 July 2021

    Accepted: 16 November 2021

    First published online: 9 June 2022

    Key words:

    Animal welfare; biometrics; computer vision;

    deep learning; machine learning

    Author for correspondence:

    Sigfredo Fuentes,

    E-mail: sigfredo.fuentes@unimelb.edu.au

    © The Author(s), 2022. Published by

    Cambridge University Press. This is an Open

    Access article, distributed under the terms of

    the Creative Commons Attribution licence

    (http://creativecommons.org/licenses/by/4.0/),

    which permits unrestricted re-use, distribution

    and reproduction, provided the original article

    is properly cited.

    The livestock farming digital transformation:

    implementation of new and emerging

    technologies using artificial intelligence

    Sigfredo Fuentes1

    , Claudia Gonzalez Viejo1

    , Eden Tongson1

    and Frank R. Dunshea1,2

    1Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and
    Food, Faculty of Veterinary and

    Agricultural Sciences, The University of Melbourne, Parkville, VIC 3010, Australia
    and 2Faculty of Biological

    Sciences, The University of Leeds, Leeds LS2 9JT, UK

    Abstract

    Livestock welfare assessment helps monitor animal health status to maintain productivity,

    identify injuries and stress, and avoid deterioration. It has also become an important
    market-

    ing strategy since it increases consumer pressure for a more humane transformation
    in animal

    treatment. Common visual welfare practices by professionals and veterinarians
    may be sub-

    jective and cost-prohibitive, requiring trained personnel. Recent advances in
    remote sensing,

    computer vision, and artificial intelligence (AI) have helped developing new and
    emerging

    technologies for livestock biometrics to extract key physiological parameters
    associated with

    animal welfare. This review discusses the livestock farming digital transformation
    by describ-

    ing (i) biometric techniques for health and welfare assessment, (ii) livestock
    identification for

    traceability and (iii) machine and deep learning application in livestock to address
    complex

    problems. This review also includes a critical assessment of these topics and
    research done

    so far, proposing future steps for the deployment of AI models in commercial farms.
    Most

    studies focused on model development without applications or deployment for the
    industry.

    Furthermore, reported biometric methods, accuracy, and machine learning approaches
    pre-

    sented some inconsistencies that hinder validation. Therefore, it is required
    to develop

    more efficient, non-contact and reliable methods based on AI to assess livestock
    health, wel-

    fare, and productivity.

    Introduction

    Climate change predictions that are affecting most agricultural regions and livestock
    transpor-

    tation routes are related to increasing ambient temperatures, rainfall variability,
    water availabil-

    ity, and increased climatic anomalies, such as heatwaves, frosts, bushfires, and
    floods, affecting

    livestock health, welfare, and productivity. These events have triggered and prioritized
    a critical

    digital transformation within livestock research and industries to be more predictive
    than

    reactive, implementing new and emerging technologies on animal monitoring for
    decision-

    making purposes. Several advances in smart livestock monitoring aim for the objective
    meas-

    urement of animal stress using digital technology to assess the effect of livestock
    welfare and

    productivity using biometrics and artificial intelligence (AI).

    The most accurate methods to measure livestock health and welfare are invasive
    tests, such

    as analysis of tissue and blood samples, and contact sensors positioned on the
    skin of animals

    or internally either by minor surgery, intravaginal, or rectally implanted (Jorquera-Chavez

    et al., 2019a; Zhang et al., 2019b; Chung et al., 2020). However, these are apparently
    imprac-

    tical approaches to monitor many animals for continuous assessments on farms.
    These

    approaches require a high level of know-how by personnel for sampling, sensor
    placement,

    data acquisition processing, analysis and interpretation. Furthermore, they impose
    medium

    to high levels of stress on animals, introducing biases in the analysis and interpretation
    of

    data; for this reason, researchers are focusing on developing novel contactless
    methods to

    improve animal welfare (Neethirajan and Kemp, 2021). There are also visual assessments

    that can be made by experts and trained personnel to assess levels of animal stress
    and welfare.

    However, these can be subjective and require human supervision and assessment
    with similar

    disadvantages of the aforementioned physiological assessments and sensor technologies
    (Burn

    et al., 2009).

    Recent digital advances in sensor technology, sensor networks with The Internet
    of Things

    (IoT) connectivity, remote sensing, computer vision and AI for agricultural and
    human-based

    applications have allowed the potential automation and integration of different
    animal science

    and animal welfare assessment approaches (Morota et al., 2018; Singh et al., 2020).
    There has

    been increasing research on implementing these new and emerging digital technologies
    and

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    adaption to livestock monitoring, such as minimal contact sensor

    technology, digital collars and remote sensing (Karthick et al.,

    2020). Furthermore, novel analysis and modeling systems have

    included machine and deep learning modeling techniques to

    obtain practical and responsible AI applications. The main

    applications for these technologies have been focused on asses-

    sing physiological changes from animals to be related to differ-

    ent types of stress or the early prediction of diseases or parasite

    infestation (Neethirajan et al., 2017; Neethirajan and Kemp,

    2021). One of the most promising approaches is implementing

    AI incorporating remote sensing and machine learning (ML)

    modeling strategies to achieve a fully automated system for non-

    invasive

    data

    acquisition,

    analysis,

    and

    interpretation.

    Specifically, this approach is based on inputs from visible, ther-

    mal, multispectral, hyperspectral cameras and light detection

    and ranging (LiDAR) to predict targets, such as animal health,

    stress, and welfare parameters. This approach is presented in

    detail in the following sections of this review.

    However, much of the research has been based on academic

    work using the limited amount of data accumulated in recent

    years to test mainly different AI modeling techniques rather

    than deployment and practical application to the industry.

    Some research groups have focused their efforts on pilots for AI

    system deployments to assess the effects of heat stress on animals

    and their respective production, welfare on farming and animal

    transport, animal identification for traceability, and monitoring

    greenhouse emissions to quantify and reduce the impact of live-

    stock farming on climate change.

    This review is based on the current research on these new and

    emerging digital technologies applied to livestock farming to

    assess

    health,

    welfare,

    and

    productivity

    (Table

    1).

    Some

    AI-based research applied for potential livestock applications

    have tried to solve too many complex problems rather than con-

    centrating on simple and practical applications, and with few

    deployment examples. However, the latter is a generalized prob-

    lem of AI applications within all industries, in which only 20%

    of AI pilots, have been applied to real-world scenarios and have

    made it to commercial production. The latter figures have

    increased slightly due to COVID-19 for 2021, with increases up

    to 20% for ML and 25% for AI deployment solutions, according

    to the Hanover Enterprise Financial Decision Making 2020 report

    (Wilcox, 2020). By establishing a top-down approach (identifying

    goldilocks problems), specific and critical solutions could be easily

    studied to develop effectively new and emerging technologies,

    including AI. In Australia and worldwide, several issues have

    been identified for livestock transport in terms of the effect of cli-

    mate change, such as effects of increased temperatures, droughts,

    and heatwaves on livestock welfare; especially during long sea

    trips through very hot transport environments, such as those in

    the Persian Gulf, with temperatures reaching over 50°C) and

    the identification and traceability of animals. Many livestock pro-

    ducing countries have identified AI and a digital transformation

    as an effective and practical solution for many monitoring and

    decision-making problems from the industry.

    Biometric techniques for health and welfare assessment

    The most common methods for animal welfare and health assess-

    ment are either visual and subjective, specifically for animal

    behavior, or invasive. They may involve collecting blood or

    urine samples to be analyzed using expensive and time-

    consuming

    laboratory

    techniques

    such

    as

    enzyme-linked

    immunosorbent assay (ELISA) and polymerase chain reaction

    (PCR)

    (Neethirajan

    et

    al.,

    2017;

    Du

    and

    Zhou,

    2018;

    Neethirajan, 2020). Other measurements that are usually related

    to the health and welfare of animals are based on their physio-

    logical responses such as body temperature, heart rate (HR),

    and respiration rate (RR) (Fuchs et al., 2019; Halachmi et al.,

    2019). To measure body temperature, the most reliable methods

    are intravaginal or measured in the ear, with the most common

    devices

    based

    on

    mercury

    or

    digital

    thermometers

    (Jorquera-Chavez et al., 2019a; Zhang et al., 2019b). Body tem-

    perature is vital for early detection and progression of heat stress,

    feed efficiency, metabolism, and disease symptoms detection such

    as inflammation, pain, infections, and reproduction stage, among

    others (McManus et al., 2016; Zhang et al., 2019b).

    Traditional techniques to assess HR may involve manual mea-

    surements

    using

    stethoscopes

    (DiGiacomo

    et

    al.,

    2016;

    Jorquera-Chavez et al., 2019b; Fuentes et al., 2020a), or automatic

    techniques based on electrocardiogram (ECG) devices, such as

    commercial monitor belts with chest electrodes, such as the

    Polar

    Sport

    Tester

    (Polar

    Electro

    Oy,

    Kempele,

    Finland)

    (Orihuela et al., 2016; Stojkov et al., 2016), and photoplethysmo-

    graphy (PPG) sensors attached to the ear (Nie et al., 2020). The

    HR parameter and variability are usually used as an indicator of

    environmental stress, gestation period, metabolic rate, and diag-

    nosis of cardiovascular diseases (Fuchs et al., 2019; Halachmi

    et al., 2019). On the other hand, RR is typically measured by

    manually counting the flank movements of animals resulting

    from breathing in 60 s using a chronometer (DiGiacomo et al.,

    2016; Fuentes et al., 2020a) or counting the breaths in 60 s

    using a stethoscope, or by attaching sensors in the nose, or thorax,

    which can detect breathing patterns (Jorquera-Chavez et al.,

    2019a). Respiration rate can be used to indicate heat stress and

    respiratory diseases (Mandal et al., 2017; Slimen et al., 2019;

    Fuentes et al., 2020a).

    The main disadvantage of traditional methods based on con-

    tact or invasive sensors to assess physiological responses is the

    potential stress they can cause to the animal by the methodology

    used, which can introduce bias. The stress may be caused by the

    anxiety provoked by the restraint and manipulation/contact with

    their bodies for the actual measurement or to attach different sen-

    sors. Furthermore, these methods tend to be costly and time-

    consuming, making it very impractical assessing a large group

    of animals. In manual measurements, they may also have

    human error and, therefore, are subjective and not that reliable.

    Some specific applications for different livestock will be discussed,

    separating cattle, sheep and pigs (Table 1).

    Cattle

    To assess the body temperature of cattle continuously, Chung

    et al. (2020) proposed an invasive method for dairy cows by

    implanting a radio frequency identification (RFID) biosensor

    (RFID Life Chip; Destron Fearing™, Fort Worth, TX, USA) on

    the lower part of ears of three cows that were monitored for 1

    week; however, this method showed medium-strength correla-

    tions when compared directly to the intravaginal temperature

    probe for two of the cows (R2 = 0.73) and low correlation in the

    third cow (R2 = 0.34). The authors then developed a ML model

    based on the long short-term memory method to increase predic-

    tion accuracy. However, the study only reported the root mean

    squared error (RMSE = 0.081) of the model but left out the accur-

    acy based on the correlation coefficient as it should be done for

    60

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Table 1. Summary of biometric methods to assess health and welfare for cattle,
    sheep, and pigs

    Animals

    Measurement

    Technique

    Groundtruth (traditional

    methods)

    Number of

    animals

    Accuracy of method

    Proposed application

    Reference

    Cattle

    Dairy cows

    Body temperature

    Implanted RFID

    biosensor and

    Machine learning

    Vaginal temperature

    (probe)

    3

    RMSE = 0.08

    First steps for precision

    agriculture methods

    (Chung et al., 2020)

    Simulated

    cows

    Temperature and

    movements

    Wearable digital

    sensors

    Wireless data

    acquisition

    None

    1 toy

    simulating a

    cow and hot

    water

    Not reported

    Health monitoring and

    disease detection

    (Tahsin, 2016)

    Cattle

    (Holstein and

    Jersey)

    Body temperature

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Rectal temperature

    (probe)

    Not specified

    Mean difference

    between methods

    0.04 ± 0.10°C

    Alternative to traditional

    temperature methods

    (Wang et al., 2021a)

    German

    Holstein

    cows

    HR

    HRV

    Wearable sensors

    None

    40

    Not reported

    Tested impact of different

    stimulation methods

    (Zipp et al., 2018)

    Dairy calves

    HR

    Wearable sensors

    None

    69

    Not reported

    Behavioral and stress

    response

    (Buchli et al., 2017)

    Cows

    HR

    RR

    Chewing

    Contactless

    biometrics

    Computer vision

    RGB images and

    laser

    HR: wearable sensor

    RR and Cheiwng: manual

    count

    6

    HR: R = 0.98

    RR: R = 0.97

    Chewing: R = 0.99

    Biomedical monitoring for

    optimized cattle treatment

    (Beiderman et al.,

    2014)

    Dairy cows

    Holstein

    Friesian

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images and RGB

    videos

    Skin temperature: vaginal

    probe

    HR: wearable sensors

    RR: manual count

    10

    Skin Temperature:

    R = 0.74

    HR: R = 0.20–0.83

    RR: R = 0.87

    Monitoring of physiological

    responses

    (Jorquera-Chavez

    et al., 2019b)

    Dairy cows

    RR

    Computer vision

    Infrared thermal

    and RGB videos

    Manual count

    15

    Mean difference

    Manual vs RGB

    video: −0.01 ± 0.87

    Manual vs infrared

    videos: 0.83 ±0.57

    Monitoring of health and

    welfare

    (Stewart et al., 2017)

    Calves

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Manual count from RGB

    videos

    5

    R2 = 0.93

    Monitoring of health and

    welfare

    (Lowe et al., 2019)

    Japanese

    Black Calves

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Deep learning

    Manual count

    5

    R2 = 0.91

    Monitoring health

    (Kim and Hidaka,

    2021)

    (Continued)

    Animal Health Research Reviews

    61

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Table 1. (Continued.)

    Animals

    Measurement

    Technique

    Groundtruth (traditional

    methods)

    Number of

    animals

    Accuracy of method

    Proposed application

    Reference

    Qinchuan

    cattle

    Body measurements

    (dimensions)

    Contactless

    biometrics

    Computer vision

    RGB images

    Manual measurements

    3

    2 mm

    Contactless body

    measurements of large

    livestock

    (Huang et al., 2018)

    Dairy cows

    Drinking behavior

    Integrated sensor

    module

    Computer vision

    Deep learning

    None

    25

    Not reported

    Automatic and quantitative

    assessment of drinking

    behavior as a measure of

    heat stress

    (Tsai et al., 2020)

    Sheep

    Dairy sheep

    Behavior activities

    Wireless system

    Wearable sensors

    RGB videos

    Manual assessment

    3

    93%

    Behavior assessment

    (Giovanetti et al.,

    2017)

    Ewes

    Behavior activities

    Wearable sensors

    RGB videos

    Machine learning

    Manual assessment

    6

    85%

    Assessment of sheep

    activity previous to

    methane measurements

    Assessment of temporal

    grazing patterns

    (Alvarenga et al.,

    2016)

    Ewes

    Body temperature

    Wearable sensor

    None

    15

    Not reported

    Measurement of

    temperature changes in

    lambing period

    (Abecia et al., 2020)

    Ewes

    Surface temperature of

    different areas (anus,

    vulva, muzzle, eyes)

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Rectal and vaginal

    temperature

    20

    Not reported

    Assessment of temperature

    during estrous cycle

    Reproductive management

    (de Freitas et al.,

    2018)

    Ewes

    Eye temperature

    HR

    HRV

    Computer vision

    Infrared thermal

    images

    Wearable sensors

    None

    20

    Not reported

    Assessment of autonomic

    nervous system responses

    (Sutherland et al.,

    2020)

    Meat sheep

    Skin temperature

    HR

    Wireless wearable

    monitoring

    system

    Traditional veterinary

    monitors

    60

    Non-significant

    differences (no

    p-value reported)

    Assessment of

    physiological responses

    with minimal stress

    (Cui et al., 2019)

    Mutton

    sheep

    HR

    Oxygen saturation

    Body temperature

    Wearable sensors

    None

    Not reported

    Not reported

    Diagnose survival status

    during transportation

    (Zhang et al., 2020)

    Merino lambs

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Machine learning

    Skin and rectal

    temperature (digital

    thermometer)

    Stethoscope

    Manual count

    12 sheep /3

    times a day

    /four weeks

    Skin temperature:

    R2 = 0.99

    HR and RR: R = 0.94

    Assessment of

    physiological responses

    and heat stress during

    transportation

    (Fuentes et al.,

    2020a)

    Sheep

    Body measurements

    (dimensions; weight)

    Contactless

    biometrics

    Computer vision

    Machine learning

    Manual measurements

    27

    Weight: R = 0.99

    Dimensions: R = 0.79

    Increase efficiency in herds

    management

    (Zhang et al., 2018)

    62

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Pigs

    Pigs

    Behavior

    Contactless

    biometrics

    Computer vision

    None

    10 pigs /2

    replications

    Not reported

    Assessment of heat stress

    (Byrd et al., 2020)

    Pigs

    Lying behavior

    Contactless

    biometrics

    Computer vision

    Not reported

    88

    96%

    Welfare assessment

    (Nasirahmadi et al.,

    2017)

    Pigs

    Body measurements

    (dimensions)

    Weight estimation

    Contactless

    biometrics

    Computer vision

    Manual measurements

    78

    R2 > 0.95 to predict

    weight

    Estimate pigs’ weight

    during weaning period

    (Pezzuolo et al.,

    2018)

    Piglets

    Sus Scrofa

    Skin temperature

    Cold/heat stress

    Thirst stress

    Hunger stress

    Pain stress

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Machine learning

    Stress conditions based

    on treatments

    72

    Cold/heat stress:

    100%

    Thirst stress: 91%

    Hunger stress: 86%

    Pain stress: 50%

    Assessment of stress

    during handling and

    transportation

    (da Fonseca et al.,

    2020)

    Sows

    Rectal temperature

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Machine learning

    Rectal temperature

    (mercury thermometer)

    99

    R2 = 0.80

    Welfare assessment

    (Feng et al., 2019)

    Pigs

    HR

    Contactless

    biometrics

    Computer vision

    Electrocardiogram

    2

    78% (Green color

    channel)

    Real-time monitoring of

    health and welfare

    (Wang et al., 2021b)

    Pigs

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared camera

    Electrocardiogram

    Ventilator data

    17

    HR: R2 = 0.96

    RR: R2 = 0.97

    Long term monitoring of

    research animals

    (Barbosa Pereira

    et al., 2019)

    Pigs

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images and RGB

    videos

    None

    46

    Not reported

    Early detection of disease

    before symptoms appear

    (Jorquera-Chavez

    et al., 2020)

    Pigs

    Eye temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    and RGB videos

    Stethoscope

    Manual count from

    videos

    28

    Eye temperature: not

    reported

    HR and RR:

    R = 0.61–0.66

    Physiological responses

    due to respiratory diseases

    (Jongman et al.,

    2020)

    * Abbreviations: RFID, radio frequency identification; RMSE, root mean squared
    error; HR, heart rate; HRV, heart rate variability; RR, respiration rate; RGB,
    red, green, blue.

    Animal Health Research Reviews

    63

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    regression ML models. On the other hand, Tahsin (2016) devel-

    oped a remote sensor system named Cattle Health Monitor and

    Disease Detector, connected using a wireless network. This system

    integrated a DS1620 digital thermometer/thermostat (Maxim

    Integrated™, San Jose, CA, USA) and a Memsic 2125 thermal

    accelerometer (Parallax, Inc., Rocklin, CA, USA) to assess the

    activity of animals by measuring the lateral and horizontal move-

    ments of the cow. The integrated sensors node was placed on the

    neck using a collar, with the option to be powered using a solar

    panel. Furthermore, Wang et al. (2021a) developed a non-

    invasive/contactless sensor system to assess the body temperature

    of cattle using an infrared thermal camera (AD-HF048; ADE

    Technology Inc., Taipei, Taiwan), an anemometer (410i; Testo SE

    & Co., Kilsyth, VIC, Australia), and a humiture sensor (RC-4HA;

    Elitech Technology, Inc., Milpitas, CA. USA). These sensors were

    placed in the feedlot at 1 m from the cows and 0.9 m above the

    ground to record the head of each cow, while these were restrained

    using a headlock. The authors used a rectal thermometer as

    groundtruth to validate the method and reported a difference of

    0.04 ± 0.10°C between the grountruth and the method proposed.

    The anemometer and humiture sensor were used to remove the

    frames affected by external weather factors to extract outliers.

    In the case of HR, Zipp et al. (2018) used Polar S810i and

    RS800CX sensors attached to the withers and close to the heart

    to measure HR and HR variability (HRV) while locked after milk-

    ing to assess the impact of different stimulation methods (acous-

    tic, manual and olfactory). However, the authors reported

    technical problems to acquire HR and HRV, which led to missing

    values and altered the analysis. This is another drawback of using

    contact sensors as they can become unreliable due to different rea-

    sons, such as natural animal movements causing sensors to lose

    contact with the animal skin and connectivity problems. Buchli

    et al. (2017) used a Polar S810i belt attached to the torso of calves

    to measure HR while the animals were in their pen. However,

    similar to the previous study, these authors had errors in the

    data acquired and excluded data from eight calves. To avoid

    these problems, remote sensing methods have been explored,

    such as those developed by Beiderman et al. (2014), based on

    an automatic system to assess HR, RR and chewing activity

    using a tripod holding a PixeLink B741 camera (PixeLink,

    Rochester, NY, USA) and a Photop D2100 laser connected to a

    computer. The laser pointed at the neck and stomach of the

    cow. The acquired signal was analyzed using the ‘findpeaks’

    Matlab® (Mathworks, Inc., Natick, MA, USA) function to assess

    HR from the neck area and RR and chewing from the stomach

    section. The authors reported a correlation coefficient R = 0.98

    for HR, R = 0.97 for RR and R = 0.99 for chewing data compared

    with manual measurements for RR and chewing and Polar sensor

    for HR. These latter methods may solve the contact problems and

    unreliability of data quality; however, they seem to still be manual

    methods requiring operators. The authors did not propose an

    automation system for measurements.

    Jorquera et al. (2019b) also presented contactless methods to

    assess skin temperature, HR and RR of dairy cows using remote

    sensing cameras and computer vision analysis. These authors

    used a FLIR AX8 camera (FLIR Systems, Wilsonville, OR, USA)

    integrated into a Raspberry Pi V2.1 camera module to record

    infrared thermal images (IRTI) and RGB videos of the face of

    the cows while restrained in the squeeze chute. The IRTIs were

    analyzed automatically using the FLIR Atlas software develop-

    ment kit (SDK) for Matlab® and cropped the videos in the eye

    and ear sections. The RGB videos were used to assess HR using

    the PPG method based on the luminosity changes in the green

    channel of the eye, forehead and full face of the cows; these signals

    were then further analyzed using a customized Matlab® code pre-

    viously developed for people (Gonzalez Viejo et al., 2018) and

    adapted for animals. On the other hand, the authors used a

    FLIR ONE camera to record non-radiometric videos of the

    cows. These were analyzed using Matlab® based on the change

    in pixel intensity in the nose section to measure the inhalations

    and exhalations from which RR was calculated.

    Regarding the RR techniques, besides the manual counts usu-

    ally conducted based on visual assessment of the flank movement

    of animals, researchers have also developed computer vision tech-

    niques, which aid in the reduction of human error and bias.

    Stewart et al. (2017) assessed 15 dairy cows using three compara-

    tive methods to determine RR with (i) manual counts of the flank

    movements by recording the time it took the cow to reach 10

    breaths, (ii) manual counts of flank movements similar to method

    (i) but from an RGB video recorded using a high-dynamic-range

    (HDR) CX220E camera (Sony Corporation, Tokyo, Japan), and

    (iii) manual count of the air movement (temperature variations)

    from the nostrils. The latter was performed from infrared thermal

    videos recorded using a ThermaCam S60 camera (FLIR Systems,

    Wilsonville, OR, USA). The three methods showed similar

    responses with the highest average difference of 0.83 ± 0.57

    between methods (i) and (iii). Furthermore, Lowe et al. (2019)

    presented a similar approach but tested only in five calves. In

    the latter study, the two methods were (i) manual count of

    flank

    movements

    from

    an

    RGB

    video

    recorded

    using

    a

    Panasonic HCV270 camera (Panasonic, Osaka, Japan), which

    was made by recording the time taken for the calf to reach five

    breath cycles, and (ii) manual count of the thermal fluctuations

    (color changes) in the nostrils from infrared thermal images

    recorded using a FLIR T650SC camera. The Adobe Premiere

    Pro CC (Adobe, San Jose, CA, USA) was used for the manual

    counts for both methods. A high determination coefficient (R2

    = 0.93) was reported comparing both methods. More recently,

    Kim and Hidaka (2021) used a FLIR ONE PRO infrared thermal

    camera to record IRTIs and RGB videos from the face of calves.

    The authors first measured the color changes from the nostril

    region manually as the time it took for the calf to complete five

    breaths. A mask region-based convolutional neural network

    (Mask R-CNN) and transfer learning were used to automatically

    develop a model using the RGB video frames to automatically

    detect and mask the calves’ noses. Once the nose was detected

    and masked in the RGB videos, co-registered IRTIs were used

    to automatically extract the mean temperature of the region of

    interest. The authors reported an R2 = 0.91 when comparing the

    manual and automatic methods.

    Besides those used to assess physiological responses, other bio-

    metrics have been explored to be applied in beef and dairy cattle.

    These methods consist of the use of biosensors and/or image/

    video analysis (remote sensing). For example, Huang et al.

    (2018) developed a computer vision method to assess body mea-

    surements (dimensions) of cattle using an O3D303 3D LiDAR

    camera to record the individual animal side view and post-

    processing using filter fusion, clustering segmentation and match-

    ing techniques. Tsai et al. (2020) developed an integrated sensor

    module composed of a Raspberry Pi 3B processing unit

    (Raspberry Pi Foundation, Cambridge, England), a Raspberry Pi

    V2 camera module and a BME280 temperature and relative

    humidity sensor for environmental measurement. This integrated

    module was placed on the top of the drinking troughs in a dairy

    64

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    farm to record the drinking behavior of the cows. The authors

    then applied convolutional neural networks (CNN) based on

    Tiny YOLOv3 real-time object detection deep learning network

    for the head detection of cows to predict the drinking length

    and frequency which were found to be correlated with the

    temperature-humidity index (THI; R2 = 0.84 and R2 = 0.96,

    respectively).

    Sheep

    Researchers have been working on different techniques to assess

    sheep behavioral and physiological responses using contact and

    contactless sensors. Giovanetti et al. (2017) designed a wireless

    system consisting of a halter with a three-axis accelerometer

    ADXL335 (Analog Devices, Wilmington, MA, USA) attached;

    this was positioned in the lower jaw of dairy sheep to measure

    the acceleration of their movements on x-, y- and z-axes.

    Furthermore, the authors used a Sanyo VPC-TH1 camera

    (Sanyo, Osaka, Japan) to record videos of the sheep during feed-

    ing and manually assessed whether the animals were grazing,

    ruminating or resting as well as the bites per minute. Similarly,

    Alvarenga et al. (2016) designed a halter attached below the jaw

    of sheep; this halter had an integrated data logger Aerobtec

    Motion Logger (AML prototype V1.0, AerobTec, Bratislava,

    Slovakia), which is able to measure acceleration in x-, y- and

    z-axes transformed into North, East and Down reference system.

    Additionally, they recorded videos of the sheep using a JVC

    Everio GZR10 camera (JVC Kenwood, Selangor, Malaysia) to

    manually assess grazing, lying, running, standing and walking

    activities. These data were used to develop ML models to auto-

    matically predict activities, obtaining an accuracy of 85%.

    Abecia et al. (2020) presented a method to measure the body

    temperature of ewes using a button-size data logger DS1921 K

    (Thermochron™ iButton®, Maxim Integrated, San Jose, CA,

    USA) taped under the tail of the animals. This sensor was able

    to record temperature data every 5 min. Using remote sensing,

    de Freitas et al. (2018) used a FLIR i50 infrared thermal camera

    to record images from different areas of the sheep: anus, vulva,

    muzzle, and eyes. The authors used the FLIR Quickreport soft-

    ware to manually select the different sections in each sheep and

    obtain each area’s mean temperature. They concluded that the

    vulva and muzzle were the best areas to assess temperature during

    the estrous cycle in ewes. Sutherland et al. (2020) also used an

    infrared thermal camera (FLIR Thermacan S60) to record videos

    of the left eye of ewes. These videos were analyzed to assess eye

    temperature using the Thermacam Researcher software ver. 2.7

    (FLIR Systems, Wilsonville, OR, USA). Additionally, the authors

    used a Polar RS800CX sensor and placed it around the ewes

    thorax to assess HR and HRV.

    In terms of potential applications of sensor technology, Cui

    et al. (2019) developed a wearable stress monitoring system

    (WSMS) consisting of master and slave units. The master unit

    was comprised of environmental sensors such as temperature,

    relative humidity and global positioning system (GPS) attached

    to an elastic band and placed around the rib cage of sheep,

    while the slave unit was composed of physiological sensors such

    as an open-source HR sensor (Pulse Sensor, World Famous

    Electronics LLC, New York, NY, USA), and a skin temperature

    infrared sensor (MLX90615; Melexis, Ypres, Belgium). This sys-

    tem was tested on meat sheep during transportation and proposed

    as a potential method to assess physiological responses with min-

    imal stress. Zhang et al. (2020) designed a wearable collar that

    included two sensors to measure (i) HR and oxygen saturation in

    the blood (MAX30102; Max Integrated, San Jose, CA, USA), and

    (ii) body temperature (MLX90614; Melexis, Ypres, Belgium).

    These sensors were connected to the Arduino Mobile App

    (Arduino LLC, Boston, MA, USA) through Bluetooth® for real-time

    monitoring and used an SD card for data storage. The authors also

    proposed this system to assess physiological responses during the

    transportation of sheep. However, these studies can only monitor

    sentinel animals, making it laborious, difficult and impractical for

    the assessment of all animals transported.

    To solve the later problem, Fuentes et al. (2020a) presented a

    contactless/non-invasive method to assess temperature, HR and

    RR of sheep using computer vision analysis and ML. The authors

    used a FLIR DUO PRO camera to simultaneously record RGB

    and infrared thermal videos of sheep. The infrared thermal videos

    were analyzed using customized Matlab® R2020a algorithms to

    automatically recognize the sheep’s head and obtain the max-

    imum temperature. Results showed a very high correlation (R2

    = 0.99) between the temperatures obtained with the thermal cam-

    era and the rectal and skin temperatures measured using a digital

    thermometer. On the other hand, RGB videos were analyzed

    using customized Matlab® R2020a codes to assess HR and RR

    based on the PPG principle using the G color channel from

    RGB scale for HR and ‘a’ from Lab scale for RR. An artificial

    neural network model was developed using the Matlab® code out-

    puts to predict the real HR and RR (measured manually), obtain-

    ing high accuracy of R = 0.94. This study also proposed a potential

    deployment system to be used for animals in transport.

    For other biometric assessments, Zhang et al. (2018) developed

    a computer vision method to measure the dimensions of sheep

    using three MV-EM120C Gigabit Ethernet charge-coupled device

    (CCD) cameras (Lano Photonics, JiLin Province, China) located

    at different positions (top, left and right side) of the weighing

    scale for sheep. The recorded images were analyzed in Matlab®

    R2013 using the superpixel segmentation algorithm. The authors

    also obtained the dimension parameters manually and found cor-

    relations of R = 0.99 for weight and R = 0.79 for dimensions

    (width, length, height and circumference) using support vector

    machine.

    Pigs

    Pigs are also commonly studied to develop biometric techniques

    to assess behavioral and physiological responses. For example,

    Byrd et al. (2020) used a KPC-N502NUB camera (KT&C,

    Fairfield, NJ, USA) mounted on top of the pigs’ pens to assess

    pig behavior. The authors used the GeoVision VMS software

    (GeoVision Inc, Taipei, Taiwan) and assessed whether the pigs

    were active (standing or sitting) or inactive (lying sternal or lat-

    eral). Nasirahmadi et al. (2017) assessed the lying behavior of

    pigs using closed-circuit television (CCTV) with a Sony RF2938

    camera above the pen. Matlab® software was used to analyze the

    videos using computer vision algorithms to detect the position

    of each pig and analyze the distance between each animal consid-

    ering their axes, orientation and centroid. On the other hand,

    Pezzuolo et al. (2018) obtained body measurements and weight

    of

    pigs

    using

    a

    Kinect

    V1

    depth

    camera

    (Microsoft

    Corporation, Redmond, WA, USA) positioned on the top and

    side of the pen. Videos were analyzed using the Scanning Probe

    Image Processor (SPIP™) software (Image Metrology, Lyngby,

    Denmark) to obtain length, front and back height, and heart

    girth. Furthermore, authors developed linear and non-linear

    Animal Health Research Reviews

    65

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    models to predict weight, obtaining an accuracy R2 > 0.95 in all

    modeling methods tested. The drawback that the authors men-

    tioned from this technique is that the system can only record

    data from a single camera at a time because there is interference

    when using simultaneous data acquisition of the two cameras.

    Regarding techniques to measure body/skin temperature from

    pigs, da Fonseca et al. (2020) used a Testo 876-1 handheld infrared

    thermal camera (Testo Instruments, Lenzkirch, Germany) to record

    images of piglets’ full bodies. The IRSoft v3.1 software (Testo

    Instruments, Lenzkirch, Germany) was used to obtain the max-

    imum and minimum skin temperature values. Rocha et al.

    (2019) presented a method to measure the body temperature of

    pigs using two IR-TCM284 infrared thermal cameras (Jenoptik,

    Jena, Germany). One camera was placed in the pen perpendicular

    to the pig’s body, while the second one was positioned 2.6 m above

    the pigs in the loading alley for transportation. The areas of interest

    evaluated were neck, rump, orbital region, and the area behind the

    ears; these were manually selected using the IRT Cronista

    Professional Software v3.6 (Grayess, Bradenton, FL, USA) and

    extracting the minimum, maximum and mean temperatures.

    Authors found that the temperatures from the orbital region and

    behind the ears were the most useful to assess different types of

    stress (cold, heat, thirst, hunger, and pain) during handling and

    transportation. On the other hand, Feng et al. (2019) developed a

    computer vision and ML method to predict the rectal temperature

    of sows using a T530 FLIR infrared thermal camera to capture

    images. The FLIR Tools software (FLIR Systems, Wilsonville, OR,

    USA) was used to obtain the maximum and mean skin tempera-

    ture in different areas such as ears, forehead, shoulder, back central

    and back end, and vulva. With these data, the authors developed a

    partial least squared regression (PLS) model to predict rectal tem-

    perature, obtaining an accuracy of R2 = 0.80.

    Wang et al. (2021b) developed a contactless method to assess

    HR of pigs using two different setups (i) a webcam C920 HD PRO

    (Logitech, Tainan, Taiwan) located on top of the operation table

    with an anesthetized pig and (ii) a Sony HDRSR5 Handycam

    located on a tripod above resting individual housing with a resting

    pig. Matlab® was used to analyze the videos by selecting and crop-

    ping the (i) neck for the first setup and (ii) abdomen, neck and

    front leg for the dual setup. The authors used the PPG principle

    with the three color channels of the RGB scale and found the G

    channel provided the most accurate results compared to measure-

    ments using an ECG. Barbosa Pereira et al. (2019) also developed

    a method using anesthetized pigs; they used a long wave infrared

    VarioCam

    HD

    head

    820

    S/30

    (InfraTecGmbH,

    Dresden,

    Germany) to assess HR and RR. The videos were analyzed

    using Matlab® R2018a, and it included the segmentation using a

    multilevel Otsu’s algorithm, region of interest (chest) selection,

    features identification and tracking using the Kanade–Lucas–

    Tomasi (KLT) algorithm, temporal filtering to measure trajectory

    and principal components analysis (PCA) decomposition and

    selection. This allowed them to obtain an estimated HR and RR

    at the selected frequency rates. The authors reported determin-

    ation coefficients of R2 = 0.96 for HR compared to the ECG

    method and R2 = 0.97 for RR compared to ventilator data.

    Jorquera-Chavez et al. (2020) developed a contactless method to

    assess temperature, HR and RR of pigs using an integrated camera

    composed of a FLIR AX8 infrared thermal camera and a

    Raspberry Pi Camera V2.1 to record IRTIs and RGB videos,

    and a FLIR ONE infrared thermal camera to record non-

    radiometric videos. The authors used the same method as that

    reported for cows (Jorquera-Chavez et al., 2019b) using Matlab®

    R2018b selecting the eyes and ears as regions of interest for tem-

    perature, eye section for HR and nose for RR. The same method

    was used in the study developed by Jongman et al. (2020), but

    they used a FLIR DUO PRO R dual camera (infrared thermal

    and RGB) and reported a correlation coefficient within the R =

    0.61–0.66

    range

    for

    HR

    and

    RR

    compared

    to

    manual

    measurements.

    Biometric techniques for recognition and identification

    Correct and accurate identification of livestock is essential for farm-

    ers and producers. It also allows relating each animal to different

    productivity aspects such as health-related factors, behavior, pro-

    duction yield and quality and breeding. Furthermore, animal iden-

    tification is essential for traceability, especially during transport and

    after selling, to avoid fraud and animal ledger or identification for-

    ging. However, traditional methods involve ear tags, tattoos, micro-

    chips and radio frequency identification (RFID) collars, which

    involve high costs, and some may be unreliable and easily hacked

    or interchanged. Furthermore, they require human labor for their

    maintenance, making them time-consuming, prone to human

    error and may lead to swapping tags (Awad, 2016; Kumar et al.,

    2016, 2017a; Zin et al., 2018). Therefore, some studies in recent

    years have focused on the development of contactless biometric

    techniques to automate the recognition and identification of differ-

    ent animals such as bears, using deep learning (Clapham et al.,

    2020), and cows based on different features such as the face (Cai

    and Li, 2013; Kumar et al., 2016), muzzle (Kumar et al., 2017a),

    body patterns (Zin et al., 2018), iris recognition (Lu et al., 2014)

    or retinal patterns (Awad, 2016).

    Cattle

    Most of these biometric techniques for recognition and identifica-

    tion have been developed for cattle. Authors have presented meth-

    ods based on one of the three main techniques (i) muzzle pattern

    identification, (ii) face recognition and (iii) body recognition and

    identification. The first technique has been applied for cattle rec-

    ognition using images of the muzzle and analyzed for features as

    it has a particular pattern that is different for each animal, similar

    to the human fingerprints. Once these features and patterns are

    recognized, a deep learning model is developed to identify each

    cow (Noviyanto and Arymurthy, 2012; Gaber et al., 2016;

    Kumar et al., 2017a, 2017b, 2018; Bello et al., 2020). Face recog-

    nition methods using different techniques such as local binary

    pattern algorithms (Cai and Li, 2013) and CNN have been pro-

    posed for specific cattle breeds with different colors and patterns,

    such as Simmental (Wang et al., 2020), Holstein, Guernseys and

    Ayrshires, among others (Kumar et al., 2016; Bergamini et al.,

    2018); however, none has been presented in single-coloured cattle

    breeds such as Angus. On the other hand, body recognition meth-

    ods have been developed to identify cows within a herd using

    computer vision and deep learning techniques. Within the pro-

    posed methods are cattle recognition from the side (Bhole et al.,

    2019), from behind (Qiao et al., 2019), different angles (de

    Lima Weber et al., 2020) or from the top (Andrew et al., 2019).

    The latter was proposed to identify and recognize Holstein and

    Friesian

    cattle

    using

    an

    unmanned

    aerial

    vehicle

    (UAV)

    (Andrew et al., 2017, 019, 2020a, 2020b). Bhole et al. (2019) pro-

    posed an extra step for cow recognition from the side by recording

    IRTIs

    to

    ease

    the

    image

    segmentation

    and

    remove

    the

    background.

    66

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Sheep

    While biometrics applied for the identification and recognition of

    sheep have not been deeply explored, the development of some

    proposed methods has been published. The techniques that

    have been reported for sheep consist of retinal recognition

    using a commercial retinal scanner, OptiReader (Optibrand®,

    Fort Collins, CO, USA) (Barron et al., 2008), and face recognition

    using classification methods such as machine or deep learning.

    Salama et al. (2019) developed a deep learning model based on

    CNN and Bayesian optimization and obtained an identification

    accuracy of 98%. Corkery et al. (2007) proposed a method

    based on independent components analysis and the InfoMax

    algorithm to identify the specific components from the normal-

    ized images of sheep faces and then find them in each tested

    image; the authors reported an accuracy within 95–95%.

    Pigs

    The biometric techniques that have been published to identify pigs

    are based mainly on face recognition and body recognition from

    the tops of pens. Hansen et al. (2018) developed a face recognition

    method using CNN with high accuracy (97%). Marsot et al. (2020)

    developed a face recognition system based on a mix of computer

    vision to identify the face and eyes and deep learning CNN for clas-

    sification purposes, obtaining an accuracy of 83%. On the other

    hand, Wang et al. (2018) proposed a method to identify pigs

    from images recorded from the whole body using integrated deep

    learning

    networks

    such

    as

    dual-path

    network

    (DPN131),

    InceptionV3 and Xception, with an accuracy of 96%. Huang

    et al. (2020) tested a Weber texture local descriptor (WTLD) iden-

    tification method with different masks to detect and recognize indi-

    vidual features such as hair, skin texture, and spots using images of

    groups of pigs; the tested WTLD methods resulted in accuracies

    >96%. Kashiha et al. (2013) based their automatic identification

    method on computer vision to recognize marked pigs within a

    pen using the Fourier algorithm for patterns description and

    Euclidean distance, this technique resulted in 89% accuracy.

    Machine and deep learning application in livestock to

    address complex problems

    This section concentrates specifically on the research on AI appli-

    cation using ML and deep learning modeling techniques on live-

    stock, specifically for cattle, sheep and pigs. One of the latest

    research studies has been focused on the use of AI to identify

    farm animal emotional responses, including pigs and cattle

    (Neethirajan, 2021). However, it may be difficult to assess and

    interpret the emotional state of farm animals only from facial

    expression and ear positioning, as proposed in the latter study,

    and more objective assessment could be performed using targets

    based on hormonal measurements from endorphins, dopamine,

    serotonin and oxytocin among others, which will require blood

    sampling. Therefore, all the in vitro and tissue applications were

    excluded from this section because they require either destructive

    or invasive methods to obtain data.

    Cattle

    A simple AI approach was proposed using historical data (4 years)

    with almost ubiquitous sensor technology in livestock farms, such

    as meteorological weather stations with daily temperature and

    relative humidity (Fuentes et al., 2020b). In this study, meteoro-

    logical data was used to calculate temperature and humidity indices

    (THI) using different algorithmic approaches as inputs to assess the

    effect of heat stress on milk productivity as targets in a robotic dairy

    farm. This approach attempted to answer complex questions with

    potentially readily available data from robotic and conventional

    dairy farms and proposed a deployment system for an AI approach

    with a general accuracy of AI models of 87%. More accurate heat

    stress assessments could be achieved by either sensor technology,

    with minimal invasiveness to animals, such as ear clips, collars or

    similar, or remote sensing cameras, computer vision and deep

    learning modeling. However, the latter digital approach requires

    assessing individual animals using extra hardware and sensors,

    camera systems located in strategic positions allowing monitoring

    of every single animal (e.g. corral systems and straight alleys).

    Furthermore, these new digital approaches require the recording

    of new data. A big question in applications of AI in cattle, in this

    case, would be whether it is worth the significant extra investment

    in hardware and ML modeling using new data to increase the

    accuracy of models by an additional 10.

    Sensor technology and sensor networks have been implemented

    in cattle to assess lameness, such as accelerometers, IoT connectivity

    and time series ML modeling approaches (Taneja et al., 2020; Wu

    et al., 2020). These applications were the first approaches to be

    implemented in animals after applications in humans for fitbits.

    Sensor readings and connectivity using IoT will facilitate the imple-

    mentation of this technology in a near or real-time fashion.

    However, there is a big downside of the requirement of sensors

    for every single animal to be monitored. This is valid to other appli-

    cations for sensor integration (Neethirajan, 2020), such as collars,

    halter and ear tag sensors (Rahman et al., 2018), to detect physio-

    logical changes, behavior and other anomalies (Wagner et al., 2020).

    As mentioned before, animal recognition using deep learning

    approaches should be considered the first step to apply further

    remote sensing and AI tools. A second step should be the identi-

    fication of key features from animals using deep learning (Jiang

    et al., 2019), which makes possible the extraction of physiological

    information from those specific regions using ML modeling, such

    as HR from the eye section or exposed skin (e.g. ears or muzzle)

    and RR from the muzzle section. These animal features should be

    recognized in a video to extract enough information to obtain

    physiological parameters that currently require 4–8 s (e.g. HR

    and RR) for the signal to stabilize and get meaningful data.

    Hence, the AI implementation steps should consider animal rec-

    ognition, specific feature recognition and tracking and extraction

    of physiological parameters using ML.

    Integration of UAV, computer vision algorithms and CNN

    have been attempted for the recognition of cattle from the air

    (Barbedo et al., 2019). However, these authors concentrated

    efforts on the feasibility and testing of different algorithms rather

    than the potential deployment of a pilot program. Furthermore,

    these approaches could also be used for animal recognition and

    the potential extraction of physiological parameters, such as

    body temperature (using infrared thermal cameras as payload).

    Dairy cows could offer more identification features than Angus

    cattle, which may require the implementation of multispectral

    cameras to include potential non-visible features from animals.

    Sheep

    Sensor technology and sensor networks have also been applied in

    parallel with ML approaches for sheep using electronic collars and

    Animal Health Research Reviews

    67

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    ear sensors as input data and supervised selecting several behavior

    parameters as targets with a reported accuracy of <90% for both

    methods (Mansbridge et al., 2018). Some predictive approaches

    from existing data have been attempted to assess carcass traits

    from early life animal records (Shahinfar et al., 2019) using super-

    vised and unsupervised regression ML methods with various low

    to high accuracies reported.

    Similar detection systems mentioned before for other animals

    have been applied for sheep counting using computer vision and

    deep learning CNN methods (Sarwar et al., 2018), which can also

    be used in parallel with other AI procedures to extract more infor-

    mation from animals for health or welfare assessments, such as

    sheep weight (Shah et al., 2021). Following this approach, add-

    itional physiological parameters, such as HR, body temperature

    and RR, can be extracted from individual sheep non-invasively

    (Fuentes et al., 2020a). The latter study also proposed using this

    AI approach for real livestock farming applications, such as ani-

    mal welfare assessment for animals during transportation.

    Other welfare assessments have been developed for sheep

    based on the facial classification expression for pain level applied

    using deep learning CNN and computer vision with 95% accur-

    acy. However, no deployment was reported, which can be used

    to assess further animal welfare (Jwade et al., 2019).

    Pigs

    Some simple ML applications have been implemented to predict

    water usage in pig farms using regression ML algorithms (Lee

    et al., 2017). However, this study reported a maximum determin-

    ation coefficient of R2 = 0.42 for regression tree algorithms, which

    could be related to poor parameter engineering, since only tem-

    perature and relative humidity were used.

    Automatic pig counting (Tian et al., 2019), pig posture detec-

    tion (Nasirahmadi et al., 2019; Riekert et al., 2020), mounting

    (Li et al., 2019) and sow behavior (Zhang et al., 2019a), localiza-

    tion and tracking (Cowton et al., 2019) aggressive behavior

    (Chen et al., 2020) have been attempted using computer vision

    and deep learning. These are relatively complex approaches for

    meaningful questions considering further pipeline of analyses.

    These approaches could be used to extract more information

    from the individual pigs once they have been recognized, such

    as biometrics, including HR and RR extracted for other animals

    such as sheep, mentioned before (Fuentes et al., 2020a), and cattle

    identification (Andrew et al., 2017) with accuracies in identifica-

    tion between 86 and 96% with a maximum of 89 individuals.

    Other approaches have been implemented for the early detec-

    tion (between 1 and 7 days of infection) of respiratory diseases in

    pigs using deep learning approaches (Cowton et al., 2018). Other

    computer vision approaches using visible and infrared thermal

    imagery analysis without ML approaches also delivered an accept-

    able assessment of respiratory diseases in pigs (Jorquera-Chavez

    et al., 2020).

    Conclusions

    Implementing remote sensing, biometrics and AI for livestock

    health and welfare assessment could have many positive ethical

    implications and higher acceptability by consumers of different

    products derived from livestock farming. Specifically, integrating

    digital technologies could directly impact increasing the willing-

    ness to purchase products from sources that introduced AI to

    increase animal welfare on the farm and transport for ethical

    and responsible animal handling and slaughtering. However, a

    systematic deployment of different digital technologies reviewed

    in this paper will require further investment, which some govern-

    ments, such as Australia, have identified as a priority.

    It is difficult to assess the applicability or deployment options

    from different research studies done so far on livestock, which

    have applied biometrics and AI, because there is no consistency

    in the reporting of the accuracy of models, performance, testing

    for over or underfitting of models, number of animals used or

    proposed pilot or deployment options (Table 1). Furthermore,

    in most of these studies, there are no follow-ups on the models

    either by establishing potential pilot deployments to test them

    in real-life scenarios. Many researchers only rely on the validation

    and testing protocols within the model development stage. The

    latter does not give any information on the practicality or applic-

    ability of these digital systems, because circumstances in real-life

    scenarios change over time and models need to be re-evaluated

    and continuously fed with new data to learn and adapt to differ-

    ent circumstances and scales of use.

    It is also clear that most of the AI developments and modeling

    for livestock farming applications are academic, and very little

    research has focused on efficient and practical deployment to real-

    world scenarios. To change this, researchers should work on real-

    life problems in the livestock industry, starting with simple ones

    and pressing questions. The next step is to solve them using effi-

    cient and affordable technology, starting with big data analysis

    from historical data accumulated by different industries. The idea

    here is to initially apply AI where the data exists, to achieve max-

    imum reach with high performance and scalable applications

    (e.g. heat stress assessment on milk production using historical

    weather information and productivity data). It is also required to

    check whether the correct data is available, avoid basing AI on

    reduced datasets, and restricted only to test different ML

    approaches. Academic exercises based on AI modeling for its

    sake only rarely reach pilot programs and applications in the

    field. Furthermore, data quality and data security are becoming

    fundamental issues that should be dealt using digital ledger systems

    for data and model deployments, such as blockchain implementa-

    tion. This approach allows treating data and AI models as a cur-

    rency to avoid hacking and adulteration, especially with AI

    models and data dealing with welfare assessments for animals in

    farms to claim ethical production or animals in transport.

    To solve these problems, AI modeling, development and

    deployment strategies should have a multidisciplinary team with

    constant communication during the model development and

    deployment stages; what could be a better approach, but very

    rare nowadays is to have an expert on animal science, data ana-

    lysis and AI dealing with companies. This could change soon

    through specialized Agriculture, Animal Science and Veterinary

    degrees in which data analysis, ML and AI are introduced in

    their respective academic curriculums.

    Integrating new and emerging digital technology with AI

    development and deployment strategies for practical applications

    would create effective and efficient AI pilot applications that can

    be easily scaled up to production to create successful innovations

    in livestock farming.

    References

    Abecia JA, María GA, Estévez-Moreno LX and Miranda-De La Lama GC

    (2020) Daily rhythms of body temperature around lambing in sheep mea-

    sured non-invasively. Biological Rhythm Research 51, 988–993.

    68

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Alvarenga F, Borges I, Palkovič L, Rodina J, Oddy V and Dobos R (2016)

    Using a three-axis accelerometer to identify and classify sheep behaviour

    at pasture. Applied Animal Behaviour Science 181, 91–99.

    Andrew W, Greatwood C and Burghardt T (2017) Visual localisation and

    individual identification of Holstein Friesian cattle via deep learning.

    Proceedings of the IEEE International Conference on Computer Vision

    Workshops, 2850–2859.

    Andrew W, Greatwood C and Burghardt T (2019) Aerial animal biometrics:

    individual Friesian cattle recovery and visual identification via an autono-

    mous uav with onboard deep inference. arXiv preprint arXiv, 1907.05310.

    Andrew W, Gao J, Mullan S, Campbell N, Dowsey AW and Burghardt T

    (2020a) Visual identification of individual Holstein-Friesian cattle via

    deep metric learning. arXiv preprint arXiv, 2006.09205.

    Andrew W, Greatwood C and Burghardt T (2020b) Fusing animal biometrics

    with autonomous robotics: drone-based search and individual id of Friesian

    cattle. Proceedings of the IEEE/CVF Winter Conference on Applications of

    Computer Vision Workshops 1, 38–43.

    Awad AI (2016) From classical methods to animal biometrics: a review on cat-

    tle identification and tracking. Computers and Electronics in Agriculture

    123, 423–435.

    Barbedo JGA, Koenigkan LV, Santos TT and Santos PM (2019) A study on

    the detection of cattle in UAV images using deep learning. Sensors 19, 5436.

    Barbosa Pereira C, Dohmeier H, Kunczik J, Hochhausen N, Tolba R and

    Czaplik M (2019) Contactless monitoring of heart and respiratory rate in

    anesthetized pigs using infrared thermography. PloS One 14, e0224747.

    Barron UG, Corkery G, Barry B, Butler F, Mcdonnell K and Ward S (2008)

    Assessment of retinal recognition technology as a biometric method for

    sheep identification. Computers and Electronics in Agriculture 60, 156–166.

    Beiderman Y, Kunin M, Kolberg E, Halachmi I, Abramov B, Amsalem R

    and Zalevsky Z (2014) Automatic solution for detection, identification

    and biomedical monitoring of a cow using remote sensing for optimised

    treatment of cattle. Journal of Agricultural Engineering 45, 153–160.

    Bello R-W, Talib AZH and Mohamed ASAB (2020) Deep learning-based

    architectures for recognition of cow using cow nose image pattern. Gazi

    University Journal of Science 33, 831–844.

    Bergamini L, Porrello A, Dondona AC, Del Negro E, Mattioli M, D’alterio

    N

    and

    Calderara

    S

    (2018)

    Multi-views

    embedding

    for

    cattle

    re-identification. 2018 14th International Conference on Signal-Image

    Technology & Internet-Based Systems (SITIS), 2018. IEEE, pp. 184–191.

    Bhole A, Falzon O, Biehl M and Azzopardi G (2019) A computer vision

    pipeline that uses thermal and RGB images for the recognition of

    Holstein cattle. International Conference on Computer Analysis of

    Images and Patterns, 2019. Springer, 108–119.

    Buchli C, Raselli A, Bruckmaier R and Hillmann E (2017) Contact with cows

    during the young age increases social competence and lowers the cardiac

    stress reaction in dairy calves. Applied Animal Behaviour Science 187, 1–7.

    Burn CC, Pritchard JC and Whay HR (2009) Observer reliability for working

    equine welfare assessment: problems with high prevalences of certain

    results. Animal Welfare 18, 177–187.

    Byrd C, Johnson J, Radcliffe J, Craig B, Eicher S and Lay Jr D (2020)

    Nonlinear analysis of heart rate variability for evaluating the growing pig

    stress response to an acute heat episode. Animal: An International

    Journal of Animal Bioscience 14, 379–387.

    Cai C and Li J (2013) Cattle face recognition using local binary pattern

    descriptor.

    2013

    Asia-Pacific

    Signal

    and

    Information

    Processing

    Association Annual Summit and Conference 29 Oct.-1 Nov. 2013,

    Kaohsiung, Taiwan. IEEE, 1–4.

    Chen C, Zhu W, Steibel J, Siegford J, Wurtz K, Han J and Norton T (2020)

    Recognition of aggressive episodes of pigs based on convolutional neural

    network and long short-term memory. Computers and Electronics in

    Agriculture 169, 105166.

    Chung H, Li J, Kim Y, Van Os JM, Brounts SH and Choi CY (2020) Using

    implantable biosensors and wearable scanners to monitor dairy cattle’s core

    body temperature in real-time. Computers and Electronics in Agriculture

    174, 105453.

    Clapham M, Miller E, Nguyen M and Darimont CT (2020) Automated facial

    recognition for wildlife that lack unique markings: a deep learning approach

    for brown bears. Ecology and Evolution 10, 12883–12892.

    Corkery G, Gonzales-Barron UA, Butler F, Mc Donnell K and Ward S

    (2007) A preliminary investigation on face recognition as a biometric iden-

    tifier of sheep. Transactions of the ASABE 50, 313–320.

    Cowton J, Kyriazakis I, Plötz T and Bacardit J (2018) A combined

    deep-learning GRU-autoencoder for the early detection of respiratory dis-

    ease in pigs using multiple environmental sensors. Sensors 18, 2521.

    Cowton J, Kyriazakis I and Bacardit J (2019) Automated individual pig local-

    isation, tracking and behaviour metric extraction using deep learning. IEEE

    Access 7, 108049–108060.

    Cui Y, Zhang M, Li J, Luo H, Zhang X and Fu Z (2019) WSMS: wearable

    stress monitoring system based on IoT multi-sensor platform for living

    sheep transportation. Electronics 8, 441.

    Da Fonseca FN, Abe JM, De Alencar Nääs I, Da Silva Cordeiro AF, Do

    Amaral FV and Ungaro HC (2020) Automatic prediction of stress in pig-

    lets (Sus Scrofa) using infrared skin temperature. Computers and Electronics

    in Agriculture 168, 105148.

    De Freitas ACB, Vega WHO, Quirino CR, Junior AB, David CMG, Geraldo

    AT, Rua MAS, Rojas LFC, De Almeida Filho JE and Dias AJB (2018)

    Surface temperature of ewes during estrous cycle measured by infrared

    thermography. Theriogenology 119, 245–251.

    De Lima Weber F, De Moraes Weber VA, Menezes GV, Junior ADSO, Alves

    DA, De Oliveira MVM, Matsubara ET, Pistori H and De Abreu UGP

    (2020) Recognition of pantaneira cattle breed using computer vision and

    convolutional neural networks. Computers and Electronics in Agriculture

    175, 105548.

    Digiacomo K, Simpson S, Leury BJ and Dunshea FR (2016) Dietary betaine

    impacts the physiological responses to moderate heat conditions in a dose-

    dependent manner in sheep. Animals 6, 51.

    Du X and Zhou J (2018) Application of biosensors to detection of epidemic

    diseases in animals. Research in Veterinary Science 118, 444–448.

    Feng Y-Z, Zhao H-T, Jia G-F, Ojukwu C and Tan H-Q (2019) Establishment

    of validated models for non-invasive prediction of rectal temperature of

    sows using infrared thermography and chemometrics. International

    Journal of Biometeorology 63, 1405–1415.

    Fuchs B, Sørheim KM, Chincarini M, Brunberg E, Stubsjøen SM,

    Bratbergsengen K, Hvasshovd SO, Zimmermann B, Lande US and

    Grøva L (2019) Heart rate sensor validation and seasonal and diurnal vari-

    ation of body temperature and heart rate in domestic sheep. Veterinary and

    Animal Science 8, 100075.

    Fuentes S, Gonzalez Viejo C, Chauhan SS, Joy A, Tongson E and Dunshea

    FR (2020a) Non-invasive sheep biometrics obtained by computer vision

    algorithms and machine learning modeling using integrated visible/infrared

    thermal cameras. Sensors 20, 6334.

    Fuentes S, Gonzalez Viejo C, Cullen B, Tongson E, Chauhan SS and

    Dunshea FR (2020b) Artificial intelligence applied to a robotic dairy

    farm to model milk productivity and quality based on Cow data and

    daily environmental parameters. Sensors 20, 2975.

    Gaber T, Tharwat A, Hassanien AE and Snasel V (2016) Biometric cattle

    identification approach based on weber’s local descriptor and adaboost clas-

    sifier. Computers and Electronics in Agriculture 122, 55–66.

    Giovanetti V, Decandia M, Molle G, Acciaro M, Mameli M, Cabiddu A,

    Cossu R, Serra M, Manca C and Rassu S (2017) Automatic classification

    system for grazing, ruminating and resting behaviour of dairy sheep using a

    tri-axial accelerometer. Livestock Science 196, 42–48.

    Gonzalez Viejo C, Fuentes S, Torrico D and Dunshea F (2018) Non-contact

    heart rate and blood pressure estimations from video analysis and machine

    learning modelling applied to food sensory responses: a case study for choc-

    olate. Sensors 18, 1802.

    Halachmi I, Guarino M, Bewley J and Pastell M (2019) Smart animal agri-

    culture: application of real-time sensors to improve animal well-being and

    production. Annual Review of Animal Biosciences 7, 403–425.

    Hansen MF, Smith ML, Smith LN, Salter MG, Baxter EM, Farish M and

    Grieve B (2018) Towards on-farm pig face recognition using convolutional

    neural networks. Computers in Industry 98, 145–152.

    Huang L, Li S, Zhu A, Fan X, Zhang C and Wang H (2018) Non-contact body

    measurement for Qinchuan cattle with LiDAR sensor. Sensors 18, 3014.

    Huang W, Zhu W, Ma C and Guo Y (2020) Weber texture local descriptor for

    identification of group-housed pigs. Sensors 20, 4649.

    Animal Health Research Reviews

    69

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Jiang B, Wu Q, Yin X, Wu D, Song H and He D (2019) FLYOLOV3 deep

    learning for key parts of dairy cow body detection. Computers and

    Electronics in Agriculture 166, 104982.

    Jongman E, Jorquera-Chavez M, Dunshea F, Fuentes S, Poblete T,

    Rajasekhara R and Morisson R (2020) Developing Remote Monitoring

    Methods for Early Detection of Respiratory Disease in Pigs. Final Report

    A1-104, Australasian Pork Research Institute Limited (23 pp). https://

    apri.com.au/wp-content/uploads/2021/06/A1-104-Final-Report.pdf

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Jongman EC and Warner RD

    (2019a) Computer vision and remote sensing to assess physiological

    responses of cattle to pre-slaughter stress, and its impact on beef quality:

    a review. Meat Science 156, 11–22.

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Warner RD, Poblete T and

    Jongman EC (2019b) Modelling and validation of computer vision techni-

    ques to assess heart rate, eye temperature, ear-base temperature and respir-

    ation rate in cattle. Animals 9, 1089.

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Warner RD, Poblete T,

    Morrison RS and Jongman EC (2020) Remotely sensed imagery for

    early detection of respiratory disease in pigs: a pilot study. Animals 10, 451.

    Jwade SA, Guzzomi A and Mian A (2019) On-farm automatic sheep breed

    classification

    using

    deep

    learning.

    Computers

    and

    Electronics

    in

    Agriculture 167, 105055.

    Karthick G, Sridhar M and Pankajavalli P (2020) Internet of things in animal

    healthcare (IoTAH): review of recent advancements in architecture, sensing

    technologies and real-time monitoring. SN Computer Science 1, 1–16.

    Kashiha M, Bahr C, Ott S, Moons CP, Niewold TA, Ödberg FO and

    Berckmans D (2013) Automatic identification of marked pigs in a pen

    using image pattern recognition. Computers and Electronics in Agriculture

    93, 111–120.

    Kim S and Hidaka Y (2021) Breathing pattern analysis in cattle using infrared

    thermography and computer vision. Animals 11, 207.

    Kumar S, Tiwari S and Singh SK (2016) Face recognition of cattle: can it be

    done? Proceedings of the National Academy of Sciences, India Section A:

    Physical Sciences 86, 137–148.

    Kumar S, Singh SK and Singh AK (2017a) Muzzle point pattern-based tech-

    niques for individual cattle identification. IET Image Processing 11, 805–814.

    Kumar S, Singh SK, Singh RS, Singh AK and Tiwari S (2017b) Real-time

    recognition of cattle using animal biometrics. Journal of Real-Time Image

    Processing 13, 505–526.

    Kumar S, Pandey A, Satwik KSR, Kumar S, Singh SK, Singh AK and

    Mohan A (2018) Deep learning framework for recognition of cattle using

    muzzle point image pattern. Measurement 116, 1–17.

    Lee W, Ryu J, Ban T-W, Kim SH and Choi H (2017) Prediction of water

    usage in pig farm based on machine learning. Journal of the Korea

    Institute of Information and Communication Engineering 21, 1560–1566.

    Li D, Chen Y, Zhang K and Li Z (2019) Mounting behaviour recognition for

    pigs based on deep learning. Sensors 19, 4924.

    Lowe G, Sutherland M, Waas J, Schaefer A, Cox N and Stewart M (2019)

    Infrared thermography—a non-invasive method of measuring respiration

    rate in calves. Animals 9, 535.

    Lu Y, He X, Wen Y and Wang PS (2014) A new cow identification system

    based on iris analysis and recognition. International Journal of Biometrics

    6, 18–32.

    Mandal R, Gupta V, Joshi V, Kumar S and Mondal D (2017) Study of

    clinico-hematobiochemical changes and therapeutic management of natur-

    ally infected cases of respiratory disease in Non-descript goats of bareilly

    region. International Journal of Livestock Research 7, 211–218.

    Mansbridge N, Mitsch J, Bollard N, Ellis K, Miguel-Pacheco GG, Dottorini

    T and Kaler J (2018) Feature selection and comparison of machine learning

    algorithms in classification of grazing and rumination behaviour in sheep.

    Sensors 18, 3532.

    Marsot M, Mei J, Shan X, Ye L, Feng P, Yan X, Li C and Zhao Y (2020) An

    adaptive pig face recognition approach using convolutional neural net-

    works. Computers and Electronics in Agriculture 173, 105386.

    Mcmanus C, Tanure CB, Peripolli V, Seixas L, Fischer V, Gabbi AM,

    Menegassi SR, Stumpf MT, Kolling GJ and Dias E (2016) Infrared therm-

    ography in animal production: an overview. Computers and Electronics in

    Agriculture 123, 10–16.

    Morota G, Ventura RV, Silva FF, Koyama M and Fernando SC (2018) Big

    data analytics and precision animal agriculture symposium: machine learn-

    ing and data mining advance predictive big data analysis in precision ani-

    mal agriculture. Journal of Animal Science 96, 1540–1550.

    Nasirahmadi A, Hensel O, Edwards S and Sturm B (2017) A new approach for

    categorizing pig lying behaviour based on a Delaunay triangulation method.

    Animal: An International Journal of Animal Bioscience 11, 131–139.

    Nasirahmadi A, Sturm B, Edwards S, Jeppsson K-H, Olsson A-C, Müller S

    and Hensel O (2019) Deep learning and machine vision approaches for

    posture detection of individual pigs. Sensors 19, 3738.

    Neethirajan S (2020) The role of sensors, big data and machine learning in

    modern animal farming. Sensing and Bio-Sensing Research 29, 100367.

    Neethirajan S (2021) Happy Cow or thinking Pig? WUR wolf – facial coding

    platform

    for

    measuring

    emotions

    in

    farm

    animals.

    bioRxiv,

    2021.04.09.439122.

    Neethirajan S and Kemp B (2021) Digital livestock farming. Sensing and

    Bio-Sensing Research 32, 100408.

    Neethirajan S, Tuteja SK, Huang S-T and Kelton D (2017) Recent advance-

    ment in biosensors technology for animal and livestock health manage-

    ment. Biosensors and Bioelectronics 98, 398–407.

    Nie L, Berckmans D, Wang C and Li B (2020) Is continuous heart rate mon-

    itoring of livestock a dream or is it realistic? A review. Sensors 20, 2291.

    Noviyanto A and Arymurthy AM (2012) Automatic cattle identification

    based on muzzle photo using speed-up robust features approach.

    Proceedings of the 3rd European Conference of Computer Science, ECCS,

    December 2-4, 2012, Paris, France, 114.

    Orihuela A, Omaña J and Ungerfeld R (2016) Heart rate patterns during

    courtship and mating in rams and in estrous and nonestrous ewes (Ovis

    aries). Journal of Animal Science 94, 556–562.

    Pezzuolo A, Guarino M, Sartori L, González LA and Marinello F (2018)

    On-barn pig weight estimation based on body measurements by a Kinect

    v1 depth camera. Computers and Electronics in Agriculture 148, 29–36.

    Qiao Y, Su D, Kong H, Sukkarieh S, Lomax S and Clark C (2019) Individual

    cattle

    identification

    using

    a

    deep

    learning-based

    framework.

    IFAC-PapersOnLine 52, 318–323.

    Rahman A, Smith D, Little B, Ingham A, Greenwood P and Bishop-Hurley

    G (2018) Cattle behaviour classification from collar, halter, and ear tag sen-

    sors. Information Processing in Agriculture 5, 124–133.

    Riekert M, Klein A, Adrion F, Hoffmann C and Gallmann E (2020)

    Automatically detecting pig position and posture by 2D camera imaging

    and deep learning. Computers and Electronics in Agriculture 174, 105391.

    Rocha LM, Devillers N, Maldague X, Kabemba FZ, Fleuret J, Guay F and

    Faucitano L (2019) Validation of anatomical sites for the measurement

    of infrared body surface temperature variation in response to handling

    and transport. Animals 9, 425.

    Salama A, Hassanien AE and Fahmy A (2019) Sheep identification using a

    hybrid deep learning and Bayesian optimization approach. IEEE Access 7,

    31681–31687.

    Sarwar F, Griffin A, Periasamy P, Portas K and Law J (2018) Detecting and

    counting sheep with a convolutional neural network. 2018 15th IEEE

    International

    Conference

    on

    Advanced

    Video

    and

    Signal

    Based

    Surveillance (AVSS), 2018. IEEE, 1–6.

    Shah NA, Thik J, Bhatt C and Hassanien A-E (2021) A deep convolutional

    encoder-decoder architecture approach for sheep weight estimation.

    Advances in Artificial Intelligence and Data Engineering 1133, 43–53.

    Shahinfar S, Kelman K and Kahn L (2019) Prediction of sheep carcass traits

    from early life records using machine learning. Computers and Electronics

    in Agriculture 156, 159–177.

    Singh M, Kumar R, Tandon D, Sood P and Sharma M (2020) Artificial

    Intelligence and IoT based Monitoring of Poultry Health: A Review. 2020

    IEEE

    International

    Conference

    on

    Communication,

    Networks

    and

    Satellite (Comnetsat), 2020. IEEE, 50–54.

    Slimen IB, Chniter M, Najar T and Ghram A (2019) Meta-analysis of some

    physiologic, metabolic and oxidative responses of sheep exposed to environ-

    mental heat stress. Livestock Science 229, 179–187.

    Stewart M, Wilson M, Schaefer A, Huddart F and Sutherland M (2017) The

    use of infrared thermography and accelerometers for remote monitoring of

    dairy cow health and welfare. Journal of Dairy Science 100, 3893–3901.

    70

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Stojkov J, Weary D and Von Keyserlingk M (2016) Nonambulatory cows:

    duration of recumbency and quality of nursing care affect outcome of flo-

    tation therapy. Journal of Dairy Science 99, 2076–2085.

    Sutherland MA, Worth GM, Dowling SK, Lowe GL, Cave VM and Stewart M

    (2020) Evaluation of infrared thermography as a non-invasive method of

    measuring the autonomic nervous response in sheep. PloS One 15, e0233558.

    Tahsin KN (2016) Development of a propeller P8X 32A based wireless biosen-

    sor system for cattle health monitoring and disease detection. British

    Journal of Applied Science and Technology 18(2), 1–14.

    Taneja M, Byabazaire J, Jalodia N, Davy A, Olariu C and Malone P (2020)

    Machine learning-based fog computing assisted data-driven approach for

    early lameness detection in dairy cattle. Computers and Electronics in

    Agriculture 171, 105286.

    Tian M, Guo H, Chen H, Wang Q, Long C and Ma Y (2019) Automated pig

    counting using deep learning. Computers and Electronics in Agriculture 163,

    104840.

    Tsai Y-C, Hsu J-T, Ding S-T, Rustia DJA and Lin T-T (2020) Assessment of

    dairy cow heat stress by monitoring drinking behaviour using an embedded

    imaging system. Biosystems Engineering 199, 97–108.

    Wagner N, Antoine V, Koko J, Mialon M-M, Lardy R and Veissier I (2020)

    Comparison of Machine Learning Methods to Detect Anomalies in the

    Activity of Dairy Cows. International Symposium on Methodologies for

    Intelligent Systems, 2020. Springer 12117, 342–351.

    Wang J, Liu A and Xiao J (2018) Video-Based Pig Recognition with

    Feature-Integrated Transfer Learning. Chinese Conference on Biometric

    Recognition, 2018. Springer, 620–631.

    Wang H, Qin J, Hou Q and Gong S (2020) Cattle face recognition method

    based on parameter transfer and deep learning. Journal of Physics:

    Conference Series, 2020. IOP Publishing, 012054.

    Wang F-K, Shih J-Y, Juan P-H, Su Y-C and Wang Y-C (2021a) Non-invasive

    cattle body temperature measurement using infrared thermography and

    auxiliary sensors. Sensors 21, 2425.

    Wang M, Youssef A, Larsen M, Rault J-L, Berckmans D, Marchant-Forde JN,

    Hartung J, Bleich A, Lu M and Norton T (2021b) Contactless video-based

    heart rate monitoring of a resting and an anesthetized pig. Animals 11, 442.

    Wilcox J (2020) Covid-19 sentiment and reactions among financial decision-

    makers in Europe. Hanover Research. https://f.hubspotusercontent30.net/

    hubfs/2020381/~Marketing%202020/Landing%20Pages/COVID-19%20Sen

    timent%20and%20Reactions%20among%20Financial%20Decision%20Mak

    ers%20in%20Europe%20-%20OneStream%20-%20October%202020%20Ext

    ernal.pdf?__hstc=231710272.7fd81c9d080729dff81a1cd93879abaa.1601655

    069581.1616156525336.1616159907458.57&__hssc=231710272.109.161615

    9907458&__hsfp=53860856&hsCtaTracking=49adcdf8-fd96-413c-b5a7-e72b

    d9087277%7C59e989fa-56ae-4b8c-bfd8-1eb1d31217d5

    Wu D, Wu Q, Yin X, Jiang B, Wang H, He D and Song H (2020) Lameness

    detection of dairy cows based on the YOLOv3 deep learning algorithm

    and a relative step size characteristic vector. Biosystems Engineering 189,

    150–163.

    Zhang AL, Wu BP, Wuyun CT, Jiang DX, Xuan EC and Ma FY (2018)

    Algorithm of sheep body dimension measurement and its applications

    based on image analysis. Computers and Electronics in Agriculture 153,

    33–45.

    Zhang M, Feng H, Luo H, Li Z and Zhang X (2020) Comfort and health

    evaluation of live mutton sheep during the transportation based on wear-

    able multi-sensor system. Computers and Electronics in Agriculture 176,

    105632.

    Zhang Y, Cai J, Xiao D, Li Z and Xiong B (2019a) Real-time sow behavior

    detection

    based

    on

    deep

    learning.

    Computers

    and

    Electronics

    in

    Agriculture 163, 104884.

    Zhang Z, Zhang H and Liu T (2019b) Study on body temperature detection of

    pig based on infrared technology: a review. Artificial Intelligence in

    Agriculture 1, 14–26.

    Zin TT, Phyo CN, Tin P, Hama H and Kobayashi I (2018) Image

    technology-based

    cow

    identification

    system

    using

    deep

    learning.

    Proceedings

    of

    the

    International

    MultiConference

    of

    Engineers

    and

    Computer Scientists 1, 236–247.

    Zipp KA, Barth K, Rommelfanger E and Knierim U (2018) Responses of

    dams versus non-nursing cows to machine milking in terms of milk perform-

    ance, behaviour and heart rate with and without additional acoustic, olfactory

    or manual stimulation. Applied Animal Behaviour Science 204, 10–17.

    Animal Health Research Reviews

    71

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    '
  inline_citation: '>'
  journal: Animal health research reviews (Print)
  limitations: '>'
  pdf_link: https://www.cambridge.org/core/services/aop-cambridge-core/content/view/3A50E1FBDEA8C13506D479828D7B2F57/S1466252321000177a.pdf/div-class-title-the-livestock-farming-digital-transformation-implementation-of-new-and-emerging-technologies-using-artificial-intelligence-div.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'The livestock farming digital transformation: implementation of new and
    emerging technologies using artificial intelligence'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agronomy13061524
  analysis: '>'
  authors:
  - Aqleem Abbas
  - Zhenhao Zhang
  - Hongxia Zheng
  - Mohammad Murtaza Alami
  - Abdulmajeed F. Alrefaei
  - Qamar Abbas
  - Syed Atif Hasan Naqvi
  - Muhammad Junaid Rao
  - Walid F. A. Mosa
  - Qamar Abbas
  - Abul Hussain
  - Muhammad Zeeshan Hassan
  - Lei Zhou
  citation_count: 6
  full_citation: '>'
  full_text: ">\nCitation: Abbas, A.; Zhang, Z.;\nZheng, H.; Alami, M.M.; Alrefaei,\n\
    A.F.; Abbas, Q.; Naqvi, S.A.H.; Rao,\nM.J.; Mosa, W.F.A.; Abbas, Q.; et al.\n\
    Drones in Plant Disease Assessment,\nEfﬁcient Monitoring, and Detection:\nA Way\
    \ Forward to Smart Agriculture.\nAgronomy 2023, 13, 1524. https://\ndoi.org/10.3390/agronomy13061524\n\
    Academic Editor: Xingang Xu\nReceived: 5 April 2023\nRevised: 23 May 2023\nAccepted:\
    \ 26 May 2023\nPublished: 31 May 2023\nCopyright:\n© 2023 by the authors.\nLicensee\
    \ MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed\n\
    under\nthe\nterms\nand\nconditions of the Creative Commons\nAttribution (CC BY)\
    \ license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nagronomy\nReview\n\
    Drones in Plant Disease Assessment, Efﬁcient Monitoring, and\nDetection: A Way\
    \ Forward to Smart Agriculture\nAqleem Abbas 1,2,†\n, Zhenhao Zhang 1,†, Hongxia\
    \ Zheng 1, Mohammad Murtaza Alami 3\n,\nAbdulmajeed F. Alrefaei 4\n, Qamar Abbas\
    \ 5, Syed Atif Hasan Naqvi 6,*\n, Muhammad Junaid Rao 7\n,\nWalid F. A. Mosa 8\n\
    , Qamar Abbas 9, Azhar Hussain 2, Muhammad Zeeshan Hassan 6 and Lei Zhou 1,*\n\
    1\nState Key Laboratory for Managing Biotic and Chemical Threats to the Quality\
    \ and Safety of Agro-Products,\nInstitute of Agro-Product Safety and Nutrition,\
    \ Zhejiang Academy of Agricultural Sciences,\nHangzhou 310021, China; aqlpath@gmail.com\
    \ (A.A.); fangzhenhao1016@126.com (Z.Z.);\nhxzh_bio@126.com (H.Z.)\n2\nDepartment\
    \ of Agriculture and Food Technology, Karakoram International University, Gilgit\
    \ 15100, Pakistan;\nazhar.hussain21@kiu.edu.pk\n3\nDepartment of Crop Cultivation\
    \ and Farming System, College of Plant Science and Technology,\nHuazhong Agricultural\
    \ University, Wuhan 430070, China; murtazaalami@webmail.hzau.edu.cn\n4\nDepartment\
    \ of Biology, Jamoum University Collage, Umm Al-Qura University, Makkah 21955,\
    \ Saudi Arabia;\nafrefaei@uqu.edu.sa\n5\nDepartment of Computer Sciences, University\
    \ of Karachi, Karachi 75270, Pakistan; qmarabbas715@gmail.com\n6\nDepartment of\
    \ Plant Pathology, Bahauddin Zakariya University, Multan 60800, Pakistan;\nranazeeshanhassan824@gmail.com\n\
    7\nState Key Laboratory for Conservation and Utilization of Subtropical Agro-Bioresources,\
    \ Guangxi Key\nLaboratory of Sugarcane Biology, College of Agriculture, Guangxi\
    \ University, Nanning 530004, China;\nmjunaidrao@gxu.edu.cn\n8\nPlant Production\
    \ Department (Horticulture-Pomology), Faculty of Agriculture, Saba Basha,\nAlexandria\
    \ University, Alexandria 21531, Egypt; walidmosa@alexu.edu.eg\n9\nDepartment of\
    \ Plant Sciences, Karakoram International University, Gilgit 15100, Pakistan;\n\
    qamar.abbasbio@kiu.edu.pk\n*\nCorrespondence: atifnaqvi@bzu.edu.pk (S.A.H.N.);\
    \ zhoul@zaas.ac.cn (L.Z.)\n†\nThese authors contributed equally to this work.\n\
    Abstract: Plant diseases are one of the major threats to global food production.\
    \ Efﬁcient monitoring\nand detection of plant pathogens are instrumental in restricting\
    \ and effectively managing the spread\nof the disease and reducing the cost of\
    \ pesticides. Traditional, molecular, and serological methods\nthat are widely\
    \ used for plant disease detection are often ineffective if not applied during\
    \ the initial\nstages of pathogenesis, when no or very weak symptoms appear. Moreover,\
    \ they are almost useless\nin acquiring spatialized diagnostic results on plant\
    \ diseases. On the other hand, remote sensing\n(RS) techniques utilizing drones\
    \ are very effective for the rapid identiﬁcation of plant diseases in\ntheir early\
    \ stages. Currently, drones, play a pivotal role in the monitoring of plant pathogen\
    \ spread,\ndetection, and diagnosis to ensure crops’ health status. The advantages\
    \ of drone technology include\nhigh spatial resolution (as several sensors are\
    \ carried aboard), high efﬁciency, usage ﬂexibility, and\nmore signiﬁcantly, quick\
    \ detection of plant diseases across a large area with low cost, reliability,\n\
    and provision of high-resolution data. Drone technology employs an automated procedure\
    \ that\nbegins with gathering images of diseased plants using various sensors\
    \ and cameras. After extracting\nfeatures, image processing approaches use the\
    \ appropriate traditional machine learning or deep\nlearning algorithms. Features\
    \ are extracted from images of leaves using edge detection and histogram\nequalization\
    \ methods. Drones have many potential uses in agriculture, including reducing\
    \ manual\nlabor and increasing productivity. Drones may be able to provide early\
    \ warning of plant diseases,\nallowing farmers to prevent costly crop failures.\n\
    Keywords: plant disease detection; drones; machine learning; precision agriculture;\
    \ image analysis\nAgronomy 2023, 13, 1524. https://doi.org/10.3390/agronomy13061524\n\
    https://www.mdpi.com/journal/agronomy\nAgronomy 2023, 13, 1524\n2 of 26\n1. Introduction\n\
    Plant diseases are responsible for enormous yield losses and for threatening global\n\
    food production [1]; hence, proper detection and reliable diagnostic methods for\
    \ identifying\nthe etiological agents of disease are essential to conserving time\
    \ and money by preventing\nor limiting crop damages [2]. Classically, diseases\
    \ were recognized based on traditional\nmethods; these methods, often subjective,\
    \ were strictly dependent on the observer and\nthough time-consuming overall,\
    \ were prone to inaccuracy. Additionally, human scouting\nis expensive and, in\
    \ many cases, impractical due to human error and/or the occurrence\nof cryptic\
    \ when not mild symptoms, making diagnosis at early stages impossible [3].\nTherefore,\
    \ a technologically driven agricultural revolution is important to permanently\n\
    solve the problems mentioned earlier at a reasonable cost with little environmental\
    \ impact.\nWith the continuous adoption of recent advanced technologies such as\
    \ Internet of Things\ndevices, intelligent algorithms, sophisticated sensors,\
    \ and modern machines, agriculture\nhas changed. It is currently changing from\
    \ being accomplished by human workers to using\nsmart agricultural machines and\
    \ robots. Smart agricultural machines and robots have been\ndeveloped which detect\
    \ plant diseases early on and at the same time monitor their long-\ndistance movement\
    \ [3,4]. Many researchers have used high-resolution imagery collected\nfrom satellites,\
    \ airplanes, on-the-ground machines, and drones to identify agricultural\ndiseases.\
    \ Satellites and airplanes can cover vast areas in a short amount of time. However,\n\
    satellites and airplanes have poor spatial and temporal image resolutions compared\
    \ to\ndrones and are highly susceptible to weather conditions that can affect\
    \ overﬂight [3–5].\nTherefore, aerial remote sensing (RS) using drones (Unmanned\
    \ Aerial Vehicles (UAV)\nor Unmanned Aerial Systems (UAS)) with intelligent visual\
    \ systems may be an efﬁcient and\ninexpensive way for farmers to detect crop and\
    \ plant diseases in a variety of agricultural\nﬁelds, from the most intimate greenhouse\
    \ to the largest farm [3–9].\nDigital (red, blue, and green or RBG), multispectral,\
    \ hyper-spectral, ﬂuorescent, and\nthermal infrared-based imaging sensors paired\
    \ with effective algorithms mounted on\ndrones can efﬁciently detect, differentiate,\
    \ and quantify the severity of the symptoms\ninduced by various pathogens under\
    \ ﬁeld conditions [10,11], as conﬁrmed by the plethora\nof studies conducted on\
    \ important cereal crops such as, rice [12], maize [13], wheat [14,15],\nfruit\
    \ trees (including citrus [16], olive [17], and grapevine [18]), vegetables (including\n\
    potatoes [19], soybeans [20], and tomatoes [21]), and many forest trees, such\
    \ as, pine [3],\nthat have demonstrated the reliability of drones for diagnostic\
    \ purposes.\nDrones are equipped with digital, multispectral, hyperspectral, thermal,\
    \ and ﬂuo-\nrescence sensors which offer ﬁner resolution of plant diseases and\
    \ assist in plant disease\ndetection at earlier stages than is possible with satellite\
    \ systems [12]. Data acquired by\ndrones can be simultaneously sampled by their\
    \ autonomous systems at various heights\nin the atmosphere; these data can then\
    \ be rapidly elaborated to provide forecasting mod-\nels across ﬁelds, regions,\
    \ and even whole continents [22]. Finally, information can be\ndelivered to farmers,\
    \ allowing them to make appropriate decisions regarding timely man-\nagement of\
    \ disease. Hence, precision agriculture (Smart Agriculture) may beneﬁt greatly\n\
    from using drone remote sensing technology because of its cheap cost and high-ﬂying\n\
    ﬂexibility [17–20]. There are a large number of studies on using drone platforms\
    \ with\ndifferent sensors for plant disease sensing. For example, drones were\
    \ equipped with a\nhyperspectral image sensor to obtain an image of winter wheat\
    \ yellow rust and realize\nits effective detection. Similarly, multispectral imaging\
    \ and a drone system were used to\nexplore myrtle rust on myrtle, and infested\
    \ corn plants were detected with drones using\nvisible light images from digital\
    \ cameras [12–15].\nEffective algorithms are required to analyze the images gathered\
    \ by drones. Tradi-\ntional machine learning methods have shortcomings due to\
    \ their reliance on manual feature\nextraction methods, which is especially ineffective\
    \ in complex environments. Deep learning\nalgorithms have recently emerged as\
    \ a promising new alternative to enhance computer\nvision-based systems for autonomous\
    \ crop disease monitoring. Without any human as-\nsistance, they can perform autonomous\
    \ feature extraction, providing farmers with data\nAgronomy 2023, 13, 1524\n3\
    \ of 26\nthat might improve crop yields and decrease treatment costs. A prominent\
    \ area of study at\npresent is the use of computer vision methods, deep learning\
    \ algorithms, and drone-based\nplatforms for the early and accurate diagnosis\
    \ of a wide variety of plant diseases [23].\nHowever, despite being highly efﬁcient,\
    \ low-cost, ﬂexible, accurate, and quick at ﬁeld scale,\ntheir limited ﬂight duration\
    \ makes drones unsuitable for data acquisition within large\nareas, and their\
    \ ability to carry heavy sensors is limited. Thus, the choice of a speciﬁc drone\n\
    and the selection of the sensors, software, algorithms, and settings of the drones\
    \ are critical\nfor achieving the best performance [24]. Keeping in view the importance\
    \ of drones in\nplant disease diagnosis, the following parts have been included\
    \ in this review: (1) methods\nfor plant disease detection, including old and\
    \ new generations; (2) types of sensors and\ncameras mounted on drones; (3) types\
    \ of drones; (4) novel approaches to detecting plant\ndiseases, focusing on drones;\
    \ and (5) drone applications for plant disease detections using\ntraditional and\
    \ deep learning algorithms.\n2. Plant Disease Detection\n2.1. Methods for the\
    \ Detection of Plant Disease: The “Old Generation”\nAppropriate and reliable evaluation\
    \ of crops’ phytosanitary status, intended as the obser-\nvation of occurrence\
    \ and outbreak of plant diseases, is very important, as timely estimation of\n\
    disease incidence, symptom severity, and the resulting impacts on economically\
    \ important\ncrops is decisive for managing agronomical interventions such as\
    \ pesticide application time.\nThe methods for disease detection have been categorized\
    \ into direct and indirect methods [25],\nas shown in Figure 1. Direct methods,\
    \ known as “old generation” methods, include traditional\n(symptomology, microscopy,\
    \ and incubation method), molecular diagnostic methods (e.g.,\npolymerase chain\
    \ reaction (PCR), rapid fragment length polymorphisms (RFLP), real-time\nPCR,\
    \ loop-mediated isothermal amplification (LAMP), recombinase polymerase amplification\n\
    (RPA), and point-of-care diagnostic methods), and serological methods [25]. However,\
    \ due\nto their slowness and low capacity, these methods are not well-suited for\
    \ implementation in\nthe field, delaying early detection and response to disease\
    \ outbreaks. To effectively prevent\nand control future outbreaks, a quick and\
    \ high-throughput approach for the early detection\nof plant diseases must be\
    \ developed., Traditional methods usually follow the evaluation\nof characteristic\
    \ disease symptoms and visible signs of the pathogens. The evaluation of\ndisease\
    \ symptoms is performed by trained experts and can be affected by temporal variations.\n\
    Moreover, traditional methods strictly depend on individual experience, and these\
    \ methods\nbecome accurate and reliable only if the guidelines and standards for\
    \ assessment are properly\nfollowed. Microscopic identification depends on the\
    \ observation of pathogen inoculum (e.g.,\nmycelia, spores, and fruiting bodies).\
    \ For microscopic methods, specific dichotomous keys\nand identification manuals\
    \ are available; however, due to the need to cultivate the pathogens\non using\
    \ artificial selective media before proceeding to identification, this method\
    \ is too\ntime-consuming [26] (Figure 1).\nMolecular and serological methods are\
    \ commonly utilized in quarantines departments\nand research institutes for detecting\
    \ and identifying phytopathogens, and can be applied\ndirectly in the greenhouse\
    \ or the ﬁeld. For example, to assess the presence of the potato\nviruses Phytophthora\
    \ infestans, Ralstonia salanacarum, Ervinia amylovora, Papillus mosaic virus,\n\
    and Tomato Mosaic Virus, a lateral ﬂow-through version of ELISA is often used\
    \ [19]. The\nmajor drawbacks of molecular and serological methods are that they\
    \ are time-consuming\nand require trained operators; in addition, it should be\
    \ mentioned that the amount of\npathogen inoculum does not always positively correlate\
    \ with the severity of the disease.\nFurthermore, these methods are particularly\
    \ unreliable at the asymptomatic stages of plant\npathogens [27], even though\
    \ they are very sensitive, accurate, and effective; unfortunately,\nthey are unsuitable\
    \ for monitoring cryptic pathogens that have entered the plants before\nshowing\
    \ visible symptoms. On top of that, the sampling method from the ﬁeld to the\n\
    laboratory is laborious and should be properly sampled. Additionally, few diseases\
    \ can\nbe detected in only a few plants [5]. The advantages and disadvantages\
    \ of serological and\nmolecular assays have been displayed in Table 1.\nAgronomy\
    \ 2023, 13, 1524\n4 of 26\nTable 1. Advantages and disadvantages of molecular-based\
    \ and serological assays for disease detection and diagnosis.\nAdvantages\nDisadvantages\n\
    Molecular assays (Nucleic acid-based methods)\nRapidly and accurately detecting\
    \ and quantifying pathogen\nIt can be used in open ﬁelds, orchards, or greenhouses.\n\
    Capability to detect a single target in multiple targets\nRapid and speciﬁc detection\
    \ of multiple targets\nPotential to detect uncultivable pathogens such as viruses\
    \ or some bacteria\nand phytoplasma.\nEfﬁcient and speciﬁc\nSample preparation\
    \ is critical and requires reproducible and efﬁcient protocols.\nNot always effective\
    \ with all types of plant material\nUnreliable, particularly at pre-symptomatic\
    \ stages.\nFalse negatives (DNA target sequence is degraded or reagents are of\
    \ insufﬁcient quality)\nFalse positives (Small sample sizes may misrepresent the\
    \ real situation.\nand sample cross-contamination, dead pathogen)\nSensitivity\
    \ problems due to inhibitors of transcriptase and/or polymerases\nMis-priming\
    \ or primer dimerizations\nHigh cost of equipment and reagents\nTime-consuming\n\
    Unable to detect early infection\nSerological assays\nHigh throughput potential\n\
    Sensitive\nLow equipment costs\nGood reliability\nPolyclonal antisera cross-reactivity\n\
    Monoclonal antibodies recognize one epitome only and are generally more expensive.\n\
    Antibodies’ shelf life is short.\nTime-consuming\nLow potential for spatialization\n\
    False negatives\nAgronomy 2023, 13, 1524\n5 of 26\nAgronomy 2023, 13, x FOR PEER\
    \ REVIEW \n4 of 27 \n \n \n \nFigure 1. Available methods for plant disease detection:\
    \ (A,B) direct methods and (C,D) indirect \nmethods [5]. Direct methods include\
    \ traditional, serological, and molecular methods, while indirect \nmethods include\
    \ biomarker-based approaches such as metabolite proﬁling from plant pathogens\
    \ \nand plant interactions and stress-based approaches such as remote sensing\
    \ using drones. \nMolecular and serological methods are commonly utilized in quarantines\
    \ depart-\nments and research institutes for detecting and identifying phytopathogens,\
    \ and can be \napplied directly in the greenhouse or the ﬁeld. For example, to\
    \ assess the presence of the \npotato viruses Phytophthora infestans, Ralstonia\
    \ salanacarum, Ervinia amylovora, Papillus mo-\nsaic virus, and Tomato Mosaic\
    \ Virus, a lateral ﬂow-through version of ELISA is often used \n[19]. The major\
    \ drawbacks of molecular and serological methods are that they are time-\nconsuming\
    \ and require trained operators; in addition, it should be mentioned that the\
    \ \namount of pathogen inoculum does not always positively correlate with the\
    \ severity of \nthe disease. Furthermore, these methods are particularly unreliable\
    \ at the asymptomatic \nstages of plant pathogens [27], even though they are very\
    \ sensitive, accurate, and eﬀective; \nunfortunately, they are unsuitable for\
    \ monitoring cryptic pathogens that have entered the \nplants before showing visible\
    \ symptoms. On top of that, the sampling method from the \nﬁeld to the laboratory\
    \ is laborious and should be properly sampled. Additionally, few \ndiseases can\
    \ be detected in only a few plants [5]. The advantages and disadvantages of \n\
    serological and molecular assays have been displayed in Table 1.\nFigure 1. Available\
    \ methods for plant disease detection: (A,B) direct methods and (C,D) indirect\n\
    methods [5]. Direct methods include traditional, serological, and molecular methods,\
    \ while indirect\nmethods include biomarker-based approaches such as metabolite\
    \ proﬁling from plant pathogens and\nplant interactions and stress-based approaches\
    \ such as remote sensing using drones.\n2.2. Methods for the Detection of Plant\
    \ Disease: The “New Generation”\nIndirect methods, known as “New Generation”,\
    \ essentially exploit biomarker-based\ntechniques such as metabolite proﬁling\
    \ from plant–pathogen interactions as well as stress-\nbased detection techniques\
    \ such as imaging and spectroscopy using drones [3]. Recently,\nvarious indirect\
    \ methods have been launched, in particular drones, which can estimate\ndisease\
    \ more accurately compared to molecular, serological, and microbiological diagnostic\n\
    techniques [3]. Sensors have been mounted on drones to measure reﬂectance, tempera-\n\
    ture, or ﬂuorescence. Sensors of various types have been developed (RGB, multispectral,\n\
    hyperspectral, thermal, and ﬂuorescence), representing emerging tools for the\
    \ detection,\nidentiﬁcation, and quantiﬁcation of plant diseases, as shown in\
    \ Table 2 [11,28]. Sensors are\nthe key components of any drone that allow it\
    \ to navigate, detect, and locate potential crop\ndiseases from visual data and\
    \ to provide a map of the condition of the crops that could be\nuseful to farmers\
    \ or other machines collaborating with the drones to carry out various tasks\n\
    autonomously with little or no human involvement. The advantages and disadvantages\n\
    of various sensors mounted on drones are shown in Table 1. The accuracy and use\
    \ of\nmultispectral and hyperspectral images for disease diagnosis are greatly\
    \ improved. This is\nbecause of the sensitivity of spectral measurements to stress\
    \ and change during a crop’s\ndevelopment and with disease severity. Nonetheless,\
    \ implementing a hyperspectral data\nacquisition protocol in the ﬁeld presents\
    \ signiﬁcant challenges. Several elements might\naffect spectral reﬂectance, including\
    \ technical characteristics (resolution, brightness, etc.),\nsample preparation\
    \ circumstances (laboratory or ﬁeld), and sample characteristics (size,\ntexture,\
    \ humidity, etc.). More research into reﬂectance using crop vegetation indices\
    \ is\nneeded throughout crop development and infection. Thermal sensors are particularly\n\
    beneﬁcial in identifying plant diseases, complementing RGB and hyperspectral imaging.\n\
    The primary impetus is that leaf temperature is a useful indicator of plant health.\
    \ Because\nplant leaf acquisition requires people to drill down the whole ﬁeld\
    \ to acquire images, which\nis an energy- and time-consuming strategy, several\
    \ researchers have explored this type\nAgronomy 2023, 13, 1524\n6 of 26\nof imaging\
    \ for disease detection approaches at the leaf level, and others have combined\n\
    these images with multispectral data for effective early detection at the ground\
    \ vehicle and\naerial vehicle level. Drones have greatly aided the process of\
    \ agricultural monitoring at\nthe plot size, including identifying plant diseases.\
    \ For this, a drone equipped with many\ndifferent cameras was deployed. The captured\
    \ photos were used with machine learning\nalgorithms to classify crop health quickly\
    \ and accurately. Hence, drones are becoming\nmore common, as spectral imaging\
    \ with drones provides valuable information on soil\nand the top portion of plants\
    \ over a broad spectrum. Two basic categories can be used to\ncategorize remote\
    \ sensing systems based on camera sensors installed on drone platforms,\nnamely,\
    \ drone type and camera sensor type. Drone-based aerial imaging is one of the\n\
    most signiﬁcant and beneﬁcial data types that can help advance the agricultural\
    \ area. The\ngoal of the desired application and the crop type are typically considered\
    \ when selecting\ndrone platforms and sensor types [10,11]. These RS approaches\
    \ rely on the detection of\nany variation in the optical properties of plants;\
    \ in other words, they essentially detect\nany change in the plant physiology\
    \ that, due to biotic or abiotic stresses, transpiration\nrates, morphology, plant\
    \ density, and changes in solar radiation between plants, determines\nmeasurable\
    \ variations in plants optical output. Due to signiﬁcant advantages such as high\n\
    spatial resolution (compared to satellite RS), high efﬁciency, low cost, and ﬂexibility\
    \ of\nuse, RS platforms play an important role in the application of precision\
    \ agriculture. With\nthe help of this technique, plant diseases and disorders\
    \ can be detected at the ﬁeld level\npromptly and accurately, thereby improving\
    \ disease management efﬁcacy through the\nuse of site-speciﬁc applications of\
    \ fungicides [29]. Furthermore, the movement of plant\npathogens or their products\
    \ can be traced from tens to hundreds of meters above crop\nﬁelds [12], and numerous\
    \ plant disease images can be captured directly and in real-time,\nallowing application\
    \ of algorithms to monitor the occurrence of speciﬁc plant diseases\n(Table 2).\n\
    Drones equipped with sensors can measure spectral and morphological information\n\
    such as plant height and canopy surface proﬁling. The advantages and disadvantages\
    \ of\ndrone utilization in agriculture are presented in Figure 2. Moreover, at\
    \ high altitudes the\ncaptured images usually have low spatial resolution, making\
    \ it difﬁcult to detect features\nof disease lesions at the level of plant organs,\
    \ even though super-resolution methods\nhave recently been developed that can\
    \ produce a high-resolution image from one or more\nlow-resolution images [29]\
    \ (Figure 2).\nAgronomy 2023, 13, x FOR PEER REVIEW \n8 of 27 \n \nrecently been\
    \ developed that can produce a high-resolution image from one or more low-\nresolution\
    \ images [29] (Figure 2). \n \nFigure 2. Advantages and challenges/disadvantages\
    \ of using drones for plant disease detection [5]. \nPlant morphological information\
    \ is acquired through two main methods: LiDAR \n(Light Detection and Ranging)\
    \ [24] and Structure-from-Motion (SfM) photogrammetry. \nLiDAR calculates the\
    \ distance from the sensor to ground objects to measure their position; \nits\
    \ beams can pass through the crop canopy and send back information about its structure,\
    \ \nplant density, and the ground surface. SfM photogrammetry collects images\
    \ from multi-\nFigure 2. Advantages and challenges/disadvantages of using drones\
    \ for plant disease detection [5].\nAgronomy 2023, 13, 1524\n7 of 26\nTable 2.\
    \ Sensors mounted on drones for plant disease detection and monitoring.\nRemote\
    \ Sensors\nAdvantages\nDisadvantages\nDiseases\nDigital camera (RGB)\nVegetation\
    \ characteristics may capture grayscale or\ncolor pictures, and the visible spectrum\
    \ allows for\nimproved disease identiﬁcation at the leaves’ level.\nLightweight,\
    \ inexpensive, extremely easy to use,\nsimple data processing, and minimal work\n\
    environments.\nReduced number of spectral bands and\nvisibility of less light.\
    \ Vulnerable to\nenvironmental factors.\nCotton bacterial angular, Ascochyta\n\
    blight, grapefruit citrus canker, sugar\nbeet Cercospora leaf spot, rust, blights,\n\
    smuts, spots\nMultispectral camera\nLow cost, fast frame imaging and high, more\
    \ robust\nthan RGB cameras, work efﬁciency; electromagnetic\nspectrum ranging\
    \ from the visible to the\nNear-Infrared (NIR), allowing the calculation of\n\
    different robust vegetation indices; sensing and\nrecording radiations from the\
    \ visible and invisible\nportions of the electromagnetic spectrum.\nFew bands,\
    \ discontinuous spectrum, and low\nspectral resolution\nBlights, Blasts, viruses\n\
    Hyperspectral sensing\nCapable of sensing and recording a wide variety of\nnarrow\
    \ bands and continuous spectra, giving\nresearchers and farmers more insight into\
    \ the\nspectral properties of illnesses and crops\nmore expensive\nBlasts, Blights,\
    \ and nematodes, viruses,\nrots, scabs, rusts\nThermal infrared cameras (InfraRed\
    \ (IR)\nregion consists of several spectral bands,\nincluding Near InfraRed (NIR),\
    \ Short-Wave\nInfra-Red (SWIR), Mid-Wave InfraRed\n(MWIR), Long-Wave InfraRed\
    \ (LWIR), and\nFar InfraRed (FIR))\nSensitive to infrared spectrum, therefore\
    \ it may be\nused day or night and is able to provide more data\non plant health\
    \ than other sensors.\nProblems with the images’ temporal and\ngeographic resolutions;\
    \ issues with the\nweather and lighting; problems with the\nvariety of crop species\
    \ and their development\nstages; problems with the height at which the\nphotographs\
    \ were taken.\nRecently used to monitor diseases such\nas Cercospora leaf spot,\
    \ scab and mildews\nFluorescence imaging\nCan determine how plant responses to\
    \ various\nstresses affect photosynthesis.\nHas been used to detect a few diseases,\
    \ difficult\nto use in field and greenhouse conditions\nmildews, rust and cankers\n\
    References; [24–34]\nAgronomy 2023, 13, 1524\n8 of 26\nPlant morphological information\
    \ is acquired through two main methods: LiDAR (Light\nDetection and Ranging) [24]\
    \ and Structure-from-Motion (SfM) photogrammetry. LiDAR cal-\nculates the distance\
    \ from the sensor to ground objects to measure their position; its beams can\n\
    pass through the crop canopy and send back information about its structure, plant\
    \ density, and\nthe ground surface. SfM photogrammetry collects images from multiple\
    \ perspectives as drones\nfly over the fields; it utilizes high-resolution digital\
    \ cameras from which images can be used to\nmeasure such phenotypical characteristics\
    \ of the plant population as individual height, lodging,\ndevelopmental stages,\
    \ and yield. The spectral reflectance or radiance is an important indica-\ntor\
    \ for the detection of plant vigour, plant diseases, and soil properties [30,31].\
    \ Multispectral\n(usually from 3 to 6 spectral bands, from 0.4 to 1.0 µm) and\
    \ thermal cameras (commonly in the\n7–14 µm range) aboard drones can detect diseases\
    \ in the fields, monitor crop vigour, estimate\nbiomass and yield, and detect\
    \ symptoms of both abiotic and biotic stresses. Digital cameras can\ndetect one\
    \ or a few broad near-infrared (NIR) bands [32], while hyperspectral cameras (tens\
    \ to\nhundreds of spectral bands) measure narrow bands; despite having been reduced\
    \ for drone\nutilization, the latter require extra space and payload capacity\
    \ [8,33,34] (Table 3).\n2.3. The Operating Mechanism of Drones Used to Detect\
    \ Plant Diseases\nDrones are aerial robots that operate independently of a human\
    \ pilot. These aircraft\nmay be piloted by hand from a distance using remote control,\
    \ or they can complete missions\nindependently using a computer running Artificial\
    \ Intelligence (AI) programs. One of the\nmost transformative steps toward “precision\
    \ agriculture” is the widespread use of agricultural\ndrones. Drones can execute\
    \ flying missions at varying heights and viewing angles, allowing\nthem to survey\
    \ hazardous and challenging places previously inaccessible to manned aircraft\
    \ or\nsatellites. Many agricultural tasks, such as detecting and treating crop\
    \ diseases, have recently\nseen widespread use of various drone types fitted with\
    \ high-resolution video sensors. Drones\nhave been categorized into two major\
    \ types based on the movement of wings, i.e., fixed-wing\nand rotary-wing on the\
    \ one hand, and hybrid Vertical Take-Off and Landing (VTOL) drones\non the other\
    \ [24–28]. More advanced cameras and sensors are carried by fixed-wing and\nVTOL\
    \ drones than by multirotor rotary-wing drones, particularly when it comes to\
    \ heavy\nhyperspectral sensors. Drones with advanced cameras can help farmers\
    \ to increase crop\noutput while saving time and money by automating tasks that\
    \ previously required a team\nof people to complete. However, rotary multirotor\
    \ drones can fly at lower altitudes, and\ntheir cameras offer superior Ground\
    \ Sampling Distance resolution. The advantages and\ndisadvantages of both types\
    \ of drones for field-based agricultural applications are shown in\nTable 3. Drone\
    \ systems to detect plant diseases comprise four sections, as shown in Figure\
    \ 3. A\nmechanism depicting the structure and operational mechanism of drone technology\
    \ for plant\ndisease detection recommended by [35] for assessing plant diseases\
    \ is presented in Figure 4\n(Table 3, Figures 3 and 4).\nAs described above, various\
    \ sensors and global positioning system (GPS) capability are\ninstalled on drones\
    \ to capture images [5]. The plant disease detection and classification model\n\
    architecture consist of the following five steps: image acquisition, image preprocessing,\
    \ image\nsegmentation, feature extraction, and classification (Figure 5). Acquiring\
    \ relevant images is the\ninitial stage in crop leaf disease identification and\
    \ categorization. This step aims to amass the\nphoto dataset utilized later in\
    \ the procedure. The drones’ cameras are used for this purpose.\nBetter results\
    \ may be achieved with proper image preparation [5–10]. Image processing can be\n\
    employed to remove background noise. Digital cameras’ large file sizes necessitate\
    \ the use of\nshrinking methods, which additionally aids in making memory smaller.\
    \ The cropping of leaves\nfrom captured photos is one of the most common images\
    \ preprocessing procedures, along\nwith color changes, resizing, background removal,\
    \ enhancing, flipping, rotating, shearing, and\nsmoothing. Crop leaf disease detection\
    \ and categorization rely heavily on image segmentation.\nThe picture is segmented\
    \ into several areas. Through a deep dive into the picture data, relevant\ndetails\
    \ are found for feature extraction. There are two main approaches to image segmentation:\n\
    those that focus on similarities, and those that focus on discontinuities. Feature\
    \ extraction\ninvolves isolating certain aspects of an image’s content. Shape,\
    \ color, and texture are often\nAgronomy 2023, 13, 1524\n9 of 26\nused in plant\
    \ disease identification and categorization. Several categories of crop diseases\
    \ can\ncause visual differences in the resulting images. The technique for detecting\
    \ crop leaf diseases\nuses an image of crop leaves to quickly and accurately identify\
    \ the diseases present. The\nsecond distinguishing characteristic is its vibrant\
    \ hue, which serves to differentiate between the\nvarious crop leaf diseases.\
    \ The last characteristic, texture, shows how varied color patterns may\nbe seen\
    \ in pictures of crop leaves. Energy, entropy, contrast, correlation, the sum\
    \ of squares,\nsum entropy, cluster shadow, cluster prominence, and homogeneity\
    \ are all characteristics\nof textures. Crop leaf diseases are classified using\
    \ traditional machines and deep learning\nclassification techniques. The main\
    \ way in which deep learning differs from conventional\nmachine learning is in\
    \ the process of feature extraction. In contrast to deep learning, where\nfeatures\
    \ are extracted automatically and used as learning weights, traditional machine\
    \ learning\nmodels manually calculate features [9–11]. Traditional machine learning\
    \ and deep learning\nmodels are discussed below. The images are then analyzed\
    \ by software and can be used to\ncharacterize the evolution of plant disease.\
    \ Color conversion features can be used to convert\ncolored space from obtained\
    \ images to detect areas of quality in the study area. High-resolution\ndigital\
    \ cameras provide higher-resolution pixels larger than RGB [5], the images from\
    \ which\ncan be used to differentiate infected areas from healthy areas, while\
    \ multispectral cameras\n(five-channel devices that can measure plant reflectance\
    \ more accurately than three-channel\ndigital RBG cameras) offer raw images in\
    \ five narrow bands from the red edge of RBG to near-\ninfrared (NIR). On the\
    \ other hand, multispectral images obtained from multispectral cameras\ncan calculate\
    \ different image-based spectral indices such as normalized difference vegetation\n\
    index (NDVI), nonlinear index (NLI), green normalized difference vegetation index\
    \ (GNDVI),\nration vegetation index (RVI), difference vegetation index (DVI),\
    \ normalized difference water\nindex (NDWI), and red edge normalized difference\
    \ vegetation index (RENDVI) [36–38]. These\nindices can be used to quantify different\
    \ levels of disease severity in study fields with accuracy\ngreater than 60%.\
    \ Among all the indices mentioned above, NDVI is the most widely used, and\nis\
    \ directly correlated to plant condition, physiological stress, and photosynthetic\
    \ activity under\nstress conditions. NDVI change maps of different disease severity\
    \ levels were reported to be\napplied in the case of rice sheath blight disease\
    \ caused by Rhizoctonia solani at the field level [39];\nthe generated data illustrated\
    \ that multispectral imagery data could detect the symptoms and\ndevelopment of\
    \ the disease at field scale [40].\nAgronomy 2023, 13, x FOR PEER REVIEW \n10\
    \ of 27 \n \n \nFigure 3. Drone system to detect plant diseases comprising four\
    \ sections: data acquisition, data \npreparation, training, and prediction. The\
    \ system interacts with the target area (ﬁelds, farms, or \nforests) directly\
    \ or indirectly to obtain information. The information is preprocessed and arranged\
    \ \ninto features, then sent to a supervised machine learning classiﬁer that processes\
    \ the data and pro-\nvides prediction reports via segmented images. The data acquisition\
    \ process includes drones which \nindirectly acquire airborne data and an expert\
    \ who acquires direct ground data through visual as-\nsessment of the ﬁelds. Drones\
    \ consist of high-performance brushless rotors, speciﬁc load capacity, \nand dimensions.\
    \ They are controlled automatically using ground station software which controls\
    \ \nthe route, speed, and altitude at a distance. \nFigure 3. Drone system to\
    \ detect plant diseases comprising four sections: data acquisition, data prepara-\n\
    tion, training, and prediction. The system interacts with the target area (fields,\
    \ farms, or forests) directly\nor indirectly to obtain information. The information\
    \ is preprocessed and arranged into features, then\nsent to a supervised machine\
    \ learning classifier that processes the data and provides prediction reports\n\
    via segmented images. The data acquisition process includes drones which indirectly\
    \ acquire airborne\ndata and an expert who acquires direct ground data through\
    \ visual assessment of the fields. Drones\nconsist of high-performance brushless\
    \ rotors, specific load capacity, and dimensions. They are controlled\nautomatically\
    \ using ground station software which controls the route, speed, and altitude\
    \ at a distance.\nAgronomy 2023, 13, 1524\n10 of 26\nTable 3. Advantages and disadvantages\
    \ of using drones for agricultural applications [8,34].\nDrones\nAdvantages\n\
    Disadvantages\nRotary-wing (Multirotor)\n•\nTake off and land vertically\n•\n\
    Return home capability\n•\nHigh-detailed plant measurements\n•\nAutomatic recovery\
    \ capability\n•\nThe lift generated by batteries\n•\nHovering and ﬂying at low\
    \ altitudes\nDepending on multiple rotors e.g., three rotors (tri-copters), four\
    \ rotors\n(quad-copters), six rotors (hexacopters), and eight rotors (octocopters)\n\
    •\nSmall payload sensor capacity\n•\nMost of the capacity taken by batteries\n\
    •\nLow endurance\n•\nLow speed\n•\nThe area covered is limited\nFixed-wing.\n\
    •\nThe lift created by wings\n•\nLonger ﬂight\n•\nHigh speed\n•\nHigh endurance\n\
    •\nHigh payload sensor capacity\n•\nSome have automatic recovery capability\n\
    •\nFly a longer time and are more suitable for covering larger areas.\n•\nHigh\
    \ altitude\n•\nFly at an airspeed above their stall speed; sometimes problems\n\
    occur in generating the desired data about the crops\n•\nLow ﬂexibility makes\
    \ small crop monitoring very difﬁcult\n•\nRequire runways and space to land and\
    \ take off\nHybrid Vertical Take-Off and Landing\n(VTOL)\n•\nResolve issues with\
    \ multirotor and ﬁxed-wing drones\n•\nCombine ﬁxed-wing drone cruise ﬂight with\
    \ multirotor drone\nVTOL capabilities.\n•\nLonger ﬂight times, extensive coverage,\
    \ quick vertical takeoffs,\nand minimal energy requirements\n•\nExpensive and\
    \ hovering issues\nAgronomy 2023, 13, 1524\n11 of 26\nAgronomy 2023, 13, x FOR\
    \ PEER REVIEW \n11 of 27 \n \n \n \nFigure 4. The plant disease detection and\
    \ classiﬁcation model architecture consist of image acquisi-\ntion, image preprocessing,\
    \ image segmentation, feature extraction, and classiﬁcation. Sensors and \ncameras\
    \ are mounted on drones that acquire plant disease images. The images are then\
    \ processed \nand segmented to remove background noise. Then, features are extracted\
    \ and classiﬁed using tra-\nditional machine learning models such as K-nearest\
    \ neighbor (KNN) or support vector machine \n(SVM) and deep learning models such\
    \ as Convolutional Neural Network (CNN). Note: PCA, prin-\ncipal component analysis;\
    \ LESC, local embedding based on spatial coherence algorithm; DWT, dis-\ncrete\
    \ wavelet transform. \nAs described above, various sensors and global positioning\
    \ system (GPS) capability \nare installed on drones to capture images [5]. The\
    \ plant disease detection and classiﬁcation \nmodel architecture consist of the\
    \ following ﬁve steps: image acquisition, image prepro-\ncessing, image segmentation,\
    \ feature extraction, and classiﬁcation (Figure 5). Acquiring \nrelevant images\
    \ is the initial stage in crop leaf disease identiﬁcation and categorization.\
    \ \nThis step aims to amass the photo dataset utilized later in the procedure.\
    \ The drones’ cam-\neras are used for this purpose. Better results may be achieved\
    \ with proper image prepara-\ntion [5–10]. Image processing can be employed to\
    \ remove background noise. Digital cam-\neras’ large ﬁle sizes necessitate the\
    \ use of shrinking methods, which additionally aids in \nmaking memory smaller.\
    \ The cropping of leaves from captured photos is one of the most \ncommon images\
    \ preprocessing procedures, along with color changes, resizing, back-\nground\
    \ removal, enhancing, ﬂipping, rotating, shearing, and smoothing. Crop leaf disease\
    \ \ndetection and categorization rely heavily on image segmentation. The picture\
    \ is seg-\nmented into several areas. Through a deep dive into the picture data,\
    \ relevant details are \nfound for feature extraction. There are two main approaches\
    \ to image segmentation: those \nthat focus on similarities, and those that focus\
    \ on discontinuities. Feature extraction in-\nvolves isolating certain aspects\
    \ of an image’s content. Shape, color, and texture are often \nused in plant disease\
    \ identiﬁcation and categorization. Several categories of crop diseases \ncan\
    \ cause visual diﬀerences in the resulting images. The technique for detecting\
    \ crop leaf \nFigure 4. The plant disease detection and classiﬁcation model architecture\
    \ consist of image acqui-\nsition (A), image preprocessing (B), image segmentation\
    \ (C), feature extraction (D), and classiﬁca-\ntion (E). Sensors and cameras are\
    \ mounted on drones that acquire plant disease images. The images\nare then processed\
    \ and segmented to remove background noise. Then, features are extracted and clas-\n\
    siﬁed using traditional machine learning models such as K-nearest neighbor (KNN)\
    \ or support vector\nmachine (SVM) and deep learning models such as Convolutional\
    \ Neural Network (CNN). Note:\nPCA, principal component analysis; LESC, local\
    \ embedding based on spatial coherence algorithm;\nDWT, discrete wavelet transform.\n\
    Agronomy 2023, 13, x FOR PEER REVIEW \n12 of 27 \n \n \ndiseases uses an image\
    \ of crop leaves to quickly and accurately identify the diseases pre-\nsent. The\
    \ second distinguishing characteristic is its vibrant hue, which serves to diﬀeren-\n\
    tiate between the various crop leaf diseases. The last characteristic, texture,\
    \ shows how \nvaried color patterns may be seen in pictures of crop leaves. Energy,\
    \ entropy, contrast, \ncorrelation, the sum of squares, sum entropy, cluster shadow,\
    \ cluster prominence, and \nhomogeneity are all characteristics of textures. Crop\
    \ leaf diseases are classiﬁed using tra-\nditional machines and deep learning\
    \ classiﬁcation techniques. The main way in which \ndeep learning diﬀers from\
    \ conventional machine learning is in the process of feature ex-\ntraction. In\
    \ contrast to deep learning, where features are extracted automatically and used\
    \ \nas learning weights, traditional machine learning models manually calculate\
    \ features [9–\n11]. Traditional machine learning and deep learning models are\
    \ discussed below. The im-\nages are then analyzed by software and can be used\
    \ to characterize the evolution of plant \ndisease. Color conversion features\
    \ can be used to convert colored space from obtained \nimages to detect areas\
    \ of quality in the study area. High-resolution digital cameras pro-\nvide higher-resolution\
    \ pixels larger than RGB [5], the images from which can be used to \ndiﬀerentiate\
    \ infected areas from healthy areas, while multispectral cameras (ﬁve-channel\
    \ \ndevices that can measure plant reﬂectance more accurately than three-channel\
    \ digital RBG \ncameras) oﬀer raw images in ﬁve narrow bands from the red edge\
    \ of RBG to near-infrared \n(NIR). On the other hand, multispectral images obtained\
    \ from multispectral cameras can \ncalculate diﬀerent image-based spectral indices\
    \ such as normalized diﬀerence vegetation \nindex (NDVI), nonlinear index (NLI),\
    \ green normalized diﬀerence vegetation index \n(GNDVI), ration vegetation index\
    \ (RVI), diﬀerence vegetation index (DVI), normalized \ndiﬀerence water index\
    \ (NDWI), and red edge normalized diﬀerence vegetation index \n(RENDVI) [36–38].\
    \ These indices can be used to quantify diﬀerent levels of disease sever-\nity\
    \ in study ﬁelds with accuracy greater than 60%. Among all the indices mentioned\
    \ \nabove, NDVI is the most widely used, and is directly correlated to plant condition,\
    \ phys-\niological stress, and photosynthetic activity under stress conditions.\
    \ NDVI change maps \nof diﬀerent disease severity levels were reported to be applied\
    \ in the case of rice sheath \nblight disease caused by Rhizoctonia solani at\
    \ the ﬁeld level [39]; the generated data illus-\ntrated that multispectral imagery\
    \ data could detect the symptoms and development of the \ndisease at ﬁeld scale\
    \ [40]. \n \nFigure 5. Remote sensing using drone technology to determine plant\
    \ health status, such as canopy \nstructure (including leaf areas and orientation),\
    \ spatial arrangement, and roughness aﬀected by dis-\nease, as well as further\
    \ optical, thermal, and dielectric characteristics of vegetation. \nFigure 5.\
    \ Remote sensing using drone technology to determine plant health status, such\
    \ as canopy\nstructure (including leaf areas and orientation), spatial arrangement,\
    \ and roughness affected by\ndisease, as well as further optical, thermal, and\
    \ dielectric characteristics of vegetation.\nAgronomy 2023, 13, 1524\n12 of 26\n\
    Digital and multi-spectral cameras have been used to capture high-resolution im-\n\
    ages in ﬁeld areas. Color-infrared (CIR) images can be generated by drones to\
    \ support\ndecision-making followed by RGB images. Other types of images used\
    \ for disease detection\ninclude visible and near-infrared (V-NIR) images, thermal\
    \ images, and multispectral (MS)\nimages. Field-based images have been mostly\
    \ generated using drones, followed by leaf\nand plant-based images [39–41]. Before\
    \ data are collected, the optimum exposure time\nfor different cameras is chosen\
    \ based on weather conditions; actual parameters are set\naccording to program\
    \ instructions. Cameras are set in drones during ﬂights, usually at\naltitudes,\
    \ using one to cover all experimental plots and the other to cover speciﬁc plots\
    \ in\neach image. Drones are then directed to move along experimental plots, usually\
    \ with wind\ndirections and speciﬁc ﬂights at a certain speed, depending on the\
    \ speed and direction of\nthe wind. The weather conditions must be favorable for\
    \ a ﬂight to detect plant diseases\nmore accurately. Afterward, software, i.e.,\
    \ ENVI (Exelis Visual Information Solutions,\nBoulder, CO, USA) is used to acquire\
    \ color features from the images, then transform the\nimages into different color\
    \ spaces. Transformation can be used to improve the presentation\nof information,\
    \ allowing the transformed images can be interpreted more easily than the\noriginal\
    \ images [35]. The digital images (RBG bands) of different levels of plant disease\n\
    severity are transformed in hue, lightness, and saturation (HLS); the average\
    \ values of\nthe HLS are calculated along with the different vegetation indices\
    \ (VI) from the acquired\nimages. Afterward, VIs change maps of different levels\
    \ of disease severity are produced to\nillustrate the imagery data that could\
    \ detect the disease at a ﬁeld scale. Along with the aerial\nVI, the ground based\
    \ VI is calculated with special hand-held plant sensors. The function of\nsensors\
    \ is based on the fact that healthy green plant leaves absorb most of the red\
    \ light and\nreﬂect most of the infrared light. The relative strength of the detected\
    \ light directly indicates\nthe density of the foliage within the sensor’s view.\
    \ The more vigorous and denser the\nplants, the greater the differences observed\
    \ between the reﬂected light signals. The sensors\nare held at a certain level\
    \ above the canopy plants, with an oval ﬁeld of view covering\na certain area.\
    \ Multiple readings are taken to increase the accuracy of vegetative indices\n\
    values. The average VI values are calculated from both the healthy and diseased\
    \ areas. On\nthe same day, the disease’s severity is rated on a scale based on\
    \ the symptoms of the disease.\nMoreover, special software (i.e., Pix4D mapper)\
    \ is used to process the images. Software\nsuch as ArcGIS is utilized for geospatial\
    \ data analysis and mapping [5,41,42]. Effective\nalgorithms are required for\
    \ the analysis of the images gathered by the drones. Traditional\nmachine learning\
    \ methods include nine different types of machine learning classiﬁers\nused to\
    \ create models for early detection of disease: k-nearest neighbors (k-NN), support\n\
    vector machine (SVM), Gaussian processing, decision tree, random forest, and multilayer\n\
    perceptron artiﬁcial neural network (MLP-ANN). However, traditional machine learning\n\
    methods have many shortcomings due to their reliance on manual feature extraction\
    \ meth-\nods, which is especially ineffective in complex environments. Deep learning\
    \ algorithms\nhave recently emerged as a promising new alternative to enhance\
    \ computer vision-based\nsystems for autonomous crop disease monitoring. Without\
    \ any human assistance, they can\nperform autonomous feature extraction, providing\
    \ farmers with data that can improve crop\nyields and decrease treatment costs.\
    \ Therefore, a solution for early crop disease detection\ncould be combining modern\
    \ drones, cameras, sensor technologies, and deep learning algo-\nrithms. Deep\
    \ learning algorithms include state-of-the-art deep learning object identiﬁcation\n\
    algorithms such as You Only Look Once version 3 (YOLOv3) and Faster Region-based\n\
    Convolutional Neural Network (CNN). Convolutional Neural Networks (CNNs) have\
    \ been\naround since the development of the AlexNet architecture in 2012. Numerous\
    \ CNN-based\ndeep learning algorithms and architectures are currently in use for\
    \ disease detection and\nclassiﬁcation in various crop systems. Current plant\
    \ disease categorization methods make\nextensive use of well-established CNN architectures\
    \ in computer vision, such as AlexNet,\nGoogleNet, VGGNet, ResNet, and EfﬁcientNet\
    \ [41–50].\nTherefore, a solution for early crop disease detection could be combining\
    \ modern\ndrone camera sensor technologies and deep learning algorithms [19–22].\
    \ To better identify\nAgronomy 2023, 13, 1524\n13 of 26\nplant diseases quickly\
    \ and accurately, scientists are actively studying how to use computer\nvision\
    \ methods, deep learning algorithms, and drone platforms dedicated to diagnosing\n\
    plant diseases. Several ﬁelds, including agriculture, electronic control, remote\
    \ sensing\ntechnologies, computer vision, and artiﬁcial intelligence, must be\
    \ addressed to enable\ndrone platforms to achieve autonomous detection and treatment\
    \ of crop diseases [19–21].\nThus, integrating modern drone camera sensor technologies\
    \ with deep learning algorithms\nmay provide a useful answer for the timely diagnosis\
    \ of crop diseases (Figure 4).\n2.4. Novel Approaches to Detecting Plant Diseases,\
    \ including RS Combined with Drones\nPlants can be affected simultaneously by\
    \ several plant pathogens, such as nematodes,\nfungi, viruses, viroids, bacteria,\
    \ and phytoplasmas. Recent novel approaches have been\nused that can rapidly,\
    \ easily, and reliably detect plant pathogens at pre-symptomatic to\nearly stages\
    \ of plant diseases, when symptoms are unclear and appear on few plants. This\n\
    method includes Lateral ﬂow microarrays [43], Analysis of Volatile Organic Compounds\n\
    (VOCs) as biomarkers [44], Remote sensing (RS) drone usages [45], electrochemistry\
    \ [46],\nPhage display [47], and biophotonics [48].\nLateral ﬂow microarrays (LFM)\
    \ are a hybridization-based nucleic acid detection\nmethod that uses an easily\
    \ visualized calorimetric signal to detect plant pathogens rapidly.\nHowever,\
    \ this method depends on the availability of strong and reliable host and pathogen\n\
    biomarkers discovered through transcriptomics and metabolomics approaches [49].\
    \ A\nclass of interesting plant metabolites highly suitable for plant health evaluation\
    \ are Volatile\nOrganic Compounds (VOCs) as biomarkers; plants are known to release\
    \ VOCs into their\nimmediate proximity for most various biological and ecological\
    \ purposes, with these com-\npounds being responsible for growth, defence, survival,\
    \ and intercommunication with\nother surrounding and/or associated organisms [28].\
    \ Representing a mediation tool for\nplant-to-plant and plant-to-pathogen communication,\
    \ VOCs released from the leaf sur-\nfaces are known as terminal metabolites and\
    \ can reﬂect the physiological status of plants.\nHowever, a single VOC biomarker\
    \ is insufﬁcient to represent a speciﬁc plant disease [49].\n•\nOther techniques\
    \ include electrochemistry, biophotonic and phage display. Phage\ndisplay technology\
    \ identiﬁes ligands that connect to speciﬁc biological molecules.\nThe ligands\
    \ can be used as antigens or immunogens to diagnose plant diseases. The\nligands\
    \ may be peptides or antibody fragments. The other methods, electrochemistry\n\
    and biophotonics, are based on signal transduction and biorecognition principles.\n\
    Optical biosensors are based on the absorption or emission of light due to biological\n\
    or chemical reactions. However, electrochemical biosensors are based on biochemical\n\
    reactions that cause electron transfer in plant sap or any other solution. Environmental\n\
    factors do not usually inﬂuence electrochemical biosensors. The underlying principle\n\
    of these plant disease detection methods is the recognition of a speciﬁc antigen\
    \ by\na speciﬁc antibody to form a stable complex, as with other serological assays\
    \ [50].\nThese biophotonic-based sensors can be used to rapidly detect plant disease\
    \ at the\nasymptomatic stage in the orchards and ﬁeld conditions. Moreover, they\
    \ could be\nintegrated into other plant disease detection methods using drones.\
    \ However, they\nare not easily available in the market.\n•\nAs previously reported,\
    \ remote sensing (RS) is based on measuring electromagnetic\nradiations reﬂected/backscattered\
    \ or emitted from the surface target object. The infor-\nmation is obtained without\
    \ any physical contact with the targeted object. Therefore,\nRS measurements are\
    \ known as non-contact measurements [45]. Hence, RS is a non-\ncontact technique.\
    \ Therefore, portable tools and various platforms such as drones\nwhich sense\
    \ the plants health and retrieve information are being used. To sense infor-\n\
    mation about plants’ health, passive sensors are being widely used. Active sensors\n\
    measure the reﬂected radiations from diseased plants, while passive sensors measure\n\
    the reﬂected solar radiation in the electromagnetic spectrum’s visible, near-infrared,\n\
    and shortwave regions. Hence, RS is used to monitor the changes in plant health.\n\
    Because plant leaves not only reﬂect radiations, transmit or absorb but release\
    \ energy\nAgronomy 2023, 13, 1524\n14 of 26\nby ﬂuorescence [51] or thermal emission\
    \ [52], different pigments found in plants ab-\nsorb radiation in speciﬁc parts\
    \ of the electromagnetic spectrum; for example, plant\npigments in chlorophyll\
    \ absorb radiation in the visible spectrum from 400–700 nm.\nTherefore, there\
    \ is an inverse relationship between the amount of radiation reﬂected\nfrom plants\
    \ and the amount of radiation absorbed by the plant pigments. When the\nplant\
    \ is infected by a pathogen or under abiotic stress conditions, variables such\
    \ as leaf\narea index (LAI), chlorophyll content, or surface temperature change.\
    \ These changes\nare called spectral signatures, and vary from the signatures\
    \ of healthy and unstressed\nplants [53,54]. However, RS presents drawbacks as\
    \ well; high costs for drones, and\nspecialized experts are required to gather\
    \ and process plant disease data. Moreover,\nthough protocols are available, they\
    \ are concentrated on only a few diseases of valu-\nable crops. Recently, the\
    \ spatial resolution of satellite sensors has been increased and\nthe cost of\
    \ acquisition of plant disease data has decreased, making RS a promising\ntool\
    \ for integration with traditional plant disease methods. Today, small, inexpensive,\n\
    high-resolution spatial and spectral sensors have been mounted on drones for crop\n\
    disease monitoring at the farm scale [6,55,56]. Hence, drone imaging offers interesting\n\
    advantages over RS. Acquiring images using drones has become a common practice\n\
    because installing onboard digital cameras is very easy [56].\nIn summary, methods\
    \ of crop disease monitoring are being improved through differ-\nent RS technologies.\
    \ Integrating drone-mounted spectral sensor data with spectroscopy,\nﬂuorescence,\
    \ and thermal imaging data, along with other non-RS based methods to provide\n\
    more accurate and fruitful plant disease detection and diagnosis, remains a work\
    \ progress.\n3. Applications of Drones for Plant Disease Detection\nResearchers\
    \ are investing in identifying infected and uninfected leaves as well as\nin categorizing\
    \ various disease severity degrees with visual symptoms even before the\nmanifestation\
    \ of visual symptoms [45]. Modern methods for disease identiﬁcation make\nuse\
    \ of machine learning algorithms to sift through information gathered through\
    \ a variety\nof acquisition methods. For the goal of disease identiﬁcation, using\
    \ drones with traditional\nmachine learning methods and deep learning models are\
    \ discussed below.\n3.1. Traditional Learning Models Used to Identify Plant Diseases\
    \ with Drones\nTraditional machine-learning techniques are applied for plant disease\
    \ identiﬁcation\nutilizing drone images. Backpropagation NN (BPNN) was an early\
    \ model that was used to\napply spectral data collected from remote sensing hyperspectral\
    \ photographs of tomato\nplants to estimate infection severity on plant leaves\
    \ from photos. A ﬁve-stage rating\nsystem was then used to assess the severity\
    \ of the light blight in the photos and test the\nBPNN using that information.\
    \ The ﬁndings supported the feasibility of using ANN with\nbackpropagation for\
    \ spectrum prediction for disease diagnosis. The authors of [56] made\nsimilar\
    \ efforts to use the Classiﬁcation and Regression Tree model to identify leafroll\
    \ illness.\nTheir strategy relied on analyzing hyperspectral photos of grapevines\
    \ taken by drones.\nSimilarly, the authors of [57] used multispectral photos taken\
    \ by drones to extract spectral\nbands, vegetation indicators, and biophysical\
    \ properties of both damaged and healthy\nplants. Due to their high accuracy in\
    \ making predictions, SVM models are widely utilized\nin the ﬁeld of plant disease\
    \ diagnosis. SVM was used for close-range hyperspectral imaging\nof barley to\
    \ identify drought stress at an early stage. Red Edge Normalized Difference\n\
    Vegetation Index (RENDVI) and Plant Senescence Reﬂectance Index (PSRI) were used\
    \ in\nthe model’s training process. Misclassiﬁcation can be further minimized\
    \ by the use of\nseveral SVM classiﬁers based on color, texture, and shape features\
    \ for disease identiﬁcation\non plant leaves [46–49].\nRemote sensed data were\
    \ initially used by the University of North Dakota, USA, where\nfarmers were involved\
    \ in verifying the effectiveness of fungicide applications against plant\ndiseases\
    \ in a sugar beet crop [57]. Moreover, the loss due to accidental spray drift\
    \ of\nfungicides was quantiﬁed. Remote sensor data have been used to investigate\
    \ physical\nAgronomy 2023, 13, 1524\n15 of 26\ndamage due to pests, inundation,\
    \ wind, and hail as well. In this instance, growers and\nranchers in rural areas\
    \ were connected via satellite. Farmers were given a ﬁrst-time\nopportunity to\
    \ use high-resolution imagery, which allowed them to identify crop stress\ndue\
    \ to diseases, pests, and damage. The information obtained was then used to draw\n\
    boundaries around crop stress areas.\nIn another study, it was found that an accurate\
    \ survey of the damage on sugar beets\nusing ground-based was not possible, as\
    \ sugar beet is more susceptible to disease at a critical\ngrowth stage of plants.\
    \ Therefore, multispectral satellite image data were used to obtain\nan immediate\
    \ and reliable estimate of the damage and reduction in sugar content caused\n\
    by plant diseases. The satellite images were used to assess the variations within\
    \ the ﬁeld\nbefore the damage inﬂicted on the plants by plant diseases, pests,\
    \ or other abiotic factors.\nThe results revealed the importance of high-resolution\
    \ imagery for timely assessment of\ndamage due to both biotic and abiotic factors\
    \ [57].\nThe concept of using high-resolution imagery from drones to capture images\
    \ of\nhealthy and diseased plants has evolved recently. Rice sheath blight is\
    \ a major disease in\nrice worldwide. In one study, drones equipped with digital\
    \ and multispectral cameras\ncaptured images of research plots with 67 cultivars\
    \ and a few elite lines. The ground-based\nnormalized difference vegetation index\
    \ and image-based normalized difference vegetation\nindex were calculated, and\
    \ the relationship between the two NDVI data indices showed\na strong correlation.\
    \ Multispectral images were then used to quantify the different levels\nof rice\
    \ sheath blight disease in ﬁeld plots with an accuracy higher than 60%. The results\n\
    indicated that drones with digital and multispectral cameras are the most effective\
    \ tool\nfor detecting rice sheath blight disease in the ﬁeld [5]. Researchers\
    \ in the USA are now\nusing drones to detect Septoria wheat fungus before any\
    \ appearance of symptoms or signs,\nwhich allows farmers to stop infections in\
    \ their tracks. The cameras mounted on the drones\ncan automatically detect the\
    \ early stages of Septoria wheat fungus. These data can be\nused to inform farmers\
    \ when to spray before the Septoria fungus damages the wheat\ncrop. The diseased\
    \ plants display a unique spectral signature that distinguishes them from\nSeptoria-free\
    \ plants [39,58,59].\nA study of multispectral detection of fungal diseases in\
    \ barley was conducted using\ndrone imagery. For this, two farmers’ ﬁelds and\
    \ four growth stages of barley (Feekes 8 to\nFeekes 11.4) were involved in determining\
    \ the spectral response (VI) of different barley\nfungicide treatment levels (Control,\
    \ Stratego, Stratego + Prosaro) using multispectral drone\nimagery (Blue 475 nm\
    \ ± 20 nm; Green 560 nm ± 20 nm; Red 668 nm ± 10 nm; Red Edge\n717 nm ± 10 nm;\
    \ Near-Infrared 840 nm ± 40 nm) with a 6.7 cm/pixel spatial resolution.\nAmong\
    \ the ﬁve vegetation indices (NDVI, RE-NDVI, RDVI, RE-RDVI, and TGI), three-way\n\
    interactions (Field × Growth Stage × Treatment) were found to be non-signiﬁcant\
    \ for NDVI\n(p = 0.415), RE-NDVI (p = 0.383), and TGI (p = 0.780), while RDVI\
    \ (p = 0.003) and RE-RDVI\n(p = 0.005) were signiﬁcant. Moreover, a consistent\
    \ trend in the spectral separability of\nfungal severity by the treatment type\
    \ was observed. Tracking the fungicide intensity was\nmade possible through mapping\
    \ and comparison with ground truth in order to save the\nenvironment from the\
    \ overuse of fungicides [58].\nIn another study, super-resolution to low-resolution\
    \ images from drones were used\nto address tomato disease. About fifty thousand\
    \ images of fourteen crops including\ntomato as target crop were obtained. Images\
    \ of eight kinds of disease caused by plant\npathogens were obtained, i.e., Xanthomonas\
    \ campestris pv. vesicatoria, Alternaria solani,\nPhytophthora infestans, Septoria\
    \ lycopersici, Tomato mosaic virus, Fulvia fulva, Corynespora\ncassiicola, and\
    \ Tomato yellow leaf curl virus. Moreover, symptoms such as lesions on plant\n\
    organs were obtained. Diseases of tomatoes were classified into three categories,\
    \ i.e.,\nhigh-resolution, low-resolution, and super-resolution images. The results\
    \ indicated that\nsuper-resolution methods are more effective than conventional\
    \ image scaling methods\nbecause they enhance the spatial resolution of disease\
    \ images [60]. To check the field re-\nsistance of potatoes against late blight\
    \ diseases, other researchers used a disease severity\nscale to visually examine\
    \ the lesion size or infection on the leaves. This visual assessment\nAgronomy\
    \ 2023, 13, 1524\n16 of 26\nis generally a time-consuming process that results\
    \ in only a tentative guess [61]. To avoid\nthe visual assessment technique, [62]\
    \ developed a new technique for estimating late\nblight disease severity under\
    \ field conditions using digital RBG cameras from a drone.\nVarious potato cultivars\
    \ and lines were planted in 262 experimental plots for assessment\nof the disease\
    \ resistance of potatoes under field conditions. Along with the conventional\n\
    visual assessment of disease severity using late blight disease severity, eleven\
    \ aerial im-\nages of the field were obtained. A special image processing protocol\
    \ was developed for\nthe study to estimate the disease severity. Further, the\
    \ estimation method was designed\nsuch that the error of the severity estimated\
    \ by image processing could be minimized\nwhen compared with the visual assessment.\
    \ The area under the disease progress curve\nwas then compared with that from\
    \ the visual assessment and time series of images, and\nthe coefficient of determination\
    \ was found to be higher than 0.7. Following year eleven,\nimages of the field\
    \ were obtained, and surprisingly, the coefficient of determination was\nagain\
    \ higher than 0.7. These results lead to the conclusion that the correlations\
    \ are valid\nand that image acquisition using drones followed by disease severity\
    \ estimations from\nthese images is a more effective method than the conventional\
    \ visual assessments. In\nconclusion, the aerial imagery was precise and objective\
    \ and allowed high throughput\nconcerning field resistance to late blight disease.\n\
    Another study used high-resolution aerial imaging for Huanglongbing (HLB) or citrus\n\
    greening disease detection using drones [63]. A multi-band imaging sensor was\
    \ connected\nto drones at the desired resolution by adjusting the ﬂying altitude\
    \ used to acquire images.\nThe results achieved with drone-based sensors were\
    \ then compared with aircraft-based\nsensors with lower spatial resolution. The\
    \ data consisted of seven vegetation indices (Vis)\nand six spectral bands with\
    \ a wavelength range from 530 to 900 nm. Regression analysis\nwas used to obtain\
    \ relevant features from the drone-based and aircraft-based spectral\nimages.\
    \ The results revealed that high-resolution aerial sensing is a reliable method\
    \ for\ndetecting HLB-infected citrus trees.\nIn Japan, viral-infected potato tubers\
    \ were taken from the field to provide certified\nseed tubers in potato seed production\
    \ areas. The farmers inspected the whole field for\ninfected potato plants based\
    \ on virus-induced visual symptoms. This is time-consuming,\nand plants showing\
    \ cryptic or mild symptoms are very difficult to identify. Japanese sci-\nentists\
    \ have devised an alternative way to detect diseased potato plants effectively.\
    \ They\nused the image classification technique as a detection method for virus-infected\
    \ plants,\nusing drones to obtain RGB images at an altitude of 5 to 10 m from\
    \ the ground. A total\nof 1300 images of healthy and 130 images of infected potato\
    \ plants were collected. They\nrotated the original images in order to increase\
    \ the number of images of infected plants\nto 1300, for a total of 2600 images\
    \ which include equal numbers of infected and healthy\nplants. Of these, they\
    \ used 1800 images for the training set and the remaining 800 images\nas the validation\
    \ set. Moreover, a convolution neural network (CNN) was used for\nclassifying\
    \ the images as infected or healthy plants. The accuracy of classification on\
    \ the\ntraining data was about 96% while the classification accuracy of the validation\
    \ data was\n84% [62].\nOther researchers have compared spectral, hyperspectral,\
    \ canopy height, and tem-\nperature information derived from handheld and drone-mounted\
    \ sensors to discriminate\nfour soybean cyst nematode (SCN) susceptible and tolerant\
    \ cultivars. The spectral indices\n(SIs) used to differentiate the cultivars were\
    \ chlorophyll, nitrogen, and water contents. In\nthe advanced stages, when SCN\
    \ infection becomes severe, the discrimination between the\ncultivars was found\
    \ to be more prominent. In addition, canopy height allowed for more\neffective\
    \ differentiation between the cultivars using drones than manual ﬁeld assessment.\n\
    Canopy temperature and SIs were used to classify the cultivars according to their\
    \ ability to\nwithstand SCN. A high correlation was found between SIs and ﬁnal\
    \ sugar beet yield. These\nresults prove that the drone hyperspectral imaging\
    \ approach is suitable for the detection of\nplant diseases caused by nematodes\
    \ [63,64].\nAgronomy 2023, 13, 1524\n17 of 26\nIn September 2018, researchers\
    \ from New Mexico State University used multispectral\ncameras to detect plant\
    \ stresses in ﬁelds and parks during a drone ﬂight. They obtained red\nand near-infrared\
    \ spectral bands, and from these bands, they calculated NDVI, which is a\nplant\
    \ stress metric. The NDVI ranges were from −1 to 1. NDVI values closer to 1 indicate\n\
    that a plant is green and healthy, while NDVI values closer to −1 indicate that\
    \ a plant is\nstressed, resulting in green images for healthy plants and red images\
    \ for stressed plants.\nThe researchers collected data by ﬂying a drone 20 m above\
    \ the ﬁeld at about ﬁve miles\nper hour during summer, looking for plants stressed\
    \ from root-eating nematodes. Before\nthe ﬂight, the drones were programmed by\
    \ autopilot software with GPS coordinates, and\ncameras took a shot every ﬁve\
    \ meters. There were a total of 60 shots which generated\n300 images; each time\
    \ the camera triggered, ﬁve images were captured. The images were\nprocessed with\
    \ Pix4D post-processing software for drone-based imagery, and NDVI values\nwere\
    \ obtained. Moreover, ground NDVI values have been collected using a hand-held\n\
    NDVI meter [8,34,65]. Soybean is an important agricultural commodity in Brazil\
    \ that\nsuffers from various foliar diseases caused by fungi, bacteria, viruses,\
    \ and nematodes.\nThese diseases have considerably reduced soybean production\
    \ in different states. The\nauthors of [20] designed a computer vision system\
    \ to monitor these foliar diseases in the\nﬁeld. The images were captured by drones,\
    \ than a computer vision system based on the\nSimple Linear Iterative Clustering\
    \ (SLIC) superpixel algorithm was used to detect plant\nleaves in the images.\
    \ The SLIC group pixel (SLIC superpixels method) is also known as\nthe SLIC segmentation\
    \ method. Then, a dataset was obtained from the captured images.\nFinally, features\
    \ were extracted and foliar diseases were classiﬁed based on the super-pixel\n\
    segment. Each super-pixel segment was associated with a speciﬁc class of foliar\
    \ disease or\nhealthy leaf samples.\nDrones are being used to monitor physiological\
    \ stress and disease outbreaks in forest\ntrees as well. The authors of [66] used\
    \ drones to monitor a disease outbreak in mature\nPinus radiate. A time-series\
    \ multi-spectral camera was mounted on drones and ﬂown over\nan herbicide-applied\
    \ pine forest area at regular intervals. Meanwhile, a forest experiment\ncarried\
    \ out a traditional ﬁeld-based assessment of crown and needle discoloration. The\n\
    results revealed that multi-spectral imagery collected from drones was very useful\
    \ in\nidentifying physiological stress in mature pine trees during the earliest\
    \ stages. Physiological\nstress was detected over the red edge and near-infrared\
    \ bands. Furthermore, NDVI was\nfound to be an effective vegetation index for\
    \ the detection of discoloration caused by\nphysiological stress over time.\n\
    Red band needle blight is a very severe disease of pine trees in the United Kingdom\n\
    (UK). The authors of [67] used a ﬁxed-wing drone (Quest UAV Ltd., Amble, UK) equipped\n\
    with thermal sensors to monitor red band needle blight (Dothistroma pini Hulbary)\
    \ disease-\ninduced canopy temperatures in ﬁve research plots of Scots pine and\
    \ lodgepole pine within\nQueen Elizabeth Forest Park in central Scotland. The\
    \ drone was equipped with a TIR PI450\ncamera (Optris GmbH, Berlin, Germany) and\
    \ a VNIR DMC-LX5 digital camera (Panasonic\nLtd., Osaka, Japan). In summer 2014,\
    \ TIR and NIR datasets were collected above the forest\nplots. Data regarding\
    \ ALS, hyperspectral, and thermal data were obtained by the Natural\nEnvironment\
    \ Research Council Airborne Research and Survey Facility. Experts assessed\nthe\
    \ severity of Red Band Needle Blight in the ﬁeld by visually assessing the symptoms.\
    \ For\nTIR and VNIR imagery obtained from the drone, a maximum local ﬁlter was\
    \ applied to the\nALS CHM, which allowed the identiﬁcation of treetops and tree-to-tree\
    \ registration. Six\ncentral pixels from each crown tree were averaged, and the\
    \ canopy temperature of trees\nwas extracted. The values were then compared with\
    \ the severity levels measured visually\nin the ﬁeld. A moderate positive correlation\
    \ was found between tree temperature and\ndisease progression [68].\nMyrtle rust\
    \ caused by Austropuccinia psidii causes signiﬁcant losses to paperbark tea\n\
    trees in New South Wales Australia. For aerial mapping of paperbark tea trees,\
    \ drones con-\ntaining hyperspectral image sensors were integrated with data processing\
    \ algorithms using\nmachine learning. Headwall Nano-Hyperspec R cameras were mounted\
    \ on the drones,\nAgronomy 2023, 13, 1524\n18 of 26\nand imagery was obtained.\
    \ The imagery was processed in Python programming language\nusing eXtreme Gradient\
    \ Boosting (XGBoost) with the Geospatial Data Abstraction Library\n(GDAL) and\
    \ Scikit-learn third-party libraries. About 11,385 samples were extracted and\n\
    assigned to ﬁve classes, divided into three classes for background objects and\
    \ two classes\nfor deterioration status. The results revealed that the individual\
    \ detection rate was 97.24%\nfor healthy trees and 94.72% for affected trees,\
    \ while the multiclass detection rate was\n97.35%. The methodology was useful\
    \ for the acquisition of large datasets with freeware\ntools using drones [69].\n\
    Grapevines in European vineyards are affected by Flavescence dorée (FD) and\n\
    Grapevine Trunk Disease (GTD). These diseases cause damage to vineyards. To detect\n\
    symptomatic vines, multispectral drone imagery can be a powerful tool. However,\n\
    different kinds of diseases produce similar leaf discoloration, as is the case\
    \ with the\nabove diseases in red vine cultivars. The authors of [70] evaluated\
    \ the potential of drones\nto distinguish between symptomatic and asymptomatic\
    \ vines. The study was conducted\nin the southern regions of France. Seven vineyards\
    \ and five different red vine cultivars\nwere selected. Drones acquired multispectral\
    \ images using the MicaSenseRedEdge®\nsensor. The images were processed to obtain\
    \ surface reflectance mosaics at 0.10 m\nground spatial resolution. About 24 variables\
    \ were selected, including five spectral\nbands, fifteen vegetation indices, and\
    \ four biophysical parameters. Vegetation indices\ndifferentiated abnormal vegetation\
    \ behavior with stress and diseases. Leaf pigment\ncontents such as chlorophyll,\
    \ carotenoids, and anthocyanin were the major biophysical\nparameters. The best\
    \ vegetation indices and biophysical parameters were found to be\nthe Red–Green\
    \ Index (RGI)/Green–Red Vegetation Index (GRVI) (based on the green\nand red spectral\
    \ bands) and Car (linked to carotenoid content). These variables were\neffective\
    \ in mapping vines with a disease severity higher than 50%. Currently, the\nauthors\
    \ of [71] have proposed a protocol for using a Multi-Spectral (MS) imaging device\n\
    to detect grapevine diseases. If embedded in a drone, this tool can provide disease\n\
    outbreak locations in a geographical information system, allowing localized and\
    \ direct\ntreatment of infected vines.\nStudies focusing on the use of drone imagery\
    \ to describe changes in crops due\nto diseases remain lacking. The researchers\
    \ in [72] evaluated late blight (Phytophthora\ninfestans) incidence in potato\
    \ using the description of spectral changes related to the\ndevelopment of late\
    \ potato blight under low disease severity levels. For this, they\nused sub-decimeter\
    \ drone optical imagery. The study’s main objective was to acquire\ninformation\
    \ regarding early changes in the potato crop with disease incidence. The drone\n\
    images were obtained on four dates during the growing season pre- and post-detection\n\
    of late blight disease in the field. Simplex Volume Maximization (SiVM) was used\
    \ to\nsummarize the spectral variability.\nMoreover, the relationship with the\
    \ different cropping systems and disease severity\nlevels was established based\
    \ on the pixel-wise log-likelihood ratio (LLR) calculation. It was\nfound that\
    \ considerable spectral changes were related to late blight incidence in different\n\
    cropping systems and the disease severity levels of affected potato plants. In\
    \ conclusion,\ntraditional machine learning algorithms have limitations, and performance\
    \ can readily\nchange across different growth periods and acquisition equipment.\
    \ The feature engineering\nprocess, which causes substantial data loss, may further\
    \ contribute to poor performance.\nMore information on the reported use of drones\
    \ for detection of plant diseases along with\nrecent and past reports regarding\
    \ the sensors/cameras/other devices mounted on drones\nfor the detection of plant\
    \ diseases is shown in Table 4.\nAgronomy 2023, 13, 1524\n19 of 26\nTable 4. Use\
    \ of drones and other devices for plant disease detection.\nS. No.\nDrones\nSensors/Cameras/Other\
    \ Devices\nDisease\nReferences\n1\nDT-18 UAV platform\nMicaSense RedEdge® sensor\n\
    Flavescence dorée (FD) and\nGrapevine Trunk Diseases (GTD)\n[70]\n2\nHexa-rotor\
    \ DJI S800 EVO\nHeadwall Nano-hyperspectral\nMyrtle rust\n[35]\n3\nPhantom 2 Vision+\n\
    Micasense RedEdgeTM\nRice sheath blight\n[5]\n4\nMikrokopter OktoXL\nMultispectral\
    \ camera\nGrapevine leaf stripe disease (GLSD)\n[73]\n5\nLong-range DT-18\nMicaSense\
    \ RedEdgeTM sensor\nGrapevine Disease\n[71]\n6\nHiSystems GmbH Mikrokopter\nSony\
    \ NEX-5N\nPotato late blight\n[61]\n7\nMX-SIGHT\nMulti-spectral sensors\nDowny\
    \ mildew of opium\n[74]\n8\nCoaxial quad-copter\nMicaSense RedEdge 3 camera\n\
    Physiological stress in pine trees\n[75]\n9\nQuestUAV Qpod\nThermal, RGB (red,\
    \ green\nand blue) and NIR (near-infrared) sensors\nRed Band Needle Blight disease\
    \ of pine trees\n[67]\n10\nS800 EVO Hexacopter\nCanon 5DsR camera, multispectral\
    \ MicaSense RedEdge\ncamera, Headwall Nano-Hyperspace\nGrape Phylloxera\n[76]\n\
    11\nDJI Phantom 3\nSony EXMOR sensor\nSoybean Foliar Diseases\n[77]\n12\neBee\n\
    IXUS 127 HS Canon camera\nCeratocystis Wilt in Eucalyptus Crops\n[75]\n13\nHiSystems\
    \ GmbH\nMiniMCA6\nHuanglongbing-infected citrus trees\n[78]\n14\nModiﬁed Sig Rascal\
    \ 110 RC airframe\nGeneral camera\nPotato late blight\n[68]\n15\nQuantalab, IAS-CSIC,\n\
    Multispectral and thermal\nVerticillium wilt of olives\n[74]\n16\nDJI Phantom\
    \ 3 quadcopter\nMicaSense RedEdge multispectral camera\nTomato Spot Wilt Disease\
    \ in Peanuts\n[79]\n17\nPhantom 4\nRGB camera\nFusarium wilt of radish\n[65]\n\
    18\nModel TFX-11\nSporangia-sampling devices\nPotato late blight\n[80]\n19\nmulti-rotor\
    \ MikrokopterOktoXL\nMultispectral and thermal\nWater status within a vineyard\n\
    [69]\n20\nDJI Phantom4\nRGB camera\nPotato Virus Y\n[62]\n21\n3D Robotics\nGamaya\
    \ OXI VNIR 40 camera\nBeet Cyst Nematode in Sugar Beet\n[64]\n22\nDJI Matrice\
    \ 600 pro\nPika L 2.4 (Resonon Inc., Bozeman, MT, USA)\nTarget spot and bacterial\
    \ spot diseases\n[21]\n23\nUAV system (S1000)\nHyperspectral imaging sensor (UHD\
    \ 185)\nYellow rust\n[15]\n24\nFeimaD200 quadrotor.\nModel RedEdge-MX\nPine Wood\
    \ Nematode Disease\n[3]\n25\nMulti-rotor DJI Mavic Pro\nMltispectral sensor (Parrot\
    \ Sequoia)\nOlive quick decline syndrome\n[17]\nAgronomy 2023, 13, 1524\n20 of\
    \ 26\n3.2. Deep Learning Models to Identify Plant Diseases Using Drones\nTo overcome\
    \ the constraints of conventional machine learning, deep learning models\nhave\
    \ been constructed and used for the problem of plant disease identiﬁcation in\
    \ drone\nimages. In the past decade, agriculture has seen promising results from\
    \ applying computer\nvision techniques based on deep learning [81,82]. Diseased\
    \ crops can change color, develop\ntwisted leaves or patches, or lose fruit. Because\
    \ of this, deep learning algorithms may be\nthe best option for diagnosing such\
    \ diseases. Image classiﬁcation, object detection, and\nimage segmentation are\
    \ the three main computer vision-based tasks that can improve\ncrop disease identiﬁcation\
    \ from drone imagery and be used to identify plant diseases.\nThe process of categorizing\
    \ a picture involves identifying the presence of the desired\ndisease across the\
    \ whole input image, known as image classiﬁcation. In most cases, disease\ndetection\
    \ at the leaf level is accomplished through the classiﬁcation task. The aim of\
    \ object\ndetection, on the other hand, aims to build a bounding box around each\
    \ identiﬁed disease\nto determine the class and precise position of the targeted\
    \ disease inside an input picture.\nSemantic segmentation is used to categorize\
    \ each pixel of an image as either diseased or\nnot. The deep learning algorithm\
    \ process for disease detection and classiﬁcation using\ndrones images is as follows:\
    \ (1) collection of data on the target plant disease by selecting a\nsuitable\
    \ ﬂight altitude for the drones; (2) data labeling, augmentation, cleaning, splitting,\n\
    and vegetative index generation; (3) use of models such as VGG or Res Net for\
    \ image\nclassiﬁcation, Faster-R-CNN or YOLO for object detection, and U-Net or\
    \ Seg-Net for image\nsegmentation; and (4) model training/validation and model\
    \ evaluation [81,82].\nThe use of deep learning algorithms applied to the analysis\
    \ of images collected by\ndrones has recently attracted a great deal of attention\
    \ due to its potential to identify plant\ndiseases. Recent research on crop disease\
    \ identiﬁcation using drone photography has relied\nheavily on deep learning models\
    \ to circumvent the shortcomings of more conventional\nmethods, particularly Convolutional\
    \ Neural Network (CNN) algorithms. Most of this\nresearch aims to improve the\
    \ yield of staple crops such as wheat, maize, potato, and\ntomato. For example,\
    \ Zhang et al. [81] developed many computer vision models based\non deep learning\
    \ to identify yellow rust illness and lessen its devastating effects. Using\n\
    multispectral data gathered through a UAV platform, they suggested a new semantic\n\
    segmentation approach derived from the U-Net model to detect wheat crop patches\
    \ afﬂicted\nwith yellow rust disease. There are three modules, namely, the Irregular\
    \ Encoder Module\n(IEM), Irregular Decoder Module (IDM), and Content-aware Channel\
    \ Re-weight Module\n(CCRM), embedded into the basic U-Net architecture as enhancements.\
    \ The authors looked\nat how the format of the input data affected the accuracy\
    \ with which the deep learning\nmodel identiﬁed wheat plants infected with yellow\
    \ rust. According to their ﬁndings, the\nproposed Ir-Unet model outperformed the\
    \ results of Su et al. [82], who only obtained an\nF1-score of 92% when using\
    \ all ﬁve bands of information obtained from the RedEdge\nmultispectral camera.\
    \ Combined with all the raw bands and their varied measurements\nof Selected Vegetation\
    \ Indices (SVIs), they were able to improve the accuracy to 96.97%.\nSimilarly,\
    \ Liu et al. [83] suggested a BPNN model to track Fusarium Head Blight using\n\
    hyperspectral aerial images, and found that it outperformed both SVM and RF, with\
    \ an\noverall accuracy of 98%. Using RGB pictures captured by UAVs, Huang et al.\
    \ [84] focused\non a different wheat disease, Helminthosporium Leaf Blotch Disease.\
    \ It was suggested that a\nLeNet-based CNN model be used to categorize HLBD according\
    \ to illness stage. Compared\nto a collection of methods plus the SVM model, the\
    \ accuracy of the adopted CNN model\nwas higher (91.43%). By merging the visible\
    \ and infrared bands from drone-collected\nphotos, Kerkech et al. [85] developed\
    \ a deep learning-based semantic segmentation system\nto automatically diagnose\
    \ mildew disease in vineyards using RGB photographs, infrared\nimages, and multispectral\
    \ data. The SegNet model was used to determine whether a given\npixel in a picture\
    \ represents a sick leaf or grapevine. Similarly, Northern Leaf Blight (NLB)\n\
    has been the focus of ongoing research, as it represents a signiﬁcant threat to\
    \ the maize\ncrop. Stewart et al. [86] used a DJI Matrice 600 to capture low-altitude\
    \ RGB aerial images\nand then used an instance segmentation approach (Mask R-CNN)\
    \ to identify NLB disease\nAgronomy 2023, 13, 1524\n21 of 26\nfrom these images.\
    \ On average, the suggested method achieved an accuracy of 96% when\ndetecting\
    \ and segmenting individual lesions.\nTo segment UAV-based RGB images into regions\
    \ affected or unaffected by NLB disease,\nWiesner-Hanks et al. [87] combined crowdsourced\
    \ ResNet-based CNN and Conditional\nRandom Field (CRF) techniques, using the crowdsourced\
    \ CNN to generate heatmaps and\nthe CRF to classify each pixel as lesion or non-lesion.\
    \ Using this method, they could detect\nNLB disease in maize crops within a millimeter,\
    \ outperforming the method used by Wu\net al. [88] by more than 2%. To automate\
    \ detection of mildew disease in vineyards, a\ndeep learning-based semantic segmentation\
    \ system was designed to process RGB photos,\ninfrared images, and multispectral\
    \ data obtained from a UAV integrating the visible and\ninfrared bands. The SegNet\
    \ model was used to determine whether a given pixel in an\nimage represented a\
    \ sick leaf or grapevine. Using visible, infrared, fusion AND, and fusion\nOR\
    \ data, the suggested technique obtained accuracies of 85.13%, 78.72%, 82.20%,\
    \ and\n90.23% at the leaf level and 94.41%, 89.16%, 88.14%, and 95.02% at the\
    \ grapevine level,\nrespectively [88]. To enhance the identiﬁcation of unhealthy\
    \ Pinus trees using RGB UAV\ndata, Hu et al. [89] integrated a Deep Convolutional\
    \ Neural Network (DCNN), a Deep\nConvolutional Generative Adversarial Network\
    \ (DCGAN), and an AdaBoost classiﬁer.\nThe suggested method outperformed classic\
    \ machine learning techniques, achieving an\nF1-score of 86.3% and a recall of\
    \ 95.7%, as opposed to recall rates of 78.3% and 65.2%,\nrespectively, for SVM\
    \ and AdaBoost classiﬁers. Deep learning models have drawbacks,\nhowever; for\
    \ example, training takes a long time, perhaps weeks, depending on the size of\n\
    the dataset, the complexity of the model, and the computer’s processing power.\
    \ When it\ncomes to early disease detection in plants, datasets are either insufﬁcient\
    \ or not accessible\nin sufﬁcient quantities. The ﬁrst step is to learn about\
    \ the area’s crop, disease, and pest\npatterns. Researchers typically choose to\
    \ either inoculate the fungus causing the disease\nin an experimental greenhouse\
    \ [81,85] or to watch and capture the natural development\nof an infestation as\
    \ it occurs. To obtain a hyperspectral picture, for instance, one must use\nsophisticated\
    \ high-priced equipment and consult with trained professionals throughout\nthe\
    \ data collection process [81]. In addition, when creating a new dataset, annotation\
    \ is\nrequired. Because the annotation of various diseases is beyond the capabilities\
    \ of ordinary\nvolunteers, this task requires the assistance of agriculture experts.\
    \ To minimize overﬁtting,\nresearchers often resort to data augmentation techniques\
    \ for tiny datasets, although these\ntechniques are not always effective. After\
    \ the data have been collected, they may be skewed,\neither because healthy plant\
    \ samples are more valuable than diseased plant samples or\nbecause of seasonal\
    \ and regional difﬁculties with different types of crop diseases [81,82].\n4.\
    \ Outlook, Future Trends, and Limitations\nTo improve crop output and attain food\
    \ security in vast agricultural areas, monitoring\nand identifying crop/plant\
    \ diseases early on with accuracy, dependability, timeliness, and\nefﬁciency is\
    \ crucial. In this review, we began by describing methods for detecting plant\
    \ dis-\neases, e.g., old and new generations. The old generation includes traditional,\
    \ molecular, and\nserological methods. However, these methods are not well suited\
    \ for the early detection of\nplant diseases. The new generation includes sensor-mounted\
    \ drones that accurately detect\nplant diseases at the earliest stages, enabling\
    \ prevention of future outbreaks by applying\nsuitable management measures. Second,\
    \ we emphasized various sensors that may be\ninstalled on drone platforms, including\
    \ digital, multispectral, hyperspectral, thermal, and\nﬂuorescence sensors. Hyperspectral\
    \ sensors are more robust than digital and multispectral\nsensors, whereas thermal\
    \ sensors can be used during either day or night, providing more\ninsight into\
    \ plant status than other sensors. Third, different types of drones and their\
    \ oper-\nating mechanisms were shown. VTOL and ﬁxed-wing drones can carry more\
    \ cameras and\nsensors than rotary-wing drones. Hence, VTOL and ﬁxed-wing drones\
    \ ﬂy for a longer time\nand can cover large areas. However, these types are expensive\
    \ and can encounter issues\nwhen hovering. Fourth, we looked at novel approaches\
    \ such as lateral ﬂow microarrays,\nbiomarkers, electrochemistry, phage display,\
    \ and biophotonics while focusing on drones.\nAgronomy 2023, 13, 1524\n22 of 26\n\
    Finally, we attempted to explain applications of plant disease detection using\
    \ drone images\nand traditional and deep learning models to obtain better results.\
    \ Deep learning models\nperform much better than traditional machine learning\
    \ models and do away with the\ntime-consuming and error-prone human feature extraction\
    \ process, resulting in improved\nprediction performance. Plant diseases in complicated\
    \ situations with overlapping plant\nleaves have been classiﬁed effectively using\
    \ deep learning models. On the other hand,\ntraditional machine learning models\
    \ cannot discriminate between diseases that have similar\nsymptoms and are unable\
    \ to beneﬁt from more training data. The training of deep learning\nmodels can\
    \ be time-consuming and may take weeks or months depending on the dataset’s\n\
    size, the model’s complexity, and the available computing power. There is a lack\
    \ of or difﬁ-\nculty gaining access to adequate databases for early disease diagnosis\
    \ in plants. Similarly,\nenvironmental, and logistical limitations such as strong\
    \ winds and rain, limited battery life,\nand the requirement for trained personnel\
    \ to initiate and oversee ﬂights are all issues when\nusing drones. Depending\
    \ on the geographic and spectral resolution of satellites, it may\nbe preferable\
    \ to use drones for monitoring plant development in a healthy environment.\nFurthermore,\
    \ current image sensor technology has various drawbacks that prevent it from\n\
    being used for early disease diagnosis. Better crop growth and health status predictions\
    \ can\nbe made by integrating data from different sensors. This explains why researchers\
    \ are more\nfocused on multimodal data fusion for crop disease detection. Data\
    \ fusion takes several\nforms in agriculture, with the most common being the combination\
    \ of satellite and drone\nimagery and the fusion of data from multiple drone sensors.\
    \ The detection procedure for\nactivities such as crop monitoring and plant categorization\
    \ can be enhanced by such data\nfusion approaches. It has become more important\
    \ in contemporary agriculture to develop\nprecise, real-time, dependable, and\
    \ autonomous drone-based systems for detecting plant\ndiseases. These systems\
    \ require sophisticated and effective algorithms to address issues\nsuch as ﬂuctuating\
    \ illumination, growing diseases, occlusion, and shifting perspectives.\nIn order\
    \ to realize improved agricultural yields, it is necessary to integrate cutting-edge\n\
    technologies such as drone systems and deep learning frameworks. The accessibility\
    \ of\nagricultural data is another key issue that must be addressed. To build\
    \ realistic datasets,\nit is necessary to either gather additional data or to\
    \ create complex algorithms based on\ngenerative deep learning architectures.\n\
    5. Conclusions\nIn conclusion, the application of drones in plant disease assessment\
    \ offers efficient moni-\ntoring and detection capabilities for smart agriculture.\
    \ Drones provide increased accessibility,\nimproved coverage, and rapid data collection,\
    \ enabling timely disease detection. With ad-\nvanced sensors and imaging techniques,\
    \ drones can capture valuable data on plant health\nindicators. These data can\
    \ be processed using analytics and machine learning algorithms\nto identify disease\
    \ patterns and assess severity. Integration of drones into plant disease as-\n\
    sessment systems allows for real-time monitoring, early detection, and targeted\
    \ intervention.\nDrones can contribute to sustainable farming practices, minimization\
    \ of yield losses, reduced\nneed for chemical treatments, and support for precision\
    \ agriculture strategies. Continued\nresearch addressing the challenges mentioned\
    \ above can further enhance the potential of\ndrones in plant disease assessment\
    \ for a resilient agricultural future.\nAuthor Contributions: Conceptualization,\
    \ L.Z., A.A., M.M.A. and S.A.H.N.; methodology, L.Z., A.A.\nand M.M.A.; software,\
    \ L.Z., H.Z., Z.Z., Q.A. (Qamar Abbas 1) and S.A.H.N.; validation, L.Z., A.A.\n\
    and M.M.A.; formal analysis, A.A. and M.M.A.; investigation, A.A. and M.M.A.;\
    \ data curation, A.A.;\nwriting—original draft preparation, L.Z., A.A. and M.M.A.;\
    \ writing—review and editing, L.Z., H.Z.,\nZ.Z., Q.A. (Qamar Abbas 1), M.M.A.,\
    \ A.F.A., M.J.R., W.F.A.M., Q.A. (Qamar Abbas 2), A.H., M.Z.H.,\nL.Z. and S.A.H.N.;\
    \ supervision, L.Z. and A.A.; project administration, L.Z., A.A. and S.A.H.N.;\
    \ funding\nacquisition, L.Z. All authors have read and agreed to the published\
    \ version of the manuscript.\nAgronomy 2023, 13, 1524\n23 of 26\nFunding: This\
    \ work was supported by the High-talent Introduction and Continuous Training Fund\
    \ to\nL.Z. (grant no: 10300000021LL05) and Discipline Construction Funds (grant\
    \ no: 10407000019CC2213G),\nsupported by Zhejiang Academy of Agricultural Sciences\
    \ (ZAAS) and State Key Laboratory for Manag-\ning Biotic and Chemical Threats\
    \ to the Quality and Safety of Agro-products (10417000022CE0601G/029).\nData Availability\
    \ Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nNing, Y.; Liu, W.; Wang, G.L. Balancing Immunity\
    \ and Yield in Crop Plants. Trends Plant Sci. 2017, 22, 1069–1079. [CrossRef]\n\
    [PubMed]\n2.\nSingh, A.K.; Ganapathysubramanian, B.; Sarkar, S.; Singh, A. Deep\
    \ Learning for Plant Stress Phenotyping: Trends and Future\nPerspectives. Trends\
    \ Plant Sci. 2018, 23, 883–898. [CrossRef] [PubMed]\n3.\nQin, J.; Wang, B.; Wu,\
    \ Y.; Lu, Q.; Zhu, H. Identifying Pine Wood Nematode Disease Using UAV Images\
    \ and Deep Learning\nAlgorithms. Remote Sens. 2021, 13, 162. [CrossRef]\n4.\n\
    Cui, S.; Ling, P.; Zhu, H.; Keener, H.M. Plant Pest Detection Using an Artiﬁcial\
    \ Nose System: A Review. Sensors 2018, 18, 378.\n[CrossRef] [PubMed]\n5.\nMartinelli,\
    \ F.; Scalenghe, R.; Davino, S.; Panno, S.; Scuderi, G.; Ruisi, P.; Villa, P.;\
    \ Stroppiana, D.; Boschetti, M.; Goulart, L.R.; et al.\nAdvanced methods of plant\
    \ disease detection. A review. Agron. Sustain. Dev. 2015, 35, 1–25. [CrossRef]\n\
    6.\nMahlein, A. Present and future trends in plant disease detection. Plant Dis.\
    \ 2016, 100, 1–11.\n7.\nGe, Y.; Thomasson, J.A.; Sui, R. Remote sensing of soil\
    \ properties in precision agriculture: A review. Front. Earth Sci. 2011, 5,\n\
    229–238. [CrossRef]\n8.\nShi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.; Rooney,\
    \ W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan, C.L.S.; Neely, H.L.;\net al.\
    \ Unmanned Aerial Vehicles for High-Throughput Phenotyping and Agronomic Research.\
    \ PLoS ONE 2016, 11, e0159781.\n[CrossRef]\n9.\nHerrmann, I.; Bdolach, E.; Montekyo,\
    \ Y.; Rachmilevitch, S.; Townsend, P.A.; Karnieli, A. Assessment of maize yield\
    \ and phenology\nby drone-mounted superspectral camera. Precis. Agric. 2020, 21,\
    \ 51–76. [CrossRef]\n10.\nBauriegel, E.; Herppich, W.B. Hyperspectral and chlorophyll\
    \ ﬂuorescence imaging for early detection of plant diseases, with\nspecial reference\
    \ to Fusarium spec. infections on wheat. Agriculture 2014, 4, 32–57. [CrossRef]\n\
    11.\nKuska, M.; Wahabzada, M.; Leucker, M.; Dehne, H.-W.; Kersting, K.; Oerke,\
    \ E.-C.; Steiner, U.; Mahlein, A.-K. Hyperspectral\nphenotyping on the microscopic\
    \ scale: Towards automated characterization of plant-pathogen interactions. Plant\
    \ Methods 2015,\n11, 28. [CrossRef] [PubMed]\n12.\nZhang, D.; Zhou, X.; Zhang,\
    \ J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath blight using an unmanned\
    \ aerial system with\nhigh-resolution color and multispectral imaging. PLoS ONE\
    \ 2018, 13, e0187470. [CrossRef] [PubMed]\n13.\nSun, Q.; Sun, L.; Shu, M.; Gu,\
    \ X.; Yang, G.; Zhou, L. Monitoring Maize Lodging Grades via Unmanned Aerial Vehicle\
    \ Multispectral\nImage. Plant Phenomics 2019, 2019, 5704154. [CrossRef]\n14.\n\
    Khot, L.R.; Sankaran, S.; Carter, A.H.; Johnson, D.A.; Cummings, T.F. UAS imaging-based\
    \ decision tools for arid winter wheat\nand irrigated potato production management.\
    \ Int. J. Remote Sens. 2016, 37, 125–137. [CrossRef]\n15.\nGuo, A.; Huang, W.;\
    \ Dong, Y.; Ye, H.; Ma, H.; Liu, B.; Wu, W.; Ren, Y.; Ruan, C.; Geng, Y. Wheat\
    \ Yellow Rust Detection Using\nUAV-Based Hyperspectral Technology. Remote Sens.\
    \ 2021, 13, 123. [CrossRef]\n16.\nSarkar, S.K.; Das, J.; Ehsani, R.; Kumar, V.\
    \ Towards autonomous phytopathology: Outcomes and challenges of citrus greening\n\
    disease detection through close-range remote sensing. In Proceedings of the 2016\
    \ IEEE International Conference on Robotics and\nAutomation (ICRA), Stockholm,\
    \ Sweden, 16–21 May 2016; pp. 5143–5148.\n17.\nCastrignanò, A.; Belmonte, A.;\
    \ Antelmi, I.; Quarto, R.; Quarto, F.; Shaddad, S.; Sion, V.; Muolo, M.R.; Ranieri,\
    \ N.A.; Gadaleta, G.;\net al. Semi-Automatic Method for Early Detection of Xylella\
    \ fastidiosa in Olive Trees Using UAV Multispectral Imagery and\nGeostatistical-Discriminant\
    \ Analysis. Remote Sens. 2020, 13, 14. [CrossRef]\n18.\nShahi, T.B.; Xu, C.-Y.;\
    \ Neupane, A.; Guo, W. Recent Advances in Crop Disease Detection Using UAV and\
    \ Deep Learning\nTechniques. Remote Sens. 2023, 15, 2450. [CrossRef]\n19.\nFranceschini,\
    \ M.H.D.; Bartholomeus, H.; van Apeldoorn, D.; Suomalainen, J.; Kooistra, L. Assessing\
    \ changes in potato canopy\ncaused by late blight in organic production systems\
    \ through Uav-Based Pushbroom imaging spectrometer. Int. Arch. Photogramm\nRemote\
    \ Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 109–112. [CrossRef]\n20.\nYamamoto, S.;\
    \ Nomoto, S.; Hashimoto, N.; Maki, M.; Hongo, C.; Shiraiwa, T. Monitoring spatial\
    \ and time-series variations in red\ncrown rot damage of soybean in farmer ﬁelds\
    \ based on UAV remote sensing. Plant Prod. Sci. 2023, 26, 36–47. [CrossRef]\n\
    21.\nAbdulridha, J.; Ampatzidis, Y.; Kakarla, S.C.; Roberts, P. Detection of target\
    \ spot and bacterial spot diseases in tomato using\nUAV-based and benchtop-based\
    \ hyperspectral imaging techniques. Precis. Agric. 2020, 21, 955–978. [CrossRef]\n\
    22.\nSchmale, I.I.I.D.G.; Ross, S.D. Highways in the sky: Scales of atmospheric\
    \ transport of plant pathogens. Annu. Rev. Phytopathol.\n2015, 53, 591–611. [CrossRef]\
    \ [PubMed]\n23.\nTallapragada, P.; Ross, S.D.; Schmale, D.G., III. Lagrangian\
    \ coherent structures are associated with ﬂuctuations in airborne\nmicrobial populations.\
    \ Chaos Interdiscip. J. Nonlinear Sci. 2011, 21, 033122. [CrossRef]\nAgronomy\
    \ 2023, 13, 1524\n24 of 26\n24.\nChristiansen, M.P.; Laursen, M.S.; Jørgensen,\
    \ R.N.; Skovsen, S.; Gislum, R. Designing and testing a UAV mapping system for\n\
    agricultural ﬁeld surveying. Sensors 2017, 17, 2703. [CrossRef] [PubMed]\n25.\n\
    Mahlein, A.-K. Plant Disease Detection by Imaging Sensors—Parallels and Speciﬁc\
    \ Demands for Precision Agriculture and Plant\nPhenotyping. Plant Dis. 2016, 100,\
    \ 241–251. [CrossRef]\n26.\nChen, J.-W.; Lau, Y.Y.; Krishnan, T.; Chan, K.-G.;\
    \ Chang, C.-Y. Recent advances in molecular diagnosis of Pseudomonas aeruginosa\n\
    infection by State-of-the-Art, Genotyping Techniques. Front. Microbiol. 2018,\
    \ 9, 1104. [CrossRef]\n27.\nTorres-Sánchez, J.; López-Granados, F.; De Castro,\
    \ A.I.; Peña-Barragán, J.M. Conﬁguration and speciﬁcations of an unmanned\naerial\
    \ vehicle (UAV) for early site speciﬁc weed management. PLoS ONE 2013, 8, e58210.\
    \ [CrossRef] [PubMed]\n28.\nBleecker, A.B.; Kende, H. Ethylene: A gaseous signal\
    \ molecule in plants. Annu. Rev. Cell Dev. Biol. 2000, 16, 1–18. [CrossRef]\n\
    29.\nBarbedo, J.G.A. Factors inﬂuencing the use of deep learning for plant disease\
    \ recognition. Biosyst. Eng. 2018, 172, 84–91.\n[CrossRef]\n30.\nBendig, J.; Bolten,\
    \ A.; Bennertz, S.; Broscheit, J.; Eichfuss, S.; Bareth, G. Estimating biomass\
    \ of barley using crop surface models\n(CSMs) derived from UAV-based RGB imaging.\
    \ Remote Sens. 2014, 6, 10395–10412. [CrossRef]\n31.\nGeipel, J.; Link, J.; Claupein,\
    \ W. Combined spectral and spatial modeling of corn yield based on aerial images\
    \ and crop surface\nmodels acquired with an unmanned aircraft system. Remote Sens.\
    \ 2014, 6, 10335. [CrossRef]\n32.\nYang, C.; Westbrook, J.K.; Suh, C.P.-C.; Martin,\
    \ D.E.; Hoffmann, W.C.; Lan, Y.; Fritz, B.K.; Goolsby, J.A. An airborne multispectral\n\
    imaging system based on two consumer-grade cameras for agricultural remote sensing.\
    \ Remote Sens. 2014, 6, 5257–5278.\n[CrossRef]\n33.\nRango, A.; Laliberte, A.;\
    \ Steele, C.; Herrick, J.E.; Bestelmeyer, B.; Schmugge, T.; Roanhorse, A.; Jenkins,\
    \ V. Using unmanned aerial\nvehicles for rangelands: Current applications and\
    \ future potentials. Environ. Pract. 2006, 8, 159–168. [CrossRef]\n34.\nLaliberte,\
    \ A.S.; Goforth, M.A.; Steele, C.M.; Rango, A. Multispectral remote sensing from\
    \ unmanned aircraft: Image processing\nworkﬂows and applications for rangeland\
    \ environments. Remote Sens. 2011, 3, 2529–2551. [CrossRef]\n35.\nSandino, J.;\
    \ Pegg, G.; Gonzalez, F.; Smith, G. Aerial mapping of forests affected by pathogens\
    \ using UAVs, hyperspectral sensors,\nand artiﬁcial intelligence. Sensors 2018,\
    \ 18, 944. [CrossRef] [PubMed]\n36.\nHuete, A.R. A soil-adjusted vegetation index\
    \ (SAVI). Remote Sens. Environ. 1988, 25, 295–309. [CrossRef]\n37.\nGitelson,\
    \ A.A.; Kaufman, Y.J.; Merzlyak, M.N. Use of a green channel in remote sensing\
    \ of global vegetation from EOS-MODIS.\nRemote Sens. Environ. 1996, 58, 289–298.\
    \ [CrossRef]\n38.\nDevadas, R.; Lamb, D.; Simpfendorfer, S.; Backhouse, D. Evaluating\
    \ ten spectral vegetation indices for identifying rust infection\nin individual\
    \ wheat leaves. Precis. Agric. 2009, 10, 459–470. [CrossRef]\n39.\nAmpatzidis,\
    \ Y.; De Bellis, L.; Luvisi, A. iPathology: Robotic applications and management\
    \ of plants and plant diseases.\nSustainability 2017, 9, 1010. [CrossRef]\n40.\n\
    Mirik, M.; Jones, D.; Price, J.; Workneh, F.; Ansley, R.; Rush, C. Satellite remote\
    \ sensing of wheat infected by wheat streak mosaic\nvirus. Plant Dis. 2011, 95,\
    \ 4–12. [CrossRef]\n41.\nLi, X.; Lee, W.S.; Li, M.; Ehsani, R.; Mishra, A.R.;\
    \ Yang, C.; Mangan, R.L. Spectral difference analysis and airborne imaging\nclassiﬁcation\
    \ for citrus greening infected trees. Comput. Electron. Agric. 2012, 83, 32–46.\
    \ [CrossRef]\n42.\nYang, C.; Odvody, G.N.; Fernandez, C.J.; Landivar, J.A.; Minzenmayer,\
    \ R.R.; Nichols, R.L. Evaluating unsupervised and\nsupervised image classiﬁcation\
    \ methods for mapping cotton root rot. Precis. Agric. 2015, 16, 201–215. [CrossRef]\n\
    43.\nCarter, D.J.; Cary, R.B. Lateral ﬂow microarrays: A novel platform for rapid\
    \ nucleic acid detection based on miniaturized lateral\nﬂow chromatography. Nucleic\
    \ Acids Res. 2007, 35, e74. [CrossRef] [PubMed]\n44.\nBaldwin, I.T.; Halitschke,\
    \ R.; Paschold, A.; Von Dahl, C.C.; Preston, C.A. Volatile signaling in plant-plant\
    \ interactions: “Talking\ntrees” in the genomics era. Science 2006, 311, 812–815.\
    \ [CrossRef] [PubMed]\n45.\nDe Jong, S.M.; Van der Meer, F.D.; Clevers, J.G. Basics\
    \ of remote sensing. In Remote Sensing Image Analysis: Including the Spatial\n\
    Domain; Springer: Berlin/Heidelberg, Germany, 2004; pp. 1–15.\n46.\nGoulart, L.R.;\
    \ Vieira, C.U.; Freschi, A.P.P.; Capparelli, F.E.; Fujimura, P.T.; Almeida, J.F.;\
    \ Ferreira, L.F.; Goulart, I.M.; Brito-Madurro,\nA.G.; Madurro, J.M. Biomarkers\
    \ for serum diagnosis of infectious diseases and their potential application in\
    \ novel sensor\nplatforms. Crit. Rev. Immunol. 2010, 30, 201–222. [CrossRef] [PubMed]\n\
    47.\nEllington, A.D.; Szostak, J.W. In vitro selection of RNA molecules that bind\
    \ speciﬁc ligands. Nature 1990, 346, 818–822. [CrossRef]\n[PubMed]\n48.\nAhmed,\
    \ M.U.; Hossain, M.M.; Tamiya, E. Electrochemical biosensors for medical and food\
    \ applications. Electroanal. Int. J. Devoted\nFundam. Pract. Asp. Electroanal.\
    \ 2008, 20, 616–626. [CrossRef]\n49.\nDegefu, Y.; Somervuo, P.; Aittamaa, M.;\
    \ Virtanen, E.; Valkonen, J.P.T. Evaluation of a diagnostic microarray for the\
    \ detection of\nmajor bacterial pathogens of potato from tuber samples. EPPO Bull.\
    \ 2016, 46, 103–111. [CrossRef]\n50.\nLuppa, P.B.; Sokoll, L.J.; Chan, D.W. Immunosensors—Principles\
    \ and applications to clinical chemistry. Clin. Chim. Acta 2001, 314,\n1–26. [CrossRef]\n\
    51.\nApostol, S.; Viau, A.A.; Tremblay, N.; Briantais, J.-M.; Prasher, S.; Parent,\
    \ L.-E.; Moya, I. Laser-induced ﬂuorescence signatures as\na tool for remote monitoring\
    \ of water and nitrogen stresses in plants. Can. J. Remote Sens. 2003, 29, 57–65.\
    \ [CrossRef]\n52.\nCohen, Y.; Alchanatis, V.; Meron, M.; Saranga, Y.; Tsipris,\
    \ J. Estimation of leaf water potential by thermal imagery and spatial\nanalysis.\
    \ J. Exp. Bot. 2005, 56, 1843–1852. [CrossRef]\nAgronomy 2023, 13, 1524\n25 of\
    \ 26\n53.\nMeroni, M.; Rossini, M.; Colombo, R. Characterization of leaf physiology\
    \ using reﬂectance and ﬂuorescence hyperspectral\nmeasurements. In Optical Observation\
    \ of Vegetation Properties and Characteristics; Research Signpost: Trivandrum,\
    \ India, 2010;\npp. 165–187.\n54.\nWitten, I.H.; Frank, E. Data mining: Practical\
    \ machine learning tools and techniques with Java implementations. ACM Sigmod\
    \ Rec.\n2002, 31, 76–77. [CrossRef]\n55.\nKhanal, S.; Fulton, J.; Shearer, S.\
    \ An overview of current and potential applications of thermal remote sensing\
    \ in precision\nagriculture. Comput. Electron. Agric. 2017, 139, 22–32. [CrossRef]\n\
    56.\nAl-Saddik, H.; Laybros, A.; Simon, J.-C.; Cointault, F. Protocol for the\
    \ Deﬁnition of a Multi-Spectral Sensor for Speciﬁc Foliar\nDisease Detection:\
    \ Case of “Flavescence Dorée”. In Phytoplasmas; Springer: New York, NY, USA, 2019;\
    \ pp. 213–238.\n57.\nSeelan, S.K.; Laguette, S.; Casady, G.M.; Seielstad, G.A.\
    \ Remote sensing applications for precision agriculture: A learning\ncommunity\
    \ approach. Remote Sens. Environ. 2003, 88, 157–169. [CrossRef]\n58.\nDunning,\
    \ H. Drones That Detect Early Plant Disease Could Save Crops; Imperial College\
    \ London: London, UK, 2017; pp. 1–3.\n59.\nBah, M.D.; Haﬁane, A.; Canals, R. Deep\
    \ learning with unsupervised data labeling for weed detection in line crops in\
    \ UAV images.\nRemote Sens. 2018, 10, 1690. [CrossRef]\n60.\nYamamoto, K.; Togami,\
    \ T.; Yamaguchi, N. Super-resolution of plant disease images for the acceleration\
    \ of image-based phenotyp-\ning and vigor diagnosis in agriculture. Sensors 2017,\
    \ 17, 2557. [CrossRef] [PubMed]\n61.\nSugiura, R.; Tsuda, S.; Tamiya, S.; Itoh,\
    \ A.; Nishiwaki, K.; Murakami, N.; Shibuya, Y.; Hirafuji, M.; Nuske, S. Field\
    \ phenotyping\nsystem for the assessment of potato late blight resistance using\
    \ RGB imagery from an unmanned aerial vehicle. Biosyst. Eng. 2016,\n148, 1–10.\
    \ [CrossRef]\n62.\nSugiura, R.; Tsuda, S.; Tsuji, H.; Murakami, N. Virus-Infected\
    \ Plant Detection in Potato Seed Production Field by UAV Imagery;\nAmerican Society\
    \ of Agricultural and Biological Engineers: St. Joseph, MI, USA, 2018; p. 1.\n\
    63.\nPande, C.B.; Moharir, K.N. Application of hyperspectral remote sensing role\
    \ in precision farming and sustainable agriculture\nunder climate change: A review.\
    \ In Climate Change Impacts on Natural Resources, Ecosystems and Agricultural\
    \ Systems; Springer:\nBerlin/Heidelberg, Germany, 2023; pp. 503–520.\n64.\nJoalland,\
    \ S.; Screpanti, C.; Varella, H.V.; Reuther, M.; Schwind, M.; Lang, C.; Walter,\
    \ A.; Liebisch, F. Aerial and ground based\nsensing of tolerance to beet cyst\
    \ nematode in sugar beet. Remote Sens. 2018, 10, 787. [CrossRef]\n65.\nDang, L.M.;\
    \ Hassan, S.I.; Suhyeon, I.; kumar Sangaiah, A.; Mehmood, I.; Rho, S.; Seo, S.;\
    \ Moon, H. UAV based wilt detection\nsystem via convolutional neural networks.\
    \ Sustain. Comput. Inform. Syst. 2018, 28, 100250. [CrossRef]\n66.\nDash, J.P.;\
    \ Watt, M.S.; Pearse, G.D.; Heaphy, M.; Dungey, H.S. Assessing very high resolution\
    \ UAV imagery for monitoring forest\nhealth during a simulated disease outbreak.\
    \ ISPRS J. Photogramm. Remote Sens. 2017, 131, 1–14. [CrossRef]\n67.\nSmigaj,\
    \ M.; Gaulton, R.; Barr, S.; Suárez, J. UAV-borne thermal imaging for forest health\
    \ monitoring: Detection of disease-induced\ncanopy temperature increase. Int.\
    \ Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2015, 40, 349. [CrossRef]\n68.\n\
    Techy, L.; Schmale, D.G., III; Woolsey, C.A. Coordinated aerobiological sampling\
    \ of a plant pathogen in the lower atmosphere\nusing two autonomous unmanned aerial\
    \ vehicles. J. Field Robot. 2010, 27, 335–343. [CrossRef]\n69.\nSantesteban, L.;\
    \ Di Gennaro, S.; Herrero-Langreo, A.; Miranda, C.; Royo, J.; Matese, A. High-resolution\
    \ UAV-based thermal\nimaging to estimate the instantaneous and seasonal variability\
    \ of plant water status within a vineyard. Agric. Water Manag. 2017,\n183, 49–59.\
    \ [CrossRef]\n70.\nAlbetis, J.; Jacquin, A.; Goulard, M.; Poilvé, H.; Rousseau,\
    \ J.; Clenet, H.; Dedieu, G.; Duthoit, S. On the potentiality of UAV\nmultispectral\
    \ imagery to detect Flavescence dorée and Grapevine Trunk Diseases. Remote Sens.\
    \ 2019, 11, 23. [CrossRef]\n71.\nAlbetis, J.; Duthoit, S.; Guttler, F.; Jacquin,\
    \ A.; Goulard, M.; Poilvé, H.; Féret, J.-B.; Dedieu, G. Detection of Flavescence\
    \ dorée\ngrapevine disease using unmanned aerial vehicle (UAV) multispectral imagery.\
    \ Remote Sens. 2017, 9, 308. [CrossRef]\n72.\nFranceschini, M.H.D.; Bartholomeus,\
    \ H.; Van Apeldoorn, D.F.; Suomalainen, J.; Kooistra, L. Feasibility of unmanned\
    \ aerial vehicle\noptical imagery for early detection and severity assessment\
    \ of late blight in potato. Remote Sens. 2019, 11, 224. [CrossRef]\n73.\nDi Gennaro,\
    \ S.F.; Battiston, E.; Di Marco, S.; Facini, O.; Matese, A.; Nocentini, M.; Palliotti,\
    \ A.; Mugnai, L. Unmanned Aerial Vehicle\n(UAV)-based remote sensing to monitor\
    \ grapevine leaf stripe disease within a vineyard affected by esca complex. Phytopathol.\n\
    Mediterr. 2016, 55, 262–275.\n74.\nCalderón, R.; Montes-Borrego, M.; Landa, B.;\
    \ Navas-Cortés, J.; Zarco-Tejada, P. Detection of downy mildew of opium poppy\n\
    using high-resolution multi-spectral and thermal imagery acquired with an unmanned\
    \ aerial vehicle. Precis. Agric. 2014, 15,\n639–661. [CrossRef]\n75.\nde Souza,\
    \ C.H.W.; Lamparelli, R.A.C.; Rocha, J.V.; Magalhães, P.S.G. Mapping skips in\
    \ sugarcane ﬁelds using object-based\nanalysis of unmanned aerial vehicle (UAV)\
    \ images. Comput. Electron. Agric. 2017, 143, 49–56. [CrossRef]\n76.\nVanegas,\
    \ F.; Bratanov, D.; Weiss, J.; Powell, K.; Gonzalez, F. Multi and hyperspectral\
    \ UAV remote sensing: Grapevine phylloxera\ndetection in vineyards. In Proceedings\
    \ of the 2018 IEEE Aerospace Conference, Big Sky, MT, USA, 3–10 March 2018; pp.\
    \ 1–9.\n77.\nTetila, E.C.; Machado, B.B.; de Souza Belete, N.A.; Guimarães, D.A.;\
    \ Pistori, H. Identiﬁcation of soybean foliar diseases using\nunmanned aerial\
    \ vehicle images. IEEE Geosci. Remote Sens. Lett. 2017, 14, 2190–2194. [CrossRef]\n\
    78.\nGarcia-Ruiz, F.; Sankaran, S.; Maja, J.M.; Lee, W.S.; Rasmussen, J.; Ehsani,\
    \ R. Comparison of two aerial imaging platforms for\nidentiﬁcation of Huanglongbing-infected\
    \ citrus trees. Comput. Electron. Agric. 2013, 91, 106–115. [CrossRef]\nAgronomy\
    \ 2023, 13, 1524\n26 of 26\n79.\nPatrick, A.; Pelham, S.; Culbreath, A.; Holbrook,\
    \ C.C.; De Godoy, I.J.; Li, C. High throughput phenotyping of tomato spot\nwilt\
    \ disease in peanuts using unmanned aerial systems and multispectral imaging.\
    \ IEEE Instrum. Meas. Mag. 2017, 20, 4–12.\n[CrossRef]\n80.\nAylor, D.E.; Schmale,\
    \ I.I.I.D.G.; Shields, E.J.; Newcomb, M.; Nappo, C.J. Tracking the potato late\
    \ blight pathogen in the atmosphere\nusing unmanned aerial vehicles and Lagrangian\
    \ modeling. Agric. For. Meteorol. 2011, 151, 251–260. [CrossRef]\n81.\nZhang,\
    \ T.; Xu, Z.; Su, J.; Yang, Z.; Liu, C.; Chen, W.-H.; Li, J. Ir-unet: Irregular\
    \ segmentation u-shape network for wheat yellow\nrust detection by UAV multispectral\
    \ imagery. Remote Sens. 2021, 13, 3892. [CrossRef]\n82.\nSu, J.; Yi, D.; Su, B.;\
    \ Mi, Z.; Liu, C.; Hu, X.; Xu, X.; Guo, L.; Chen, W.-H. Aerial visual perception\
    \ in smart farming: Field study of\nwheat yellow rust monitoring. IEEE Trans.\
    \ Ind. Inform. 2020, 17, 2242–2249. [CrossRef]\n83.\nLiu, L.; Dong, Y.; Huang,\
    \ W.; Du, X.; Ma, H. Monitoring wheat fusarium head blight using unmanned aerial\
    \ vehicle hyperspectral\nimagery. Remote Sens. 2020, 12, 3811. [CrossRef]\n84.\n\
    Huang, H.; Deng, J.; Lan, Y.; Yang, A.; Zhang, L.; Wen, S.; Zhang, H.; Zhang,\
    \ Y.; Deng, Y. Detection of helminthosporium leaf\nblotch disease based on UAV\
    \ imagery. Appl. Sci. 2019, 9, 558. [CrossRef]\n85.\nKerkech, M.; Haﬁane, A.;\
    \ Canals, R. Vine disease detection in UAV multispectral images using optimized\
    \ image registration and\ndeep learning segmentation approach. Comput. Electron.\
    \ Agric. 2020, 174, 105446. [CrossRef]\n86.\nStewart, E.L.; Wiesner-Hanks, T.;\
    \ Kaczmar, N.; DeChant, C.; Wu, H.; Lipson, H.; Nelson, R.J.; Gore, M.A. Quantitative\
    \ phenotyping\nof Northern Leaf Blight in UAV images using deep learning. Remote\
    \ Sens. 2019, 11, 2209. [CrossRef]\n87.\nWiesner-Hanks, T.; Stewart, E.L.; Kaczmar,\
    \ N.; DeChant, C.; Wu, H.; Nelson, R.J.; Lipson, H.; Gore, M.A. Image set for\
    \ deep\nlearning: Field images of maize annotated with disease symptoms. BMC Res.\
    \ Notes 2018, 11, 440. [CrossRef]\n88.\nWu, B.; Liang, A.; Zhang, H.; Zhu, T.;\
    \ Zou, Z.; Yang, D.; Tang, W.; Li, J.; Su, J. Application of conventional UAV-based\
    \ high-\nthroughput object detection to the early diagnosis of pine wilt disease\
    \ by deep learning. For. Ecol. Manag. 2021, 486, 118986.\n[CrossRef]\n89.\nHu,\
    \ G.; Zhu, Y.; Wan, M.; Bao, W.; Zhang, Y.; Liang, D.; Yin, C. Detection of diseased\
    \ pine trees in unmanned aerial vehicle images\nby using deep convolutional neural\
    \ networks. Geocarto Int. 2022, 37, 3520–3539. [CrossRef]\nDisclaimer/Publisher’s\
    \ Note: The statements, opinions and data contained in all publications are solely\
    \ those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or\
    \ the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury\
    \ to\npeople or property resulting from any ideas, methods, instructions or products\
    \ referred to in the content.\n"
  inline_citation: '>'
  journal: Agronomy (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2073-4395/13/6/1524/pdf?version=1685589617
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'Drones in Plant Disease Assessment, Efficient Monitoring, and Detection:
    A Way Forward to Smart Agriculture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1049/iet-ipr.2017.1203
  analysis: '>'
  authors:
  - Frédéric Bousefsaf
  - Mohamed Tamaazousti
  - Souheil Hadj Said
  - R. Michel
  citation_count: 4
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register IET HUB HOME JOURNALS
    IET PRIZE PROGRAMME SUBJECTS Visit IET IET Image Processing Research Article Free
    Access Image completion using multispectral imaging Frédéric Bousefsaf,  Mohamed
    Tamaazousti,  Souheil Hadj Said,  Rémi Michel First published: 01 July 2018 https://doi.org/10.1049/iet-ipr.2017.1203Citations:
    2 SECTIONS PDF TOOLS SHARE Abstract Here, the authors explore the potential of
    multispectral imaging applied to image completion. Snapshot multispectral cameras
    correspond to breakthrough technologies that are suitable for everyday use. Therefore,
    they correspond to an interesting alternative to digital cameras. In their experiments,
    multispectral images are acquired using an ultracompact snapshot camera-recorder
    that senses 16 different spectral channels in the visible spectrum. Direct exploitation
    of completion algorithms by extension of the spectral channels exhibits only minimum
    enhancement. A dedicated method that consists in a prior segmentation of the scene
    has been developed to address this issue. The segmentation derives from an analysis
    of the spectral data and is employed to constrain research area of exemplar-based
    completion algorithms. The full processing chain takes benefit from standard methods
    that were developed by both hyperspectral imaging and computer vision communities.
    Results indicate that image completion constrained by spectral presegmentation
    ensures better consideration of the surrounding materials and simultaneously improves
    rendering consistency, in particular for completion of flat regions that present
    no clear gradients and little structure variance. The authors validate their method
    with a perceptual evaluation based on 20 volunteers. This study shows for the
    first time the potential of multispectral imaging applied to image completion.
    1 Introduction Image completion consists in filling or restoring missing or damaged
    regions in a visually plausible way. This image processing technique has many
    applications, such as the removal of unwanted objects in photos and panoramas
    [1], image restoration [2], and diminished reality [3]. The research in this field
    has reached an advanced level of maturity, some of the methods being incorporated
    in raster graphics editors [e.g. PatchMatch [4] in Photoshop CS5 (http://www.adobe.com/technology/projects/patchmatch.html)].
    The completion task is non-trivial and is of growing importance in computer vision
    and computer graphics. New completion methods were recently proposed to guide
    the filling of missing regions using prior information about structures [5] and
    perspectives [6], by using guidance maps [7] or by using statistics of similar
    patches [8]. This high-level information corresponds to prior knowledge on the
    geometry of the scene. At last, the completion process is performed and represented
    with red, green, and blue (RGB) values. Rather than employing RGB cameras, multispectral
    camera-recorders [9] provide more detailed information about the spectrum of objects
    present in the scene. Those cameras may be of help to address standard computer
    vision tasks [10], especially when considering the recent introduction of snapshot
    multispectral camera-recorders [11]. Basically, the content of an image depends
    on both its geometrical and spectral dimensions [10]. Multispectral images are
    represented through three-dimensional (3D) datacubes, where a set of 2D images
    is acquired at different bands of wavelengths using dedicated optical devices
    [9]. In the fields of Earth and planetary sciences, datacubes delivered by multispectral
    or hyperspectral cameras are processed and analysed to provide relevant information
    about the chemical composition of the recorded scenes. One of the important advantages
    of this technique is that physical processes like absorption, reflectance, or
    fluorescence spectrum can be estimated for each pixel in the image. It allows
    the detection of chemical changes of objects that cannot be identified with monochromatic
    or colour (RGB) data [12]. The spectral information has been notably employed
    to characterise ocean colour [13], classify glacier surfaces [14], or to sense
    gypsum on Mars [15]. Also, spectral imaging corresponds to a powerful analytical
    tool for biological and biomedical research, notably in order to identify tissue
    abnormalities [12]. The spectral information of a pure material is enough scale-invariant
    to provide very valuable cues to better understand the contents of an image [10].
    Material recognition is presumed to reinforce image processing and understanding
    techniques such as object detection, object recognition, and image segmentation
    [16]. To date, there have been no studies that analyse the relevance of multispectral
    imaging in the image completion context. Analysing multispectral frames instead
    of RGB frames amounts to process the spectral dimension at each pixel of the image.
    This information can be used to improve the renderings by properly updating photometric
    parameters, in particular for diminished reality applications [3]. In this study,
    we propose to investigate the relevance of multispectral frames applied to image
    completion, an application initially dedicated to 3D RGB images. The study first
    provides, in Section 2, a focused description of image completion algorithms that
    will be of interest for the use of multispectral images within the next sections.
    Some basics of multispectral imaging (sensor specifications and pre-processing
    operations) are then presented in Section 3. As the main purpose consists in better
    completing images dedicated to visualisation, this section also includes elements
    about the conversion from the recorded multispectral channels to the standard
    RGB colour space. In Section 4, we describe the behaviour of a reference completion
    algorithm on multispectral datacubes by directly extending its input (from 3D
    RGB images to 16 multispectral channels). Section 5 presents a better two-step
    method dedicated to the use of multispectral channels for image completion. A
    pre-segmentation of the geometry of the scene based on the spectral dimension
    is described in the first step. Research of substitution pixels is then geometrically
    constrained to a predefined area: only the segments located in the vicinity of
    the missing region are considered (see Fig. 1 for a representative example). Fig.
    1 Open in figure viewer PowerPoint Image completion constrained by spectral segmentation
    (a) Image recorded by the multispectral camera and converted to RGB. The red box
    in the paperboard was selected by the user and corresponds to the area to be completed
    (missing region), (b) Pixels selected by standard exemplar-based completion algorithm
    (i.e. PatchMatch [4]) to complete the missing region are highlighted in green.
    The algorithm considers (by mistake) some pixels from the curtains to complete
    the paperboard, their RGB values being very similar, (c) Resulting completion
    is visually altered and is partially grey, (d) Spectral segmentation deriving
    from noise-adjusted principal component analysis of the multispectral image. Note
    that the spectral segmentation produces regions that seem to be consistent with
    the geometry and materials of the objects, (e) The research is geographically
    limited to the segments in the neighbourhood of the region to be completed (i.e.
    the magenta segment in d), (f) Completion constrained by the spectral segments
    is more compatible with standard visual assessment in computer vision and computer
    graphics Section 6 is dedicated to the analysis of results from a perceptual quality
    assessment procedure based on standard subjective questionnaires over a panel
    of 20 observers. The proposed method (presented in Section 5) delivers completed
    images that are more compatible with standard visual assessment in computer vision
    and computer graphics. 2 Related work Image completion methods can be classified
    in three categories in the literature [2]: diffusion-based [17] and exemplar-based
    methods [4, 6, 8, 18]. Recent studies provided methods to extend single-frame
    completion to video sequences [19] or in real time [3] for diminished reality
    applications. More recently, Baek et al. [20] proposed to complete both colour
    and depth channels from multiview images. 2.1 Diffusion-based methods Diffusion-based
    techniques were developed to fill small or narrow holes by propagating adjacent
    image pixels into the missing area [17]. Smoothness priors are introduced through
    parametric models or through partial differential equations to diffuse local structures
    [21]. These techniques tend to blur and are less effective in handling large missing
    regions due to their inability to properly recover the textures. Tensor decomposition
    and completion [22] consist in propagating structures to fill missing regions
    by estimating missing values in tensors of visual data. The methods are based
    on matrix completion, which uses the matrix trace norm, but extended to the tensor
    case. Thus, tensor completion corresponds to a high-order extension of matrix
    completion and is formulated as a global convex optimisation problem [2, 22].
    2.2 Exemplar-based methods Exemplar-based methods take their origin from texture
    synthesis methods [23]. These approaches use textures in an image as exemplars
    based on the assumption that the patches in the target region are similar to those
    in the rest of the image [4, 6, 8, 18]. In a progressive manner, texture is successively
    copied to the boundary of the target region. Although this method can replicate
    complex textures in the missing region, the success of the structure propagation
    largely depends on the order of copy [18]. To tackle this issue and to produce
    more plausible results, the order of copy can be determined with particular criteria.
    For example, Criminisi et al. [18] proposed a gradient-based method that encourages
    the propagation of textures along linear structures like strong edges. As they
    progress patch per patch in a greedy fashion, the previous approaches do not ensure
    global consistency. To address this issue, Wexler et al. [24] proposed to constrain
    the missing values by solving a global optimisation problem. Based on this work
    and to reduce the computational burden, Barnes et al. have developed PatchMatch
    [4], a fast algorithm that iteratively generates textures in the target region
    by minimising an objective function based on pattern similarity between the missing
    region and the rest of the image. The relative location from where exemplar-based
    methods copy the content (a pixel or a patch) is called an offset. All possible
    offsets are generally accepted in the optimisation process. Nevertheless, constraints
    on offsets can be imposed to produce better results in terms of quality, particularly
    by using statistics of similar patches [8]. Additional constraints can be included
    to exemplar-based algorithms by, for example, guiding the reconstruction with
    a prior geometric scheme in order to propagate long edges [5] or by segmenting
    the known region into planes to properly consider the perspectives of the scene
    [6]. 2.3 Learning-based methods Hays and Efros [25] proposed to pair the image
    completion process with a database that contains a large amount of reference images.
    The missing regions are completed by copying similar patches from the database.
    The candidate patches must be seamless and semantically consistent. Pathak et
    al. [26] have used convolutional neural networks trained to generate the contents
    of a missing region by capturing the context of the image. Recent results show
    that approaches based on deep neural networks can effectively compete with most
    recent exemplar-based completion methods [27]. In this study, we have employed
    PatchMatch [4] as a reference image completion technique. The algorithm ensures
    consistency by solving a global optimisation problem and is faster than comparable
    completion techniques. 3 Multispectral data This section presents details about
    the multispectral device in addition to the image processing operations that were
    employed to analyse the multispectral data. 3.1 Camera specifications The multispectral
    imaging technology we used (Fig. 2a) in this study was designed by IMEC [11].
    The device corresponds to a snapshot (i.e. non-scanning) and ultra-compact spectrometer.
    The camera records the spectral irradiance of a scene through a multispectral
    image, i.e. a 3D data set typically called a datacube or hypercube [9]. The device
    can nominally deliver 170 datacubes per second in real time. This value is constrained
    by the exposure time in practice. Fig. 2 Open in figure viewer PowerPoint Multispectral
    camera specifications (a) Snapshot real-time multispectral camera designed by
    IMEC [11], (b) Spectral sensitivity of the 16 camera channels, which uniformly
    encompass most of the visible spectrum (475–650 nm). Spectral bandwidth is ∼20
    nm per channel. In practice, partial correlation between channels results in 14
    independent components instead of 16, (c) ColorChecker Classic (X-Rite). The colour
    chart contains 24 colour patches [28]. Their reference spectra, defined between
    380 and 730 nm, are provided by the manufacturer, (d) Image and spectra derived
    from the multispectral camera. Reference and reconstructed spectra match within
    up to 90% RMS. The slight discrepancies result from uncertainties in the spectral
    calibration procedure Practically, the camera senses 16 different spectral bands
    between 475 and 650 nm. The bandwidth of each band comprises between 15 and 20
    nm (Fig. 2b). The full resolution of the CMOS sensor is defined to 2048 × 1024
    pixels but reduced to 512 × 256 pixels for each spectral channel (each cell being
    formed by a 4 × 4 multispectral mosaic [11]). Pixel intensity (bit depth) is signed
    over 10 bits. 3.2 Pre-processing 3.2.1 Spectral reconstruction Spectral reconstruction
    corresponds to a primary procedure essentially employed to calibrate multi- or
    hyperspectral sensors in order to assess apparent reflectances from raw spectral
    channels [29]. In the present study, spectral reconstruction was performed using
    a colour chart that includes 24 different colour patches (see Fig. 2c). Given
    that all the optical parameters cannot be estimated beforehand, an indirect method
    was employed to calibrate the multispectral sensor. For the sake of completeness,
    the interested reader can refer to the original article [29] in order to get the
    full implementation details. The reconstructed reflectance of the blue, red, and
    green patches of the colour chart are illustrated in Fig. 2d. The observable discrepancies
    result from uncertainties on the calibration procedure, which closely depends
    on the spectral sensitivity responses (Fig. 2b). 3.2.2 Multispectral to RGB conversion
    As completion algorithms deliver images that are displayed on screen and visually
    evaluated by humans, a conversion to the standard RGB colour space is required.
    In practice, this conversion is achieved using apparent reflectances deriving
    from the camera calibration procedure (Section 3.2.1) and by the means of the
    CIE colour matching functions (see Fig. 3b). An example of standard RGB conversion
    is presented in Fig. 2d. Fig. 3 Open in figure viewer PowerPoint Spectral resolution
    significance. Averaged spectra along with their respective RGB values have been
    extracted from P1 and P2 patches. RGB values indicate that the colours are very
    similar. Multispectral sampling allows a more precise observation of chromatic
    differences. A reduction in spectra to three R, G, and B values leads to smooth
    and filter out spectral details, in particular when relevant variations are cancelled
    due to integration by the CIE matching functions (, , and curves) 4 Preliminary
    analyses 4.1 Significance of the spectral sampling Image completion is based on
    colour and brightness analysis of different image patches. Fig. 3a presents a
    typical example, where P1 and P2 correspond to patches of similar RGB colour.
    Working with more spectral bands (by increasing the spectral sampling) can be
    helpful in order to reveal additional relevant information. Fig. 3b presents the
    spectrum along with the corresponding RGB values of both the P1 and P2 patches.
    Herein, important chromatic differences appear between 590 and 730 nm. These disparities
    are partially cancelled due to integration by the CIE colour matching functions
    (, , and on Fig. 3b). Employing more spectral bands seems relevant in order to
    better consider chromatic variations when performing image completion. 4.2 Experimental
    procedure A set composed of ten different multispectral images was employed to
    assess the relevance of the multispectral data applied to image completion. The
    frames were recorded with the multispectral device presented in Section 3.1, the
    scenes being selected to emphasise current image completion limits. To this purpose,
    objects and backgrounds of similar colours were employed (Fig. 4a). Fig. 4 Open
    in figure viewer PowerPoint Experimental set-up (a) Typical image acquired with
    the multispectral device and converted to RGB. The red region, which is selected
    by the user, denotes the area to be completed (missing region), (b) Ground truth
    material mask (). The white region has been manually segmented and corresponds
    to the best zone of research (in terms of material) for completion candidates
    Each area to be completed was manually chosen and presents no clear gradients
    and little spatial structure variance. For validation purpose, the regions were
    defined to avoid entire overlapping of an object and are comprised on a single
    material. To evaluate the behaviour of the completion procedure, ground truth
    material masks were manually defined (Fig. 4b). They correspond to the region
    defined by the same material than the one which surrounds the area to be completed.
    These material masks are also used to evaluate the relevance of the spectral segmentation
    proposed in this study (see Section 5.2.2). We propose to assess the behaviour
    of standard completion algorithm (Section 4.3) in regard to the materials that
    surround the region to be completed (Section 4.4), in particular when increasing
    the number of multispectral channels. We also propose to empirically evaluate
    the quality of the completion by comparing the synthesised area with its original
    content (Section 4.5). 4.3 Implementation details PatchMatch [4], which was initially
    proposed by Barnes et al., is used as a reference image completion technique.
    The algorithm ensures consistency by solving a global optimisation problem and
    is faster than comparable completion techniques. The method is composed of a sequence
    of specific steps. The interested reader can refer to the original article [4]
    in order to get the full implementation details. Briefly, the method is defined
    over three main steps: (i) initialisation: a random patch offset is given to each
    pixel at the coarsest pyramid level of the image. The result is propagated to
    the next pyramid level where a propagation and random search steps are applied
    at each level; (ii) propagation: the pertinence of the offsets is evaluated with
    respect to the neighbouring patches at each iteration using an objective function;
    (iii) random search: a search step is employed to look for better patch within
    a concentric radius around the current offset. The new offset is adopted if the
    new objective function is lower. A particular implementation of the initialisation
    step was employed in this study. A first exhaustive search of the best matching
    offsets is performed [30] instead of a random one. Also, the patch size has been
    set to 13 × 13 pixels. Owing to the random process included in PatchMatch, 50
    trials per image were launched to compute statistical tendencies, a single run
    being non-representative. 4.4 Materials consideration In this section, we propose
    to assess the behaviour of the completion algorithm in regard to the materials
    that surround the region to be completed, in particular when increasing the number
    of multispectral channels. The full image I is separated into two disjoint sets:
    T corresponds to the target (or missing) region, completed using pixels in S (source
    region). , , and . The image completion algorithm replaces all pixels included
    in T. The offsets represent the difference of position between a pixel in the
    area to be completed (target region) and its corresponding candidate in the source
    region. Offsets are defined with a mapping function f that maps each target position
    to a source position (see Figs. 1b and e for typical examples): (1) f corresponds
    to a transformation that solves a global minimisation problem and is determined
    for each target pixel. The synthesised image is then created by replacing all
    target pixels with their corresponding source pixels. It is important to note
    that only the offsets, i.e. the difference of position between a pixel included
    in the area to be completed and its corresponding candidate in the rest of the
    image, are susceptible to fluctuate. The synthesising procedure (pixel copy) is
    ultimately performed on RGB frames using the defined offsets. To understand if
    the completion algorithm is able to correctly use pixels from surrounding materials,
    the percentage of good match [ in (2)] between the offsets and the ground truth
    material mask was assessed for each of the ten input images. It corresponds to
    the number of times the completion algorithm uses a pixel from the ground truth
    material region over the total number of pixels in the target area: (2) (3) where
    corresponds to the ground truth material region (Fig. 4b). is defined for each
    target pixel (p). N corresponds to the total number of pixels from the target
    region and to the match rate (units: %). Results are presented in Figs. 5a and
    b using boxplot representations. Each boxplot includes 500 computed match rates
    (10 images recorded by the multispectral camera × 50 completion trials per image).
    For comparison purposes, the match rates computed using RGB images were reported
    on these figures (red boxes). Fig. 5 Open in figure viewer PowerPoint Respect
    of the surrounding materials by the completion algorithm. The match rates are
    computed between offsets and the ground truth material mask for each image. The
    presented results integrate all the 500 trials. For comparison purposes, match
    rates computed using RGB images are indicated on each figure (red boxplot) (a)
    Match rates computed on raw multispectral channels, starting from single (monochromatic)
    channel to all the 16 channels, (b) Match rates computed on noise-adjusted principal
    components Fig. 5a presents the match rates computed when completion is performed
    on raw multispectral channels. Starting from all the 16 channels, we progressively
    averaged the spectral image two channels by two channels until reaching a single
    channel (monochromatic image). Fig. 5b presents the same percentage of good match,
    but when performing completion on principal components. The latter were computed
    from a noise-adjusted principal component analysis, a transformation developed
    to sort principal components by image quality (decreasing image quality with increasing
    component number). We have employed minimum/maximum autocorrelation factors to
    estimate the noise covariance matrix. The method has been proposed by Green et
    al. [31] and uses between-neighbour differences to estimate the noise covariance.
    Results presented in Fig. 5a exhibit an increase in the match rates that are correlated
    with the augmentation of the number of channels. Also, the boxplots length indicates
    that the variance tends to simultaneously decrease. Adding a more precise spectral
    information to the completion algorithm leads to better considerate the physical
    properties of materials. Subtle variations that were not necessarily observable
    in the standard RGB colour space are considered (see Section 4.1). Image completion
    based on principal components (Fig. 5b) tends to better consider the surrounding
    materials, the maximum median value being equal to 99% (instead of maximally 80%
    when considering raw multispectral channels). In addition, only four components
    are necessary to achieve this score. The last principal components containing
    more and more noise, the induced artefacts generate a bias that leads the completion
    to pick patches in a random fashion, thus reducing the mean percentage of good
    match while increasing the variance. 4.5 Rendering analysis In this section, we
    propose to empirically assess the quality of the completion by comparing the synthesised
    area with its original content using an error function. The latter corresponds
    to the Euclidean distance based on the R, G, and B channels and is computed for
    each pixel of the target region. Fig. 6a presents the Euclidean errors computed
    using offsets that where determined on raw multispectral channels. As before (see
    Section 4.4) and starting from all the 16 channels, we progressively averaged
    the spectral image two channels by two channels until reaching a single channel
    (monochromatic image). Fig. 6b presents the same information, but when performing
    completion on principal components. A close-up view is displayed on the top of
    the figure to identify the error minimum. Errors computed when completion is performed
    on standard RGB images are, respectively, reported on the two figures using red
    boxes. Fig. 6 Open in figure viewer PowerPoint Rendering analysis assessed using
    Euclidean errors computed between synthesised and original RGB images. The results
    are averaged over the 500 trials. For comparison purposes, the errors computed
    when completion used offset defined on RGB frames are indicated on each figure
    using a red boxplot (a) Errors computed using offsets determined on raw multispectral
    channels, (b) Errors computed using offsets determined on principal components
    Completion based on four multispectral channels (Fig. 6a) presents the general
    minimum error. From Fig. 6b, completion based on the first two principal components
    presents the minimum error. Employing more principal components gives worse completion
    results. This effect is inherent to the noise-adjusted principal component transform:
    the last components containing more and more noise, the induced artefacts generate
    a bias that leads the completion to pick patches in a random fashion. In addition
    to these statistical tendencies, illustrative completion results are presented
    in Fig. 7 to visually compare renderings. From these results, we can conclude
    that completion based on four multispectral channels produces plausible results
    when the materials are respected (i.e. when only the pixels included in the region
    defined by the material that surrounds the missing region are used for completion.
    See Fig. 7, images # 1 and 6, for a typical example). In comparison, completion
    based on the first four principal components produces less consistent results.
    The chromaticity (colours) is respected, but the intensity (brightness) seems
    inconsistently distributed. In contrast, completion based on four multispectral
    channels tends to produce chromatic inconsistencies when the materials are not
    respected (see results of images # 8 and 9 on Fig. 7). This time, completion based
    on the first four principal components delivers more plausible results, even if
    brightness discrepancies can still be noted. Fig. 7 Open in figure viewer PowerPoint
    Spectral completion (a) Source image with (b) its corresponding close-up view.
    The red pattern corresponds to the area to be completed, (c) Ground truth (close-up),
    (d) Pixels selected using four multispectral channels to complete the missing
    region are highlighted in green, (e) Completion results (close-up) based on the
    selected pixels from (d), (f) Pixels selected using the first four principal components
    to complete the missing region are highlighted in green, (g) Completion results
    (close-up) based on the selected pixels from (f) 5 Image completion constrained
    by spectral segmentation 5.1 Motivation Two important points emerged from the
    preliminary analyses results (Sections 4.4 and 4.5): Noise-adjusted principal
    components, computed from the full spectral data, tend to better consider the
    materials of the scene: this particular representation constitutes a good way
    to separate pixels from different material classes and therefore ensure the stability
    of the completion in terms of materials. The first four principal components gives
    a maximum match rate (Fig. 5b). Completion based on multispectral channels produces
    more plausible results than completion based on principal components when the
    materials are respected (Fig. 7). Completion based on four multispectral channels
    presents the minimum error (Fig. 6a). Thereby, we can conclude that principal
    components must be considered in order to fill the missing region with pixels
    included in the same material area. In addition, the completion must be based
    on the raw multispectral channels to ensure a consistent and plausible rendering.
    The method we propose in this section is based on these two observations: completion
    is performed on four raw spectral channels but limited to a predefined and coherent
    area. The latter is estimated through spectral segmentation based on the first
    four principal components. 5.2 Spectral segmentation 5.2.1 Method The full pipeline
    is presented graphically in Fig. 8. The input of the method corresponds to the
    raw multispectral image, where each pixel is defined by its 16 points spectral
    signature (Fig. 8a). Noise-adjusted principal component transform [31] is computed
    (Fig. 8b) to reduce the input dimensionality and, based on the results presented
    in Section 4.4, only the first four components are retained for further processing
    (Fig. 8c). Fig. 8 Open in figure viewer PowerPoint Spectral segmentation procedure
    (a) Raw multispectral image defined over 16 spectral channels, (b) Noise-adjusted
    principal component analysis [31]. Eigenvectors correspond to images with decreasing
    eigenvalues, (c) Thresholding operation is applied to recover the first four principal
    components, which are more adapted to the context (see Section 4.4), (d) Projection
    of the pixel values in the spectral space defined by the first four principal
    components (only the first three dimensions are represented). In this spectral
    space, clusters (see dashed-line ellipses) match with the geometry and materials
    of the scene, (e) Resulting segmentation, based on hierarchical data clustering
    (number of clusters: 8) The spectral segmentation is based on agglomerative hierarchical
    clustering, which consists in grouping data by creating a cluster tree (dendrogram).
    The similarity between every pair of pixels is firstly evaluated by computing
    Euclidean distances. Note that each pixel is defined by four different coordinates,
    one coordinate by principal component. The distance information is used to link
    pairs of pixels that are close together into binary clusters. Each binary cluster
    is made up of two pixels. The newly formed clusters are then linked once again
    to create bigger clusters using the Ward''s method (minimum increase in sum-of-squares)
    [32]. This step is repeated until all the pixels in the original data set are
    linked together, thus forming a hierarchical tree. The tree may inherently separate
    the data into distinct clusters, in particular for dendrograms created from groups
    of densely packed pixels. These groups may correspond to pixels of similar materials.
    For example, if we only consider the second principal component (presented in
    Fig. 8c), the hierarchical tree will contain four large and separate clusters:
    the paperboard (dark pixels), the table (dark-grey pixels), the radiator (white
    pixels), and the background (light-grey pixels). The hierarchical cluster tree
    is pruned to partition the data set into separated clusters. Usually, the number
    of clusters must be carefully selected to avoid over- and under-segmentation.
    Under-segmentation is not permitted: pixels that belong to different materials
    will be grouped in a single segment, thus resulting in a probable inaccurate completion.
    The segments located in the vicinity of the region to be completed being merged
    (Fig. 9), over-segmentation is tolerated. To properly perform completion, the
    fused segments of interest (Fig. 9c) must include an acceptable amount of pixels.
    Fig. 9 Open in figure viewer PowerPoint Segments merging and post-processing treatments
    (a) Spectral segmentation based on agglomerative hierarchical data clustering
    (number of clusters: 20). The black rectangle indicates the area to be completed,
    (b) The segments located in the vicinity of the region to be completed are merged
    to form a single, binary mask, (c) Post-processing treatments are applied to remove
    small group of pixels and fill small holes, (d) Ground truth material mask (manually
    segmented) Practically, the function clusterdata included in Matlab (The MathWorks
    Inc.) was employed. Euclidean distance and Ward''s method were used to, respectively,
    compute every distance and create the hierarchical tree. As presented in Fig.
    9, post-processing treatments were developed to remove artefacts. In particular,
    morphological operations were employed to remove small isolated groups of pixels
    (surface area 200 pixels) and fill small holes (morphological closing using a
    disk-shaped structural element of radius 3 pixels). 5.2.2 Evaluation The spectral
    segmentation is evaluated through Jaccard''s distance and precision and recall
    indexes [33]. The metrics were computed between the spectral segmentation, given
    by its binary mask (Fig. 9c), and the ground truth material mask (Fig. 9d) for
    each scene. The results are presented in Table 1. Generally, the average values
    for the precision and recall indexes are >85%. Table 1. Evaluation of the spectral
    segmentation proposed in this study (see Section 5.2). Precision and recall indexes,
    as well as Jaccard''s distance, were computed using ground truth material masks
    (Fig. 4) Scene # 1 2 3 4 5 6 7 8 9 10 All scenes mean All scenes standard deviation
    Precision, % 92 88 90 87 91 89 96 88 58 72 85 11 Recall, % 100 100 96 97 36 91
    98 63 89 95 86 21 Jaccard''s distance, % 92 88 87 85 35 82 94 58 54 70 74 20 5.3
    Constrained multispectral completion The binary mask delivered by the spectral
    segmentation procedure (Fig. 9c) is employed to constrain completion in a predefined
    region. Technically, we deactivate the research process on source pixels located
    outside the segmentation area. Note that completion is constrained by spectral
    segmentation, which is based on the first four principal components (Section 4.4),
    but ultimately performed by analysing pixel values on four multispectral channels
    (Section 4.5). Some excerpts are illustrated in Fig. 10. All the synthesised images
    were compared against baseline (standard RGB completion) through subjective quality
    assessment metrics. Fig. 10 Open in figure viewer PowerPoint Comparisons with
    representative baseline algorithm [4] (a) Source image, (b) Ground truth material
    mask, (c) Corresponding close-up view. In (a) and (c), the red pattern indicates
    the area to be completed, (d) Ground truth (close-up), (e) Offsets computed using
    baseline completion method (green pixels). They correspond to the pixels used
    to complete the missing region, (f) Completion results based on the selected pixels
    from (e), (g) Spectral segmentation mask. Research of substitution pixels is geometrically
    constrained to the white area, (h) Offsets computed using the method proposed
    in this study (green pixels), (i) Completion results based on the selected pixels
    from (h) 6 Perceptual quality assessment 6.1 Introduction Most of computer graphics
    rendering methods require perceptually plausible results: simple pixel intensity
    error computed between synthesised and original images (see Section 4.5) does
    not necessarily reflect and guarantee perceived image quality [34]. Image quality
    assessment consists in providing a metric that expresses overall quality by rating
    and ranking methods. Image and video quality assessment has been particularly
    employed in video compression and transmission applications [35]. To assess visual
    quality as perceived by observers, ratings and preferences are recorded through
    subjective questionnaires. These two metrics have been widely used in experimental
    sciences to assess relative judgements from human participants [34]. Decision
    times, which are related to the degree of difficulty encountered by the observers
    to perform the tasks, were also recorded. Quality is assessed for each of the
    ten scenes recorded by the multispectral camera (see Section 4.2). The experiment
    was conducted by 20 different observers (17 males and 3 females, 25–37 years).
    Three images are employed for each scene to perform quality assessment: (i) image
    completed by standard RGB method; (ii) image completed by the technique proposed
    in this study (spectral completion); and (iii) reference (unmodified) image. 6.2
    Assessment methods All observers received a prior explanation before the beginning
    of the session. The experiment started with a short training session, in which
    observers could manipulate the interface and perform training tasks. To avoid
    effects caused by side variables, all test sets were presented to each observer
    in a random fashion. Also and to avoid fatigue, no session took longer than 20
    min. A typical session lasts ∼15 min. Three methods were employed to assess image
    quality: (i) single and (ii) double stimulus represent continuous rating while
    (iii) pairwise similarity judgement method is employed to evaluate relative preference
    between two images [35]. Single stimulus: observers judge the quality on a continuous
    five-point Likert scale [36]. Each image is displayed for only 4 s. After that
    short period, a voting interface is displayed on screen. Five categories are indicated
    right over the continuous scale: bad, poor, fair, good, and excellent (Fig. 11a).
    Reference images are included into the set. Thus, observers must evaluate a set
    composed of 30 randomly arranged images, which includes 10 reference images and
    20 completed images (10 by standard RGB completion and 10 by the spectral completion
    proposed in this study). Fig. 11 Open in figure viewer PowerPoint Overview of
    the three subjective methods employed in this study to assess image quality (a)
    Single stimulus. Observers have to rate the quality of the displayed image, (b)
    Double stimulus. Observers must rate the quality of the first and the second image,
    (c) Similarity judgements. Observers have to express their preference by evaluating
    the quality differences between the two displayed images Double stimulus: is similar
    to the single stimulus method, except that a reference and a completed image are
    successively displayed in random order one after another, each one for 4 s (Fig.
    11b). Observers are asked to independently evaluate the quality of the first and
    the second image. Herein, observers rate 40 images (20 related to RGB completion
    and 20 related to spectral completion). Pairwise similarity judgement: observers
    are asked to mark their preference by indicating how large the difference in quality
    is between images synthesised by each of the two completion methods (Fig. 11c).
    A continuous seven-point scale has been employed. Observers can select the central
    position if no differences were identified between the pair of images. 6.3 Results
    and analysis Rating methods: It has been shown that direct rating results correspond
    to very unreliable estimates [35]. Thus, we choose to present only differential
    scores in this section. The latter were computed between pairs of images, in particular
    between reference and completed images and by means of difference mean opinion
    scores (4): (4) (5) corresponds to the rating for a given (reference or completed)
    image. Indexes correspond to the ith observer, jth completion method (standard
    or spectral), and kth scene. ref(k) corresponds to the reference for scene k.
    z-scores (5) are computed to adjust scale variations between observers in order
    to properly compare results. To unify scales, a common way consists in normalising
    opinion scores by removing the mean [ in (5)] and unifying standard deviation
    across observers [ in (5)]. Results for both single and double stimulus experiments
    are presented in Fig. 12. Generally and for both experiments, observers showed
    preference for images completed by the method proposed in this study: the z-scores
    are significantly higher than those computed from images completed by the baseline
    method. In addition, we can notice that observers gave similar opinions for scenes
    # 4 and 5, indicating that both completion methods performed identically, and
    particularly well, on these two scenes. Fig. 12 Open in figure viewer PowerPoint
    Ratings for each scene. Figures exhibit z-scores from single stimulus (first row)
    and double stimulus (second row). On each figure, the left boxplot has been formed
    using ratings from images completed by the proposed method (completion constrained
    by spectral segmentation, see Section 5). The blue central boxplot corresponds
    to ratings from reference (unmodified) images and the red boxplot on the right
    to ratings from images completed by baseline method (standard RGB completion).
    Each boxplot integrates ratings results over all observers, the central mark corresponding
    to the median, the edges of the box to the 25th and 75th percentiles and the whiskers
    to the most extreme data points (not considered outliers). Outliers are plotted
    individually using red crosses Pairwise similarity judgement: In order to be compared
    and because each observer could employ a different range of voting values, quality
    judgements are normalised per observer: each vote has been divided by the observer
    global standard deviation. The results are similar to z-scores, except that the
    mean value () is not removed. Similarity judgements include preferences, the sign
    indicating which image was judged better. Results are presented in Fig. 13a. Positive
    values indicate that images completed by the method we propose in this study (spectral
    completion) are preferred to images completed by the baseline method. Results
    produced by spectral completion were preferred in 186 over a total of 200 votes,
    which correspond to 93% of the total number of votes. Seven votes over 200 (3.5%
    of the total number) indicate no difference in quality between the two completion
    methods (values located on the zero axis in Fig. 13a). Finally, seven votes over
    200 (3.5%) were in favour of images completed by the baseline method (negative
    values in Fig. 13a). Fig. 13 Open in figure viewer PowerPoint Pairwise judgments
    and time needed to complete the experiment (a) Pairwise judgements for each scene.
    Results are presented in normalised units: each vote has been divided by the observer
    global standard deviation. Positive scores indicate that images completed by the
    method we propose in this study (spectral completion) are preferred to images
    completed by the baseline method (standard RGB completion), (b) Time needed to
    complete the pairwise comparison experiment From these results, we can conclude
    that the quality of images produced by the spectral completion was preferred in
    most cases. Except for scenes # 4 and 5, all judgements opt in favour of the method
    we propose in this study. In accordance with results from single and double stimulus
    experiments (Fig. 12), both completion methods performed well for these two particular
    scenes, their ratings being close to the reference image. Thus, it appears that
    observers were less able to distinguish differences in quality between the images
    produced by the two completion methods. This observation seems to be consistent
    with the time took by the observers to complete the experiment (Fig. 13b), which
    is significantly higher for scenes # 4 and 5 than for the other scenes. 7 Discussion
    Employing snapshot multispectral cameras instead of hyperspectral ones ensures
    a real-time exploitation of the method, which corresponds to a necessary prerequisite
    for many practical applications. In contrast, the direct integration of the spectrum,
    signed over 16 different values, imposes a drastic extension of computational
    times. This limitation was considered by integrating dimensionality reduction
    transforms to the method: based on preliminary analyses (Sections 4.4 and 4.5),
    the first four principal components were used to segment the scene while four
    spectral channels were employed to perform completion by determining which pixels
    must be copied into the missing region. Incorporating recent completion techniques
    (e.g. constraining the completion process with guidance maps [7] or using statistics
    of similar patches [8]) to the method proposed in this study could be relevant
    and of interest but is out of the scope of this paper. Indeed, the main objective
    of this study consists in comparing completion based on multispectral images against
    completion based on RGB images. Adding supplementary constraints, like prior information
    about structures or guidance maps, may tend to denaturate this comparison. 7.1
    Limitations Improvement of the database: The multispectral database currently
    includes ten multispectral indoor scenes. The latter were selected to emphasise
    current image completion limits. To this purpose, objects and backgrounds slightly
    textured and of similar colour were employed. Owing to the random process included
    in PatchMatch, 50 trials per image were launched to compute statistics. Despite
    the low number of images included in the database, we believe that the tendencies
    presented in Sections 4.4 and 4.5 are adequately representative. Spatial-spectral
    clustering: The spectral segmentation developed in this study (Section 5) is based
    on research of clusters in the spectral space. They tend to respect the geometry
    of the objects but no explicit information, like material-invariant features such
    as shape or texture for example, is currently incorporated into the method. 7.2
    Future works In regard to the limitations exposed beforehand, the first milestone
    will consist in expanding the database by including varied indoor and natural
    scenes. Developments will be conducted to improve spectral segmentation by coupling
    spatial and spectral dimensions using Schrödinger Eigenmaps [37], a recent technique
    that extends Laplacian Eigenmaps in order to fuse spatial and spectral information
    through non-diagonal potentials. Also, deep learning [38] and support tensor machine
    [39] correspond to promising avenues that need to be examined. 8 Conclusion We
    have proposed, in this study, to assess potential of multispectral imaging applied
    to image completion. Regions to be completed were chosen to present no clear gradients
    and slight textures. This lack of variance in the spatial structure coupled to
    the presence of objects of similar colour within the image leads to repeated RGB
    completion failures. Herein, the contribution of the spectral information is of
    interest and allows better discrimination and, therefore, an increasing rate of
    successful completion. Preliminary results indicate that direct exploitation of
    completion algorithms by extension of the spectral channels shows only minimum
    enhancement. Based on these observations, we proposed a two-step method dedicated
    to the use of multispectral channels for image completion. A pre-segmentation
    of the scene has been developed to geometrically constrained the research of substitution
    pixels to a predefined area. Only the segments located in the vicinity of the
    missing region are considered. Results indicate that image completion constrained
    by spectral segmentation improves rendering consistency and simultaneously ensures
    better stability in terms of materials. Results were validated using numerical
    criteria and perceptual assessment experiments. The proposed method delivers completed
    images that are more compatible with standard visual assessment in computer vision
    and computer graphics. Snapshot multispectral devices correspond to breakthrough
    technologies that can be employed to improve computer vision methods by accurately
    sensing the physical properties of a scene. This study shows for the first time
    the potential of snapshot multispectral imaging applied to computer vision and
    particularly to image completion. 9 References Citing Literature Volume12, Issue7
    July 2018 Pages 1164-1174 Citation Statements beta Supporting 0 Mentioning 3 Contrasting
    0 Explore this article''s citation statements on scite.ai powered by   Figures
    References Related Information Recommended Single image dehazing using local linear
    fusion Yakun Gao,  Haiyan Chen,  Haibin Li,  Wenming Zhang IET Image Processing
    Hyperspectral image super‐resolution under misaligned hybrid camera system Yonggang
    Lin,  Yongrong Zheng,  Ying Fu,  Hua Huang IET Image Processing Text segmentation
    using superpixel clustering Yuanping Zhu,  Kuang Zhang IET Image Processing Image
    seamless stitching and straightening based on the image block Zhong Qu,  Tengfeng
    Wang,  Shiquan An,  Ling Liu IET Image Processing Realistic endoscopic image generation
    method using virtual‐to‐real image‐domain translation Masahiro Oda,  Kiyohito
    Tanaka,  Hirotsugu Takabatake,  Masaki Mori,  Hiroshi Natori,  Kensaku Mori Healthcare
    Technology Letters Download PDF ABOUT THE IET IET PRIVACY STATEMENT CONTACT IET
    Copyright (2024) The Institution of Engineering and Technology. The Institution
    of Engineering and Technology is registered as a Charity in England & Wales (no
    211014) and Scotland (no SC038698) Additional links ABOUT WILEY ONLINE LIBRARY
    Privacy Policy Terms of Use About Cookies Manage Cookies Accessibility Wiley Research
    DE&I Statement and Publishing Policies HELP & SUPPORT Contact Us Training and
    Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies.'
  inline_citation: '>'
  journal: IET image processing (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Image completion using multispectral imaging
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agriculture13051005
  analysis: '>'
  authors:
  - Luis Emmi
  - Roemi Fernández
  - P. González de Santos
  - Matteo Francia
  - Matteo Golfarelli
  - Giuliano Vitali
  - Hendrik Sandmann
  - Michael Hustedt
  - Merve Meinhardt‐Wollweber
  citation_count: 7
  full_citation: '>'
  full_text: ">\nCitation: Emmi, L.; Fernández, R.;\nGonzalez-de-Santos, P.; Francia,\
    \ M.;\nGolfarelli, M.; Vitali, G.; Sandmann,\nH.; Hustedt, M.; Wollweber, M.\n\
    Exploiting the Internet Resources for\nAutonomous Robots in Agriculture.\nAgriculture\
    \ 2023, 13, 1005. https://\ndoi.org/10.3390/agriculture13051005\nAcademic Editors:\
    \ Jin Yuan, Wei Ji,\nQingchun Feng and Massimo\nCecchini\nReceived: 16 March 2023\n\
    Revised: 17 April 2023\nAccepted: 29 April 2023\nPublished: 2 May 2023\nCopyright:\n\
    © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an\
    \ open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the\
    \ Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nagriculture\nArticle\nExploiting the Internet Resources for Autonomous\
    \ Robots\nin Agriculture\nLuis Emmi 1,*\n, Roemi Fernández 1\n, Pablo Gonzalez-de-Santos\
    \ 1\n, Matteo Francia 2\n, Matteo Golfarelli 2,\nGiuliano Vitali 3\n, Hendrik\
    \ Sandmann 4, Michael Hustedt 4 and Merve Wollweber 4\n1\nCentre for Automation\
    \ and Robotics (UPM-CSIC), 28500 Arganda del Rey, Madrid, Spain\n2\nDepartment\
    \ of Computer Science and Engineering (DISI), Alma Mater Studiorum-University\
    \ of Bologna,\n40127 Bologna, Italy\n3\nDepartment of Agricultural and Food Sciences\
    \ (DISTAL), Alma Mater Studiorum-University of Bologna,\n40127 Bologna, Italy\n\
    4\nLaser Zentrum Hannover e.V., Hollerithallee 8, 30419 Hannover, Germany\n*\n\
    Correspondence: luis.emmi@car.upm-csic.es\nAbstract: Autonomous robots in the\
    \ agri-food sector are increasing yearly, promoting the application\nof precision\
    \ agriculture techniques. The same applies to online services and techniques implemented\n\
    over the Internet, such as the Internet of Things (IoT) and cloud computing, which\
    \ make big data, edge\ncomputing, and digital twins technologies possible. Developers\
    \ of autonomous vehicles understand\nthat autonomous robots for agriculture must\
    \ take advantage of these techniques on the Internet to\nstrengthen their usability.\
    \ This integration can be achieved using different strategies, but existing\n\
    tools can facilitate integration by providing beneﬁts for developers and users.\
    \ This study presents an\narchitecture to integrate the different components of\
    \ an autonomous robot that provides access to\nthe cloud, taking advantage of\
    \ the services provided regarding data storage, scalability, accessibility,\n\
    data sharing, and data analytics. In addition, the study reveals the advantages\
    \ of integrating new\ntechnologies into autonomous robots that can bring signiﬁcant\
    \ beneﬁts to farmers. The architecture is\nbased on the Robot Operating System\
    \ (ROS), a collection of software applications for communication\namong subsystems,\
    \ and FIWARE (Future Internet WARE), a framework of open-source components\nthat\
    \ accelerates the development of intelligent solutions. To validate and assess\
    \ the proposed\narchitecture, this study focuses on a speciﬁc example of an innovative\
    \ weeding application with\nlaser technology in agriculture. The robot controller\
    \ is distributed into the robot hardware, which\nprovides real-time functions,\
    \ and the cloud, which provides access to online resources. Analyzing the\nresulting\
    \ characteristics, such as transfer speed, latency, response and processing time,\
    \ and response\nstatus based on requests, enabled positive assessment of the use\
    \ of ROS and FIWARE for integrating\nautonomous robots and the Internet.\nKeywords:\
    \ precision agriculture; autonomous robots; artiﬁcial intelligence; IoT; cloud\
    \ computing\n1. Introduction\nThe year 2022 ended with more than 8 billion inhabitants\
    \ of the world. Most govern-\nments understand that feeding this vast and growing\
    \ population is one of the signiﬁcant\nchallenges they must face in the coming\
    \ years. Some associations have predicted that\nfood production will need to increase\
    \ by 70% to feed the entire population in 2050 [1].\nIn developed countries, cultivated\
    \ land is close to its maximum output; therefore, the\nsolution is oriented toward\
    \ optimizing the available resources. Many different cultural and\ntechnological\
    \ methods for increasing crop yield are being used. Some improve crop yields,\n\
    but at the extra cost of increasing environmental pollution and the carbon footprint.\
    \ These\nside effects are unacceptable in many industrialized nations, such as\
    \ those in the European\nUnion, which is committed to using sustainable methods.\n\
    Agriculture 2023, 13, 1005. https://doi.org/10.3390/agriculture13051005\nhttps://www.mdpi.com/journal/agriculture\n\
    Agriculture 2023, 13, 1005\n2 of 22\nPrecision agriculture leverages technologies\
    \ to achieve those objectives and avoids\nundesired effects. PA is a concept for\
    \ farm management founded on observation, mea-\nsurement, and response to crop\
    \ variability [2]. It assembles different methods to manage\nvariations in a farm\
    \ to enhance crop yield, improve commercial proﬁt, and guarantee eco-\nenvironmental\
    \ sustainability. PA uses current information and communication technologies\n\
    (ICT), automation, and robotics to monitor crop growth, predict the weather accurately,\n\
    perform optimal irrigation, apply fertilizers smartly, manage weeds and pests\
    \ accurately,\ntest soil quality precisely, etc.\nSince the late 1980s, precision\
    \ agriculture techniques have been introduced step by\nstep in the agricultural\
    \ production sector, integrating the following:\n•\nSensors to acquire geolocated\
    \ biodata of crops and soil, e.g., nitrogen sensors, vision\ncameras, global navigation\
    \ satellite systems (GNSS), etc.\n•\nComputers for analyzing those data and running\
    \ simple algorithms to help farmers\nmake simple decisions (applying or not applying\
    \ a given process, modifying a process\napplication map, etc.).\n•\nActuators\
    \ in charge of executing the decisions (opening/closing valves, altering a\ntrajectory,\
    \ etc.) for modifying crops. As an actuator, we consider the agricultural tool,\n\
    also called the agricultural implement, and the vehicle, manually or automatically\n\
    driven, to move the tool throughout the working ﬁeld and apply the farming process.\n\
    The integration of subsystems onboard robotic vehicles started in the late 1990s.\
    \ Some\nillustrative examples, based on retroﬁtting conventional vehicles, are\
    \ the autonomous\nagricultural sprayer [3], which focuses on achieving a pesticide\
    \ spraying system that\nis cheap, safe, and friendly to the environment, and the\
    \ autonomous orchard vehicles\nfor mowing, tree pruning, and training, spraying,\
    \ blossoming, and fruit thinning, fruit\nharvesting, and sensing [4], both deployed\
    \ in the USA. In Europe, we can ﬁnd the RHEA\nﬂeet (see Figure 1a), consisting\
    \ of a ﬂeet of three tractors that cooperate and collaborate in\nthe application\
    \ of pesticides [5]. Regarding robotic systems based on speciﬁc structures\ndesigned\
    \ for agriculture (see Figure 1b), we can remark on LadyBird in Australia, intended\n\
    for the valuation of crops using thermal and infrared detecting systems, hyperspectral\n\
    cameras, stereovision cameras, LIDAR, and GPS [6], and Vibro Crop Robotti in Europe,\
    \ built\nfor accurate seeding and mechanical row crop cleaning [7]. These robots\
    \ were integrated\naround computing systems based on centralized or elementary\
    \ distributed architectures to\nhandle a few sensors and control unsophisticated\
    \ agricultural tools.\nIn addition to those developments, related technologies\
    \ have evolved drastically\nin recent years, and now sensors can be spread throughout\
    \ the ﬁeld and communicate\nwith each other. This is possible because of the Internet\
    \ of Things (IoT). This computing\nconcept describes how to cluster and interconnect\
    \ objects and devices through the Internet,\nwhere all are visible and can interact\
    \ with each other. IoT deﬁnes physical objects with\ndevices (mainly sensors)\
    \ and includes processing power, software applications, and other\ntechnologies\
    \ to exchange data with other objects through the Internet.\nMoreover, computers\
    \ can run artiﬁcial intelligence (AI) algorithms, considering AI as\nthe ability\
    \ of a machine (computer) to emulate intelligent human actions. The application\n\
    of AI to agriculture has been focused on three primary AI techniques: expert systems,\n\
    artiﬁcial neural networks, and fuzzy systems, with signiﬁcant results in the management\n\
    of crops, pests, diseases, and weeds, as well as the monitoring of agricultural\
    \ production,\nstore control, and yield prediction, for example [8].\nAI techniques\
    \ are also applied to provide vehicles with autonomy; therefore, au-\ntonomous\
    \ agricultural robots leverage this technology. AI-based vision systems can fulﬁll\n\
    the following roles:\n•\nDetecting static or dynamic objects in their surroundings.\n\
    •\nDetecting row crops for steering purposes.\n•\nIdentifying plants and locating\
    \ their positions for weeding are clear examples of the\ncurrent use of AI techniques\
    \ in agricultural robotics [9].\nAgriculture 2023, 13, 1005\n3 of 22\nAgriculture\
    \ 2023, 13, x FOR PEER REVIEW \n \n \nFigure 1. (a) Agricultural robots based\
    \ on retroﬁtted conventional vehicles (RHEA\ncultural robots designed on purpose\
    \ (Courtesy of AgreenCulture SaS). \nMoreover, computers can run artiﬁcial intelligence\
    \ (AI) algorithms, c\nas the ability of a machine (computer) to emulate intelligent\
    \ human action\ntion of AI to agriculture has been focused on three primary AI\
    \ techniques: e\nartiﬁcial neural networks, and fuzzy systems, with signiﬁcant\
    \ results in th\nof crops, pests, diseases, and weeds, as well as the monitoring\
    \ of agricultur\nstore control, and yield prediction, for example [8]. \nAI techniques\
    \ are also applied to provide vehicles with autonomy; th\nomous agricultural robots\
    \ leverage this technology. AI-based vision system\nfollowing roles: \n• \nDetecting\
    \ static or dynamic objects in their surroundings. \n• \nDetecting row crops for\
    \ steering purposes. \n•\nIdentifying plants and locating their positions for\
    \ weeding are clear e\nFigure 1. (a) Agricultural robots based on retroﬁtted conventional\
    \ vehicles (RHEA ﬂeet); (b) agricul-\ntural robots designed on purpose (Courtesy\
    \ of AgreenCulture SaS).\nAnother technology that has evolved in the last decade\
    \ is cloud computing, deﬁned\nas the on-demand delivery of computing services,\
    \ mainly data storage and computing\npower, including servers, storage, databases,\
    \ networking, software applications, artiﬁcial\nintelligence methods, and analytics\
    \ algorithms over the Internet. The main objective of\ncloud computing systems\
    \ is to provide ﬂexible resources at adapted prices. A cloud\ncomputing system\
    \ allows the integration of data of different types, loaded from many\nsources\
    \ in batch and real-time. In particular, the integration can be based on georeferenced\n\
    data in the precision farming area. Data can range from trajectory data to images\
    \ and\nvideos related to ﬁelds and missions and any sensors installed on the autonomous\
    \ robot.\nAgriculture 2023, 13, 1005\n4 of 22\nCloud computing allows the use\
    \ of services available in the cloud (computing, storing,\netc.), with increasing\
    \ advantages provided by big data techniques. Many agricultural\napplications\
    \ of big data technologies have already been introduced in agriculture [10] and\n\
    should be present in future robotic systems.\nThis article presents an architecture\
    \ to integrate new technologies and Internet trends\nin agricultural autonomous\
    \ robotic systems and has two main objectives. The ﬁrst objective\nis to provide\
    \ an example of designing control architectures to connect autonomous robots to\n\
    the cloud. It is oriented toward robot designers and gives signiﬁcant technical\
    \ details. The\nsecond objective is to disclose to farmers the advantages of integrating\
    \ the new technologies\nin autonomous robots that can provide farmers with signiﬁcant\
    \ advantages regarding\n(i) data storage, which is a secure and efﬁcient way to\
    \ store, but also access and share,\ndata, eliminating the need of physical storage\
    \ and, thus, reducing the risk of data loss;\n(ii) scalability, which allow the\
    \ farmers to expand or reduce their storage needs, efﬁciently\noptimizing their\
    \ resources, and (iii) analytics services, which allow a farmer to analyze their\n\
    own data to make informed decisions taking advantage of the AI tools available\
    \ on the\ncloud. These are general advantages of using the cloud, but autonomous\
    \ robots have great\npotential for collecting data and must facilitate communicating\
    \ those data to the cloud.\nTo base the architecture on a speciﬁc example, the\
    \ integration of a laser-based system\nfor weed management is considered. Thus,\
    \ Section 2 presents the material, deﬁning the\nrobot’s components, and the methodology,\
    \ detailing the system’s architecture. Section 3\nthen introduces the experiments\
    \ to be assessed and discussed in Section 4. Finally, Section 5\nsummarizes the\
    \ conclusions.\n2. Materials and Methods\nThis section ﬁrst describes the components\
    \ and equipment integrated for building\nthe autonomous robot used to validate\
    \ and assess the proposed integration methodology.\nSecond, the methods for the\
    \ integration of components are detailed.\n2.1. System Components\n2.1.1. Main\
    \ Process Loop in PA Autonomous Robots\nThe autonomous systems used for precision\
    \ agriculture generally follow the structure\nof an automatic control loop that\
    \ consists of the following (see Figure 2):\n•\nSelecting the references for the\
    \ magnitudes to be controlled, i.e., deﬁning the desired\nplan.\n•\nMeasuring\
    \ the magnitudes of interest.\n•\nMaking decisions based on the measured and desired\
    \ values of the magnitudes\n(control strategy).\n•\nExecuting the decided actions\n\
    In our application, the selecting references are made with the smart navigation\
    \ man-\nager (mission planner), the measures of the magnitudes of interest are\
    \ performed with\nthe perception system and the IoT sensor network, the decisions\
    \ are made with the smart\nnavigation manager (smart operation manager), and the\
    \ actions are executed with the\nagricultural tool and the autonomous robot that\
    \ move the implement throughout the mis-\nsion ﬁeld. In addition, our system also\
    \ takes care of the interaction with the cloud and\nthe operator. In our proposed\
    \ integration method, these components are grouped into\nmodules, as illustrated\
    \ in Figures 2 and 3. These modules are as follows.\nAgriculture 2023, 13, 1005\n\
    5 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n5 of 23 \n \n \n \nFigure 2.\
    \ Components of a precision agriculture robotic system and main information ﬂow.\
    \ \n \nFigure 2. Components of a precision agriculture robotic system and main\
    \ information ﬂow.\nAgriculture 2023, 13, x FOR PEER REVIEW \n5 \n \n \n \nFigure\
    \ 2. Components of a precision agriculture robotic system and main information\
    \ ﬂow. \n \nFigure 3. Computing architecture.\nAgriculture 2023, 13, 1005\n6 of\
    \ 22\n2.1.2. Agricultural Robot\nA manually driven or autonomous vehicle is essential\
    \ in agricultural tasks to perform\nthe necessary actions throughout the working\
    \ ﬁeld. In this case, we use a compact mobile\nplatform based on a commercial\
    \ vehicle manufactured by AgreenCulture SaS, France. This\nis a tracked platform,\
    \ and, thus, it operates as a skid-steer mechanism. The track distance\ncan be\
    \ adapted to the crop row space. Equipped with an engine or batteries, the platform\n\
    can follow predeﬁned trajectories at 6 km/h with a position accuracy of ±0.015\
    \ m using a\nglobal positioning system (GPS) based on the real-time kinematic\
    \ (RTK) technique. This\nmobile platform is illustrated in Figure 4a.\nFigure\
    \ 3. Computing architecture. \n2.1.2. Agricultural Robot \nA manually driven or\
    \ autonomous vehicle is essential in agricultu\nthe necessary actions throughout\
    \ the working ﬁeld. In this case, we us\nplatform based on a commercial vehicle\
    \ manufactured by AgreenC\nThis is a tracked platform, and, thus, it operates\
    \ as a skid-steer mecha\ntance can be adapted to the crop row space. Equipped\
    \ with an eng\nplatform can follow predeﬁned trajectories at 6 km/h with a position\
    \ a\nusing a global positioning system (GPS) based on the real-time kin\nnique.\
    \ This mobile platform is illustrated in Figure 4a. \n \nFigure 4. (a) Mobile\
    \ platform (AgreenCulture SaS) and (b) autonomous laser \n2.1.3. Perception System\
    \  \nA perception system is based on computer vision algorithms t\nanalyze, and\
    \ understand images and data from the environment. Wi\nsystem produces numerical\
    \ and symbolic information for making de\ntion system for this study consists\
    \ of the following systems: \n• \nGuiding vision system: This system aims to detect\
    \ static and d\nFigure 4. (a) Mobile platform (AgreenCulture SaS) and (b) autonomous\
    \ laser weeding system.\n2.1.3. Perception System\nA perception system is based\
    \ on computer vision algorithms that obtain, process,\nanalyze, and understand\
    \ images and data from the environment. With these inputs, the\nsystem produces\
    \ numerical and symbolic information for making decisions. The perception\nsystem\
    \ for this study consists of the following systems:\nAgriculture 2023, 13, 1005\n\
    7 of 22\n•\nGuiding vision system: This system aims to detect static and dynamic\
    \ obstacles in\nthe robot’s path to prevent the robot tracks from stepping on\
    \ the crops during the\nrobot’s motion. Furthermore, it is also used to detect\
    \ crop rows in their early growth\nstage to guide the robot in GNSS-denied areas\
    \ [8]. The selected perception system\nconsisted of a red–green–blue (RGB) wavelength\
    \ vision camera and a time-of-ﬂight\n(ToF) camera attached to the front of the\
    \ mobile platform using a pan-tilt device, which\nallows control of the camera\
    \ angle with respect to the longitudinal axis of the mobile\nplatform, x. Figure\
    \ 4 illustrates both cameras and their locations onboard the robot.\n•\nWeed–meristem\
    \ vision system: The system is based on 3D vision cameras to provide\nthe controller\
    \ with data on crops and weeds. These data are used to carry out the main\nactivity\
    \ of the tool for which it has been designed: weed management, in this case.\n\
    For example, the perception system used in this study consists of an AI vision\
    \ system\ncapable of photographing the ground and discriminating crops from weeds\
    \ in a ﬁrst\nstep using deep learning algorithms. In the second step, the meristems\
    \ of the detected\nweeds are identiﬁed. Figure 3 sketches this procedure.\n2.1.4.\
    \ Agricultural Tools\nAgricultural tools focus on direct action on the crop and\
    \ soil and rely on physical\n(mechanical, thermal, etc.) or chemical (pesticides,\
    \ fertilizers, etc.) foundations. This study\nused a thermal weeding tool based\
    \ on a high-power laser source that provided lethal laser\ndoses to be deployed\
    \ on the weed meristems using scanners.\nAn AI video system provided the positions\
    \ of the weed meristems. Indeed, this\nspeciﬁc solution physically integrated\
    \ the AI vision system, the laser scanner, and the\nhigh-power laser source into\
    \ the laser-based weeding tool component. The video frames\nacquired with this\
    \ system were sent to the central controller at a rate of 4 frames/s. After\n\
    the mission, all stored images were sent to the cloud.\n2.1.5. The Smart Navigation\
    \ Manager (SNM)\nThis manager is a distributed software application responsible\
    \ for driving the au-\ntonomous robot and coordinating all other modules and systems.\
    \ The SNM is split into\n(i) the smart operation manager and (ii) the central\
    \ manager, which also includes the\nhuman–machine interface (HMI).\nSmart Operation\
    \ Manager (SoM)\nThe smart operation manager is a human–computer interaction module\
    \ that can\nacquire, process, and deliver information based on computer algorithms\
    \ and is devoted to\nassisting farmers in making accurate, evidence-based decisions.\
    \ The SoM is specialized for\nlaser weeding technology, the tool selected for\
    \ this study.\nData management is performed through the Internet using FIWARE.\
    \ Data access\ncontrol is provided via a virtual private network (VPN) to secure\
    \ data transfer to/from\nthe cloud. The visual dashboard will also be available\
    \ on the HMI for ﬁeld operations.\nThrough the dashboard, the operator will also\
    \ interact with the robot.\nThe smart operation manager is allocated in the cloud.\
    \ It contains the global mission\nplanner and supervisor, the map builder, and\
    \ the module for managing the IoT and cloud\ncomputing system (see Figures 3 and\
    \ 5). The hardware of the SoM relies on a cluster of 10\nservers.\nAgriculture\
    \ 2023, 13, 1005\n8 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n8 of 23 \n\
    \ \n \n \nFigure 5. Cloud computing modules/containers. \n(a) Global Mission Planner\
    \ \nA planner is a software tool responsible for computing the trajectories of\
    \ the vehicle \nand an a priori known treatment map. The planner obtains some\
    \ types of information \nfrom the Internet, including the following: \n• \nMap\
    \ information according to the data models on the Internet; \n• \nOther information\
    \ provided by third parties, such as weather forecasts; \n• \nData models to create\
    \ maps for accessing already known treatment maps (sets of \npoints in the ﬁeld)\
    \ which commonly originate from third-party map descriptions \n(Google Earth;\
    \ Geographic Information System (GIS); GeoJSON.io, an open standard \nformat to\
    \ represent geographical features with nonspatial qualities). \nRegarding robot\
    \ location, two types of systems are envisaged, as follows: \n• \nAbsolute location\
    \ based on GNSS: GNSS integrates several controllers for line track-\ning and\
    \ is based on Dubins paths [11]; \no \nRelative location based on RGB and ToF\
    \ cameras, LIDAR, and IoT sensors: \nThese methods are based on diﬀerent techniques\
    \ for navigation in the ﬁeld and \nnavigation on the farm, such as hybrid topological\
    \ maps, semantic localization \nand mapping, and identiﬁcation/detection of natural\
    \ and artiﬁcial elements \n(crops, trees, people, vehicles, etc.) through machine\
    \ learning techniques. \n(b) Global Mission Supervisor \nA supervisor is a computational\
    \ tool responsible for overseeing and monitoring the \nexecution of the mission\
    \ plan while helping the farmer (operator) manage potential fail-\nures. Most\
    \ supervisor systems are designed around two actions: fault detection and fault\
    \ \ndiagnosis. The supervisor executes the following actions: \n• \nReceiving\
    \ alarms from the system components (vehicle, sensors, weeding tool, etc.). \n\
    • \nDetecting faults in real-time. \n• \nExecuting diagnosis protocols. \nFigure\
    \ 5. Cloud computing modules/containers.\n(a)\nGlobal Mission Planner\nA planner\
    \ is a software tool responsible for computing the trajectories of the vehicle\n\
    and an a priori known treatment map. The planner obtains some types of information\
    \ from\nthe Internet, including the following:\n•\nMap information according to\
    \ the data models on the Internet;\n•\nOther information provided by third parties,\
    \ such as weather forecasts;\n•\nData models to create maps for accessing already\
    \ known treatment maps (sets of\npoints in the ﬁeld) which commonly originate\
    \ from third-party map descriptions\n(Google Earth; Geographic Information System\
    \ (GIS); GeoJSON.io, an open standard\nformat to represent geographical features\
    \ with nonspatial qualities).\nRegarding robot location, two types of systems\
    \ are envisaged, as follows:\n•\nAbsolute location based on GNSS: GNSS integrates\
    \ several controllers for line tracking\nand is based on Dubins paths [11];\n\
    #\nRelative location based on RGB and ToF cameras, LIDAR, and IoT sensors: These\n\
    methods are based on different techniques for navigation in the ﬁeld and navi-\n\
    gation on the farm, such as hybrid topological maps, semantic localization and\n\
    mapping, and identiﬁcation/detection of natural and artiﬁcial elements (crops,\n\
    trees, people, vehicles, etc.) through machine learning techniques.\n(b)\nGlobal\
    \ Mission Supervisor\nA supervisor is a computational tool responsible for overseeing\
    \ and monitoring\nthe execution of the mission plan while helping the farmer (operator)\
    \ manage potential\nfailures. Most supervisor systems are designed around two\
    \ actions: fault detection and\nfault diagnosis. The supervisor executes the following\
    \ actions:\n•\nReceiving alarms from the system components (vehicle, sensors,\
    \ weeding tool, etc.).\n•\nDetecting faults in real-time.\n•\nExecuting diagnosis\
    \ protocols.\nAgriculture 2023, 13, 1005\n9 of 22\n•\nCollecting all available\
    \ geo-referred data generated by every module onboard the\nrobot. The data are\
    \ stored in both the robot and the cloud.\n(c)\nMap Builder\nA map builder is\
    \ an application used to convert maps based on GeoJSON into FIWARE\nentities.\
    \ Its main function is to support farmers in using the robotic system in a simple,\n\
    reliable, and robust way by giving the robot enough information a priori (e.g.,\
    \ farm schema\nand boundaries, ﬁeld locations and shapes, crop types, and status).\
    \ This module takes\nadvantage of the data models created by the FIWARE community\
    \ to represent the farm and\nother environments digitally, where they have been\
    \ conditioned to be adapted to robotic\nsystems and especially oriented to navigation\
    \ [12]. The design of the Map Builder allows\nthe user to accomplish the following:\n\
    •\nSelect the ﬁeld in GeoJSON.IO, an open-source geographic mapping tool that\
    \ allows\nmaps and geospatial data to be created, visualized, and shared in a\
    \ simple and\nmultiformat way.\n•\nAssign essential attributes to comply with\
    \ FIWARE. These attributes are those based\non the farmer’s knowledge. They can\
    \ include static (i.e., location, type, category) and\ndynamic (i.e., crop type\
    \ and status, seeding date, etc.) attributes.\n•\nExport in * GeoJSON format.\
    \ The map obtained will be imported for extracting the\ninformation required to\
    \ ﬁll in the FIWARE templates, which include the farms and\nparcel data models,\
    \ and other elements in a farm, such as buildings and roads.\nThis conversion\
    \ makes it easier to connect the robot to the cloud by standardizing\ndata. These\
    \ data, after processing, constitute a source for the design of processes with\
    \ the\nrobot, and its storage and subsequent analysis can provide forecasts of\
    \ future events in the\nﬁeld or behavior of the robot.\n(d)\nIoT System\nThis\
    \ study integrates an IoT sensor network to collect data from the following:\n\
    •\nThe autonomous vehicle: The data and images acquired with IoT sensors onboard\
    \ the\nvehicle are used to monitor and evaluate performances and efﬁciency and\
    \ to identify\nthe effects of treatments and trafﬁc on surfaces.\n•\nThe environment:\
    \ Data acquired with IoT sensors deployed on the cropland are used\nto (i) monitor\
    \ crop development and (ii) collect weather and soil information.\nTwo IoT sets\
    \ of devices are used in our study, as follows:\n•\nRobot–IoT set: It consists\
    \ of two WiFi high-deﬁnition cameras installed onboard the\nautonomous robot (IoT-R1\
    \ and IoT-R2 in Figure 3). The cameras are triggered from\nthe cloud or the central\
    \ controller to obtain a low frame rate (approximately 1/5 sec).\nThe pictures\
    \ are stored in the cloud and are used to monitor the effects of the passage\n\
    of the autonomous vehicle; therefore, they should include the robot’s tracks.\n\
    •\nField–IoT set: It consists of the following (see Figure 3):\n#\nTwo multispectral\
    \ cameras (IoT-F1 and IoT-F2) placed at the boundary of cropped\nareas to obtain\
    \ hourly pictures of crops.\n#\nA weather station (IoT-F3) to measure precipitation,\
    \ air temperature (Ta), relative\nhumidity (RH), radiation, and wind.\n#\nThree\
    \ soil multi-depth probes (IoT-F4) for acquiring moisture (Ts) data and three\n\
    respiration probes (IoT-F5) to measure CO2 and H2O.\nEvery one of these components\
    \ or nodes exchanges messages with the Message\nQueuing Telemetry Transport (MQTT)\
    \ protocol, carrying JavaScript Object Notation (JSON)\nserialized information\
    \ from node sensors/cameras interpreted as the entity. While metering\nnodes (weather,\
    \ soil probe, and respirometer) communicate by MQTT messages, camera\nnodes have\
    \ to transmit images (maximum of 100 pictures/day for periodic snapshots of\n\
    the area or alarms), and the use of FTP made a wide-band networking solution,\
    \ such as\nWiFi, mandatory instead of narrowband solutions.\nAgriculture 2023,\
    \ 13, 1005\n10 of 22\n(e)\nCloud Computing System\nThis study sets up a cloud-based\
    \ data platform, which is an ecosystem that incorpo-\nrates data acquired in the\
    \ ﬁeld. The data platform supports end-to-end data needs, such as\ningestion,\
    \ processing, and storage, to provide the following:\n•\nA data lake repository\
    \ for storing mission data to be downloaded in batches for\npost-mission analysis.\n\
    •\nA web interface for post-mission data analysis based on graphical dashboards,\
    \ georef-\nerenced visualizations, key performance indicators, and indices.\n\
    •\nA container framework for implementing “Decision Support System” functionalities\n\
    that deﬁne missions to be sent to the robot. These functionalities (e.g., the\
    \ mission\nplanner) can be implemented and launched from the cloud platform.\n\
    •\nA soft real-time web interface for missions. The interface visualizes real-time\
    \ robot\nactivities and performances or sends high-level commands to the robot\
    \ (e.g., start,\nstop, change mission).\nThese functionalities are ordered based\
    \ on the strictness of real-time constraints.\nThe cloud-computing platform is\
    \ based on the Hadoop stack and is powered by\nFIWARE. We adopted an open-source\
    \ solution with well-known components that can be\nimported into different cloud\
    \ service providers if no on-premises hardware is available.\nThe core component\
    \ of the platform is the (FIWARE) Orion Context Broker (OCB) from\nTelefonica\
    \ [13], a publish/subscribe context broker that also provides an interface to\
    \ query\ncontextual information (e.g., obtain all images from the cameras in a\
    \ speciﬁc farm), update\ncontext information (e.g., update the images), and be\
    \ notiﬁed when the context is updated\n(e.g., when a new image is added into the\
    \ platform). The images and raw data are stored in\nthe HDFS (Hadoop distributed\
    \ ﬁle system), while the NoSQL (not only structured query\nlanguage) MongoDB database\
    \ is used to collect the contextual data from FIWARE and\nfurther metadata necessary\
    \ to manage the platform [14]. Additionally, we use Apache\nKAFKA, an open-source\
    \ distributed event bus, to distribute context updates from FIWARE\nto all the\
    \ modules/containers hosted on the cloud platform. The different cloud computing\n\
    modules/containers used in this study are illustrated in Figure 5.\nCentral Manager\n\
    This central manager is an application that is divided into the following:\n•\n\
    Obstacle detection system. This module acquires visual information from the front\
    \ of\nthe robot (robot vision system) to detect obstacles based on machine vision\
    \ techniques.\n•\nLocal mission planner and supervisor. The planner plans the\
    \ motion of the robot near\nits surroundings. The local mission supervisor oversees\
    \ the execution of the mission\nand reports malfunctions to the operator (see\
    \ Section 2.1.5).\n•\nGuidance system. This system is responsible for steering\
    \ the mobile platform to follow\nthe trajectory calculated by the planner. It\
    \ is based on the GNSS if its signal is available.\nOtherwise, the system uses\
    \ the information from the robot vision system to extract the\ncrop row positions\
    \ and follow them without harming the crop.\n•\nHuman–machine interface\nA human–machine\
    \ interface (HMI) is a device or program enabling a user to commu-\nnicate with\
    \ another device, system, or machine. In this study, a HMI using portable devices\n\
    (android tablets) is addressed to allow farmers to perform the following:\n-\n\
    Supervise the mission.\n-\nMonitor and control the progress of agricultural tasks.\n\
    -\nIdentify and solve operational problems.\n-\nObtain real-time in-ﬁeld access\
    \ in an ergonomic, easy-to-use, and robust way.\n-\nMaintain the real-time safety\
    \ of the entire system.\nTo achieve these characteristics, a graphic device was\
    \ integrated with the portable/remote\ncontroller of the mobile platform. This\
    \ controller provides manual and remote vehicle\ncontrol and integrates an emergency\
    \ button.\nAgriculture 2023, 13, 1005\n11 of 22\n2.1.6. Sequence of Actions\n\
    The relationships among these components and modules and the information ﬂow\n\
    are illustrated in Figures 2 and 3. The process is a repeated sequence of actions\
    \ (A0 to A6),\ndeﬁned as follows:\nA0\nThe system is installed in the ﬁeld, The\
    \ operator/farmer deﬁnes or selects a previously\ndescribed mission using the\
    \ HMI and starts the mission.\nA1\nThe sensors of the perception module (M1) installed\
    \ onboard the autonomous robot\n(M2) extract features from the crops, soil, and\
    \ environment in the area of interest in\nfront of the robot.\nA2\nThe data acquired\
    \ in action A1 are sent to the smart operation manager, determining\nthe consequent\
    \ instructions for the robots and the agricultural tool.\nA3\nThe required robot\
    \ motions and agricultural tool actions are sent to the robot controller,\nwhich\
    \ generates the signal to move the robot to the desired positions.\nA4\nThe robot\
    \ controller forwards the commands sent by the smart navigation manager\nor generates\
    \ the pertinent signals for the agricultural tool to carry out the treatment.\n\
    A5\nThe treatment is applied, and the procedure is repeated from action A1 to\
    \ action A5\nuntil ﬁeld completion (A6).\nA6\nEnd of mission.\n2.2. Integration\
    \ Methods\nIntegrating all of the components deﬁned in the previous section to\
    \ conﬁgure an\nautonomous robot depends on the nature of the applications the\
    \ robot is devoted to and the\nconnections and communication among the different\
    \ components that must be precisely\ndeﬁned. Thus, this section ﬁrst describes\
    \ the computing architecture of the controller,\nwhich integrates the different\
    \ subsystems and modules. Second, the interfaces between\nsubsystems are precisely\
    \ deﬁned. Finally, the operation procedure is deﬁned.\n2.2.1. Computing Architecture\n\
    A distributed architecture based on an open-source Robot Operating System (ROS)\
    \ is\nproposed to integrate the system’s main components onboard the mobile platform\
    \ in this\nstudy. ROS is the operating system most widely accepted by software\
    \ developers to create\nrobotics applications. It consists of a set of software\
    \ libraries and tools that include drivers\nand advanced algorithms to help developers\
    \ build robot applications [15].\nIn this study, ROS, installed in the central\
    \ controller, is used as a meta-operating system\nfor the testing prototype. The\
    \ necessary interfaces (bridges) are developed to establish\ncommunication with\
    \ the autonomous vehicle, the perception system, and the laser-based\nweeding\
    \ tool. Because of ROS versatility and its publisher/subscriber communication\n\
    model, it is possible to adapt the messages to protocols commonly used in IoT,\
    \ such as\nMessage Queuing Telemetry Transport (MQTT).\nROS supports software\
    \ developers in creating robotics functionalities to monitor and\ncontrol robot\
    \ components connected to a local network. However, this solution is not\nextendible\
    \ to a wider network, such as the Internet. Fortunately, there exist some ROS\n\
    modules that solve the problem. One is ROSLink, a protocol for extensions deﬁning\
    \ an\nasynchronous communication procedure between the users and the robots through\
    \ the\ncloud [16]. ROSLink performance has been shown to be efﬁcient and reliable,\
    \ and it is\nwidely accepted by the robotics software community [17]. Although\
    \ ROSLink has been\nwidely used to connect robotic systems with the cloud, it\
    \ is oriented toward transmitting\nlow-level messages. There is no convention\
    \ to deﬁne standard data models that allow\nintelligent robotics systems to be\
    \ scalable.\nOne alternative to a more internet-oriented communication framework\
    \ is FIWARE,\nwhich offers interaction with the cloud using cloud services that\
    \ provide well-known bene-\nﬁts, such as (a) cost and ﬂexibility, (b) scalability,\
    \ (c) mobility, and (d) disaster recovery [18].\nFIWARE is an open software curated\
    \ platform fostered by the European Commission\nand the European Information and\
    \ Communication Technology (ICT) industry for the\nAgriculture 2023, 13, 1005\n\
    12 of 22\ndevelopment and worldwide deployment of Future Internet applications.\
    \ It attempts to\nprovide a completely open, public, and free architecture and\
    \ a collection of speciﬁcations\nthat allows organizations (designers, service\
    \ providers, businesses, etc.) to develop open\nand innovative applications and\
    \ services on the Internet that fulﬁll their needs [19].\nIn this study, a cloud-based\
    \ communication architecture has been implemented us-\ning FIWARE as the core,\
    \ which allows messages between the edge and the cloud to be\ntransferred and\
    \ stored. The selection was made because this is an open-source platform\nthat\
    \ provides free development modules and has many enablers already developing and\n\
    integrating solutions for smart agriculture.\nIn addition to FIWARE, we use KAFKA,\
    \ a robust distributed framework for streaming\ndata (see Section 2.1.5) that\
    \ allows producers to send data and for consumers to subscribe to\nand process\
    \ such updates. KAFKA enables the processing of streams of events/messages\nin\
    \ a scalable and fault-tolerant manner, and decouples producers and consumers\
    \ (i.e., a\nconsumer can process data even after a producer has gone ofﬂine).\
    \ For historic data, HDFS\nallows the download of batches of data at any time\
    \ and replicates each data in three copies\nto prevent data loss.\nThe visual\
    \ dashboard will also be available on the HMI for the ﬁeld operations.\nThrough\
    \ the dashboard, the operator will also interact with the robot. FIWARE smart\
    \ data\nmodels do not sufﬁce to represent our application domain or to integrate\
    \ the agricultural\nand robotic domains; therefore, we have extended the existing\
    \ models and updated some\nexisting entities. Since smart data models from FIWARE\
    \ are overlapping and sometimes\ninconsistent, we had to envision a uniﬁed model\
    \ to integrate and reconcile the data. To\nconnect the robotic system with the\
    \ cloud, speciﬁc data models were developed to represent\nthe different robotic\
    \ elements, following the guidelines of FIWARE and its intelligent data\nmodels\
    \ [12].\nThe IoT devices deployed in the ﬁeld must be able to establish connections\
    \ through\nWiFi and LoRa technologies. WiFi is a family of wireless network protocols.\
    \ These protocols\nare generally used for Internet access and communication in\
    \ local area networks, allowing\nnearby electronic devices to exchange data using\
    \ radio waves. LoRa technology is a\nwireless protocol designed for long-range\
    \ connectivity and low-power communications\nand is primarily targeted for the\
    \ Internet of Things (IoT) and M2M networks. LoRa tolerates\nnoise, multipath\
    \ signals, and the Doppler effect. The cost of achieving this is a very low\n\
    bandwidth compared to other wireless technologies. This study uses a 4G LTE-M\
    \ modem\nto connect to the Internet.\nAt a lower level of communication, CANbus\
    \ or ISOBUS is generally used to control\nand monitor the autonomous vehicle.\
    \ This study uses CANbus and its communication\nprotocol CANopen. Autonomous vehicles\
    \ and agricultural tools typically contain their\nown safety controllers. The\
    \ ﬁrst behaves as a master and, in the case of a risky situation, it\ncommands\
    \ the tool to stop.\nThe human–machine interface (HMI) will include a synchronous\
    \ remote procedure\ncall-style communication over the services protocol and asynchronous\
    \ communications to\nensure the robot’s safety. In addition to these ROS-based\
    \ protocols, the HMI has a safety\ncontrol connected to the low-level safety system\
    \ (by radiofrequency) for emergency stops\nand manual control.\nFigure 6 illustrates\
    \ the overall architecture, indicating the following:\n•\nThe modules (Mi), presented\
    \ in the previous sections.\n•\nThe interconnection between modules, presented\
    \ in the next section.\n•\nThe communication technologies and protocols to conﬁgure\
    \ agricultural robotic sys-\ntems that integrate IoT and cloud computing technologies.\n\
    The main characteristics of this architecture are summarized in Table 1.\nAgriculture\
    \ 2023, 13, 1005\n13 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n13 of 23\
    \ \n \n \nThe main characteristics of this architecture are summarized in Table\
    \ 1. \n \nFigure 6. Experimental ﬁelds. \nTable 1. Architecture components. \n\
    Architecture Component \nSolutions/Comments \nOperating system \nROS (Robot Operating\
    \ System) \nIoT–controller bridge \nHypertext Transfer Protocol (HTTP) to FIWARE\
    \ \nNote: FIWARE is used as a communication protocol in the \ncloud; therefore,\
    \ it is not necessary to use ROSLink. \nROS-based system for FI-\nWARE tools \n\
    HTTP protocol to FIWARE \nNote: FIROS has several disadvantages when developing\
    \ \nnew data models to represent the robot, so a particular ena-\nbler will not\
    \ be used to establish communication between \nthe robot and the cloud. \nCommunication\
    \ with IoT \ndevices \nWiFi, serial communication \nFigure 6. Experimental ﬁelds.\n\
    Table 1. Architecture components.\nArchitecture Component\nSolutions/Comments\n\
    Operating system\nROS (Robot Operating System)\nIoT–controller bridge\nHypertext\
    \ Transfer Protocol (HTTP) to FIWARE\nNote: FIWARE is used as a communication\
    \ protocol in the cloud;\ntherefore, it is not necessary to use ROSLink.\nROS-based\
    \ system for FIWARE tools\nHTTP protocol to FIWARE\nNote: FIROS has several disadvantages\
    \ when developing new data\nmodels to represent the robot, so a particular enabler\
    \ will not be\nused to establish communication between the robot and the cloud.\n\
    Communication with IoT devices\nWiFi, serial communication\nNote: Since a certain\
    \ amount of data needs to be transmitted, WiFi\nwould sufﬁce.\nThe Internet\n\
    4G LTE-M modem\nDevices onboard the mobile platform\nCANopen, serial\nHuman–machine\
    \ interface (HMI).\nSynchronous remote procedure call-style communication over\n\
    services protocol.\nAsynchronous communications to ensure the safety of the robot.\n\
    Note: The HMI is used to provide access to SoM services through a\nweb interface.\n\
    Agriculture 2023, 13, 1005\n14 of 22\n2.2.2. Interfaces between System Components\n\
    This architecture considers four main interfaces between systems and modules,\
    \ as\nfollows:\nSmart Navigation Manager (M4)/Perception System (M1) interface\n\
    To receive the raw information from the perception system (sensors, cameras, etc.),\n\
    the central manager uses direct connections via the transmission control protocol/Internet\n\
    protocol (TCP/IP) for sensors and the universal serial bus (USB) for RGB and ToF\
    \ cameras.\nAll IoT devices use the available wireless communication technologies\
    \ (WiFi and LoRa) to\naccess the Internet and the cloud.\nTo guide the robot,\
    \ the obstacle detection system obtains data from the guiding\nvision system (RGB\
    \ and ToF cameras) through the Ethernet that communicates the central\nmanager\
    \ with the perception system. This communication is stated using the ROS manager\n\
    and the perception–ROS bridge (see Figure 3).\nSmart Navigation Manager (M4)/Agricultural\
    \ Tool (M3) interface\nThese systems can communicate through ROS messaging protocols,\
    \ where the pub-\nlisher/subscriber pattern is preferred. This interface exchanges\
    \ simple test messages to\nverify the communication interface.\nIt is worth mentioning\
    \ that the perception system and the agricultural tool are con-\nnected directly\
    \ in some speciﬁc applications. This solution decreases the latency of data\n\
    communication but demands moving a portion of the decision algorithms from the\
    \ smart\nnavigation manager to the tool controller; therefore, the tool must exhibit\
    \ computational\nfeatures. This scheme is used in the weeding system to test the\
    \ proposed architecture.\nSmart Navigation Manager (M4)/Autonomous Robot (M2)\
    \ interface\nInitially, these systems communicate via CANbus with the CANopen\
    \ protocol. The\ncentral manager uses this protocol to receive information on\
    \ the status of the autonomous\nvehicle and basic information from the onboard\
    \ sensors (GNSS, IMU, safety system, etc.).\nA CANbus–ROS bridge is used to adapt\
    \ the communication protocols.\nAutonomous Robot (M2)/Agricultural Tool (M3) interface\n\
    Usually, it is not necessary for the vehicle to directly communicate with the\
    \ tool because\nthe smart navigation manager coordinates them. However, as autonomous\
    \ vehicles and\nagricultural tools usually have safety controllers, there is wired\
    \ communication between\nthe two safety controllers. In such a case, the autonomous\
    \ vehicle safety controller works as\na master and commands the tool safety controller\
    \ to stop the tool if a dangerous situation\nappears.\nPerception System (M1)/Agricultural\
    \ Tool (M3)\nThis communication is required to inform the agricultural tools about\
    \ the crop status.\nIn weeding applications, the information is related to the\
    \ positions of the weeds. In\nthis speciﬁc application, the perception system\
    \ (weed meristem detection module) sends\nthe weed meristem positions to the laser\
    \ scanner module of the agricultural tool. This\ncommunication is carried out\
    \ using a conventional Ethernet connection. The metadata\ngenerated via the detection\
    \ system are made available in the existing ROS network and\nsent to the smart\
    \ navigation manager.\nSmart Navigation Manager internal/cloud communications\n\
    The smart navigation manager is a distributed system that consists of three main\n\
    modules:\n•\nThe central manager running on the central controller.\n•\nThe smart\
    \ operation manager running on the cloud.\n•\nThe HMI running in a portable device.\n\
    Agriculture 2023, 13, 1005\n15 of 22\nThe central manager and the smart operation\
    \ manager communicate via NGSI v2,\na FIWARE application programming interface,\
    \ using a FIWARE–ROS bridge to adapt\nROS protocols to NGSI v2 messages. In contrast,\
    \ the HMI communicates with the central\nmanager via WiFi and Internet, directly\
    \ accessing the web services hosted in the cloud.\nThe HMI exhibits a panic button\
    \ connected via radiofrequency to the safety systems of the\nautonomous robot\
    \ and the agricultural tool.\nIoT system/Cloud\nThere is a direct link from the\
    \ IoT system to the cloud using MQTT.\n2.2.2.8. Operation Procedure\nTo use the\
    \ proposed architecture and method, the user must follow the method below.\n•\n\
    Creating the map: The user creates the ﬁeld map following the procedure described\
    \ in\nthe MapBuilder module (see Section 2.1.5).\n•\nCreating the mission: The\
    \ user creates the mission by selecting the mission’s initial\npoint (home garage)\
    \ and destination ﬁeld (study site).\n•\nSending the mission: The user selects\
    \ the mission to be executed with the HMI (all\ndeﬁned missions are stored in\
    \ the system) and sends it to the robot using the cloud\nservices (see Section\
    \ Smart Operation Manager (SoM)).\n•\nExecuting the mission: The mission is executed\
    \ autonomously following the sequence\nof actions described in Section 2.1.6.\
    \ The user does not need to act except for when\nalarms or collision situations\
    \ are detected and warned of by the robot.\n•\nApplying the treatment: When the\
    \ robot reaches the crop ﬁeld during the mission, it\nsends a command to activate\
    \ the weeding tool, which works autonomously. The tool\nis deactivated when the\
    \ robot performs the turns at the headland of the ﬁeld and is\nstarted again when\
    \ it re-enters. The implement was designed to work with its own\nsensory and control\
    \ systems, only requiring the mobile platform for mobility and\ninformation when\
    \ it must be activated/deactivated.\n•\nSupervising the mission: When the robotic\
    \ system reaches the crop ﬁeld, it also sends\na command to the IoT sensors, warning\
    \ that the treatment is in progress. Throughout\nthe operation, the mission supervisor\
    \ module analyzes all the information collected by\nthe cloud computing system,\
    \ generated by both the robotic system and the IoT sensors.\nIt evaluates if there\
    \ is a possible deviation from the trajectory or risk of failure.\n•\nEnding the\
    \ mission: The mission ends when the robot reaches the last point in the\nﬁeld\
    \ map computed by the MapBuilder. Optionally, the robot can stay in the ﬁeld or\n\
    return to the home garage. During the mission execution, the user can stop, resume,\n\
    and abort the mission through the HMI.\n3. Experimental Assessment\nThis section\
    \ states the characteristics of the described autonomous robot with IoT\nand cloud\
    \ computing connectivity. To achieve this purpose, the experimental ﬁeld for this\n\
    study is ﬁrst described. Then, a test mission is deﬁned to acquire data from the\
    \ different\nsubsystems. Finally, the system characteristics are analyzed and\
    \ assessed.\nThe characteristics obtained are not compared with similar robotic\
    \ systems due to\nthe lack of such information in the literature. There are no\
    \ published results in weeding\napplications; therefore, it is difﬁcult to compare,\
    \ and the indicators have been geared\ntowards general cloud computing and mobile\
    \ robotics characteristics. Therefore, cross-\nvalidation has been carried out,\
    \ comparing the features of the autonomous robot with the\ngeneral performance\
    \ of the robot and cloud communication. Productivity, cost, and other\nindicators\
    \ of the presented architecture are those of the general use of cloud computing.\n\
    3.1. Study Site\nThe system developed for this study was tested in an experimental\
    \ ﬁeld located in\nMadrid, Spain (40◦18′45.166′′, −3◦28′51.096′′). The climate\
    \ of the study site is classiﬁed as\nAgriculture 2023, 13, 1005\n16 of 22\na hot\
    \ summer Mediterranean climate with an average annual temperature of 14.3 ◦C and\n\
    precipitation of 473 mm.\nThe experimental ﬁeld consisted of two areas of 60 ×\
    \ 20 m2 that grew wheat (Triticum\naestivum L.), with crop rows at a distance\
    \ of 0.10 m, and maize (Zea mays L.), with crop\nrows at a distance of 0.50 m,\
    \ respectively. Each area was divided into three sections of\n20 × 20 m2. The\
    \ sections in one area were seeded in consecutive weeks, allowing us to\nconduct\
    \ experiments in three-week windows. Figure 6 shows the experimental ﬁeld and\n\
    the distribution of the areas and sections.\n3.2. Description of the Test Mission\n\
    Tests were conducted to assess the performance and quality of integrating new\
    \ tech-\nnologies in autonomous robots for agriculture. First, the testing prototype\
    \ was integrated\nwith the components introduced in Section 2; then, several IoT\
    \ devices were disseminated\nthroughout the ﬁeld (RGB and multispectral cameras,\
    \ weather stations, soil probes, etc.);\nﬁnally, a mission was deﬁned to acquire\
    \ data in the study site to perform quantitative\nanalyses. The mission consisted\
    \ of covering sections of 20 × 20 m2 with wheat and maize\ncrops while the following\
    \ occurred:\n•\nAcquiring data from the IoT sensor network.\n•\nTaking pictures\
    \ of the crop.\n•\nAcquiring data from the guidance system.\n•\nSending all the\
    \ acquired information to the cloud.\nThe mission proposed by the planner is illustrated\
    \ in Figure 7. The robot tracked the\npath autonomously, and the following procedures\
    \ were carried out.\nAgriculture 2023, 13, x FOR PEER REVIEW \n17 of 23 \n \n\
    \ \nFigure 7. Robot’s path from the home garage to the study site. The planner\
    \ provides the mission for \ncovering the study site. \nPerception system procedure\
    \ \n• \nGuiding vision system: This experiment was conducted in the treatment\
    \ stage, where \nthe crop was detected to adjust the errors derived from planning\
    \ and the lack of pre-\ncision of the maps. YOLOv4 [20], a real-time object detector\
    \ based on a one-stage \nobject detection network, was the base model for detecting\
    \ early-stage growth in \nmaize [8], a wide-row crop. The model was trained using\
    \ a dataset acquired in an \nagricultural season before these tests using the\
    \ same camera system [21]. Moreover, \nin the case of wheat, which is a narrow-row\
    \ crop, a diﬀerent methodology was ap-\nplied through the use of segmentation\
    \ models, such as MobileNet, a convolutional \nl\nk f\nbil\ni i\nli\ni\n[22]\n\
    i\nd\ni\nd\ni\nd\nHome garage \nStudy site \nFigure 7. Robot’s path from the home\
    \ garage to the study site. The planner provides the mission for\ncovering the\
    \ study site.\nPerception system procedure\n•\nGuiding vision system: This experiment\
    \ was conducted in the treatment stage, where\nthe crop was detected to adjust\
    \ the errors derived from planning and the lack of\nprecision of the maps. YOLOv4\
    \ [20], a real-time object detector based on a one-stage\nAgriculture 2023, 13,\
    \ 1005\n17 of 22\nobject detection network, was the base model for detecting early-stage\
    \ growth in\nmaize [8], a wide-row crop. The model was trained using a dataset\
    \ acquired in an\nagricultural season before these tests using the same camera\
    \ system [21]. Moreover, in\nthe case of wheat, which is a narrow-row crop, a\
    \ different methodology was applied\nthrough the use of segmentation models, such\
    \ as MobileNet, a convolutional neural\nnetwork for mobile vision applications\
    \ [22], trained using a dataset acquired in an\nagricultural season before these\
    \ tests [23], with the same camera system. The detection\nof both crops was evaluated\
    \ with regard to the GNSS positions collected manually for\nthe different crop\
    \ lines.\nThe maize and wheat datasets were built with 450 and 125 labeled images,\
    \ respectively.\nData augmentation techniques (rotating, blurring, image cropping,\
    \ and brightness changes)\nwere used to increase the size of the datasets. For\
    \ both crops, 80% of the data was destined\nfor training, 10% for validation,\
    \ and 10% for testing.\n•\nThe AI vision system: This system uses data from the\
    \ installed RGB cameras to enable\nrobust automated plant detection and discrimination.\
    \ For this purpose, the state-\nof-the-art object detection algorithm Yolov7 is\
    \ used in combination with the Nvidia\nframework DeepStream. Tracking the detected\
    \ plants is performed in parallel by a\npretrained DeepSort algorithm [24]. The\
    \ reliability of the object detection algorithm\nis evaluated using test datasets\
    \ with the commonly used metrics “intersection over\nunion” (IoU) and “mean average\
    \ precision” (mAP). This system works cooperatively\nwith laser scanners as a\
    \ stand-alone system. The information is not stored in the cloud.\nThe dataset\
    \ used for training weed/crop discrimination was generated in ﬁelds in\nseveral\
    \ European countries. It contains 4000 images, 1000 of which are fully labeled.\n\
    Distinctions are made according to the processing steps to be applied: weeds,\
    \ grasses,\nand crops. In addition, the dataset was expanded to three times its\
    \ original size through\naugmentation measures. As well as generating new training\
    \ data, this enables robustness\nagainst changing environmental inﬂuences, such\
    \ as changing color representation, motion\nblur, and camera distortion. The YoloV7\
    \ network achieved a mean average precision (mAP)\nof 0.891 after 300 epochs of\
    \ training. The dataset was divided into 80%, 10%, and 10% for\ntraining, validation,\
    \ and testing subsets, respectively.\nAutonomous robot procedure\nThe navigation\
    \ controller: Given a set of trajectories based on RTK-GNSS, the perfor-\nmance\
    \ of the guidance controller was evaluated by measuring lateral and angular error\n\
    through the incorporation of colored tapes on the ground and using the onboard\
    \ RGB\ncamera and ToF to extract the tape positions to compute the errors concerning\
    \ the robot’s\npace.\nSmart Navigation Manager procedure:\n•\nSmart operation\
    \ manager: The processing time, latency, success rate, response time,\nand response\
    \ status based on requests of the mission planner, IoT sensors, and cloud\ncomputing\
    \ services were evaluated using ROS functionalities that provide statistics\n\
    related to the following:\n#\nThe period of messages by all publishers.\n#\nThe\
    \ age of messages.\n#\nThe number of dropped messages.\n#\nTrafﬁc volume to be\
    \ measured in real-time.\n•\nCentral manager: The evaluation is similar to that\
    \ used for the navigation controller.\n•\nObstacle detection system: YOLOv4 and\
    \ a model already developed based on the\nCOCO database were introduced to detect\
    \ common obstacles in agricultural environ-\nments and were also used for evaluation.\
    \ YOLOv4 is a one-stage object detection\nmodel, and COCO (common objects in context)\
    \ is a large-scale object detection, seg-\nmentation, and captioning dataset.\n\
    Agriculture 2023, 13, 1005\n18 of 22\n4. System Assessment and Discussion\nThe\
    \ mission described in the previous section produced crop images, sensor data,\
    \ and\ntrafﬁc information with the following characteristics:\n•\nCrop images:\
    \ During the robot’s motion, images are acquired at a rate of 4 frames/s\nto guide\
    \ the robot. The RGB images are 2048 × 1536 pixels with a weight of 2.2 MB\n(see\
    \ Figures 8 and 9), and the ToF images feature 352 × 264 points (range of 300–5000\n\
    mm) (see Figure 10). The images are sent to the guiding and obstacle detection\
    \ system\nthrough the Ethernet using ROS (perception–ROS bridge in the perception\
    \ system\nand ROS manager in the central manager). A subset of these images is\
    \ stored in the\ncloud for further analysis. Using a FIWARE–ROS bridge with the\
    \ NGSI application\nprogramming interface, the system sends up to 4 frames/s.\n\
    •\nSensor data: IoT devices send the acquired data using 2.4 GHz WiFi with the\
    \ MQTT\nprotocol and JSON format.\n•\nTrafﬁc information: The ROS functionalities\
    \ mentioned above revealed that during\na ﬁeld experiment (10 min duration), the\
    \ total number of delivered messages was\n2,395,692, with a rate of only 0.63%\
    \ dropped messages (messages that were dropped\ndue to not having been processed\
    \ before their respective timeout), with average trafﬁc\nof 10 MB/s and maximum\
    \ trafﬁc of 160 MB at any instant of time. No critical messages\n(command messages)\
    \ were lost, demonstrating robustness within the smart navigation\nmanager. Regarding\
    \ cloud trafﬁc, during a period of time of approximately 3 h, the\nmessages sent\
    \ to the cloud were monitored, where the number of messages received by\nthe cloud\
    \ was measured; the delay time of the transmission of the messages between\nthe\
    \ robot (edge) and the OCB, and between the robot and the KAFKA bus (see Figure\
    \ 3),\nwere also measured. During this interval of time, around 4 missions were\
    \ executed,\nand a total of 14,368 messages were sent to the cloud, mainly the\
    \ robot status and the\nperception system data. An average delay of about 250\
    \ ms was calculated between\nthe moment the message is sent from the robot and\
    \ the moment it is received in the\nOCB (see Figure 11a). Moreover, the KAFKA\
    \ overhead, i.e., the time it takes for a\nmessage received by the OCB to be forwarded\
    \ to the KAFKA bus and eventually\nprocessed by a KAFKA consumer, was approximately\
    \ 1.24 ms, demonstrating that the\ninternal communications within the server and\
    \ hosted cloud services are robust (see\nFigure 11b).\nAgriculture 2023, 13, x\
    \ FOR PEER REVIEW \n19 of 23 \n \nKAFKA bus (see Figure 3), were also measured.\
    \ During this interval of time, around \n4 missions were executed, and a total\
    \ of 14,368 messages were sent to the cloud, \nmainly the robot status and the\
    \ perception system data. An average delay of about \n250 ms was calculated between\
    \ the moment the message is sent from the robot and \nthe moment it is received\
    \ in the OCB (see Figure 11a). Moreover, the KAFKA over-\nhead, i.e., the time\
    \ it takes for a message received by the OCB to be forwarded to the \nKAFKA bus\
    \ and eventually processed by a KAFKA consumer, was approximately \n1.24 ms, demonstrating\
    \ that the internal communications within the server and \nhosted cloud services\
    \ are robust (see Figure 11b).  \n \nFigure 8. Example of a wheat image acquired\
    \ with the guiding vision system and uploaded to the \ncloud. \nFigure 8. Example\
    \ of a wheat image acquired with the guiding vision system and uploaded to the\n\
    cloud.\nAgriculture 2023, 13, 1005\n19 of 22\n \n \nFigure 8. Example of a wheat\
    \ image acquired with the guiding vision system and uploaded to the \ncloud. \n\
    \ \nFigure 9. Example of a maize image acquired with the guiding vision system\
    \ and uploaded to the \ncloud. \nThe system has been tested in a ﬁeld with two\
    \ diﬀerent crops. Data related to cloud \ncommunication and robot guidance algorithms\
    \ have been collected. The communication \nFigure 9. Example of a maize image\
    \ acquired with the guiding vision system and uploaded to the\ncloud.\nThe system\
    \ has been tested in a ﬁeld with two different crops. Data related to cloud\n\
    communication and robot guidance algorithms have been collected. The communication\n\
    performance is similar to that obtained using conventional mechanisms, so we beneﬁt\
    \ from\nusing ROS and FIWARE without compromising performance.\nAgriculture 2023,\
    \ 13, x FOR PEER REVIEW \n20 of 23 \n \nperformance is similar to that obtained\
    \ using conventional mechanisms, so we beneﬁt \nfrom using ROS and FIWARE without\
    \ compromising performance. \n \nFigure 10. Example of a ToF intensity image acquired\
    \ with the guidance system and uploaded to \nthe cloud. \nFigure 10. Example of\
    \ a ToF intensity image acquired with the guidance system and uploaded to the\n\
    cloud.\nAgriculture 2023, 13, 1005\n20 of 22\n \n \nFigure 10. Example of a ToF\
    \ intensity image acquired with the guidance system and uploaded to \nthe cloud.\
    \ \n \nFigure 11. Example of a ToF intensity image acquired with the guidance\
    \ system and uploaded to \nthe cloud. (a) Message delay and (b) Kafka overhead.\
    \ \nFigure 11. Example of a ToF intensity image acquired with the guidance system\
    \ and uploaded to the\ncloud. (a) Message delay and (b) Kafka overhead.\n5. Conclusions\n\
    An architecture is presented to conﬁgure autonomous robots for agriculture with\n\
    access to cloud technologies. This structure takes advantage of new concepts and\
    \ technolo-\ngies, such as IoT and cloud computing, allowing big data, edge computing,\
    \ and digital\ntwins to be incorporated into modern agricultural robots.\nThe\
    \ architecture is based on ROS, the most universally accepted collection of software\n\
    libraries and tools for building robotic applications, and FIWARE, an open architecture\n\
    that enables the creation of new applications and services on the Internet. ROS\
    \ and FI-\nWARE provide attractive advantages for developers and farmers. ROS\
    \ and FIWARE offer\npowerful tools for developers to build control architectures\
    \ for complex robots with cloud\ncomputing/IoT features, making development easier\
    \ and leveraging open-source frame-\nworks. ROS and FIWARE, as in the proposed\
    \ integration, provide reusability, scalability,\nand maintenance using the appropriate\
    \ hardware resources. In addition, integrating the\nrobot controller into the\
    \ Internet allows the exploitation of autonomous robot services for\nagriculture\
    \ through the Internet.\nOn the other hand, the use of this type of architecture\
    \ reveals to farmers the advantages\nof communicating autonomous robots with the\
    \ cloud, providing them with leading beneﬁts\nto storing data safely and efﬁciently,\
    \ eliminating physical storage, and, thus, reducing the\nrisk of data loss. Data\
    \ stored in the cloud makes it easy to access data from anywhere and\nshare it\
    \ with other farmers or platforms. In addition, the services offered in the cloud\
    \ are\nvery ﬂexible to contract the actual storage needed at all times, optimizing\
    \ the farmer’s\nresources. Finally, farmers can use the analysis tools available\
    \ in the cloud to make their\nAgriculture 2023, 13, 1005\n21 of 22\nown decisions.\
    \ In any case, working in the cloud requires an initial investment, which is\n\
    usually recovered quickly.\nThe different components of the robot, particularized\
    \ for a laser-based weeding robot,\nare described, and the general architecture\
    \ is presented, indicating the speciﬁc interfaces.\nBased on these components,\
    \ the article presents the action sequence of the robot and the\noperating procedure\
    \ to illustrate how farmers can use the system and what beneﬁts they\ncan obtain.\n\
    Several experiments with two crops were conducted to evaluate the proposed in-\n\
    tegration based on the data communication characteristics, demonstrating the system’s\n\
    capabilities. The crop row detection system works correctly for both crops, tracking\
    \ the\nrows with an accuracy of ±0.02 m. The evaluation concluded that the system\
    \ could send\nimage frames to the cloud at 4 frames/s; messages between subsystems\
    \ and modules can\nbe passed with a 0.63% rejection rate. Regarding the trafﬁc\
    \ of the information exchanged,\nan average delay of 250 ms was detected in the\
    \ messages between the robot and the OCB.\nIn contrast, the OCB and the KAFKA\
    \ bus measured an average message of 1.24 ms. This\nindicates the robustness of\
    \ internal communications within the server and hosted cloud\nservices. This performance\
    \ is in the range obtained when a system communicates with the\ncloud using conventional\
    \ methods, so ROS and FIWARE facilitate communication with the\ncloud without\
    \ compromising performance.\nFuture work will focus on extending cloud computing\
    \ architecture to integrate digital\ntwins, orchestrate big data ensembles, and\
    \ facilitate the work of robots with edge computing\nperformance.\nAuthor Contributions:\
    \ Conceptualization, L.E., R.F., P.G.-d.-S., M.F., M.G., G.V., H.S., M.H. and\n\
    M.W.; methodology, L.E. and R.F.; software, L.E., M.F., H.S. and M.W.; validation,\
    \ L.E., M.F., G.V. and\nH.S.; investigation, L.E., R.F., P.G.-d.-S., M.F., M.G.,\
    \ G.V., H.S., M.H. and M.W.; writing—original draft\npreparation, P.G.-d.-S.;\
    \ writing—review and editing, L.E., P.G.-d.-S. and R.F.; supervision, L.E. and\n\
    P.G.-d.-S.; funding acquisition, P.G.-d.-S., G.V., M.G. and M.W. All authors have\
    \ read and agreed to\nthe published version of the manuscript.\nFunding: This\
    \ article is part of a project that has received funding from the European Union’s\n\
    Horizon 2020 research and innovation program under grant agreement No 101000256.\n\
    Institutional Review Board Statement: The study was conducted in accordance with\
    \ the Declaration\nof Helsinki, and approved by the Ethics Committee of CSIC.\n\
    Data Availability Statement: Herrera-Diaz, J.; Emmi, L.A.; Gonzalez de Santos,\
    \ P. Maize Dataset.\n2022. Available online: http://doi.org/10.20350/digitalCSIC/14566\
    \ (accessed on 1 April 2023).\nHerrera-Diaz, J.; Emmi, L.; Gonzalez de Santos,\
    \ P. Wheat Dataset. 2022. Available online: http:\n//doi.org/10.20350/digitalCSIC/14567\
    \ (accessed on 1 April 2023).\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nGhose, B. Food security and food self-sufﬁciency\
    \ in China: From past to 2050. Food Energy Secur. 2014, 3, 86–95. [CrossRef]\n\
    2.\nZhang, N.; Wang, M.; Wang, N. Precision agriculture—A worldwide overview.\
    \ Comput. Electron. Agric. 2002, 36, 113–132.\n[CrossRef]\n3.\nStentz, A.; Dima,\
    \ C.; Wellington, C.; Herman, H.; Stager, D. A System for Semi-Autonomous Tractor\
    \ Operations. Auton. Robot.\n2002, 13, 87–104. [CrossRef]\n4.\nBergerman, M.;\
    \ Maeta, S.M.; Zhang, J.; Freitas, G.M.; Hamner, B.; Singh, S.; Kantor, G. Robot\
    \ Farmers: Autonomous Orchard\nVehicles Help Tree Fruit Production. IEEE Robot.\
    \ Autom. Mag. 2015, 22, 54–63. [CrossRef]\n5.\nGonzalez-De-Santos, P.; Ribeiro,\
    \ A.; Fernandez-Quintanilla, C.; Lopez-Granados, F.; Brandstoetter, M.; Tomic,\
    \ S.; Pedrazzi, S.;\nPeruzzi, A.; Pajares, G.; Kaplanis, G.; et al. Fleets of\
    \ robots for environmentally-safe pest control in agriculture. Precis. Agric.\
    \ 2017,\n18, 574–614. [CrossRef]\n6.\nUnderwood, J.P.; Calleija, M.; Taylor, Z.;\
    \ Hung, C.; Nieto JFitch, R.; Sukkarieh, S. Real-time target detection and steerable\
    \ spray for\nvegetable crops. In Proceedings of the International Conference on\
    \ Robotics and Automation: Robotics in Agriculture Workshop,\nSeattle, WA, USA,\
    \ 9–11 May 2015.\n7.\nKongskilde. New Automated Agricultural Platform—Kongskilde\
    \ Vibro Crop Robotti. 2017. Available online: http://conpleks.\ncom/robotech/new-automated\
    \ (accessed on 14 December 2022).\nAgriculture 2023, 13, 1005\n22 of 22\n8.\n\
    Emmi, L.; Herrera-Diaz, J.; Gonzalez-De-Santos, P. Toward Autonomous Mobile Robot\
    \ Navigation in Early-Stage Crop Growth.\nIn Proceedings of the 19th International\
    \ Conference on Informatics in Control, Automation and Robotics (ICINCO 2022),\
    \ Lisbon,\nPortugal, 14–16 July 2022; pp. 411–418. [CrossRef]\n9.\nBannerjee,\
    \ G.; Sarkar, U.; Das, S.; Ghosh, I. Artiﬁcial Intelligence in Agriculture: A\
    \ Literature Survey. Int. J. Sci. Res. Comput. Sci.\nAppl. Manag. Stud. 2018,\
    \ 7, 3.\n10.\nOsinga, S.A.; Paudel, D.; Mouzakitis, S.A.; Athanasiadis, I.N. Big\
    \ data in agriculture: Between opportunity and solution. Agric.\nSyst. 2021, 195,\
    \ 103298. [CrossRef]\n11.\nYang, D.; Li, D.; Sun, H. 2D Dubins Path in Environments\
    \ with Obstacle. Math. Probl. Eng. 2013, 2013, 291372. [CrossRef]\n12.\nEmmi,\
    \ L.; Parra, R.; González-de-Santos, P. Digital representation of smart agricultural\
    \ environments for robot navigation. In\nProceedings of the 10th International\
    \ Conference on ICT in Agriculture, Food & Environment (HAICTA 2022), Athens,\
    \ Greece,\n22–25 September 2022; pp. 1–6.\n13.\nOrion Context Broker. Telefonica.\
    \ Available online: https://github.com/telefonicaid/ﬁware-orion (accessed on 22\
    \ February\n2023).\n14.\nFrancia, M.; Gallinucci, E.; Golfarelli, M.; Leoni, A.G.;\
    \ Rizzi, S.; Santolini, N. Making data platforms smarter with MOSES. Futur.\n\
    Gener. Comput. Syst. 2021, 125, 299–313. [CrossRef]\n15.\nROS—The Robot Operating\
    \ System. 2023. Available online: https://www.ros.org/ (accessed on 24 April 2020).\n\
    16.\nROSLink. 2023. Available online: https://github.com/aniskoubaa/roslink (accessed\
    \ on 5 January 2023).\n17.\nKoubaa, A.; Alajlan, M.; Qureshi, B. ROSLink: Bridging\
    \ ROS with the Internet-of-Things for Cloud Robotics. In Robot Operating\nSystem\
    \ (ROS); Koubaa, A., Ed.; Studies in Computational Intelligence; Springer: Cham,\
    \ Switzerland, 2017; Volume 707. [CrossRef]\n18.\nFiware Community Fiware: The\
    \ Open Source Platform for Our Smart Digital Future. Available online: https://www.ﬁware.org/\n\
    (accessed on 5 January 2023).\n19.\nLópez-Riquelme, J.; Pavón-Pulido, N.; Navarro-Hellín,\
    \ H.; Soto-Valles, F.; Torres-Sánchez, R. A software architecture based on\nFIWARE\
    \ cloud for Precision Agriculture. Agric. Water Manag. 2017, 183, 123–135. [CrossRef]\n\
    20.\nBochkovskiy, A.; Wang, C.Y.; Liao, H.Y.M. YOLOv4: Optimal speed and accuracy\
    \ of object detection. arXiv 2020, arXiv:2004.10934.\n21.\nHerrera-Diaz, J.; Emmi,\
    \ L.A.; Gonzalez de Santos, P. Maize Dataset. 2022. Available online: https://digital.csic.es/handle/10261/\n\
    264581 (accessed on 1 April 2023).\n22.\nHoward, A.G.; Zhu, M.; Chen, B.; Kalenichenko,\
    \ D.; Wang, W.; Weyand, T.; Andreetto, M.; Adam, H. MobileNets: Efﬁcient\nConvolutional\
    \ Neural Networks for Mobile Vision Applications. arXiv 2017, arXiv:1704.04861.\n\
    23.\nHerrera-Diaz, J.; Emmi, L.; Gonzalez de Santos, P. Wheat Dataset. 2022. Available\
    \ online: https://digital.csic.es/handle/10261/\n264622 (accessed on 1 April 2023).\n\
    24.\nWojke, N.; Bewley, A.; Paulus, D. Simple Online and Realtime Tracking with\
    \ a Deep Association Metric. In Proceedings of the\nIEEE International Conference\
    \ on Image Processing (ICIP), Beijing, China, 17–20 September 2017.\nDisclaimer/Publisher’s\
    \ Note: The statements, opinions and data contained in all publications are solely\
    \ those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or\
    \ the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury\
    \ to\npeople or property resulting from any ideas, methods, instructions or products\
    \ referred to in the content.\n"
  inline_citation: '>'
  journal: Agriculture (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2077-0472/13/5/1005/pdf?version=1683018153
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Exploiting the Internet Resources for Autonomous Robots in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/e25070987
  analysis: '>'
  authors:
  - Nikita Stasenko
  - Islomjon Shukhratov
  - Maxim Savinov
  - Dmitrii Shadrin
  - Andrey Somov
  citation_count: 1
  full_citation: '>'
  full_text: ">\nCitation: Stasenko, N.; Shukhratov, I.;\nSavinov, M.; Shadrin, D.;\
    \ Somov, A.\nDeep Learning in Precision\nAgriculture: Artiﬁcially Generated\n\
    VNIR Images Segmentation for Early\nPostharvest Decay Prediction in\nApples. Entropy\
    \ 2023, 25, 987.\nhttps://doi.org/10.3390/e25070987\nAcademic Editors: Oleg Sergiyenko,\n\
    Wendy Flores-Fuentes, Julio Cesar\nRodriguez-Quinonez and Jesús Elías\nMiranda-Vega\n\
    Received: 5 May 2023\nRevised: 19 June 2023\nAccepted: 22 June 2023\nPublished:\
    \ 28 June 2023\nCopyright:\n© 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nentropy\nArticle\nDeep Learning in Precision Agriculture: Artiﬁcially\
    \ Generated\nVNIR Images Segmentation for Early Postharvest Decay\nPrediction\
    \ in Apples\nNikita Stasenko 1\n, Islomjon Shukhratov 1\n, Maxim Savinov 2\n,\
    \ Dmitrii Shadrin 1,3 and Andrey Somov 1,*\n1\nSkolkovo Institute of Science and\
    \ Technology, 121205 Moscow, Russia; d.shadrin@skoltech.ru (D.S.)\n2\nSaint-Petersburg\
    \ State University of Aerospace Instrumentation (SUAI), 190000 Saint-Petersburg,\
    \ Russia\n3\nDepartment of Information Technology and Data Science, Irkutsk National\
    \ Research Technical University,\n664074 Irkutsk, Russia\n*\nCorrespondence: a.somov@skoltech.ru\n\
    Abstract: Food quality control is an important task in the agricultural domain\
    \ at the postharvest stage\nfor avoiding food losses. The latest achievements\
    \ in image processing with deep learning (DL) and\ncomputer vision (CV) approaches\
    \ provide a number of effective tools based on the image colorization\nand image-to-image\
    \ translation for plant quality control at the postharvest stage. In this article,\n\
    we propose the approach based on Generative Adversarial Network (GAN) and Convolutional\n\
    Neural Network (CNN) techniques to use synthesized and segmented VNIR imaging\
    \ data for early\npostharvest decay and fungal zone predictions as well as the\
    \ quality assessment of stored apples.\nThe Pix2PixHD model achieved higher results\
    \ in terms of VNIR images translation from RGB\n(SSIM = 0.972). Mask R-CNN model\
    \ was selected as a CNN technique for VNIR images segmentation\nand achieved 58.861\
    \ for postharvest decay zones, 40.968 for fungal zones and 94.800 for both the\n\
    decayed and fungal zones detection and prediction in stored apples in terms of\
    \ F1-score metric. In\norder to verify the effectiveness of this approach, a unique\
    \ paired dataset containing 1305 RGB and\nVNIR images of apples of four varieties\
    \ was obtained. It is further utilized for a GAN model selection.\nAdditionally,\
    \ we acquired 1029 VNIR images of apples for training and testing a CNN model.\
    \ We\nconducted validation on an embedded system equipped with a graphical processing\
    \ unit. Using\nPix2PixHD, 100 VNIR images from RGB images were generated at a\
    \ rate of 17 frames per second\n(FPS). Subsequently, these images were segmented\
    \ using Mask R-CNN at a rate of 0.42 FPS. The\nachieved results are promising\
    \ for enhancing the food study and control during the postharvest stage.\nKeywords:\
    \ GAN; CNN; precision agriculture; postharvest decay; fungi; image processing\n\
    1. Introduction\nAccording to the data provided by United Nations, the human population\
    \ has grown\nto 8 billion people [1], and it is expected to increase up to 9.8\
    \ billion by 2050 [2]. The\ngrowing population will need more sustainable and\
    \ affordable food sources. It increases\nthe importance of agriculture in the\
    \ light of sustainable development. In terms of food\nproducing and quality control,\
    \ agricultural challenges can be divided into preharvesting,\nharvesting and postharvesting\
    \ stages [3]. Each stage includes various factors that should be\ntaken into account\
    \ in order to minimize food losses. During the postharvest stage, farmers\nprimarily\
    \ concentrate on factors that impact the shelf-life of harvested products during\n\
    storage and transportation. These factors include temperature [4], humidity [5],\
    \ as well\nas the use of gases and chemicals in food containers [6,7]. Each crop\
    \ has its own number\nof factors affecting the shelf-life during the postharvest\
    \ stage, and these factors should\nbe also taken into account [8]. Disparagement\
    \ of one of these factors or violation during\nthe storage or transportation may\
    \ result in postharvest losses of food products. Examples\nof postharvest losses\
    \ in stored fruits and vegetables include decayed and spoiled areas,\nEntropy\
    \ 2023, 25, 987. https://doi.org/10.3390/e25070987\nhttps://www.mdpi.com/journal/entropy\n\
    Entropy 2023, 25, 987\n2 of 30\noften attributed to mishandling, hygiene issues,\
    \ inadequate humidity control, improper\ntemperature management, and mechanical\
    \ damages [9]. These factors contribute to the\ndeterioration and loss of quality\
    \ of stored subjects.\nApple is one of the most popular harvested and cultivated\
    \ crops. Its global production\nachieved 93 millions tonnes in 2021 [10]. It is\
    \ one of the major reasons to monitor apple\nfruits quality during all the above-mentioned\
    \ stages to prevent postharvest losses and to\navoid potential economic losses.\
    \ However, there are special factors affecting apple quality\nduring the postharvest\
    \ stage, e.g., water as loss in apple fruits [11], residual pesticides [12],\n\
    or concentration of carbon dioxide, ethylene, ethanol or ammonia surrounding apples\
    \ due\nto insufﬁcient ventilation in the storage facility [13]. The most common\
    \ non-destructive\nmethods for preventing postharvest losses include the control\
    \ of objects using RGB video\ncameras and sensors [14], near infrared (NIR) data\
    \ [15], gas sensing spectroscopy [13],\nﬂuorescence spectroscopy [16], magnetic\
    \ resonance imaging (MRI) [17], and even electronic\nnose [18]. Nevertheless,\
    \ postharvest losses are still estimated in the range of 40–50% [9]. It\nshould\
    \ be noted that the control of apple fruits at the postharvest stage is quite\
    \ comprehen-\nsive, making it difﬁcult to monitor each fruit at each step, while\
    \ any damage may lead to a\nfungi infection [19] in the stored fruits and also\
    \ to the formation (and even a rapid growth)\nof rotten areas which are also known\
    \ as decayed areas [20]. Moreover, these areas are not\nwell seen visually at\
    \ early stages, and the decay growth process can be quite dynamic [21].\nArtiﬁcial\
    \ intelligence (AI) and its domains, including machine learning (ML) and deep\n\
    learning (DL), in conjunction with the latest achievements in computer vision\
    \ (CV), remote\nsensing, wireless sensing technologies, and Internet of Things\
    \ (IoT), have provided the\nadded value in a number of application including the\
    \ space domain [22], medicine [23],\npower engineering [24], agriculture [25]\
    \ and food supply [26]. For example, farmers rely\non CV for crop quality management,\
    \ e.g., plant growth monitoring [27], fruit detection [28],\ndisease detection\
    \ [29] and weed detection [30]. It is necessary for improving the food quality\n\
    of each plant at preharvest, harvest, and postharvest stages, respectively. Also,\
    \ there is a\nset of CV-based approaches for postharvest losses estimation and\
    \ the evaluation in stored\ncrops [31–33]. However, some postharvest losses, e.g.,\
    \ fungi or postharvest decay zones,\nshould be detected immediately, since the\
    \ visible decayed or fungi zones (acquired visually\nor with RGB cameras and sensors)\
    \ in stored plants may indicate their serious spoilage if\nwe use other types\
    \ of imaging data, e.g., NIR or thermal imaging, to monitor their quality.\nThis\
    \ monitoring process requires a special device and equipment, e.g., multispectral\
    \ or\nhyperspectral cameras, which are expensive and often not easy to use, given\
    \ fast detection\nof defects is still extremely challenging.\nIn this article,\
    \ we present an approach based on the application of generative ad-\nversarial\
    \ network (GAN) and convolutional neural network (CNN) for early detection\nand\
    \ segmentation of decayed and fungi areas in stored apples at the postharvest\
    \ stage\nusing visible near-infrared (vis-NIR, or just VNIR) imaging data. We\
    \ show how artiﬁcially\ngenerated VNIR imaging data can be used for early postharvest\
    \ decay detection in stored\napples and examine whether GAN- and CNN-based approaches\
    \ can achieve promising\nresults for image segmentation tasks. The idea of the\
    \ proposed approach can be divided\ninto two parts:\n•\nGeneration of VNIR imaging\
    \ data containing the stored apples with postharvest decay\nand fungi zones using\
    \ the GAN technique.\n•\nSegmentation of generated VNIR images using the CNN technique\
    \ in order to detect\nthe decayed and fungi zones in the stored apples.\nIn this\
    \ research, we study the original and generated VNIR images containing apples\n\
    of four varieties with several treatments in order to simulate various occasions\
    \ with ap-\nples during the storage. The aim is to present an approach based on\
    \ the DL techniques\ncombining the GAN and CNN models, for instance, with segmentation\
    \ of postharvest\ndecay zones and fungi areas. The GAN model will provide the\
    \ procedure of NIR images\nsynthesis from the input RGB data, while the CNN model\
    \ is supposed to be used for the\ninstance segmentation of generated images. This\
    \ is important for the proposed approach,\nEntropy 2023, 25, 987\n3 of 30\nas\
    \ we aim to train and validate our models to detect the postharvest decay zones\
    \ and\nfungi areas separately from each other. For realizing this idea into practice,\
    \ we propose the\nfollowing stages.\nFirst, we need to select a GAN based model\
    \ for the NIR images generation from the\ninput RGB data. There are many available\
    \ networks, but for the image-to-image translation\ntasks the following architectures\
    \ Pix2Pix [34], CycleGAN [35], and Pix2PixHD [36] are\nmostly applied in agricultural\
    \ domain [37–43]. We compare Pix2Pix, CycleGAN, and\nPix2PixHD models using the\
    \ dataset containing the paired RGB and NIR images. We are\ngoing to work with\
    \ the images acquired in VNIR range since it includes the full visible\nspectrum\
    \ with an abutting portion of the infrared spectrum [44]. The paired images\n\
    collected in the visible (380–700 nm) and VNIR (400–1100 nm) ranges are required\
    \ to make\nsure that the decayed and fungal traits in stored apples are the same\
    \ for these two ranges.\nSection 3.1.1, Section 3.1.2, and Section 3.1.3 provide\
    \ detailed information about the Pix2Pix,\nCycleGAN, and Pix2PixHD models, respectively.\n\
    Second, it is necessary to choose the CNN model for the decayed and fungal areas\n\
    segmentation in the synthesized VNIR images. In this work, we implement a Mask\
    \ R-CNN\nmodel due to the Feature Pyramid Network (FPN) and ResNet101 backbone,\
    \ which allow\nfor generating the bounding boxes (object detection) and segmentation\
    \ masks (instance\nsegmentation). In [45], we have compared the Mask R-CNN to\
    \ such applied CNN-based\nmodels as U-Net [46] and Deeplab [47] for early postharvest\
    \ decay detection, and Mask R-\nCNN achieved the highest performance in terms\
    \ of average precision, namely 67.1% against\n59.7% and 56.5%, respectively. Moreover,\
    \ the Mask R-CNN model generates the bounding\nboxes and segmentation masks of\
    \ the postharvest decay and fungal zones separately from\neach other. This is\
    \ a so-called ‘a tried and tested’ method, and that is why we use Mask\nR-CNN\
    \ as a CNN-based segmentation model. We discuss the Mask R-CNN model in more\n\
    detail in Section 3.1.4.\nFinally, our plan is to implement the proposed approach\
    \ and execute it on a Single\nBoard Computer (SBC) with the AI capabilities. This\
    \ implementation will serve as an\nevaluation platform for generating segmented\
    \ VNIR images that highlight any postharvest\ndecay and fungal zones on apples.\
    \ These zones may be imperceptible to the human eye,\nbut can be detected and\
    \ selected through our system. We use NVIDIA Jetson Nano as\nan embedded system\
    \ with AI capabilities for evaluation. It is a compact and powerful\nSBC supplied\
    \ with the accelerated libraries for computer vision and deep learning applica-\n\
    tions, and is widely used for different real-time problems in agriculture including\
    \ weed\ncontrol [48], soil mapping in greenhouse [49], and harvest product detection\
    \ [50–54]. That\nis why the presented research is supposed to be an alternative\
    \ solution for the high-cost\nNIR hyperspectral devices used for the early postharvest\
    \ decay detection and prediction\nfor stored food. Figure 1 illustrates the proposed\
    \ approach.\nThe contribution of this work is as follows:\n•\nTwo experimental\
    \ testbeds for paired RGB and VNIR imaging data collection under\nvarious environmental\
    \ (temperature and humidity) conditions.\n•\nApplication of CNN models, for instance,\
    \ on the segmentation of decayed and fungi\nareas in apples at the postharvest\
    \ stage.\n•\nSeparate segmentation of fungi zones and postharvest decay areas\
    \ in stored apples\nusing the CNN model.\n•\nApplication of the trained CNN-based\
    \ model for the instance segmentation of posthar-\nvest decay zones and fungi\
    \ areas in VNIR images generated by the GAN-based model.\n•\nImplementation of\
    \ the proposed approach based on the GAN and CNN techniques\nfor postharvest decay\
    \ detection, segmentation and prediction using generated VNIR\nimaging data on\
    \ a low-cost embedded system with the AI capabilities.\nEntropy 2023, 25, 987\n\
    4 of 30\nFigure 1. Diagram summarizing the proposed approach for the application\
    \ of segmented VNIR\nimagery data via deep learning for early postharvest decay\
    \ prediction in apples.\nThis article is organized as follows: Section 2 provides\
    \ an introduction to relevant\nresearch works aimed at early postharvest decay\
    \ detection and prediction in apples using\nRGB and VNIR imaging data with the\
    \ CV and ML methods. Section 3 presents the methods\nused in this work. Section\
    \ 3.3 demonstrates the experimental testbeds used for RGB and\nVNIR imaging data\
    \ collection and describes the procedure of data annotation. Section 4\nshows\
    \ the results of the comparison of the GAN techniques applied to VNIR images\n\
    generation from the RGB ones (see Section 4.1). It also presents the application\
    \ of the\nCNN technique, for instance, on the segmentation on the generated VNIR\
    \ images (see\nSection 4.2), and describes the embedded system running the proposed\
    \ GAN and CNN (see\nSection 4.3). Conclusions and discussion of the future work\
    \ are summarized in Section 5.\n2. Related Works\n2.1. CV Approaches Based on\
    \ CNN Models Using RGB Imaging Data\nCV techniques with the implementation of\
    \ ML and DL methods are becoming one of\nthe most useful tools for fruit quality\
    \ estimation and evaluation at the postharvest stage.\nThe majority of approaches\
    \ are based on the collection and analysis of visible mor-\nphological traits,\
    \ such as changes in fruit shape, size, or color during the storage, from\nstored\
    \ fruits with CNN models using RGB images as the most acceptable and user-friendly\n\
    type of data. RGB imagery is closely similar to human vision because red, green\
    \ and\nblue are the primary colors in these color models, which makes the process\
    \ of visible non-\ndestructive quality monitoring and defect detection of stored\
    \ food production easy and\nunderstandable [55]. The majority of cameras and devices\
    \ for RGB imaging data collection\ncontain a patterned Bayer ﬁlter mosaic consisting\
    \ of squares of four pixels with one red,\none blue and two green ﬁlters [56].\
    \ Usually, the Bayer ﬁlter is located on the camera chip.\nGenerally, a CNN model\
    \ contains convolutional and pooling layers (added one by\none), ﬂatten, fully\
    \ connected layer and softmax classiﬁer. The convolutional and pooling\nlayers\
    \ are used in the features extraction part, while the classiﬁcation part involves\
    \ the\nﬂatten, fully connected layers and softmax classiﬁer. When the image reaches\
    \ the input\nlayer, a ﬁlter in the convolution layer allows it for the selection\
    \ of feature neurons. An\nactivation function (Sigmoid, Rectiﬁed Linear Unit (ReLU),\
    \ or Softplus) is added to obtain\nnonlinear results by passing feature neurons\
    \ through it, and the resulting feature map\nsize is reduced by the pooling layer\
    \ functions. The ﬂatten layer is the ﬁrst input layer for\nthe classiﬁer model\
    \ as it keeps the feature map from the convolution layers. The fully\nconnected\
    \ layer transforms the obtained feature neurons into a matrix, which performs\
    \ the\nclassiﬁcation function with a classiﬁcation method.\nIn this way, the CNN\
    \ structure showed its efﬁciency in classiﬁcation, and then in\ndetection and\
    \ segmentation tasks using RGB imaging data. For example, the automated\nEntropy\
    \ 2023, 25, 987\n5 of 30\nbanana grading system was reported in [57] where a ﬁne-tuned\
    \ VGG-16 Deep CNN model\nwas applied for banana classiﬁcation using such traits\
    \ as skin quality, size, and maturity\nwith the acquired RGB imagery data. A similar\
    \ approach was proposed in [58] where the\nVGG-16 model was trained to predict\
    \ the date of the fruit ripening stage using RGB images\nwith an overall classiﬁcation\
    \ of 96.98%.\nIn [59], the authors developed an automated online carrot grading\
    \ system, where a\nlightweight carrot defect detection network (CDDNet) based\
    \ on ShufﬂeNet [60] and transfer\nlearning was implemented for carrot quality\
    \ inspection using RGB and grayscale images.\nThe CDDNet was compared to other\
    \ CNN models including AlexNet, ResNet50, MobileNet\nv2, and ShufﬂeNet, and it\
    \ demonstrated good performance in terms of detection accuracy\nand time consuming\
    \ for binary classiﬁcation of normal and defective carrots (99.82%),\nand for\
    \ classiﬁcation of normal, bad spots, abnormal, and ﬁbrous root carrots (93.01%).\n\
    However, the images of carrots contained the carrots of different size and appearance,\
    \ and\nthe idea of the presented approach was to detect the carrots with visible\
    \ defects without\ntaking into account the spoilage stage of the defective carrots.\
    \ Moreover, there was no\nmention of a possible situation when the carrots are\
    \ infected, but still there are no visible\ntraits of spoilage.\nIn [61], the\
    \ authors report on the implementation of the DeeplabV3+ model [62] with\na classical\
    \ image processing algorithm, e.g., threshold binary segmentation, morphological\n\
    processing and mask extraction for banana bunches segmentation during sterile\
    \ bud\nremoval (SBD) on the total of 1500 RGB images. Moreover, YOLOv5-Banana\
    \ model [63]\nfor the banana ﬁngers segmentation and centroid points extraction,\
    \ while edge detection\nand centroid extraction of banana ﬁngers included binarization,\
    \ morphological opening\noperation, canny edge detection, and extracting centroid\
    \ point set. DeeplabV3 was reported\nto achieve a detection accuracy rate of 86%,\
    \ mean intersection over union (MIoU) of 0.878\nduring the debudding period for\
    \ target segmentation, and the mean pixel precision of 0.936.\nYOLOv5-Banana achieved\
    \ 76% detection accuracy rates for the banana bunches during\nthe harvest period.\
    \ The authors also designed and presented the software to estimate the\nbanana\
    \ fruit weight during the harvest period.\nIn [64], several CNN-based models including\
    \ VGG-16, VGG-19, ResNet50, ResNet101,\nand ResNet152 were compared to each other\
    \ for such physiological disorders classiﬁcation\nin stored apples as bitter pit,\
    \ shriveling, and superﬁcial scald. The authors acquired a\ndataset containing\
    \ 1080 RGB images (dataset-1) of apples and 4320 augmented images\n(dataset-2)\
    \ with the aim to improve data representation during model training and to\nconsider\
    \ apple position under the monitoring camera and lighting conditions during the\n\
    storage. The CNN-based models were used and compared for feature extraction, while\
    \ such\nclassical ML methods as support vector machines (SVM), random forest (RF),\
    \ k-nearest\nneighbors algorithm (kNN), and XGBoost were used for the extracted\
    \ features classiﬁcation.\nThe highest average accuracy was reported for the VGG-19\
    \ model in conjunction with the\nSVM method in the dataset-1 and dataset-2 with\
    \ 96.11 and 96.09%.\n2.2. Machine Learning and Deep Learning Methods for NIR Data\
    \ Analysis\nNIR spectroscopy covers spectral regions from 780 to 2500 nm that\
    \ cannot be seen\nwith human eyes, but it allows for obtaining spectral information\
    \ from ten (generally,\nreferred to as multispectral data [65]) and to more than\
    \ a hundred wavebands (referred to\nas hyperspectral data [65]). Measurements\
    \ performed in the visible (380–700 nm), visible\nnear-infrared (vis-NIR, or just\
    \ VNIR, 400–1100 nm), and NIR (780–2500 nm) ranges provide\nthe user with more\
    \ detailed information on the chemical composition of scanned samples.\nIn our\
    \ case, by samples we mean stored plants, crops and fruits. The state-of-the-art\
    \ cameras\nand devices for the hyperspectral data acquisition provide not only\
    \ spectral information\nabout the scanned samples, but also allow the users to\
    \ obtain the images of scanned zones\nin the range of device bands. Spectral information\
    \ on chemical composition from a wide\nrange of wavebands has simpliﬁed the procedure\
    \ of food quality monitoring and defect\ndetection at the postharvest stage. Moreover,\
    \ not only the decay zones may occur in stored\nEntropy 2023, 25, 987\n6 of 30\n\
    fruits, but also some fungi like Sclerotinia sclerotiorum [66], Penicillium expansum\
    \ [67], Botrytis\ncinerea [68], Botryosphaeria dothidea [69] and many others,\
    \ which should be immediately\ndetected at the early stage. Otherwise, the appearance\
    \ and growth of decayed and fungi\nzones may lead to the loss of all stored fruits.\
    \ It is vital to distinguish various types of\npostharvest losses, e.g., postharvest\
    \ decay, and diseases, e.g., various fungi varieties, since\neach type of loss\
    \ requires a special type of treatment or removal of spoiled samples from\nthe\
    \ storage. It should be noted here that the formation of fungal areas may not\
    \ always lead\nto the formation of decayed areas. That is why we should detect\
    \ and identify the fungi and\npostharvest decay zones separately from each other\
    \ [70–72].\nBoth classical ML methods and the DL techniques based on the CNN models\
    \ are\nwidely used for postharvest losses evaluation in stored plants using VNIR\
    \ and NIR imaging\nand spectral data.\nIn [73], the authors compared several ML\
    \ methods including linear discriminant\nanalysis (LDA), random forest (RF), support\
    \ vector machines (SVM), kNN, gradient tree\nboosting (GTB), and partial least\
    \ squares-discriminant analysis (PLS-DA) for early Codling\nMoth zones detection\
    \ in “Gala”, “Granny Smith”, and “Fuji” stored apples. The research\nwas carried\
    \ out at the pixel level using NIR hyperspectral reﬂectance imaging data in the\n\
    range of 900–1700 nm with an optimal selection of wavelengths. GTB was reported\
    \ to obtain\nbetter results at a pixel level classiﬁcation with 97.4% of total\
    \ accuracy for validation dataset.\nIn [74], the authors implemented the AlexNet\
    \ model for detecting pesticide residues in\npostharvest apples using hyperspectral\
    \ imaging data. There were 12,288 hyperspectral ac-\nquired images for the training\
    \ set and 6144 images for the test set in the 865.11–1711.71 nm\nrange (the camera\
    \ included 256 bands) and with 3.32 nm spectral resolution. Otsu segmen-\ntation\
    \ algorithm [75] was used for the apples and pesticide residue positioning (they\
    \ were\nthe regions of interests, or just ROIs), while deep AlexNet [76] provided\
    \ pesticide category\ndetection. AlexNet was reported to show better results in\
    \ terms of detection accuracy and\ntime consumption in comparison to the SVM and\
    \ kNN algorithms (99.09% and 0.0846 s\nagainst 74.34% and 11.2301 s, and 43.75%\
    \ and 0.7645 s, respectively).\nAs we can see, NIR hyperspectral and multispectral\
    \ imaging data ensures early disease\ndetection with more details than RGB imaging,\
    \ but also requires sophisticated equipment,\nwhich usually includes a camera\
    \ with wavebands, imaging spectrograph (or spectrometer),\nsample stage, illumination\
    \ lamps and lightning system, as well as supplementary software\nand devices for\
    \ processing and capturing NIR data and images [77–79]. However, this\nis the\
    \ reason why hyperspectral imaging devices are so expensive and may cost from\n\
    thousands to ten thousand USD [80]. These high prices reduce the availability\
    \ and usage of\nhyperspectral cameras for farmers and food selling companies to\
    \ perform food quality con-\ntrol at postharvest stages. This issue has raised\
    \ a demand for developing new approaches\nfor NIR imaging data generation without\
    \ using high cost hyperspectral systems.\n2.3. GAN-Based Models for RGB and NIR\
    \ Data Analysis\nGenerative Adversarial Networks (GANs) and, in particular, conditional\
    \ GAN\n(cGAN) [81] have demonstrated their effectiveness in a variety of tasks\
    \ in the agricul-\ntural domain including remote sensing [82], image augmentation\
    \ [83], animal farming [84],\nand plant phenotyping [85]. The general idea of\
    \ GAN is based on the usage of two neural\nnetwork models, where the ﬁrst network\
    \ is called generator (generative part, G) and its\ngoal is to create plausible\
    \ samples, while the second network is called discriminator (ad-\nversarial part,\
    \ D), and it learns to verify whether the created plausible sample is real or\n\
    fake. GANs are also applied for the so-called image-to-image translation tasks,\
    \ i.e., where\nthere is a need for high-quality image synthesis from one domain\
    \ to another. For example,\nGAN-based models were successfully applied for the\
    \ multi-channel attention selection in\nthe RGB imagery considering an external\
    \ semantic guidance in [86,87], MRI data estimation\nin [88], diffusion models\
    \ evaluation [89], and NIR imaging generation from the input RGB\nimages in [82,90,91].\n\
    Entropy 2023, 25, 987\n7 of 30\nTherefore, the approaches based on GAN models\
    \ allow synthesizing high-quality NIR\nimages from the input RGB images while\
    \ saving detailed spectral information. At the same\ntime, it is crucial not only\
    \ to transform the image together with all the relevant information,\nbut also\
    \ to segment various types of postharvest diseases and defects separately from\n\
    each other in stored food production in order to choose the speciﬁc processing\
    \ strategy\nfor defected or spoiled food samples. At present, most GAN models\
    \ provide only the\nimages transformation from one domain to another, but not\
    \ object detection or instance\nsegmentation operations in the synthesized images.\
    \ However, as shown in Section 2.1,\nCNN models demonstrate reasonably good results\
    \ for the object detection and instance\nsegmentation both for the RGB and the\
    \ NIR images.\n3. Materials and Methods\n3.1. DL Techniques\n3.1.1. Pix2Pix\n\
    The Pix2Pix model [34] is a type of cGAN that has been demonstrated on a range\
    \ of\nimage-to-image translation tasks, such as converting a satellite image to\
    \ corresponding\nmaps, or black and white photos to color images. In conditional\
    \ GANs, the generation of\nthe output image is conditional on the input image.\
    \ In the case of the Pix2Pix model, the\ngeneration process is conditional on\
    \ the source image. The discriminator covers both the\nobserved source image (domain\
    \ A) and the target image (domain B) and must determine\nwhether the target is\
    \ a plausible transformation of the source image. The generator is\ntrained via\
    \ the adversarial loss which encourages the generator to make plausible images\n\
    in the target domain. The generator is also updated via L1 loss measured between\
    \ the\ngenerated image and the expected output image. This additional loss encourages\
    \ the\ngenerator model to create the plausible translations of the source image.\
    \ Mathematically,\nthe whole process in Pix2Pix can be deﬁned as:\nLcGAN(G, D)\
    \ = Ex,y∼pdata(x,y)[logD(x, y)] + Ex,z∼pdata(x,z)[log(1 − D(x, G(x, z))]\n(1)\n\
    where G is the generator, D is the discriminator, x is the observed image, y is\
    \ the target\nimage, z is the random noise vector, and λ controls the relative\
    \ importance of the two objec-\ntives between domain A and domain B. The following\
    \ objective function is used to train the\nmodel:\nG = arg min\nG max\nD\nLcGAN(G,\
    \ D) + λLL1(G)\n(2)\nPix2Pix requires perfectly aligned paired images for the\
    \ training procedure. In this\nresearch, the CNN-based architecture is used both\
    \ as the generator and the discriminator.\nGenerally, the U-Net model [46] is\
    \ applied in Pix2Pix as a generator. U-Net trains to generate\nthe images from\
    \ the images in domain A similar to the images in domain B. The discriminator\n\
    is usually a PatchGAN (which is also known as Markovian discriminator [92]), and\
    \ it trains\nsimultaneously to distinguish the generated images from the real\
    \ images in domain B. The\nreconstruction loss measures the similarity between\
    \ the real images and the generated\nimages. Figure 2 shows the block diagram\
    \ of Pix2Pix.\nEntropy 2023, 25, 987\n8 of 30\nFigure 2. Pix2Pix block diagram.\n\
    3.1.2. CycleGAN\nThe goal of the CycleGAN model [35] is to learn the mapping G\
    \ : X → Y such that\nthe distribution of images from G(X) is indistinguishable\
    \ from the distribution Y using an\nunpaired set of image pairs. This mapping\
    \ is coupled with an inverse mapping F : Y → X\nand a cycle consistency loss introduced\
    \ to enforce F(G(X)) ≈ X and vice versa due to the\nreason that it is highly underconstrained.\
    \ For the mapping function G : X → Y and its\ndiscriminator DY\nLGAN(G, DY, X,\
    \ Y) = Ey[logDY(y)] + Ex[log(1 − DY(G(x)]\n(3)\nand the objective is as follows:\n\
    G, F = arg min\nG,F max\nDX,DY\nL(G, F, DX, DY)\n(4)\nCycleGAN learns a translation\
    \ mapping in the absence of aligned paired images. The\nimage generated from domain\
    \ A to domain B by the CNN-based generator (G1) is converted\nback to domain A\
    \ by another CNN-based generator (G2), and vice versa, in the attempt to\noptimize\
    \ the cycle-consistency loss in addition to the adversarial loss. The block diagram\n\
    of CycleGAN is shown in Figure 3.\nFigure 3. CycleGAN block diagram.\nEntropy\
    \ 2023, 25, 987\n9 of 30\n3.1.3. Pix2PixHD\nThe Pix2PixHD model [36] is a modiﬁcation\
    \ of the solution realized in the Pix2Pix\nmodel, which includes several improvements\
    \ including the Coarse-to-Fine generator, multi-\nscale discriminators, and improved\
    \ adversarial loss. Pix2PixHD generally consists of global\ngenerator G1 and local\
    \ enhancer G2 (see Figure 4, where *** are referred to the residual\nblocks).\
    \ Throughout the training process, the global generator is initially trained,\
    \ followed\nby the training of the local enhancer in a progressive manner based\
    \ on their respective\nresolutions. Subsequently, all the networks are ﬁne-tuned\
    \ jointly. The purpose of this\ngenerator is to efﬁciently combine global and\
    \ local information for the task of image\nsynthesis. Three discriminators are\
    \ used for effective detail capturing on multiple scales.\nFigure 4. Pix2PixHD\
    \ generator block diagram.\nA signiﬁcant performance boost was provided by the\
    \ loss modiﬁcation and two extra\nterms, LFM-feature matching loss and perceptual\
    \ loss, were added LVGG [93] as objective\nfunctions. The feature matching loss\
    \ performs the stabilization of the training. It happens\ndue the point that the\
    \ generator has to produce natural statistics at multiple scales:\nLFM(G, Dk)\
    \ = λFMEy,x ∑\ni=1\n1\nNi\n[||D(i)\nk (y, x) − D(i)\nk (y, G(y))||1]\n(5)\nwhere\
    \ D(i)\nk\ndenotes the output of the i-th layer of the Dk discriminator.\nLVGG\
    \ = λVGGEy,x ∑\ni=1\n1\nMi\n[||F(i)(x) − F(i)(G(y))||1]\n(6)\nwhere F(i) denotes\
    \ the i-th layer with Mi elements of the VGG network.\n3.1.4. Mask R-CNN\nMask\
    \ R-CNN [94] is a CNN-based architecture that provides the instance segmentation\n\
    of various objects in the images. These objects in images are usually called the\
    \ Regions of\nInterest (ROIs). This is the latest version of the R-CNN model [95],\
    \ where R-CNN stands\nfor Regions detected with CNN. Firstly, R-CNN has been improved\
    \ to Fast R-CNN [96],\nthen to Faster R-CNN [97], and, ﬁnally, to Mask R-CNN.\
    \ As it was mentioned earlier, in\nR-CNN based models the ROIs are detected with\
    \ the CNN feature’s selective search. In\nMask R-CNN, this selective search was\
    \ improved to Mask R-CNN by adding the Region\nProposal Network (RPN) in order\
    \ to initiate and identify the ROIs and by adding a new\nbranch for the prediction\
    \ of the mask that covers the found region, i.e., an object in the\nimage. The\
    \ RPN and ResNet101 backbone allow for making the object detection (bounding\n\
    boxes generation) and instance segmentation if there are several ROIs in one image\
    \ and\nEntropy 2023, 25, 987\n10 of 30\nthey have different sizes and partially\
    \ overlap each other. Figure 5 presents a block diagram\nof Mask R-CNN architecture.\n\
    Figure 5. Mask R-CNN block diagram.\n3.2. Performance Metrics\nIn this study,\
    \ we compare the original VNIR images with the VNIR images generated\nby the Pix2PixHD\
    \ model. To perform this, we considered the Mean Average Error (MAE),\nMean Average\
    \ Percentage Error (MAPE), Mean Squared Error (MSE), Root Mean Square\nError (RMSE),\
    \ Peak Signal to Noise Ratio (PSNR), Structural Similarity Index Measure\n(SSIM),\
    \ and Feature Similarity Index Measure (FSIM) as follows:\nMAE = 1\nn\nn\n∑\n\
    i=1\n|(yi − xi)|\n(7)\nMAPE = 100%\nn\nn\n∑\ni=1\n\f\f\f\f\n(yi − xi)\nyi\n\f\f\
    \f\f\n(8)\nMSE = 1\nn\nn\n∑\ni=1\n(yi − xi)2\n(9)\nRMSE =\ns\n1\nn\nn\n∑\ni=1\n\
    (yi − xi)2\n(10)\nPSNR = 10 log10\n \nR2\n1\nn ∑n\ni=1(yi − xi)2\n!\n(11)\nSSIM\
    \ =\n\x14\nl(xi, yi)α · c(xi, yi)β · s(xi, yi)γ\n\x15\n(12)\nFSIM =\n\x14\nSPC(xi,\
    \ yi)α · SGM(xi, yi)β\n\x15\n(13)\nwhere yi is the generated or synthesized image,\
    \ xi is the original image, n is the number\nof observations, R is the image maximum\
    \ possible pixel value, l is the luminance, c is the\ncontrast, s is the structure,\
    \ α, β, and γ are the weights, SPC is the invariant to light variation\nin images,\
    \ and SGM is the computation of image gradient.\nWe used precision, recall, mean\
    \ Intersection over Union (IoU), mean Average Precision\n(mAP), and F1-score to\
    \ verify the efﬁciency of the Mask R-CNN model on the synthesized\nVNIR pictures\
    \ during the training and validation stages, which are deﬁned as follows:\nEntropy\
    \ 2023, 25, 987\n11 of 30\nPrecision =\nTP\nTP + FP\n(14)\nRecall =\nTP\nTP +\
    \ FN\n(15)\nIoU = Area o f Overlap\nArea o f Union\n(16)\nAP = ∑\nn\n(Recalln\
    \ − Recalln−1)Precisionn\n(17)\nF1-score = 2 ∗ Precision ∗ Recall\nPrecision +\
    \ Recall\n(18)\nPrecision and recall are based on True Positives (TP), True Negatives\
    \ (TN), False\nPositives (FP), and False Negatives (FN). TP denotes instances\
    \ in which the model correctly\npredicts a speciﬁc object from a given class in\
    \ images, TN denotes the instances in which\nthe model correctly predicts an object\
    \ that does not belong to a given class, and FP denotes\nthe instances in which\
    \ the model predicts a speciﬁc class, but the object does not actually\nbelong\
    \ to that class. In contrast, FN are the cases in which the model makes no prediction\n\
    of a particular class, but the object actually belongs to one of the classes.\
    \ The object classes\nare described in Section 3.4.\nThe AP is a region that lies\
    \ beneath the precision–recall curve. The weighted mean of\nprecisions at each\
    \ IoU threshold, with the increase in recall from the preceding threshold\nas\
    \ the weight, is how AP summarizes a precision–recall curve. It is calculated\
    \ using (17),\nwhere Precisionn and Recalln are the Precision and Recall at the\
    \ n-th IoU threshold.\nThe mAP over all classes or overall IoU thresholds is calculated\
    \ with the mAP score.\nAP is averaged over all the classes. There is no distinction\
    \ between AP and mAP in this case.\nIn our scenario, since AP is averaged across\
    \ all the classes, there is no difference between\nAP and mAP. We calculated AP\
    \ values for IoU = 0.50 (AP50), for IoU = 0.75 (AP75), for the\nobjects with an\
    \ area less than 32 squared pixels (APS), for the objects with an area ranging\n\
    from 32 to 96 squared pixels (APM), and for the objects with an area higher 96\
    \ squared\npixels (APL).\n3.3. Experimental Testbeds and Data Acquisition\nIn\
    \ this section, we describe the apple fruits used for the experiments and present\n\
    experimental testbeds for data collection:\n(i) The experimental testbed for acquiring\
    \ the dataset containing paired RGB and\nVNIR images of stored apples;\n(ii) The\
    \ experimental testbed for stored apple VNIR images collection containing VNIR\n\
    images acquired by a multispectral camera.\nThe ﬁrst testbed is designed for paired\
    \ RGB and VNIR images collection in order\nto train and validate the GAN-based\
    \ DL models for VNIR images translation from RGB\nimages (see Section 3.3.1).\
    \ The second testbed is used for the stored apples VNIR images\ncollection as\
    \ well as for the CNN-based model training and validation of postharvest decay\n\
    zones detection and segmentation in the generated VNIR images (see Section 3.3.2).\n\
    3.3.1. Experimental Testbed for Paired RGB and VNIR Imaging Data Collection\n\
    We selected 16 apples of four kinds (“Delicious”, “Fuji”, “Gala”, “Reinette Simirenko”)\n\
    and divided them into four rows according to their kind (each row corresponds\
    \ to each\napple kind). Each row contained four apples of different types, where\
    \ every apple has\ndifferent treatment from left to right: an apple with no treatment,\
    \ a thoroughly washed\nand wiped apple, a mechanically damaged apple, and a shock-frozen\
    \ apple supercooled\nunder −20◦, respectively. The apple without treatment serves\
    \ as a reference for each kind.\nA thoroughly washed apple indicates the removal\
    \ of the natural protective wax layer\nEntropy 2023, 25, 987\n12 of 30\nfrom an\
    \ apple. A mechanically damaged apple imitates the wrong storing conditions. A\n\
    shock-frozen apple simulates the wrong storing conditions. Figure 6 shows these\
    \ apples.\nFigure 6. Apples selected for data collection.\nThe ﬁrst testbed is\
    \ used for data collection under the recommended room storage\nconditions. The\
    \ temperature ranges from 25 ◦C to 32 ◦C and Relative Humidity (RH) of\n34% [98].\
    \ The testbed contains aluminum frames and is 1 m in length, 1 m wide, and 1.7\
    \ m\nhigh. Apples lie on a table with a white tray at the height of 1.3 m above\
    \ the ﬂoor level.\nWe also use SLR camera Canon M50 and the multispectral camera\
    \ CMS-V1 CMS18100073\n(CMS-V) attached at the middle top of the frame and connected\
    \ to a PC laptop via the USB\nhub. The distance between the table with the apples\
    \ on top and the camera is 500 mm.\nThe lamps allowed us to simulate real storage\
    \ conditions for apples as well as perform the\ncollection of images under full\
    \ and partial illumination. Detailed information about the\nacquired dataset and\
    \ the ﬁrst experimental testbed is described in [99]. Figure 7 shows the\nﬁrst\
    \ testbed.\nFigure 7. Experimental testbed for paired RGB and VNIR image capturing.\n\
    The multispectral camera CMS-V allows acquiring images in the range of 561–838\
    \ nm,\nincluding the visible and NIR ranges. This camera imager is characterized\
    \ by the modiﬁed\nBayer matrix made of a group of 3 × 3 pixels, called macro-pixel,\
    \ ﬁltering 3 × 3 (9) spectral\nbands. The raw image delivered by the camera is\
    \ built of 9 interleaved spectral sub-images\n(8 colors + 1 Panchromatic) with\
    \ the 1280 × 1024 pixels resolution. Each RGB image re-\nlates to 9 images from\
    \ the following spectral bands channel0 = 561 nm, channel1 = 597 nm,\nchannel2\
    \ = 635 nm,\nchannel3\n=\n673\nnm,\nchannel4\n=\n724\nnm,\nchannel5 = 762 nm,\n\
    channel6 = 802 nm, channel7 = 838 nm, and channel8 (panchromatic channel) = 0\
    \ nm. The\nresolution of the nine sub-images is 426 × 339 pixels.\nWe acquired\
    \ 1305 sequential RGB images and 1305 corresponding VNIR images in\n838 nm range\
    \ to see the decay dynamics in presented apples. The examples of images are\n\
    shown in Figure 8.\nEntropy 2023, 25, 987\n13 of 30\nFigure 8. Types of images\
    \ obtained during the experiments: (A)—RGB image of apples acquired\nunder the\
    \ full illumination; (B)—VNIR image of apples acquired under the full illumination\
    \ (838 nm);\n(C)—VNIR image of apples acquired under the partial illumination\
    \ (838 nm).\n3.3.2. Experimental Testbed for VNIR Imaging Data Collection\nIn\
    \ this experiment, we selected 22 apples of the “Alesya”, “Fuji”, “Golden” and\n\
    “Reinette Simirenko” seasonal types for data acquisition. The apples were between\
    \ 8 and\n10 cm in diameter, and most of them were multicolor with red and yellow\
    \ sections. There\nwere also some apples containing fungi zones, i.e., grey-brown\
    \ moldy areas in apples, as\nthe examples of apples stored under violated storage\
    \ conditions. These apples were used\nin order to increase the data representation\
    \ for early postharvest decay detection tasks in\nthe stored apples using VNIR\
    \ imaging data. These apples are demonstrated in Figure 9.\nFigure 9. VNIR image\
    \ of apples selected for data collection.\nThe second testbed presented in Figure\
    \ 10 is a greenhouse that includes silicon frames\nand ﬁve shelves, a plastic\
    \ wrap, a multispectral camera, 10 LED strip lights with red/blue\ndiodes, a power\
    \ supply (total power is 150 Watt) for controlling the LEDs, a logger, and\na\
    \ pallet with apples. It can be used for the simulation of different processes\
    \ related to\nplant breeding in various environmental conditions including extremely\
    \ dry or wet modes.\nTemperature and humidity regulation in the testbed is provided\
    \ with the LED strip lights,\nthe plastic wrap, and several water pallets located\
    \ on three lower bottom separate shelves.\nThe silica frames are the basic elements\
    \ of a presented greenhouse characterized by\nthe following dimensions 170 cm\
    \ in height, 48 cm in length, and 67 cm in width. Two strip\nlights were ﬁxed\
    \ on each shelf while the multispectral camera and the pallet with the apples\n\
    were ﬁxed on the separate shelves (see Figure 10). Each selected strip has 60\
    \ LEDs with the\nwavelength of 650–660 nm (red light LEDs) and 455–465 nm (blue\
    \ light LEDs) for highest\nchlorophyll concentration in plants to provide the\
    \ most effective photosynthesis processes.\nThis is also fair for crops and plants\
    \ at the postharvest stages [100]. It is necessary to keep\nthe quality of plant\
    \ production which is another reason why these LED strip lights are\nused in the\
    \ greenhouse. We rely on the power supply (12 V DC, 150 W, IP33) as the energy\n\
    source for the SMD 5050 LED strip lights, and GL100-N/GL100-WL logger by Graphtech\n\
    Corporation, supplied with the GS-TH sensor module, for temperature and humidity\n\
    values registration during the data collection process.\nFor the VNIR image capturing,\
    \ the multispectral camera CMS-V described in Section 3.3.1\nwas also chosen.\
    \ The camera was connected via USB-A wire to the HP EliteBook 820 G3\nLaptop with\
    \ IntelCore i3-6100 CPU 2.30 GHz, where all the images were acquired and\nsaved\
    \ as JPG-ﬁles with 426 × 339 pixels.\nEntropy 2023, 25, 987\n14 of 30\nFigure\
    \ 10. Experimental greenhouse for data acquisition.\nWe obtained 1029 sequential\
    \ VNIR images in the 838 nm range collected from CMS-V\ncamera’s channel7. These\
    \ images were acquired under the temperature range from 35 ◦C\nto 40 ◦C and RH\
    \ equal to 70% with the goal to simulate potential violation of the storage\n\
    process of selected apples. This violation is necessary to speed up the decay\
    \ processes in\napples. We also collected 100 sequential RGB images (see the example\
    \ in Figure 11) for\nthe CNN-based model training and validation with the aim\
    \ to demonstrate the up-to-date\napproach based on the combination of pre-trained\
    \ GAN-based and CNN-based models.\nRGB sequential images had the dimensions of\
    \ 339 pixels × 426 pixels × 3 channels (or\nsimply 339 × 426 × 3).\nFigure 11.\
    \ RGB image of apples selected for data collection.\n3.4. Data Annotation\nIn\
    \ order to apply a CNN-based deep learning model for the image instance segmenta-\n\
    tion, we used the Supervisely Ecosystem [101] for annotation and labeling of VNIR\
    \ imaging\ndata. It is worth reiterating here that we provide this labeling only\
    \ for the VNIR images\nacquired with the testbed, described in Section 3.3.2 as\
    \ these images were specially collected\nas the sequential VNIR imaging dataset\
    \ for the DL model training and validation on early\npostharvest decay detection\
    \ and segmentation of apples.\nFour classes of objects in the images are deﬁned\
    \ as: Healthy apple, Decay, Fungi, and\nSpoiled apple. By the Healthy apple we\
    \ understand the apples without any visible damages\nor spoiled zones in the images.\
    \ The dark gray colored areas with the postharvest decay in\napples were indicated\
    \ as Decay. By Fungi we indicate white colored moldy zones in apples.\nHere we\
    \ distinguish the postharvest decay zones marked as the Decay class, and moldy\n\
    zones marked as the Fungi class. If an apple has objects of the Fungi class, it\
    \ means that this\napple is supposed to have been stored under the violated storage\
    \ conditions, e.g., extreme\ntemperature or humidity, which resulted in the apple’s\
    \ full spoilage. The apples with only\nEntropy 2023, 25, 987\n15 of 30\nthe postharvest\
    \ decay zones (Decay) can be sent for recycling, while apples with moldy\nzones\
    \ (Fungi) must be removed from others in order to prevent the spoilage of all\
    \ samples.\nWe also deﬁned the Spoiled apples class: there are stored apples with\
    \ more than 50 percent of\nspoiled areas (Decay objects) or moldy zones (Fungi\
    \ objects) coverage. Figure 12 illustrates\nthe procedure of image annotation.\n\
    Figure 12. The example of image annotation and objects classes in Supervisely.\n\
    4. Results and Discussion\n4.1. Image-to-Image Models Comparison for VNIR Images\
    \ Generation from RGB\nIn this section, we show the results of deep learning models\
    \ based on generative\nadversarial networks comparison for VNIR images translation\
    \ from RGB images. We\nprovide this comparison on the dataset sequential RGB images\
    \ and corresponding VNIR\nimages in the 838 nm range presented in Section 3.3.1.\
    \ To estimate the performance, we\nsplit the data into the train set (80%) and\
    \ the validation set (20%). The augmentation\ntechniques as Random Rotations,\
    \ Shifts, Zoom, and Flips are implemented to increase the\ndata representativity\
    \ and to keep the model’s efﬁciency during the training and validation\nstages.\
    \ We do not use the transformations such as Contrast/Brightness adjustments because\n\
    they may lead to the information loss from the acquired VNIR imaging data. Taking\
    \ into\naccount that the image-to-image translation is also known as the translation\
    \ from the\ndomain B to domain A (or just BtoA), it was necessary to label domain\
    \ B and domain A images\nfrom our acquired paired dataset. We identiﬁed the RGB\
    \ images as domain B and domain A\nas the VNIR images. All models were evaluated\
    \ by 200 epochs where the ﬁrst 100 were\nimplemented with the constant learning\
    \ rate and the remaining 100 with linearly decreasing\nto zero. The models training\
    \ and validation were realized via the Python scripts launched\nin Google Colab.\n\
    For the CycleGAN model, we use ResNet encoder–decoder architecture consisting\
    \ of\ntwo downsampling layers, six ResNet bottleneck blocks and two upsampling\
    \ layers. We\nalso employ an Adam optimizer with the learning rate of 0.0002 and\
    \ momentum parameters\nβ1 = 0.5 and β2 = 0.999.\nFor the Pix2Pix model training,\
    \ we ﬁxed the same parameters: batch size = 1, β1 = 0.5,\nβ2 = 0.999, and learning\
    \ rate = 0.0002. The U-Net generator had 4 downsampling blocks.\nOptimization\
    \ included the generator loss optimization step and the discriminator loss opti-\n\
    mization step, respectively. Regularization parameters are as follows: λVGG =\
    \ λFeat = 10,\nλL1 = 100.\nFor the Pix2PixHD model, we also implement the same\
    \ parameters: Adam optimizer,\nbatch size = 1, β1 = 0.5, β2 = 0.999, and learning\
    \ rate = 0.0002.\nFigure 13 shows the discriminator values of CycleGAN (Figure\
    \ 13a), Pix2Pix (Figure 13b),\nand Pix2PixHD (Figure 13c) models during the training\
    \ stage. We show the model’s\ndiscriminator losses because they show the ability\
    \ of GAN-based models to identify the\nquality of synthesized VNIR images by generator\
    \ in comparison to original VNIR images.\nEntropy 2023, 25, 987\n16 of 30\n(a)\n\
    (b)\n(c)\nFigure 13. GAN-based models evaluation: (a) CycleGAN discriminator loss\
    \ values during the\ntraining; (b) Pix2Pix discriminator loss values during the\
    \ training; and (c) Pix2PixHD discriminator\nloss values during the training.\n\
    For selected GAN-based models we see that the training stage is unstable, but\
    \ the\ndiscriminator losses tend to decrease over time. Pix2PixHD shows the lowest\
    \ loss value in\ncomparison to CycleGAN and Pix2Pix. For the models validation,\
    \ we reconstructed the\nVNIR images using model weights acquired during the training.\
    \ We used MAE, MAPE,\nEntropy 2023, 25, 987\n17 of 30\nMSE, PSNR and SSIM metrics\
    \ to estimate the quality of VNIR reconstructed images in\ncomparison with original\
    \ VNIR images. Figure 14 shows these images (with ‘cyclegan’,\n‘pix2pix’, ‘pix2pixHD’\
    \ labels, respectively) in comparison to the original VNIR image\n(‘reference’\
    \ label) via Python visualization tools.\n(a)\n(b)\nFigure 14. Examples of VNIR\
    \ generated images in comparison to original VNIR image: (a) obtained\nunder full\
    \ illumination; and (b) obtained under partial illumination.\nEntropy 2023, 25,\
    \ 987\n18 of 30\nTable 1 summarizes the results of considered models performance,\
    \ where the results\nfor Pix2PixHD model are highlighted with the black blod.\
    \ Considering both the pixel-\nbased and the image metrics, one can conclude on\
    \ the promising results. The generated\nimages look more or less similar to the\
    \ original ones. The images containing apples, overall\nlight intensity similar\
    \ to the ground truth and the decay region are mainly preserved.\nHowever, all\
    \ the models have particular artifacts. The CycleGAN model has the big\nstamp-like\
    \ artifacts and there are a lot of missed decayed zones in the apples. In terms\
    \ of\nmetrics mentioned in Section 3.2, Pix2Pix and Pix2PixHD models perform the\
    \ comparable\nand much better than others, and decay regions preserved relatively\
    \ well, although the\nintensity level mismatch can be seen. Pix2PixHD models produce\
    \ perceptually good\nimages preserving importance for task features and the mean\
    \ error level is equal to 0.6%.\nIn terms of important metrics for the image quality\
    \ estimation, such as PSNR and SSIM, the\nPix2PixHD model showed higher values\
    \ in comparison to Pix2Pix (46.859 against 46.433,\nand 0.972 against 0.955, respectively).\
    \ Taking into account the results of this comparison,\nwe decided to use the Pix2PixHD\
    \ model for VNIR images generation from RGB during the\nnext stages.\nTable 1.\
    \ Image-to-image models comparison for RGB to VNIR images generation.\nModels\n\
    MAE\nMAPE\nMSE\nPSNR\nSSIM\nCycleGAN\n0.067\n0.105\n0.01127\n27.375\n0.856\nPix2Pix\n\
    0.004\n0.006\n0.00003\n46.433\n0.955\nPix2PixHD\n0.004\n0.006\n0.00003\n46.859\n\
    0.972\n4.2. Segmentation of Generated VNIR Images for Early Postharvest Decay\
    \ Detection in Apples\nIn this section, we apply the CNN-based models for instance\
    \ segmentation of gen-\nerated VNIR images. Based on the results reported in Section\
    \ 4.1, we use the Pix2PixHD\nmodel for the VNIR image generation. The dataset\
    \ containing 456 images of stored apples\n(see Section 3.3.2) was used as the\
    \ input for trained weights of the Pix2PixHD model to\ngenerate VNIR images. The\
    \ examples of synthesized VNIR images from corresponding\ninput RGB images are\
    \ presented in Figure 15. Comparing the quality of new images with\nthe images\
    \ that were synthesizing during Pix2PixHD training stage (see Section 4.1), PSNR\n\
    and SSIM values increased from 46.859 to 52.876 and from 0.972 to 0.994, respectively.\n\
    Mask R-CNN is used as the CNN-based model for the images instance segmentation.\n\
    However, before applying Mask R-CNN to images, synthesized with Pix2PixHD, it\
    \ was\nnecessary to train Mask R-CNN on real VNIR images to detect and segment\
    \ the fungi and\ndecayed areas in stored apples. We used the labeled dataset containing\
    \ 1029 VNIR images\n(see Section 3.3.2) for Mask R-CNN model training and validation.\
    \ We report on the object\nclasses used for data labeling in Section 3.4.\nIn\
    \ this work, we implemented Mask R-CNN with the L1 as a loss function, ResNet50\n\
    as the backbone, Stochastic gradient descent (SGD) as an optimizer, and COCO weights\n\
    to use Detectron2 library [102]. GaussianNoise, RandomGamma, RandomBrightness,\
    \ and\nHorizontalFlip were applied as the data augmentation function to keep the\
    \ efﬁciency of the\nproposed model during the training and validation stages.\
    \ The model was developed in\nPython, and all calculations were realized in Google\
    \ Colab.\nIn our experiment, we apply the cross-validation for Mask R-CNN model\
    \ training on\nthe dataset containing VNIR images. Cross-validation is a widespread\
    \ technique helping\navoid the overﬁtting during the model training on big data.\
    \ In our case, we deal with\nthe sequential images, i.e., one apple can be located\
    \ in many images without any changes\nin position, which may resulted in improving\
    \ the loss value after decreasing during the\ntraining procedure. During cross-validation,\
    \ the data is usually split into several groups,\ncalled folds, where each group\
    \ is used for the training and validation one by one. For\nexample, if the dataset\
    \ is separated into three folds, the pipeline is the following: (i) the ﬁrst\n\
    fold is a validation set, the second and third folds form the train set; (ii)\
    \ the ﬁrst and the\nEntropy 2023, 25, 987\n19 of 30\nthird folds are train set,\
    \ the second fold is a validation set; and (iii) the ﬁrst and the second\nfolds\
    \ are training set, the third fold is a validation set. This pipeline is also\
    \ fair for the\ncross-validation with four and higher folds distribution. By default,\
    \ the number of folds,\nwhich is also called k-folds, is usually set equal to\
    \ ﬁve or ten, but the k-folds may be different.\nIn this work, we set the number\
    \ of folds equal to two, three, six, and nine. We show the\nmean Average Precision\
    \ values for each k-fold during Mask R-CNN models in Table 2.\nFigure 15. Examples\
    \ of synthesized VNIR images with Pix2PixHD model weights.\nEntropy 2023, 25,\
    \ 987\n20 of 30\nTable 2. Comparison of Average Precision for Mask R-CNN model.\n\
    k-Folds\nmAP\nmAP50\nmAP75\nmAPS\nmAPM\nmAPL\n2\n64.251\n90.205\n65.606\n37.202\n\
    75.980\n97.412\n3\n67.652\n90.354\n65.348\n35.400\n75.290\n96.290\n6\n67.026\n\
    90.950\n67.055\n38.188\n74.609\n98.871\n9\n67.993\n91.120\n64.871\n31.575\n75.181\n\
    97.257\nThe results for each object class segmentation (or per-category segmentation)\
    \ during\nMask R-CNN model during all folds are given in Tables 3 and 4). We also\
    \ used mAP\nand F1-score metrics to evaluate the segmentation quality during model\
    \ training for folds\ndistribution. Tables 3 and 4 present the mean mAP and F1-score\
    \ values for each fold,\nrespectively. As can be seen, the number of folds leads\
    \ to increasing of the metrics values\nand segmentation accuracy. This is a demonstration\
    \ of a cross-validation technique in\ncomparison to ordinary data splitting on\
    \ the training and validation sets. Figure 16 shows\nthe examples of VNIR images\
    \ with predicted annotations of object classes (see Section 3.4)\nacquired during\
    \ the Mask R-CNN model validation. Here we show the examples of\nsynthesized and\
    \ annotated images from k-folds = 9, as the distribution with the better mAP\n\
    and F1-score values (see the column for k-folds = 9 with black bold in the Tables\
    \ 3 and 4).\nEven though the postharvest decay zones (Decay object class in Tables\
    \ 3 and 4) and the\nfungal areas (Fungi object class in Tables 3 and 4) are detected\
    \ with small values of an F1-\nscore metric (58.861 and 40.968, respectively),\
    \ a trained Mask R-CNN model allows for the\ndetection and segmentation of spoiled\
    \ apples (Spoiled apple object class), containing either\ndecayed zones or fungal\
    \ areas, or both, with an F1-score of 94.800, which is promising.\nTable 3. Results\
    \ on per-category segmentation by Mask R-CNN using mAP metric.\nCategory\nmAP\n\
    k-Folds = 2\nk-Folds = 3\nk-Folds = 6\nk-Folds = 9\nHealthy apple\n94.785\n95.154\n\
    93.951\n98.350\nSpoiled apple\n87.839\n92.567\n93.678\n93.997\nDecay\n53.509\n\
    53.408\n54.620\n57.562\nFungi\n31.581\n30.609\n34.285\n39.967\nTable 4. Results\
    \ on per-category segmentation by Mask R-CNN using F1-score metric.\nCategory\n\
    F1-Score\nk-Folds = 2\nk-Folds = 3\nk-Folds = 6\nk-Folds = 9\nHealthy apple\n\
    95.640\n95.589\n94.799\n98.375\nSpoiled apple\n88.120\n93.134\n94.689\n94.800\n\
    Decay\n53.309\n53.213\n54.850\n58.861\nFungi\n31.686\n37.247\n35.126\n40.968\n\
    Entropy 2023, 25, 987\n21 of 30\n(a)\n(b)\nFigure 16. Comparison of object classes\
    \ annotation in real VNIR images (a,b, on the left with\n‘Annotated image’ label)\
    \ to predicted object annotations (a,b, on the right with ‘Predicted annotations’\n\
    label) during Mask R-CNN model training.\nTaking into account the results of Mask\
    \ R-CNN evaluation on real VNIR imaging data\nand the results of the Pix2PixHD\
    \ evaluation in comparison to other GAN-based models\n(see Section 4.1), we provide\
    \ the proposed pipeline for segmentation of generated VNIR\nimages. To estimate\
    \ it we acquired the dataset containing only 456 sequential RGB images\nwithout\
    \ the corresponded VNIR images (see Section 3.4). The images were acquired in\n\
    the greenhouse (see Section 3.3.2) under the same environmental conditions (temperature\n\
    range is from 35 ◦C to 40 ◦C, and RH is 70%, respectively). In order to simulate\
    \ possible\noccasion during the real storage, spoiled apples with the decayed\
    \ and fungi zones were\nadded to healthy (non-damaged) apples. The concept is\
    \ as follows: (i) we utilize a set\nof RGB images as input data; (ii) these RGB\
    \ images are passed through a GAN-based\nmodel (speciﬁcally, Pix2PixHD with pre-trained\
    \ weights in our case); (iii) VNIR images\nare generated from the input RGB images\
    \ using Pix2PixHD; and (iv) the generated VNIR\nimages are fed into a CNN-based\
    \ model (speciﬁcally, Mask R-CNN with pre-trained\nweights) to obtain these images\
    \ with predicted annotation masks. Figure 17 shows the\nexamples of images which\
    \ were synthesized and segmented with the proposed pipeline.\nAs it can be seen\
    \ in Figure 17b,c, the proposed approach helps detect and segment the\ndecayed\
    \ zones separately from the fungi zones in the stored apples. All computations\
    \ were\nalso provided in Google Colab.\nEntropy 2023, 25, 987\n22 of 30\n(a)\n\
    (b)\n(c)\nFigure 17. Synthesized VNIR images (a–c) segmentation with Mask R-CNN\
    \ model.\n4.3. Early Postharvest Decay Detection in Stored Apples Using Generated\
    \ VNIR Imaging Data on\nan Embedded System\nTo evaluate the applicability of a\
    \ GAN- and CNN-based models in real-life scenarios\nwe conduct an experiment using\
    \ the NVIDIA Jetson Nano embedded system [103]. The\ngoal of the experiment is\
    \ to validate the model’s ability to handle video streams with\nvarying frames\
    \ per second (FPS).\nEntropy 2023, 25, 987\n23 of 30\nWe used 100 RGB images.\
    \ Input RGB images are characterized by the size of 256 pixels.\nA GAN model was\
    \ used to generate VNIR images from input images and processed over\n100 images\
    \ at an average rate of 17 FPS. The generated images were then tested with\nMask\
    \ R-CNN, resulting in an average rate of 0.420 FPS. Low FPS in Mask R-CNN can\
    \ be\nattributed to its complexity compared to Pix2PixHD. As the two-stage detection\
    \ model that\nperforms instance segmentation by detecting objects and generating\
    \ pixel-level masks for\neach object, it requires more computational resources.\
    \ Figure 18 shows the examples of\nVNIR images generated and segmented using the\
    \ NVIDIA Jetson Nano based on the input\nRGB data.\n(a)\n(b)\n(c)\nFigure 18.\
    \ Generated and segmented VNIR images (a–c) using Jetson Nano.\n4.4. Discussion\n\
    In this section, we compare our results with other relevant research works in\
    \ the ﬁeld\nof application of NIR imaging data and deep learning techniques for\
    \ early postharvest\ndecay and fungal zones prediction in stored apples. The proposed\
    \ approach is based on\nthe joint application of GAN and CNN techniques for artiﬁcial\
    \ generation and subsequent\nEntropy 2023, 25, 987\n24 of 30\nsegmentation of\
    \ VNIR images. However, in order to segment the decayed and fungal zones\nin artiﬁcially\
    \ generated VNIR images, we had to train and validate a CNN technique on the\n\
    real VNIR images containing these zones in stored apples. To perform this, we\
    \ acquired the\ndataset of VNIR images (see Section 3.3) and then trained and\
    \ validated the Mask R-CNN\nmodel (see Section 4.2).\nTaking into the account\
    \ the ability of Mask R-CNN to provide the multi-class instance\nand semantic\
    \ segmentation (see Section 3.1.4), we trained the model not only to detect\n\
    and identify the quality of apple (Healthy apple or Spoiled apple, see Section\
    \ 3.4), but also to\ndetect and predict the decayed and fungal zones separately\
    \ from each other. Novelty is that\nthe model is trained and validated to identify\
    \ the quality of stored apples by taking into\naccount the presence of decayed\
    \ and fungal areas in the apples themselves. In this context,\nan apple is classiﬁed\
    \ as Spoiled apple if it contains the decayed or fungal zones, whether\nthey are\
    \ separate or combined. Conversely, if an apple does not exhibit any decayed or\n\
    fungal zones prior to storage stage, i.e., during the VNIR image collection, it\
    \ is classiﬁed as\na Healthy apple. However, if the decayed and/or fungal zones\
    \ emerge in the apple during\nthe storage stage, its classiﬁcation transitions\
    \ from a Healthy apple to Spoiled apple.\nRelevant works in this area can be classiﬁed\
    \ into three main groups according to main\ntasks: (i) defective apples detection\
    \ based on the internal quality parameters [104,105];\n(ii) early defect detection\
    \ in apples [104,106]; and (iii) early fungi detection in apples [73,107,108].\n\
    Table 5 presents a comparative study of these works.\nTable 5. Comparative table\
    \ of relevant research works.\nReferences\nTask\nNIR Images\nRange, nm\nTechnique\n\
    Metric\nValue\n[104]\nReal-time apple defect inspection\n850\nYOLO v4\nF1\n92.000\n\
    [105]\nApples surface defect segmentation\n460–842\nU-Net\nF1-score\n87.000\n\
    [105]\nApples surface defect segmentation\n460–842\nthe improved U-Net\nF1-score\n\
    91.000\n[106]\nEarly bruise detection in apples\n900–2350\nFaster R-CNN\nmAP\n\
    96.900\n[106]\nEarly bruise detection in apples\n900–2350\nYOLO v3-Tiny\nmAP\n\
    99.100\n[106]\nEarly bruise detection in apples\n900–2350\nYOLO 5s\nmAP\n99.600\n\
    [107]\nMoldy core detection in apples\n400–850\nCARS-PLS-DA model\nAccuracy\n\
    87.880\n[73]\nCodling Moth detection in apples\n900–1700\nGradient tree boosting\n\
    F1-score\n97.000\n[108]\nMoldy core detection in apples\n200–1100\nBP-ANN\nAccuracy\n\
    95.000\nThe authors applied various tools and methods based on machine learning\
    \ for de-\ntecting the defected and diseased zones in wide NIR ranges (400–2350\
    \ nm, globally) with\ndetailed spectral information on the diseased zones. The\
    \ most relevant and similar ap-\nproach to the current research is reported in\
    \ [104], where a YOLO v4 model in sorting\nmachine for real-time detection of\
    \ defects in “Red Fuji”, “Golden Delicious”, and “Granny\nSmith” apples is implemented.\
    \ The authors used the RGB and corresponded NIR images\nin the range of 850 nm\
    \ of the apples in the machine’s sorting line. Moreover, the ability of\ntrained\
    \ YOLO v4 models to detect with bounding box ‘calyx’ and ‘stem’ zones separately\n\
    from ‘defect’ zones was demonstrated. In this work, we applied the Mask R-CNN\
    \ not\nonly to detect (with bounding box) and segment (with mask) the decayed\
    \ and the fungal\nareas in stored apples, but also to identify the quality of\
    \ apples as diseased (Spoiled apple)\nif such zones are detected by the model.\
    \ F1-score and mAP values for Decay and Fungi\nzones are not that high. These\
    \ problems can be ﬁxed in our future work by obtaining more\nVNIR images containing\
    \ the fungal and the decayed areas in order to increase the data\nrepresentation\
    \ during the model validation. On the other hand, the results for Spoiled apple\n\
    (apple contains Fungi and/or Decay zones) segmentation are 98.350 and 98.375,\
    \ respectively,\nwhich is promising. Finally, the proposed approach is for an\
    \ apple quality control during\nEntropy 2023, 25, 987\n25 of 30\nthe storage stage,\
    \ i.e., before sending the stored apples to the fruit sorting machine. The\nsystem,\
    \ which could generate VNIR images without a multispectral or hyperspectral cam-\n\
    era based only on the input RGB images with segmented fungal and decayed zones,\
    \ if they\noccur in stored apples, can be applied as an additional stage for the\
    \ fruit and vegetable\ncontrol before sending them to a sorting machine.\nIn [106],\
    \ the authors compared several Faster R-CNN, YOLO v3-Tiny, and YOLO 5s\nmodels\
    \ for early decay (or bruise) detection in apples. The approach proposed in this\
    \ work\nshowed promising results in terms of the mAP metric (98.350 for Mask R-CNN\
    \ validation,\nin our case, against 96.900 for Faster R-CNN, 99.100 for YOLO v3-Tiny,\
    \ and 96.600 for\nYOLO 5s), and the selected model was trained to segment the\
    \ decayed and fungal zones in\napples, while authors in [106] trained the models\
    \ to identify and predict the apples without\n(‘No bruise’), with a small (‘Mild\
    \ bruise’) and signiﬁcant (‘Severe bruise’) decayed areas\nin apples. The authors\
    \ also acquired the NIR images in spectral range of 900–2350 nm,\nwhile in this\
    \ work the images from 838 nm range were used in order to make sure that the\n\
    diseased zones in VNIR images are visible in the RGB images as well.\nIn [105],\
    \ the authors trained and validated U-Net and the improved U-Net model for\nthe\
    \ defect segmentation in VNIR images of apples. In this work, we have demonstrated\n\
    the semantic segmentation of decayed and fungal areas with an advanced experimental\n\
    methodology. We simulated ordinary and extreme storage conditions during the paired\n\
    RGB and VNIR images collection procedures. Taking this into account, we achieved\
    \ a\nrelevant value for the diseased apples segmentation in terms of the F1-score\
    \ metric.\nWe have demonstrated the potential for the postharvest decay and fungi\
    \ prediction\nfor stored apples. However, it can be scaled to other crops that\
    \ are widely used in food\nproduction, e.g., carrots, tomatoes, cucumber, fruits\
    \ or bananas. For example, the system\nthat allows the generation and segmentation\
    \ of VNIR images can be applied for segmenta-\ntion and prediction of such fungi\
    \ as Sclerotinia sclerotiorum or Botrytis cinerea. ’Sclerotinia’\nand ’Botrytis’\
    \ fungal zones have similar morphology and, if they occur in plants, it is a\n\
    nontrivial task to identify one fungi variety from another one using only RGB\
    \ imagery or\nvisual estimation of the internal fungal traits with human eyes\
    \ [109]. The system supplied\nwith the trained and validated DL technique based\
    \ on the GAN and CNN models can\nassist the user with the additional spectral\
    \ information about each fungi acquired from the\ngenerated VNIR images. It is\
    \ useful for more precise antifungal activities during the food\nquality control.\n\
    Another potential scenario is the application of the proposed research for the\
    \ prehar-\nvest diseases and the defect detection for the plants both growing\
    \ in natural environments\nand in artiﬁcially controlled systems. For example,\
    \ it can be a robot moving platform or\nunmanned aerial vehicle without a hyperspectral\
    \ camera, but with an embedded system\nthat may generate and segment the NIR imaging\
    \ data from the input RGB one. However,\nDL technique should be trained, tested\
    \ and validated precisely, as the proposed system has\nto detect and segment not\
    \ only the diseased plants from the healthy ones, but also to detect\nthe kind\
    \ of defect (damage, decay, fungi variety) with the following suggestion of spoiled\n\
    fruit processing.\n5. Conclusions\nNIR imagery provides detailed information about\
    \ the diseased areas in stored fruits,\nwhich is why the hyperspectral cameras\
    \ containing thousands of bands are used for food\nquality monitoring at postharvest\
    \ stages. However, hyperspectral devices are expensive\nand are not friendly for\
    \ the farmers and sellers’ usage. In this article, we have presented the\napproach\
    \ based on the GAN and CNN DL techniques for early postharvest decay zones\nand\
    \ fungi areas detection and prediction in stored apples using synthesized and\
    \ segmented\nVNIR images.\nEntropy 2023, 25, 987\n26 of 30\nThe conclusions of\
    \ this work are as follows:\n•\nThe analysis of Pix2Pix, CycleGAN, and Pix2PixHD\
    \ models, which are widely used\nGAN techniques, and their application to a dataset\
    \ containing paired 1305 sequential\nRGB images and 1305 sequential VNIR images\
    \ of stored apples of different varieties\nand various pre-treatments. The images\
    \ were acquired under the full and partial\nillumination with the goal to simulate\
    \ real storage conditions.\n•\nComparison of the real VNIR images with the VNIR\
    \ images synthesized by selected\nGAN based models. The VNIR images generated\
    \ via Pix2PixHD a 0.972 score for the\nSSIM metric.\n•\nThe training and test\
    \ of Mask R-CNN on another dataset containing only 1029 sequen-\ntial VNIR images\
    \ of apples under violated storage conditions. Within this test, an\nF1-score\
    \ of 58.861 is achieved for the postharvest decay zones and F1-score 40.968 for\n\
    the fungal zones detection. The spoiled apples with the decayed and fungal zones\
    \ are\ndetected and segmented with F1-score 94.800.\n•\nTesting of the proposed\
    \ solution on an embedded system with AI capabilities. We\nused 100 RGB images\
    \ of stored apples as an input data for NVIDIA Jetson Nano, and\nthe time processing\
    \ of VNIR images generation by Pix2PixHD showed 17 FPS. The\ndetection and segmentation\
    \ by Mask R-CNN achieved 0.42 FPS.\nThe proposed approach is a promising solution\
    \ able to substitute expensive hyper-\nspectral imaging devices for early postharvest\
    \ decay prediction tasks in postharvest food\nquality control.\nAuthor Contributions:\
    \ Conceptualization, N.S., D.S. and A.S.; methodology, N.S.; software, N.S. ans\n\
    I.S.; validation, N.S., I.S. and M.S.; formal analysis, N.S. and D.S.; investigation,\
    \ N.S.; resources, N.S.;\ndata curation, N.S.; writing—original draft preparation,\
    \ N.S., I.S.; writing—review and editing, N.S.,\nI.S., M.S., D.S. and A.S.; visualization,\
    \ N.S. and I.S.; supervision, A.S. All authors have read and\nagreed to the published\
    \ version of the manuscript.\nFunding: This research received no external funding\n\
    Institutional Review Board Statement: Not applicable.\nData Availability Statement:\
    \ Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n\
    Abbreviations\nThe following abbreviations are used in this manuscript:\nNIR\n\
    Near Infrared Image\nVNIR\nVisible Near Infrared Image\nAI\nArtiﬁcial Intelligence\n\
    CV\nComputer Vision\nML\nMachine Learning\nSVM\nSupport Vector Machines\nRF\n\
    Random Forest\nkNN\nK-Nearest Neighbors Algorithm\nGTB\nGradient Tree Boosting\n\
    DL\nDeep Learning\nCNN\nConvolutional Neural Network\nGAN\nGenerative Adversarial\
    \ Network\nROI\nRegions of Interests\nSBC\nSingle Board Computer\nRH\nRelative\
    \ Humidity\nEntropy 2023, 25, 987\n27 of 30\nReferences\n1.\nUnited Nations Data\
    \ about Current World Population. Available online: https://www.worldometers.info/world-population/\n\
    (accessed on 26 June 2023).\n2.\nUnited Nations Data on Current and Prospected\
    \ World Population. Available online: https://population.un.org/wpp/Graphs/\n\
    Probabilistic/POP/TOT/900 (accessed on 26 June 2023).\n3.\nUllah, S.; Hashmi,\
    \ M.; Lee, J.; Youk, J.H.; Kim, I.S. Recent Advances in Pre-harvest, Post-harvest,\
    \ Intelligent, Smart, Active, and\nMultifunctional Food Packaging. Fibers Polym.\
    \ 2022, 23, 2063–2074. [CrossRef]\n4.\nCoradi, P.C.; Maldaner, V.; Lutz, É.; da\
    \ Silva Daí, P.V.; Teodoro, P.E. Inﬂuences of drying temperature and storage conditions\
    \ for\npreserving the quality of maize postharvest on laboratory and ﬁeld scales.\
    \ Sci. Rep. 2020, 10, 22006. [CrossRef] [PubMed]\n5.\nMohammed, M.; Alqahtani,\
    \ N.; El-Shaﬁe, H. Development and evaluation of an ultrasonic humidiﬁer to control\
    \ humidity in a\ncold storage room for postharvest quality management of dates.\
    \ Foods 2021, 10, 949. [CrossRef] [PubMed]\n6.\nSun, X.; Baldwin, E.; Bai, J.\
    \ Applications of gaseous chlorine dioxide on postharvest handling and storage\
    \ of fruits and\nvegetables—A review. Food Control 2019, 95, 18–26. [CrossRef]\n\
    7.\nYahia, E.M.; Fonseca, J.M.; Kitinoja, L. Postharvest losses and waste. In\
    \ Postharvest Technology of Perishable Horticultural Commodities;\nElsevier: Amsterdam,\
    \ The Netherlands, 2019; pp. 43–69.\n8.\nPalumbo, M.; Attolico, G.; Capozzi, V.;\
    \ Cozzolino, R.; Corvino, A.; de Chiara, M.L.V.; Pace, B.; Pelosi, S.; Ricci,\
    \ I.; Romaniello, R.; et al.\nEmerging Postharvest Technologies to Enhance the\
    \ Shelf-Life of Fruit and Vegetables: An Overview. Foods 2022, 11, 3925. [CrossRef]\n\
    9.\nElik, A.; Yanik, D.K.; Istanbullu, Y.; Guzelsoy, N.A.; Yavuz, A.; Gogus, F.\
    \ Strategies to reduce post-harvest losses for fruits and\nvegetables. Strategies\
    \ 2019, 5, 29–39.\n10.\nFAO Data on Global Apple Production. Available online:\
    \ https://www.fao.org/faostat/en/#data/QCL/visualize (accessed on\n26 June 2023).\n\
    11.\nHarker, F.; Feng, J.; Johnston, J.; Gamble, J.; Alavi, M.; Hall, M.; Chheang,\
    \ S. Inﬂuence of postharvest water loss on apple quality:\nThe use of a sensory\
    \ panel to verify destructive and non-destructive instrumental measurements of\
    \ texture. Postharvest Biol.\nTechnol. 2019, 148, 32–37. [CrossRef]\n12.\nde Andrade,\
    \ J.C.; Galvan, D.; Effting, L.; Tessaro, L.; Aquino, A.; Conte-Junior, C.A. Multiclass\
    \ Pesticide Residues in Fruits\nand Vegetables from Brazil: A Systematic Review\
    \ of Sample Preparation Until Post-Harvest. Crit. Rev. Anal. Chem. 2021, 1–23.\n\
    Available online: https://www.tandfonline.com/doi/abs/10.1080/10408347.2021.2013157\
    \ (accessed on 26 June 2023).\n13.\nBratu, A.M.; Petrus, M.; Popa, C. Monitoring\
    \ of post-harvest maturation processes inside stored fruit using photoacoustic\
    \ gas\nsensing spectroscopy. Materials 2020, 13, 2694. [CrossRef]\n14.\nSottocornola,\
    \ G.; Baric, S.; Nocker, M.; Stella, F.; Zanker, M. Picture-based and conversational\
    \ decision support to diagnose\npost-harvest apple diseases. Expert Syst. Appl.\
    \ 2022, 189, 116052. [CrossRef]\n15.\nMalvandi, A.; Feng, H.; Kamruzzaman, M.\
    \ Application of NIR spectroscopy and multivariate analysis for Non-destructive\
    \ evaluation\nof apple moisture content during ultrasonic drying. Spectrochim.\
    \ Acta Part A Mol. Biomol. Spectrosc. 2022, 269, 120733. [CrossRef]\n16.\nSchlie,\
    \ T.P.; Dierend, W.; Koepcke, D.; Rath, T. Detecting low-oxygen stress of stored\
    \ apples using chlorophyll ﬂuorescence\nimaging and histogram division. Postharvest\
    \ Biol. Technol. 2022, 189, 111901. [CrossRef]\n17.\nWang, L.; Huang, J.; Li,\
    \ Z.; Liu, D.; Fan, J. A review of the polyphenols extraction from apple pomace:\
    \ Novel technologies and\ntechniques of cell disintegration. Crit. Rev. Food Sci.\
    \ Nutr. 2022, 1–14. [CrossRef] [PubMed]\n18.\nWu, X.; Fauconnier, M.L.; Bi, J.\
    \ Characterization and Discrimination of Apples by Flash GC E-Nose: Geographical\
    \ Regions and\nBotanical Origins Studies in China. Foods 2022, 11, 1631. [CrossRef]\
    \ [PubMed]\n19.\nBiasi, A.; Zhimo, V.Y.; Kumar, A.; Abdelfattah, A.; Salim, S.;\
    \ Feygenberg, O.; Wisniewski, M.; Droby, S. Changes in the fungal\ncommunity assembly\
    \ of apple fruit following postharvest application of the yeast biocontrol agent\
    \ Metschnikowia fructicola.\nHorticulturae 2021, 7, 360. [CrossRef]\n20.\nBartholomew,\
    \ H.P.; Lichtner, F.J.; Bradshaw, M.; Gaskins, V.L.; Fonseca, J.M.; Bennett, J.W.;\
    \ Jurick, W.M. Comparative Penicillium\nspp. Transcriptomics: Conserved Pathways\
    \ and Processes Revealed in Ungerminated Conidia and during Postharvest Apple\n\
    Fruit Decay. Microorganisms 2022, 10, 2414. [CrossRef]\n21.\nMorales-Cedeno, L.R.;\
    \ del Carmen Orozco-Mosqueda, M.; Loeza-Lara, P.D.; Parra-Cota, F.I.; de Los Santos-Villalobos,\
    \ S.; Santoyo,\nG. Plant growth-promoting bacterial endophytes as biocontrol agents\
    \ of pre-and post-harvest diseases: Fundamentals, methods\nof application and\
    \ future perspectives. Microbiol. Res. 2021, 242, 126612. [CrossRef]\n22.\nNikparvar,\
    \ B.; Thill, J.C. Machine learning of spatial data. ISPRS Int. J. Geo-Inf. 2021,\
    \ 10, 600. [CrossRef]\n23.\nZhang, Y.; Liu, M.; Yu, F.; Zeng, T.; Wang, Y. An\
    \ o-shape neural network with attention modules to detect junctions in biomedical\n\
    images without segmentation. IEEE J. Biomed. Health Inform. 2021, 26, 774–785.\
    \ [CrossRef]\n24.\nZhao, S.; Blaabjerg, F.; Wang, H. An overview of artiﬁcial\
    \ intelligence applications for power electronics. IEEE Trans. Power\nElectron.\
    \ 2020, 36, 4633–4658. [CrossRef]\n25.\nMeshram, V.; Patil, K.; Meshram, V.; Hanchate,\
    \ D.; Ramkteke, S. Machine learning in agriculture domain: A state-of-art survey.\n\
    Artif. Intell. Life Sci. 2021, 1, 100010. [CrossRef]\n26.\nKakani, V.; Nguyen,\
    \ V.H.; Kumar, B.P.; Kim, H.; Pasupuleti, V.R. A critical review on computer vision\
    \ and artiﬁcial intelligence in\nfood industry. J. Agric. Food Res. 2020, 2, 100033.\
    \ [CrossRef]\n27.\nRasti, S.; Bleakley, C.J.; Holden, N.; Whetton, R.; Langton,\
    \ D.; O’Hare, G. A survey of high resolution image processing techniques\nfor\
    \ cereal crop growth monitoring. Inf. Process. Agric. 2022, 9, 300–315. [CrossRef]\n\
    Entropy 2023, 25, 987\n28 of 30\n28.\nTang, Y.; Qiu, J.; Zhang, Y.; Wu, D.; Cao,\
    \ Y.; Zhao, K.; Zhu, L. Optimization strategies of fruit detection to overcome\
    \ the challenge\nof unstructured background in ﬁeld orchard environment: A review.\
    \ Precis. Agric. 2023, 24, 1183–1219. [CrossRef]\n29.\nOuhami, M.; Haﬁane, A.;\
    \ Es-Saady, Y.; El Hajji, M.; Canals, R. Computer vision, IoT and data fusion\
    \ for crop disease detection\nusing machine learning: A survey and ongoing research.\
    \ Remote Sens. 2021, 13, 2486. [CrossRef]\n30.\nWu, Z.; Chen, Y.; Zhao, B.; Kang,\
    \ X.; Ding, Y. Review of weed detection methods based on computer vision. Sensors\
    \ 2021, 21, 3647.\n[CrossRef] [PubMed]\n31.\nMendigoria, C.H.; Aquino, H.; Concepcion,\
    \ R.; Alajas, O.J.; Dadios, E.; Sybingco, E. Vision-based postharvest analysis\
    \ of musa\nacuminata using feature-based machine learning and deep transfer networks.\
    \ In Proceedings of the 2021 IEEE 9th Region 10\nHumanitarian Technology Conference\
    \ (R10-HTC), Bangalore, India, 30 September–2 October 2021; pp. 1–6.\n32.\nBucio,\
    \ F.; Isaza, C.; Gonzalez, E.; De Paz, J.Z.; Sierra, J.R.; Rivera, E.A. Non-Destructive\
    \ Post-Harvest Tomato Mass Estimation Model\nBased on Its Area via Computer Vision\
    \ and Error Minimization Approaches. IEEE Access 2022, 10, 100247–100256. [CrossRef]\n\
    33.\nRopelewska, E. Postharvest Authentication of Potato Cultivars Using Machine\
    \ Learning to Provide High-Quality Products. Chem.\nProc. 2022, 10, 30.\n34.\n\
    Isola, P.; Zhu, J.Y.; Zhou, T.; Efros, A.A. Image-to-Image Translation with Conditional\
    \ Adversarial Networks. arXiv 2018, arXiv:1611.07004.\n35.\nZhu, J.Y.; Park, T.;\
    \ Isola, P.; Efros, A.A. Unpaired Image-to-Image Translation using Cycle-Consistent\
    \ Adversarial Networks.\narXiv 2020, arXiv:1703.10593.\n36.\nWang, T.C.; Liu,\
    \ M.Y.; Zhu, J.Y.; Tao, A.; Kautz, J.; Catanzaro, B. High-Resolution Image Synthesis\
    \ and Semantic Manipulation\nwith Conditional GANs. arXiv 2018, arXiv:1711.11585.\n\
    37.\nChristovam, L.E.; Shimabukuro, M.H.; Galo, M.d.L.B.; Honkavaara, E. Pix2pix\
    \ conditional generative adversarial network with\nMLP loss function for cloud\
    \ removal in a cropland time series. Remote Sens. 2022, 14, 144. [CrossRef]\n\
    38.\nde Lima, D.C.; Saqui, D.; Mpinda, S.A.T.; Saito, J.H. Pix2pix network to\
    \ estimate agricultural near infrared images from rgb data.\nCan. J. Remote Sens.\
    \ 2022, 48, 299–315. [CrossRef]\n39.\nFarooque, A.A.; Afzaal, H.; Benlamri, R.;\
    \ Al-Naemi, S.; MacDonald, E.; Abbas, F.; MacLeod, K.; Ali, H. Red-green-blue\
    \ to\nnormalized difference vegetation index translation: A robust and inexpensive\
    \ approach for vegetation monitoring using machine\nvision and generative adversarial\
    \ networks. Precis. Agric. 2023, 24, 1097–1115. [CrossRef]\n40.\nBertoglio, R.;\
    \ Mazzucchelli, A.; Catalano, N.; Matteucci, M. A comparative study of Fourier\
    \ transform and CycleGAN as domain\nadaptation techniques for weed segmentation.\
    \ Smart Agric. Technol. 2023, 4, 100188. [CrossRef]\n41.\nJung, D.H.; Kim, C.Y.;\
    \ Lee, T.S.; Park, S.H. Depth image conversion model based on CycleGAN for growing\
    \ tomato truss\nidentiﬁcation. Plant Methods 2022, 18, 83. [CrossRef] [PubMed]\n\
    42.\nvan Marrewijk, B.M.; Polder, G.; Kootstra, G. Investigation of the added\
    \ value of CycleGAN on the plant pathology dataset.\nIFAC-PapersOnLine 2022, 55,\
    \ 89–94. [CrossRef]\n43.\nYang, J.; Zhang, T.; Fang, C.; Zheng, H. A defencing\
    \ algorithm based on deep learning improves the detection accuracy of caged\n\
    chickens. Comput. Electron. Agric. 2023, 204, 107501. [CrossRef]\n44.\nTsuchikawa,\
    \ S.; Ma, T.; Inagaki, T. Application of near-infrared spectroscopy to agriculture\
    \ and forestry.\nAnal. Sci. 2022,\n38, 635–642. [CrossRef]\n45.\nStasenko, N.;\
    \ Savinov, M.; Burlutskiy, V.; Pukalchik, M.; Somov, A. Deep Learning for Postharvest\
    \ Decay Prediction in Apples.\nIn Proceedings of the IECON 2021—47th Annual Conference\
    \ of the IEEE Industrial Electronics Society, Toronto, ON, Canada,\n13–16 October\
    \ 2021; pp. 1–6.\n46.\nRonneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional\
    \ networks for biomedical image segmentation. In Proceedings of the\nInternational\
    \ Conference on Medical Image Computing and Computer-Assisted Intervention, Munich,\
    \ Germany, 5–9 October\n2015; Springer: Berlin/Heidelberg, Germany, 2015; pp.\
    \ 234–241.\n47.\nYurtkulu, S.C.; ¸Sahin, Y.H.; Unal, G. Semantic Segmentation\
    \ with Extended DeepLabv3 Architecture. In Proceedings of the 2019\n27th Signal\
    \ Processing and Communications Applications Conference (SIU), Sivas, Turkey,\
    \ 24–26 April 2019; pp. 1–4.\n48.\nAssunção, E.; Gaspar, P.D.; Mesquita, R.; Simões,\
    \ M.P.; Alibabaei, K.; Veiros, A.; Proença, H. Real-Time Weed Control Application\n\
    Using a Jetson Nano Edge Device and a Spray Mechanism. Remote Sens. 2022, 14,\
    \ 4217. [CrossRef]\n49.\nSaddik, A.; Latif, R.; Taher, F.; El Ouardi, A.; Elhoseny,\
    \ M. Mapping Agricultural Soil in Greenhouse Using an Autonomous\nLow-Cost Robot\
    \ and Precise Monitoring. Sustainability 2022, 14, 15539. [CrossRef]\n50.\nde\
    \ Aguiar, A.S.P.; dos Santos, F.B.N.; dos Santos, L.C.F.; de Jesus Filipe, V.M.;\
    \ de Sousa, A.J.M. Vineyard trunk detection using\ndeep learning–An experimental\
    \ device benchmark. Comput. Electron. Agric. 2020, 175, 105535. [CrossRef]\n51.\n\
    Mazzia, V.; Khaliq, A.; Salvetti, F.; Chiaberge, M. Real-time apple detection\
    \ system using embedded systems with hardware\naccelerators: An edge AI application.\
    \ IEEE Access 2020, 8, 9102–9114. [CrossRef]\n52.\nBeegam, K.S.; Shenoy, M.V.;\
    \ Chaturvedi, N. Hybrid consensus and recovery block-based detection of ripe coffee\
    \ cherry bunches\nusing RGB-D sensor. IEEE Sens. J. 2021, 22, 732–740. [CrossRef]\n\
    53.\nZhang, W.; Liu, Y.; Chen, K.; Li, H.; Duan, Y.; Wu, W.; Shi, Y.; Guo, W.\
    \ Lightweight fruit-detection algorithm for edge computing\napplications. Front.\
    \ Plant Sci. 2021, 12, 740936. [CrossRef]\n54.\nVilcamiza, G.; Trelles, N.; Vinces,\
    \ L.; Oliden, J. A coffee bean classiﬁer system by roast quality using convolutional\
    \ neural\nnetworks and computer vision implemented in an NVIDIA Jetson Nano. In\
    \ Proceedings of the 2022 Congreso Internacional de\nInnovación y Tendencias en\
    \ Ingeniería (CONIITI), Bogota, Colombia, 5–7 October 2022; pp. 1–6.\nEntropy\
    \ 2023, 25, 987\n29 of 30\n55.\nFan, K.J.; Su, W.H. Applications of Fluorescence\
    \ Spectroscopy, RGB-and MultiSpectral Imaging for Quality Determinations of\n\
    White Meat: A Review. Biosensors 2022, 12, 76. [CrossRef]\n56.\nZou, X.; Zhang,\
    \ Y.; Lin, R.; Gong, G.; Wang, S.; Zhu, S.; Wang, Z. Pixel-level Bayer-type colour\
    \ router based on metasurfaces. Nat.\nCommun. 2022, 13, 3288. [CrossRef]\n57.\n\
    Rivero Mesa, A.; Chiang, J. Non-invasive grading system for banana tiers using\
    \ RGB imaging and deep learning. In Proceedings\nof the 2021 7th International\
    \ Conference on Computing and Artiﬁcial Intelligence, Tianjin, China, 23–26 April\
    \ 2021; pp. 113–118.\n58.\nNasiri, A.; Taheri-Garavand, A.; Zhang, Y.D. Image-based\
    \ deep learning automated sorting of date fruit. Postharvest Biol. Technol.\n\
    2019, 153, 133–141. [CrossRef]\n59.\nDeng, L.; Li, J.; Han, Z. Online defect detection\
    \ and automatic grading of carrots using computer vision combined with deep\n\
    learning methods. LWT 2021, 149, 111832. [CrossRef]\n60.\nZhang, X.; Zhou, X.;\
    \ Lin, M.; Sun, J. ShufﬂeNet: An extremely efﬁcient convolutional neural network\
    \ for mobile devices. arXiv\n2017, arXiv:1707.01083.\n61.\nWu, F.; Yang, Z.; Mo,\
    \ X.; Wu, Z.; Tang, W.; Duan, J.; Zou, X. Detection and counting of banana bunches\
    \ by integrating deep\nlearning and classic image-processing algorithms. Comput.\
    \ Electron. Agric. 2023, 209, 107827. [CrossRef]\n62.\nBaheti, B.; Innani, S.;\
    \ Gajre, S.; Talbar, S. Semantic scene segmentation in unstructured environment\
    \ with modiﬁed DeepLabV3+.\nPattern Recognit. Lett. 2020, 138, 223–229. [CrossRef]\n\
    63.\nWu, F.; Duan, J.; Ai, P.; Chen, Z.; Yang, Z.; Zou, X. Rachis detection and\
    \ three-dimensional localization of cut off point for\nvision-based banana robot.\
    \ Comput. Electron. Agric. 2022, 198, 107079. [CrossRef]\n64.\nBuyukarikan, B.;\
    \ Ulker, E. Classiﬁcation of physiological disorders in apples fruit using a hybrid\
    \ model based on convolutional\nneural network and machine learning methods. Neural\
    \ Comput. Appl. 2022, 34, 16973–16988. [CrossRef]\n65.\nLi, J.; Zheng, K.; Yao,\
    \ J.; Gao, L.; Hong, D. Deep unsupervised blind hyperspectral and multispectral\
    \ data fusion. IEEE Geosci.\nRemote Sens. Lett. 2022, 19, 1–5. [CrossRef]\n66.\n\
    Liang, J.; Li, X.; Zhu, P.; Xu, N.; He, Y. Hyperspectral reﬂectance imaging combined\
    \ with multivariate analysis for diagnosis of\nSclerotinia stem rot on Arabidopsis\
    \ thaliana leaves. Appl. Sci. 2019, 9, 2092. [CrossRef]\n67.\nVashpanov, Y.; Heo,\
    \ G.; Kim, Y.; Venkel, T.; Son, J.Y. Detecting green mold pathogens on lemons\
    \ using hyperspectral images.\nAppl. Sci. 2020, 10, 1209. [CrossRef]\n68.\nFahrentrapp,\
    \ J.; Ria, F.; Geilhausen, M.; Panassiti, B. Detection of gray mold leaf infections\
    \ prior to visual symptom appearance\nusing a ﬁve-band multispectral sensor. Front.\
    \ Plant Sci. 2019, 10, 628. [CrossRef]\n69.\nWan, L.; Li, H.; Li, C.; Wang, A.;\
    \ Yang, Y.; Wang, P. Hyperspectral Sensing of Plant Diseases: Principle and Methods.\
    \ Agronomy\n2022, 12, 1451. [CrossRef]\n70.\nBłaszczyk, U.; Wyrzykowska, S.; G\
    \ ˛astoł, M. Application of Bioactive Coatings with Killer Yeasts to Control Post-Harvest\
    \ Apple\nDecay Caused by Botrytis cinerea and Penicillium italicum. Foods 2022,\
    \ 11, 1868. [CrossRef]\n71.\nAmaral Carneiro, G.; Walcher, M.; Baric, S. Cadophora\
    \ luteo-olivacea isolated from apple (Malus domestica) fruit with post-\nharvest\
    \ side rot symptoms in northern Italy. Eur. J. Plant Pathol. 2022, 162, 247–255.\
    \ [CrossRef]\n72.\nGhooshkhaneh, N.G.; Golzarian, M.R.; Mollazade, K. VIS-NIR\
    \ spectroscopy for detection of citrus core rot caused by Alternaria\nalternata.\
    \ Food Control 2023, 144, 109320. [CrossRef]\n73.\nEkramirad, N.; Khaled, A.Y.;\
    \ Doyle, L.E.; Loeb, J.R.; Donohue, K.D.; Villanueva, R.T.; Adedeji, A.A. Nondestructive\
    \ detection of\ncodling moth infestation in apples using pixel-based nir hyperspectral\
    \ imaging with machine learning and feature selection.\nFoods 2022, 11, 8. [CrossRef]\n\
    74.\nJiang, B.; He, J.; Yang, S.; Fu, H.; Li, T.; Song, H.; He, D. Fusion of machine\
    \ vision technology and AlexNet-CNNs deep learning\nnetwork for the detection\
    \ of postharvest apple pesticide residues. Artif. Intell. Agric. 2019, 1, 1–8.\
    \ [CrossRef]\n75.\nHuang, C.; Li, X.; Wen, Y. AN OTSU image segmentation based\
    \ on fruitﬂy optimization algorithm. Alex. Eng. J. 2021, 60, 183–188.\n[CrossRef]\n\
    76.\nKrizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep\
    \ convolutional neural networks. Commun. ACM 2017,\n60, 84–90. [CrossRef]\n77.\n\
    Zhang, D.; Zhou, X.; Zhang, J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath\
    \ blight using an unmanned aerial system with\nhigh-resolution color and multispectral\
    \ imaging. PLoS ONE 2018, 13, e0187470. [CrossRef]\n78.\nSun, Y.; Xiao, H.; Tu,\
    \ S.; Sun, K.; Pan, L.; Tu, K. Detecting decayed peach using a rotating hyperspectral\
    \ imaging testbed. LWT\n2018, 87, 326–332. [CrossRef]\n79.\nLi, J.; Luo, W.; Wang,\
    \ Z.; Fan, S. Early detection of decay on apples using hyperspectral reflectance\
    \ imaging combining both principal\ncomponent analysis and improved watershed\
    \ segmentation method. Postharvest Biol. Technol. 2019, 149, 235–246. [CrossRef]\n\
    80.\nHyperspectral Imaging Systems Market Size Report. Available online: https://www.grandviewresearch.com/industry-analysis/\n\
    hyperspectral-imaging-systems-market (accessed on 26 June 2023).\n81.\nMirza,\
    \ M.; Osindero, S. Conditional generative adversarial nets. arXiv 2014, arXiv:1411.1784.\n\
    82.\nIllarionova, S.; Shadrin, D.; Trekin, A.; Ignatiev, V.; Oseledets, I. Generation\
    \ of the nir spectral band for satellite images with\nconvolutional neural networks.\
    \ Sensors 2021, 21, 5646. [CrossRef]\n83.\nLu, Y.; Chen, D.; Olaniyi, E.; Huang,\
    \ Y. Generative adversarial networks (GANs) for image augmentation in agriculture:\
    \ A\nsystematic review. Comput. Electron. Agric. 2022, 200, 107208. [CrossRef]\n\
    Entropy 2023, 25, 987\n30 of 30\n84.\nKhatri, K.; Asha, C.; D’Souza, J.M. Detection\
    \ of Animals in Thermal Imagery for Surveillance using GAN and Object Detection\n\
    Framework. In Proceedings of the 2022 International Conference for Advancement\
    \ in Technology (ICONAT), Goa, India, 21–22\nJanuary 2022; pp. 1–6.\n85.\nValerio\
    \ Giuffrida, M.; Scharr, H.; Tsaftaris, S.A. Arigan: Synthetic arabidopsis plants\
    \ using generative adversarial network. In Pro-\nceedings of the IEEE International\
    \ Conference on Computer Vision Workshops, Venice, Italy, 22–29 October 2017;\
    \ pp. 2064–2071.\n86.\nTang, H.; Xu, D.; Yan, Y.; Corso, J.J.; Torr, P.H.; Sebe,\
    \ N. Multi-channel attention selection gans for guided image-to-image\ntranslation.\
    \ arXiv 2020, arXiv:2002.01048.\n87.\nGuo, Z.; Shao, M.; Li, S. Image-to-image\
    \ translation using an offset-based multi-scale codes GAN encoder. Vis. Comput.\
    \ 2023,\n1–17. [CrossRef]\n88.\nFard, A.S.; Reutens, D.C.; Vegh, V. From CNNs\
    \ to GANs for cross-modality medical image estimation. Comput. Biol. Med. 2022,\n\
    146, 105556. [CrossRef] [PubMed]\n89.\nSaharia, C.; Chan, W.; Chang, H.; Lee,\
    \ C.; Ho, J.; Salimans, T.; Fleet, D.; Norouzi, M. Palette: Image-to-image diffusion\
    \ models. In\nProceedings of the ACM SIGGRAPH 2022 Conference Proceedings, Vancouver,\
    \ BC, Canada, 7–11 August 2022; pp. 1–10.\n90.\nKshatriya, B.S.; Dubey, S.R.;\
    \ Sarma, H.; Chaudhary, K.; Gurjar, M.R.; Rai, R.; Manchanda, S. Semantic Map\
    \ Injected GAN Training\nfor Image-to-Image Translation. In Proceedings of the\
    \ Satellite Workshops of ICVGIP 2021, Gandhinagar, India, 8–10 December\n2022;\
    \ Springer: Berlin/Heidelberg, Germany, 2022; pp. 235–249.\n91.\nSa, I.; Lim,\
    \ J.Y.; Ahn, H.S.; MacDonald, B. deepNIR: Datasets for generating synthetic NIR\
    \ images and improved fruit detection\nsystem using deep learning techniques.\
    \ Sensors 2022, 22, 4721. [CrossRef] [PubMed]\n92.\nLi, C.; Wand, M. Precomputed\
    \ real-time texture synthesis with markovian generative adversarial networks.\
    \ In Proceedings of the\nEuropean Conference on Computer Vision, Amsterdam, The\
    \ Netherlands, 11–14 October 2016; Springer: Berlin/Heidelberg,\nGermany, 2016,\
    \ pp. 702–716.\n93.\nSimonyan, K.; Zisserman, A. Very deep convolutional networks\
    \ for large-scale image recognition. arXiv 2014, arXiv:1409.1556.\n94.\nHe, K.;\
    \ Gkioxari, G.; Dollár, P.; Girshick, R. Mask r-cnn. In Proceedings of the IEEE\
    \ International Conference on Computer Vision,\nVenice, Italy, 22–29 October 2017;\
    \ pp. 2961–2969.\n95.\nGirshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich\
    \ feature hierarchies for accurate object detection and semantic segmentation.\
    \ In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\
    \ Washington, DC, USA, 23–28 June 2014; pp. 580–587.\n96.\nGirshick, R. Fast r-cnn.\
    \ In Proceedings of the IEEE International Conference on Computer Vision, Santiago,\
    \ Chile, 7–13 December\n2015; pp. 1440–1448.\n97.\nRen, S.; He, K.; Girshick,\
    \ R.; Sun, J. Faster r-cnn: Towards real-time object detection with region proposal\
    \ networks. Adv. Neural\nInf. Process. Syst. 2015, 28, 91–99. [CrossRef] [PubMed]\n\
    98.\nSaletnik, B.; Zaguła, G.; Saletnik, A.; Bajcar, M.; Słysz, E.; Puchalski,\
    \ C. Method for Prolonging the Shelf Life of Apples after\nStorage. Appl. Sci.\
    \ 2022, 12, 3975. [CrossRef]\n99.\nNesteruk, S.; Illarionova, S.; Akhtyamov, T.;\
    \ Shadrin, D.; Somov, A.; Pukalchik, M.; Oseledets, I. XtremeAugment: Getting\
    \ More From\nYour Data Through Combination of Image Collection and Image Augmentation.\
    \ IEEE Access 2022, 10, 24010–24028. [CrossRef]\n100. Martínez-Zamora, L.; Castillejo,\
    \ N.; Artés-Hernández, F. Postharvest UV-B and photoperiod with blue+ red LEDs\
    \ as strategies to\nstimulate carotenogenesis in bell peppers. Appl. Sci. 2021,\
    \ 11, 3736. [CrossRef]\n101. Supervisely Data Annotator. Available online: https://app.supervise.ly\
    \ (accessed on 26 June 2023).\n102. Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.Y.;\
    \ Girshick, R. Detectron2. 2019. Available online: https://github.com/facebookresearch/\n\
    detectron2 (accessed on 26 June 2023).\n103. NVIDIA. Jetson Modules Technical\
    \ Speciﬁcatons. 2023. Available online: https://developer.nvidia.com/embedded/jetson-\n\
    modules (accessed on 26 June 2023).\n104. Fan, S.; Liang, X.; Huang, W.; Zhang,\
    \ V.J.; Pang, Q.; He, X.; Li, L.; Zhang, C. Real-time defects detection for apple\
    \ sorting using\nNIR cameras with pruning-based YOLOV4 network. Comput. Electron.\
    \ Agric. 2022, 193, 106715. [CrossRef]\n105. Tang, Y.; Bai, H.; Sun, L.; Wang,\
    \ Y.; Hou, J.; Huo, Y.; Min, R. Multi-Band-Image Based Detection of Apple Surface\
    \ Defect Using\nMachine Vision and Deep Learning. Horticulturae 2022, 8, 666.\
    \ [CrossRef]\n106. Yuan, Y.; Yang, Z.; Liu, H.; Wang, H.; Li, J.; Zhao, L. Detection\
    \ of early bruise in apple using near-infrared camera imaging\ntechnology combined\
    \ with deep learning. Infrared Phys. Technol. 2022, 127, 104442. [CrossRef]\n\
    107. Zhang, Z.; Pu, Y.; Wei, Z.; Liu, H.; Zhang, D.; Zhang, B.; Zhang, Z.; Zhao,\
    \ J.; Hu, J. Combination of interactance and transmittance\nmodes of Vis/NIR spectroscopy\
    \ improved the performance of PLS-DA model for moldy apple core. Infrared Phys.\
    \ Technol. 2022,\n126, 104366. [CrossRef]\n108. Hu, Q.X.; Tian, J.; Fang, Y. Detection\
    \ of moldy cores in apples with near-infrared transmission spectroscopy based\
    \ on wavelet and\nBP network. Int. J. Pattern Recognit. Artif. Intell. 2019, 33,\
    \ 1950020. [CrossRef]\n109. Sadek, M.E.; Shabana, Y.M.; Sayed-Ahmed, K.; Abou\
    \ Tabl, A.H. Antifungal activities of sulfur and copper nanoparticles against\n\
    cucumber postharvest diseases caused by Botrytis cinerea and Sclerotinia sclerotiorum.\
    \ J. Fungi 2022, 8, 412. [CrossRef]\nDisclaimer/Publisher’s Note: The statements,\
    \ opinions and data contained in all publications are solely those of the individual\n\
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or\
    \ the editor(s) disclaim responsibility for any injury to\npeople or property\
    \ resulting from any ideas, methods, instructions or products referred to in the\
    \ content.\n"
  inline_citation: '>'
  journal: Entropy (Basel. Online)
  limitations: '>'
  pdf_link: https://www.mdpi.com/1099-4300/25/7/987/pdf?version=1687938691
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep Learning in Precision Agriculture: Artificially Generated VNIR Images
    Segmentation for Early Postharvest Decay Prediction in Apples'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/comst.2023.3312221
  analysis: '>'
  authors:
  - Harrison Kurunathan
  - Hailong Huang
  - Kai Li
  - Wei Ni
  - Ekram Hossain
  citation_count: 5
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading [MathJax]/extensions/MathZoom.js
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Journals & Magazines >IEEE Communications Surveys &... >Volume: 26 Issue:
    1 Machine Learning-Aided Operations and Communications of Unmanned Aerial Vehicles:
    A Contemporary Survey Publisher: IEEE Cite This PDF Harrison Kurunathan; Hailong
    Huang; Kai Li; Wei Ni; Ekram Hossain All Authors 8 Cites in Papers 1013 Full Text
    Views Abstract Document Sections I. Introduction II. Background to UAV Systems
    III. Survey of Surveys IV. Background of ML for UAV Applications V. ML for UAV
    Trajectory Planning and Mission Scheduling Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: Over the past decade, Unmanned Aerial Vehicles
    (UAVs) have provided pervasive, efficient, and cost-effective solutions for data
    collection and communications. Their excellent mobility, flexibility, and fast
    deployment enable UAVs to be extensively utilized in agriculture, medical, rescue
    missions, smart cities, and intelligent transportation systems. Machine learning
    (ML) has been increasingly demonstrating its capability of improving the automation
    and operation precision of UAVs and many UAV-assisted applications, such as communications,
    sensing, and data collection. The ongoing amalgamation of UAV and ML techniques
    is creating a significant synergy and empowering UAVs with unprecedented intelligence
    and autonomy. This survey aims to provide a timely and comprehensive overview
    of ML techniques used in UAV operations and communications and identify the potential
    growth areas and research gaps. We emphasize the four key components of UAV operations
    and communications to which ML can significantly contribute, namely, perception
    and feature extraction, feature interpretation and regeneration, trajectory and
    mission planning, and aerodynamic control and operation. We classify the latest
    popular ML tools based on their applications to the four components and conduct
    gap analyses. This survey also takes a step forward by pointing out significant
    challenges in the upcoming realm of ML-aided automated UAV operations and communications.
    It is revealed that different ML techniques dominate the applications to the four
    key modules of UAV operations and communications. While there is an increasing
    trend of cross-module designs, little effort has been devoted to an end-to-end
    ML framework, from perception and feature extraction to aerodynamic control and
    operation. It is also unveiled that the reliability and trust of ML in UAV operations
    and applications require significant attention before full automation of UAVs
    and potential cooperation between UAVs and humans come to fruit... (Show More)
    Published in: IEEE Communications Surveys & Tutorials ( Volume: 26, Issue: 1,
    Firstquarter 2024) Page(s): 496 - 533 Date of Publication: 11 September 2023 ISSN
    Information: DOI: 10.1109/COMST.2023.3312221 Publisher: IEEE Funding Agency: I.
    Introduction With their excellent mobility, versatility, and ability to cover
    wide and harsh environments, unmanned aerial vehicles (UAVs) have been increasingly
    proliferating with extensive applications, as shown in Fig. 1. The Global UAV
    Market has been projected to grow at a cumulative rate of 19.9%, and generate
    a revenue of 55.649 billion from 2020 to 2027 [1]. In the past, UAVs were primarily
    used for surveillance and reconnaissance in military applications [2], [3], [4].
    With the new trends in aerial photography and monitoring over the past decade
    [5], UAVs have started to enable many civil and commercial application domains.
    For example, UAVs have been increasingly implemented in several monitoring domains
    [6], such as marine [7], [8], traffic [9], [10], public safety [11], [12], [13],
    and agriculture [14]. UAVs are also extensively considered to extend the connectivity
    and coverage of terrestrial communications systems, for example, mobile cellular
    systems. They can serve as aerial cellular base stations (BSs) [15], [16] or mobile
    repeaters and transponders [17], [18] with radio transceivers to offer connectivity
    and data services to users on the ground [19], or deliver confidential messages
    [20]. Fig. 1. The paradigms of UAV-assisted IoT that demand variable quality-of-services
    ranging from guaranteed reliability, minimal latency, the freshness of information,
    and energy efficiency. Sign in to Continue Reading Authors Figures References
    Citations Keywords Metrics More Like This UAV-Assisted Data Collection for Internet
    of Things: A Survey IEEE Internet of Things Journal Published: 2022 A Survey and
    Tutorial of EEG-Based Brain Monitoring for Driver State Analysis IEEE/CAA Journal
    of Automatica Sinica Published: 2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Communications surveys and tutorials
  limitations: '>'
  pdf_link: null
  publication_year: 2024
  relevance_score1: 0
  relevance_score2: 0
  title: 'Machine Learning-Aided Operations and Communications of Unmanned Aerial
    Vehicles: A Contemporary Survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/syscon.2017.7934725
  analysis: '>'
  authors:
  - André Pierre Mattei
  - Luis Loures
  - Pierre de Saqui‐Sannes
  - Bénédicte Escudier
  citation_count: 1
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2017 Annual IEEE Internationa... Feasibility study
    of a multispectral camera with automatic processing onboard a 27U satellite using
    model based space system engineering Publisher: IEEE Cite This PDF André Pierre
    Mattei; Luis Loures; Pierre de Saqui-Sannes; Bénédicte Escudier All Authors 1
    Cites in Paper 185 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Methodology IV. SysML and TTool V. Phase 0 Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: The paper discusses
    an experience in using SysML and the TTool software for the feasibility study
    of a novel multispectral camera for agricultural monitoring. Innovation lies in
    both automatic image processing onboard and mission control capabilities designed
    to comply with a 27U microsatellite. In addition to the mission accomplishment
    control, this innovative payload is capable of sending processed data directly
    to farms, critically reducing the delay between image making and its use in the
    field. This paper shows how MBSE and SysML may comply with phases 0 and A of a
    space project. Published in: 2017 Annual IEEE International Systems Conference
    (SysCon) Date of Conference: 24-27 April 2017 Date Added to IEEE Xplore: 29 May
    2017 ISBN Information: Electronic ISSN: 2472-9647 DOI: 10.1109/SYSCON.2017.7934725
    Publisher: IEEE Conference Location: Montreal, QC, Canada SECTION I. Introduction
    Agriculture field conditions are dynamic and may change faster than the time necessary
    to imagery collection, processing, and product delivery to the farmer. Currently,
    raw images are downloaded from satellites to ground stations where they are processed
    in order to respond to specific services based on customer requests. This time
    delay may be not acceptable to satisfy the needs of several end users. Moreover,
    the presence of clouds between the satellite and the area of interest may prevent
    the satellite from the collection of valid data. The project underlying the work
    presented in the paper aims to develop a novel payload capable of both controlling
    mission accomplishment and performing real time image processing. Instead of relying
    on the ground to acquire the intelligence needed for land management, farmers
    will have direct access to an almost real-time information to manage their property,
    as in Fig. 1. This figure shows the satellite collecting data for transmission
    to both a control station and a receiving station close to the farms. The project
    has started with technical feasibility study of an innovative multispectral camera
    with automatic onboard processing, using Brazil as target and partially complying
    with phases 0 and A of a space project [1]. The camera is assumed onboard of a
    27U satellite and incorporating those necessary elements for image processing,
    mission management, and data management (storing and transmission). The new architecture
    allows the payload to manage mission accomplishment by controlling the payload
    subsystems and sending directives to satellite subsystems. Fig. 1. After both
    gathering and processing data, the satellite sends mapsto receiving stations on
    either farms or control centers. Show All The feasibility study of this agricultural
    monitoring satellite has been supported by a Model-Based System Engineering approach
    that uses SysML [2] as modeling language and the free software TTool [3] for model
    edition, simulation, and verification. The paper is organized as follows. Section
    II surveys related work. Section III introduces the methodology used during the
    feasibility study. Section IV presents the SysML diagrams supported by TTool,
    as well as the simulation and verification capabilities it offers. Section V and
    Section VI highlight the main results obtained during Phase O and Phase A, respectively.
    Section VII concludes the paper. SECTION II. Related Work A. Technological Solutions
    for Satellites Precision agriculture uses several image-processing techniques
    to improve the efficiency of normal activities and corrective actions performed
    by farmers [4] [5]: image acquisition, pre-processing, segmentation, object detection,
    and classification. Image acquisition is performed by a platform distant of the
    field of interest, such as an airplane or a satellite [6], and may use different
    cameras, such as multispectral [7] [8], hyperspectral [9], or radar [10]. A thematic
    map highlights information related to a specific crop and geographic area, [4].
    These data may result in the correct placement and in the correct amount of agricultural
    inputs (pesticides and nutrients) that shall be used by the farmer, [11]. Nonetheless,
    as emphasized by Oštir et al. [12], the time necessary for image processing is
    currently the main obstacle for a faster cycle (meaning better management) and
    a way to speed up the process is to make it as automated as possible. Several
    projects demonstrate that onboard data processing allows both a faster cycle for
    information generation and mission control. An automated monitoring system using
    a multispectral imaging device for precision agriculture is presented in [13].
    Onboard image processing was used by [14] to detect and track objects on the ocean
    surface. In [15], it is found a real-time system for weed discrimination using
    a multispectral camera. The onboard data processing of a hyperspectral camera
    has also been studied by several groups to facilitate the transmission of data
    to the ground [16] [17]. In [18], the mission system recognizes the presence of
    clouds in the pictures taken by a cubesat for prioritizing data downlink. The
    reference [19] developed an electronic card using a FPGA Virtex7 for an earth
    observation satellite onboard data treatment. B. Use of MBSSE and SysML for Satellite
    Design INCOSE (The International Council on Systems Engineering) defines Model-Based
    Systems Engineering (MBSE) as: “the formalized application of modeling to support
    system requirements, design, analysis, verification and validation activities
    beginning in the conceptual design phase and continuing throughout development
    and later life cycle phases” [20]. A model-based methodology defines what, how,
    and the tools by using a model-centric approach design. The authors have adopted
    Model-Based methodology to support the Space System Engineering (MBSSE) for the
    project, since it is well fitted for phases 0, A, and B of the system life cycle,
    [1] [21]. Model-Based Systems Engineering (MBSE) has advantages when compared
    to the document-based approach because of its intrinsic enhanced communication
    and more efficient knowledge management when dealing through project phases and
    subsystems. According to [22], MBSE is “the formalized application of modelling
    to support system requirements, design, analysis, verification, and validation
    activities beginning in the conceptual design phase and continuing throughout
    development and later life cycle phases”. In the work of Kaslow et al. [23] SysML
    is used to support the development of a cubesat using MBSE. SECTION III. Methodology
    This project adopts a Model-Based methodology to support Model Based Space System
    Engineering (MBSSE) using SysML as language for system definition, [24]. A methodology
    is defined by [25] as “a collection of related processes, methods, and tools”.
    Phases and milestones are used as defined by [1], and this project addresses phase
    0, mission analysis/needs identification, and phase A, feasibility. Fig. 2. MBSSE
    methodology applied for phases 0 and A, [21]. Show All The MBSSE is the choice
    for this project since facilitates application of concurrent engineering in the
    early phases of the space system life cycle, 0, A, and B, [1]. Phase 0 refers
    to mission analysis and needs identification. Phase A is a feasibility study containing
    possible system concepts and assess its technical and programmatic aspects. Phase
    B establishes a preliminary design definition by confirming the technical solutions
    using trade-off studies for the selected system concept. Activities performed
    during this project follow the general schema presented in Fig. 2, [24]. The process
    adopted for this work is organized as follows: Mission Requirements Definition
    (phase 0); Requirements Analysis (phase A); and Architectural Design and Review
    Activities (partially accomplished in phase A). These processes shall be employed
    repeatedly during project phases. Mission requirements and system constraints
    are initially considered as a starting point, 0.1, and included as Requirement
    Diagrams (RD) and Modeling Assumptions Diagrams (MAD). In 0.1, possible mission
    objectives are identified and mission statement presented. In 0.2, Use-Case Diagrams
    (UCD), Sequence Diagrams (SD), and Activity Diagrams (AD) are engendered for better
    understanding of the actions, goals, and interactions during different activities
    performed by the satellite. During this phase, a number of tools besides SysML
    diagrams may be used in order to help in a first mission analysis, such as software
    Matlab, STK, and CNES Celestlab. In 0.3, blocks are created to provide a first
    approach for system design and requirements used to define some possible orbits.
    In A.1, system requirements are derived from 0.1 to provide more detailed RD and
    MAD diagrams. In A.2, analysis uses more detailed system requirements. In 0.2,
    mission analysis uses SysML diagrams and other software, such as Matlab and STK
    [26]. In A.3, a Block Instance Diagram (BID) describes a system architecture and
    State Machine Diagrams (SMD) give each block instance a behavior whose correctness
    is checked using simulations. SysML is used as modeling language to describe processes
    from requirements definition to architecture design and verification. SysML is
    indicated for space systems development by both INCOSE, [20], and NASA, [27].
    Other modeling applications are used in the project in specific fields (Matlab,
    STK, Scade Suite etc.). SECTION IV. SysML and TTool TTool supports a customized
    version of SysML designed with real-time system design in mind, and a 3-step process.
    Fig. 3. System use case. Show All A. Requirement Capture During the requirement
    capture phase, requirement diagrams (RD) express requirements, refinements between
    pairs of requirements, derivation of technical requirements from the set of requirements.
    Assuming that a model abstracts a real system, that model is valid under a precise
    set of assumptions. TTool invites you to make these assumptions an explicit part
    of the SysML model. A modeling assumption diagram (MAD, not offered by the OMG-based
    SysML) enables expression of the modeling assumptions associated with the system
    and its environment. B. Analysis A use-case diagram (UCD) identifies the main
    functions to be offered by the system, the relations between pairs of functions,
    and the interactions between the system and its environment. Use-cases are documented
    by scenarios (sequence diagrams) and flow charts (activity diagrams). C. Design
    (Including Simulation and Verification) The design step defines the architecture
    of the system in the form of a block instance diagram, and assigns each block
    instance a behavior expressed by a state machine diagram. Design diagrams have
    a formal semantics, making them executable by the TTool''s simulator. TTool further
    implements a press-button approach to offer verification capabilities (model checking
    and abstractions) by reasoning on the SysML model without writing a piece of formal
    code. Model checking decides whether a state or an action in the model is reachable
    or not. Abstraction reduces the labeled reachability graph of the SysML Model
    to a quotient automaton that provides the model designer with events of interest.
    SECTION V. Phase 0 0.1: Requirement Capture Requirements that define the mission
    and preliminary aspects related to both satellite and payload are expressed in
    the form of SysML Requirement Diagrams (RD), not presented here due to their size;
    they would be unreadable. The Modeling Assumption Diagram created for camera channels
    captures a set of assumptions and attributes important for shaping the camera
    as real-time system. 0.2: Analysis Requirements demands updated information in
    less than 15 days'' period necessary for crop management and agriculture production
    increase. The UCD in Fig. 3 depicts payload functions and relations with ground
    station, farms, and satellite. The ≪include≫ function expresses a mandatory inclusion.
    0.3: Design Taking into account requirements and analysis, Phase 0 identifies
    a first approach for both mission and orbit design. A. Mission Since mission statement
    establishes an onboard image processing for agriculture, some potential indices
    are identified. These indices allow the identification of suitable processing
    techniques in later phases. For predicting yield: Normalized Difference Vegetation
    Index (NDVI), Green Vegetation Index (GVI), and Soil-Adjusted Vegetation Index
    (SAVI). [28] GVI= SAVI= NDVI=( ρ nir − ρ red )/( ρ nir + ρ red ) ( ρ nir − ρ green
    )/( ρ nir + ρ green ) [( ρ nir − ρ red )/( ρ nir + ρ red +L)]. (1+L) (1) (2) (3)
    View Source where ρ nir , ρ red , and ρ green are spectral reflectance for near
    infrared, red, and green wavelengths. L is a correction factor and its value is
    dependent on the vegetation cover and a value L=0.5 is suggested by [28] to minimize
    the effect of soil variations in green vegetation compared to Normalized Difference
    Vegetation Index (NDVI) and represents intermediate vegetation cover (0.25 for
    high and 1.0 for low density vegetation). For three decades, NDVI has been used
    to estimate vegetation water content (VWC) with limited success. The limitation
    is due to NDVI saturation when vegetation coverage is dense. SWIR in 1640nm and
    2130nm were used in the Normalized Difference Water Index, NDWI, with good results
    for corn: NDWI=( ρ nir − ρ swir )/( ρ nir + ρ swir ), (4) View Source where ρ
    swir are spectral reflectance in the SWIR band. Table I. Wavelengths selected
    for the multispectral camera. Taking into account the selected indices, Table
    I presents the channels and corresponding wavelengths considered through this
    work. B. Satellite Bus The platform choice is a 27U microsatellite, 54kg of total
    mass. This satellite type allows the use of off the shelf components and has a
    standard launching system called “PPOD” which makes it faster and cheaper the
    development of the service module. During operation, the satellite will be able
    to supply energy to enable the proper functioning of onboard equipment. This is
    normally achieved by the solar panels except when passing in the shadow of the
    earth. During these periods of eclipse, the solar panels are not illuminated and
    therefore cannot supply energy, that''s when the batteries take over. The estimated
    power budget is less than 70W, including a 20% margin, and this value was used
    for sizing the batteries. C. Orbit An orbit is characterized by its six orbital
    parameters, or Keplerian: Semi-major axis/elevation: Imposed by requirements,
    the altitude must be between 500 and 750 km; Eccentricity: zero (circular orbit);
    Tilt: to cover all of Brazil, orbital inclination of 33°; Longitude of ascending
    node: considered a longitude of the ascending node of −44.39° (Alcantara Launch
    Center); Argument of periapsis: calculated from the semi-major axis and tilt;
    Mean anomaly: it is equal to the true anomaly within the circular orbit; the initial
    value considered in the calculations is zero. This section made use of the software
    CNES Celestlab and STK. The orbit analysis has taken into account mission needs
    as presented in requirements and not launcher availability. Using orbital parameters
    considered above, an orbit phase diagram was developed for presenting the duration
    of revisits according to the altitude of 128 possible orbits. An orbit is phased
    when the satellite passes exactly over the same track after a number of revolutions.
    Fig. 4. Average visibility per day (minutes). Show All Fig. 5. Intertrace cycle
    according to the duration of revisits. Show All The ground stations used as references
    are in the Brazilian cities of Natal, Cuiabá, Campinas, and São Jose dos Campos.
    Visibility cones with an elevation of 5 degrees are considered to obtain the average
    visibility duration on a day in minutes as function of the orbit altitude (Fig.
    4). The figure shows that the average duration of daily exposure increases with
    the altitude. The intertrace cycle is defined as the distance, at the equator,
    between two consecutive traces in space (not in time). Intertrace shall be considered
    along with swath since if the second is smaller than the first one, the satellite
    will be forced to change its pointing direction and show deflection capability.
    Deflection is defined as the satellite rotation angle needed to cover the entire
    area of the intertrace and swath is defined as the distance on earth corresponding
    to the maximum width of an image. As a general rule, smaller revisit time demands
    larger intertrace, as shown in Fig. 5, and this will force larger swath and eventually
    also larger deflections (5), which can potentially make it harder onboard processing.
    The satellite deflection (α) depends on the satellite altitude (h) , intertrace
    cycle ( I c ) , and the opening angle of the optical instrument ( FOV , Field
    Of View), as be seen in (5). α= tan −1 ( I c /2h)−(FOV/2) (5) View Source Most
    suitable orbits to this mission were chosen using multicriteria optimization with
    Visual Prometheus software [29]. Criteria are based on the calculation of aggregate
    preference indexes π to express the degree of preference between two alternatives
    orbits considering the decision criteria. These indices are calculated from weight
    ϖ i associated with each criterion c i and intensity functions P i , dependent
    on the considered alternatives. Thus, considering a number N of criteria and a
    pair of alternatives (a, b) among M alternatives, the index of aggregate preference
    is given by: π(a,b)= ∑ N i ϖ i . P i (a,b), (6) View Source where π(a,b) is a
    real between 0 and 1. The closer π(a, b) is to the unit, the stronger is the preference
    of a over b . The calculation of this index for all pairs of alternatives enables
    a matrix of preference. Decision criteria were: Altitude (weight of 15%); Cycle
    time (weight of 32.5%); Average visibility (weight of 20%); Deflection (weight
    of 32.5%). Orbits were selected with this method. With a swath of 100km and altitude
    of 625km, one may conclude from Fig. 6 that the designer shall consider both FOV
    and swath for complying with 15 days revisit time requirement. Fig. 6. Brazil
    coverage assessment using STK. Show All SECTION VI. Phase A A.1: Analysis A. Satellite
    Bus As usual, solar panels and batteries (during shadow periods) supply the necessary
    power. The power that can be generated through photovoltaic cells P avlb is related
    to the angle θ between the normal to the solar panels and the direction of the
    sun. The estimated number of cells was 116 in the lateral faces, resulting in
    3.3kg of total mass and 107W of maximum power. Due to low orbit inclination, the
    minimum value of the average daily duration of eclipse being 32 minutes and the
    maximum is 35 minutes (illumination about 60min). The total work capacity of the
    batteries shall be equal to: C mission =(35/60). P max =0.58. P max ≅41W (7) View
    Source For a 3-year mission, there are about 17,000 charge / discharge cycles,
    with 30% depth of discharge. In this case, the batteries only provide a maximum
    of 3W and it is possible to preserve the battery and to reach 35,000 of maximum
    number of cycles, thus it is necessary to have at least 14 batteries to provide
    41W. Taking into account the necessary energy for systems, the power solar panels
    is not sufficient in early life and situation will degraded during operation,
    it is then recommended the use of at least one deployable panel. For communication
    network, it was considered two solutions related to the physical layer, Controller
    Area Network Bus (CAN) and Spacewire. CAN bus is considered a good option taking
    into account requirements and its lower cost in comparison to Spacewire. S band
    (2-4GHz) is the option for telecommand and telemetry and X band (8-12GHz) is the
    option for downloading images. Considering the resolutions of 5m and 30m for approximately
    2,500 pictures, and each photo having 250MB and 25MB, respectively, there would
    be an amount of 625 GB and 62.5 GB of images. With an average time of visibility
    of 120 minutes per day, it is necessary to determine the required transmission
    rate T x rate to send the stored data (Im) during the time available (visibility
    time, V time ) is T x rate =Im/ V time . Fig. 7. General view of system context,
    including payloadmodules and some platform subsystems. Show All Using these data
    would require a transponder with capacity of 87Mbps and 8.7Mbps for ground resolutions
    of 5 and 30m, respectively. Using a component available in the cubesat market,
    such as EWC 27 HDR-TM [30], the available capacity is 50Mbps. Using this transponder
    for higher spatial resolutions, it is possible to send approximately 57% of surveilled
    area. Taking into account the particular interest in crop areas, the data rate
    is considered sufficient. B. Payload Fig. 7 depicts a context diagram using the
    syntax of block diagrams for payload modules/functions. A black diamond denotes
    a “is made up” relation. Optical collects the light flux from the ground through
    a telescope; Image processing reads and processes data generated by Optical; Mission
    control is responsible for controlling mission accomplishment and payload modules;
    Interface exchanges data with the satellite onboard computer and distributes them
    through payload modules; Memory stores data generated by the Image processing
    module; Transmitter sends data directly to the ground. Optical Module Fig. 8 presents
    some key figures used for developing the imager. A represents the area acquired
    by the imager and it depends on the detector configuration, f is the focal distance,
    D the aperture diameter, and h the satellite altitude. Each detector acquires
    image with size R (ground resolution) and detector element angle θ r , solid angle
    ω d , and field of view (FOV) θ . Using these basic figures, some parameters regarding
    both detector and optical system telescope may be determined. [31] Usually, imaging
    systems are separated into three categories: whiskbroom, pushbroom, and staring.
    Taking into account that onboard automatic image processing is challenging for
    the payload, a staring imager is a better choice for this project since it decreases
    both stability and vibration demands for the satellite control system. [32] The
    speed of the satellite ( V sat ) can be calculated with a certain degree of accuracy
    from the circular orbit altitude. Let V im be the satellite speed in relation
    to the earth surface, then taking into account satellite orbit (500-750km), its
    speed is: 7.5 km/s≤ V sat ≤7.6 km/s and V im is 6.7 km/h≤ V im ≤7.1 km/h Fig.
    8. General parameters used for imager development. Show All Fig. 9. Swath as a
    function of focal length f and altitude h for both VNIR and SWIR. Show All The
    most common telescopes are: Catadioptric, Three-Mirror Anastigmat (TMA), and Korsch,
    [33]. In the light of this comparison, both Korsch and Catadioptric telescopes
    types are considered for analysis at phase B. For the telescope, main figures
    are: f and D . The ground resolution considered is 5-10m for the VNIR and the
    area of interest is Brazil, and the revisit time shall be lower than 15 days.
    Using the required ground resolution in the VNIR range (5-10m) and 10μm pixel
    size for VNIR range: 0.5m≤f≤1.5m . The instantaneous swath (P) may be found in
    (19) and is presented in Fig. 9 as a function of f,h , and detector characteristics.
    It may be seen that greater swaths are found in the VNIR range due to the larger
    number of elements in the detector, 9216×9232 against 2048×2048 pixels for SWIR''s.
    The swath values found are not sufficient for achieving the requirement for revisit
    time even for VNIR range and this result may be considered for either relax requirement
    for revisit time or include attitude agility for the satellite. The CCD parameters
    also determine the system field of view 2θ . In order to avoid image distortions,
    during the time necessary to acquire each image (exposure time T E ), satellite
    movement should not be more than the projection a photosensitive element onto
    the region along flight speed direction. The exposure time T E ≤2.5s for VNIR
    ( R vnir ≥5m) and T E ≤3.5s for the case of SWIR (minimum R swir ≥7m ), ( V im
    =1.97m/s) . For the telescope, it was considered as cylindrical with volume [(f/2).
    D 2 ] , with the combination of at least two mirrors. Furthermore, using data
    provided by the CNES, density for mirrors 50kg/m2 and aluminum protection 3kg/m2,
    estimated mass is 0.95kg. i. Image Processing Analysis and Mission Control It
    is considered that each pixel will be encoded in 8 bits. Consistently with the
    payload, the image processing software will receive 6 images from the different
    camera channels. The Mission control proposed for the payload is a software architecture
    that provides the autonomy necessary to make the platform more capable when managing
    its mission. The new architecture proposed allows the payload to control the mission
    accomplishment management by controlling the payload subsystems and sending directives
    to satellite subsystems in a three-layer architecture, [34]. Taking into account
    the mission goals, a planner schedules activities and send orders to a robust
    execution software, Fig. 10. The robust execution software is responsible for
    optical and processing setup as well as monitor orders execution. Depending on
    the crop, region, and period of the year, different settings may be necessary
    for the optics and image processing. Planner may take many minutes to process
    all parameters while the execution software shall perform in seconds to create
    orders based on planner decisions. ii. Memory Considering the chosen components,
    VNIR detectors generate 681Mb for each picture and SWIR 33Mb. As an example, with
    a swath of 100km in VNIR range, 852 pictures may cover the entire country with
    580Gb per band. With a swath of 40km in SWIR range, 5,323 pictures may be necessary
    to cover the entire country, meaning 176Gb per band. The total amount is 2.7Tb,
    and using lossless data compression of 50% would lead the value for 1.4Tb. If
    after processing image size decreases another 50%, it is necessary a capacity
    of 668Gb during the revisit time. Concluding a 1Tb solid state memory is enough
    to store all processed images even considering the whole country as target. Fig.
    10. Payload software architecture. Show All Fig. 11. TTool simulation over the
    system services state machine. Show All A.2: Modeling with SysML and TTool Due
    to space constraints, the use-case diagrams of the payload are not depicted here,
    but were considered for the services provided by the satellite. Whether a Block
    Instance Diagram usually depicts the architecture of the real system, it may also
    express a composition of elementary services to be provided by the system. A service
    is behaviorally described by a state machine, as the one depicted by Fig. 11.
    Fig. 11 depicts how the state machine diagram is associated with the services
    provided by the system. In this figure, messages are numbered using three letters
    and three numbers. Letters refer to the service provided and numbers start with
    000 and increase sequentially as messages flow between systems, ending with a
    final message numbered XXX. TEL refers to telemetry, TXG to transmission to ground
    station, TXF to transmission to a farm, SET is used when payload setup is changed
    by the ground station, POS and ATT is a service provided by the satellite systems
    for informing current GPS position and set the platform attitude to perform imagery,
    and MAK refers to those messages sent when making and processing imagery before
    storing them in the memory. Simulation has enabled early debugging of the model.
    Fig. 11 shows (green arrow) how the simulator shows where the simulation has been
    stopped. Simulations also allow checking of details of the state machine, as a
    simulation trace may be seen in Erro! Fonte de referência não encontrada. In this
    figure, only messages and states involved in the telemetry service are presented.
    For telemetry, the ground station is the actor responsible for starting the service
    and for simulation purposes; it is inside the element called “Services Controller”.
    Fig. 12. Simulation trace output by TTool. Show All SECTION VII. Conclusions Satellites
    have played an increasing and acknowledged role monitoring agriculture, particularly
    in Brazil. The paper discusses an experience in using SysML and TTool for a feasibility
    study of a new payload aiming to decrease the time between information gathering
    and actions on the ground. A novelty proposed is a multispectral camera payload
    capable of processing images and manage mission accomplishment automatically.
    Data obtained in this work provided the necessary information for the ongoing
    phase B. Next steps include the development and field test of both image processor
    and mission manager using manned aircraft and commercial multispectral cameras,
    before integration in the final destination. ACKNOWLEDGMENT TTool has been developed
    by Dr Ludovic Apvrille. Contributions from ISAE-SUPAERO and ITA students are acknowledged.
    Acknowledgements are also due to Brazilian Space Agency (AEB), Department of Aerospace
    Science and Technology of the Brazilian Air Force (DCTA), and the Thales-Alenia
    Space (TAS) company. Authors Figures References Citations Keywords Metrics More
    Like This An integral time calculation model for agile satellite TDICCD camera
    2016 IEEE 13th International Conference on Signal Processing (ICSP) Published:
    2016 An analytical echo model for satellite ISAR imaging based on the Kepler orbit
    2016 CIE International Conference on Radar (RADAR) Published: 2016 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: 2017 Annual IEEE International Systems Conference (SysCon)
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Feasibility study of a multispectral camera with automatic processing onboard
    a 27U satellite using model based space system engineering
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11240-023-02528-0
  analysis: '>'
  authors:
  - Hans Bethge
  - Zahra Mohammadi Nakhjiri
  - Thomas Rath
  - Traud Winkelmann
  citation_count: 2
  full_citation: '>'
  full_text: ">\nVol.:(0123456789)\n1 3\nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573 \nhttps://doi.org/10.1007/s11240-023-02528-0\nORIGINAL ARTICLE\n\
    Towards automated detection of hyperhydricity in plant in vitro \nculture\nHans Bethge1,2\
    \  · Zahra Mohammadi Nakhjiri2 · Thomas Rath1  · Traud Winkelmann2 \nReceived:\
    \ 24 March 2023 / Accepted: 12 May 2023 / Published online: 7 June 2023 \n© The\
    \ Author(s) 2023\nAbstract\nHyperhydricity (HH) is one of the most important physiological\
    \ disorders that negatively affects various plant tissue culture \ntechniques.\
    \ The objective of this study was to characterize optical features to allow an\
    \ automated detection of HH. For this \npurpose, HH was induced in two plant species,\
    \ apple and Arabidopsis thaliana, and the severity was quantified based on \n\
    visual scoring and determination of apoplastic liquid volume. The comparison between\
    \ the HH score and the apoplastic liq-\nuid volume revealed a significant correlation,\
    \ but different response dynamics. Corresponding leaf reflectance spectra were\
    \ \ncollected and different approaches of spectral analyses were evaluated for\
    \ their ability to identify HH-specific wavelengths. \nStatistical analysis of\
    \ raw spectra showed significantly lower reflection of hyperhydric leaves in the\
    \ VIS, NIR and SWIR \nregion. Application of the continuum removal hull method\
    \ to raw spectra identified HH-specific absorption features over \ntime and major\
    \ absorption peaks at 980 nm, 1150 nm, 1400 nm, 1520 nm, 1780 nm and 1930 nm for\
    \ the various conducted \nexperiments. Machine learning (ML) model spot checking\
    \ specified the support vector machine to be most suited for classifi-\ncation\
    \ of hyperhydric explants, with a test accuracy of 85% outperforming traditional\
    \ classification via vegetation index with \n63% test accuracy and the other ML\
    \ models tested. Investigations on the predictor importance revealed 1950 nm,\
    \ 1445 nm \nin SWIR region and 415 nm in the VIS region to be most important for\
    \ classification. The validity of the developed spectral \nclassifier was tested\
    \ on an available hyperspectral image acquisition in the SWIR-region.\nKey message\
    \ \nThis study provides an approach that paves the way to automatic detection\
    \ of hyperhydricity by identifying the key spectral \nfeatures of this phenomenon.\n\
    Keywords Hyperhydricity · Spectral analysis · Phenotyping · Machine learning ·\
    \ Automated object detection\nAbbreviations\nHH \n Hyperhydricity\nML \n Machine\
    \ learning\nUV \n Ultra violet\nVIS \n Visible radiation\nNIR \n Near infrared\
    \ radiation\nSWIR \n Shortwave infrared radiation\nMWIR  Mid-wave infrared radiation\n\
    LWIR \n Longwave infrared radiation\nDAT \n Days after treatment/transfer\nCV\
    \ \n Cross validation\nCNN \n Convolutional neuronal network\nHSI \n Hyperspectral\
    \ imaging\nIntroduction\nHyperhydricity (HH) represents one of the major chal-\n\
    lenges for increasing the efficiency of plant in  vitro \npropagation as it limits\
    \ plant quality, adventitious root \nformation and ex vitro survival rate, in\
    \ particular when \nCommunicated by Victor M. Jimenez.\n * Hans Bethge \n \nbethge@baum.uni-hannover.de\n\
    1 \nLaboratory for Biosystems Engineering, Faculty \nof Agricultural Sciences\
    \ and Landscape Architecture, \nOsnabrück University of Applied Sciences, Oldenburger\
    \ \nLandstraße 24, 49090 Osnabrück, Germany\n2 \nInstitute of Horticultural Production\
    \ Systems, Section \nof Woody Plant and Propagation Physiology, Leibniz \nUniversität\
    \ Hannover, Herrenhäuser Str. 2, 30419 Hannover, \nGermany\n552\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nusing liquid culture\
    \ or bioreactor systems (Cardoso et al. \n2018; Debergh et al. 1992; Gribble 1999).\
    \ According to \nKemat et al. (2020), at least 200 species are sensitive to \n\
    HH and around 150 species can be affected seriously by \nHH emphasizing the relevance\
    \ for commercial micropro-\npagation. HH not only restricts the propagation of\
    \ in vitro \nplants, but also affects the efficiency of genetic transforma-\n\
    tion mediated by Agrobacterium (van Altvorst et al. 1996) \nand the conservation\
    \ of important species in germplasm \nbanks (Lizárraga et al. 2017).\nHH is a\
    \ physiological disorder occurring under the spe-\ncific conditions of plant tissue\
    \ culture such as high humid-\nity, high supplementation of sucrose, impaired\
    \ gaseous \nexchange capacity and consequently low photosynthetic \nactivity (George\
    \ et al. 2008; Ziv 1991). The work of van \nden Dries et al. (2013) and Rojas-Martínez\
    \ et al. (2010) \nprovided strong evidence that the underlying mechanism of \n\
    the HH etiology is the flooding of the apoplast, resulting \nin hypoxia and causing\
    \ oxidative stress. This in turn leads \nto the macroscopic symptoms of water-soaked,\
    \ wrinkled, \ncurled, brittle and translucent tissue. The occurrence of HH \n\
    was shown to be increased when the water availability for \nthe in vitro explant\
    \ was increased (Smith and Spomer 1995), \ne.g., by reduced concentration of the\
    \ gelling agent (Ivanova \nand Van Staden 2011), or by the type of the gelling\
    \ agent \nused (Pasqualetto et al. 1988; Tsay et al. 2006). The gelling \nagent\
    \ gelrite induced HH in a wide range of plant genera \n(e.g., Arabidopsis sp.,\
    \ van den Dries et al. 2013, Malus sp. \nPasqualetto et al. 1988, Prunus sp. Franck\
    \ et al. 1998), even \nthough the same gel strength as agar was used.\nIn addition\
    \ to the anatomical changes of hyperhydric tis-\nsue which include larger intercellular\
    \ spaces in the meso-\nphyll and a drastically reduced number of palisade cells\
    \ \n(Vieitez et al. 1985), several biochemical changes of hype-\nrhydric tissue\
    \ such as decreased chlorophyll contents (Phan \nand Letouze 1983; Franck et al.\
    \ 1998), hypolignification \n(Kevers et al. 1987; Kemat et al. 2021), and high\
    \ apoplastic \nwater volume (Dries et al. 2013; Tian et al. 2015; de Klerk \n\
    and Pramanik 2017) were reported. Paques et al. (1985) \nrefer to HH as an inducible\
    \ and reversible phenomenon and \ndemonstrated that Malus sp. ‘M26’ plantlets\
    \ could return \nto non-hyperhydric state if the induction phase in liquid \n\
    culture did not exceed five days or if the symptoms of HH \nwere not too severe.\
    \ Recently, there were reports that hype-\nrhydricity can be reversed by supplementation\
    \ of agents to \nmedia such as silver nitrate and trichloroacetate (Gao et al.\
    \ \n2017; de Klerk and Pramanik 2017) or by controlling the \nenvironmental conditions\
    \ in addition to media optimization \n(Mohamed et al. 2023), but no general countermeasure\
    \ has \nbeen derived up to now. In commercial in vitro laboratories \nvisual monitoring\
    \ for contaminations and disorders are part \nof the routine work and therefore\
    \ a costly and time-consum-\ning repetitive matter (Mestre et al. 2017).\nNowadays,\
    \ digitalization enters the horticultural sector, \ndriven by digital solutions\
    \ to increasingly complex work \nprocesses achieved through technological advances\
    \ in sen-\nsors, automation and robotization, as well as data analysis \nthrough\
    \ classical and advanced machine learning (ML) \ntechniques. Automation of processes\
    \ offers great economic \npotential for micropropagation laboratories since 60–70%\
    \ \nof total costs of a micropropagated plant is due to manual \nlabor (Chen 2016).\
    \ An increasing number of reports on auto-\nmating micropropagation processes\
    \ such as explant cutting \n(Huang and Lee 2010), the commercial laser-based robotic\
    \ \ncut and transplanting system RoBo®Cut (Bock Biosciences \nGmbH 2018), monitoring\
    \ of cultures (Dhondt et al. 2014, \nBethge et al. 2023) and transplanting of\
    \ explants (Lee et al. \n2019) were published within the recent years. In addition,\
    \ \nthere are several studies on the application of computer \nvision to micropropagation\
    \ (Smith et al. 1989; Aynalem \net al. 2006; Dhondt et al. 2014; Gupta and Karmakar\
    \ 2017; \nMestre et al. 2017) with imaging sensors being the crucial \ntechnology.\
    \ Imaging sensors used in horticulture consist of \naffordable RGB cameras, multispectral\
    \ cameras, thermal \ncameras, expensive hyperspectral imaging (HSI) systems, \n\
    ToF (Time of Flight), LIDAR systems (Light Detection and \nRanging) and more.\
    \ The different sensor systems can be \ndiscriminated by their operating spectral\
    \ range (UV, VIS, \nNIR, SWIR, MWIR, LWIR/Thermal-IR), spectral resolu-\ntion\
    \ from one (monochrome) to > 100 (hyperspectral) chan-\nnels and cost of purchase.\
    \ For example, the price of silicon \n(Si)-based hyperspectral cameras rise by\
    \ a factor of 2 to 20 \nwhen switching the operating spectral range from VIS/NIR\
    \ \n(400–1000 nm) to SWIR (900–1700 nm) with an Indium-\nGalium-Asenide (InGaAs)\
    \ camera chip (Tisserand 2021). \nThis needs to be considered, when selecting\
    \ the appropriate \nspectral range and corresponding imaging technology. While\
    \ \ncomputer vision coupled with ML offers already great poten-\ntial to solve\
    \ complex detection task in agriculture (reviewed \nin Patrício and Rieder 2018),\
    \ for application in plant tissue \nculture only few reports are available up\
    \ to now (reviewed \nin Prasad and Gupta 2008; Hesami and Jones 2020). How-\n\
    ever, these are limited in terms of live-monitoring, since \nthey followed the\
    \ “object to sensor” approach for plantlet \nclustering (Mahendra et al. 2004),\
    \ classification of somatic \nembryos (Zhang et al. 1999) and estimation of shoot\
    \ length \n(Honda et al. 1997).\nThe visual appearance of plants, and in particular\
    \ leaf \npigments, can be estimated by spectroscopic approaches \nbased on their\
    \ interaction with electromagnetic radiation. \nSingle biochemical plant metabolites\
    \ can be associated with \nspecific wavelengths based on their major absorption\
    \ peaks \n(Table 1).\nUnivariate data analysis, e.g., spectral indices or multi-\n\
    variate data analyses like partial least square (PLS), allows \nthe prediction\
    \ of leaf pigments’ concentrations and can be \n553\nPlant Cell, Tissue and Organ\
    \ Culture (PCTOC) (2023) 154:551–573 \n1 3\nused for classification. These techniques\
    \ also enable the \ndiscrimination of different plant species or the identifica-\n\
    tion of growth anomalies by specific spectral features (Shaw \nand Kelley 2005).\
    \ According to Hesami and Jones (2020), \nML techniques applied to plant tissue\
    \ culture problems will \nhelp in future to solve classification and regression\
    \ problems \nand can be employed for automation and mechanization of \nin vitro\
    \ propagation, genetic engineering and genome edit-\ning technologies. In addition,\
    \ Nezami-Alanagh et al. (2019) \ndemonstrated the positive impact of ML models\
    \ in optimiz-\ning culture media in terms of time, cost and the occurrence \n\
    of physiological disorders in the propagation of pistachio \nrootstocks. Prasad\
    \ and Gupta (2008) proposed that an auto-\nmated decision-making system based\
    \ on computer vision \ncoupled with ML models and combined with a robotic sys-\n\
    tem will result in the mechanization of commercial mass \npropagation and help\
    \ in evaluating various aspects of plant \nquality such as HH status, which might\
    \ be difficult to deter-\nmine by human visual inspection. To our knowledge, the\
    \ \nspectral properties of HH have not yet been studied or used \nas a distinguishing\
    \ feature for ML classification of in vitro \ncultured explants.\nThe objective\
    \ of this study was to investigate the spectral \nfingerprints of hyperhydric\
    \ tissue in two different plant spe-\ncies (Malus sp. and Arabidopsis thaliana)\
    \ after forced induc-\ntion of the growth anomaly and subsequent spectral analysis\
    \ \nof the explants. Here, we selected Malus as a representa-\ntive of classical\
    \ in vitro shoot cultures and Arabidopsis as a \nmodel plant for the underlying\
    \ mechanism of HH. A novel \nphenotyping system was tested to monitor the morphological\
    \ \ncharacteristics of hyperhydric explants in time-series image \ndata. Furthermore,\
    \ we aimed at identifying specific absorp-\ntion features of hyperhydric tissues\
    \ that are sufficient for \ndiscrimination by ML techniques and to locate them\
    \ within \nin the electromagnetic radiation spectrum. Putative discrimi-\nnating\
    \ models should be validated and discussed in terms of \ntheir feasibility in\
    \ plant tissue culture. The findings of this \nstudy should pave the way for an\
    \ automatic detection of HH \nby live-monitoring of in vitro cultures.\nMaterial\
    \ and methods\nPlant material and experimental setup\nMorphological characteristics\
    \ of hyperhydricity\nFrom in vitro apple shoot cultures (Malus sp. ‘G214’) uni-\n\
    form shoots of 10–15 mm length were prepared and culti-\nvated on modified MS\
    \ medium (Murashige and Skoog 1962) \ncontaining 2.2 µM 6-benzylaminopurine (BAP),\
    \ 0.5 µM \nindole-3-butyric acid (IBA), 3% (w/v) sucrose and solidified \nwith\
    \ either 0.8% (w/v) agar (Plant agar, Duchefa, Haarlem, \nThe Netherlands) for\
    \ the control variant (“MS + agar”) or \nwith 0.25% (w/v) gelrite (Duchefa, Haarlem,\
    \ The Nether-\nlands) for the HH induction variant (“MS + gelrite”). The \npH\
    \ of the medium was adjusted to 5.8 prior to autoclaving \nat 121 °C for 15 min.\n\
    Arabidopsis thaliana ‘Col-0’ seeds which had been stored \nat 4 °C, were surface-disinfected\
    \ using 70% (v/v) isopro-\npanol for 30 s, followed by 2% (v/v) sodium hypochlorite\
    \ \nplus Tween 20 for 5 min and then rinsed thoroughly three \ntimes using sterile\
    \ deionized water. The seeds were germi-\nnated for 10 days at 24 °C in 9 cm-Petri\
    \ dishes (polysty-\nrene) on modified plant growth regulator-free B5 medium \n\
    (Gamborg et al. 1968), containing 1.5% (w/v) sucrose with \n0.8% (w/v) Plant agar\
    \ and pH 5.8. Uniform 10 day-old seed-\nlings were selected and five seedlings\
    \ per 500 mL-vessel \nwere transferred to modified plant growth regulator-free\
    \ \nB5 medium (Gamborg et al. 1968), containing 1.5% (w/v) \nsucrose and either\
    \ 0.8% (w/v) Plant agar for the control vari-\nant (“B5 + agar”) or 0.25% (w/v)\
    \ gelrite (“B5 + gelrite”) to \ninduce HH. The pH of the medium was adjusted to\
    \ 5.8 prior \nto autoclaving at 121 °C for 15 min.\nTen 500 mL polypropylene vessels\
    \ were prepared for \nExperiment I (Table 2) and Experiment II, each with four\
    \ \nplantlets and containing ~ 80 mL of one of the two dif-\nferent media (“B5/MS\
    \ + agar”/“B5/MS + gelrite” supple-\nmented with 1 g L-1 titanium dioxide). Titanium\
    \ dioxide \n(food dye; Ruth GmbH & Co.KG, Bochum, Germany) \nTable 1  Selected\
    \ reported symptoms of hyperhydric tissues (HH) and corresponding expected major\
    \ changes in optical absorbance features\n*Absorptions peaks according to Curran\
    \ (1989) in a wavelength range of 400 to 2000 nm. Bold wavelength indicating stronger\
    \ absorption of the \nrespective chemical compound\nReference\nPlant species\n\
    Observation\nDeduced optical absorbance \nfeatures in VIS-SWIR [nm]*\nPhan and\
    \ Letouze (1983)\nP. avium\nLower chlorophyll content in HH\n430, 460, 640, 660\n\
    Van den Dries et al. (2013)\nA. thaliana\nHigher apoplastic water volume in HH\n\
    970, 1200, 1400, 1450, 1940\nPhan and Letouze (1983)\nP. avium\nLess protein content\
    \ in HH\n910, 1020, 1510, 1940, 1980\nKemat et al. (2021)\nA. thaliana\nHypolignification\
    \ in HH\n1200, 1420, 1450, 1690, 1940\nSaher et al. (2005)\nD. caryophyllus\n\
    Higher sugar content in HH\n1450, 1490, 1580, 1780, 1960\nVan den Dries et al.\
    \ (2013)\nA. thaliana\nAnthocyanins accumulation in HH\n550\n554\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nwas used to add a\
    \ white color to the medium, because \nthis enabled the height measurements of\
    \ the robot system \ndue to increased reflection of the culture media. A plastic\
    \ \nfilm (PVC system foil; Klarsichtpackung GmbH, Hofheim, \nGermany) sealed the\
    \ containers as a substitution of the \nlid of the containers to provide a fully\
    \ transparent view \nwhile ensuring the aseptic condition of the cultures. These\
    \ \ncultures were cultivated at 22 °C with a 16 h photoperiod \nand under a PPFD\
    \ (Photosynthetic Photon Flux Density) \nof 35–40 μmolm−2s−1 , provided by two\
    \ tubular fluores-\ncent lamps (Philips MASTER TL-D 58W/865). The lab’s \nbottom-cooling\
    \ system—provided by water-cooled plastic \ntubes below the shelf—prevented water\
    \ condensation due \na local shift of dew point. Room temperature ranged from\
    \ \n19 (night) to 25 °C (day) with an average of 22 °C over \n24 h, while the\
    \ average surface temperature of the cooled \ncultivation area ranged from 19\
    \ (night) to 24 °C (day) \nwith an average of 21 °C over 24 h. In addition to\
    \ the \nnon-destructive monitoring approach (Exp. I & II), three \nexperiments\
    \ (Exp. III, Exp. IV, Exp. V; Table 2) were con-\nducted with different evaluation\
    \ time points. The evalua-\ntion time points were chosen based on the key events\
    \ in \nthe dynamic etiology of hyperhydricity during a culture \npassage (~ 4–5 weeks\
    \ for Malus). Important morphological \nchanges were observed during the first\
    \ two weeks, so Exp. \nIII and V covered this time span, while measurements in\
    \ \nExp. IV were undertaken to cover the second half of the \nculture passage.\n\
    Hyperhydricity induction\nFor Malus shoot cultures, 500 mL polypropylene containers\
    \ \ncontaining 80 mL of the two different media were used and \neach container\
    \ was inoculated with five shoots. Cultivation \ntook place for 20 days (Table 2;\
    \ Exp. III), 28 days (Exp. IV) \nand 16 days (Exp. V) at 22 °C (room temperature\
    \ ranged \nfrom 19 (night) to 25 °C (day) with an average of 22 °C \nover 24 h)\
    \ with a 16 h photoperiod and under a PPFD (Pho-\ntosynthetic Photon Flux Density)\
    \ of 35–40 µmol  m−2  s−1, \nprovided by tubular fluorescent lamps (Philips MASTER\
    \ \nTL-D 58W/865). Arabidopsis plantlets were cultivated as \ndescribed above\
    \ for 20 days (Exp. III).\nEvaluations\nMorphological characteristics of hyperhydricity\
    \ via image \nanalysis\nFor visualization of the etiology of HH, the multisensory\
    \ \nrobot system “Phenomenon” (Bethge et al. 2023) was used. \nRGB images were\
    \ captured in Exp. I and Exp. II every 4 h \nwith a 12.3-megapixel RGB camera\
    \ (Raspberry Pi Camera \nHQ, Raspberry Pi Foundation, Cambridge, UK) equipped\
    \ \nwith a 6 mm fixed focal length low-distortion lens (Edmund \nOptics: 6 mm\
    \ wide angle lens, f/1.2, high resolution = 120 \nlp  mm−1 (lp = line pairs),\
    \ low distortion < 0.5%) and with the \nfollowing camera parameters: resolution\
    \ = 4054 px × 3040 \nTable 2  Overview of conducted experiments and measurements\
    \ ten\na The multisensory robot system “Phenomenon” developed by Bethge et al. (2023),\
    \ consisting of 4 sensors (RGB camera, laser distance sensor, \nthermal camera\
    \ and a microspectrometer), was used to enable in-situ measurement of the morphology\
    \ through the lid of the culture vessels\nExperiment\nPlant species\nTime series\
    \ [day]\nEvaluations\nDetermination/Device\nI\nArabidopsis thaliana\n0–20\nRGB\
    \ growth curve\nRGB image sensor of  Phenomenona\nRGB shape analysis\nRGB image\
    \ sensor of  Phenomenona\nDepth mean canopy height\nLaser distance sensor of \
    \ Phenomenona\nDepth maximum plant height\nLaser distance sensor of  Phenomenona\n\
    II\nMalus sp.\n0–27\nRGB growth curve\nRGB image sensor of  Phenomenona\nRGB shape\
    \ analysis\nRGB image sensor of  Phenomenona\nRGB image data set\nRGB image sensor\
    \ of  Phenomenona\nDepth mean canopy height\nLaser distance sensor of  Phenomenona\n\
    Depth maximum plant height\nLaser distance sensor of  Phenomenona\nIII\nMalus\
    \ sp.,\nArabidopsis thaliana\n0, 5, 10, 15, 20\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\nIV\nMalus sp.\n14, 21, 28\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\nV\nMalus sp.\n0, 4, 8, 12, 16\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\n555\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\npx, shutter speed = 2000  ms, iso = 100, autowhite-bal-\nance = off and\
    \ a fixed gain of 3.3, 1.5 (red, blue).\nSensor data from the multisensory robot\
    \ system “Phe-\nnomenon” (Bethge et al. 2023) were processed, segmented, \nand\
    \ various parameters were calculated. RGB image analysis \nwas performed in Python\
    \ (Van Rossum and Drake 2009), \nusing the following packages: OpenCV v3.4.9 (Bradski\
    \ \n2000), NumPy v1.20.2 (Van Der Walt et al. 2011) and \nPlantCv v3.11.0 (Gehan\
    \ et al. 2017) and the Software toolkit \nIlastik v1.3.3 (Sommer et al. 2011)\
    \ headless integrated in \nthe Python script. RGB image analysis included a histogram\
    \ \nstretching for normalization, segmentation via a trained ran-\ndom forest\
    \ classifier, normalization to the day 0 plant area \nand calculation of projected\
    \ plant area (37.7 px = 1 mm). \nShape analyses were performed on the four largest\
    \ objects \nby area and limited to the first nine days to avoid errors from \n\
    overlapping explants. We used the installed shape function \nof PlantCv to calculate\
    \ solidity (measure of density as the \nratio between object area and area of\
    \ the convex hull of the \nobject) and eccentricity (measure of deviation of an\
    \ ellipse \nto a circle (eccentricity = 0) as the ratio between major and \nminor\
    \ axis).\nDepth data were acquired once per day for each culture \ncontainer with\
    \ the point-measuring laser distance sensor as a \nspatial scan by sequential\
    \ readout of the sensor while shifting \nthe detector head of the “Phenomenon”\
    \ robot system in xy \ndirection, according to the scan pattern (100 mm × 100 mm;\
    \ \nwith a resolution of 1 mm × 1 mm). The laser distance sen-\nsor (OD-Mini OB1-B100,\
    \ Sick AG, Waldkirch, Germany) \nused in this setup was specified by the manufacturer\
    \ with a \npower consumption of < 1.92 W, laser emission wavelength \nof 655 nm,\
    \ max. output of 390 µW (laser class 1), a measur-\ning range of 50 to 150 mm\
    \ and a linearity of ± 100 µm as \nwell as spot size of 700 µm × 600 µm at a measuring\
    \ distance \nof 100 mm. The analog output of the laser distance sensor \n(10 V)\
    \ was connected via a small voltage divider circuit to \na high precision 16-bit\
    \ A/D-converter (ADS 1115), which \ncommunicated via Inter-Integral Circuit  (I2C)\
    \ with a micro-\ncontroller board (Wemos D1 Mini). Each distance measure-\nment\
    \ consisted of a up to ten single readouts and averaging \n(excluding default\
    \ sensor values), to achieve a robust and \nlow-noise measurement. A detailed\
    \ description of the robot \nsystem “Phenomenon” can be found in Bethge et al.\
    \ (2023).\nDepth data of explants were obtained by measuring \n10,000 data points\
    \ of each culture vessel once a day with \na scanning laser distance sensor. The\
    \ depth data processing \npipeline included the segmentation of culture media\
    \ by a \nRANSAC (random sample consensus, Fischler and Bolles \n1981) segmentation\
    \ approach, subtraction of RANSAC \nplane, normalization to the day 0 plant height\
    \ with the \nPython libraries: Open3D v0.15.1 (Zhou et al. 2018) and \nPyvista\
    \ v0.34.0 (Sullivan et al. 2019). Pipelines construction \nis described in detail in\
    \ Bethge et al. (2023).\nStatistical analysis of repeated measures data was per-\n\
    formed using R software. Data were transformed, if neces-\nsary, with the R package\
    \ bestNormalize v1.8.2 (Peterson and \nPeterson 2020). Different linear mixed-effect\
    \ models from \nnlme v3.1-153 package (Pinheiro et al. 2017) were fitted to \n\
    the data with different covariance structures: scaled identity, \nfirst-order\
    \ autoregressive, first-order heterogeneous autore-\ngressive, compound symmetry,\
    \ Toeplitz and heterogenous \nToeplitz. The mean model consisted of the fixed\
    \ effects \ntreatment/medium type and time and their interaction terms. \nAn extra\
    \ random effect was included in the model to account \nfor the dependencies between\
    \ measurements from the same \nculture container or in SI. 1 for shape analysis\
    \ from the same \nexplant (as nested random effect). We also included linear \n\
    models with random intercept (CulturecontainerID) and \nrandom slope (Time). The\
    \ respective best model (Fig. 1A: \nlinear mixed model with scaled identity covariance\
    \ struc-\nture and random slope; Fig. 1B and SI. 1A: linear mixed \nmodel with\
    \ heterogenous Toeplitz covariance structure and \nrandom slope; SI. 1B: linear\
    \ mixed model with heteroge-\nnous Toeplitz covariance structure and random intercept)\
    \ \nwas selected based on the Akaike Information Criterion \n(AIC, Sakamoto et al.\
    \ 1986) values and residual analysis \n(QQ-plot). Pairwise comparisons using Tukey’s\
    \ HSD test \nat p < 0.05 was performed and show significant differences \nbetween\
    \ treatments within a time point.\nVisual scoring of hyperhydricity severity level\n\
    In the Experiments III to V, the severity of HH was assessed \nfor each explant\
    \ and at every time point (Exp. III: 0, 5, 10, \n15, 20 days; Exp. IV: 14, 21,\
    \ 28 days; Exp. V: 0, 4, 8, 12, \n16 days) according to Tian et al. (2015) with\
    \ minor modi-\nfications (Table 3). The starting plant material cultivated \n\
    on control media represented the samples of 0 days after \ntransfer (DAT 0).\n\
    Determination of apoplastic liquid volume\nPer time point at least 10 samples\
    \ per treatment were col-\nlected for the determination of apoplastic liquid volume,\
    \ \nwith DAT 0 samples representing the starting material. \nApoplastic liquid\
    \ was extracted from leaf tissue by mild \ncentrifugation according to van den\
    \ Dries et al. (2013) and \nTerry and Bonner (1980): Leaves (50–150 mg FM) from\
    \ \na single explant were excised, weighed, and placed into a \n2 mL tube microcentrifuge\
    \ filter without membrane (Clear-\nLine®; Kisker Biotech GmbH & Co, Steinfurt,\
    \ Germany). \nSamples were centrifuged at 3000 g for 20 min at 4 °C. \nImmediately\
    \ after centrifugation, the leaves were reweighed \nto determine the apoplastic\
    \ liquid volume  (VAL) in µL  g−1 \nfresh mass (FM) using the Eq. 1.\n556\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nwhere FM\
    \ = fresh mass of leaves in mg,  Mac = mass of leaves \nafter centrifugation and\
    \ ρH2O = water density (the water den-\nsity was taken as equal to 1 g  mL−1 assuming\
    \ the apoplastic \nliquid is mainly water and has a temperature of 4 °C).\nSpectral\
    \ data acquisition and analysis\nPrior to the quantification of apoplastic liquid\
    \ volume, one \nfully expanded leaf per explant under study was collected. \n\
    The leaf was then placed in a 3D printed sample holder \n(SI. 2) in an adaxial\
    \ position that allowed for flat clamping \n(1)\nVAL =\n(FM − Mac\n) ⋅ 휌H2O\n\
    FM\nFig. 1  Morphological differences in growth patterns of explants of \nA. thaliana\
    \ Col-0 and Malus ‘G214’ cultivated on either agar or gel-\nrite solidified media\
    \ (Mean ± SD). A The curve for the increase in \nthe projected plant area was\
    \ calculated from the analysis of the seg-\nmented RGB images normalized to the\
    \ plant area of day 0 and pre-\nsented as projected plant area  [cm2]. Since flower\
    \ initiation started at \nlater time points for A. thaliana and thus an error\
    \ in the estimation of \nprojected plant area might occur, the analysis of growth\
    \ curves was \nlimited to the first ten days. B The relative increase in mean\
    \ canopy \nheight resulted from analysis of segmented depth data collected with\
    \ \na scanning laser distance sensor and normalized to day 0 plant height. \n\
    Yellows lines indicates cultivation on standard media formulation \non Gamborg-B5\
    \ (A. thaliana) and MS-Medium (Malus) solidified \nwith 0.8% agar (w/v), while\
    \ dark gray lines display the cultivation \non induction media containing 0.25%\
    \ (w/v) gelrite, inducing HH. C \nRepresentative images at the endpoint of the\
    \ experiments. Sample \nnumber (n) indicates the individual culture containers.\
    \ Significance \nstars indicate comparisons of treatments within a time point\
    \ (day) \nwith *p < 0.05, **p < 0.01, ***p < 0.001. RGB and depth data were \n\
    acquired with the multisensory robot system “Phenomenon” (Bethge \net al. 2023).\
    \ (Color figure online)\nTable 3  Scoring of hyperhydricity by visual observation\
    \ (Tian et al. \n2015, with minor modifications)\nHyperhydricity score\nSymptoms\n\
    0\nNo visual symptoms\n1\n≤ 50% curled leaves\n2\n > 50% curled leaves\n3\n >\
    \ 50% curled and thickened leaves\n4\nCurled, thickened, translucent, fragile\
    \ leaves\n557\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\nwithout exerting too much pressure on the leaf (with a cav-\nity of 1 mm).\
    \ The curled hyperhydric leaves were handled \nwith care to obtain reflection\
    \ spectra from a planar surface. \nThe leaf reflectance spectra were examined\
    \ with a Perkin-\nElmer Lambda 900 UV–VIS-NIR-SWIR spectrometer \n(Perkin-Elmer\
    \ Instruments, Norwalk, USA) equipped with \n150 mm Indium-Gallium-Arsenide (InGaAs)\
    \ integrating \nsphere. The reflectance intensity was measured in steps of \n\
    1 nm in the wavelength range between 200 and 2000 nm, \nand the reflectance was\
    \ calculated using the reflection \nspectrum of the white reference standard Spectralon®.\
    \ \nRaw spectra were pre-processed in R v4.1.2 using Rstudio \n(RStudio Team 2015)\
    \ with the hsdar v1.0.4 package (Leh-\nnert et al. 2018) allowing the cleaning\
    \ of device errors, \ntrimming to spectral range of 400 mn to 2000 nm and \nsmoothing\
    \ with the Savitzky-Golay filter at a window size \nof 25 data points of third-degree\
    \ polynomials to remove \nnoise from data.\nSpectra of leaves obtained from three\
    \ experiments \n(Exp. III: 147, Exp. IV: 39 spectra, Exp. V: 51) were \ndivided\
    \ into two groups based on the significance level \nof the apoplastic liquid volume\
    \ and the HH score of the \nwhole explant was assessed by visual observation.\
    \ Here, \nthe explants with a HH score of 0 and 1 were classified as \nnormal\
    \ explants while the explants with a HH score of 2 \nto 4 represented hyperhydric\
    \ explants. This resulted in 100 \nand 137 spectra of normal and hyperhydric leaves,\
    \ respec-\ntively, covering the two plant species Malus ‘G214’ (187 \nspectra)\
    \ and A. thaliana (50 spectra). For visualization \nand isolation of the HH-specific\
    \ absorption features, leaf \nspectra were further processed with the segmented\
    \ upper \nhull continuum removal method described in detail in Leh-\nnert et al.\
    \ (2018). This normalization method allowed a \ncomparison of individual absorption\
    \ features on a com-\nmon baseline formed by a segmented upper hull of local \n\
    maxima and resulted in absorption features spectra. In \naddition, difference\
    \ spectra of absorption feature spectra \nwere calculated by subtracting normal\
    \ leaf spectra from \nhyperhydric leaf spectra.\nIn addition, we defined three\
    \ spectral ranges based on \nthe sensitivity of the state-of-the-art sensor technologies\
    \ \nsuch as standard RGB camera systems with silicon sensor \nchips (3 channels:\
    \ B: 400 nm to 500 nm, G: 500 nm to \n600 nm and R: 600 nm to 700 nm), multispectral\
    \ camera \nsystems with silicon sensor chips (4 channels: B: 400 nm \nto 500 nm,\
    \ G: 500 nm to 600 nm, R: 600 nm to 700 nm and \nNIR: 750 nm to 850 nm) and SWIR-HSI\
    \ camera systems \nwith Indium-Gallium-Arsenide (InGaAs) sensor chips \n(SWIR:\
    \ 900 nm to 1700 nm). This division was made as a \ndecision support for assessing\
    \ the potential of the candi-\ndate detection systems to detect HH based on their\
    \ spectral \nsensitivity range and considering their affordability.\nIdentification\
    \ of hyperhydricity‑specific absorption features\nDifferent ML models were trained\
    \ with the caret v6.0-90 \npackage (Kuhn 2008) in the R software to identify the\
    \ \nkey absorption features that discriminate between nor-\nmal and hyperhydric\
    \ explant leaf spectra. Here, pre-pro-\ncessed spectral data sets (237) were centered\
    \ and scaled \nand divided into a training set (178 spectra; Malus: 143, \nA.\
    \ thaliana: 35, with 103 normal and 75 hyperhydric \nexplants, in total) and a\
    \ test set (59 spectra; Malus: 44, \nA. thaliana: 15, with a total of 34 normal\
    \ and 25 hype-\nrhydric explants). All classification models were trained \nwith\
    \ the same resampling procedure consisting of a 10 \ntimes tenfold repeated cross\
    \ validation (CV). The tenfold \nrepeated CV divides the training data into 10\
    \ equal parts \n(10 subsamples with a size of 178/10). These parts are \niterated\
    \ 10 times, during each iteration, 9 of the 10 parts \nserve as training data,\
    \ and the remaining 10th part as the \nvalidation set to calculate model performance\
    \ metrics. In \n10 times repeated tenfold CV this process is repeated 10 \ntimes;\
    \ therefore, performance of training was validated on \n100 validation subsamples\
    \ consisting of 17–18 individual \nspectra.\nIn the confusion metrics, correctly\
    \ classified normal \nand hyperhydric leaves formed the true-positive (TP) \n\
    and the true-negative (TN) class, while false classified \nones constituted the\
    \ false-positive (FP) and false-negative \n(FN) class, respectively. For evaluation\
    \ of model valida-\ntion performance, the sensitivity (Eq. 2; TPR: true posi-\n\
    tive rate) and the specificity (Eq. 3; TNR: true negative \nrate) were calculated\
    \ with normal explants as the positive \nclass and the area under the curve (AUC)\
    \ of the receiver-\noperator-characteristics (Eq. 4; AUC ROC), while for evalu-\n\
    ation of model test performance, the accuracy (Eq. 5) was \ndetermined. Here,\
    \ misclassifications are described by the \nfalse negative rate (FNR) and false\
    \ positive rate (FPR). \nBalanced accuracy (Eq. 6) and  F1 score (Eq. 7) were\
    \ calcu-\nlated to account for putative class imbalances. To find the \nbest suitable\
    \ model for discriminating between normal and \nhyperhydric leaf spectra, different\
    \ ML model structures \nwere tested, including a neuronal net with the maximum\
    \ \nallowable number of weights set to 2000 (“nnet” from nnet \nv7.3-16 package;\
    \ Ripley et al. 2016), a linear discriminate \nanalysis (“lda” from caret package),\
    \ a supported vector \nmachine (“svmLinear” from caret package), a random for-\n\
    est (“rf” from caret package), a high dimensional discrimi-\nnate analysis (“hdda”\
    \ from caret package) as well as a \nlinear discriminate analysis (“lda” from\
    \ caret package with \nPCA-preprocessed data set) with an upstream principal \n\
    component analysis (PCA). Based on their resampled per-\nformance metrics, the\
    \ best model was selected to identify \nits most relevant features/wavelengths\
    \ on the basis of the \nunderlying variable importance in the model.\n558\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nAutomated\
    \ hyperhydricity detection\nTo test the validity of the developed spectral classifier,\
    \ an \nHSI-system operating in the shortwave infrared (SWIR) \nregion was used\
    \ to acquire a single HSI data cube from a \nculture vessel containing a HH-sensitive\
    \ apple genotype \n(Malus ‘Selection 4’). The imaging system that was devel-\n\
    oped and described by Thiel (2018) consisted of an EVK \nHelios Core NIR Line-scan\
    \ camera (240 px × 1 px and 252 \nspectral channels in the wavelength region of\
    \ 900 nm to \n1700 nm), two 65 W halogen spot lights and a conveyer-\nbelt system\
    \ to move the sample. Image acquisition was \nperformed in closed polypropylene\
    \ culture vessels, so that \nsterile conditions could be maintained inside the\
    \ vessel and \nwater condensation was prevented by heat radiation from \nthe halogen\
    \ lamps. Since only one HSI data cube could be \nacquired, these results were\
    \ considered to be an exemplary \nand preliminary validation test.\nThe developed\
    \ spectral classifier was retrained with a \nreduced number of features to match\
    \ the spectral channels \nof the imaging system (Features/wavelengths: 252 channels\
    \ \nin the range between 900 and 1700 nm). Due to the binary \nclassification\
    \ output of the classifier, most of the background \npixels were removed by creation\
    \ of a binary mask with \nsimple thresholding of the image slice at a wavelength\
    \ of \n1000 nm. Then each pixel of the segmented hyperspectral \ndata cube was\
    \ inserted as an input to the spectral classifier \nand class membership was predicted.\n\
    As a more affordable approach and as a proof of con-\ncept, an object detection\
    \ model based on annotated RGB \nimages acquired by the robot system was trained.\
    \ Therefore, \n250 images were randomly selected and annotated with the \n(2)\n\
    Sensitivity = TPR = 1 − FNR =\nTP\nTP + FN\n(3)\nSpeciﬁcity = TNR = 1 − FPR =\n\
    TN\nTN + FP\n(4)\nAUCROC = ∫ TPR (FPR) d(FPR)\n(5)\nAccuarcy =\nTP + TN\nTP +\
    \ FP + TN + FN\n(6)\nBalanced accuarcy = TPR + TNR\n2\n(7)\nF1 score = 2 ×\n(\n\
    TP\nTP + FP\n)\n× TPR\n(\nTP\nTP + FP\n)\n+ TPR\ngraphical user interface  Roboflow©\
    \ (Dwyer et al. 2022). \nThe image data set consisted of 200 annotated images\
    \ of \neight culture containers from Experiment II and 50 images \nfrom a comparable\
    \ experiment to increase variance in the \nnumber of explants, background colour,\
    \ and colour of cul-\nture media. A total of 504 normal explants and 545 hype-\n\
    rhydric explants were included. The image data set was \ndivided into 175 images\
    \ as training set, 50 images as vali-\ndation set and 25 images as test set. Data\
    \ augmentation of \nannotated bounding boxes increased the training set to 1800\
    \ \nimages and included: horizontal and vertical flip, rotation \nby 90° (clockwise,\
    \ counter-clockwise, upside down), rota-\ntion by ± 5°, brightness by ± 10%, exposure\
    \ by ± 7%, blur \nwith 2px and noise with 2% of pixels. The data set (Bethge \n\
    2023) is publicly accessible via  Roboflow© universe. Time \nseries images of\
    \ two culture vessels from Experiment II were \nretained and used to visualize\
    \ the trained model. Object \ndetection models perform attempts to identify and\
    \ locate \nobjects in images while assigning them to the appropriate \nclasses.\
    \ We selected YOLOv8 (Jocher et al. 2023) archi-\ntecture as the latest versions\
    \ of the YOLO (“You only look \nonce”, Redmond et al. 2016) family. YOLO is a\
    \ single-stage \nobject detector, consisting of three parts in its architecture:\
    \ \nbackbone, neck and head. The backbone is defined by sev-\neral convolutional\
    \ layers which extract key features from \nthe images, the neck uses the features\
    \ and forms the feature \npyramid by fully connected layers and the head is the\
    \ final \noutput layer for prediction of bounding boxes and classifi-\ncation.\
    \ The training process was performed in the Google \nColaboratory (Colab/Colab\
    \ Pro) environment on a NVIDIA \nA100-SXM4-40 GB graphical processing unit (GPU)\
    \ ser-\nviced by Google. In addition, the model was trained with \nthe following\
    \ parameters: epochs = 250 (early stopping \noccurred after 188 epochs), batch\
    \ size = 16 images, image \nsize = 640 px, patience = 100 epochs, learning rate\
    \ = 0.01, \nmomentum = 0.94, intersection over union (IoU) = 0.7. We \nlet Roboflow\
    \ train two object detection models, one from \nscratch and one with weights from\
    \ a previously trained \nmodel (additional 125 images from the same experiment)\
    \ to \nsee the full potential of the dataset with the optimized pipe-\nline. Evaluation\
    \ of model performance was based on preci-\nsion (Eq. 8), recall (Eq. 9), average\
    \ precision (AP; Eq. 10) \nand mean average precision (mAP; Eq. 11) of the validation\
    \ \nset. Here true positive (TP) indicate a correct detection and \nclassification,\
    \ false negative (FN) describes cases where the \nprediction missed the detection\
    \ contained in the ground truth \ndata, while in a false positive (FP) case a\
    \ bounding box was \npredicted on a location not contained in the ground truth\
    \ \ndata. Thereby, AP represents the area under the precision-\nrecall-curve across\
    \ a range of probability confidence thresh-\nold values from 0 to 1. The mAP is\
    \ the sum of AP of each \nclass (k) divided by the number of classes (n) at a\
    \ given \nintersection over union (IuO) threshold of 0.5. Intersection \n559\n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\nover union\
    \ is defined as ratio between the overlap area to the \nunited area of the predicted\
    \ and ground truth bounding box. \nAfter the training process predications were\
    \ obtained using \nthe Python library roboflow v0.2.25 (Dwyer et al. 2021) with\
    \ \nIuO threshold and confidence threshold set to 0.5.\nMorphological characteristics\
    \ of hyperhydricity \nvia image analysis\nStudying the morphology of the shoots\
    \ of the two treatments \nrevealed major differences in horizontal and vertical\
    \ growth. \nSignificantly stronger growth, quantified as projected plant \narea,\
    \ was observed for the gelrite treatment at early time \n(8)\nPrecision =\nTP\n\
    TP + FP\n(9)\nRecall =\nTP\nTP + FN\n(10)\nAP =\n1\n∫\n0\nPrecision(Recall) d\
    \ (Recall)\n(11)\nmAP = 1\nn\nk=n\n∑\nk=1\nAP(k)\npoints (5 days) for both plant\
    \ species (Fig. 1A). After \n4 weeks of cultivation, shoots of Malus in culture\
    \ vessels \nwith gelrite medium had with 24.5  cm2 a 2.4 times greater \nincrease\
    \ in projected plant area than shoots in vessels with \nagar medium with 10.3\
    \  cm2. Here, 65% of the explants of \nMalus had a HH score > 2 in the gelrite\
    \ treatment compared \nto 0% for agar treatment. For A. thaliana we evaluated\
    \ the \nprojected plant area only until day 10 to avoid distorting \neffects on\
    \ projected plant area due to flower initiation start-\ning at day 12. Shape analysis\
    \ of single explants showed sig-\nnificant differences in solidity at day 3 and\
    \ in eccentricity \nat day 6 for A. thaliana, whereas the shape differences of\
    \ \nMalus explants were not significant (SI. 1). Vertical growth \nanalysis, quantified\
    \ as mean canopy height (Fig. 1B) and \nmaximum shoot height as mean of upper\
    \  10th percentile (SI. \n1), showed a significantly higher mean canopy height\
    \ of \nMalus for the gelrite treatment at day 18 and of A. thaliana \nat day 16.\
    \ An even earlier distinction was recorded for the \nmaximum shoot height, i.e.\
    \ at day 11 and 14 for A. thaliana \nand Malus, respectively.\nHyperhydricity\
    \ induction\nVisual scoring of HH revealed the dynamics of HH induc-\ntion using\
    \ gelrite in the two plant species under investiga-\ntion. Anthocyanin accumulation\
    \ was noted within the first \n4 days in both treatments for Malus. However, it\
    \ persisted \nFig. 2  Visual scoring of hyperhydricity of A Malus ‘G214’ and B\
    \ \nA. thaliana Col-0 in  vitro cultures over 20  days (DAT, Days After \nTreatment).\
    \ Samples from 0  days after transfer (DAT 0) represent \nthe starting plant material\
    \ cultured on control media. Yellows bars \nindicate cultivation on standard media\
    \ formulation A MS-Medium, \nB Gamborg-B5 solidified with 0.8% (w/v) agar, while\
    \ gray bars dis-\nplay the cultivation on induction media containing 0.25% (w/v)\
    \ gel-\nrite. Dashed lines represent the medians of each histogram. Sample \n\
    number (n) indicates the individual explants. The different sample \nnumbers result\
    \ from the combined evaluation with different methods \n(apoplastic liquid evaluation,\
    \ reflection spectroscopy) of the same \nsamples. Different letters resulting\
    \ from Kruskal–Wallis test followed \nby Fisher’s LSD (p  < 0.05) indicate significant\
    \ differences between \nhistograms. Kruskal–Wallis effect size could be determined\
    \ to be \nvery strong with A η2 = 0.62 and B η2 = 0.65. (Color figure online)\n\
    560\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\n\
    only in the gelrite treatment until the end of the experiment \nin most explants.\
    \ In Malus, severe symptoms of HH were \ninduced even on the agar control medium\
    \ in 12.5% of the \nshoots (Fig. 2A). In two experiments, significant differences\
    \ \nin the HH score and the occurrence of severe symptoms \n(curled, thickened\
    \ and translucent leaves = level 4 of the \nHH score) between the agar control\
    \ and the gelrite induction \ntreatment were identified 10 days (Fig. 2A) and\
    \ 8 days (SI. \n3) after transfer. When performing this experiment under a \n\
    novel phenotyping system, time-lapse videos were taken. \nThey confirmed these\
    \ observations and visualized the tem-\nporal development of HH in the two plant\
    \ species (Malus: \nSI. 4 and A. thaliana: SI. 5). Three of four apple shoots\
    \ \nturned into a hyperhydric status and formed first hyperhy-\ndric leaves (SI.\
    \ 4: arrows) on gelrite at 5 DAT (SI. 4: 100 h). \nThey started to curl at 8 DAT\
    \ and also became much larger \nthan those on agar. After 27 days, severe symptoms\
    \ appeared \non dark green to reddish explants that exhibited compact \ngrowth\
    \ with curled, epinastic, and brittle leaves.\nFor A. thaliana, there was already\
    \ a significant increase \nin the HH score after 5 days of treatment (Fig. 2B).\
    \ Fur-\nthermore, decolorization of leaves was the predominating \nsymptom of\
    \ HH in A. thaliana on gelrite induction medium. \nFor A. thaliana seedlings,\
    \ first signs of HH (SI. 5: arrows) \nbecame visible at 5 DAT (SI. 5: 100 h) on\
    \ gelrite-solidified \nmedium and shoots developed longer petioles and much \n\
    larger leaves with severe HH symptoms.\nThe apoplastic liquid volume increased\
    \ steadily for Malus \n(Fig. 3A) until 15 DAT, while Experiment IV (SI. 3) demon-\n\
    strated a decrease at later time points: 21 days and 28 days. \nA significant\
    \ difference in apoplastic liquid content in both \nplant species was detected\
    \ at the earliest time point: 4 DAT \n(SI. 3) and 5 DAT (Fig. 3A and B), where\
    \ the apoplastic \nliquid volumes of explants on gelrite induction media were\
    \ \nalready twice as high as those of explants on agar control \nmedia. In Malus,\
    \ three independent experiments (Fig. 3A \nand SI. 3) allowed us to confine the\
    \ time of peak in apo-\nplastic liquid volume at 12 to 16 DAT. Apparently, up\
    \ to \nthis timepoint, quantification of apoplastic liquid volume \nreflected\
    \ the HH score well—even the occurrence of some \nhyperhydric explants on the\
    \ agar control medium was also \nreflected in the increase in apoplastic water\
    \ volume (Fig. 3A \nvs. SI. 3). However, at later time points, the severity of\
    \ HH \nsymptoms steadily increased, while apoplastic liquid volume \nstayed constant.\n\
    To prove the relation between the objective quantification \nof apoplastic liquid\
    \ volume and the HH score determined \nby visual scoring, data pairs of a total\
    \ of 349 measurements \nFig. 3  Apoplastic liquid volume of A Malus ‘G214’ and\
    \ B A. thali-\nana Col-0 in vitro cultures over time (DAT, Days After Treatment).\
    \ \nSamples from 0  days after transfer (DAT 0) represent the starting \nplant\
    \ material cultured on control media. Yellow lines indicate cul-\ntures on standard\
    \ media A MS-Medium, B Gamborg-B5 solidified \nwith 0.8% (w/v) agar, while gray\
    \ dashed lines display the cultures \non HH induction media containing 0.25% (w/v)\
    \ gelrite (Mean ± SD). \nThe values of A. thaliana at DAT 0 B were masked in gray\
    \ to indicate \nthe authors’ uncertainty, because the plants were very small when\
    \ \nthe apoplastic water volume was determined at this time, and there-\nfore\
    \ a large influence of adhering water could not be excluded. Rep-\nlicate number\
    \ (n) indicates the individual explants. Different letters \nresulted from Tukey’s\
    \ HSD test at p < 0.05 and indicate significant \ndifferences when comparing time\
    \ points within one treatment, while \nasterisks indicate comparisons of treatments\
    \ within a time point with \n* =  p < 0.05, ** =  p < 0.01, *** =  p < 0.001.\
    \ (Color figure online)\n561\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023)\
    \ 154:551–573 \n1 3\nfrom both treatments of Malus were used (Fig. 4). We found\
    \ \nthe highest correlation between HH score and apoplastic \nvolume to be ρ(18)\
    \ = 0.83 (p < 0.001) 12 DAT using spear-\nman’s rank correlation for all acquired\
    \ time points for Malus. \nInterestingly, only three groups could be distinguished\
    \ \nsignificantly by apoplastic liquid volume. Explants with a \nHH score of two\
    \ had more than > 50% curled leaves and \nin average a double amount of apoplastic\
    \ liquid volume. \nWe therefore restricted the three significant groups to two\
    \ \nclasses (HH score 0–1: normal explants and HH score 2–4: \nhyperhydric explants)\
    \ in further analysis, regardless of the \ntreatment in order to exclude treatment-depended\
    \ effects on \nthe spectral analysis.\nSpectral analysis of hyperhydricity\nThe\
    \ evaluation of leaf explants via UV–VIS-NIR-SWIR \nspectroscopy (Fig. 5) revealed\
    \ the first major difference in \nsignificantly reduced reflectance in the RGB\
    \ (400 nm to700 \nnm) region of 6.5 ± 3.2% for the hyperhydric explants com-\n\
    pared to 8.7 ± 3.9% for normal explants. The largest dif-\nference in reflectance\
    \ was recorded for NIR (750 nm to \n850 nm) region with a reflectance of 20.5\
    \ ± 9.0% hyperhy-\ndric explants and normal explants with 28.3 ± 9.5%. Also, \n\
    for the SWIR (950 nm to 1700 nm) region the overall reflec-\ntance was lower in\
    \ hyperhydric explants (13.8 ± 8.2% for \nhyperhydric and 21.4 ± 8.7% for normal\
    \ explants). Differ-\nences in average reflectance were most significant in the\
    \ blue \n(p< 2.2e-16) region followed by SWIR (p< 4.3e-14), green \n(p< 1.1e-12),\
    \ red (p< 2.5e-11) and the NIR region (p< 2.0e-\n09) according to the results\
    \ of a Mann–Whitney test.\nThe emergence of HH-specific absorption features \n\
    over time was recorded applying the continuum removal \nmethod to pre-processed\
    \ spectra of Experiment V and the \nFig. 4  Relation between visual scoring of\
    \ hyperhydricity and apo-\nplastic liquid volume of Malus ’G214’ (Mean ± SD).\
    \ Data obtained \nfrom three different induction experiments (Exp. III-V) covering\
    \ time \npoints from 0 to 28 DAT. Replicate number (n) indicates the indi-\nvidual\
    \ explants. Different letters resulted from Tukey’s HSD test at \np < 0.05 and\
    \ show significant differences between score levels\nFig. 5  Raw reflectance spectra\
    \ of Malus ‘G214’ and A. thaliana \nCol-0 in vitro leaves. Mean (solid) ± SD (dashed)\
    \ spectra of normal \nleaves (N, in green) and hyperhydric leaves (HH, in blue).\
    \ The dis-\ntinction was based on visual scoring of HH (N: 0–1 HH score; HH: \n\
    2–4 HH score). Wavebands represent different spectral regions, \ndefined by the\
    \ sensitivity of silicon-based (Si) cameras, such as \naffordable RGB and RGB-NIR\
    \ multispectral cameras, and a more \nexpensive Indium-Galium-Asenide-based (InGaAs)\
    \ detection sen-\nsor. Reflectance spectra were measured with an UV–VIS-NIR-SWIR\
    \ \nspectrometer (PerkinElmer Lambda 950) in a wavelength range of \n200 nm to\
    \ 2000 nm and at a resolution of 1 nm. (Color figure online)\n562\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nformation of difference\
    \ spectra of the isolated band depth \nspectra, where the absorption features\
    \ spectra of normal \nexplants were subtracted from absorption features spectra\
    \ \nof hyperhydric explants (Fig. 6). Greater absorption of hype-\nrhydric explants\
    \ was observed as early as 8 DAT, with a \nmaximum at 1402 nm and a full width\
    \ at half maximum of \n157 nm. At later time points the difference in absorbance\
    \ at \naround 980 nm, 1150 nm, 1400 nm, 1520 nm and 1780 nm \nincreased negatively,\
    \ while at around 1930 nm the difference \npositively increased. In the VIS region,\
    \ two further local \nmaxima arose at 460 nm and 695 nm at DAT 10, which were\
    \ \nalso detected at the later time points. However, these peaks \ncan be considered\
    \ as artefacts of the reduced reflection in the \ngreen region due to the continuum\
    \ removal method based \non connection of local maxima. In addition, a consistent\
    \ \npositive peak indicating less absorption or higher reflec-\ntion of hyperhydric\
    \ explants was found with a maximum \naround 1930 nm, besides the two local minima\
    \ at 1400 nm \nand 1520 nm. When combining data from all experiments \n(SI. 6),\
    \ including different time points and the two differ-\nent plant species, we identified\
    \ reliable minima (arrows) at \n980 nm, 1150 nm, 1400 nm, 1520 nm, and 1780 nm,\
    \ indicat-\ning stronger absorption of the hyperhydric explants, and a \nreliable\
    \ maximum at 1930 nm.\nFig. 6  Spectral contrasting reflectance of Malus ‘G214’\
    \ normal and \nhyperhydric explants over time (DAT, Days After Treatment). The\
    \ \nspectral data used originate from Experiment V. Upper row: Raw \nreflection\
    \ spectra; middle row: extracted absorption features after \nsegmented convex-hull\
    \ removal of raw spectra; bottom row: differ-\nence spectrum of the absorption\
    \ peaks, where the absorption features \nspectra of normal explants were subtracted\
    \ from absorption features \nspectra of hyperhydric explants. Mean (solid) ± SD\
    \ (dashed) spectra \nof normal explant leaves (N, in green) and hyperhydric explant\
    \ leaves \n(HH, in blue). Distinction of N and HH was based on visual scoring\
    \ \nof HH (N: 0–1 HH score; HH: 2–4 HH score). Arrows indicate puta-\ntive major\
    \ biochemical compounds absorbing in the given wavelength \nregion, according\
    \ to Curran (1989). Colored arrows represent: \"Chl\" \n= chlorophyll (dark green),\
    \ \"Antho\" = anthocyanin (red), \"H20\" = \nwater (dark blue), \"L\" = lignin\
    \ (dark red), \"P\" = protein (green), \"S\" \n= sugar (yellow). Reflection spectra\
    \ were measured with an UV–VIS-\nNIR-SWIR spectrometer (PerkinElmer Lambda 950)\
    \ in a wavelength \nrange of 200 mn to 2000 and at a resolution of 1 nm. Absorption\
    \ fea-\ntures spectra of the other conducted experiments, showing similar \nresults,\
    \ can be found in SI. 6. (Color figure online)\n563\nPlant Cell, Tissue and Organ\
    \ Culture (PCTOC) (2023) 154:551–573 \n1 3\nTo demonstrate whether the observed\
    \ differences in \nreflectance spectra are sufficient to reliably discriminate\
    \ \nbetween hyperhydric and non-hyperhydric explants, while \ngeneralizing plant\
    \ species and time points, we performed \na model spot checking for several ML\
    \ models with whole \nspectral data sets as input (Table 4). The models spot check\
    \ \nbased on AUC ROC metrics identified partial least square \n(PLS) and linear\
    \ discriminate analysis with upstream prin-\ncipal component analysis (PCA.LD)\
    \ with 0.94 to be superior \nin the training step when classifying the explants\
    \ against \nthe other models, while supported vector machine (SVM), \nneutral\
    \ net (NNET) with 0.93 and random forest model (RF) \nwith 0.92 performed only\
    \ slightly worse. High dimensional \nlinear discriminate analysis (HD.LD) showed\
    \ the lowest per-\nformance and was therefore excluded. Furthermore, SVM \nwas\
    \ best in classifying normal explants as expressed in the \nsensitivity metrics\
    \ with 0.91 ± 0.08, while NNET reached \nwith 0.93 ± 0.13 the highest specificity\
    \ indicating the best \nperformance in identifying hyperhydric explants. On the\
    \ \ntest set consisting of 59 unseen spectra, SVM outperformed \nthe other models\
    \ with the highest accuracy with 0.85, the \nhighest balanced accuracy with 0.84,\
    \ the highest sensitivity \nwith 0.91 and the highest F1 score of 0.87 and was\
    \ therefore \nselected as final model, besides for its low training time and \n\
    its better human interpretability. As a reference of a classical \napproach, we\
    \ checked the classification performance of a \ntwo-band normalized difference\
    \ ratio index using a threshold \nof 0.35, which resulted in a low accuracy of\
    \ 0.63.\nThe evaluation of the predictor importance based on \nROC-curve importance\
    \ of SVM revealed the most impor-\ntant wavelength for classification (Fig. 7).\
    \ The most relevant \nwavelength for classification was found at 1949 nm, followed\
    \ \nby the peak at 1445 nm in the SWIR region, 424 nm in the \nblue region and\
    \ 676 nm in the red region. The wavelength \nregion from 700 to 900 nm, including\
    \ the NIR region, con-\ntained the least essential information for the classification.\
    \ In \nthe green region, 500 nm was most important, while in the \nSWIR region\
    \ two further peaks were identified at 975 nm \nand 1202 nm.\nThe 237 acquired\
    \ spectra of the two species were further \nused to simulate three in literature\
    \ stated HH-affected leaf \ncompounds over time (anthocyanin, water, lignin) via\
    \ \nTable 4  Performance metrics of machine learning (ML)-based spectral classifiers.\
    \ Bold letters indicate the value for the best performing model \nin each column\n\
    * Principal component analysis (PCA) was performed prior to linear discriminate\
    \ analysis (LD), therefore the training time should be considered \nslightly higher\n\
    a Note: AUC ROC, sensitivity and specificity were calculated with normal explants\
    \ as the positive class\nb Note: Normalized difference ratio index with a threshold\
    \ of 0.35\nML model\nTraining\nData set [No. of \nspectra]\nTrain time [s]\nAUC\
    \ ROC\na [Mean ± SD]\nSensitivitya \n[Mean ± SD]\nSpecificitya \n[Mean ± SD]\n\
    NNET\n178\n761.7\n0.93 ± 0.07\n0.89 ± 0.10\n0.83 ± 0.13\nLDA\n178\n115.2\n0.87\
    \ ± 0.09\n0.82 ± 0.12\n0.74 ± 0.16\nSVM\n178\n42.2\n0.93 ± 0.07\n0.91 ± 0.08\n\
    0.79 ± 0.15\nRF\n178\n819.5\n0.92 ± 0.06\n0.84 ± 0.11\n0.80 ± 0.15\nPLS\n178\n\
    5.6\n0.94 ± 0.06\n0.90 ± 0.09\n0.82 ± 0.13\nHD.DA\n178\n38.0\n0.83 ± 0.09\n0.83\
    \ ± 0.12\n0.78 ± 0.14\nPCA.LD*\n178\n1.4\n0.94 ± 0.06\n0.89 ± 0.09\n0.82 ± 0.13\n\
    NDRIb\n–\n–\n–\n–\nTest\nML model\nData set [No. of \nspectra]\nAccuracy\nAccuracy\
    \ [95% CI]\nBalanced \naccuracy\nSensitivitya\nF1 score\nSpecificitya\nNNET\n\
    59\n0.81\n0.69–0.90\n0.81\n0.85\n0.84\n0.76\nLDA\n59\n0.73\n0.59–0.84\n0.72\n\
    0.79\n0.77\n0.64\nSVM\n59\n0.85\n0.82–0.93\n0.84\n0.91\n0.87\n0.76\nRF\n59\n0.71\n\
    0.58–0.82\n0.72\n0.68\n0.73\n0.76\nPLS\n59\n0.69\n0.56–0.81\n0.70\n0.68\n0.72\n\
    0.72\nHD.DA\n59\n0.61\n0.47–0.73\n0.62\n0.53\n0.61\n0.72\nPCA.LD*\n59\n0.69\n\
    0.56–0.81\n0.70\n0.68\n0.72\n0.72\nNDRIb\n59\n0.63\n0.51–0.77\n0.66\n0.47\n0.59\n\
    0.84\n564\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n\
    1 3\ndescribed vegetation indices (Fig. 8; mARI, Gitelson \net al. 2006; NDWI,\
    \ Gao 1996; NDLI, Serrano et al. 2002). \nHyperhydric explants of A. thaliana\
    \ showed a relatively \nsmall increase in ARI, high increase in NDWI and nota-\n\
    ble reduction in NDLI compared to normal explants. All \nthree vegetation indices\
    \ simulated using spectra of Malus \n“G214” classified as hyperhydric, revealed\
    \ a strong change \nover time compared to normal spectra.\nAutomated detection\
    \ of hyperhydricity\nTo test the validity of the SWIR region of the trained spec-\n\
    tral classifier as spectral region with high importance for \ndiscrimination of\
    \ HH and to see the generalization to a \nnew domain, the classifier was applied\
    \ on a previously \nacquired SWIR-HSI data set from culture vessels contain-\n\
    ing normal (N, green) and hyperhydric explants (HH, blue) \nFig. 7  Variable importance\
    \ of spectral classification of hyperhydric-\nity using a support vector machine\
    \ approach. Classification classes \nconsisted of reflection spectra from either\
    \ normal or hyperhydric \nleaves based on visual scoring of HH (N: 0–1 HH score;\
    \ HH: 2–4 HH \nscore). Wavebands representing different spectral regions, defined\
    \ by \nthe sensitivity of silicon-based (Si) cameras, such as affordable RGB \n\
    and RGB-NIR multispectral cameras, and more expensive Indium-\nGalium-Asenide-based\
    \ (InGaAs) sensors as candidates for detection\nFig. 8  Selection of contrasting\
    \ vegetation indices to hyperhydricity \ninducing cultivation of Malus 'G214'\
    \ and A. thaliana 'Col-0'. Vegeta-\ntion indices were calculated from spectra\
    \ from three different experi-\nments (Exp. III-V). Data points from normal explants\
    \ are indicated in \ngreen (N: 0–1 HH score), while the blue color represents\
    \ data from \nhyperhydric tissue (HH: 2–4 HH score). Estimated 95% confidence\
    \ \ninterval was colorized in light gray, while lines illustrate the locally \n\
    weighted data trend by  2nd order polynomial regression. ARI/mARI \ndefined according\
    \ to Gitelson et al. (2006), NDWI from Gao (1996) \nand NDLI according to Serrano\
    \ et al. (2002). (Color figure online)\n565\nPlant Cell, Tissue and Organ Culture\
    \ (PCTOC) (2023) 154:551–573 \n1 3\nof Malus ‘Selection 4’ (Fig. 9A). From the\
    \ spectral signa-\ntures (Fig. 9B), a normalized difference ratio index (NDRI,\
    \ \nFig. 9C–E) as a two-band index with a HH-insensitive \nwavelength at 1086 nm\
    \ (Fig. 9C) and a HH-responsive \nwavelength at 1432 nm (Fig. 9D) was derived.\
    \ Hyperhy-\ndric explants became almost invisible due to their high \nabsorption/\
    \ reduced reflection (R) at 1432 nm (Fig. 9D \nand SI. 7). Based on the acquired\
    \ spectral signature a nor-\nmalized difference ratio index (NDRI) could be derived\
    \ \n(Eq. 12), which is formed by two wavelengths, an HH-\ninsensitive correction\
    \ wavelength at 1086 nm and a HH-\nsensitive at 1432 mn.\nThe NDRI image (Fig. 9E)\
    \ was segmented with a mask \nfor plant pixels (Fig. 9F) and a threshold was applied\
    \ to \nproduce the classification image (Fig. 9G). For the ML \napproach that\
    \ included the application of the spectral clas-\nsifier (Fig. 9H), some modifications\
    \ were made to the trained \nspectral classifier, such as spectral resampling\
    \ to fit the spec-\ntral sensor channels and segmentation to limit the task to\
    \ \na two-class problem (see Materials and Methods section).\n(12)\nNDRI =\n(R1086nm\
    \ − R1432nm\n)\n(R1086nm + R1432nm\n)\nAs a more affordable approach of HH detection—SWIR\
    \ \ncamera systems can cost hundred to thousand times more \nthan an RGB camera\
    \ system—three different object detec-\ntion models were trained based on RGB\
    \ image time series \ndata sets to determine if the information contained in the\
    \ \nthree spectral channels of the RGB images (in addition to \nthe observed morphological\
    \ differences in the shape of the \nexplants) was sufficient to correctly classify\
    \ the hyperhy-\ndric explants. With all three trained models (Table 5) a high\
    \ \nmAP of > 88% was observed for the validation set, indicat-\ning a high accuracy\
    \ in localization and correct classification \nof the explants in the images.\
    \ Highest precision of 86.8% \nin validation set was reached with the model PCTOC_V2.\
    \ \nFor this model, we used the Roboflow Train option to train \nan object model\
    \ from scratch. The model PCTOC_V3 per-\nformed best in terms of the recall metric\
    \ with 95.7% and \nmAP with 95.6% in the validation set. In an unseen test data\
    \ \nset PCTOC_V3 outperformed the other models in mAP with \n97.0% and highest\
    \ recall 89.0% and was therefore selected \nto visualize its performance on a\
    \ selection of test set images \n(Fig. 10) and on unseen time-series data from\
    \ two culture \nvessels of the same experiment (SI. 8).\nThe PCTOC_V3 model identified\
    \ multiple objects on \nthe selection of test set images (Fig. 10) with only slightly\
    \ \ngreater predicted bounding boxes compared to ground truth. \nFig. 9  Validation\
    \ test of major absorption features by hyperspectral \nimaging. A Reference RGB\
    \ image of a Malus ‘Selection 4’ vessel \nwith normal (N, green) and hyperhydric\
    \ (HH, blue) explants used \nfor hyperspectral imaging of the SWIR region with\
    \ the EVK Helios \nCore NIR Line-scan camera (240 px × 1 px and 252 spectral chan-\n\
    nels in the wavelength region of 900 nm to 1700 nm, according to \nThiel 2018).\
    \ B SWIR reflectance spectra of normal leaf, hyperhydric \nleaf and culture media\
    \ (CM) pixels (green, blue and orange) with \nthe dashed vertical lines indicating\
    \ spectral locations of selected \nwavelengths. C and D False-color images of\
    \ selected wavelengths \nat 1086 nm and 1432 nm. E Normalized difference ratio\
    \ of selected \nwavelengths used to illustrate classical discriminating approach\
    \ via F \nsegmentation of plant pixels and G binarization by thresholding. Pro-\n\
    posed discriminating approach by application of H ML model “Sup-\nport vector\
    \ machine“ (ML-SVM) on segmented plant pixels of the \nSWIR-Hyperspectral-Image-Cube\
    \ (SWIR-HSI-Cube). ML-SVM was \nlaboratory-trained with single leaf reflection\
    \ spectra and is presented \nas the predicted probability images of plant pixels.\
    \ Hyperspectral \nimaging was performed through the lid of theculture vessel.\
    \ (Color \nfigure online)\n566\n \nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573\n1 3\nA supposedly perfect classification could be reached\
    \ with \nthe prediction settings used. However, severely hyperhy-\ndric explants\
    \ (Fig. 10B1) received a lower class member-\nship probability then explants with\
    \ developing HH symp-\ntoms (Fig. 10B2). Class membership probability of normal\
    \ \nexplants was generally high on the test set (Fig. 10B3) and \nseemed to be\
    \ stable even on the time-series RGB image data \nset (SI. 8, left image). For\
    \ hyperhydric explants, prediction \nconfidence increased until day 10 and decreased\
    \ at day 16 \nin the time-series RGB image data set (SI. 8, right image).\nDiscussion\n\
    Time‑lapse videos enable insights into early phases \nof HH development\nHH is\
    \ a serious limitation of plant tissue propagation affect-\ning multiple phases\
    \ of in vitro cultivation. The use of the \nnovel monitoring system “Phenomenon”\
    \ capturing time \nseries image data (SI. 4 and SI. 5) identified (i) the first\
    \ \nvisual symptoms of HH to occur 5 DAT and (ii) an acceler-\nated and higher\
    \ growth of shoots of the gelrite treatment \n(Fig. 1A). Thereby, significant\
    \ differences in the projected \nplant area between the two treatments were found\
    \ already \n5 days after transferring to the culture media in both species. \n\
    As discussed previously by Kevers et al. (1984), HH may \nbe considered as morphological\
    \ response to waterlogging, \nwhich in turn induces ethylene synthesis. For A.\
    \ thaliana, we \nobserved a higher vertical growth (Fig. 1B) with hyponasty \n\
    (SI. 5), which was described as ethylene-triggered strategy \nof ex vitro plants\
    \ in waterlogging conditions to re-estab-\nlish contact with air and restore successful\
    \ gas exchange \n(Voesenek and Blom 1989). Furthermore, Vreeburg et al. \n(2005)\
    \ described a flooding-induced petiole elongation in a \ntwo-stage process, starting\
    \ with acidification of the apoplast \nfollowed by cell wall expansion. This is\
    \ in agreement with \nour observation of a significantly higher eccentricity (devia-\n\
    tion of the ellipse to circle) and significantly less solidity \n(density of the\
    \ object) for explants in the gelrite treatment \n(SI. 1B). A more pronounced\
    \ curling of the leaves was \nobserved in Malus (SI. 4) which also resulted in\
    \ epinastic \nleaf growth. In addition, a significant higher mean canopy \nheight\
    \ (Fig. 1B) and maximum shoot height of Malus shoots \non gelrite medium (SI.\
    \ 1A) indicated a more pronounced \nvertical orientation of growth.\nHyperhydricity\
    \ induction by increased water \navailability\nAlthough HH symptoms vary between\
    \ different plant spe-\ncies and cultivars, and several factors have been described\
    \ \nto trigger HH, a putative common underlying mechanism \nof apoplast flooding\
    \ has been described (van den Dries \net al. 2013). Several studies showed that\
    \ increasing the \nwater availability by decreasing the concentration of \ngelling\
    \ agent, changing the type of gelling agent or the \ncultivation in liquid media\
    \ induced HH in a large set of \nplant species (Dianthus sp., Casanova et al.\
    \ 2008; Aloe \nsp., Ivanova and Van Staden 2011, Malus sp. Chakrabarty \net al.\
    \ 2003).\nIn our study, we demonstrated the HH-inducing effects \nof gelrite for\
    \ Malus and Arabidopsis indicated by the \noverall increase in HH scores (Fig. 2\
    \ and SI. 3) and apo-\nplastic liquid volume (Fig. 3 and SI. 3) over time. Gelrite\
    \ \ndiffers from agar in terms of consistency and purity and \nresulted in a superior\
    \ growth of explants at a comparable \ngel strength (Scherer 1987; Tsay et al.\
    \ 2006; Pasqualetto \net al. 1988). However, gelrite induced HH in several spe-\n\
    cies (Arabidopsis sp., van den Dries et al. (2013), Malus \nsp. Pasqualetto et al.\
    \ 1988, Prunus sp. Franck et al. 1998) \nlimiting the use of this gelling agent.\
    \ Scherer et al. (1988) \nTable 5  Performance metrics of object detection models\
    \ trained on RGB images. Bold letters indicate the value for the best performing\
    \ model in \neach column\na Note: Not yet implemented in Ultralytics YOLOv8.0.20\n\
    b Note: Trained with weights from PCTOC_V2 (based 250 images) and additionally\
    \ 125 images\nc Note: Precision and recall on test set were calculated with IoU\
    \ and confidence threshold of 0.5\nName\nModel archi-\ntecture\nTraining\nValidation\n\
    Test\nData set \n[No. of \nimages]\nDescription\nData set \n[No. of \nimages]\n\
    mAP \n[%]\nPrecision \n[%]\nRecall \n[%]\nData set \n[No. of \nimages]\nmAP \n\
    [%]\nPrecisionc \n[%]\nRecallc \n[%]\nPCTOC_\nV1\nYOLOv8\n250\nColab with weights\
    \ \nfrom scratch\n50\n88.4\n83.0\n82.1\n25\nNYIa\n94.4\n49.5\nPCTOC_\nV2\nRoboflow\
    \ 2.0 \nOD\n250\nWeights from scratch\n50\n93.5\n86.8\n86.4\n25\n95.0\n90.4\n\
    87.6\nPCTOC_\nV3\nRoboflow 2.0 \nOD\n375b\nWeights from \nPCTOC_V2\n50\n95.6\n\
    83.8\n95.7\n25\n97.0\n93.7\n89.0\n567\nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573 \n1 3\ncould show that there is no difference in the osmotic\
    \ and \nwater potential of gelrite compared to agar. Van den Dries \net al. (2013)\
    \ suspected therefore a local dissolution of the \nculture medium due to the excretion\
    \ of chelators by the \nexplants and thus a higher water availability and water\
    \ \nuptake. This higher water availability in gelrite-solidified \nmedia most\
    \ likely explains HH-induction and acceler-\nated growth, but other putative factors\
    \ like differences in \nFig. 10  Object detection performance of the PCTOC_V3\
    \ model on \nan image selection of the test set. (A) Ground truth RGB image of\
    \ \nthe Malus ‘G214’ test set annotated with normal (Normal, green) \nand hyperhydric\
    \ (HH, blue) explants (A1–A3). (B) Predicted objects \n(B1–B3) and class membership\
    \ probability (0 to 1 corresponds 0 to \n100%). Prediction was performed with\
    \ confidence threshold and \nintersection of union threshold of 0.5. (Color figure\
    \ online)\n568\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n\
    1 3\nuptake of nutrients or plant hormones were also found: \nHigher contents\
    \ of magnesium (Mg) and a higher ratio of \npotassium (K) to sodium (Na) were\
    \ detected in the leaves \nof walnut explants grown on gelrite medium compared\
    \ \nto agar, which can affect stomatal function (Barbes et al. \n1993). Furthermore,\
    \ Arthur et al. (2004) found a lower \nconcentration of IAA-like compounds in\
    \ gelrite than in \ndifferent types of agar powder.\nWith the collected data and\
    \ the time-lapse videos, we \ncould narrow down crucial key points within the\
    \ develop-\nment of HH of the two species in time. First visual identifi-\nable\
    \ symptoms (SI. 4 and SI. 5) and significant increases in \napoplastic liquid\
    \ volume were observed already after 5 days \nof cultivation on gelrite media\
    \ in both species. Time series \ndynamics of apoplastic liquid volume confirmed\
    \ previous \ndata for A. thaliana (van den Dries et al. 2013)—in both \nstudies\
    \ hyperhydric explants of A. thaliana had an apoplas-\ntic liquid volume of around\
    \ 300 µL  g−1 FM 15 days after \ntreatment, but were carried out for the first\
    \ time for Malus. \nQuantification of apoplastic liquid volume of A. thaliana\
    \ \nseedlings at very early time points was limited by the very \nsmall amounts\
    \ of apoplastic liquid and the distorting effect \nof adhering water (Fig. 3B).\
    \ For Malus, the highest increase \nin apoplastic liquid volume for the gelrite\
    \ treatment was \ndetected within the first 4–5 days in two independent experi-\n\
    ments (SI. 3, Fig. 3B). Furthermore, a different behavior of \nthe HH score and\
    \ the apoplastic liquid volume was found \nafter 21 days of cultivation in Malus:\
    \ While the severity \nof HH symptoms steadily increased over time, the apoplas-\n\
    tic liquid volume seemed to reach saturation at later time \npoints (SI. 3¸ Fig. 4).\
    \ Therefore, we suggest the HH score \nto be useful to determine the symptoms\
    \ of HH, whereas the \nquantification of apoplastic liquid volume better reflects\
    \ the \nphysiological state of the explants.\nIdentification of HH‑specific spectral\
    \ absorption \nfeatures\nDespite the fact that clear visible symptoms (Table 1)\
    \ of HH \nwere reported and still are the major distinguishing param-\neter for\
    \ classification, spectroscopic analysis of HH is lim-\nited. Only Marques et al.\
    \ (2021) using Fourier-transform \ninfrared spectroscopy in attenuated total reflectance\
    \ mode \n(FTIR-ATR), evaluated chemical properties of prepared \ncell walls of\
    \ hyperhydric Arbutus unedo. Assuming HH as \na consequence of flooding of the\
    \ air-filled apoplast by water, \nUV–VIS–NIR–SWIR reflection spectroscopy was\
    \ expected \nto detect these physiological changes due to higher light \nabsorption\
    \ of water compared to air. Therefore, we applied \nthis technique to identify\
    \ specific absorption features of HH \nessential for designing an automated detection\
    \ system. How-\never, we excluded the UV region (< 400 nm) from further \nanalysis\
    \ due to the low penetration depth of UV light in plant \ntissue (Qi et al. 2010),\
    \ since most reflection signals can only \nbe attributed to anatomical and biochemical\
    \ properties of \ncuticle, trichomes and the upper epidermis.\nThe observed overall\
    \ reduction in reflectance of hyperhy-\ndric explants (Fig. 5) compared to normal\
    \ ones is consistent \nwith the visual appearance of the observed darkening of\
    \ the \naffected explants (SI. 4 and SI. 5). The visualization of iso-\nlated\
    \ absorption features over the time course of the develop-\nment of HH in Malus\
    \ (Fig. 6) should give insights whether \nthere is at least a trend in the time\
    \ course of the presumed \nabsorption characteristics. We used the continuum removal\
    \ \nmethod to exclude the observed overall absolute reduc-\ntion in reflection\
    \ and to compare all spectra on a common \nbase. This allowed an automated extraction\
    \ of absorption \npeaks for the SWIR region with predominant absorptions \nvalleys,\
    \ however, produced artefacts in the VIS region. In \nthe SWIR region, a consistent\
    \ difference between absorp-\ntion of normal and hyperhydric leaves was observed\
    \ for the \nwavelengths 980 nm, 1150 nm, 1400 nm, 1520 nm, 1780 nm \nand 1930 nm,\
    \ both over time (Fig. 6) and in the different \nexperiments (SI. 6). Most likely,\
    \ the absorption of water \nin the plant tissue is most responsible for the wavelengths\
    \ \n980 nm, 1150 nm, 1400 nm. Curran et al. (1989) described \nthe intense absorption\
    \ of liquid water at 970 nm, 1200 nm, \n1400 nm and 1450 nm due to the fundamental\
    \ O–H bend-\ning vibrations of the first overtone. Thus, the tendency of an \n\
    increase in water absorption (970 mn, 1200 nm, 1400 nm, \n1450 nm) within time\
    \ is in accordance with the increase \nin apoplastic liquid volume over time.\
    \ However, absorption \nbands of other compounds like proteins, lignins and sug-\n\
    ars are located within the peak between 1300 to 1600 nm \nand contribute to the\
    \ total absorption in this region. Curran \net al. (1989) associated the absorption\
    \ at 1780 nm to cel-\nlulose, sugars and starch. Since for this wavelength a higher\
    \ \nabsorption in hyperhydric leaves was observed in our study, \nthis is in line\
    \ with the detection of a higher sugar content \n(sucrose, glucose and fructose)\
    \ in hyperhydric explants of \nDianthus (Saher et al. 2005), but contradicting\
    \ Kevers et al. \n(1987) who reported less lignin and cellulose in hyperhydric\
    \ \nDianthus.\nSimulation of vegetation indices (Fig. 8) demonstrated \ntraceable\
    \ trends, that closely match the dynamics of the \nphysiological reference data\
    \ (Fig. 3 & SI. 3) and support \nthe observation of time-series data (SI. 4 &\
    \ SI. 5). Overall, \nthe vegetation indices from normal explants exhibited low\
    \ \nvariance, although they were derived from different experi-\nments. The high\
    \ variance of hyperhydric explants indicated \nby the confidence interval can\
    \ be explained by different \nphysiological states of explants with different\
    \ degrees of \nhyperhydricity. The simulation of a modified anthocyanin \nindex,\
    \ indicated a higher anthocyanin content in hyperhydric \nleaves of Malus, but\
    \ not Arabidopsis, supporting our RGB \nimage time series. The normalized difference\
    \ water index \n569\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\n(NDWI) displayed higher water contents for hyperhydric \nexplants of\
    \ both species and supported our observation that \napoplastic liquid volume did\
    \ not increase any more after \n4 weeks of cultivation. The normalized difference\
    \ lignin \nindex (NDLI) showed in both species less lignin for hype-\nrhydric\
    \ leaves. However, the trend of the NDLI curves in \nboth species followed inversely\
    \ that of the NDWI indicat-\ning a putative dependency on plant water content.\
    \ Marques \net al. (2021) found no significant difference in the lignin \ncontent\
    \ per dry weight of hyperhydric and normal leaves of \nArbutus, whereas Kevers\
    \ et al. (1987) reported a lower lignin \ncontent per fresh weight of hyperhydric\
    \ tissue. It remains to \nbe clarified, whether these divergent results are due\
    \ to differ-\nent species or to the fact that the fresh mass of hyperhydric \n\
    explants is much higher.\nAutomated detection of HH by machine learning\nIn order\
    \ to evaluate the performance of the spectral data \nin the classification of\
    \ hyperhydric and normal leaves, we \ntrained different ML models (Table 4), investigated\
    \ the \nmost important wavelengths of the best model (Fig. 7) and \ncompared them\
    \ against a novel vegetation index as the clas-\nsical approach (Fig. 9). The\
    \ ML models differed in their \narchitecture, complexity, performance, prediction\
    \ time and \ninterpretability (Singh et al. 2016; Liakos et al. 2018; and \nHesami\
    \ and Jones 2020). All ML models reached a high \nAUC ROC > 0.83 in training,\
    \ however only SVM and NNET \nhad a high accuracy > 0.80 on test data. Both ML\
    \ models \noutperformed with an accuracy of 0.81 for NNET (balanced \naccuracy\
    \ of 0.81) and 0.85 for SVM (balanced accuracy of \n0.84) the univariate vegetation\
    \ index approach with a lower \naccuracy of 0.63 (balanced accuracy of 0.66).\
    \ Furthermore, \nSVM was best in classifying normal spectra indicated by \nhighest\
    \ sensitivity of 0.91 on the test set. The two-band veg-\netation index NDRI reached\
    \ the highest specificity of 0.84 \nin the test data, followed by SVM with 0.76,\
    \ meaning high-\nest ratio in the identification of hyperhydric tissue, however,\
    \ \nlow sensitivity of 0.47, low accuracy of 0.63 and low  F1 \nscore of 0.59\
    \ indicated a conservative behavior of classifica-\ntion towards hyperhydric explants.\
    \ SVM was selected due \nto its high performance on training and testing datasets,\
    \ low \ntraining data volume requirements, performance on high-\ndimensional datasets,\
    \ low risk of overfitting, good gener-\nalization ability, and its advantages\
    \ over NNET in terms \nof training time, simplified structure, and interpretability\
    \ \n(Singh et al. 2016; Liakos et al. 2018). The evaluation of \nthe feature importance\
    \ of SVM for classification (Fig. 7) \nsupported our findings that bands (peaks\
    \ with maxima at \n1949 nm, 1445 nm, 1202 nm and 975 nm) associated with \nwater\
    \ absorption were crucial to distinguish between hype-\nrhydric and normal leaves.\
    \ However, the method indicated \nessential features importance in the VIS region\
    \ with maxima \nat 424 nm and 676 nm. In regard of an automated HH detec-\ntion\
    \ system, we further evaluated two different approaches \n(i) HH detection based\
    \ on an HSI-SWIR camera system \n(Fig. 9) and (ii) HH detection based on RGB camera\
    \ system \ncoupled with a deep neuronal network (DNN) to provide two \nputative\
    \ solutions for commercial plant propagation based \non our findings (Table 5,\
    \ Fig. 10, SI. 8).\nFollowing the HSI-SWIR camera system approach, we \ncould\
    \ only test the validity of our spectral classifier as a \nproof of concept because\
    \ we only had a single HSI acquisi-\ntion (SI. 7), so these results should be\
    \ interpreted with cau-\ntion. In addition, our analysis followed a two-class\
    \ classifica-\ntion problem, but under the assumption that an automated \nHH detection\
    \ system monitors explants on culture media \nduring cultivation, culture media\
    \ spectra could presumably \ninterfere with the other classes within classification.\
    \ There-\nfore, for further studies, we propose to include the acquisi-\ntion\
    \ of reflectance spectra of the culture media in the dataset. \nNevertheless,\
    \ we could test our ML-SVM classifier, trained \non spectra from Malus ‘G214’\
    \ and A. thaliana, on the single \nSWIR-HSI acquisition of Malus ‘Selection 4’\
    \ segmented \nplant pixels, indicating the generalization ability of the clas-\n\
    sifier with respect to experimental setup and plant species/\ngenotype. The NDRI\
    \ and ML-SVM both classified most \npixels correctly, however, ML-SVM segmented\
    \ the borders \nof different classes much sharper. These preliminary results \n\
    demonstrated that classification of HH is possible during \nin vitro cultivation\
    \ and through the lid of the vessel with \neither an expensive SWIR-HSI system\
    \ classifying with our \nnovel ML-SVM classifier or more cost-effectively with\
    \ a \ntwo-channel SWIR camera system using a novel vegetation \nindex.\nAlternatively,\
    \ an RGB camera setup coupled with con-\nvolutional neural network (CNN) can be\
    \ the most cost-\neffective solution for an automated HH-detection. Since \nwe\
    \ had identified feature importance also in the VIS \nregion, a proof-of-concept\
    \ study was conducted to dem-\nonstrate object detection via CNN. Therefore, we\
    \ used \nthe  Roboflow© pipeline, which allowed an easy access to \nthese tools\
    \ and provided an interface for data annotation, \npre-processing, data augmentation,\
    \ training, data avail-\nability and deployment of the trained models. Comparing\
    \ \na self-trained YOLOv8 with the unknown object detec-\ntion algorithms of Roboflow\
    \ Train (Table 5), we did not \nreach the performance of their optimized model,\
    \ which \nwas particularly evident in the performance on test set, \nwhere PCTOC_V1\
    \ reached the highest precision with \n94.4%, but with low recall of 49.5%—indicating\
    \ only half \nof all explants could be detected. The best trained model \nPCTOC_V3,\
    \ however had a precision of 83.8% on valida-\ntion and of 97.0% on test set,\
    \ indicating that prediction \nwas mostly correct (Table 5, Fig. 10, SI. 8). In\
    \ addition \nthe explants were reliably detected (recall of 95.7% on \n570\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nvalidation\
    \ set and 89.0% on test set). By using the rela-\ntively new Python library roboflow,\
    \ we encountered some \nunsolved issues as seen in SI. 8 where non-maximum-\n\
    suppression only works so far within one class, resulting \nin multiple predictions\
    \ per object. Considering the prop-\nerties of the dataset, the low amount of\
    \ data (250 to 375 \nimages), resulting from time series images (1049 explants)\
    \ \nof only 32 individual explants, we could see already good \nperformance on\
    \ the test set and the time-series set (Fig. 10 \n& SI. 8).\nConclusions\nTo our\
    \ knowledge this study is the first report of (i) iden-\ntifying discriminating\
    \ wavelengths in the VIS–NIR-SWIR \nregion for the detection of HH, (ii) application\
    \ of short \nwave infrared hyperspectral imaging to detect growth \nanomalies\
    \ in vitro, (iii) proposing a spectral classifier \nfor hyperhydricity. Wavelength\
    \ bands (around 1940 nm, \n1450 nm, 1200 nm and 970 nm) associated with absorp-\n\
    tion of water are the most distinguishable between hype-\nrhydric and normal leaves\
    \ within the analyzed spectral \ndata set (400 nm to 2000 nm). In addition, minor\
    \ impor-\ntant wavelengths were found in the RGB region (around \n430 nm and 680 nm),\
    \ whereas the NIR region seemed to \nbe less important. Furthermore, RGB images\
    \ of hyperhy-\ndric explants contain sufficient morphological and spectral \n\
    features to allow a reliable detection of HH in an afforda-\nble manner via convolutional\
    \ neuronal networks. However, \nthis needs to be proven in an in-depth study.\
    \ Nonetheless, \nthese results can serve as a proof-of-concept for CNN-\nassisted\
    \ live monitoring of plant tissue cultures and pave \nthe way for increased use\
    \ of CNN to estimate other key \nparameters such as multiplication rate, nutrient\
    \ deficiency, \nand contamination.\nSupplementary Information The online version\
    \ contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/\
    \ s11240- 023- 02528-0.\nAcknowledgements We thank the technical assistants Ewa\
    \ Schneider \nand Bärbel Ernst of the department of Woody Plant and Propagation\
    \ \nPhysiology, Institute of Horticultural Production Systems, Leibniz Uni-\n\
    versität Hannover for their excellent support in the lab. Furthermore, \nwe thank\
    \ Matthias Igelbrink and Prof. Dr. Arno Ruckelshausen at Uni-\nversity of Applied\
    \ Science Osnabrück in their support in recording the \nSWIR-HSI data. In addition,\
    \ we are grateful for the scholarship for \nthe completion of a dissertation of\
    \ the University of Applied Science \nOsnabrück.\nAuthor contributions HB and\
    \ TW designed the experiments, HB and \nZM performed the experiments and analysed\
    \ the data. HB wrote the \nmanuscript and HB, ZM, TR, TW revised the manuscript.\
    \ All the \nauthors discussed the results and collectively edited the manuscript.\
    \ \nAll authors read and approved the final manuscript.\nFunding Open Access funding\
    \ enabled and organized by Projekt \nDEAL. This project took place within the\
    \ research project “Experi-\nmentierfeld Agro-Nordwest”, which is funded by the\
    \ Federal Ministry \nof Food and Agriculture (BMEL, Grant No.: 28DE103F18) via\
    \ the \nFederal Agency for Agriculture and Food (BLE).\nData availability The\
    \ datasets generated during the current study are \navailable from the corresponding\
    \ author on reasonable request. RGB \nimage dataset analysed during the current\
    \ study available in the Bethge \n(2023) repository, [https:// unive rse. robofl\
    \ ow. com/ hains/ hh- detec tion- \nin- vitro/ datas et/8].\nDeclarations \nCompeting\
    \ interests The authors declare no competing interests.\nEthical approval The\
    \ research work was carried out in compliance with \nthe ethical standards that\
    \ do not involve the use of humans.\nOpen Access This article is licensed under\
    \ a Creative Commons Attri-\nbution 4.0 International License, which permits use,\
    \ sharing, adapta-\ntion, distribution and reproduction in any medium or format,\
    \ as long \nas you give appropriate credit to the original author(s) and the source,\
    \ \nprovide a link to the Creative Commons licence, and indicate if changes \n\
    were made. The images or other third party material in this article are \nincluded\
    \ in the article’s Creative Commons licence, unless indicated \notherwise in a\
    \ credit line to the material. If material is not included in \nthe article’s\
    \ Creative Commons licence and your intended use is not \npermitted by statutory\
    \ regulation or exceeds the permitted use, you will \nneed to obtain permission\
    \ directly from the copyright holder. To view a \ncopy of this licence, visit\
    \ http://creativecommons.org/licenses/by/4.0/.\nReferences\nArthur GD, Stirk WA,\
    \ Van Staden J, Thomas TH (2004) Screening of \naqueous extracts from gelling\
    \ agents (Agar and Gelrite) for root-\nstimulating activity. S Afr J Bot 70(4):595–601.\
    \ https:// doi. org/ \n10. 1016/ S0254- 6299(15) 30197-6\nAynalem HM, Righetti\
    \ TL, Reed BM (2006) Non-destructive evalu-\nation of in vitro-stored plants:\
    \ a comparison of visual and image \nanalysis. In Vitro Cell Dev Biol-Plant 42(6):562–567.\
    \ https:// doi. \norg/ 10. 1079/ IVP20 06816\nBarbas E, Jay-Allemand C, Doumas\
    \ P, Chaillou S, Cornu D (1993) \nEffects of gelling agents on growth, mineral\
    \ composition and \nnaphthoquinone content of in vitro explants of hybrid walnut\
    \ tree \n(Juglans regia × Juglans nigra). Annales Des Sci for 50(2):177–\n186.\
    \ https:// doi. org/ 10. 1051/ forest: 19930 205\nBethge H (2023) HH Detection\
    \ in vitro Image Dataset. https:// unive \nrse. robofl ow. com/ hains/ hh- detec\
    \ tion- in- vitro/ datas et/8. Accessed \n10 Feb 2023\nBethge H, Winkelmann T,\
    \ Lüdeke P (2023) Rath T (2023) Low-cost \nand automated phenotyping system “Phenomenon”\
    \ for multi-\nsensor in situ monitoring in plant in vitro culture. Plant Methods\
    \ \n19(1):1–25. https:// doi. org/ 10. 1186/ s13007- 023- 01018-w\nBock Biosciences\
    \ GmbH (2018)  RoBo®Cut. https:// www. robot ec- \nptc. com/. Accessed 14 Feb\
    \ 2023\nBradski G (2000) The openCV library. Dr. Dobb’s J Softw Tools Prof \n\
    Progr 25(11):120–123\nCardoso JC, Sheng Gerald LT, Teixeira da Silva JA (2018)\
    \ Micro-\npropagation in the twenty-first century. In: Loyola-Vargas VM, \nOchoa-Alejo\
    \ N (eds) Plant cell culture protocols. Springer, \nDordrecht, pp 17–46\n571\n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\nCasanova\
    \ E, Moysset L, Trillas MI (2008) Effects of agar concentra-\ntion and vessel\
    \ closure on the organogenesis and hyperhydricity \nof adventitious carnation\
    \ shoots. Biol Plant 52:1–8. https:// doi. \norg/ 10. 1007/ s10535- 008- 0001-z\n\
    Chakrabarty D, Hahn EJ, Yoon YJ, Paek KY (2003) Micropropaga-\ntion of apple rootstock\
    \ M. 9 EMLA using bioreactor. J Hortic \nSci Biotechnol 78(5):605–609. https://\
    \ doi. org/ 10. 1080/ 14620 \n316. 2003. 11511 671\nChen C (2016) Cost analysis\
    \ of plant micropropagation of Phalae-\nnopsis. Plant Cell, Tis Organ Cult 126(1):167–175.\
    \ https:// doi. \norg/ 10. 1007/ s11240- 016- 0987-4\nCurran PJ (1989) Remote\
    \ sensing of foliar chemistry. Remote Sens \nEnviron 30(3):271–278. https:// doi.\
    \ org/ 10. 1016/ 0034- 4257(89) \n90069-2\nde Klerk GJ, Pramanik D (2017) Trichloroacetate,\
    \ an inhibitor of \nwax biosynthesis, prevents the development of hyperhydricity\
    \ in \nArabidopsis seedlings. Plant Cell, Tiss Organ Cult 131(1):89–\n95. https://\
    \ doi. org/ 10. 1007/ s11240- 017- 1264-x\nDebergh P, Aitken-Christie J, Cohen\
    \ D, Grout B, Von Arnold S, Zim-\nmerman R, Ziv M (1992) Reconsideration of the\
    \ term ‘vitrifica-\ntion’ as used in micropropagation. Plant Cell, Tissue Organ\
    \ Cult \n30(2):135–140. https:// doi. org/ 10. 1007/ BF000 34307\nDhondt S, Gonzalez\
    \ N, Blomme J, De Milde L, Van Daele T, Van \nAkoleyen D, Storme V, Coppens F,\
    \ Beemster TS, Inzé D (2014) \nHigh-resolution time-resolved imaging of in vitro\
    \ Arabidopsis \nrosette growth. Plant J 80(1):172–184. https:// doi. org/ 10.\
    \ 1111/ \ntpj. 12610\nDwyer B, Nelson J, Solawetz J (2021) Roboflow python package.\
    \ \nhttps:// github. com/ robofl ow/ robofl ow- python. Accessed 10 Feb \n2023\n\
    Dwyer B, Nelson J, Solawetz J (2022) Roboflow (v1.0). https:// robof \nlow. com.\
    \ Accessed 14 Feb 2023\nFischler MA, Bolles RC (1981) Random sample consensus:\
    \ a para-\ndigm for model fitting with applications to image analysis and \nautomated\
    \ cartography. Commun ACM 24(6):381–395. https:// \ndoi. org/ 10. 1145/ 358669.\
    \ 358692\nFranck T, Crèvecoeur M, Wuest J, Greppin H, Gaspar T (1998) \nCytological\
    \ comparison of leaves and stems of Prunus avium \nL. shoots cultured on a solid\
    \ medium with agar or gelrite. Bio-\ntechnic Histochem 73(1):32–43. https:// doi.\
    \ org/ 10. 3109/ 10520 \n29980 91405 04\nGamborg OL, Miller R, Ojima K (1968)\
    \ Nutrient requirements of sus-\npension cultures of soybean root cells. Exp Cell\
    \ Res 50(1):151–\n158. https:// doi. org/ 10. 1016/ 0014- 4827(68) 90403-5\nGao\
    \ BC (1996) NDWI—A normalized difference water index for \nremote sensing of vegetation\
    \ liquid water from space. Remote \nSens Environ 58(3):257–266. https:// doi.\
    \ org/ 10. 1016/ S0034- \n4257(96) 00067-3\nGao H, Xia X, An L, Xin X, Liang Y\
    \ (2017) Reversion of hyperhydric-\nity in pink (Dianthus chinensis L.) plantlets\
    \ by AgNO3 and its \nassociated mechanism during in vitro culture. Plant Sci 254:1–1.\
    \ \nhttps:// doi. org/ 10. 1016/j. plant sci. 2016. 10. 008\nGehan MA, Fahlgren\
    \ N, Abbasi A, Berry JC, Callen ST, Chavez L, \nDoust AN, Feldman MJ, Gilbert\
    \ KB, Hodge JG, Hoyer JS (2017) \nPlantCV v2: image analysis software for high-throughput\
    \ plant \nphenotyping. PeerJ 5:e4088. https:// doi. org/ 10. 7717/ peerj. 4088\n\
    George EF, Hall MA, De Klerk GJ (2008) Plant propagation by tissue \nculture.\
    \ In: George EF, Hall MA, De Klerk G-J (eds) Volume I. \nThe background. Plant\
    \ propagation by tissue culture. Springer, \nDordrecht\nGitelson AA, Keydan GP,\
    \ Merzlyak MN (2006) Three-band model \nfor noninvasive estimation of chlorophyll,\
    \ carotenoids, and \nanthocyanin contents in higher plant leaves. Geophys Res\
    \ Lett. \nhttps:// doi. org/ 10. 1029/ 2006G L0264 57\nGribble K (1999) The influence\
    \ of relative humidity on vitrifica-\ntion, growth and morphology of Gypsophila\
    \ paniculata L. Plant \nGrowth Regul 27(3):181–190. https:// doi. org/ 10. 1023/A:\
    \ 10062 \n35229 848\nGupta SD, Karmakar A (2017) Machine vision based evaluation\
    \ of \nimpact of light emitting diodes (LEDs) on shoot regeneration \nand the\
    \ effect of spectral quality on phenolic content and anti-\noxidant capacity in\
    \ Swertia chirata. J Photochem Photobiol, B \n174:162–172. https:// doi. org/\
    \ 10. 1016/j. jphot obiol. 2017. 07. 029\nHesami M, Jones AM (2020) Application\
    \ of artificial intelligence \nmodels and optimization algorithms in plant cell\
    \ and tissue cul-\nture. Appl Microbiol Biotechnol 104(22):9449–9485. https://\
    \ \ndoi. org/ 10. 1007/ s00253- 020- 10888-2\nHonda H, Takikawa N, Noguchi H,\
    \ Hanai T, Kobayashi T (1997) \nImage analysis associated with a fuzzy neural\
    \ network and \nestimation of shoot length of regenerated rice callus. J Fer-\n\
    ment Bioeng 84(4):342–347. https:// doi. org/ 10. 1016/ S0922- \n338X(97) 89256-2\n\
    Huang YJ, Lee FF (2010) An automatic machine vision-guided \ngrasping system for\
    \ Phalaenopsis tissue culture plantlets. \nComput Electron Agric 70(1):42–51.\
    \ https:// doi. org/ 10. 1016/j. \ncompag. 2009. 08. 011\nIvanova M, Van Staden\
    \ J (2011) Influence of gelling agent and cyto-\nkinins on the control of hyperhydricity\
    \ in Aloe polyphylla. Plant \nCell, Tissue Organ Cult 104(1):13–21. https:// doi.\
    \ org/ 10. 1007/ \ns11240- 010- 9794-5\nJocher G, Chaurasia, A, Qiu J (2023) YOLO\
    \ by Ultralytics (Ver-\nsion 8.0.0). https:// github. com/ ultra lytics/ ultra\
    \ lytics. Accessed \n14 Feb 2023\nKemat N (2020) Improving the quality of tissue-cultured\
    \ plants by \nfixing the problems related to an inadequate water balance, \nhyperhydricity.\
    \ Doctoral dissertation, Wageningen University \nand Research. https:// doi. org/\
    \ 10. 18174/ 517434\nKemat N, Visser RG, Krens FA (2021) Hypolignification: a\
    \ decisive \nfactor in the development of hyperhydricity. Plants 10(12):2625.\
    \ \nhttps:// doi. org/ 10. 3390/ plant s1012 2625\nKevers C, Coumans M, Coumans-Gillès\
    \ MF, Caspar TH (1984) \nPhysiological and biochemical events leading to vitrification\
    \ \nof plants cultured in vitro. Physiol Plant 61(1):69–74. https:// \ndoi. org/\
    \ 10. 1111/j. 1399- 3054. 1984. tb061 02.x\nKevers C, Prat R, Gaspar T (1987)\
    \ Vitrification of carnation in vitro: \nchanges in cell wall mechanical properties,\
    \ cellulose and lignin \ncontent. Plant Growth Regul 5(1):59–66. https:// doi.\
    \ org/ 10. \n1007/ BF000 35020\nKuhn M (2008) Building predictive models in R\
    \ using the caret pack-\nage. J Stat Softw 28:1–26. https:// doi. org/ 10. 18637/\
    \ jss. v028. i05\nLee TJ, Zobayed SM, Firmani F, Park EJ (2019) A novel auto-\n\
    mated transplanting system for plant tissue culture. Biosys Eng \n181:63–72. https://\
    \ doi. org/ 10. 1016/j. biosy stems eng. 2019. 02. \n012\nLehnert LW, Meyer H,\
    \ Obermeier WA, Silva B, Regeling B, Bendix J \n(2018) Hyperspectral data analysis\
    \ in R: the hsdar package. arXiv \nPreprint. https:// doi. org/ 10. 48550/ arXiv.\
    \ 1805. 05090\nLiakos KG, Busato P, Moshou D, Pearson S, Bochtis D (2018) Machine\
    \ \nlearning in agriculture: a review. Sensors 18(8):2674. https:// doi. \norg/\
    \ 10. 3390/ s1808 2674\nLizárraga A, Fraga M, Ascasíbar J, González ML (2017)\
    \ In vitro propa-\ngation and recovery of eight apple and two pear cultivars held\
    \ in \na germplasm bank. Am J Plant Sci 8(9):2238–2254. https:// doi. \norg/ 10.\
    \ 4236/ ajps. 2017. 89150\nMahendra PVS, Gupta SD (2004) Trichromatic sorting\
    \ of in vitro \nregenerated plants of gladiolus using adaptive resonance theory.\
    \ \nCurr Sci 10:348–353\nMarques MP, Martins J, de Carvalho LA, Zuzarte MR, da\
    \ Costa RM, \nCanhoto J (2021) Study of physiological and biochemical events \n\
    leading to vitrification of Arbutus unedo L. cultured in vitro. Trees \n35:241–253.\
    \ https:// doi. org/ 10. 1007/ s00468- 020- 02036-0\n572\n \nPlant Cell, Tissue\
    \ and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nMestre D, Fonseca JM, Mora\
    \ A (2017) Monitoring of in-vitro plant \ncultures using digital image processing\
    \ and random forests. 8th \nInternational Conference on Pattern Recognition Systems.\
    \ https:// \ndoi. org/ 10. 1049/ cp. 2017. 0137\nMohamed SM, El-Mahrouk ME, El-Banna\
    \ AN, Hafez YM, El-Ramady \nH, Abdalla N, Dobránszki J (2023) Optimizing medium\
    \ composi-\ntion and environmental culture condition enhances antioxidant \nenzymes,\
    \ recovers Gypsophila paniculata L. hyperhydric shoots \nand improves rooting\
    \ in vitro. Plants 12(2):306. https:// doi. org/ \n10. 3390/ plant s1202 0306\n\
    Murashige T, Skoog F (1962) A revised medium for rapid growth and \nbio assays\
    \ with tobacco tissue cultures. Physiol Plant 15(3):473–\n497. https:// doi. org/\
    \ 10. 1111/j. 1399- 3054. 1962. tb080 52.x\nNezami-Alanagh E, Garoosi GA, Landín\
    \ M, Gallego PP (2019) \nComputer-based tools provide new insight into the key\
    \ fac-\ntors that cause physiological disorders of pistachio rootstocks \ncultured\
    \ in vitro. Sci Rep 9(1):1–5. https:// doi. org/ 10. 1038/ \ns41598- 019- 46155-2\n\
    Paques M, Boxus P, Dulos M (1985) “ Vitrification”: an induceable \nand reversible\
    \ phenomenon. In: symposium on in vitro problems \nrelated to mass propagation\
    \ of horticultural plants 212. pp 253–\n258. https:// doi. org/ 10. 17660/ ActaH\
    \ ortic. 1987. 212. 38\nPasqualetto PL, Zimmerman RH, Fordham I (1988) The influence\
    \ of \ncation and gelling agent concentrations on vitrification of apple \ncultivars\
    \ in vitro. Plant Cell, Tissue Organ Cult 14(1):31–40. \nhttps:// doi. org/ 10.\
    \ 1007/ BF000 29573\nPatrício DI, Rieder R (2018) Computer vision and artificial\
    \ intelligence \nin precision agriculture for grain crops: a systematic review.\
    \ Com-\nput Electron Agric 153:69–81. https:// doi. org/ 10. 1016/j. compag. \n\
    2018. 08. 001\nPeterson RA, Peterson MR (2020) Package ‘bestNormalize’. Normal-\n\
    izing transformation functions. R package version\nPhan CT, Letouze R (1983) A\
    \ comparative study of chlorophyll, \nphenolic and protein contents, and of hydroxycinnamate:\
    \ CoA \nligase activity of normal and ‘vitreous’ plants (Prunus avium L.) \nobtained\
    \ in vitro. Plant Sci Lett 31(2–3):323–327. https:// doi. org/ \n10. 1016/ 0304-\
    \ 4211(83) 90071-8\nPinheiro J, Bates D, DebRoy S, Sarkar D, Heisterkamp S, Van\
    \ Willigen \nB, Maintainer R (2017) Package ‘nlme.’ Linear Nonlinear Mixed \n\
    Eff Models Vers 3(1):274\nPrasad VS, Gupta SD (2008) Applications and potentials\
    \ of artificial \nneural networks in plant tissue culture. In: Gupta SD, Ibaraki\
    \ Y \n(eds) Plant tissue culture engineering. Springer, Dordrecht, pp \n47–67.\
    \ https:// doi. org/ 10. 1007/ 978-1- 4020- 3694-1_3\nQi Y, Heisler GM, Gao W,\
    \ Vogelmann TC, Bai S (2010) Character-\nistics of UV-B radiation tolerance in\
    \ broadleaf trees in southern \nUSA. In: Gao W, Slusser JR, Schmoldt DL (eds)\
    \ UV radiation in \nglobal climate change. Springer, Berlin, Heidelberg\nRedmon\
    \ J, Divvala S, Girshick R, Farhadi A (2016). You only look \nonce: Unified, real-time\
    \ object detection. In: Proceedings of the \nIEEE conference on computer vision\
    \ and pattern recognition, pp. \n779–788. https:// doi. org/ 10. 1109/ CVPR. 2016.\
    \ 91\nRipley B, Venables W, Ripley MB (2016) Package ‘nnet’. R Package \nversion.\
    \ 2;7(3–12):700\nRojas-Martínez L, Visser RG, De Klerk GJ (2010) The hyperhydricity\
    \ \nsyndrome: waterlogging of plant tissues as a major cause. Propag \nOrnam Plants\
    \ 10(4):169–175\nRstudio Team (2015) RStudio: integrated development for R. RStudio.\
    \ \nInc., Boston, p 879\nSaher S, Fernández-García N, Piqueras A, Hellín E, Olmos\
    \ E (2005) \nReducing properties, energy efficiency and carbohydrate metab-\n\
    olism in hyperhydric and normal carnation shoots cultured \nin vitro: a hypoxia\
    \ stress? Plant Physiol Biochem 43(6):573–\n582. https:// doi. org/ 10. 1016/j.\
    \ plaphy. 2005. 05. 006\nSakamoto Y, Ishiguro M, Kitagawa G (1986) Akaike information\
    \ \ncriterion statistics. D. Reidel, Dordrecht. https:// doi. org/ 10. \n2307/\
    \ 29830 28\nScherer PA (1987) Standardization of plant micropropagation by \n\
    usage of a liquid medium with polyurethane foam plugs or a \nsolidified medium\
    \ with the gellan gum gelrite instead of agar. \nIn: International Symposium on\
    \ Propagation of Ornamental \nPlants 226. pp. 107–114. https:// doi. org/ 10.\
    \ 17660/ ActaH ortic. \n1988. 226. 10\nScherer PA, Müller E, Lippert H, Wolff\
    \ G (1988) Multielement analysis \nof agar and gelrite impurities investigated\
    \ by inductively coupled \nplasma emission spectrometry as well as physical properties\
    \ of \ntissue culture media prepared with agar or the gellan gum gelrite. \nIn:\
    \ International Symposium on Propagation of Ornamental Plants \n226, pp 655–658.\
    \ https:// doi. org/ 10. 17660/ ActaH ortic. 1988. 226. \n91\nSerrano L, Penuelas\
    \ J, Ustin SL (2002) Remote sensing of nitrogen and \nlignin in Mediterranean\
    \ vegetation from AVIRIS data: decom-\nposing biochemical from structural signals.\
    \ Remote Sens Envi-\nron 81(2–3):355–364. https:// doi. org/ 10. 1016/ S0034-\
    \ 4257(02) \n00011-1\nShaw DR, Kelley FS (2005) Evaluating remote sensing for\
    \ determining \nand classifying soybean anomalies. Precision Agric 6(5):421–429.\
    \ \nhttps:// doi. org/ 10. 1007/ s11119- 005- 3681-9\nSingh A, Thakur N, Sharma\
    \ A (2016) A review of supervised machine \nlearning algorithms. In2016 3rd international\
    \ conference on \ncomputing for sustainable global development (INDIACom). pp.\
    \ \n1310–1315. https:// doi. org/ 10. 35940/ ijsce. E3583. 11125 22\nSmith MA,\
    \ Spomer L (1995) Vessels, gels, liquid media, and sup-\nport systems. In: Aitken-Christie\
    \ J, Kozai T, Smith MAL (eds) \nAutomation and environmental control in plant\
    \ tissue culture. \nSpringer, Dordrecht, pp 371–404. https:// doi. org/ 10. 1007/\
    \ 978- \n94- 015- 8461-6_ 16\nSmith MA, Spomer L, Meyer MJ, McClelland MT (1989)\
    \ Non-invasive \nimage analysis evaluation of growth during plant micropropaga-\n\
    tion. Plant Cell, Tissue Organ Cult 19(2):91–102. https:// doi. org/ \n10. 1007/\
    \ BF000 35809\nSommer C, Straehle C, Koethe U, Hamprecht FA (2011) Ilastik: Inter-\n\
    active learning and segmentation toolkit. In: 2011 IEEE interna-\ntional symposium\
    \ on biomedical imaging: From nano to macro. \npp. 230–233. https:// doi. org/\
    \ 10. 1109/ ISBI. 2011. 58723 94\nSullivan C, Kaszynski A (2019) PyVista: 3D plotting\
    \ and mesh analy-\nsis through a streamlined interface for the Visualization Toolkit\
    \ \n(VTK). J Open Sour Softw 4(37):1450. https:// doi. org/ 10. 21105/ \njoss.\
    \ 01450\nTerry ME, Bonner BA (1980) An examination of centrifugation as \na method\
    \ of extracting an extracellular solution from peas, and \nits use for the study\
    \ of indoleacetic acid-induced growth. Plant \nPhysiol 66(2):321–325. https://\
    \ doi. org/ 10. 1104/ pp. 66.2. 321\nThiel M (2018) Bildgebende NIR-Hyperspektral-Technologie\
    \ zur in-\nsitu Erfassung des Blattwassergehalts. Doctoral dissertation, Leib-\n\
    niz Universität Hannover. https:// doi. org/ 10. 15488/ 3882\nTian J, Jiang F,\
    \ Wu Z (2015) The apoplastic oxidative burst as a \nkey factor of hyperhydricity\
    \ in garlic plantlet in vitro. Plant \nCell Tiss Organ Cult 120(2):571–584. https://\
    \ doi. org/ 10. 1007/ \ns11240- 014- 0623-0\nTisserand S (2021) Vis-NIR hyperspectral\
    \ cameras. Photoniques \n110:58–64\nTsay HS, Lee CY, Agrawal DC, Basker S (2006)\
    \ Influence of ventila-\ntion closure, gelling agent and explant type on shoot\
    \ budprolifera-\ntion and hyperhydricity in Scrophularia yoshimurae—a medicinal\
    \ \nplant. In Vitro Cell Dev Biol-Plant 42(5):445–449.https:// doi. org/ \n10.\
    \ 1079/ IVP20 06791\nvan Altvorst AC, Koehorst H, de Jong J, Dons HJ (1996) Transgenic\
    \ \ncarnation plants obtained by Agrobacterium tumefaciens-mediated \n573\nPlant\
    \ Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\ntransformation\
    \ of petal explants. Plant Cell, Tissue Organ Cult \n45(2):169–173. https:// doi.\
    \ org/ 10. 1007/ BF000 48762\nvan den Dries N, Giannì S, Czerednik A, Krens FA,\
    \ de Klerk GJ (2013) \nFlooding of the apoplast is a key factor in the development\
    \ of \nhyperhydricity. J Exp Bot 64(16):5221–5230. https:// doi. org/ 10. \n1093/\
    \ jxb/ eru497\nVan Der Walt S, Colbert SC, Varoquaux G (2011) The NumPy array:\
    \ \na structure for efficient numerical computation. Comput Sci Eng \n13(2):22–30.\
    \ https:// doi. org/ 10. 48550/ arXiv. 1102. 1523\nVan Rossum G, Drake FL (2009)\
    \ Python 3 Reference manual: python \ndocumentation manual part 2. Scotts Valley,\
    \ CA: CreateSpace\nVieitez AM, Ballester A, San-José MC, Vieitez E (1985) Anatomical\
    \ \nand chemical studies of vitrified shoots of chestnut regenerated \nin vitro.\
    \ Physiol Plant 65(2):177–184. https:// doi. org/ 10. 17660/ \nActaH ortic. 1987.\
    \ 212. 34\nVoesenek LA, Blom CW (1989) Growth responses of Rumex spe-\ncies in\
    \ relation to submergence and ethylene. Plant Cell Environ \n12(4):433–439. https://\
    \ doi. org/ 10. 1111/j. 1365- 3040. 1989. tb019 \n59.x\nVreeburg RA, Benschop\
    \ JJ, Peeters AJ, Colmer TD, Ammerlaan AH, \nStaal M, Elzenga TM, Staals RH, Darley\
    \ CP, McQueen-Mason SJ, \nVoesenek LA (2005) Ethylene regulates fast apoplastic\
    \ acidifica-\ntion and expansin A transcription during submergence-induced \n\
    petiole elongation in Rumex palustris. Plant J 43(4):597–610. \nhttps:// doi.\
    \ org/ 10. 1111/j. 1365- 313X. 2005. 02477.x\nZhang C, Timmis R, Hu WS (1999)\
    \ A neural network based pattern \nrecognition system for somatic embryos of Douglas\
    \ fir. Plant Cell, \nTissue Organ Cult 56:25–35. https:// doi. org/ 10. 1023/A:\
    \ 10062 \n87917 534\nZhou QY, Park J, Koltun V (2018) Open3D: a modern library\
    \ for 3D \ndata processing. arXiv Preprint. https:// doi. org/ 10. 48550/ arXiv.\
    \ \n1801. 09847\nZiv M (1991) Vitrification: morphological and physiological disorders\
    \ \nof in vitro plants. In: Debergh PC, Zimmerman RH (eds) Micro-\npropagation.\
    \ Springer, Dordrecht, pp 45–69. https:// doi. org/ 10. \n1007/ 978- 94- 009-\
    \ 2075-0_4\nPublisher's Note Springer Nature remains neutral with regard to \n\
    jurisdictional claims in published maps and institutional affiliations.\n"
  inline_citation: '>'
  journal: Plant cell, tissue and organ culture (Print)
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/s11240-023-02528-0.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Towards automated detection of hyperhydricity in plant in vitro culture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1101/2020.08.27.263186
  analysis: '>'
  authors:
  - Junfeng Gao
  - Jesper Cairo Westergaard
  - Ea Høegh Riis Sundmark
  - Merethe Bagge
  - Erland Liljeroth
  - Erik Alexandersson
  citation_count: 1
  full_citation: '>'
  full_text: ">\nAutomatic late blight lesion recognition and severity quantification\
    \ based on \nfield imagery of diverse potato genotypes by deep learning \nJunfeng\
    \ Gao1,*, Jesper Cairo Westergaard2, Ea Høegh Riis Sundmark3, Merethe Bagge3,\
    \  \nErland Liljeroth4, Erik Alexandersson4,* \n \n \n1, Lincoln Agri-Robotics,\
    \ Lincoln Institute for Agri-Food Technology, University of Lincoln, Lincoln,\
    \ UK \n2, Department of Plant and Environmental Sciences, University of Copenhagen,\
    \ Taastrup, Denmark \n3, Danespo Breeding Company, Give, Denmark \n4, Department\
    \ of Plant Protection Biology, Swedish University of Agricultural Sciences, Alnarp,\
    \ \nSweden \n \nAbstract:  \nThe plant pathogen Phytophthora infestans causes\
    \ the severe disease late blight in potato, which \nresults in a huge loss for\
    \ potato production. Automatic and accurate disease lesion segmentation \nenables\
    \ fast evaluation of disease severity and assessment of disease progress for precision\
    \ crop \nbreeding. Deep learning has gained tremendous success in computer vision\
    \ tasks for image \nclassification, object detection and semantic segmentation.\
    \ To test whether we could extract late \nblight lesions from unstructured field\
    \ environments based on high-resolution visual field images and \ndeep learning\
    \ algorithms, we collected ~500 field RGB images in a set of diverse potato genotypes\
    \ \nwith different disease severity (0-70%), resulting in 2100 cropped images.\
    \ 1600 of these cropped \nimages were used as the dataset for training deep neural\
    \ networks. Finally, the developed model was \ntested on the 250 cropped images.\
    \ The results show that the intersection over union (IoU) values of \nbackground\
    \ (leaf and soil) and disease lesion classes in the test dataset are 0.996 and\
    \ 0.386, \nrespectively. Furthermore, we established a linear relationship (R2\
    \ = 0.655) between manual visual \nscores of late blight and the number of lesions\
    \ at the canopy level. We also learned that imbalance \nweights of lesion and\
    \ background classes improved segmentation performance, and that fused \nmasks\
    \ based on the majority voting of the multiple masks enhanced the correlation\
    \ with the visual \nscores. This study demonstrates the feasibility of using deep\
    \ learning algorithms for disease lesion \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nsegmentation\
    \ and severity evaluation based on proximal imagery for crop resistance breeding\
    \ in field \nenvironments. \n1. Introduction \nCrop disease poses a threat to\
    \ global food security [1]. Automated field phenotyping can become a \npowerful\
    \ tool for future resistance breeding as well as for precision agriculture [2][3],and\
    \ can thus be \na successful way to mitigate crop disease. Potato is today the\
    \ third most important food crop and is \nan important part of many diets, especially\
    \ in temperate climates. The oomycete Phytophthora \ninfestans (Mont.) de Bary\
    \ which causes potato late blight (PLB) and potato tuber blight (PTB) can be \n\
    very destructive in potato cultivation if it is not managed (Wiik, Rosenqvist,\
    \ & Liljeroth, 2018). In \npractice, the prevention of PLB is in the field is\
    \ highly relying on regular blanket spraying of fungicide \nduring the growth\
    \ season. As an example, in Sweden the potato production consumes around 20% of\
    \ \nall fungicides used in agriculture, largely to combat P. infestans, in spite\
    \ of occupying less than 1% of \nthe area under cultivation [4]. In addition,\
    \ PLB prevention requires frequent use of fungicides with \nsometimes more than\
    \ 10 applications per growth season in Northern Europe to avoid significant \n\
    yield loss.  This management is effective in general and widely accepted by farmers,\
    \ but also results \nin usage of large amounts of fungicide as well as fossil\
    \ fuels, which hampers the sustainable \ndevelopment of agriculture. \nThe loss\
    \ caused by PLB can be reduced by breeding PLB resistant cultivars. To breed for\
    \ high PLB \nresistance, plant breeders establish experimental plots to quantify\
    \ the PLB severity of different \npotato genotypes and progeny lines. This is\
    \ currently manually done by estimating visual scores \nbased on the number and\
    \ area of lesions on plants [4]. This process is time consuming, can be \nsubjective\
    \ and also requires experienced raters for visual scoring. Therefore, it is highly\
    \ needed to \ndevelop an automated disease evaluation system to facilitate and\
    \ speed up the breeding processes. \nHowever, one of the main challenges in automating\
    \ the system is to accurately segment the lesions \nunder field conditions. There\
    \ are some studies which have employed PLB lesion segmentation. For \nexample,\
    \ Abdu et al [5] developed a pattern recognition approach to recognize early blight,\
    \ caused \nby Alternaria solani, and PLB visual disease symptoms using soft computing\
    \ and machine learning \nalgorithms. Barbedo [6] and Camargo et al [7] also carried\
    \ out similar studies on plant disease \ndetection and segmentation. However,\
    \ the image datasets used in these studies above are collected \nat foliage level\
    \ under relatively clear and uniform backgrounds in controlled settings and the\
    \ \npipelines are not readily implemented into large scale field environments.\
    \ Moreover, the majority of \nprevious works have only investigated the symptom\
    \ segmentation problems based on a single potato \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    cultivar. These models might fail to segment other lesions due to differences\
    \ in lesion morphology \nand leaf color between different potato cultivars. \n\
    Attempts have also been made for early detection of PLB. Various sensors from\
    \ imaging to non-\nimaging sensors have been employed in these applications. For\
    \ example, Fernández et al [8] \ninvestigated the classification accuracy changes\
    \ of infested and healthy potato leaves over different \ndays post-inoculation\
    \ (DPI) with a spectroradiometer and a multispectral camera under structured \n\
    environments. Appeltans et al [9] discussed the imaging parameter settings for\
    \ hyperspectral and \nthermal proximal disease sensing in potato and leek fields\
    \ with a ground-based vehicle. Other than \nground-based platforms, an unmanned\
    \ aerial vehicle (UAV) platform equipped with imaging sensors \nalso has showed\
    \ its feasibility on field PLB monitoring with the detection of spectral changes\
    \ in crop \ntraits [10] [11]. \nDeep learning has proved to be an effective approach\
    \ for traditional computer vision problems such \nas image classification, object\
    \ detection and segmentation, as it is capable of extracting features \nhierarchically\
    \ [12]. In addition, the applications with deep learning in the agriculture domain\
    \ also \nshow unprecedented advancements. Specifically, in precision farming it\
    \ has been deployed for weed \ndetection [13], agricultural pest detection [14]\
    \ and selective fruit harvesting [15], as well as leaf \ncounting  [16]. Furthermore,\
    \ deep convolutional neural networks, one of the most used deep \nlearning algorithms,\
    \ combined with computer vision techniques have been exploited for crop disease\
    \ \nclassification and detection. Polder et al  [17] adapted a fully convolutional\
    \ neural network (FCN) for \npotato virus Y (PVY) detection based on hyperspectral\
    \ imagery. It proved that the deep learning-\nbased approach outperformed the,\
    \ conventional disease assessment and indicated the suitability of \nthis method\
    \ for real-world disease detection. Stewart et al [18] [19] [20] developed the\
    \ deep neural \nnetworks for northern leaf blight (NLB) in maize from field RGB\
    \ images collected from an unmanned \naerial vehicle (UAV) platform. By contrast,\
    \ the quantification and detection of PLB lesions have still \nbeen confined at\
    \ a laboratory scale. To the best of our knowledge, the use of deep convolutional\
    \ \nneural networks has not previously been explored for PLB lesion segmentation\
    \ in diverse potato \ngenotypes based on RGB imagery from the field. \nVisual\
    \ scoring in the field provides an important metric to quantify disease severity,\
    \ but is prone to \nbe biased and error can be subjected to raters. Thanks to\
    \ automation, effectivity and objectiveness, \nsensor-based measurement, especially\
    \ imaging sensors, provides potential advantages compared \nwith visual scoring.\
    \ Image-based analysis including images from RGB, multispectral and hyperspectral\
    \ \nsensors has measured disease severity under controlled conditions [21] or\
    \ based on PLACL \nCC-BY-NC-ND 4.0 International license.\navailable under a\n\
    (which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\n(Percentage of Leaf\
    \ Area covered by Lesions) at single leaf level [22], but is yet to demonstrate\
    \ its full \npotential for accurate estimation in field environments [23].   \n\
    The specific objectives of this study are (1) to evaluate the performances of\
    \ deep convolutional \nneural networks for PLB lesion segmentation; (2) to determinate\
    \ the optimal class weights for the \nclasses PLB disease lesion and background\
    \ (i.e. leaf and soil); (3) to fuse prediction masks at multiple \nscales for\
    \ more accurate lesion prediction; (4) to determine the correlation between visual\
    \ scoring \nand the number of lesions at the canopy level. The early pre-symptom\
    \ PLB detection is out of the \nscope of this study as only RGB images were analyzed.\
    \ \n2. Material and Methods \n2.1 Image data collection  \nThe images were acquired\
    \ with a hand-held RGB camera (Sony RX 100 iii) in nadir (+/- 5 degrees) at \n\
    approximately 40 cm over each canopy. The ISO, aperture and FOV of the camera\
    \ were set to 125, \nf/5.6 and 8.8mm, respectively. Images were acquired in full\
    \ cloudy, semi-cloudy or sunny light \nsettings. No flash was used. Especially\
    \ for the semi-cloudy and sunny acquisitions, consideration was \ntaken to ensure\
    \ no additional shade was being cast onto the canopy from person or camera.  No\
    \ \npost-processing regarding color correction was performed. \nThe field location\
    \ was outside Give, Denmark (N 55.859188, E 9.331065). Figure 1 shows a single\
    \ \nimage of the field from 100 meters above ground, at 15 Days After Infection\
    \ (DAI). It is clearly seen \nhow a large part of the trial field is decimated\
    \ by PLB. The trial was set up with guard rows of Oleva \n(cv.) around the trial\
    \ area. Each plot within the trial consisted of 4 plants in a 2x2 formation \n\
    with infector rows on each side and a 50 cm gap between plots. The total trial\
    \ consisted of 775 \ngenotypes: \n48 genotypes \nwith \nthree \nreplicates, \n\
    59 genotypes with \ntwo \nreplicates \nand \n513 genotypes represented \nby \n\
    single \nplots. Infector \nrows \nconsisted \nof \nalternating Bintje \n(cv.)\
    \ and Oleva (cv.).  \n \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\n \nFigure 1. A bird’s-eye\
    \ view of the experimental trial taken from a drone platform at 100 m above \n\
    ground (a); an image example at 15 DAI (b).  \n2.2 Manual visual scoring of PLB\
    \ \nManual visual scoring was done 2 times a week, starting 5 days after inoculation\
    \ of infector rows and \ncontinuing until the standard cultivar Robijn had reached\
    \ 50% infection. Infection of PLB at each time \npoint was scored as percentage\
    \ of leaf area infected according to the Euroblight protocol [24] with a \nsingle\
    \ lesion per plot scored as 0.1%, 2-5 lesions scored as 0.5%, 5-10 lesions scored\
    \ as 1%. The \nmanual detection of lesions was done by walking between the two\
    \ rows of each plot at a slow pace \nand looking at the plants at an angle. If\
    \ a lesion was spotted, it was further investigated for PLB \npresence and the\
    \ plot were investigated for presence of more lesions. Percentage of infection\
    \ \nwas used to create an area under disease progression curve (AUDPC) as follows:\
    \ \n                                                     AUDPC =∑\nA\0\x02\x03\
    \x04\x05 \x03 \x04T\0\x02\x03\x04\x05 \x06 T\0\x02\x05 \a\n\a\n\b\t\n        \
    \                                          (1) \nwhere T0 is innoculation date\
    \ and Ti is evaluation dates (i=1,2,3,4). Ai+1 is the percentage of infection\
    \ \nat Ti+1.  \n2.3 Image preprocessing  \nThere are 70 original images (5472x3648)\
    \ which were all labelled manually with the annotation \ntoolbox LabelMe [25].\
    \ The distribution of genotypes among these images is shown in Figure 2. The \n\
    majority of images are from the Bintje and Oleva potato cultivars. These two genotypes\
    \ are \nsusceptible to P. infestans, providing more disease lesions for training.\
    \ The original image is unable to \nbe directly fed to neural networks as the\
    \ spatial resolution of original images is too large and requires \nintensive\
    \ computation memories. It is also not advisable to shrink the whole image to\
    \ a small size, \nmaking processing possible but heavily degrading the quality\
    \ of small features in tiny lesion \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nannotations.\
    \ We first cut each original image into 6x5 (horizontal x vertical) sub-images\
    \ (912x730)   \nand then resized to 512x512 in order to feed to the neural networks\
    \ for training. Each original image \ncontributes 30 sub-images leading to 2100\
    \ sub-images in total. 1600 sub-images were randomly \nselected for the training\
    \ set and the remaining 500 sub-images were randomly separated equally as \nthe\
    \ validation set (250 sub-images) or the test set (250 sub-images). The same procedures\
    \ were \napplied to their corresponding ground truth images to construct image\
    \ pairs for training neural \nnetworks. \n  \nFigure 2. Histogram of raw image\
    \ genotypes, the red bar represents the number of images belonging \nto that genotype.\
    \ \n2.4 Deep learning   \nWe adopted an encoder-decoder neural network architecture\
    \ based on SegNet [26] for lesion \nsegmentation. The proposed network operates\
    \ on input images of 512x512 pixels and outputs \nsegmentation masks in the same\
    \ size as the input image. The architecture has an hourglass shape \nconsisting\
    \ of a bunch of convolutional and up-convolutional layers. A diagrammatic overview\
    \ of the \nnetwork is shown in Figure. 3. The encoder comprises a series of convolutional\
    \ operations, activation \nand pooling operations. Semantic features from low-level\
    \ to high-level could be extracted at the end \nof the encoder process. Because\
    \ of max-pooling layers, the spatial resolution output shrunk 2 times \ncompared\
    \ to the previous convolution block in the encoder phase. The index of the maximum\
    \ \nfeature value in each pooling window was recorded for each encoder feature\
    \ map (Figure 4). In \ncontrast, the decoder part upsamples its input feature\
    \ maps using the recorded max-pooling indices \nlearned from the corresponding\
    \ layers in encoder part and finally generates the prediction results \nwith same\
    \ spatial size as original input images. The upsampling layers, such as deconvolution\
    \ layers, \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which\
    \ was not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\nin the decoder are also capable\
    \ of learning a mapping from feature map to semantic segmentation \nresults. Each\
    \ convolutional layer is followed by several operational layers including a non-linear\
    \ ReLU \nactivation function (max(0, x)) and batch normalization. The total parameters\
    \ are 29,442,122 with \n29,434,694 trainable parameters and 7,428 non-trainable\
    \ parameters. \n \nFigure 3. The neural network is based on an encoder and decoder\
    \ architecture, followed by a final \npixel-wise classification layer.  \n \n\
    Figure 4. Example of max-pooling and upsampling with index in SegNet \nTable 1.\
    \ Details of the proposed deep neural network architecture. All the filter size\
    \ is 3 x 3 and \npadding mode is set as ‘same’ with zero-filled. The final decoder\
    \ output is fed to a softmax classifier \nto produce class probabilities for each\
    \ pixel.  \nModule Layers  \nDimensions  \nFeature maps Block structure  \nInput\
    \ \nInput (RGB) \n512 x 512 \n3 \n \nEn-coder \nConvolution 1 block  \n256 x 256\
    \ \n64 \n2xconvolution + pooling \nConvolution 2 block  \n128 x 128 \n128 \n2xconvolution\
    \ + pooling \nConvolution 3 block  \n64 x 64 \n256 \n3xconvolution + pooling \n\
    Convolution 4 block \n32 x 32 \n512 \n3xconvolution + pooling \nConvolution 5\
    \ block  \n16 x 16 \n512 \n3xconvolution + pooling \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    De-coder  \nUp \nconvolution \n1 \nblock  \n32 x 32 \n512 \n3xconvolution + upsampling\
    \ \nUp \nconvolution \n2 \nblock  \n64 x 64  \n256 \n3xconvolution + upsampling\
    \ \nUP \nconvolution \n3 \nblock  \n128 x 128 \n128 \n3xconvolution + upsampling\
    \ \nUP \nconvolution \n4 \nblock  \n256 x 256  \n64 \n2xconvolution + upsampling\
    \ \nUP \nconvolution \n5 \nblock  \n512 x 512  \n64 \n2xconvolution + upsampling\
    \ \nOutput  Softmax  \n512 x 512 \n2 \nconvolution + sigmoid \n \n2.5 Loss function\
    \ \nThe loss function is key for training a robust and high-performance network.\
    \ The most commonly \nused loss function for semantic segmentation is pixel-wise\
    \ cross-entropy loss. In our study, the \nfrequency of appearance for lesion and\
    \ background class is highly imbalanced. The number of \ndisease lesion pixels\
    \ is far less than other pixels such as healthy plant organs and soil backgrounds\
    \ in \nfield images at early infection stages. Only using standard loss function\
    \ without adaption would make \na deep neural network model tend to only correctly\
    \ classify dominant class pixels (backgrounds), \nignoring the importance of lesion\
    \ pixels. This is also called accuracy paradox, which a model provides \na very\
    \ high overall accuracy but performs poorly over classes.  One common way to mitigate\
    \ this \neffect is the use of a class-balancing approach by assigning different\
    \ weights over classes based on \ntheir median frequency [27]. In this study,\
    \ we regard the weights for each class as a hyperparameter \nto tune. Other than\
    \ weights calculated from Equation (2), we also compared the prediction \nperformance\
    \ with different weight ratios from 1 to 9 to select the optimal weight. The weighted\
    \ loss \nfunction used in the network is shown in Equation (3).    \n        \
    \                                                  \b\v \t\n\f\r\x0E\b\x0F\x10\
    _\x12\x13\r\x14\x15\r\x10\v\x16\n\x12\x13\r\x14\x15\r\x10\v\x16\0\v\x05\n    \
    \                                                                     (2) \nwhere\
    \ frequency(c) represents the frequency of occurrences of pixels of class c divided\
    \ by the total \nnumber of pixels in any images containing that class, and median_frequency\
    \ is the median of these \nfrequencies overall all classes. \n               \
    \                                       \v \t \x06\n\x04\n\x17 ∑\n∑\n\b\v\n\x18\
    \n\v\t\x04\n\x03 \f\x19,\v log \x10\x19,\v\n\x17\n\b\t\x04\n                 \
    \                               (3) \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nwhere\
    \ N is the number of observations, C is the number of classes (background and\
    \ lesion), Wc is the \nweight for class c, y is a binary indicator (0 or 1) if\
    \ a class label is correctly classified for observation o, \np is predicted probability\
    \ of observation o being of class c.  \n2.6 Training  \nThe network was trained\
    \ end to end from scratch with Adam optimizer [28] using a stable learning \n\
    rate 0.0001 to minimize the loss values. The batch size was set to 18 with 500\
    \ epochs in total. We \nemployed 3 Nvidia Tesla V100-SXM2 GPUs with around 32G\
    \ memory each for training the network. \nEach epoch took around 171s to finish,\
    \ accounting for 23.75 hours of training time in total. Data \naugmentation was\
    \ used to reduce the risk of overfitting in the training phase. Specifically,\
    \ in each \nbatch, cropping, horizontal or vertical flipping, and a zoom range\
    \ from 0.8 to 1.2 were randomly \napplied in the images and their corresponding\
    \ ground truth masks. All network training and \nvalidation were done using the\
    \ Tensorflow deep learning framework. The model was saved only with \nthe decline\
    \ of loss values in each epoch. The accuracy and loss values in the validation\
    \ dataset were \nrecorded as well in every epoch. \n2.7 Model evaluation  \nThe\
    \ model was evaluated with three standard metrics for semantic segmentation. The\
    \ three metrics \nare overall average accuracy, class average accuracy and mean\
    \ intersection over union (mIoU), \nrespectively. The calculations are listed\
    \ below. We also used confusion matrix to check how many \npixels of each class\
    \ are correctly classified. Overall average accuracy (Calculation (4)) measures\
    \ the \nperformance overall all pixels. The high value means that the majority\
    \ pixels are correctly classified \nbut does not indicate good lesion segmentation\
    \ as majority of pixels in our image are background. \nHence, this value is sometimes\
    \ quite biased for evaluating model performances. Class average \naccuracy (Calculation\
    \ (5)) averages the performance of each class. A high value represents good \n\
    performance across all classes. IoU (intersection over union), also known as Jaccard\
    \ index, is a \ncommonly used and effective metric in semantic segmentation. It\
    \ measures the area of overlap \nbetween the predicted segmentation and the ground\
    \ truth divided by the union area of the \npredicted segmentation and the ground\
    \ truth in labelled images. mIoU is calculated by averaging the \nIoU of each\
    \ class (Calculation (6)).                                                   \
    \             \n                                                          \n∑\n\
    \x10\0\0\x02\n\0\x03\x04\n∑\n∑\n\x10\0\x05\n\x02\n\x05\x03\x04\n\x02\n\0\x03\x04\
    \n                                                            (4) \n         \
    \                                                   \n\x04\n\v ∑\n\x10\0\0\x1C\
    \0\v\n\b\t\x04\n                                                             \
    \ (5)    \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was\
    \ not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\n                             \
    \                       \n\x04\n\v ∑\n\x10\0\0\x1C\0\x03∑\n\x10\x05\0\x02\n\x05\
    \x03\x04\n\x1D\x10\0\0\v\n\b\t\x04\n                                         \
    \           (6)                                                 \n \nwhere ti\
    \ is the total number of pixels of class i in ground truth image, nji is the number\
    \ of pixels of \nclass j predicted as belonging to class i and c is the total\
    \ number of classes. \n \n2.8 Post-processing  \nFully connected conditional random\
    \ fields (FCCRFs) [29] were used first for post-processing the \npredicted masks.\
    \ It combines single pixel prediction and shared structure through unary and pairwise\
    \ \nterms to improve smoothness and to maximize agreement between similar neighboring\
    \ pixels. The \nFCCRFs establish pairwise potential by using a Gaussian function\
    \ on all pixel pairs in an image. The \nmain benefits of using FCCRFs are determining\
    \ the optimal decision boundary at conflict regions of \npixels, while not having\
    \ notable negative effects on successfully segmented pixels.  In post-\nprocessing\
    \ of images, the prior knowledge of P. infestans disease lesion area is relatively\
    \ small in an \nearly infection stage. We found that some false positive areas\
    \ likely represent shadowed area \nbetween leaves (Figure 9). These false lesion\
    \ areas are far larger than normal lesions appeared at \nthat early infection\
    \ stage, so a simple threshold algorithm was operated to filter out part of false\
    \ \npositives in the test images. We set a reasonable lesion area range to be\
    \ [50, 10000] pixels to exclude \nthe extreme false positives. Furthermore, the\
    \ canopy heights and structures of potato plants vary in \ntrial fields. That\
    \ means even the same lesion spots represent differently in 2D images, which can\
    \ lead \nto failed predictions. For some failed lesions the network can successfully\
    \ predict the lesions at a \ndifferent scale.  A majority voting approach for\
    \ lesion counting was proposed based on multiple \nprediction masks from various\
    \ scales. The generic pseudo-code is listed in Table 2. Each image for \nprediction\
    \ was cropped into sub-images at 7 multiple scales from 3x2 to 9x8. The sub-images\
    \ with \nthe same scale were predicted separately by the model, resulting in 7\
    \ predictions for each image. \nThe final prediction mask of an image is obtained\
    \ based on the majority voting of its 7 prediction \nmasks. \nTable 2. Pseudo-codes\
    \ of fused mask generation based on a majority voting approach \nAlgorithm: Generic\
    \ pseudo-code of lesion counting  \n1:  Input: test dataset  \n2:  For image in\
    \ test dataset do: \n3:       for i=3 to 9 do:  \n4:            cut image horizontally\
    \ i and vertically i-1, i x (i-1) sub-images generated in total;  \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    5:            resize sub-images (512x512); \n6:            predict each subimage\
    \ with the optimal model, i x (i-1) sub-masks obtained in total;  \n7:       \
    \     align i x (i-1) sub-masks/sub-images and reconstruct masks/images;  \n8:\
    \            resize masks/images in original size; \n9:            save masks/images\
    \ (each image has 7 corresponding masks); \n10:      end \n11:        obtain class\
    \ label in each pixel of an image by the majority vote of the class labels in\
    \ its    \n              corresponding pixels of its 7 masks; \n12: Output: count\
    \ the number of lesions in each image; \n \n3. Results \n3.1 Network training\
    \  \nOverall, the training loss and validation loss decreased with the increment\
    \ of training time (Figure 5). \nThe validation loss fluctuated much in the early\
    \ training stage (< 150 epochs) and then slowly \nconverged at 0.0626 at the end\
    \ of the training. By contrast, the training loss smoothly dropped until \nthe\
    \ end of training and finally converged at 0.0398, slightly lower than the final\
    \ validation loss \n(0.0626). It also can be observed that both the training loss\
    \ and validation loss were substantially \nstable after 450 epochs, indicating\
    \ that the model stopped improving on a hold-out validation \ndataset. The model\
    \ weights were saved at 450 epochs to prevent the risk of overfitting. At this\
    \ epoch \npoint, the overall accuracy values in the training and validation datasets\
    \ were 0.9962 and 0.9945, \nrespectively. The same procedures were followed when\
    \ training other models with different \nhyperparameters (the weight ratio of\
    \ lesion and background) for performance comparison in the test \ndataset.  \n\
    \ \nFigure 5. Loss curves of the network (1:7 weight ratio) in the training and\
    \ validation datasets \n3.2 The weight ratio of two classes (lesion and background)\
    \  \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was not\
    \ certified by peer review) is the author/funder, who has granted bioRxiv a license\
    \ to display the preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    The weight ratio of lesion and background classes is one of the important hyperparameters\
    \ needed \nto be fine-tuned. In this study, we investigated 13 group weight ratios\
    \ in order to determine the \noptimal weight ratio for lesion semantic segmentation.\
    \ One of the weight ratios (1:2.5) was obtained \nbased on the median frequency\
    \ of two classes and the remaining weight ratios were ranging from 1 \nto 12.\
    \ The metrics in the validation dataset are shown in Table 3. It shows that the\
    \ imbalance weights \ncan effectively improve the segmentation performance as\
    \ all mIoU values exceed 0.65 compared to \nthe 0.551 obtained when the weight\
    \ ratio was set to 1:1. Interestingly, mIoU value does not continue \nto increase\
    \ with a larger weight ratio (>7). The maximum mIoU value was achieved with 1:7\
    \ weight \nratio. We selected the model with this weight ratio as the optimal\
    \ model for lesion segmentation. \nTable 3. Metrics of the models with different\
    \ class weight ratios in the validation dataset \nWeight ratio  Overall accuracy\
    \ \nClass \naverage \naccuracy  \nIoU (background) \nIoU (lesion) \nmIoU \n1:1\
    \ \n0.998 \n0.671 \n0.854 \n0.248 \n0.551 \n1:2 \n0.999 \n0.743 \n0.997 \n0.361\
    \ \n0.679 \n1:2.5 \n0.999 \n0.789 \n0.997 \n0.371 \n0.684 \n1:3 \n0.998 \n0.802\
    \ \n0.996 \n0.345 \n0.671 \n1:4 \n0.999 \n0.810 \n0.997 \n0.400 \n0.698 \n1:5\
    \ \n0.999 \n0.799 \n0.996 \n0.337 \n0.618 \n1:6 \n0.999 \n0.833 \n0.997 \n0.395\
    \ \n0.696 \n1:7 \n0.998 \n0.804 \n0.997 \n0.401 \n0.699 \n1:8 \n0.998 \n0.817\
    \ \n0.997 \n0.397 \n0.697 \n1:9 \n0.997 \n0.841 \n0.996 \n0.372 \n0.684 \n1:10\
    \ \n0.998 \n0.857 \n0.997 \n0.397 \n0.697 \n1:11 \n0.998 \n0.814 \n0.997 \n0.397\
    \ \n0.697 \n1:12 \n0.998 \n0.816 \n0.997 \n0.376 \n0.687 \n \n3.3 Test image prediction\
    \  \nSimilar to the image process for training, the original test images (5472x3648)\
    \ were first cropped and \nthen resized to sub-images (512x512) for prediction.\
    \ We used the model with 1:7 weight ratio as the \noptimal model to test the images.\
    \ Figure 6 shows confusion matrix in the validation dataset (a) and in \nthe test\
    \ dataset (b). The IoU values of background and lesion classes in the test dataset\
    \ are 0.996 and \n0.386, respectively. The metrics in the test dataset are lower\
    \ than in the validation dataset. Most \nbackground pixels (99.8%) are correctly\
    \ classified from the confusion matrix. As the majority of pixels \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    are leaf, belonging to background class, they can be easily classified based on\
    \ the color differences \nwith lesion class. Around 40% of lesion pixels were\
    \ classified as being background class in the test \ndataset. The prediction examples\
    \ are illustrated in Figure 7. Generally, most lesions, marked as the \nred areas\
    \ in the images, can be correctly segmented. Some tiny lesions were failed to\
    \ be manually \nlabelled on the ground truth images, but they were successfully\
    \ segmented by the model (shown in \n#2 and #4 columns in Figure 7). \n      \
    \   \n \n                                             (a)                    \
    \  ces                                                                  (b)  \
    \       \nFigure 6. Confusion matrix in the validation dataset (a) and in the\
    \ test dataset (b). \n \n \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\nFigure 7. Examples\
    \ of sub-image predictions (512x512) in the test dataset (row #1: raw sub-images,\
    \ \nrow #2: ground truth, row #3: predicted images) \nThe prediction masks of\
    \ sub-images were reconstructed back to the predicted images of the original \n\
    test images (5472x3648). Two examples of the predicted images are shown in Figure\
    \ 8. There are \nsome examples of failed cases in some predictions. For example,\
    \ no disease lesions were visually \nobserved on potato leaves (Figure 9). But\
    \ 3 lesion areas were predicted by the model. The three false \npositives are\
    \ all from soil patches which largely have similar shape features and areas as\
    \ typical \nlesions. Moreover, these soil patches are surrounded by leaves, resulting\
    \ in more confusion for \ninference. Also, some other wrongly predicted cases\
    \ are located in the image border (#1 column in \nFigure 7). An entire lesion\
    \ can be cut with two pieces when cropping a whole image into multiple \nsub-images.\
    \ In this case, the partial lesion significantly changes the morphological features\
    \ and loses \nthe important neighbor pixel information for models to predict.\
    \ This might lead to these failure cases.  \n \nFigure 8. Examples of the predicted\
    \ raw images (5472x3648) in the test dataset and ground truth \nimages (Left column:\
    \ ground truth images, Right column: predicted images)  \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    \ \nFigure 9. False positives in a test image (5472x3648). \n3.4 Model validation\
    \ in negative examples \nWe also tested the generalization ability and validity\
    \ of the model with some difficult images \n(negative examples) without P. infestans\
    \ lesions but with tissue damages caused by biotic or abiotic \nstresses. These\
    \ images were collected from different potato fields in the summer of 2020. The\
    \ model \ndid not predict any lesions in these images in Figure 10. These damages\
    \ were from various sources \nsuch as fertilization, herbicide and pathogens other\
    \ than P. infestans. Figure 11 shows that the model \nfailed to recognize the\
    \ P. infestans lesions. Specifically, some lesions from Alternaria solani (Early\
    \ \nBlight) were recognized as being P. infestans lesions. However, the model\
    \ did not recognize damages \nin stems caused by leaf mold as P. infestans lesions,\
    \ though they have similar color features as P. \ninfestans lesions.  \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    \ \nFigure 10. Correct prediction samples with damages of burning by lime nitrate\
    \ (a); with deformity \nand necrosis caused by herbicide damage (b); with damage\
    \ caused by eutrophication with lime \nnitrate (c); with infestation of possible\
    \ grey mold (d).  \n \nFigure 11. Failure cases of P. infestans lesion recognition\
    \ where some cases of  Alternaria solani \ninfections are recognized (Left); and\
    \ in an image with sever damages caused by leaf mold infestation \n(Right).  \
    \ \n3.5 Correlation between visual scores and the number of lesions at the canopy\
    \ level \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was\
    \ not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\nAs the visual scores were obtained\
    \ based on the rating of the whole plant, we only selected the \nunseen images\
    \ that covered the full crop canopy to avoid bias due to partial view. There were\
    \ 43 \nimages selected in total. The histogram of visual scores is shown in Figure\
    \ 12. The average value of \nvisual scores is 3% ranging from 0 to 40%. In order\
    \ to minimize the failure cases raised from cropping, \neach image was predicted\
    \ with multiple scales. Specifically, each image was cropped at 3x2, 4x3, 5x4,\
    \ \n6x5, 7x6, 8x7, 9x8 scales in horizontal x vertical directions. The number\
    \ of lesions in each image was \nobtained based on the majority voting algorithm\
    \ described in the section of post-processing. The \nhistogram of the final detected\
    \ lesions is displayed in Figure 13. There were 1063 lesions detected in \nthese\
    \ test images. The mean value of the detected lesion areas was 892 pixels. The\
    \ maximum and \nminimum values are 7520 and 52 pixels, respectively. More than\
    \ 40% of lesion areas are below 500 \npixels, and very few lesion areas exceed\
    \ 6000 pixels. This is consistent with the visual scores where \naround 80% of\
    \ the scores are below 5%.  \n \nFigure 11. Histogram of visual scores; the red\
    \ bar represents the number of visual scores in that x axis \nrange (y axis in\
    \ the left), and the blue line represents cumulative percentage of visual scores\
    \ (y axis in \nthe right).   \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\n \nFigure 13.\
    \ Histogram of lesion areas; the red bar represents the number of lesion areas\
    \ in that x axis \nrange (y axis in the left), and the blue line represents the\
    \ cumulative percentage of lesion areas (y \naxis in the right).   \nA linear\
    \ model was fitted, to quantify the relationship between visual scores obtained\
    \ from an \nexperienced plant breeder from the Danespo company and the number\
    \ of lesions that appeared at \ncanopy level. Figures 14 and 15 show the fitted\
    \ linear relationships between visual scores and the \nnumber of lesions predicted\
    \ from 3x2 and 5x4 scales, respectively. Figure 16 illustrates the fitted \nlinear\
    \ relationship between visual scores and the number of lesions obtained from majority\
    \ voting of \nall scales. Compared to prediction masks with only one scale, the\
    \ fused masks based on majority \nvoting achieved a better linear relationship\
    \ by increasing the R2 value from around 0.4 to 0.655. \n \n \nCC-BY-NC-ND 4.0\
    \ International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    Figure 14. Fitted linear relationship between visual scores and number of lesions\
    \ assigned from \nimages  from 3x2 scale.  \n \nFigure 15. Fitted linear relationship\
    \ between visual scores and number of lesions assigned from \nimages from 6x5\
    \ scale. \n \n \nFigure 16. Fitted linear relationship between visual scores and\
    \ number of lesions from majority \nvoting from all scales.  \n4. Discussion \n\
    Deep learning has demonstrated its superior performance on disease detection in\
    \ field settings \ncompared to convolutional machine learning methods [17][20][18].\
    \ Imbalance classes are a common \nproblem in deep learning and is widely represented\
    \ in the agricultural field, and thus hampers \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    applications in for example distinction between weed and crop [13][30], pest detection\
    \ [14] and \ndisease segmentation [18]. In these cases, pixels of one class, generally\
    \ soil or crop, are dominant in \nimages. Training a network in an appropriate\
    \ way is critical for delivering a good segmentation result \nfor each class.\
    \ Table 3 shows that assigning imbalance weights in the loss function can effectively\
    \ \nmitigate the issue with imbalance classes, which is consistent with the conclusion\
    \ by Yasrab et al \n(2019) [31]. The mIOU value is only 0.551 with same weights\
    \ across classes, while it can be improved \nto nearly 0.7 with imbalance weight\
    \ assignment. The IOU value (>0.99) of the background class (soil \nand crop plant)\
    \ is far higher than the IOU value of disease lesion class. Based on the confusion\
    \ matrix, \nit is concluded that majority of lesion pixels (>60%) were correctly\
    \ classified. The relatively low IOU of \nlesion class (<0.5) indicates that a\
    \ few of false positives (Figure 9) from soil patches, which have \nsimilar shapes\
    \ to lesions were predicted by the model. Stewart et al [18] also found this kind\
    \ of false \npositives for Northern Leaf Blight (NLB) lesion segmentation in maize\
    \ fields. Besides, senesced leaves \ncould also contribute to the false positives.\
    \ Setting a threshold to filter out some false positives, \nbased on lesion size,\
    \ might improve the IOU value of lesion class. This threshold value can be \n\
    estimated based on the prior knowledge of the maximum lesion area in certain developmental\
    \ stage \nof PLB disease or plant development, related to the start of the outbreak\
    \ or even disease prediction \nbased on weather and cultivar. It should, however,\
    \ be noted that a lesion size threshold is of course \ndependent on that images\
    \ are collected at a fixed height. We drew a group of connected key points \n\
    to define the lesion area when manually labelling images with the tool LabelMe.\
    \ This way of \nannotating speeds up the pixel-wise labelling process for segmentation.\
    \ However,  it is difficult to \nalways draw a very accurate boundary line with\
    \ those key points especially since a majority of PLB \nlesions in our images\
    \ are tiny and have an irregular shape. As a consequence, the labelled lesion\
    \ \nareas at times inevitably include pixels belonging to the background class\
    \ (soil and leaf), leading to \nthe relatively low IOU of lesion class. To overcome\
    \ this problem, Wiesner-Hanks et al [19] discussed \nusing crowdsourced data for\
    \ NLB lesion detection at millimeter-level based on aerial visual images \nand\
    \ concluded that increasing the number of workers per image could improve the\
    \ quality of \nannotation polygons.  \nImage preprocessing is essential before\
    \ feeding images to train a neural network. It is encouraged to \nrandomly apply\
    \ blur, contrast and brightness as data augmentation for the benefit of model\
    \ \nrobustness. The training dataset with 1600 labelled images from 31 potato\
    \ genotypes is still limited \nand unlikely to include all lesion variations.\
    \ We tested the generalization ability and validity of model \nin recognition\
    \ of P. infestans lesions with some images from other fields and conditions (Figure\
    \ 10). \nThe failure cases featured highly similar colour and morphological characteristics\
    \ as P. infestans \nlesions still are represented. Including such failed images\
    \ in the training dataset again can improve \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nthe\
    \ performance [32]. The creation of further lesion variations in the training\
    \ datasets  by synthetic \nlesion images could be a good way to improve model\
    \ performances . To this end, Sun et al [33] \ndeveloped a conventional image\
    \ processing algorithm to optimize synthetic lesion images obtained \nfrom a generative\
    \ adversarial network (GAN). Cap et al [34] proposed a LeafGAN algorithm for lesion\
    \ \nimage generations, which improved the diagnostic performance by 7.4%.  \n\
    The use of a majority voting  to generate accurate lesion masks from multiple\
    \ prediction scales, \ninspired from random forest machine learning algorithm\
    \ described in [35], proved its effectiveness  \nto establish a linear relationship\
    \ between visual scores and number of lesions at canopy level (Figure \n16). Very\
    \ few studies have tried to automatize the visual scoring in field environments\
    \ for plant \nbreeding. In reality, visual scores are evaluated based on the number\
    \ of lesions and their areas on \nsingle leaflets at early infection stages, which\
    \ brings difficulties with analysis based on with 2D \nimages at the canopy level.\
    \ The lesion recognition should be further  explored by three-dimensional \n(3D)\
    \ imaging to obtain full plant structures and by employing the state-of-the-art\
    \ network \narchitecture in semantic segmentation to reduce the inference time.\
    \  \nIn precision farming, it is necessary to detect PLB disease as early as possible\
    \ to bring in appropriate \nmeasurements to avoid yield loss. As only RGB images\
    \ were used in this analysis, pre-symptomatic \ndetection of PLB is inevitable\
    \ missed. For pre-symptomatic crop disease detection, hyperspectral \nmeasurements\
    \ from spectroradiometers or spectral imaging sensors are generally employed [36].\
    \ \nFor example, Anderegg et al [37] used an ASD FieldSpec spectroradiometer (350-2500nm)\
    \ to \nmeasure wheat plants at a canopy level for Septoria Tritici Blotch (STB)\
    \ disease detection and \nquantification. Gold et al [38] measured contact leaf\
    \ reflectance with a field spectrometer for pre-\nsymptomatic PLB detection. For\
    \ many applications [39] [40] [41], spectral imaging sensors are more \npopular\
    \ than non-imaging hyperspectral sensors due to the additional capability of providing\
    \ spatial \ninformation on shape, texture and color. Partial least square discriminant\
    \ analysis (PLSDA) is \ngenerally used to process full spectral data [42]. Our\
    \ study also has the potential to monitor the \ndevelopment of PLB disease after\
    \ lesion appearance, which could be useful to screen for high PLB \nresistance\
    \ potato genotypes from a diverse germplasm in precision breeding. In terms of\
    \ PLB \nmanagement in potato production, it is useful to acknowledge how early\
    \ PLB should be detected \nafter the appearance of lesions. To address this question,\
    \ Wiik et al [43] carried out trials over two \nyears in 2018 and 2019 to test\
    \ the need of first intervention after the first visual symptom were \ndetected.\
    \ The preliminary results showed that it is acceptable for farmers to apply a\
    \ first spray with \ncurative systematic fungicides after the discovery of first\
    \ symptom,  corresponding to a very low 0.01% \ninfection provided that it is\
    \ sprayed more or less immeadiatly as it was shown that a first spray \ndelayed\
    \ by 5 days later first symptom appears was too late to stop the disease. 0.01%\
    \ infection \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which\
    \ was not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\ncorresponds to  to only 300 spots/ha\
    \ if the disease is evenly spread over the field. Thus it is clear that \nprotection\
    \ against PLB for precision agriculture requires very  early detection of the\
    \ symptoms with \nRGB.   \n5. Conclusion and future work  \nIn this study, we\
    \ demonstrated the feasibility of using a deep learning algorithm based on an\
    \ \nencoder-decoder architecture for potato late blight disease lesion semantic\
    \ segmentation based on \nfield images. The results show that the intersection\
    \ over union (IoU) values of background (soil and \nleaf) and lesion classes in\
    \ the test dataset are 0.996 and 0.386, respectively. Assigning different \nweights\
    \ for imbalance class could improve the performance of the model. This work also\
    \ presents \nthe possibility of accurate lesion counting at the plant canopy level\
    \ with the use of image alignment. \nA linear relationship between visual scoring\
    \ and the number of lesions was established. We can also \nconclude that the fused\
    \ masks obtained from majority voting of the masks predicted with multiple \n\
    scales achieved higher R2 value (0.655) compared to prediction with a single scale.\
    \ The proposed \nmethodology has the potential to monitor the lesion development\
    \ under field conditions and \nevaluate the resistance of genotypes against potato\
    \ late blight enabling more precise and automated \npotato breeding. \nThis study\
    \ will be followed by further field tests and the model will continue to be tested\
    \ in terms of \nrobustness and accuracy by adding new field image datasets. The\
    \ updated model will also be used to \ntest on images collected from different\
    \ time points to predict area under disease progression curve \n(AUDPC).  In addition,\
    \ we will continuously update the models with the new labeled datasets and \n\
    synthetic images to improve the generalization ability. Multiple imaging sensors\
    \ like multispectral \nand hyperspectral cameras hold promise to also detect and\
    \ maybe even quantify pre-symptomatic \ndisease. Also, the sensor combinations,\
    \ e.g. a spectroradiometer (early stage) and high-resolution \nRGB camera (late\
    \ stage), can be considered for monitoring PLB progression. Multimodal data fusion\
    \ \nand machine learning are suggested to be fully exploited for this application.\
    \ Furthermore, it would \nbe interesting to explore new vehicle and sensor techniques\
    \ to build three-dimensional imaging to be \nable to detect disease lesions below\
    \ the canopy. \n \nAcknowledgement \nWe acknowledge the Flemish Supercomputer\
    \ Center (VSC) for providing the GPU computational \nresources and services for\
    \ this work. We thank Mathieu Gremillet for field assistance, Hanne Grethe \n\
    Kirk at Danespo for visual scoring of disease, and Linnea Almqvist from SLU for\
    \ providing image \nCC-BY-NC-ND 4.0 International license.\navailable under a\n\
    (which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nexamples in Figures\
    \ 10 and 11. This research was founded by Nordic Council of Ministers (PPP #6P2),\
    \ \nNordForsk (#84597) and Vinnova (#2016-04386). \nReference: \n[1] \nS. Savary,\
    \ A. Ficke, J.N. Aubertot, C. Hollier, Crop losses due to diseases and their implications\
    \ \nfor global food production losses and food security, Food Secur. 4 (2012)\
    \ 519–537. \nhttps://doi.org/10.1007/s12571-012-0200-5. \n[2] \nA. Chawade, J.\
    \ Van Ham, H. Blomquist, O. Bagge, E. Alexandersson, R. Ortiz, High-throughput\
    \ \nfield-phenotyping tools for plant breeding and precision agriculture, Agronomy.\
    \ 9 (2019). \nhttps://doi.org/10.3390/agronomy9050258. \n[3] \nA.K. Mahlein, Plant\
    \ disease detection by imaging sensors – Parallels and specific demands for \n\
    precision agriculture and plant phenotyping, Plant Dis. 100 (2016) 241–254. \n\
    https://doi.org/10.1094/PDIS-03-15-0340-FE. \n[4] \nL. Colon, B. Nielsen, U. Darsow,\
    \ Field Test for Foilage Blight Resistance, 2004. \n[5] \nA.M. Abdu, M.M. Mokji,\
    \ U.U. Sheikh, A Pattern Analysis-based Segmentation to Localize Early \nand Late\
    \ Blight Disease Lesions in Digital Images of Plant Leaves, in: 2020: pp. 116–121.\
    \ \nhttps://doi.org/10.1109/icsipa45851.2019.8977798. \n[6] \nJ.G.A. Barbedo,\
    \ A new automatic method for disease symptom segmentation in digital \nphotographs\
    \ of plant leaves, Eur. J. Plant Pathol. 147 (2017) 349–364. \nhttps://doi.org/10.1007/s10658-016-1007-6.\
    \ \n[7] \nA. Camargo, J.S. Smith, An image-processing based algorithm to automatically\
    \ identify plant \ndisease visual symptoms, Biosyst. Eng. 102 (2009) 9–21. \n\
    https://doi.org/10.1016/j.biosystemseng.2008.09.030. \n[8] \nC.I. Fernández, B.\
    \ Leblon, A. Haddadi, K. Wang, J. Wang, Potato late blight detection at the \n\
    leaf and canopy levels based in the red and red-edge spectral regions, Remote\
    \ Sens. 12 (2020). \nhttps://doi.org/10.3390/RS12081292. \n[9] \nS. Appeltans,\
    \ A. Guerrero, S. Nawar, J. Pieters, A.M. Mouazen, Practical Recommendations for\
    \ \nHyperspectral and Thermal Proximal Disease Sensing in Potato and Leek Fields,\
    \ Remote Sens. \n12 (2020) 1939. https://doi.org/10.3390/rs12121939. \n[10] \n\
    R. Sugiura, S. Tsuda, S. Tamiya, A. Itoh, K. Nishiwaki, N. Murakami, Y. Shibuya,\
    \ M. Hirafuji, S. \nNuske, Field phenotyping system for the assessment of potato\
    \ late blight resistance using RGB \nimagery from an unmanned aerial vehicle,\
    \ Biosyst. Eng. 148 (2016) 1–10. \nhttps://doi.org/10.1016/j.biosystemseng.2016.04.010.\
    \ \n[11] \nM.H.D. Franceschini, H. Bartholomeus, D.F. van Apeldoorn, J. Suomalainen,\
    \ L. Kooistra, \nFeasibility of unmanned aerial vehicle optical imagery for early\
    \ detection and severity \nassessment of late blight in Potato, Remote Sens. 11\
    \ (2019). \nhttps://doi.org/10.3390/rs11030224. \n[12] \nY.A. LeCun, Y. Bengio,\
    \ G.E. Hinton, Deep learning, Nature. 521 (2015) 436–444. \nhttps://doi.org/10.1038/nature14539.\
    \ \n[13] \nJ. Gao, A.P. French, M.P. Pound, Y. He, T.P. Pridmore, J.G. Pieters,\
    \ Deep convolutional neural \nnetworks for image-based Convolvulus sepium detection\
    \ in sugar beet fields, Plant Methods. \n16 (2020). https://doi.org/10.1186/s13007-020-00570-z.\
    \ \n[14] \nZ. Liu, J. Gao, G. Yang, H. Zhang, Y. He, Localization and Classification\
    \ of Paddy Field Pests \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nusing a Saliency Map\
    \ and Deep Convolutional Neural Network, Sci. Rep. 6 (2016) 20410. \nhttps://doi.org/10.1038/srep20410.\
    \ \n[15] \nR. Barth, J. Hemming, E.J. Van Henten, Angle estimation between plant\
    \ parts for grasp \noptimisation in harvest robots, Biosyst. Eng. 183 (2019) 26–46.\
    \ \nhttps://doi.org/10.1016/j.biosystemseng.2019.04.006. \n[16] \nJ. Ubbens, M.\
    \ Cieslak, P. Prusinkiewicz, I. Stavness, The use of plant models in deep learning:\
    \ \nAn application to leaf counting in rosette plants, Plant Methods. 14 (2018).\
    \ \nhttps://doi.org/10.1186/s13007-018-0273-z. \n[17] \nG. Polder, P.M. Blok,\
    \ H.A.C. de Villiers, J.M. van der Wolf, J. Kamp, Potato Virus Y Detection in\
    \ \nSeed Potatoes Using Deep Learning on Hyperspectral Images, Front. Plant Sci.\
    \ 10 (2019) 1–13. \nhttps://doi.org/10.3389/fpls.2019.00209. \n[18] \nE.L. Stewart,\
    \ T. Wiesner-Hanks, N. Kaczmar, C. DeChant, H. Wu, H. Lipson, R.J. Nelson, M.A.\
    \ \nGore, Quantitative Phenotyping of Northern Leaf Blight in UAV Images Using\
    \ Deep Learning, \nRemote Sens. 11 (2019). https://doi.org/10.3390/rs11192209.\
    \ \n[19] \nT. Wiesner-Hanks, H. Wu, E. Stewart, C. DeChant, N. Kaczmar, H. Lipson,\
    \ M.A. Gore, R.J. \nNelson, Millimeter-Level Plant Disease Detection From Aerial\
    \ Photographs via Deep Learning \nand Crowdsourced Data, Front. Plant Sci. 10\
    \ (2019). https://doi.org/10.3389/fpls.2019.01550. \n[20] \nH. Wu, T. Wiesner-Hanks,\
    \ E.L. Stewart, C. DeChant, N. Kaczmar, M.A. Gore, R.J. Nelson, H. \nLipson, Autonomous\
    \ Detection of Plant Disease Symptoms Directly from Aerial Imagery, Plant \nPhenome\
    \ J. 2 (2019) 1–9. https://doi.org/10.2135/tppj2019.03.0006. \n[21] \nB. Laflamme,\
    \ M. Middleton, T. Lo, D. Desveaux, D.S. Guttman, Image-based quantification of\
    \ \nplant immunity and disease, Mol. Plant-Microbe Interact. 29 (2016) 919–924.\
    \ \nhttps://doi.org/10.1094/MPMI-07-16-0129-TA. \n[22] \nP. Karisto, A. Hund,\
    \ K. Yu, J. Anderegg, A. Walter, F. Mascher, B.A. McDonald, A. Mikaberidze, \n\
    Ranking quantitative resistance to septoria tritici blotch in elite wheat cultivars\
    \ using \nautomated image analysis, Phytopathology. 108 (2018) 568–581. \nhttps://doi.org/10.1094/PHYTO-04-17-0163-R.\
    \ \n[23] \nC.H. Bock, J.G.A. Barbedo, E.M. Del Ponte, D. Bohnenkamp, A.-K. Mahlein,\
    \ From visual \nestimates to fully automated sensor-based measurements of plant\
    \ disease severity: status \nand challenges for improving accuracy, Phytopathol.\
    \ Res. 2 (2020). \nhttps://doi.org/10.1186/s42483-020-00049-8. \n[24] \nU.D. Leontine\
    \ Colon, Bent Nielsen, Field test for foliage blight resistance, 2004. \n[25]\
    \ \nB.C. Russell, A. Torralba, K.P. Murphy, W.T. Freeman, LabelMe: A database\
    \ and web-based \ntool for image annotation, Int. J. Comput. Vis. 77 (2008) 157–173.\
    \ \nhttps://doi.org/10.1007/s11263-007-0090-8. \n[26] \nV. Badrinarayanan, A.\
    \ Kendall, R. Cipolla, SegNet: A Deep Convolutional Encoder-Decoder \nArchitecture\
    \ for Image Segmentation., IEEE Trans. Pattern Anal. Mach. Intell. 39 (2017) 2481–\n\
    2495. https://doi.org/10.1109/TPAMI.2016.2644615. \n[27] \nD. Eigen, R. Fergus,\
    \ Predicting depth, surface normals and semantic labels with a common \nmulti-scale\
    \ convolutional architecture, in: Proc. IEEE Int. Conf. Comput. Vis., 2015: pp.\
    \ 2650–\n2658. https://doi.org/10.1109/ICCV.2015.304. \n[28] \nD.P. Kingma, J.\
    \ Ba, Adam: A Method for Stochastic Optimization, in: Int. Conf. Learn. \nRepresent.,\
    \ 2015: pp. 1–15. \nhttps://doi.org/http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503.\
    \ \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was not\
    \ certified by peer review) is the author/funder, who has granted bioRxiv a license\
    \ to display the preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    [29] \nP. Krähenbühl, V. Koltun, Efficient Inference in Fully Connected CRFs with\
    \ Gaussian Edge \nPotentials, in: J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett,\
    \ F. Pereira, K.Q. Weinberger (Eds.), Adv. \nNeural Inf. Process. Syst. 24, Curran\
    \ Associates, Inc., 2011: pp. 109–117. \nhttp://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-\n\
    edge-potentials.pdf. \n[30] \nJ. Gao, W. Liao, D. Nuyttens, P. Lootens, J. Vangeyte,\
    \ A. Pižurica, Y. He, J.G. Pieters, Fusion of \npixel and object-based features\
    \ for weed mapping using unmanned aerial vehicle imagery, Int. \nJ. Appl. Earth\
    \ Obs. Geoinf. 67 (2018) 43–53. https://doi.org/10.1016/j.jag.2017.12.012. \n\
    [31] \nR. Yasrab, J.A. Atkinson, D.M. Wells, A.P. French, T.P. Pridmore, M.P.\
    \ Pound, RootNav 2.0: \nDeep learning for automatic navigation of complex plant\
    \ root architectures, Gigascience. 8 \n(2019) 1–16. https://doi.org/10.1093/gigascience/giz123.\
    \ \n[32] \nZ. Huang, E. Sklar, S. Parsons, Design of automatic strawberry harvest\
    \ robot suitable in \ncomplex environments, in: ACM/IEEE Int. Conf. Human-Robot\
    \ Interact., 2020: pp. 567–569. \nhttps://doi.org/10.1145/3371382.3377443. \n\
    [33] \nR. Sun, M. Zhang, K. Yang, J. Liu, Data enhancement for plant disease classification\
    \ using \ngenerated lesions, Appl. Sci. 10 (2020). https://doi.org/10.3390/app10020466.\
    \ \n[34] \nQ.H. Cap, H. Uga, S. Kagiwada, H. Iyatomi, LeafGAN: An Effective Data\
    \ Augmentation Method \nfor Practical Plant Disease Diagnosis, arXiv Prepr. arXiv2002.10100.\
    \ (2020). \n[35] \nL. Breiman, Random forests, Mach. Learn. 45 (2001) 5–32. \n\
    https://doi.org/10.1023/A:1010933404324. \n[36] \nA.-K. Mahlein, M.T. Kuska, J.\
    \ Behmann, G. Polder, A. Walter, Hyperspectral Sensors and \nImaging Technologies\
    \ in Phytopathology: State of the Art, Annu. Rev. Phytopathol. 56 (2018) \n535–558.\
    \ https://doi.org/10.1146/annurev-phyto-080417-050100. \n[37] \nJ. Anderegg, A.\
    \ Hund, P. Karisto, A. Mikaberidze, In-Field Detection and Quantification of \n\
    Septoria Tritici Blotch in Diverse Wheat Germplasm Using Spectral–Temporal Features,\
    \ Front. \nPlant Sci. 10 (2019). https://doi.org/10.3389/fpls.2019.01355. \n[38]\
    \ \nK.M. Gold, P.A. Townsend, A. Chlus, I. Herrmann, J.J. Couture, E.R. Larson,\
    \ A.J. Gevens, \nHyperspectral measurements enable pre-symptomatic detection and\
    \ differentiation of \ncontrasting physiological effects of late blight and early\
    \ blight in potato, Remote Sens. 12 \n(2020) 1–21. https://doi.org/10.3390/rs12020286.\
    \ \n[39] \nJ. Gao, X. Li, F. Zhu, Y. He, Application of hyperspectral imaging\
    \ technology to discriminate \ndifferent geographical origins of Jatropha curcas\
    \ L. seeds, Comput. Electron. Agric. 99 (2013) \n186–193. \n[40] \nJ. Gao, D.\
    \ Nuyttens, P. Lootens, Y. He, J.G. Pieters, Recognising weeds in a maize crop\
    \ using a \nrandom forest machine-learning algorithm and near-infrared snapshot\
    \ mosaic \nhyperspectral imagery, Biosyst. Eng. 170 (2018) 39–50. \nhttps://doi.org/10.1016/j.biosystemseng.2018.03.006.\
    \ \n[41] \nS. Appeltans, A. Guerrero, S. Nawar, J. Pieters, A.M. Mouazen, Practical\
    \ Recommendations for \nHyperspectral and Thermal Proximal Disease Sensing in\
    \ Potato and Leek Fields, Remote Sens. \n12 (2020) 1939. https://doi.org/10.3390/rs12121939.\
    \ \n[42] \nK. Yu, J. Anderegg, A. Mikaberidze, P. Karisto, F. Mascher, B.A. McDonald,\
    \ A. Walter, A. Hund, \nHyperspectral canopy sensing of wheat septoria tritici\
    \ blotch disease, Front. Plant Sci. 9 (2018). \nhttps://doi.org/10.3389/fpls.2018.01195.\
    \ \n[43] \nL. Wiik, M. Nilsson, L. Aldén, A. Gerdtsson, L.G.-B. Didymus, E. Liljeroth,\
    \ Sweden attempts trial \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nreport 2019, 2019.\
    \ https://sverigeforsoken.se/. \n \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\n"
  inline_citation: '>'
  journal: bioRxiv (Cold Spring Harbor Laboratory)
  limitations: '>'
  pdf_link: https://www.biorxiv.org/content/biorxiv/early/2020/08/28/2020.08.27.263186.full.pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Automatic late blight lesion recognition and severity quantification based
    on field imagery of diverse potato genotypes by deep learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
