- analysis: '>'
  authors:
  - Khan A.
  - Malebary S.J.
  - Dang L.M.
  - Binzagr F.
  - Song H.K.
  - Moon H.
  citation_count: '0'
  description: Our research focuses on addressing the challenge of crop diseases and
    pest infestations in agriculture by utilizing UAV technology for improved crop
    monitoring through unmanned aerial vehicles (UAVs) and enhancing the detection
    and classification of agricultural pests. Traditional approaches often require
    arduous manual feature extraction or computationally demanding deep learning (DL)
    techniques. To address this, we introduce an optimized model tailored specifically
    for UAV-based applications. Our alterations to the YOLOv5s model, which include
    advanced attention modules, expanded cross-stage partial network (CSP) modules,
    and refined multiscale feature extraction mechanisms, enable precise pest detection
    and classification. Inspired by the efficiency and versatility of UAVs, our study
    strives to revolutionize pest management in sustainable agriculture while also
    detecting and preventing crop diseases. We conducted rigorous testing on a medium-scale
    dataset, identifying five agricultural pests, namely ants, grasshoppers, palm
    weevils, shield bugs, and wasps. Our comprehensive experimental analysis showcases
    superior performance compared to various YOLOv5 model versions. The proposed model
    obtained higher performance, with an average precision of 96.0%, an average recall
    of 93.0%, and a mean average precision (mAP) of 95.0%. Furthermore, the inherent
    capabilities of UAVs, combined with the YOLOv5s model tested here, could offer
    a reliable solution for real-time pest detection, demonstrating significant potential
    to optimize and improve agricultural production within a drone-centric ecosystem.
  doi: 10.3390/plants13050653
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Plants All Article Types Advanced   Journals
    Plants Volume 13 Issue 5 10.3390/plants13050653 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editors Gerassimos
    Peteinatos Hugo Moreno Subscribe SciFeed Recommended Articles Related Info Links
    More by Authors Links Article Views 700 Table of Contents Abstract Introduction
    Literature Review The Proposed Methodology Experiments and Results Conclusions
    Author Contributions Funding Data Availability Statement Conflicts of Interest
    References share Share announcement Help format_quote Cite question_answer Discuss
    in SciProfiles thumb_up Endorse textsms Comment first_page settings Order Article
    Reprints Open AccessArticle AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data by Asma Khan 1, Sharaf J. Malebary 2, L. Minh Dang 3,
    Faisal Binzagr 4, Hyoung-Kyu Song 3 and Hyeonjoon Moon 1,* 1 Department of Computer
    Science and Engineering, Sejong University, Seoul 05006, Republic of Korea 2 Department
    of Information Technology, Faculty of Computing and Information Technology, King
    Abdulaziz University, P.O. Box 344, Rabigh 21911, Saudi Arabia 3 Department of
    Information and Communication Engineering and Convergence Engineering for Intelligent
    Drone, Sejong University, Seoul 05006, Republic of Korea 4 Department of Computer
    Science, Faculty of Computing and Information Technology, King Abdulaziz University,
    P.O. Box 344, Rabigh 21911, Saudi Arabia * Author to whom correspondence should
    be addressed. Plants 2024, 13(5), 653; https://doi.org/10.3390/plants13050653
    Submission received: 23 January 2024 / Revised: 23 February 2024 / Accepted: 23
    February 2024 / Published: 27 February 2024 (This article belongs to the Special
    Issue The Future of Artificial Intelligence and Sensor Systems in Agriculture)
    Download keyboard_arrow_down     Browse Figures Review Reports Versions Notes
    Abstract Our research focuses on addressing the challenge of crop diseases and
    pest infestations in agriculture by utilizing UAV technology for improved crop
    monitoring through unmanned aerial vehicles (UAVs) and enhancing the detection
    and classification of agricultural pests. Traditional approaches often require
    arduous manual feature extraction or computationally demanding deep learning (DL)
    techniques. To address this, we introduce an optimized model tailored specifically
    for UAV-based applications. Our alterations to the YOLOv5s model, which include
    advanced attention modules, expanded cross-stage partial network (CSP) modules,
    and refined multiscale feature extraction mechanisms, enable precise pest detection
    and classification. Inspired by the efficiency and versatility of UAVs, our study
    strives to revolutionize pest management in sustainable agriculture while also
    detecting and preventing crop diseases. We conducted rigorous testing on a medium-scale
    dataset, identifying five agricultural pests, namely ants, grasshoppers, palm
    weevils, shield bugs, and wasps. Our comprehensive experimental analysis showcases
    superior performance compared to various YOLOv5 model versions. The proposed model
    obtained higher performance, with an average precision of 96.0%, an average recall
    of 93.0%, and a mean average precision (mAP) of 95.0%. Furthermore, the inherent
    capabilities of UAVs, combined with the YOLOv5s model tested here, could offer
    a reliable solution for real-time pest detection, demonstrating significant potential
    to optimize and improve agricultural production within a drone-centric ecosystem.
    Keywords: convolution neural network; deep learning; sustainable agriculture;
    UAV technology; computer vision; monitoring system 1. Introduction The agricultural
    sector is vital to economic enhancement, and it is essential to identify the pests
    that harm it. Pest detection, a persistent challenge, leads to a substantial annual
    loss of 20% in global crop yields [1,2]. Timely detection of plant diseases and
    pests is critical for efficient agricultural output, as shown in Figure 1. Embracing
    UAV technology becomes important for a more efficient and technologically driven
    approach to safeguard crop yields, including enhanced crop monitoring using UAVs
    [1,2]. This process significantly impacts grain yield, agricultural progress,
    and farmers’ income [3]. Additionally, high-resolution data for yield prediction
    further enhances the precision of our approach. To tackle the aforementioned challenge,
    an artificial intelligence (AI)-driven model stands out as the optimal choice,
    playing a pivotal role in advancing modern agricultural research. Leveraging the
    inherent capabilities of this cutting-edge technology, pest detection, and classification
    are seamlessly executed with exceptional efficiency, facilitating prompt intervention
    in agricultural production. The effectiveness of this approach not only hints
    at a reduction in losses but also promises a significant boost in agricultural
    production, especially when integrating UAV technology for heightened precision
    and real-time monitoring [4]. Detecting and preventing crop diseases are crucial
    aspects of this innovative approach, contributing to overall agricultural resilience
    and productivity. To this end, researchers have explored both ML- and DL-based
    modelsfor wheat diseases and pest recognition [5,6]. Figure 1. UAV-based early
    pest detection system to assist agricultural department. Plant analysis laboratories
    relying on morphological features for pest identification are limited because
    taxonomy specialists need to perform accurate classifications [7]. However, these
    approaches for pest identification have certain limitations, including the fact
    that the accurate classification of pests requires experts in the field of taxonomy,
    possible human error, and difficulties in identifying pests quickly. Several methods
    have been proposed for automatic pest detection using traditional ML [8]; for
    example, Faith et al. [9] used manual feature extraction and relative filters
    to identify different pest species using K-means clustering algorithms. However,
    this approach is very time-consuming, particularly for large datasets. Next, Rumpf
    et al. [10] presented a method based on support vector machines and spectral vegetation-based
    detection of sugar beet disease. Pests can be detected using these methods; however,
    they have several limitations, including inefficiency when manual feature extraction
    is required, making them time-consuming, tedious, error-prone, and dependent on
    computer experts. Later, owing to the inadequate feature extraction process, researchers
    diverted to DL, characterized by multilayer neural networks that enable automated
    end-to-end feature extraction has emerged as an alternative solution. This shift
    toward DL improves recognition efficiency while minimizing the lengthy manual
    feature extraction process [11]. A DL-based technique for pest and disease detection
    in tomato leaves was developed by Shijie et al. [12] with an average accuracy
    of 89%. However, this method is limited to identifying pests with a simple background
    and is difficult to implement in real-time applications. Generative adversarial
    network augmentation was utilized by Gandi et al. [13] to create an efficient
    DL model for categorizing plant diseases. Leonardo et al. [14] discussed the economic
    importance of fruit flies in Brazil and the challenges associated with identifying
    these pests. Later, they applied DL and transfer learning techniques that achieved
    an impressive accuracy of 95.68%. Transfer learning was also used by Dawei et
    al. [15] to detect ten pest species with 93.84% accuracy. In contrast, DL pest
    classification models performed well and boosted the existing discriminative score.
    However, challenges such as deplorability over resource-constrained devices, robustness
    concerns, lower accuracy, and high equipment costs hinder the integration of existing
    DL approaches into real-world scenarios. Considering these challenges, this study
    presents a novel approach to efficient pest detection using a modified YOLOv5
    model. Our model performs better than object recognition models due to its fast
    inference speed, high mAP, robust adaptation, and higher accuracy. Moreover, the
    integration of high-resolution data for yield prediction enhances the overall
    reliability of our model. In summary, this study represents a significant step
    towards efficient pest detection and has profound implications for sustainable
    agriculture. The proposed model has the potential to revolutionize pest management
    by maximizing agricultural yields while mitigating losses. We aim to pave the
    way for a more resilient and productive agricultural future by combining cutting-edge
    methods and custom enhancements. The major contributions of the proposed model
    are summarized below. We present an advanced system that uses UAVs to identify
    pests in real time. This groundbreaking method surpasses previous approaches with
    enhanced accuracy and a notable reduction in false alarms. By incorporating UAV
    technology, we have achieved a significant improvement in pest detection, highlighting
    the effectiveness of merging UAVs with this innovative solution. We refined the
    internal architecture of YOLOv5s by replacing smaller kernels in SSP (Neck) with
    larger ones and introducing a Stem module into the backbone. This strategic modification
    enhances the model’s capability to efficiently identify pests of varying sizes
    in images, reducing time complexity. Through extensive experimentation and comparison
    with nine object-detection models using a pest detection dataset, our model demonstrated
    empirical effectiveness and outperformed existing methods. A qualitative assessment
    further solidified the superior performance of our UAV-assisted pest detection
    technology. The rest of the paper is structured as follows: Section 2 provides
    a comprehensive summary of recent research relevant to the topic of a modified
    YOLOv5s architecture for pest and insect detection, and Section 3 outlines the
    data collection and methodology employed for the system. The experimental results
    are presented and analyzed in Section 4 and Section 5 present the conclusions
    drawn from this research. 2. Literature Review To cope with the aforementioned
    challenges, therefore, several researchers have been working on automatic systems
    for detecting insects in sustainable agriculture. Cheeti et al. [16] made a significant
    addition to this field when they used cutting-edge methods to categorize and detect
    pests in sustainable agriculture, including convolutional neural networks (CNN).
    Notably, their research involved creating a dataset using data from Internet sources,
    which yielded promising performance. To detect rice diseases and insects with
    an accuracy of 90.9%, Mique et al. [17] used CNN and image processing. They also
    released their suggested approach as an application for mobile devices for public
    use. However, this method is expensive in terms of computations and needs to be
    more accurate. A single-shot multi-box detector (SSD) with fine-tuning procedures
    was developed by Nam et al. [18] to recognize and classify collected insects.
    Academic research was the focus of a thorough comparative study by Burhan et al.
    [19]. In agricultural environments, their study concentrated on using four trained
    deep-learning models to identify diseases and detect pests in rice-growing regions.
    With an accuracy rating of 86.799%, their model showed good performance. Nonetheless,
    further improvements are necessary to enhance the model’s performance across evaluation
    metrics. Kouba et al. [20] used accelerometer sensors to create a unique dataset
    as part of a sensor-based approach to agricultural monitoring. Their system is
    also integrated into a mobile application accessible to the public, allowing for
    the early detection of red palm weevil (RPW) through movement analysis. However,
    their system is based on voice and movement analysis, which has a higher false-positive
    rate. Habib et al.’s [21] model for identifying and categorizing brown- and yellow-rusted
    illnesses in wheat crops uses classical machine learning. To help coffee growers,
    Esgario et al. [22] created a mobile app and a CNN model specifically designed
    to identify biotic stresses in coffee leaves. Svenning et al. [23] introduced
    a pre-trained CNN model along with fine-tuning techniques to classify carabid
    beetle species. Of the test images, 51.9% were correctly classified to species
    level, giving an average classification rate of 74.6% due to their efforts. Nonetheless,
    the model’s testing phase speed has impeded its feasibility for real-time implementation.
    The Deep-PestNet model, which has eight convolutional layers and three fully connected
    layers for effective pest detection, has been introduced by researchers [24].
    However, the approaches have several limitations, they used CNN methods in the
    pest detection domain, which mainly focus on classification. These methods classify
    the entire image as a single class that does consider the fact that pests or insects
    typically occupy a small portion of an image. When the object features are not
    prominently visible, relying solely on the complete image feature, without region
    proposals, can lead to reduced detection performance. Moreover, the CNN methods
    come with a high computational complexity, rendering them unsuitable for practical
    real-world implementation. Therefore, researchers have used DL-based object-detection
    models to overcome the limitation of the CNN-based methods. For instance, Li et
    al. [25] utilized the IPI02 dataset to detect insects in fields using various
    DCN networks, including Faster-RCNN, Mask-RCNN, and YOLOv5. They obtained promising
    results and demonstrated that YOLOv5 outperformed Faster-RCNN and Mask-RCNN, which
    attained 99% accuracy, whereas YOLOv5 gained 97%. Hu et al. [26] used YOLOv5 and
    near-infrared imaging technologies to classify and detect pests in agricultural
    landscapes accurately, and they were able to do so with a significant mean average
    precision (mAP) of 99.7%. Similarly, Chen et al. [27] suggested an AI mobile-based
    approach utilizing a particular dataset, especially for pest identification in
    agricultural areas. To achieve a high identification score, they examined various
    pre-trained DL models, including YOLOv4, single-shot detectors (SSDs), and faster
    R-CNNs. Notable achievements include a 100% F1 score for mealybugs, an 89% F1
    score for coccidia, and a 97% F1 score. YOLOv4 has also demonstrated remarkable
    performance in terms of F1 score. Legaspi et al. [28] developed a YOLOv3 model
    for identifying pests, such as fruit flies and white flies, using hardware alternatives,
    such as Raspberry Pi, desktop, and online apps accessible to the general population.
    Their model achieved 83.07% accuracy for pest classification and detection but
    required additional refinement for more accurate predictions. Furthermore, using
    a smartphone application, Karar et al. [29] presented a DL method, with the Faster-RCNN
    architecture outperforming other advanced DL models, and achieved a remarkable
    99.0% accuracy for pest detection in agricultural fields. To identify and categorize
    red palm weevils (RPW), Alsanea et al. [30] developed a region-based CNN that
    performs best in evaluation matrices when utilizing the RPW dataset. The model’s
    complexity and speed of inference make it hard to implement in real time. Using
    a customized dataset, Liu et al. [31] developed a YOLOv3-based DL model for tomato
    disease and pest identification in realistic agricultural environments. In the
    above-mentioned method, Faster-RCNN has a large number of learning parameters
    and model size, which restrict the model from real-world implementation. In the
    context of the YOLO base model, there is a need for further improvement in deep-learning-based
    object detection to detect small objects in complex backgrounds with higher inference
    speeds [32]. Furthermore, YOLOv5 models have demonstrated effectiveness in object
    detection, but their conventional versions may lack optimal efficiency and precision
    in the context of pest detection, particularly when applied to UAV-based monitoring
    in agricultural settings. The inherent challenges of agricultural environments,
    such as diverse crop types, varying terrains, and the need for real-time monitoring,
    can strain the performance of traditional YOLOv5 models. These models may not
    be fully adapted to the intricacies of pest identification within the dynamic
    and variable conditions present in agricultural fields. As a result, there is
    a potential for reduced accuracy and reliability in pest classification, limiting
    their practicality for comprehensive and real-time pest management solutions.
    This underscores the necessity for tailored optimizations, as presented in our
    study, to enhance the efficacy of YOLOv5 models in addressing the specific demands
    of UAV-centric pest detection systems in sustainable agriculture. 3. The Proposed
    Methodology Based on insights from the existing literature, we adapted a modified
    YOLOv5s model to effectively identify the region of interest in each image and
    subsequently assign appropriate class labels to them. The entire steps followed
    by dataflow of the proposed model is given in Figure 2, while technical detail
    is provided in the following subsections. Figure 2. The generic overview of the
    proposed work for pest detection comprises the following components: (a) the backbone
    module, (b) the neck module, and (c) the output head module. “Focus module” and
    “Stem module” presented in green and pink box are our modification blocks in YOLO.
    3.1. The Proposed Pest Detection Model Object-detection models play a crucial
    role in identifying and categorizing specific regions in an image. These models
    are widely used in various areas of computer vision, owing to their performance
    and efficiency. However, selecting object recognition models for specific areas
    can be challenging when determining the exact location of objects and assigning
    the correct label while saving computational resources. Several studies have been
    conducted to construct object identification models to modify the YOLO-based model,
    which has shown remarkable progress. The original YOLO approach, initiated by
    Redmon et al. [33], addresses object detection as a regression challenge rather
    than classification. This approach detects target objects using a single neural
    network, resulting in impressive performance and real-time inference capabilities.
    In addition, YOLO-based object recognition models show exceptional generalizability,
    as they quickly adapt to recognize a wide range of objects through training. Enhanced
    crop monitoring using UAVs, high-resolution data for yield prediction, and detecting
    and preventing crop diseases are integral components that enhance the efficacy
    of our proposed approach. The YOLO architecture has recently undergone several
    improvements to boost its efficacy and efficiency for insect identification. From
    2016 to 2018, Redmon and Farhadi [34] proposed the initial three versions of the
    YOLO-based object-detection models, which attracted several researchers due to
    its fast inference speed and model accuracy YOLOv4, showing an impressive average
    accuracy of 43.5% on the MS-COCO dataset while maintaining high speed, was introduced
    in 2020 by Bochkovskiy et al. [35]. Additionally, YOLOv5 was introduced by Feng
    et al. [36], achieving a remarkable performance and speed turning point. There
    are five variations of YOLOv5, with diverse feature map depths and breadths: YOLOv5x,
    YOLOv5l, YOLOv5m, YOLOv5s, and YOLOv5n. A thorough evaluation of the MS-COCO dataset
    underscored the exceptional results of these models. YOLOv5n excelled in fast
    inference, whereas YOLOv5x exhibited the highest object-detection accuracy. These
    models have common structural elements, including input, backbone, neck, and prediction
    components [37]. Compared to previous YOLO iterations, YOLOv5 provides higher
    recognition accuracy, lower computational complexity, and a smaller model footprint,
    making it optimal for resource-constrained devices. This study leveraged YOLOv5’s
    improved recognition capabilities and real-time inference to refine its inherent
    architecture and to enable efficient and reliable insect recognition. 3.2. Network
    Architecture This study used the YOLOv5s object identification system as our primary
    model because of its quick inference and outstanding performance. The goal of
    this study was to optimize this system for pest or insect detection. To this end,
    we made several modifications to the YOLOv5s model to identify small and large
    insects in the image effectively. The input, backbone, neck, and head modules
    comprise the modified model’s four essential components. The three sub-modules
    comprise the input module: imagine scaling, adaptive anchor-box computation, and
    pre-processing of the mosaic data. The pre-processing phase of the mosaic data
    includes three techniques: random scaling, cropping, and order, which introduce
    variability in the positioning of image segments that improve the network’s ability
    to detect smaller objects, such as insects. The various modules architecture used
    in the proposed model are given in Figure 3. Figure 3. Various modules architecture
    used in the proposed YOLO for pest detection. To improve the recognition accuracy
    for small objects and expand the range of available data, we randomly select the
    fusion point for merging images. During model training, YOLOv5s dynamically generates
    multiple prediction boxes based on the initial anchor box. Non-maximum suppression
    (NMS) was also used to determine which prediction box resembled the original box
    the most. We continually resized the adaptively zoomed picture before entering
    it into the network for identification, removing any potential inconsistencies
    resulting from different image sizes and ensuring compatibility with the feature
    tensor and the fully connected layer. The YOLOv5 backbone architecture uses a
    CSPNet backbone based on the Dense Net architecture and easily integrates the
    focus module. The focus module is a critical component that performs down-sampling
    by decomposing the input image, which is initially 640 × 640 × 3, and then concatenated
    to produce a 320 × 320 × 12 feature map. However, our analysis of YOLOv5 models
    that have already been trained indicates that the focus layer struggles to effectively
    capture spatial information from tiny objects, which impacts model performance.
    We suggest adding a Stem unit following the focus module to overcome this restriction,
    as shown in Figure 2. The Stem module facilitates the creation of more sophisticated
    feature maps by providing additional down-sampling with a stride of two and an
    increase in channel dimensions. The Stem module significantly enhances display
    possibilities while just slightly increasing processing complexity. In the YOLOv5
    model, the CSP module is constructed using a series of 1 × 1 and 3 × 3 convolutional
    layers. This module divides the initial input into two segments, each passing
    through its processing path. The first segment is fully processed by a CBS block
    consisting of convolution, normalization of batch size, and activation functions
    of SILU, as in the ingenious work of Elfwing et al. [38]. The second segment passes
    through a convolutional layer, as shown in Figure 1. A CBS module is then added
    after merging the two partitions. In addition, the spatial pyramid pooling (SPP)
    block plays an important role in expanding the receptive field and extracting
    essential features, which are then proficiently passed to the feature aggregation
    block. This adaptation aimed to improve the ability of the network to identify
    insects by extracting relevant features from smaller objects, ultimately leading
    to better insect detection performance. In contrast to using multiple CNN models
    that require fixed-size input images, the integration of SSP results in the generation
    of a fixed-size output. This approach also facilitates the acquisition of important
    features by pooling the different scales of the same module. In our modified version,
    the SSP block replaces traditional sizes of the kernel, such as 5 × 5, 9 × 9,
    and t3 × 13, with 3 × 3, 5 × 5, and 7 × 7, as shown in Figure 2. To further improve
    accuracy, The input image’s longer edges normalized to 640 pixels, whereas the
    smaller edge was adjusted appropriately. Furthermore, the shortest edge coincided
    with the maximum step size of the SPP block. In the absence of P6, the shortest
    edge had a factor of 32. If P6, however, the shorter edge must be a factor of
    64. The neck component plays a critical role in integrating the feature maps generated
    by various folds in the spine, preparing them for the head segment. According
    to Hu et al. [39], the neck segment has PAN and FPN structures to improve the
    network’s feature fusion capabilities. The PAN technology is typically used by
    YOLOv5, producing three output features: P3, P4, and P5, with dimensions of 80
    × 80 × 16, 40 × 40 × 16, and 20 × 20 × 16. But, as you can see in Figure 2, we
    added an extra P6 output block to our customized model, with a 10 × 10 × 16-pixel
    feature map. The ability of the model to identify both large and small objects
    in the input image is greatly enhanced by this addition, which was previously
    employed for face detection by Qi et al. [38]. Lastly, convolutional layers are
    used in the head component to identify objects by defining bounding rectangles
    around the regions of significance and then classifying them. Our model incorporated
    a comprehensive scaling method that uniformly modified the backbone, neck, and
    head’s resolution, width, and depth. This all-encompassing modification guarantees
    higher accuracy and efficiency for insect detection. 4. Experiments and Results
    This section provides a comprehensive overview of the experimental setup, evaluation
    parameters, dataset selection, model performance, and comparison with state-of-the-art
    methods. 4.1. Experimental Setup To implement the proposed work, we employed PyTorch
    with CUDA support to analyze pest and insect detection results. Our hardware setup
    included an NVIDIA (GeForce RTX 3070 GPU) with 32 GB of RAM. We evaluated our
    model with various hyperparameter settings, and the optimal performance was obtained
    with the following configuration: a batch size of 32, the SGD optimizer with a
    learning rate of 0.001, and training for 200 epochs. The proposed model is evaluated
    using well-known evaluation metrics such as precision, recall, and mAP [40] and
    considered state-of-the-art evaluation metrics in the target domain. Precision
    is an evaluation metric used in ML and DL to assess model performance. It is defined
    as the ratio of true-positive samples to the sum of true-positive and false-positive
    samples, as expressed in Equation (1). P= TP TP+FP (1) Recall is another evaluation
    metric that focuses solely on positive samples within a given dataset, excluding
    negative ones, as formulated in Equation (2). R= TP TP+FN (2) The mAP metric is
    used to assess object-detection models. It computes a score based on the relationship
    between the localized boxes and ground-truth bounding boxes. Achieving the highest
    score is an indicator of an accurate detection approach, as formalized by Equation
    (3). mAP= 1 N ∑ N i−1 A P i (3) In the equations, TP represents the number of
    correctly identified positive samples, while FP signifies the number of negative
    samples that are falsely identified as positive. Similarly, FN corresponds to
    the number of misclassified positive samples. 4.2. Dataset Selection The data-collection
    method is essential for the efficacy of model training in artificial intelligence.
    This information is meticulously organized into five taxonomic groups: Ants, Grasshoppers,
    Palm Weevils, Shield Bugs, and Wasps. The classes were organized as follows: the
    ant class contained 392 images, the grasshopper class had 315 images, and the
    palm weevil, shield bug, and wasp categories had 148, 392, and 318 images, respectively.
    We used 70% of the dataset for training, 20% for validation, and 10% for testing.
    Figure 4 demonstrates the total number of instances used to train the proposed
    model. Figure 4. Pest or insect detection dataset distribution for training purposes.
    The dataset was carefully annotated in accordance with the object recognition
    model [6] using a publicly available annotation tool from GitHub (accessed on
    23 June 2022). The annotation process was written in Python with the cross-platform
    Qt GUI (graphical user interface) toolkit. To match the criteria of the YOLO-based
    model, the data collection was annotated in YOLO format, which requires annotated
    files to be in .txt format. The dataset is divided into three sub-categories training,
    validation, and testing which contain 70%, 20%, and 10% of the data, respectively.
    Figure 5 shows samples of the pests from each dataset class. Figure 5. Various
    samples of pests for each dataset class. 4.3. The Proposed Model Evaluation In
    this subsequent section, we provide a detailed description of the training process
    for our model. We used an early stopping algorithm to halt the training once the
    model has reached a certain level of performance and can no longer learn efficiently
    and effectively. The evaluation of the proposed model includes the examination
    of various metrics such as loss, recall, precision, and mAP, as depicted in Figure
    6. Figure 6. The X-axis shows the number of epochs, and the Y-axis shows the corresponding
    score of each evaluation matrix. This shows the model’s efficacy using various
    evaluation metrics. The loss graph visually represents the model’s capability
    to identify objects accurately, indicating its proficiency in performing tasks
    effectively. The object loss function assesses the model’s ability to complete
    tasks within relevant regions of interest, and an increase in accuracy corresponds
    to a decrease in the loss function. Accurate and effective object classification
    relies on reducing associated losses. In Figure 6, Bounding box loss, class loss,
    and distribution focal loss in the training phase are reached to a minimum of
    0.50, 0.02, and 1.0, respectively. Similarly, these metrics in the validation
    phase reached 1.1, 0.50, and 1.95, respectively. In the context of ML and ML,
    recall and precision are important metrics for assessing model performance. Elevated
    precision and recall, as observed in Figure 6, signify enhanced model accuracy.
    The loss function value consistently decreases during the training process, and
    the model demonstrates a continuous ability to reduce loss and rapidly improve
    recall, and precision within a few epochs. At the last epoch, the precision and
    recall in the training phase reached 0.96 and 0.93 as shown in Figure 6. Furthermore,
    the maximum accuracy, recall, and mAP values are achieved at approximately the
    120th epoch, highlighting the efficacy of our model. Therefore, the proposed model
    proves to be a robust and capable solution across various evaluation metrics.
    We conducted a performance evaluation of the proposed model using the test set
    of the dataset, employing a confusion matrix for in-depth analysis, as shown in
    Figure 7. This confusion matrix encompasses five distinct pest classes and introduces
    an additional category, “background FN”, to emphasize scenarios where the model
    failed to identify objects within the image. To present a comprehensive insight
    into the confusion matrix, we closely analyzed the accuracy in predicting various
    pest classes, including ants, grasshoppers, palm weevils, shield bugs, and wasps.
    The accuracy values for these categories were 0.79, 1.00, 0.78, 0.96, and 0.86,
    respectively. Figure 7. The proposed model’s confusion matrix using a self-created
    dataset. 4.4. Comparative Analysis with State-of-the-Art Models In this subsequent
    section, we provide a comprehensive comparison of the proposed model with nine
    other state-of-the-art models. The proposed model consistently demonstrates superior
    performance in pest detection, particularly excelling in the identification of
    ants, grasshoppers, palm weevils, shield bugs, and wasps. The results are summarized
    in Table 1, indicating that the Faster-RCNN emerges as a powerful contender, achieving
    promising performance. However, the proposed model surpasses Faster-RCNN with
    higher average precision, recall, and mAP values of 0.04, 0.03, and 0.3, respectively.
    Table 1. Analysis of the proposed model with Faster-RCNN and various versions
    of the YOLO models. YOLOv3 and YOLOv4 exhibit relatively weaker performance compared
    to other models. YOLOv5n falls short, with average values of 0.87, 0.88, and 0.89
    for precision, recall, and mAP. YOLOv5s achieves 0.91 accuracy, 0.83 recall, and
    0.90 mAP, while YOLOv5m outperforms YOLOv5s models, obtaining an average precision
    value of 0.94, recall of 0.84, and mAP of 0.91. YOLOv5l and YOLOv5x exhibit better
    performance, as given in Table 1. The EPD model also outperforms other models,
    but the proposed model surpasses the EPD with higher precision, recall, and mAP
    values of 0.02, 0.03, and 0.1, respectively. Therefore, Table 1 highlights that
    our proposed model achieves superior performance across all evaluation metrics.
    4.5. Splitting Dataset Using 5-Fold Cross Validation In this strategy, the whole
    dataset is utilized for training and validation by making five different folds
    to effectively evaluate the model performance. Figure 8 shows that the proposed
    model obtained optimal performance for each fold in terms of precision, recall
    and mAP. The proposed model obtained precision values for each fold: Fold 1: 96.88%,
    Fold 2: 96.34%, Fold 3: 96.16%, Fold 4: 95.4a 5%, and Fold 5: 95.17%. The recall
    value of 93.80%, 93.45%, 93.10%, 92.53%, and 92.12% for Fold 1, Fold 2, Fold 3,
    Fold 4, and Fold 5, respectively. Similarly, the proposed model achieved higher
    mAP of 95.83%, 95.42%, 95.00%, 94.68%, and 94.07%, respectively. In conclusion,
    our model obtained an average precision, recall, and mAP of 96.00%, 93.00%, and
    95.00%, respectively. Figure 8. Illustrated the model generalization capability
    using 5-fold cross validation. 4.6. Model Complexity Analysis Table 2 provides
    a comprehensive assessment of the proposed model’s feasibility, calculating giga
    floating point operations per second (GFLOPs), model size, and FPS for all models,
    which are then compared to the suggested approach. In Table 2, the proposed model
    is compared with YOLOv5n, YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. In the comparison,
    the YOLOv5n model exhibited faster inference speed with reduced GFLOPs and model
    size. However, the YOLOv5n model obtained poor performance as shown in Table 1.
    In contrast, when compared with YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x, the proposed
    model obtained significantly higher FPS, with 1.50, 12.59, 16.13, and 18.85 times
    the speed, respectively. This underscores that our model combines high performance
    with fast inference speed while maintaining a favorable balance of GFLOPs and
    model size. The results obtained show the efficacy of our model, suggesting that
    it is a viable solution for real-world application. Table 2. Comparative Analysis
    of our model with different YOLOv5 versions in terms of model size, GFLOPs, and
    FPS using CPU. 4.7. Visual Result of the Proposed Model We performed a visual
    analysis of the output predictions of the suggested model to evaluate how robust
    it was. The results of the model for the localization and identification of five
    different insects or pests are shown in Figure 9. This figure clearly shows how
    well our model selects regions of interest and correctly labels classes. It performs
    admirably in both detection and recognition tasks, accurately delineating bounding
    boxes around objects and precisely labeling their classes. This visual representation
    highlights the model’s potential for real-time applications. However, it is worth
    noting that there are some limitations to the proposed approach, as highlighted
    in Figure 9. While most images are accurately classified, exceptions exist. For
    instance, the first and last images in the fifth row present recognition challenges.
    Additionally, instances of misdetection are observed in the second and third images
    of the second row, as well as in the last image of the third row. These visual
    results offer valuable insights into the model’s performance, acknowledging its
    strengths while identifying areas where improvement may be needed. Figure 9. The
    visual results of the proposed model, which show the model effective analysis.
    4.8. Discussion Our study addresses the challenge of crop diseases and pest infestations
    in agriculture by leveraging UAV technology for enhanced crop monitoring through
    UAVs and improving the detection and classification of agricultural pests. Conventional
    methods often involve laborious manual feature extraction or computationally intensive
    DL techniques. To overcome these limitations, we present an optimized model specifically
    tailored for UAV-based applications. Our modifications to the proposed YOLOv5s
    model incorporate advanced attention modules, expanded cross-stage partial network
    modules, and refined multiscale feature extraction mechanisms, enabling precise
    pest detection and classification. Inspired by the efficiency and adaptability
    of UAVs, our research aims to transform pest management into sustainable agriculture
    while also combating crop diseases. We conducted extensive experiments on a medium-scale
    dataset, identifying five agricultural pests: ants, grasshoppers, palm weevils,
    shield bugs, and wasps. Our thorough experimental analysis demonstrates superior
    performance compared to various Faster-RCNN [41,42] and YOLO model versions [43,44].
    Compared with existing methodologies, our model demonstrates competitive performance.
    For instance, while Faster-PestNet [41] achieves an accuracy of 82.43% on the
    IP102 dataset. Similarly, Pest-YOLO [43] and PestLite [44] achieve mean average
    precision scores of 73.4% and 90.7%, respectively. Jiao et al. [45] integrated
    an anchor-free convolutional neural network (AF-RCNN) with Faster-RCNN for pest
    detection on the Pest24 dataset, yielding an mAP and mRecall of 56.4% and 85.1%,
    respectively. Wang et al. [46] employed four detection networks, including YOLOv3,
    SSD, Faster-RCNN, and Cascade-RCNN, for detecting 24 common pests, with YOLOv3
    achieving the highest mAP value of 63.54%. Furthermore, AgriPest-YOLO achieves
    a mean average precision (mAP) of 71.3% on a multi-pest image dataset by integrating
    a coordination and local attention mechanism, grouping spatial pyramid pooling
    fast, and soft non-maximum suppression, facilitating efficient and accurate real-time
    pest detection from light-trap images [47]. The proposed model achieved higher
    performance metrics, with an average precision of 96.0%, average recall of 93.0%,
    and mean average precision (mAP) of 95.0%, as shown in Table 1, which shows that
    our proposed model achieved the highest accuracy score than other SOTA models.
    Visual results of the proposed modified YOLOv5s are shown in Figure 9. Furthermore,
    the inherent capabilities of UAVs, coupled with the YOLOv5s model evaluated in
    this study, offer a reliable solution for real-time pest detection, showcasing
    significant potential to optimize and enhance agricultural production within a
    drone-centric ecosystem. 5. Conclusions In our study evaluating nine object recognition
    models, the standout performer was our specialized UAV-oriented model. Throughout
    the experiments, conducted with a meticulously curated dataset featuring five
    distinct insect species, ants, grasshoppers, palm weevils, shield bugs, and wasps,
    our model consistently outshone its counterparts in accuracy, recall, and mean
    average precision (mAP). What sets our model apart is not just its impressive
    performance but also its efficiency, demonstrating superior inference capabilities
    while demanding fewer computational GFLOPs and maintaining a more manageable model
    size. This positions our proposed model as a robust solution for real-time species
    detection, highlighting its prowess in the context of UAV and technology integration.
    Moreover, our model excels in enhanced crop monitoring using UAVs, demonstrates
    proficiency in handling high-resolution data for yield prediction, and proves
    effective in detecting and preventing crop diseases. Author Contributions Conceptualization,
    A.K. and S.J.M.; methodology, A.K., F.B. and S.J.M.; software, A.K.; validation,
    A.K., F.B. and L.M.D.; formal analysis, H.-K.S. and F.B.; investigation, S.J.M.
    and L.M.D.; resources, S.J.M.; data curation, A.K., F.B. and H.-K.S.; writing—original
    draft preparation, A.K. and F.B.; writing—review and editing, L.M.D., H.-K.S.,
    H.M. and F.B.; visualization, A.K. and H.-K.S.; supervision, H.-K.S. and L.M.D.;
    project ad-ministration, S.J.M.; funding acquisition, H.M. All authors have read
    and agreed to the published version of the manuscript. Funding This work was supported
    by Basic Science Research Program through the National Research Foundation of
    Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540) and by Institute
    of Information & communications Technology Planning & Evaluation (IITP) under
    the metaverse support program to nurture the best talents (IITP-2023-RS-2023-00254529)
    grant funded by the Korea government (MSIT) and by Korea Institute of Planning
    and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries (IPET)
    through Digital Breeding Transformation Technology Development Program, funded
    by Ministry of Agriculture, Food and Rural Affairs (MAFRA) (322063-03-1-SB010).
    Data Availability Statement Data is contained within the article. Conflicts of
    Interest The authors declare no conflicts of interest. References Amiri, A.N.;
    Bakhsh, A. An effective pest management approach in potato to combat insect pests
    and herbicide. 3 Biotech 2019, 9, 16. [Google Scholar] [CrossRef] Fernández, R.M.;
    Petek, M.; Gerasymenko, I.; Juteršek, M.; Baebler, Š.; Kallam, K.; Giménez, E.M.;
    Gondolf, J.; Nordmann, A.; Gruden, K.; et al. Insect pest management in the age
    of synthetic biology. Plant Biotechnol. J. 2022, 20, 25–36. [Google Scholar] [CrossRef]
    Habib, S.; Khan, I.; Aladhadh, S.; Islam, M.; Khan, S. External Features-Based
    Approach to Date Grading and Analysis with Image Processing. Emerg. Sci. J. 2022,
    6, 694–704. [Google Scholar] [CrossRef] Zhou, J.; Li, J.; Wang, C.; Wu, H.; Zhao,
    C.; Teng, G. Crop disease identification and interpretation method based on multimodal
    deep learning. Comput. Electron. Agric. 2021, 189, 106408. [Google Scholar] [CrossRef]
    Khan, H.; Haq, I.U.; Munsif, M.; Mustaqeem; Khan, S.U.; Lee, M.Y. Automated Wheat
    Diseases Classification Framework Using Advanced Machine Learning Technique. Agriculture
    2022, 12, 1226. [Google Scholar] [CrossRef] Aladhadh, S.; Habib, S.; Islam, M.;
    Aloraini, M.; Aladhadh, M.; Al-Rawashdeh, H.S. An Efficient Pest Detection Framework
    with a Medium-Scale Benchmark to Increase the Agricultural Productivity. Sensors
    2022, 22, 9749. [Google Scholar] [CrossRef] Al Hiary, H.; Ahmad, S.B.; Reyalat,
    M.; Braik, M.; Alrahamneh, Z. Fast and accurate detection and classification of
    plant diseases. Int. J. Comput. Appl. 2011, 17, 31–38. [Google Scholar] [CrossRef]
    Nguyen, T.N.; Lee, S.; Nguyen-Xuan, H.; Lee, J. A novel analysis-prediction approach
    for geometrically nonlinear problems using group method of data handling. Comput.
    Methods Appl. Mech. Eng. 2019, 354, 506–526. [Google Scholar] [CrossRef] Faithpraise,
    F.; Birch, P.; Young, R.; Obu, J.; Faithpraise, B.; Chatwin, C. Automatic plant
    pest detection and recognition using k-means clustering algorithm and correspondence
    filters. Int. J. Adv. Biotechnol. Res. 2013, 4, 189–199. [Google Scholar] Rumpf,
    T.; Mahlein, A.-K.; Steiner, U.; Oerke, E.-C.; Dehne, H.-W.; Plümer, L. Early
    detection and classification of plant diseases with support vector machines based
    on hyperspectral reflectance. Comput. Electron. Agric. 2010, 74, 91–99. [Google
    Scholar] [CrossRef] Nguyen, T.N.; Nguyen-Xuan, H.; Lee, J. A novel data-driven
    nonlinear solver for solid mechanics using time series forecasting. Finite Elem.
    Anal. Des. 2020, 171, 103377. [Google Scholar] [CrossRef] Shijie, J.; Peiyi, J.;
    Siping, H. Automatic detection of tomato diseases and pests based on leaf images.
    In Proceedings of the 2017 Chinese Automation Congress (CAC), Jinan, China, 20–22
    October 2017. [Google Scholar] Gandhi, R.; Nimbalkar, S.; Yelamanchili, N.; Ponkshe,
    S. Plant disease detection using CNNs and GANs as an augmentative approach. In
    Proceedings of the 2018 IEEE International Conference on Innovative Research and
    Development (ICIRD), Bangkok, Thailand, 11–12 May 2018. [Google Scholar] Leonardo,
    M.M.; Carvalho, T.J.; Rezende, E.; Zucchi, R.; Faria, F.A. Deep feature-based
    classifiers for fruit fly identification (Diptera: Tephritidae). In Proceedings
    of the 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI),
    Parana, Brazil, 29 October–1 November 2018. [Google Scholar] Dawei, W.; Limiao,
    D.; Jiangong, N.; Jiyue, G.; Hongfei, Z.; Zhongzhi, H. Recognition pest by image-based
    transfer learning. J. Sci. Food Agric. 2019, 99, 4524–4531. [Google Scholar] [CrossRef]
    [PubMed] Cheeti, S.; Kumar, G.S.; Priyanka, J.S.; Firdous, G.; Ranjeeva, P.R.
    Pest detection and classification using YOLO AND CNN. Ann. Rom. Soc. Cell Biol.
    2021, 15295–15300. [Google Scholar] Mique, E.L., Jr.; Palaoag, T.D. Rice pest
    and disease detection using convolutional neural network. In Proceedings of the
    1st International Conference on Information Science and Systems, Jeju, Republic
    of Korea, 27–29 April 2018. [Google Scholar] [CrossRef] Nam, N.T.; Hung, P.D.
    Pest detection on traps using deep convolutional neural networks. In Proceedings
    of the 1st International Conference on Control and Computer Vision, Singapore,
    15–18 June 2018. [Google Scholar] Burhan, S.A.; Minhas, S.; Tariq, A.; Hassan,
    M.N. Comparative study of deep learning algorithms for disease and pest detection
    in rice crops. In Proceedings of the 2020 12th International Conference on Electronics,
    Computers and Artificial Intelligence (ECAI), Bucharest, Romania, 25–27 June 2020.
    [Google Scholar] Koubaa, A.; Aldawood, A.; Saeed, B.; Hadid, A.; Ahmed, M.; Saad,
    A.; Alkhouja, H.; Ammar, A.; Alkanhal, M. Smart Palm: An IoT framework for red
    palm weevil early detection. Agronomy 2020, 10, 987. [Google Scholar] [CrossRef]
    Habib, S.; Khan, I.; Islam, M.; Albattah, W.; Alyahya, S.M.; Khan, S.; Hassan,
    M.K. Wavelet frequency transformation for specific weeds recognition. In Proceedings
    of the 2021 1st International Conference on Artificial Intelligence and Data Analytics
    (CAIDA), Riyadh, Saudi Arabia, 6–7 April 2021. [Google Scholar] Esgario, J.G.;
    de Castro, P.B.; Tassis, L.M.; Krohling, R.A. An app to assist farmers in the
    identification of diseases and pests of coffee leaves using deep learning. Inf.
    Process. Agric. 2022, 9, 38–47. [Google Scholar] [CrossRef] Hansen, O.L.P.; Svenning,
    J.; Olsen, K.; Dupont, S.; Garner, B.H.; Iosifidis, A.; Price, B.W.; Høye, T.T.
    Species-level image classification with convolutional neural network enables insect
    identification from habitus images. Ecol. Evol. 2020, 10, 737–747. [Google Scholar]
    [CrossRef] [PubMed] Ullah, N.; Khan, J.A.; Alharbi, L.A.; Raza, A.; Khan, W.;
    Ahmad, I. An Efficient Approach for Crops Pests Recognition and Classification
    Based on Novel DeepPestNet Deep Learning Model. IEEE Access 2022, 10, 73019–73032.
    [Google Scholar] [CrossRef] Li, W.; Zhu, T.; Li, X.; Dong, J.; Liu, J. Recommending
    advanced deep learning models for efficient insect pest detection. Agriculture
    2022, 12, 1065. [Google Scholar] [CrossRef] Hu, Z.; Xiang, Y.; Li, Y.; Long, Z.;
    Liu, A.; Dai, X.; Lei, X.; Tang, Z. Research on Identification Technology of Field
    Pests with Protective Color Characteristics. Appl. Sci. 2022, 12, 3810. [Google
    Scholar] [CrossRef] Chen, J.-W.; Lin, W.-J.; Cheng, H.-J.; Hung, C.-L.; Lin, C.-Y.;
    Chen, S.-P. A smartphone-based application for scale pest detection using multiple-object
    detection methods. Electronics 2021, 10, 372. [Google Scholar] [CrossRef] Legaspi,
    K.R.B.; Sison, N.W.S.; Villaverde, J.F. Detection and Classification of Whiteflies
    and Fruit Flies Using YOLO. In Proceedings of the 2021 13th International Conference
    on Computer and Automation Engineering (ICCAE), Melbourne, Australia, 20–22 March
    2021. [Google Scholar] Karar, M.E.; Alsunaydi, F.; Albusaymi, S.; Alotaibi, S.
    A new mobile application of agricultural pests recognition using deep learning
    in cloud computing system. Alex. Eng. J. 2021, 60, 4423–4432. [Google Scholar]
    [CrossRef] Alsanea, M.; Habib, S.; Khan, N.F.; Alsharekh, M.F.; Islam, M.; Khan,
    S. A Deep-Learning Model for Real-Time Red Palm Weevil Detection and Localization.
    J. Imaging 2022, 8, 170. [Google Scholar] [CrossRef] Liu, J.; Wang, X. Tomato
    diseases and pests detection based on improved Yolo V3 convolutional neural network.
    Front. Plant Sci. 2020, 11, 898. [Google Scholar] [CrossRef] [PubMed] Nguyen,
    T.N.; Lee, S.; Nguyen, P.-C.; Nguyen-Xuan, H.; Lee, J. Geometrically nonlinear
    postbuckling behavior of imperfect FG-CNTRC shells under axial compression using
    isogeometric analysis. Eur. J. Mech.-A/Solids 2020, 84, 104066. [Google Scholar]
    [CrossRef] Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once:
    Unified, real-time object detection. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016.
    [Google Scholar] Redmon, J.; Farhadi, A. Yolov3: An incremental improvement. arXiv
    2018, arXiv:1804.02767. [Google Scholar] Bochkovskiy, A.; Wang, C.-Y.; Liao, H.-Y.M.
    Yolov4: Optimal speed and accuracy of object detection. arXiv 2020, arXiv:2004.10934.
    [Google Scholar] Feng, Z.; Guo, L.; Huang, D.; Li, R. Electrical insulator defects
    detection method based on yolov5. In Proceedings of the 2021 IEEE 10th Data Driven
    Control and Learning Systems Conference (DDCLS), Suzhou, China, 14–16 May 2021.
    [Google Scholar] Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection
    network model based on YOLOv5. Eng. Appl. Artif. Intell. 2022, 113, 104914. [Google
    Scholar] [CrossRef] Elfwing, S.; Uchibe, E.; Doya, K. Sigmoid-weighted linear
    units for neural network function approximation in reinforcement learning. Neural
    Netw. 2018, 107, 3–11. [Google Scholar] [CrossRef] Hu, W.; Xiong, J.; Liang, J.;
    Xie, Z.; Liu, Z.; Huang, Q.; Yang, Z. A method of citrus epidermis defects detection
    based on an improved YOLOv5. Biosyst. Eng. 2023, 227, 19–35. [Google Scholar]
    [CrossRef] Li, D.; Ahmed, F.; Wu, N.; Sethi, A.I. Yolo-JD: A Deep Learning Network
    for jute diseases and pests detection from images. Plants 2022, 11, 937. [Google
    Scholar] [CrossRef] Ali, F.; Qayyum, H.; Iqbal, M.J. Faster-PestNet: A Lightweight
    deep learning framework for crop pest detection and classification. IEEE Access
    2023, 11, 104016–104027. [Google Scholar] [CrossRef] Hua, S.; Xu, M.; Xu, Z.;
    Ye, H.; Zhou, C. Multi-feature decision fusion algorithm for disease detection
    on crop surface based on machine vision. Neural Comput. Appl. 2022, 34, 9471–9484.
    [Google Scholar] [CrossRef] Tang, Z.; Lu, J.; Chen, Z.; Qi, F.; Zhang, L. Improved
    Pest-YOLO: Real-time pest detection based on efficient channel attention mechanism
    and transformer encoder. Ecol. Inform. 2023, 78, 102340. [Google Scholar] [CrossRef]
    Dong, Q.; Sun, L.; Han, T.; Cai, M.; Gao, C. PestLite: A Novel YOLO-Based Deep
    Learning Technique for Crop Pest Detection. Agriculture 2024, 14, 228. [Google
    Scholar] [CrossRef] Jiao, L.; Dong, S.; Zhang, S.; Xie, C.; Wang, H. AF-RCNN:
    An anchor-free convolutional neural network for multi-categories agricultural
    pest detection. Comput. Electron. Agric. 2020, 174, 105522. [Google Scholar] [CrossRef]
    Wang, Q.-J.; Zhang, S.-Y.; Dong, S.-F.; Zhang, G.-C.; Yang, J.; Li, R.; Wang,
    H.-Q. Pest24: A large-scale very small object data set of agricultural pests for
    multi-target detection. Comput. Electron. Agric. 2020, 175, 105585. [Google Scholar]
    [CrossRef] Zhang, W.; Huang, H.; Sun, Y.; Wu, X. AgriPest-YOLO: A rapid light-trap
    agricultural pest detection method based on deep learning. Front. Plant Sci. 2022,
    13, 1079384. [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements,
    opinions and data contained in all publications are solely those of the individual
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or
    the editor(s) disclaim responsibility for any injury to people or property resulting
    from any ideas, methods, instructions or products referred to in the content.  ©
    2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open
    access article distributed under the terms and conditions of the Creative Commons
    Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share
    and Cite MDPI and ACS Style Khan, A.; Malebary, S.J.; Dang, L.M.; Binzagr, F.;
    Song, H.-K.; Moon, H. AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data. Plants 2024, 13, 653. https://doi.org/10.3390/plants13050653
    AMA Style Khan A, Malebary SJ, Dang LM, Binzagr F, Song H-K, Moon H. AI-Enabled
    Crop Management Framework for Pest Detection Using Visual Sensor Data. Plants.
    2024; 13(5):653. https://doi.org/10.3390/plants13050653 Chicago/Turabian Style
    Khan, Asma, Sharaf J. Malebary, L. Minh Dang, Faisal Binzagr, Hyoung-Kyu Song,
    and Hyeonjoon Moon. 2024. \"AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data\" Plants 13, no. 5: 653. https://doi.org/10.3390/plants13050653
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 27. Feb 3. Mar 8. Mar 13. Mar
    18. Mar 23. Mar 28. Mar 2. Apr 0 200 400 600 800 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Plants, EISSN 2223-7747, Published by MDPI RSS Content Alert Further
    Information Article Processing Charges Pay an Invoice Open Access Policy Contact
    MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians
    For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum
    MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series
    Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications
    and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel,
    Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Plants
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: AI-Enabled Crop Management Framework for Pest Detection Using Visual Sensor
    Data
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alohali M.A.
  - Al-Mutiri F.
  - Othman K.M.
  - Yafoz A.
  - Alsini R.
  - Salama A.S.
  citation_count: '0'
  description: 'Smart agricultural techniques employ current information and communication
    technologies, leveraging artificial intelligence (AI) for effectually managing
    the crop. Recognizing rice seedlings, which is crucial for harvest estimation,
    traditionally depends on human supervision but can be expedited and enhanced via
    computer vision (CV). Unmanned aerial vehicles (UAVs) equipped with high-resolution
    cameras bestow a swift and precise option for crop condition surveillance, specifically
    in cloudy states, giving valuable insights into crop management and breeding programs.
    Therefore, we improved an enhanced tunicate swarm algorithm with deep learning-based
    rice seedling classification (ETSADL-RSC). The presented ETSADL-RSC technique
    examined the UAV images to classify them into two classes: Rice seedlings and
    arable land. Initially, the quality of the pictures could be enhanced by a contrast
    limited adaptive histogram equalization (CLAHE) approach. Next, the ETSADL-RSC
    technique used the neural architectural search network (NASNet) method for the
    feature extraction process and its hyperparameters could be tuned by the ETSA
    model. For rice seedling classification, the ETSADL-RSC technique used a sparse
    autoencoder (SAE) model. The experimental outcome study of the ETSADL-RSC system
    was verified for the UAV Rice Seedling Classification dataset. Wide simulation
    analysis of the ETSADL-RSC model stated the greater accuracy performance of 97.79%
    over other DL classifiers.'
  doi: 10.3934/math.2024498
  full_citation: '>'
  full_text: '>

    "Home Journals Journal Home About Contribute Articles AIMS Mathematics 2024, Volume
    9, Issue 4: 10185-10207. doi: 10.3934/math.2024498 Previous Article Next Article
    Research article Special Issues An enhanced tunicate swarm algorithm with deep-learning
    based rice seedling classification for sustainable computing based smart agriculture
    Manal Abdullah Alohali 1 ,  Fuad Al-Mutiri 2 ,  Kamal M. Othman 3 ,  , ,  Ayman
    Yafoz 4 ,  Raed Alsini 4 ,  Ahmed S. Salama 5 1. Department of Information Systems,
    College of Computer and Information Sciences, Princess Nourah bint Abdulrahman
    University, P.O. Box 84428, Riyadh 11671, Saudi Arabia 2. Department of Mathematics,
    Faculty of Sciences and Arts, King Khalid University, Muhayil Asir, Saudi Arabia
    3. Department of Electrical Engineering, College of Engineering and Islamic Architecture,
    Umm Al-Qura University, Makkah, Saudi Arabia 4. Department of Information Systems,
    Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah,
    Saudi Arabia 5. Department of Electrical Engineering, Faculty of Engineering &
    Technology, Future University in Egypt, New Cairo 11845, Egypt Received: 21 January
    2024 Revised: 29 February 2024 Accepted: 06 March 2024 Published: 14 March 2024
    MSC : 11Y40 Special Issue: Advances of Artificial Intelligence-based mathematical
    modeling optimization in engineering applications Abstract Full Text(HTML) Download
    PDF Smart agricultural techniques employ current information and communication
    technologies, leveraging artificial intelligence (AI) for effectually managing
    the crop. Recognizing rice seedlings, which is crucial for harvest estimation,
    traditionally depends on human supervision but can be expedited and enhanced via
    computer vision (CV). Unmanned aerial vehicles (UAVs) equipped with high-resolution
    cameras bestow a swift and precise option for crop condition surveillance, specifically
    in cloudy states, giving valuable insights into crop management and breeding programs.
    Therefore, we improved an enhanced tunicate swarm algorithm with deep learning-based
    rice seedling classification (ETSADL-RSC). The presented ETSADL-RSC technique
    examined the UAV images to classify them into two classes: Rice seedlings and
    arable land. Initially, the quality of the pictures could be enhanced by a contrast
    limited adaptive histogram equalization (CLAHE) approach. Next, the ETSADL-RSC
    technique used the neural architectural search network (NASNet) method for the
    feature extraction process and its hyperparameters could be tuned by the ETSA
    model. For rice seedling classification, the ETSADL-RSC technique used a sparse
    autoencoder (SAE) model. The experimental outcome study of the ETSADL-RSC system
    was verified for the UAV Rice Seedling Classification dataset. Wide simulation
    analysis of the ETSADL-RSC model stated the greater accuracy performance of 97.79%
    over other DL classifiers. Keywords: smart agriculture, crop monitoring, rice
    seedling, computer vision, image classification, deep learning Citation: Manal
    Abdullah Alohali, Fuad Al-Mutiri, Kamal M. Othman, Ayman Yafoz, Raed Alsini, Ahmed
    S. Salama. An enhanced tunicate swarm algorithm with deep-learning based rice
    seedling classification for sustainable computing based smart agriculture[J].
    AIMS Mathematics, 2024, 9(4): 10185-10207. doi: 10.3934/math.2024498 Related Papers:
    [1] S. Rama Sree, E Laxmi Lydia, C. S. S. Anupama, Ramya Nemani, Soojeong Lee,
    Gyanendra Prasad Joshi, Woong Cho . A battle royale optimization with feature
    fusion-based automated fruit disease grading and classification. AIMS Mathematics,
    2024, 9(5): 11432-11451. doi: 10.3934/math.2024561 [2] Mashael Maashi, Mohammed
    Abdullah Al-Hagery, Mohammed Rizwanullah, Azza Elneil Osman . Deep convolutional
    neural network-based Leveraging Lion Swarm Optimizer for gesture recognition and
    classification. AIMS Mathematics, 2024, 9(4): 9380-9393. doi: 10.3934/math.2024457
    [3] Abdelwahed Motwake, Aisha Hassan Abdalla Hashim, Marwa Obayya, Majdy M. Eltahir
    . Enhancing land cover classification in remote sensing imagery using an optimal
    deep learning model. AIMS Mathematics, 2024, 9(1): 140-159. doi: 10.3934/math.2024009
    [4] Eman A. Al-Shahari, Marwa Obayya, Faiz Abdullah Alotaibi, Safa Alsafari, Ahmed
    S. Salama, Mohammed Assiri . Accelerating biomedical image segmentation using
    equilibrium optimization with a deep learning approach. AIMS Mathematics, 2024,
    9(3): 5905-5924. doi: 10.3934/math.2024288 [5] Alaa O. Khadidos . Advancements
    in remote sensing: Harnessing the power of artificial intelligence for scene image
    classification. AIMS Mathematics, 2024, 9(4): 10235-10254. doi: 10.3934/math.2024500
    [6] Thavavel Vaiyapuri, M. Sivakumar, Shridevi S, Velmurugan Subbiah Parvathy,
    Janjhyam Venkata Naga Ramesh, Khasim Syed, Sachi Nandan Mohanty . An intelligent
    water drop algorithm with deep learning driven vehicle detection and classification.
    AIMS Mathematics, 2024, 9(5): 11352-11371. doi: 10.3934/math.2024557 [7] Tamilvizhi
    Thanarajan, Youseef Alotaibi, Surendran Rajendran, Krishnaraj Nagappan . Improved
    wolf swarm optimization with deep-learning-based movement analysis and self-regulated
    human activity recognition. AIMS Mathematics, 2023, 8(5): 12520-12539. doi: 10.3934/math.2023629
    [8] Fahad F. Alruwaili . Ensuring data integrity in deep learning-assisted IoT-Cloud
    environments: Blockchain-assisted data edge verification with consensus algorithms.
    AIMS Mathematics, 2024, 9(4): 8868-8884. doi: 10.3934/math.2024432 [9] Yuzi Jin,
    Soobin Kwak, Seokjun Ham, Junseok Kim . A fast and efficient numerical algorithm
    for image segmentation and denoising. AIMS Mathematics, 2024, 9(2): 5015-5027.
    doi: 10.3934/math.2024243 [10] Majdy M. Eltahir, Ghadah Aldehim, Nabil Sharaf
    Almalki, Mrim M. Alnfiai, Azza Elneil Osman . Reinforced concrete bridge damage
    detection using arithmetic optimization algorithm with deep feature fusion. AIMS
    Mathematics, 2023, 8(12): 29290-29306. doi: 10.3934/math.20231499             Reader
    Comments                  Your name:* Email:* We recommend A battle royale optimization
    with feature fusion-based automated fruit disease grading and classification S.
    Rama Sree et al., AIMS Mathematics, 2024 Tunicate swarm algorithm with deep convolutional
    neural network-driven colorectal cancer classification from histopathological
    imaging data Abdullah S. AL-Malaise AL-Ghamdi et al., Mathematical Modelling and
    Control Enhancing land cover classification in remote sensing imagery using an
    optimal deep learning model Abdelwahed Motwake et al., AIMS Mathematics, 2023
    A hybrid network intrusion detection using darwinian particle swarm optimization
    and stacked autoencoder hoeffding tree B. Ida Seraphim et al., Mathematical Biosciences
    and Engineering, 2021 Hybrid arithmetic optimization algorithm with deep learning
    model for secure Unmanned Aerial Vehicle networks Sultanah M. Alshammari et al.,
    AIMS Mathematics, 2024 Betting on drones as smart agricultural tools for pesticide
    use in farms by Tokyo University of Science, Phys.org, 2021 Eyes in the sky: Using
    drones to assess the severity of crop diseases by NanJing Agricultural University,
    Phys.org, 2023 A Knee Point Based Coevolution Multi-objective Particle Swarm Optimization
    Algorithm for Heterogeneous UAV Cooperative Multi-task Allocation WANG Feng et
    al., Acta Automatica Sinica, 2023 UAV swarm task allocation algorithm based on
    the alternating direction method of multipliers network potential game theory
    PENG Ya-lan et al., Chinese Journal of Engineering, 2022 Stacked spectral feature
    space patch: An advanced spectral representation for precise crop classification
    based on convolutional neural network Hui Chen et al., The Crop Journal, 2022
    Powered by © 2024 the Author(s), licensee AIMS Press. This is an open access article
    distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0)
    AIMS Mathematics 2.2 3 Metrics Article Views(116) PDF Downloads(11) Cited By(0)
    Preview PDF Download XML Export Citation Figures and Tables Figures(17)  /  Tables(4)
    Other Articles By Authors On This Site On Google Scholar Related pages on Google
    Scholar on PubMed Tools Email to a friend About AIMS Press Open Access Policy
    Contact us Copyright © AIMS Press"'
  inline_citation: '>'
  journal: AIMS Mathematics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An enhanced tunicate swarm algorithm with deep-learning based rice seedling
    classification for sustainable computing based smart agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Fantin Irudaya Raj E.
  - Appadurai M.
  - Thiyaharajan D.
  - Lurthu Pushparaj T.
  citation_count: '0'
  description: Over the last decade of the second millennium, precision agriculture
    has gained a lot of attention in the agricultural industry. It began in the mid1980s
    as a way to improve fertilizer application by adjusting blends and rates throughout
    fields as needed, using newly available technologies. The concept has now been
    applied to a wide range of crops, activities, and countries. Its prevalence varies
    widely depending on the cropping system, region, and country, but it is increasingly
    being implemented or assessed worldwide. Various modern technologies, such as
    global positioning systems, drones, image processing techniques, artificial intelligence,
    machine learning, and 22deep learning are adopted into agriculture and make it
    more farmer-friendly and economically beneficial. Agricultural drones are one
    of the essential developments in the agriculture area for enhancing crop output.
    Crop monitoring and the requirement to spray pesticides and fertilizers at the
    right time and in the right place on the plants are crucial components for increasing
    crop yield. The current work explores the various modern state-of-the-art technologies
    employed in precision agriculture for crop health monitoring in detail.
  doi: 10.1201/9781003435228-3
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeEnvironment & AgricultureAgriculture & Environmental SciencesAgriculturePrecision
    Agriculture for SustainabilityState-Of-The-Art Technologies for Crop Health Monitoring
    in the Modern Precision Agriculture Chapter State-Of-The-Art Technologies for
    Crop Health Monitoring in the Modern Precision Agriculture ByE. Fantin Irudaya
    Raj, M. Appadurai, D. Thiyaharajan, T. Lurthu Pushparaj Book Precision Agriculture
    for Sustainability Edition 1st Edition First Published 2024 Imprint Apple Academic
    Press Pages 19 eBook ISBN 9781003435228 Share ABSTRACT Over the last decade of
    the second millennium, precision agriculture has gained a lot of attention in
    the agricultural industry. It began in the mid1980s as a way to improve fertilizer
    application by adjusting blends and rates throughout fields as needed, using newly
    available technologies. The concept has now been applied to a wide range of crops,
    activities, and countries. Its prevalence varies widely depending on the cropping
    system, region, and country, but it is increasingly being implemented or assessed
    worldwide. Various modern technologies, such as global positioning systems, drones,
    image processing techniques, artificial intelligence, machine learning, and 22deep
    learning are adopted into agriculture and make it more farmer-friendly and economically
    beneficial. Agricultural drones are one of the essential developments in the agriculture
    area for enhancing crop output. Crop monitoring and the requirement to spray pesticides
    and fertilizers at the right time and in the right place on the plants are crucial
    components for increasing crop yield. The current work explores the various modern
    state-of-the-art technologies employed in precision agriculture for crop health
    monitoring in detail. Previous Chapter Next Chapter Your institution has not purchased
    this content. Please get in touch with your librarian to recommend this.  To purchase
    a print version of this book for personal use or request an inspection copy  GO
    TO ROUTLEDGE.COM  Policies Privacy Policy Terms & Conditions Cookie Policy Journals
    Taylor & Francis Online Corporate Taylor & Francis Group Help & Contact Students/Researchers
    Librarians/Institutions Connect with us Registered in England & Wales No. 3099067
    5 Howick Place | London | SW1P 1WG © 2024 Informa UK Limited"'
  inline_citation: '>'
  journal: 'Precision Agriculture for Sustainability: Use of Smart Sensors, Actuators,
    and Decision Support Systems'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: STATE-OF-THE-ART TECHNOLOGIES FOR CROP HEALTH MONITORING IN THE MODERN PRECISION
    AGRICULTURE
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sarantakos T.
  - Gutierrez D.M.J.
  - Amaxilatis D.
  citation_count: '0'
  description: The use of computer vision, deep learning, and drones has revolutionized
    agriculture by enabling efficient crop monitoring and disease detection. Still,
    many challenges need to be overcome due to the vast diversity of plant species
    and their unique regional characteristics. Olive trees, which have been cultivated
    for thousands of years, present a particularly complex case for leaf-based disease
    diagnosis as disease symptoms can vary widely, both between different plant variations
    and even within individual leaves on the same plant. This complexity, coupled
    with the susceptibility of olive groves to various pathogens, including bacterial
    blight, olive knot, aculus olearius, and olive peacock spot, has hindered the
    development of effective disease detection algorithms. To address this challenge,
    we have devised a novel approach that combines deep learning techniques, leveraging
    convolutional neural networks, vision transformers, and cloud computing-based
    models. Aiming to detect and classify olive tree diseases the experimental results
    of our study have been highly promising, demonstrating the effectiveness of the
    combined transformer and cloud-based machine learning models, achieving an impressive
    accuracy of approximately 99.6% for multiclass classification cases including
    healthy, aculus olearius, and peacock spot infected leaves. These results highlight
    the potential of deep learning models in tackling the complexities of olive leaf
    disease detection and the need for further research in the field.
  doi: 10.1007/978-3-031-49361-4_2
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Algorithmic Aspects of Cloud
    Computing Conference paper Olive Leaf Infection Detection Using the Cloud-Edge
    Continuum Conference paper First Online: 14 December 2023 pp 25–37 Cite this conference
    paper Access provided by University of Nebraska-Lincoln Download book PDF Download
    book EPUB Algorithmic Aspects of Cloud Computing (ALGOCLOUD 2023) Themistoklis
    Sarantakos , Daniel Mauricio Jimenez Gutierrez & Dimitrios Amaxilatis   Part of
    the book series: Lecture Notes in Computer Science ((LNCS,volume 14053)) Included
    in the following conference series: International Symposium on Algorithmic Aspects
    of Cloud Computing 85 Accesses Abstract The use of computer vision, deep learning,
    and drones has revolutionized agriculture by enabling efficient crop monitoring
    and disease detection. Still, many challenges need to be overcome due to the vast
    diversity of plant species and their unique regional characteristics. Olive trees,
    which have been cultivated for thousands of years, present a particularly complex
    case for leaf-based disease diagnosis as disease symptoms can vary widely, both
    between different plant variations and even within individual leaves on the same
    plant. This complexity, coupled with the susceptibility of olive groves to various
    pathogens, including bacterial blight, olive knot, aculus olearius, and olive
    peacock spot, has hindered the development of effective disease detection algorithms.
    To address this challenge, we have devised a novel approach that combines deep
    learning techniques, leveraging convolutional neural networks, vision transformers,
    and cloud computing-based models. Aiming to detect and classify olive tree diseases
    the experimental results of our study have been highly promising, demonstrating
    the effectiveness of the combined transformer and cloud-based machine learning
    models, achieving an impressive accuracy of approximately 99.6% for multiclass
    classification cases including healthy, aculus olearius, and peacock spot infected
    leaves. These results highlight the potential of deep learning models in tackling
    the complexities of olive leaf disease detection and the need for further research
    in the field. Keywords Computer Vision Olive Leaf Infection Machine Learning Image
    Analysis Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF Similar content being viewed by others Lite-Agro: Exploring Light-Duty
    Computing Platforms for IoAT-Edge AI in Plant Disease Identification Chapter ©
    2024 Intelligent detection for sustainable agriculture: A review of IoT-based
    embedded systems, cloud platforms, DL, and ML for plant disease detection Article
    06 February 2024 Multi-class Plant Leaf Disease Classification on Real-Time Images
    Using YOLO V7 Chapter © 2023 1 Introduction The use of advanced technologies in
    agriculture has revolutionized traditional farming practices, enabling more efficient
    and precise methods for crop monitoring and disease detection. Among these transformative
    technologies, computer vision and drones have emerged as powerful tools for the
    detection of leaf infections in olive trees, offering new possibilities for early
    disease diagnosis and effective disease management. By leveraging the capabilities
    of computer vision algorithms and aerial surveillance provided by drones, farmers
    and researchers can now detect and respond to leaf infections in olive trees more
    accurately and promptly than ever before. Olive trees are an integral part of
    the agricultural landscape, providing valuable yields of olives and olive oil.
    However, they are susceptible to various diseases, including leaf infections caused
    by fungal pathogens, bacteria, and viruses. Identifying and addressing these infections
    in their early stages is crucial for preventing severe crop damage and yield losses.
    Traditional methods of disease detection often involve manual inspection of individual
    trees, which can be time-consuming, labor-intensive, and prone to human error.
    Fortunately, computer vision technologies and drones have emerged as promising
    solutions to overcome these limitations. Computer vision refers to the field of
    artificial intelligence that enables machines to analyze and interpret visual
    data, such as images or videos. By employing sophisticated algorithms, computer
    vision systems can extract meaningful information from visual inputs, enabling
    them to identify patterns, detect anomalies, and classify objects accurately.
    In the context of olive tree leaf infection detection, computer vision algorithms
    can be trained to recognize specific disease symptoms, such as discoloration,
    spots, lesions, or other visible signs of infection. By analyzing high-resolution
    images captured by drones, these algorithms can quickly scan vast olive tree groves
    and identify potentially infected trees with remarkable speed and precision. Drones,
    or unmanned aerial vehicles (UAVs), have rapidly gained popularity in agriculture
    due to their versatility and ability to provide an aerial perspective of crops
    and farmland. Equipped with high-resolution cameras and advanced sensors, drones
    can capture detailed images of olive trees from various angles and altitudes.
    These images offer a comprehensive view of the entire grove, allowing farmers
    and researchers to monitor the health and condition of individual trees remotely.
    Combined with computer vision algorithms, drones can autonomously survey large
    areas and generate real-time insights on the presence and severity of leaf infections.
    This enables farmers to make data-driven decisions and implement targeted interventions
    for disease control. The integration of computer vision technologies and drones
    for olive tree leaf infection detection offers several advantages over traditional
    methods. Firstly, it significantly reduces the time and effort required for disease
    surveillance, allowing farmers to cover large areas quickly and accurately. Secondly,
    by enabling early detection, this technology facilitates timely intervention,
    thereby minimizing the spread of infections and potential crop losses. Additionally,
    the use of drones eliminates the need for manual tree inspection, reducing the
    risk of human error and improving overall efficiency in disease management. The
    rest of the paper is structured as follows: Section 2 presents the literature
    state of the art on the fields of computer vision, machine learning, and edge
    computing technologies. Section 3 describes our methodology and the proposed architecture
    for identifying the condition of olive leaves. The evaluation of this methodology
    and its comparison with other systems is showcased in Sect. 4. Finally, in Sect.
    5 we present our conclusions and future directions. 2 Related Work Image processing
    and analysis have been utilized by many proposed solutions in past years in literature
    to efficiently detect plant and crop diseases. In [3] disorders in tomato plants
    are detected in leaf images using contour tracing, feature extraction, and Convolutional
    Neural Network (CNN) or K-Nearest Neighbor (KNN) machine learning approaches.
    [6] showcases a similar system that was developed in the Kingdom of Saudi Arabia
    that uses Deep Learning techniques like ResNet-50 and MobileNet models to classify
    olive leaf images collected through remote-controlled UAVs with high accuracy
    reaching up to 97%. X-FIDO [2] is a vision-based program to detect symptoms of
    Olive Quick Decline Syndrome. It showcases how transfer learning can be leveraged
    when collecting thousands of new leaf images is impossible, using only a few hundred
    images of healthy and infected leaves reaching over 98% accuracy. Olive tree disease
    analysis and classification is also to target of [8]. In more details, the infected
    area is isolated using the histogram thresholding and k-means segmentation methodologies
    with the latter being evaluated as more accurate. Finally, [10] solves a problem
    close to the one we face, by providing an automated methodology for detecting
    and counting olive trees using a multi-step classification system using colored
    satellite images. In [1] the authors explore the utilization of transformers for
    multiclass classification to predict olive leaf disease. They present their findings,
    highlighting the effectiveness of this approach in accurately identifying and
    categorizing various diseases affecting olive leaves. 2.1 Computer Vision Approaches
    Convolutional Neural Networks (CNNs) are a powerful class of deep learning models
    specifically designed for computer vision applications. CNNs excel at automatically
    learning and extracting meaningful features from visual data, such as images or
    videos. They employ a series of convolutional layers that apply learnable filters
    to capture local patterns and features. These filters scan the input data, enabling
    the network to detect edges, textures, shapes, and other visual attributes. By
    stacking multiple convolutional layers, CNNs can progressively learn more complex
    and abstract representations of the input. Additionally, pooling layers are used
    to downsample the learned features and retain important information while reducing
    spatial dimensions. The final layers of a CNN typically consist of fully connected
    layers that map the learned features to specific outputs, such as object classes
    or semantic labels. CNNs have demonstrated remarkable performance in various computer
    vision tasks, including image classification, object detection, image segmentation,
    and image generation. Their ability to automatically learn and extract relevant
    features from visual data has made them a fundamental and widely-used tool in
    the field of computer vision. Transfer learning is a machine learning technique
    where knowledge gained from training one model on a specific task is applied to
    a related task. A pre-trained model is used as a starting point and adapted to
    the new task by fine-tuning or retraining specific parts leveraging the pre-trained
    model’s learned features and representations, leading to improved performance,
    especially when data is limited. ResNet-50 is a highly regarded transfer learning
    model in the field of computer vision. Originally trained on the extensive ImageNet
    dataset, it has acquired intricate image representations. As a transfer learning
    technique, ResNet-50 ’s pre-trained layers can serve as feature extractors for
    novel tasks. This capability allows researchers to achieve remarkable performance,
    even when faced with limited annotated data. Due to its adaptability, ResNet-50
    finds extensive utility across diverse computer vision applications, including
    image classification, object detection, and image segmentation. The versatility
    and effectiveness of ResNet-50 make it a widely favored choice in the field of
    computer vision. VGG-19, like ResNet-50, is a renowned convolutional neural network
    architecture that stands out as a potent transfer learning model. Like its counterpart,
    VGG-19 was initially trained on the ImageNet dataset, which has played a crucial
    role in shaping its effectiveness across diverse computer vision tasks. By harnessing
    the pre-trained weights and learned features of VGG-19, researchers can tap into
    its vast knowledge and extend its applicability to new domains or tasks, particularly
    in scenarios where labeled data is scarce. The extensive capabilities of VGG-19
    make it a valuable asset for leveraging transfer learning in the realm of computer
    vision. You Only Look Once (YOLO) stands as a groundbreaking object detection
    framework, ushering in a new era in computer vision. It diverges from traditional
    methods by adopting a singular-stage approach, resulting in exceptional speed
    and efficiency. YOLO revolutionizes object detection by dividing the input image
    into a grid and directly predicting bounding boxes, object probabilities, and
    class labels from each grid cell. Through the ingenious employment of a solitary
    neural network, YOLO concurrently identifies multiple objects in a single pass,
    achieving remarkable real-time performance that proves invaluable in time-sensitive
    applications. With its precise and swift object detection capabilities, YOLO has
    become an indispensable tool across diverse domains, including autonomous driving,
    surveillance systems, and image analysis. Amazon Rekognition stands at the forefront
    of cutting-edge computer vision technology, offering a comprehensive and powerful
    solution for image and video analysis. Developed by Amazon Web Services (AWS),
    this state-of-the-art service provides an array of advanced features, including
    object recognition, facial analysis, emotion detection, text recognition, and
    more. With its deep learning algorithms and extensive training on vast datasets,
    Amazon Rekognition exhibits remarkable accuracy and robust performance, allowing
    users to extract valuable insights from visual content. Roboflow Roboflow is a
    powerful platform and suite of tools designed to streamline the process of building
    computer vision models and managing image datasets. It offers a comprehensive
    set of functionalities that simplify and accelerate various stages of the computer
    vision workflow. With Roboflow, users can annotate and label images, enabling
    the creation of high-quality datasets for training models. The platform also provides
    pre-processing capabilities, allowing users to resize, augment, and transform
    their image data to enhance model performance. Furthermore, Roboflow offers integration
    with popular machine learning frameworks and APIs, making it easy to train and
    deploy models in different environments. 2.2 Edge Computing Edge computing is
    a distributed computing approach that brings processing and storage closer to
    the devices and sensors at the edge of the network. It reduces latency by processing
    data locally and enables real-time decision-making. Edge computing offers benefits
    such as improved efficiency, enhanced privacy and security, offline capabilities,
    and scalability. It is applied in various industries and use cases to optimize
    data processing, reduce network bandwidth usage, and enable faster insights and
    better user experiences. Using edge computing in agriculture enables farmers to
    make timely decisions based on on-site data processing, reduces reliance on cloud
    connectivity, and ensures continuous farm operations even in remote areas. Edge
    computing empowers farmers with actionable insights, improves resource allocation,
    and promotes sustainable farming practices [5, 7, 9]. 2.3 Learning at the Cloud-Edge
    Continuum Federated Learning is a decentralized machine learning approach that
    prioritizes data privacy by keeping training data on local or edge devices instead
    of a central server or cloud. In this method, the model is distributed to edge
    devices for local training, with only model updates transmitted back and aggregated
    to improve the global model. The key objective is to protect data privacy and
    security by minimizing data sharing risks. Federated learning facilitates distributed
    model training while respecting privacy constraints and reducing the need for
    large-scale data transfers. Its benefits include personalized model training on
    individual devices, preserving data privacy, collaborative learning from diverse
    sources, and applicability in various scenarios such as mobile and IoT devices
    as well as edge computing environments. 2.4 Leaf Datasets Most of the systems
    presented above utilize a set of existing open datasets that contain curated images
    of plant leaves and fruits. These images contain mostly clean views of plant leaves,
    in neutral backgrounds, so their classification will be most accurate. One such
    dataset is the PlantVillage [4] dataset. It is a widely used and publicly available
    dataset for plant disease detection and classification research created by the
    PlantVillage project, that aims to assist in the development and evaluation of
    computer vision algorithms for plant disease diagnosis. It consists of 54303 healthy
    and unhealthy leaf images divided into 38 categories by species and diseases.
    The Olive Leaf Dataset 1 was created for olive tree disease detection and classification
    using convolutional neural networks (CNNs). It is intended to aid researchers
    and developers working on olive leaf disease analysis and machine learning algorithms
    for olive tree health monitoring and consists of high-resolution images, of three
    distinct classes: 2068 images of leaves infected by aculus orealus, 3717 images
    of leaves infected by peacock spot and 2155 healthy leaves. 3 Proposed Methodology
    Our proposed solution focuses on the analysis of the collected olive tree images
    collected using on-field hardware like drones or autonomous vehicles that can
    traverse the olive grove with limited or no human intervention. These images contain
    as expected a portion of the olive tree, with multiple leaves, photographed from
    a close or medium distance. The pipeline followed for the analysis of the images
    collected is depicted in Fig. 1. As depicted there, all collected images are bundled
    into a dataset, that are to be analyzed. This analysis consists of two parts:
    The segmentation of the leaves from each one of the tree images collected. The
    characterization of each detected leaf as healthy or unhealthy and the decision
    on the parasite it is infected with. These two processes can be either performed
    in two steps, using two discrete ML models, or as a single step using a single
    ML model trained to detect all 3 possible conditions. The first approach allows
    us to optimize the two ML models performing each operation, and increase their
    expected accuracy, resulting in better results, but on the other hand, requires
    more computational power for the analysis of each image. Using the second approach,
    we are able to perform both operations with a single prediction, that can both
    extract the olive leaves in each image and categorize them in one of the 3 available
    categories. Fig. 1. Leaf image collection pipeline across the Cloud-Edge continuum.
    Full size image After careful and extensive evaluation of all available software
    solutions, we were able to identify the best configuration for the two approaches
    we will evaluate in the rest of this paper. To implement the first approach and
    in scenarios where accuracy and flexibility are key, we determined that a combination
    of a self-trained YOLO ML model for image segmentation, complemented by a ResNet-50
    ML model using transfer learning techniques, is expected to provide the best outcome.
    This hybrid approach allows us to leverage the strengths of YOLO’s real-time object
    detection capabilities while benefiting from the high accuracy of ResNet-50. However,
    in situations where efficiency takes precedence over flexibility, we identified
    that leveraging the advanced capabilities of a single YOLO multi-class segmentation
    ML model would be a viable alternative. That is why we based on it the second
    approach described above. Moving parts of the leaf infection detection to the
    edge of the network offers a multitude of significant benefits. Firstly, it helps
    reduce the burden of data transfer to cloud services. By processing data locally
    at the edge of the network, the need to transmit the total volume of images or
    video footage to centralized cloud services is minimized, resulting in more efficient
    data management, lower network congestion and lower communication costs. Additionally,
    edge computing accelerates the delivery of results. With data processing occurring
    on-site or close to the data source, the latency associated with sending data
    to a distant data center is substantially reduced, leading to faster and more
    real-time outcomes. Finally, moving computing to the edge helps keep sensitive
    data under our own control, enhancing data security and privacy, and reducing
    the exposure of critical information to external networks. 3.1 Leaf Extraction
    The process of detecting the exact position of olive leaves inside a photo taken
    by an autonomous vehicle can be quite a complex task. Most applications that we
    investigated from the previous work (presented in Sect. 2) dealt with images of
    single leaves. These leaves were mostly placed in neutral backgrounds without
    any other information that could affect the process. Our goal was to be able to
    detect much more than single leaves and identify and extract each leaf from high-resolution
    images that contain portions or even whole olive trees. To achieve such a result
    we had to move of course from the single leaf images to branches of olive trees
    and finally high-resolution images as the ones available in Fig. 2. Fig. 2. Single
    leaf, branch of leaves and tree image used to evolve the leaf extraction ML model.
    Full size image In the pursuit of robust solution for object segmentation, our
    research led us to YOLOv8, an advanced framework renowned for its exceptional
    performance. In parallel, we delved into the available tools for the preparation
    of the datasets for YOLO ML model training, exploring two state-of-the-art applications:
    trainYOLO 2 and roboflow 3. With these applications, we could easily label and
    annotate the specific regions of interest in our experiments, the olive leaves.
    To ensure the model’s efficiency, we divided the dataset into training and validation
    subsets. Following an extensive training process, we evaluated our model against
    a diverse set of tree images that contained either single leaves, branches with
    multiple leaves in neutral backgrounds, or multiple leaves in images of trees
    in natural surroundings. Once the leaves were detected, we are able to use the
    YOLO bounding boxes detected to precisely crop and extract each individual leaf
    as a separate image, ready for their subsequent analysis (in the first approach)
    or their exact categorization (in the second approach). 3.2 Leaf Classification
    In the image classification part, our exploration led us to four distinct approaches.
    Firstly, we experimented with custom-built CNN trained from scratch, leveraging
    our domain expertise to develop a tailored architecture. The image classification
    model we employed is a sequential model with a series of convolutional and pooling
    layers followed by fully connected layers. The model begins with an initial convolutional
    layer with 8 filters and a 3\\(\\,\\times \\,\\)3 kernel, utilizing the ReLU activation
    function. Subsequently, a max pooling layer with a pool size of 2\\(\\,\\times
    \\,\\)2 and stride of 2 downsamples the feature maps. This is followed by two
    more convolutional layers, each with increasing filter size, and their corresponding
    max pooling layers. After the final pooling layer, the features are flattened
    into a 1D vector. Two fully connected layers are then applied, with the first
    layer consisting of 64 units and ReLU activation, and the final layer producing
    class probabilities using the softmax activation function. This model architecture
    enables extracting and learning hierarchical features from the input images, facilitating
    accurate image classification. The CNN model structure was carefully chosen through
    a comprehensive grid search process, which involved evaluating its performance
    on our specific dataset. Secondly, we evaluated models that harness the power
    of transfer learning, utilizing pre-trained VGG-19 and ResNet-50 models, originally
    trained on the ImageNet dataset, and further fine-tuned them with our own dataset.
    This approach allowed us to benefit from the models’ learned features and representations,
    enabling enhanced classification performance. These two models was selected based
    on thorough research and analysis of literature papers. We observed that VGG-19
    and ResNet-50 demonstrated superior performance on our dataset compared to other
    models or variations. Finally, we experimented with cloud computing tools, more
    specifically AWS Rekognition, to train a ML model using our own dataset. This
    cloud-based approach would provide us with easy-to-scale infrastructures for efficient
    training and inference operations. Throughout the implementation and evaluation
    of these diverse methodologies, we aimed to understand the best configurations
    for our system and find the optimal strategies for achieving accurate and reliable
    image classification results. 4 Evaluation 4.1 Metrics To evaluate the performance
    of classification ML models we use the most commonly used metrics of the field,
    accuracy, precision, recall, and f1 score. Each metric provides a different perspective
    on the model’s effectiveness. In more detail, accuracy (1) measures the proportion
    of correct predictions to the total number of predictions made; precision (2)
    calculates the ratio of true positive predictions to the total number of positive
    predictions, indicating how well the model correctly identifies positive instances;
    recall (3) calculates the ratio of true positive predictions to the total number
    of actual positive instances, indicating how well the model captures all positive
    instances; f1 score (4) is the harmonic mean of precision and recall providing
    a single metric that combines both precision and recall, giving a balanced measure
    of the model’s performance. TP: True Positive, TN: True Negative, FP: False Positive,
    FN: False Negative Fig. 3. Leaf detection on a branch and tree image using a YOLOv8
    image segmentation ML model. Full size image 4.2 Leaf Segmentation Figure 3 showcases
    the result of using the YOLO-based ML model we developed for the leaf segmentation
    operations of our system. This model is trained on a dataset consisting of more
    than 200 annotated images with more than 1000 objects belonging to one each of
    the three classes we use (healthy, aculus orealus, and peacock spot). In Fig.
    4 we showcase the evolution of all the available metrics during the training of
    our model for the whole 50 epochs of the training procedure. By minimizing the
    training losses (box loss, segmentation loss, classification loss, and localization
    loss) over time, the model improves its ability to accurately classify objects
    and predict their bounding box coordinates. This indicates that the model is learning
    to make more precise and accurate predictions. Simultaneously, the metrics such
    as precision, recall, mAP@0.5, and mAP@0.5:0.95 are all increasing. This suggests
    that the model’s performance in object detection is improving. Higher precision
    indicates a lower rate of false positive detections, while higher recall indicates
    a lower rate of false negative detections. Fig. 4. Metrics for the training of
    the YOLO-based segmentation model. Full size image Additionally, in Table 1 we
    present the results achieved by our YOLO-based segmentation ML model, for the
    two-step and approach. In the two-step approach, we can see the amount of leaves
    the system can detect out of the total number of leaves available in the dataset.
    This total number of leaves available is not the actual leaves presented in the
    image but the leaves that are usable, in focus, and in correct distance from the
    camera. Table 1. Statistics for the olive leaves detected in the YOLO-based approach.
    Full size table 4.3 Leaf Classification To identify the condition of each of the
    leaves detected by our system we needed to develop a ML model that is capable
    of classifying each cropped image to one of the 3 available categories. In our
    attempts to find the best possible ML model, we evaluated multiple configurations
    and design: a CNN-based ML model, a ResNet-50 ML model, a VGG-19 ML model and
    AWS Rekognition. Table 2. Accuracy scores achieved in the evaluation of multiple
    ML models for the classification of single olive leaves. Full size table The accuracy
    of each model is noted in Table 2. We can there identify that our custom CNN model
    has achieved the lower accuracy across all our attempts with \\(91.2\\%\\). The
    VGG-19 model showed a huge improvement, that reached \\(98.2\\%\\) as it is designed
    for similar tasks. ResNet-50 and AWS Rekognition were capable of reaching an accuracy
    score of more than \\(99\\%\\) using our combined dataset. 4.4 Comparison with
    Other Solutions In [6] a custom ML solution called MobilResNet was presented to
    facilitate the task of olive leaf classification. As this task is similar to our
    goal, based on transfer learning techniques in this section we will try to replicate
    these experiments and evaluate our own ML models in the same dataset to evaluate
    their performance. The same is with [1], where the authors use transformers for
    multi-class classification. Taking their exceptional achievements as the state
    of the art, characterized by high accuracy and remarkable precision, recall, and
    F1 score values, we aimed to surpass their performance. To achieve this, we leveraged
    the powerful Amazon Rekognition API, which proved to be crucial in our experiments
    and produced the best results. Table 3. Comparison of our cloud-based model with
    models described in [1, 6] Full size table The results of our experiments are
    presented in Table 3. Based on the outcome, it is evident that we achieved remarkable
    results, surpassing the initial objectives in our research on olive leaf classification.
    Our findings represent a significant advancement in this field, opening up new
    possibilities for further exploration and development. 5 Conclusions The combination
    of computer vision technologies and drones presents a promising approach to revolutionize
    the detection of leaf infections in olive trees. By harnessing the power of computer
    vision algorithms and aerial surveillance capabilities, farmers and researchers
    can gain valuable insights into the health of olive tree groves, detect infections
    at an early stage, and implement targeted interventions for effective disease
    control. As these technologies continue to advance, the future holds great potential
    for optimizing olive tree health and ensuring sustainable olive production. Our
    end-to-end system provides a robust solution for olive grove inspections, implemented
    with real-world photos, captured by UAVs within Mediterranean olive groves. The
    system is capable of detecting infections on images containing hundreds of leaves
    at a time, instead of individual leaves with remarkable accuracy even when compared
    to solutions existing in the literature. Notes 1. https://github.com/sinanuguz/CNN_olive_dataset.
    2. https://trainyolo.com/. 3. https://roboflow.com/. References Alshammari, H.,
    Karim, G., Ben Ltaifa, I., Krichen, M., Ben Ammar, L., Mahmood, M.: Olive disease
    classification based on vision transformer and cnn models. Computational Intelligence
    and Neuroscience 2022, 1–10 (07 2022). https://doi.org/10.1155/2022/3998193 Cruz,
    A.C., Luvisi, A., De Bellis, L., Ampatzidis, Y.: X-fido: an effective application
    for detecting olive quick decline syndrome with deep learning and data fusion.
    Front. Plant Sci. 8 (2017). https://doi.org/10.3389/fpls.2017.01741, https://www.frontiersin.org/articles/10.3389/fpls.2017.01741
    Harakannanavar, S.S., Rudagi, J.M., Puranikmath, V.I., Siddiqua, A., Pramodhini,
    R.: Plant leaf disease detection using computer vision and machine learning algorithms.
    Global Trans. Proc. 3(1), 305–310 (2022) Article   Google Scholar   Hughes, D.,
    Salathé, M., et al.: An open access repository of images on plant health to enable
    the development of mobile disease diagnostics. arXiv preprint arXiv:1511.08060
    (2015) Kalyani, Y., Collier, R.: A systematic survey on the role of cloud, fog,
    and edge computing combination in smart agriculture. Sensors 21(17) (2021). https://doi.org/10.3390/s21175922,
    https://www.mdpi.com/1424-8220/21/17/5922 Ksibi, A., Ayadi, M., Soufiene, B.O.,
    Jamjoom, M.M., Ullah, Z.: Mobires-net: a hybrid deep learning model for detecting
    and classifying olive leaf diseases. Applied Sciences 12(20) (2022). https://doi.org/10.3390/app122010278,
    https://www.mdpi.com/2076-3417/12/20/10278 O’Grady, M., Langton, D., O’Hare, G.:
    Edge computing: a tractable model for smart agriculture? Artif. Intell. Agricultu.
    3, 42–51 (2019) Google Scholar   Sinha, A., Shekhawat, R.S.: Olive spot disease
    detection and classification using analysis of leaf image textures. Proc. Comput.
    Sci. 167, 2328–2336 (2020). https://doi.org/10.1016/j.procs.2020.03.285https://www.sciencedirect.com/science/article/pii/S1877050920307511,
    international Conference on Computational Intelligence and Data Science Uddin,
    M.A., Ayaz, M., Mansour, A., Aggoune, e.H.M., Sharif, Z., Razzak, I.: Cloud-connected
    flying edge computing for smart agriculture. Peer-to-Peer Network. Appl. 14(6),
    3405–3415 (2021) Google Scholar   Waleed, M., Um, T.W., Khan, A., Khan, U.: Automatic
    detection system of olive trees using improved k-means algorithm. Remote Sensing
    12(5), 760 (2020) Article   Google Scholar   Download references Acknowledgements
    This work has been supported by the European Union’s Horizon 2020 research and
    innovation programme under Secure and Seamless Edge-to-Cloud Analytics (GA 957286),
    EU H2020, ICT-50-2020 - Software Technologies and Next Generation IoT as part
    of Next Generation Internet project (GA 957246), EU H2020, ICT-56-2020 - Next
    Generation Internet of Things. Author information Authors and Affiliations SparkWorks
    Ltd., Galway, Ireland Themistoklis Sarantakos & Dimitrios Amaxilatis Sapienza
    University of Rome, Rome, Italy Daniel Mauricio Jimenez Gutierrez Corresponding
    author Correspondence to Dimitrios Amaxilatis . Editor information Editors and
    Affiliations Sapienza University of Rome, Rome, Italy Ioannis Chatzigiannakis
    Ionian University, Corfu, Greece Ioannis Karydis Rights and permissions Reprints
    and permissions Copyright information © 2024 The Author(s), under exclusive license
    to Springer Nature Switzerland AG About this paper Cite this paper Sarantakos,
    T., Gutierrez, D.M.J., Amaxilatis, D. (2024). Olive Leaf Infection Detection Using
    the Cloud-Edge Continuum. In: Chatzigiannakis, I., Karydis, I. (eds) Algorithmic
    Aspects of Cloud Computing. ALGOCLOUD 2023. Lecture Notes in Computer Science,
    vol 14053. Springer, Cham. https://doi.org/10.1007/978-3-031-49361-4_2 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-49361-4_2 Published
    14 December 2023 Publisher Name Springer, Cham Print ISBN 978-3-031-49360-7 Online
    ISBN 978-3-031-49361-4 eBook Packages Computer Science Computer Science (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Related Work Proposed Methodology Evaluation Conclusions Notes References
    Acknowledgements Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Olive Leaf Infection Detection Using the Cloud-Edge Continuum
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chamara N.
  - Bai G.
  - Ge Y.
  citation_count: '1'
  description: 'Precision Agriculture (PA) promises to meet the future demands for
    food, feed, fiber, and fuel while keeping their production sustainable and environmentally
    friendly. PA relies heavily on sensing technologies to inform site-specific decision
    supports for planting, irrigation, fertilization, spraying, and harvesting. Traditional
    point-based sensors enjoy small data sizes but are limited in their capacity to
    measure plant and canopy parameters. On the other hand, imaging sensors can be
    powerful in measuring a wide range of these parameters, especially when coupled
    with Artificial Intelligence. The challenge, however, is the lack of computing,
    electric power, and connectivity infrastructure in agricultural fields, preventing
    the full utilization of imaging sensors. This paper reported AICropCAM, a field-deployable
    imaging framework that integrated edge image processing, Internet of Things (IoT),
    and LoRaWAN for low-power, long-range communication. The core component of AICropCAM
    is a stack of four Deep Convolutional Neural Networks (DCNN) models running sequentially:
    CropClassiNet for crop type classification, CanopySegNet for canopy cover quantification,
    PlantCountNet for plant and weed counting, and InsectNet for insect identification.
    These DCNN models were trained and tested with >43,000 field crop images collected
    offline. AICropCAM was embodied on a distributed wireless sensor network with
    its sensor node consisting of an RGB camera for image acquisition, a Raspberry
    Pi 4B single-board computer for edge image processing, and an Arduino MKR1310
    for LoRa communication and power management. Our testing showed that the time
    to run the DCNN models ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet,
    and power consumption ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet.
    The classification model CropClassiNet reported 94.5 % accuracy, and the segmentation
    model CanopySegNet reported 92.83 % accuracy. The two object detection models
    PlantCountNet and InsectNet reported mean average precision of 0.69 and 0.02 for
    the test images. Predictions from the DCNN models were transmitted to the ThingSpeak
    IoT platform for visualization and analytics. We concluded that AICropCAM successfully
    implemented image processing on the edge, drastically reduced the amount of data
    being transmitted, and could satisfy the real-time need for decision-making in
    PA. AICropCAM can be deployed on moving platforms such as center pivots or drones
    to increase its spatial coverage and resolution to support crop monitoring and
    field operations.'
  doi: 10.1016/j.compag.2023.108420
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusion and future perspectives Funding
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgements
    Data availability References Show full outline Cited by (1) Figures (12) Show
    6 more figures Tables (7) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show
    all tables Computers and Electronics in Agriculture Volume 215, December 2023,
    108420 AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge Author links open overlay
    panel Nipuna Chamara a, Geng Bai a, Yufeng Ge a b Show more Share Cite https://doi.org/10.1016/j.compag.2023.108420
    Get rights and content Under a Creative Commons license open access Highlights
    • We developed AICropCAM, an edge-computing enabled camera system for crop monitoring.
    • It integrated Raspberry Pi and Arduino for image processing and LoRa communication.
    • It ran a stack of four deep neural networks to characterize multiple plant/canopy
    parameters. • We quantified the power consumption and speed of the system for
    edge image-processing. • AICropCAM is a next-generation enabling technology for
    real-time, low-latency Ag applications. Abstract Precision Agriculture (PA) promises
    to meet the future demands for food, feed, fiber, and fuel while keeping their
    production sustainable and environmentally friendly. PA relies heavily on sensing
    technologies to inform site-specific decision supports for planting, irrigation,
    fertilization, spraying, and harvesting. Traditional point-based sensors enjoy
    small data sizes but are limited in their capacity to measure plant and canopy
    parameters. On the other hand, imaging sensors can be powerful in measuring a
    wide range of these parameters, especially when coupled with Artificial Intelligence.
    The challenge, however, is the lack of computing, electric power, and connectivity
    infrastructure in agricultural fields, preventing the full utilization of imaging
    sensors. This paper reported AICropCAM, a field-deployable imaging framework that
    integrated edge image processing, Internet of Things (IoT), and LoRaWAN for low-power,
    long-range communication. The core component of AICropCAM is a stack of four Deep
    Convolutional Neural Networks (DCNN) models running sequentially: CropClassiNet
    for crop type classification, CanopySegNet for canopy cover quantification, PlantCountNet
    for plant and weed counting, and InsectNet for insect identification. These DCNN
    models were trained and tested with >43,000 field crop images collected offline.
    AICropCAM was embodied on a distributed wireless sensor network with its sensor
    node consisting of an RGB camera for image acquisition, a Raspberry Pi 4B single-board
    computer for edge image processing, and an Arduino MKR1310 for LoRa communication
    and power management. Our testing showed that the time to run the DCNN models
    ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet, and power consumption
    ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet. The classification
    model CropClassiNet reported 94.5 % accuracy, and the segmentation model CanopySegNet
    reported 92.83 % accuracy. The two object detection models PlantCountNet and InsectNet
    reported mean average precision of 0.69 and 0.02 for the test images. Predictions
    from the DCNN models were transmitted to the ThingSpeak IoT platform for visualization
    and analytics. We concluded that AICropCAM successfully implemented image processing
    on the edge, drastically reduced the amount of data being transmitted, and could
    satisfy the real-time need for decision-making in PA. AICropCAM can be deployed
    on moving platforms such as center pivots or drones to increase its spatial coverage
    and resolution to support crop monitoring and field operations. Graphical abstract
    Download : Download high-res image (227KB) Download : Download full-size image
    Previous article in issue Next article in issue Keywords Artificial intelligenceComputer
    visionEdge computingInternet of thingsLoRaWANPrecision agriculture 1. Introduction
    The demands for food, feed, fiber, and fuel increase rapidly due to the fast expansion
    of the global population, income growth, technological advancement, and transport
    and logistics improvements (van Dijk et al., 2021). Precision agriculture (PA),
    which seeks to apply the right amount of inputs (fertilizers, irrigation water,
    pesticides, and other chemicals) in the right location at the right time, is essential
    to meet the requirements of future global food production, as well as environmental
    sustainability and climate resilience. PA is predicated on accurate sensor measurements,
    timely and sound decision-making, and automated actuators. The backbone of PA
    is the Internet of Things (IoT) technology that automates data collection, data
    analytics, data presentation, control, and efficient data communication (Chamara
    et al., 2022). Imaging sensors or digital cameras are essential for PA as they
    can capture more information than traditional scalar or vector sensors. Images
    can capture crop phenology for precise decision-making (Taylor and Browning, 2022,
    Tian et al., 2020). Cyclic events such as vegetative growth, flowering, leaf count
    and color change, maturation, and senescence are studied in crop phenology, which
    is essential to PA as it determines the management inputs required by crops. Moreover,
    images have rich information on the scene that allows for pest pressure evaluation.
    At present, a limited number of sensors are available for pest identification
    and pest pressure estimation. Among them, imaging sensors provide the most promising
    solution. Conventional (handcrafted feature extraction) and Artificial Intelligence
    (AI)-based image processing are the two branches of image processing. Traditional
    approaches extract image features defined by shape, texture, and color (Anubha
    et al., 2019, Yuan et al., 2019). The AI-based methods use Convolutional Neural
    Networks (CNN) to extract features from images (Luis et al., 2020). CNN models
    with multiple hidden layers for feature extraction and learning are considered
    Deep Convolutional Neural Networks (DCNN) (LeCun et al., 1998). Conventional imaging
    platforms in PA store images locally using onboard storage memories. Post processing
    refers to the processing of images stored at the central data storage in batches
    at a later time to extract useful information (Aasen et al., 2020). Imaging platforms
    that can access the internet through a stable connection with high bandwidth can
    automatically upload images to Cloud data storage. The vast majority of farmlands
    worldwide are in rural and remote areas with poor access to electric power and
    internet connectivity. This represents a big challenge for camera systems deployed
    in rural farmlands for high-speed image processing, data transmission, and low-latency
    decision-making (Richardson, 2019). Post-processing of crop images has been used
    for the estimation of leaf area index (Aasen et al., 2020), growth rate (Sakamoto
    et al., 2012), leaf chlorophyll and nitrogen content (Wang et al., 2014), fruit
    counts (Wang et al., 2014), and plant height (Sritarapipat et al., 2014). Further
    post-image processing allows for the assessment of biotic stress, such as pest
    density (Barbedo, 2014; Park et al., 2007) and weed pressure (Wang et al., 2019),
    as well as abiotic stress, such as nutrient deficiency (Ghorai et al., 2021).
    Richardson (2019) suggested that deep learning-based methods have the potential
    to facilitate the extraction of more sophisticated phenological data from both
    new and previously archived camera imagery compared to conventional image processing.
    Semantic segmentation-based canopy coverage (CC) estimation (Chamara et al., 2021;
    Liang et al., 2023), image classification-based crop identification (Anubha et
    al., 2019), disease identification (Sharma et al., 2020), growth stage prediction
    (Yasrab et al., 2021) and object detection-based plant feature identification
    (A. Wang et al., 2019) are examples of DCNN applications in agriculture. Conventional
    image processing requires less computational power and less energy, but they are
    limited in adaption to new scenarios, while deep learning requires high computational
    power and consumes more energy. DCNN models require large memory due to the large
    number of parameters these models hold. Therefore, it is not easy to implement
    these models practically in embedded systems that have less memory and computation
    power. These models also require a large amount of data to train to predict with
    high accuracy. Therefore, it is resource intensive. Edge image processing is the
    image processing done on image-capturing devices. The main advantage of edge image
    computing is that it lowers the high throughput data transmission requirement
    over a wireless IoT-enabled imaging network (Cao et al., 2020). Wang et al. (2022a)
    demonstrated the capability of identifying potted flowers with precision above
    89 % in real-time in a Jetson TX 2 computing module based on a DCNN algorithm.
    These authors suggested that a cloud-edge collaborative framework could achieve
    real-time and automatic learning for the DCNN model they have developed. Wang
    et al. (2022b) proposed a real-time weed detection model run on Jetson AGX Xavier
    for field robots. The authors proved it was possible to do real-time weed detection
    with a precision above 90 % yet required expensive hardware. Wang et al. (2022a)
    reviewed Raspberry Pi single-board computer-based real-time image processing applications.
    They concluded that Raspberry Pi (Datasheet Raspberry Pi Model B, 2019) is a cost-effective
    edge computing unit that could potentially be used as an edge image processing
    unit, and the capability of integrating it with IoT was also discussed. Zualkernan
    et al. (2022) demonstrated an edge image processing platform for the classification
    of animals and transmitting the identified animal and time of identification via
    LoRa for a camera trap. Past literature on IoT and image processing applications
    in agriculture has highlighted a research gap in edge image processing with IoT-enabled
    crop monitoring cameras. In-field crop cameras are expected to make real-time
    crop management decisions based on real-time image processing; however, poor internet
    connectivity in agricultural fields severely limits their capability. To address
    this gap, we have developed a novel imaging platform named AICropCAM that extracts
    plant and crop canopy level parameters through DCNN and uploads them to the Cloud
    via low-power, low-throughput communication protocols. We also demonstrated AICropCAM
    on an IoT-enable wireless sensor network in corn and soybean fields. A technology
    that addresses image processing at the lowest level (edge) and transmits only
    useful information can revolutionize real-time decision-making in PA. Therefore,
    the main objective of this paper is to demonstrate AICropCAM to perform edge image
    processing and low-throughput, low-power, and long-range data transmission through
    IoT technology. In this novel AICropCAM platform, multiple DCNN image processing
    algorithms run in series to extract plant-level and canopy-level features in an
    embedded system. Image classification, object detection with classification, and
    image segmentation are the three major applications of DCNN image processing,
    and all three are included in AICropCAM to demonstrate the capabilities of DCNN
    for image processing in PA. AICropCAM has trained models for canopy segmentation,
    crop classification, plant growth stage identification, plant counting, weed counting,
    and plant type identification. All the protocols that transmit data from AICropCAM
    to the Cloud were custom designed. AICropCAM sends the generated data to a cloud
    platform for logging, visualization, and analysis. Furthermore, this paper explains
    the DCNN model training process, model performance, and test results. We reported
    the model training comprehensively because it was essential for AICropCAM development.
    2. Materials and methods Essential activities in this research were data/image
    collection and preprocessing, hardware design for AICropCAM, software design for
    data transmission between the edge and the Cloud, deep learning model design,
    and model training and optimization (Fig. 1). AICropCAM was implemented in a corn
    and soybean field at the field phenotyping facility in Mead, Nebraska, USA (Bai
    et al., 2019). We demonstrated the training of the following DCNNs: CropClassiNet
    for classifying images based on image quality and crop type, CanopySegNet for
    segmenting crop canopy from the background, PlantCountNet for classifying and
    counting soybean and weed plants, and InsectNet for identifying insects and counting
    them. Download : Download high-res image (412KB) Download : Download full-size
    image Fig. 1. Steps of edge image processing program deployment on the embedded
    system (edge devices). 2.1. Image collection, annotation, preprocessing, and augmentation
    Image collection for DCNN model training occurred in four growing seasons using
    three different types of cameras: (i) commercially available Meidas SL122 trail
    cameras in 2019 (Meidas Trail Cameras, 2022), (ii) OV5642 imaging sensors with
    ArduCAM camera shields in 2020, and (iii) Raspberry Pi Camera Module V2 with Raspberry
    Pi Zero in 2021 and 2022 (Chamara, 2021). All the cameras were mounted on the
    bars horizontally extended and fixed on stationary poles erected vertically in
    the fields, as shown in Fig. 2A. The distance between the crop canopy and the
    cameras was maintained between 0.5 and 1.5 m throughout the growing seasons. Images
    used for training the InsectNet were also captured with smartphones as we could
    not collect enough images with insects from the three types of cameras mentioned
    above. Download : Download high-res image (338KB) Download : Download full-size
    image Fig. 2. Left: An Illustration of how AICropCAM was set up in the field for
    image collection. In addition to the camera, other components such as the solar
    panel and data logger were also shown. Right: A close-up view of AICropCAM and
    its hardware components. All three standard image annotation techniques in deep
    learning model training were utilized: (1) folder labeling for the image classification
    models, (2) pixel-level annotation for the semantic segmentation model, and (3)
    bounding boxes for object detection models. Images belonging to the same class
    were grouped into a single folder, and five distinct classes (or folders) were
    created: rejected, corn, soybean, grass, and night. Separating the crop canopy
    from the soil was done with pixel-level annotation and semantic segmentation.
    Bounding boxes, the smallest rectangle around an object, were drawn for corn plants,
    soybean plants, weed plants, and insects. Table 1 explains each type of annotation
    used in the model training. Table 1. Annotation criteria used to generate labels
    from the images to train and test the four deep convolutional neural network models
    in AICropCAM. Labeling Type Class Description Image classification (CropClassiNet)
    Rejected Images were labeled as rejected due to multiple reasons: blurred images
    caused by water droplets on the lens; the cameras turned away from the targeted
    crop; crops growing up to the camera blocking the view or capturing only a few
    leaves; people present in the images; lens covered with different stuff; and the
    camera was not installed in the field. Corn Images entirely covered by corn plants
    at different growth stages. Soybean Images entirely covered by soybean plants
    at different growth stages. Grass/Weed Images only comprise grass/weed plants
    at different growth stages. Night Images captured under low lighting conditions.
    Most of the cameras were not programmed to stop collecting images under low light.
    Crop canopy and background (CanopySegNet) Canopy Pixel labeling was done on the
    crop canopy. We used assisted freehand tool and the superpixel option in the MATLAB
    image labeler. Background Pixel labeling was done on the crop canopy. We used
    assisted freehand tool and the superpixel option in the MATLAB image labeler.
    Plant-type (PlantCountNet) Weed Weed present in the image was labeled using bounding
    boxes. It was challenging to locate the weed after the corn or soybean canopy
    was closed. Soybean Soybean plants present in the image were labeled using bounding
    boxes. Insects (InsectNet) Insects During the labeling process, without distinguishing
    insects based on their type, all the insects present in the images were labeled
    using bounding boxes. Image preprocessing is necessary for DCNN model training
    and real-time edge image processing. Differences in the input layer size in different
    DCNN models demand that images be resized before passing through the model for
    training or prediction purposes. High-resolution images improve accuracy but require
    more computational power. For specific applications, labeled datasets were only
    limitedly available. Therefore, image augmentation techniques were used to increase
    the number of image data sets, including scaling, flipping, cropping, rotation,
    color transformation, PCA color augmentation, and noise rejection (Paymode and
    Malode, 2022). Multiple augmentation techniques were used for each model, as detailed
    in Table 2. Additionally, Table 2 provides the numbers of images in training,
    validation, and testing for the four DCNN models. Table 2. DCNN model image allocation
    and image augmentation. Model Number of images Data Augmentation Techniques Total
    Training Validation Test CropClassiNet 43,611 30,528 9,810 3,273 Random rotation,
    random X  and Y reflection CanopySegNet 51 31 10 10 Transformation (random left/right
    reflection and random X/Y translation of ±10 pixels) PlantCountNet 110 88 11 11
    Transformation (same as CanopySegNet) InsectNet 542 326 108 108 Transformation
    (same as CanopySegNet) Our main objective was not to make the most accurate prediction
    for the DCNN models but to demonstrate the concept of implementing edge image
    processing and transmitting the results to the Cloud for decision-making. Therefore,
    we selected a limited number of images for CanopySegNet, PlantCountNet, and InsectNet,
    which were sufficient to train models with a reasonable degree of accuracy. 2.2.
    DCNN model architecture selection, training, evaluation, and deployment on the
    edge device The steps to select model architecture/model backbone weights and
    image input sizes to train the best model for CropClassiNet, CanopySegNet, PlantCountNet,
    and InsectNet are summarized in Fig. 3. Unlike many DCNN applications that prioritize
    higher accuracy, our application focused on finding the balance between accuracy
    and model deployability on the edge device. Download : Download high-res image
    (771KB) Download : Download full-size image Fig. 3. DCNN model selection process
    during the training and testing by attempting different model architectures, model
    backdone weights, and input image sizes. For example, in the development of CropSegNet
    (Segmentation), we selected DeepLabv3+ (Firdaus-Nawi et al., 2018) with weights
    initialized from pre-trained networks of ResNet18 (He et al., 2016), ResNet50,
    Xception, InceptionresnetV2, and MobileNetV2. The input image sizes tested were
    512 × 512 × 3 and 256 × 256 × 3, and training options were kept constant to find
    the best-performing networks, which should also be deployable to Raspberry Pi
    4B. This process identified DeepLabv3 + with ResNet50 as the most suitable model
    for CropSegNet, with an input image size of 512 × 512 × 3. Table 3 summarizes
    the hyperparameter values and training options for the final DCNN models deployed
    to the edge device. (1) (2) (3) (4) (5) (6) (7) Table 3. Hyperparameter values
    and training options for the best models (SGDM - stochastic gradient descent with
    momentum, RMSProp - Root mean square propagation). Training option and the function/Hyperparameters
    Values for CropClassiNet Values for CanopySegNet Values for InsectNet (320 × 320
    × 3) Values for PlantCountNet (320 × 320 × 3) Optimizer SGDM SGDM SGDM RMSProp
    Momentum 0.9 0.9 0.99 NA Initial learning rate 0.001 0.001 0.001 0.001 Learn rate
    schedule Piecewise Piecewise Piecewise Piecewise Learn rate drop period 10 10
    10 10 Learn rate drop factor 0.3 0.3 0.1 0.3 Minibatch size 16 4 16 32 L2Regularization
    NA 0.005 0.005 0.005 Validation frequency 3 3 3 10 Shuffle Every epoch Every epoch
    Every epoch Every epoch Validation patience 4 10 10 10 Max epochs 100 300 1000
    100 Execution environment Multi GPU Multi GPU GPU GPU The performance of the four
    DCNN models was evaluated using the indices calculated from Eq. (1), (2), (3),
    (4), (5), (6), (7). Accuracy, Precision, Recall, F1 score, and Jaccard index were
    used for the classification models CropClassiNet and CropSegNet, whereas IoU and
    mAP (Mean Average Precision) were used for PlantCountNet and InsectNet. Jaccard
    index gives the proportion of correctly predicted labels to the total number of
    labels. Model training was performed on an NVIDIA GeForce GTX 1650 Ti Mobile processor,
    a dedicated mid-range graphics card with 4 GB GDDR6 memory on a Dell XPS 15 9500
    Laptop. The laptop had an Intel Core i7-10750H 10th Gen processor,16 GB DDR4 RAM,
    and 1 TB SSD hard disk. 2.3. Hardware and software of AICropCAM The IoT data transmission
    and edge image processing hardware comprised the following major components: a
    Raspberry Pi 4B single-board computer, an Arduino MKR1310 development board, an
    Arduino MKR Relay Proto Shield, and a Dragino OLG02 outdoor dual channels LoRa
    Gateway (Fig. 4). The 12 V 8Ah battery powered the Raspberry Pi 4B, controlled
    through the relay shield managed by the Arduino MKR1310. A 3.7 V lithium polymer
    battery powered the Arduino MKR1310 board. There are two advantages of having
    a separate Arduino board. First, the Arduino board consumes less power than the
    Raspberry Pi 4B module. It can be switched on and off according to user requirements.
    Second, it allows uninterrupted communication between the edge node and the Cloud
    with low power. Download : Download high-res image (303KB) Download : Download
    full-size image Fig. 4. Hardware overview of AICropCAM and data flow. AICropCAM
    required programming on two hardware platforms. Arduino was programmed using C++
    in Arduino’s Integrated Development Environment. Raspberry Pi imaging and image
    processing program was developed in MATLAB and deployed onto the Raspberry Pi
    4B using the MATLAB Coder and MATLAB Compiler. A python program was designed to
    read the saved data in the Raspberry Pi 4B and serially communicate to the Arduino
    MKR1310. The primary functions of the MRK1310 program were to (1) turn on the
    Raspberry Pi 4B module based on the user-defined time intervals, (2) get the processed
    data, including the results of DCNN model predictions, through serial communication
    from the Raspberry Pi 4B, and (3) transmit the data to the ThingSpeak Cloud channel
    through the LoRa gateway. All the DCNN models were trained using the MATLAB deep
    learning toolbox. In the edge deployment, a MATLAB program runs multiple models
    logically depending on the prediction result of the previous model estimation,
    as shown in Fig. 5. MATLAB coder generated the C and C++ code derived from the
    program we developed to run on the Raspberry Pi. MATLAB Compiler generated the
    standalone application on the Raspberry Pi (The MathWorks, 2022). Download : Download
    high-res image (477KB) Download : Download full-size image Fig. 5. Overall sequential
    image processing and data generation flow chart. Table 4 lists the parameters
    generated by the models in AICropCAM. The abbreviations in Table 4 are fields
    holding data in the program to reduce the complexity of system development and
    maintain a common standard among different platforms. Fig. 6 shows the data generation
    from images. According to Fig. 6, the size of the images were around 2 MB before
    being fed into the image processing pipeline. The output message contains the
    crop type (CT), plant count (PC), weed count (WC), canopy coverage (CC), and pest
    count (PstC). The resulting message is typically less than 100 bytes. This represents
    a substantial reduction of memory size with the output being 0.00005 times the
    size of the original image. Consequently, this message can be transmitted in a
    single message via LoRa as the maximum LoRa packet size is around 256 bytes. Table
    4. List of parameters used to represent information in the images. Parameter Abbreviation
    Represent information Image location LOC Node ID manually entered/Global positioning
    system location coordinates Image orientation IO Accelerometer/Manually feed/Gravity
    switch Image quality/Crop type CT Image classification based on image quality
    and the crop type Plant count/Weed count PC/WC Multiclass object detection/classification
    Crop canopy coverage CC Semantic segmentation Pest count PstC Multiclass object
    detection/classification Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 6. Examples of message generation and data size reduction
    for LoRa transmission. 2.4. Data transmission, visualization, and storage The
    data generated after image processing were saved on the Raspberry Pi 4B SD card,
    allowing access to the data remotely or through manual retrieval during field
    visits. Two options for transmitting the collected data to the ThingSpeak IoT
    platform are available. Firstly, the data can be uploaded directly from the Raspberry
    Pi 4B if internet connectivity is available for growers with Wi-Fi access. Secondly,
    the Raspberry Pi 4B transmits the recently acquired data to the Arduino MKR1310.
    The Arduino MKR1310 decodes the data received from the Raspberry Pi 4B and forwards
    it to the ThingSpeak. The second method is for low-rate, long-range communication
    beyond the limit of Wi-Fi. A single message receivable to the ThingSpeak server
    includes data for eight fields. In our demonstration, a single message was enough
    to transmit the data generated. Fields 1 and 2 are reserved for geographic coordinates
    (namely, latitude and longitude) to represent the device''s location. The third
    field was for camera orientation. Image quality/crop type, plant count, weed count,
    insect count, and crop canopy coverage were allocated from fields four to eight.
    ThingSpeak supports eight channels per gateway. If additional data is generated
    in the future, we have to create new channels to accommodate them. However, only
    data in a single channel can be passed through a single message. The Arduino-LoRa
    library was used to prepare the LoRa messages forwarded to the gateway (Mistry,
    2016). The message generated from the Arduino MKR1310 includes the device identification
    number and the data with the field number. Once the gateway receives this message,
    it adds the target client ID (generated by ThingSpeak when defining a device),
    host address (mqtt://mqtt3.thingspeak.com), server port number, username and password,
    channel ID, and the data in each field according to the Message Queuing Telemetry
    Transport (MQTT) protocol. Username and password ensure that only authorized devices
    can transmit data to the ThingSpeak platform. ThingSpeak provides two ways to
    interact with its platform, REST (Representative State Transfer) and MQTT protocols.
    The advantages of using MQTT over REST protocol are that it supports ThingSpeak
    data publishing, including immediate and minimum power consumption and data transmission
    over limited bandwidth, which encouraged us to select the MQTT protocol in our
    demonstration. 3. Results and discussion 3.1. DCNN model performance CropClassiNet
    had a test accuracy of 91.26 %, a Jaccard Index of 0.77, and an F1-score of 0.91;
    the confusion matrix is given in Fig. 7. The highest precision is for the “grass”
    class (100 %), and the lowest is for “soybean” (92.0 %). The highest recall is
    for the “corn” class (99.9 %), and the lowest is for “grass” (67.1 %). The primary
    goal of CropClassiNet is to determine the quality of new images and direct them
    for subsequent processing (Fig. 5). This step has never been executed in an image-based
    crop monitoring platform before. Further, CropClassiNet can eliminate erroneous
    images when humans are present in the camera’s field of view or when the camera
    is misaligned due to external factors. AICropCAM can send maintenance requests
    through IoT analytics if rejected images are continuously generated. Download
    : Download high-res image (275KB) Download : Download full-size image Fig. 7.
    Confusion matrix for test images by CropClassiNet. CanopySegNet on the test images
    achieved a global accuracy of 0.93, a weighted IoU of 0.87, and a mean BF score
    of 0.73. Fig. 8 shows an example of an original soybean image and the corresponding
    segmentation result by CanopySegNet, which estimated CC to be 18.72 %. Season-long,
    time-series images can be fed into CanopySegNet to generate diurnal and seasonal
    curves of crop CC, as shown in Fig. 9. Download : Download high-res image (621KB)
    Download : Download full-size image Fig. 8. An image of soybean crop and the segmentation
    result by CropSegNet to calculate canopy coverage. Download : Download high-res
    image (367KB) Download : Download full-size image Fig. 9. Examples of diurnal
    and seasonal variations of canopy coverage as computed by CropSegNet. According
    to Fig. 9, canopy coverage percentage variation is low during the daytime and
    reaches zero at night. This verifies the need to eliminate low-light images before
    segmenting. As shown in Fig. 5, it is possible to eliminate the generation of
    false values when the camera captures images under low light conditions by halting
    the process of running CanopySegNet. There are three diurnal variation series
    on 6/8/2021, 6/26/2021, and 7/12/2021 in Fig. 9. The CC increased from 8 % to
    95 % between 6/8/2021 to 7/12/2021. The seasonal trend showed that the CC reached
    a maximum around 7/8/2021. These results suggest that the proposed stacked models
    can track the daily and seasonal CC variation and eliminate the effect of lighting
    conditions on false value generation. Table 5. Performance of PlantCountNet and
    InsectNet on the test image set (Root mean square error (RMSE)/Final validation
    loss (FVL)). Model Name Architecture Input size Validation RMSE/FVL Mean average
    precision Object class PlantCountNet YOLOv2 320 × 320 × 3 0.888 (RMSE) 0.66 Soybean
    0.86 Weed InsectNet YOLOv4 320 × 320 × 3 26.2 (FVL) 0.02 Insect The overall performance
    of the PlantCountNet and InsectNet is given in Table 5. Fig. 10(A) and 10(B) show
    the result obtained by PlantCountNet for a soybean image at an early vegetative
    stage (V3). Meanwhile Fig. 10(C) and 10(D) shows the result at a reproductive
    stage (R1). It can be seen that, at V3 stage, the model outputs matched the labels
    of soybean and weed plants well, indicating a level of high accuracy. Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 10. The
    result of PlantCountNet for soybean and weed counting: Manually annotated vs.
    model-predicted bounding boxes at V3 growth stage (A and B); manually annotated
    vs. model-predicted bounding boxes at R1 growth stage (C and D). The size of insects
    is very small compared to the size of images (Fig. 11), which is the main reason
    for the low mAP for InsectNet (Table 5). Increasing input image resolution beyond
    480 × 480 × 3 is impractical as it exceeds the memory limitation to load models
    into Raspberry Pi 4B. A potential solution could be to increase the resolution
    of the region of interest by splitting the original image while keeping the resolution
    the same. Also, we suggest using the approach recommended by Tetila et al., 2020a,
    Tetila et al., 2020b in the future on Raspberry Pi model 4B. As technology advances,
    we expect the memory capacities will increase for edge computing units. At the
    same time, the state-of-the-art object detection algorithms will improve the accuracy
    for small object detection. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 11. The result of InsectNet for insect counting
    in soybean. The top row shows a situation of high false positives and low false
    negatives: (A) and (B) are manually annotated and model-predicted insect labels,
    respectively. The bottom row shows a situation of low false positive and high
    false negative: (C) and (D) are manually annotated and model-predicted insect
    labels. 3.2. Power consumption for Raspberry Pi 4B Since edge cameras in farmlands
    have limited access to electric power, information on their power consumption
    is essential for designing IoT devices and systems. AICropCAM is designed to be
    energized by solar power. It runs on a rechargeable battery when there is no solar
    power. We monitored the maximum energy consumption of each task performed by AICropCAM,
    and the result is presented in Table 6. Four main strategies are available for
    the power management of IoT edge devices: Selecting power-efficient hardware,
    maintaining low power modes, dynamic power management, and cloud-based management.
    Raspberry Pi 4B is an affordable power-efficient single-board computer suitable
    for our application, but it does not naturally support low-power modes. Therefore,
    we introduced the Arduino MKR1310 LoRa module for the Raspberry Pi 4B dynamic
    power management. Furthermore, this Arduino module allows us to perform cloud-based
    central management independently. Table 6. Electrical power consumption of the
    Raspberry Pi 4B and the Arduino MKR1310 during edge image processing. Device Activity
    The maximum current range and the voltage recorded Raspberry Pi 4B Idle run 5.25
    V × (0.45 – 0.53) A Image classification 5.25 V × (0.97 – 1.04) A Image segmentation
    5.25 V × (0.98 – 1.11) A Weed and plant detection 5.25 V × (0.62 – 0.70) A Insect
    detection 5.25 V × (0.62 – 0.70) A Arduino MKR1310 Sleep mode <0.01A Serial communication
    <0.01A LoRa transmission <0.01A For our measurements, we used a Raspberry Pi 4B
    with 8 GB of RAM, connected to an HDMI monitor, a USB keyboard, and a USB mouse,
    and ran a MathWorks® Raspbian image (file used to boot the Raspberry Pi 4B). The
    Raspberry Pi 4B was operated at room temperature and connected to a wireless LAN
    access point and a laptop via an Ethernet cable. The electric current consumption
    for running each DCNN model was recorded during the test. CropClassiNet had the
    highest current consumption, while the PlantCountNet and InsectNet models had
    the lowest. As for LoRa transmission, we could not measure its current consumption
    because the lowest value our instrument could measure was 0.01A. Based on the
    manufacturer''s specifications, the Arduino MKR1310 consumes 104 uA at 5 V. The
    average time to run the DCNN models is essential to estimate the energy consumed
    for each prediction. These parameters listed in Table 7 provide essential guidelines
    for designing IoT sensor nodes with suitable batteries and power sources. We also
    noticed that typically the first prediction of a model took the longest time,
    but the rest take a considerably shorter time to predict. Table 7. Time duration
    needed for the selected DCNN models deployed in the Raspberry Pi 4B. Model/Task
    Input image size Time for predicting results (s) The maximum power demand for
    the activity (W) CropClassiNet/Image quality evaluation and crop classification
    224 × 224 × 3 6.44 5.46 CanopySegNet/Semantic segmentation to separate canopy
    from background 512 × 512 × 3 20.20 5.83 PlantCountNet/Weed and plant detection,
    classification, and counting 320 × 320 × 3 14.38 3.68 InsectNet/Insect detection
    320 × 320 × 3 0.20 3.68 Semantic segmentation was the most power-demanding activity,
    while insect detection was the least. Changing the order of the image processing
    models and adding new models or dropping existing models is possible during regular
    operation. It enables dynamic power management within the Raspberry Pi module.
    The main advantage of AICropCAM is that it implements a stack of four DCNN-based
    image processing models with multiple objectives. To the best of our knowledge,
    this is the first time such a system has been developed for a field crop monitoring
    camera. AICropCAM has applications such as setting up smart in-field or greenhouse
    IoT camera networks with edge computing capability, monitoring crops by attaching
    them to sprinkler irrigation systems (pivots and linear moves), or collecting
    crop information through ground or aerial mobile robots. The relatively short
    time to run each DCNN model makes the system suitable for real-time applications,
    including variable rate irrigation, fertilization, and spraying. For example,
    a pivot irrigated multi-cropping system with AICropCAM can automate irrigation
    or fertigation transition between different crops or crops at different growth
    stages by automatically providing the crop type or growth stage information to
    the irrigation controller. Additionally, existing herbicide or pesticide sprayers
    can get the feedback of the PlantCountNet and InsectNet in the AICropCAM for precision
    spraying. 4. Conclusion and future perspectives This paper outlines the essential
    components of constructing a functional edge image processing framework for real-time
    crop monitoring. From a software standpoint, CropClassiNet can categorize captured
    images according to image quality and detect the presence of specific crop types
    for further processing. CanopySegNet can further quantify the degree of canopy
    coverage; PlantCountNet can count the number of plants and weeds in the image;
    and finally, InsectNet can count the number of insects in the image. These four
    DCNN models, when implemented on edge devices, can extract an array of important
    crop and canopy parameters from field images and enable real-time, low-latency
    decision making and applications. Deep learning-based image processing on the
    edge has excellent potential in PA. Applications of AICropCAM are not limited
    to image classification, segmentation, plant counting, or weed counting. Potential
    future applications include insect classification and crop damage estimation,
    weed classification and pressure estimation, fruit identification and yield estimation,
    decision on replanting (Whigham et al., 2000), and disease identification and
    disease damage estimation in real time using actual field images collected by
    AICropCAM. AICropCAM shows excellent potential in enhancing crop management through
    crop monitoring. However, the current demonstration requires significant improvements
    on both hardware and software fronts. Customized circuitry and modular design
    are required to put AICropCAM in commercial farm applications. The full potential
    of the AICropCAM can be achieved by putting this camera on a moving platform like
    a center pivot with a GPS receiver to generate spatiotemporal data. Crop classification
    must include more crop types, and segmentation models need training data from
    other crop types. The DCNN models for weed and insect identification require the
    capability to identify different weed types, their growth stage, different insect
    types, and their growth stages to generate effective pest control decisions. Additionally,
    improving the models’ accuracy in image classification, segmentation, and object
    detection is crucial. It can be achieved by increasing the number of training
    image data sets. We also planned to expand the research for multiple edge architecture
    evaluation. Architectures such as a high-performance edge computer that accepts
    images from multiple edge devices through short-range, high-speed communication
    (e.g., Wi-Fi) and can run more accurate deep learning models with higher numbers
    of parameters, might be a better solution for the primary objectives addressed
    in this paper. We aim to expand the AICropCAM applications to other crops beyond
    corn and soybean. By making these improvements, AICropCAM will become a more effective
    tool for crop management, potentially revolutionizing how we grow and manage crops.
    Funding This work was supported by the United States Department of Agriculture
    – National Institute of Food and Agriculture grants [Award 2020-68013-32371 to
    YG and GB, Award 2021-67021-34417 to YG]. CRediT authorship contribution statement
    Nipuna Chamara: Methodology, Software, Visualization. Geng Bai: Conceptualization,
    Methodology, Resources. Yufeng Ge: Conceptualization, Resources, Supervision,
    Project administration, Funding acquisition. Declaration of Competing Interest
    The authors declare the following financial interests/personal relationships which
    may be considered as potential competing interests: Nipuna Chamara, Yufeng Ge,
    Geng Bai has patent pending to University of Nebraska-Lincoln. Acknowledgements
    Jianxin Sun assisted in developing the imaging device with Raspberry Pi Zero used
    for image acquisition. David Scoby helped the field management and AICropCAM installation.
    Junxiao Zhang supported the field installation of AICropCAM and smart-phone based
    acquisition of crop images with insects. Data availability Data will be made available
    on request. References Aasen et al., 2020 H. Aasen, N. Kirchgessner, A. Walter,
    F. Liebisch PhenoCams for field phenotyping: using very high temporal resolution
    digital repeated photography to investigate interactions of growth, phenology,
    and harvest traits Front. Plant Sci., 11 (June) (2020), pp. 1-16, 10.3389/fpls.2020.00593
    Google Scholar Anubha et al., 2019 P.S. Anubha, V. Sathiesh Kumar, S. Harini A
    study on plant recognition using conventional image processing and deep learning
    approaches J. Intell. Fuzzy Syst., 36 (3) (2019), pp. 1997-2004, 10.3233/JIFS-169911
    Google Scholar ArduCAM, 2016 ArduCAM ESP8266 UNO board User Guide (pp. 0–9). (2016).
    www.ArduCAM.com. Google Scholar Bai et al., 2019 G. Bai, Y. Ge, D. Scoby, B. Leavitt,
    V. Stoerger, N. Kirchgessner, S. Irmak, G. Graef, J. Schnable, T. Awada NU-Spidercam:
    A large-scale, cable-driven, integrated sensing and robotic system for advanced
    phenotyping, remote sensing, and agronomic research Comput. Electron. Agric.,
    160 (March) (2019), pp. 71-81, 10.1016/j.compag.2019.03.009 View PDFView articleView
    in ScopusGoogle Scholar Barbedo, 2014 J.G.A. Barbedo Using digital image processing
    for counting whiteflies on soybean leaves J. Asia Pac. Entomol., 17 (4) (2014),
    pp. 685-694, 10.1016/j.aspen.2014.06.014 View PDFView articleView in ScopusGoogle
    Scholar Cao et al., 2020 K. Cao, Y. Liu, G. Meng, Q. Sun An Overview on Edge Computing
    Research IEEE Access, 8 (2020), pp. 85714-85728, 10.1109/ACCESS.2020.2991734 View
    in ScopusGoogle Scholar Chamara et al., 2021 N. Chamara, K. Alkhadi, H. Jin, F.
    Bai, A. Samal, Y. Ge A deep convolutional neural network based image processing
    framework for monitoring the growth of soybean crops. 2021 ASABE Annual International
    Meeting, 2100259 (2021), 10.13031/aim.202100259 Google Scholar Chamara et al.,
    2022 N. Chamara, M.D. Islam, G.F. Bai, Y. Shi, Y. Ge Ag-IoT for crop and environment
    monitoring: Past, present, and future Agr. Syst., 203, 103497 (2022), 10.1016/j.agsy.2022.103497
    Google Scholar Chamara, 2021 N. Chamara Development of an Internet of Things (IoT)
    Enabled Novel Wireless Multi-Sensor Network for Infield Crop Monitoring. Master’s
    Thesis, Department of Biological Systems Engineering, University of Nebraska-Lincoln
    (2021) Google Scholar Datasheet Raspberry Pi Model, 2019 Datasheet Raspberry Pi
    Model B, 2019. https://datasheets.raspberrypi.org. Accessed 11 November 2023.
    Google Scholar Firdaus-Nawi et al., 2018 Firdaus-Nawi, M., Noraini, O., Sabri,
    M.Y., Siti-Zahrah, A., Zamri-Saad, M., Latifah, H., 2018. DeepLabv3+_Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation. In: Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 801–818. Google Scholar
    Ghorai et al., 2021 A.K. Ghorai, A.R. Barman, B. Chandra, K. Viswavidyalaya, S.
    Jash, B. Chandra, K. Viswavidyalaya, B. Chandra, K. Viswavidyalaya Image processing
    based detection of diseases and nutrient deficiencies in plants SATSA Mukhapatra,
    25 (1) (2021), pp. 1-24 Google Scholar He et al., 2016 He, K., Zhang, X., Ren,
    S., Sun, J., 2016. Deep residual learning for image recognition kaiming. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778.
    doi: 10.1002/chin.200650130. Google Scholar LeCun et al., 1998 LeCun, Y., Bottou,
    L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document
    recognition. Proc. IEEE 86(11), 2278–2323. doi: 10.1109/5.726791. Google Scholar
    Liang et al., 2023 Liang, W. Z., Oboamah, J., Qiao, X., Ge, Y., Harveson, B.,
    Rudnick, D. R., Wang, J., Yang, H., Gradiz, A., 2023. CanopyCAM – an edge-computing
    sensing unit for continuous measurement of canopy cover percentage of dry edible
    beans. Comput. Electron. Agric. 204 (January), 107498. https://doi.org/10.1016/j.compag.2022.107498.
    Google Scholar Luis et al., 2020 Luis, S., Filipe, N.S., Paulo, M.O., Pranjali,
    S., 2020. Deep Learning applications in agriculture: a short review. Deep Learning
    Applications in Agriculture: A Short Review, 1092 AISC(January), C1. doi: 10.1007/978-3-030-35990-4.
    Google Scholar Meidas Trail Cameras, 2022 Meidas Trail Cameras, 2022. https://www.meidase.com/product-category/trail-cameras/.
    Accessed 11 November 2023. Google Scholar Mistry, 2016 Mistry, S., 2016. Arduino
    LoRa. MIT License. https://github.com/sandeepmistry/arduino-LoRa. Accessed 11
    November 2023. Google Scholar Park et al., 2007 Y. Park, R.K. Krell, M. Carroll
    Theory, technology, and practice of site-specific insect pest management J. Asia
    Pac. Entomol., 10 (2) (2007), pp. 89-101 View PDFView articleView in ScopusGoogle
    Scholar Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for
    multi-crop leaf disease image classification using convolutional neural network
    VGG Artif. Intell. Agric., 6 (2022), pp. 23-33, 10.1016/j.aiia.2021.12.002 View
    PDFView articleView in ScopusGoogle Scholar Richardson, 2019 A.D. Richardson Tracking
    seasonal rhythms of plants in diverse ecosystems with digital camera imagery New
    Phytol., 222 (4) (2019), pp. 1742-1750, 10.1111/nph.15591 View in ScopusGoogle
    Scholar Sakamoto et al., 2012 T. Sakamoto, A.A. Gitelson, A.L. Nguy-Robertson,
    T.J. Arkebauer, B.D. Wardlow, A.E. Suyker, S.B. Verma, M. Shibayama An alternative
    method using digital cameras for continuous monitoring of crop status Agric. For.
    Meteorol., 154–155 (2012), p. 113, 10.1016/j.agrformet.2011.10.014 View PDFView
    articleView in ScopusGoogle Scholar Sharma et al., 2020 P. Sharma, Y.P.S. Berwal,
    W. Ghai Performance analysis of deep learning CNN models for disease detection
    in plants using image segmentation Inf. Process. Agric., 7 (4) (2020), pp. 566-574,
    10.1016/j.inpa.2019.11.001 View PDFView articleView in ScopusGoogle Scholar Sritarapipat
    et al., 2014 T. Sritarapipat, P. Rakwatin, T. Kasetkasem Automatic rice crop height
    measurement using a field server and digital image processing Sensors (Switzerland),
    14 (1) (2014), pp. 900-926, 10.3390/s140100900 View in ScopusGoogle Scholar Taylor
    and Browning, 2022 S.D. Taylor, D.M. Browning Classification of daily crop phenology
    in phenocams using deep learning and hidden markov models Remote Sens. (Basel),
    14 (2) (2022), pp. 1-22, 10.3390/rs14020286 Google Scholar Tetila et al., 2020a
    Tetila, E.C., Machado, B.B., Astolfi, G., Belete, N.A.S., Amorim, W.P., Roel,
    A.R., Pistori, H., 2020. Detection and classification of soybean pests using deep
    learning with UAV images. Computers and Electronics in Agriculture, 179(May).
    doi: 10.1016/j.compag.2020.105836. Google Scholar Tetila et al., 2020b E.C. Tetila,
    B.B. MacHado, G.V. Menezes, N.A. De Souza Belete, G. Astolfi, H. Pistori A deep-learning
    approach for automatic counting of soybean insect pests IEEE Geosci. Remote Sens.
    Lett., 17 (10) (2020), pp. 1837-1841, 10.1109/LGRS.2019.2954735 View in ScopusGoogle
    Scholar The MathWorks, 2022 The MathWorks, I., 2022. MATLAB Coder - MATLAB. MathWorks.
    https://www.mathworks.com/products/matlab-coder.html. Google Scholar Tian et al.,
    2020 H. Tian, T. Wang, Y. Liu, X. Qiao, Y. Li Computer vision technology in agricultural
    automation—a review Inf. Process. Agric., 7 (1) (2020), pp. 1-19, 10.1016/j.inpa.2019.09.006
    View PDFView articleView in ScopusGoogle Scholar van Dijk et al., 2021 M. van
    Dijk, T. Morley, M.L. Rau, Y. Saghai A meta-analysis of projected global food
    demand and population at risk of hunger for the period 2010–2050 Nat. Food, 2
    (7) (2021), pp. 494-501, 10.1038/s43016-021-00322-9 View in ScopusGoogle Scholar
    Wang et al., 2022b Q. Wang, M. Cheng, S. Huang, Z. Cai, J. Zhang, H. Yuan A deep
    learning approach incorporating YOLO v5 and attention mechanisms for field real-time
    detection of the invasive weed Solanum rostratum Dunal seedlings Comput. Electron.
    Agric., 199 (July) (2022), Article 107194, 10.1016/j.compag.2022.107194 View PDFView
    articleView in ScopusGoogle Scholar Wang et al., 2022a J. Wang, Z. Gao, Y. Zhang,
    J. Zhou, J. Wu, P. Li Real-time detection and location of potted flowers based
    on a ZED camera and a YOLO V4-tiny deep learning algorithm Horticulturae, 8 (1)
    (2022), 10.3390/horticulturae8010021 Google Scholar Wang et al., 2014 Y. Wang,
    D. Wang, P. Shi, K. Omasa Estimating rice chlorophyll content and leaf nitrogen
    concentration with a digital still color camera under natural light Plant Methods,
    10 (3) (2014), pp. 273-286, 10.1016/S0378-4290(99)00063-5 View in ScopusGoogle
    Scholar Wang et al., 2019 A. Wang, W. Zhang, X. Wei A review on weed detection
    using ground-based machine vision and image processing techniques Comput. Electron.
    Agric., 158 (January) (2019), pp. 226-240, 10.1016/j.compag.2019.02.005 View PDFView
    articleView in ScopusGoogle Scholar Whigham et al., 2000 K. Whigham, D. Farnham,
    J. Lundvall, D. Tranel Soybean replant decision, Department of Agronomy, Iowa
    State University (2000) Google Scholar Yasrab et al., 2021 R. Yasrab, J. Zhang,
    P. Smyth, M.P. Pound Predicting plant growth from time-series data using deep
    learning Remote Sens. (Basel), 13 (3) (2021), pp. 1-17, 10.3390/rs13030331 View
    in ScopusGoogle Scholar Yuan et al., 2019 W. Yuan, N.K. Wijewardane, S. Jenkins,
    G. Bai, Y. Ge, G.L. Graef Early prediction of soybean traits through color and
    texture features of canopy RGB imagery Sci. Rep., 9 (2019), p. 14089, 10.1038/s41598-019-50480-x
    View in ScopusGoogle Scholar Zualkernan et al., 2022 I. Zualkernan, S. Dhou, J.
    Judas, A.R. Sajun, B.R. Gomez, L.A. Hussain An IoT system using deep learning
    to classify camera trap images on the edge Computers, 11 (1) (2022), pp. 1-24,
    10.3390/computers11010013 Google Scholar Cited by (1) YOLO performance analysis
    for real-time detection of soybean pests 2024, Smart Agricultural Technology Show
    abstract © 2023 The Authors. Published by Elsevier B.V. Part of special issue
    Agricultural Cybernetics: A New Methodology of Analysis and Development for Modern
    Agricultural Production Systems Edited by Yanbo Huang, Manoj Karkee, Lie Tang,
    Dong Chen View special issue Recommended articles LSCA-net: A lightweight spectral
    convolution attention network for hyperspectral image processing Computers and
    Electronics in Agriculture, Volume 215, 2023, Article 108382 Ziru Yu, Wei Cui
    View PDF Joint control method based on speed and slip rate switching in plowing
    operation of wheeled electric tractor equipped with sliding battery pack Computers
    and Electronics in Agriculture, Volume 215, 2023, Article 108426 Qi Wang, …, Yongjie
    Cui View PDF Monitoring maize lodging severity based on multi-temporal Sentinel-1
    images using Time-weighted Dynamic time Warping Computers and Electronics in Agriculture,
    Volume 215, 2023, Article 108365 Xuzhou Qu, …, Yuchun Pan View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 1 Captures Readers: 19 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rogers M.
  - Blanc-Talon J.
  - Urschler M.
  - Delmas P.
  citation_count: '1'
  description: Over the past two decades, hyperspectral imaging has become popular
    for non-destructive assessment of food quality, safety, and crop monitoring. Imaging
    delivers spatial information to complement the spectral information provided by
    spectroscopy. The key challenge with hyperspectral image data is the high dimensionality.
    Each image captures hundreds of wavelength bands. Reducing the number of wavelengths
    to an optimal subset is essential for speed and robustness due to the high multicollinearity
    between bands. However, there is yet to be a consensus on the best methods to
    find optimal subsets of wavelengths to predict attributes of samples. A systematic
    review procedure was developed and applied to review published research on hyperspectral
    imaging and wavelength selection. The review population included studies from
    all disciplines retrieved from the Scopus database that provided empirical results
    from hyperspectral images and applied wavelength selection. We found that 799
    studies satisfied the defined inclusion criteria and investigated trends in their
    study design, wavelength selection, and machine learning techniques. For further
    analysis, we considered a subset of 71 studies published in English that incorporated
    spatial/texture features to understand how previous works combined spatial features
    with wavelength selection. This review ranks the wavelength selection techniques
    from each study to generate a table of the comparative performance of each selection
    method. Based on these findings, we suggest that future studies include spatial
    feature extraction methods to improve the predictive performance and compare them
    to a broader range of wavelength selection techniques, especially when proposing
    novel methods.
  doi: 10.1007/s11694-023-02044-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Food Measurement and Characterization
    Article Wavelength and texture feature selection for hyperspectral imaging: a
    systematic literature review Review Paper Open access Published: 17 August 2023
    Volume 17, pages 6039–6064, (2023) Cite this article Download PDF You have full
    access to this open access article Journal of Food Measurement and Characterization
    Aims and scope Submit manuscript Mitchell Rogers , Jacques Blanc-Talon, Martin
    Urschler & Patrice Delmas  1876 Accesses 1 Citation Explore all metrics A Correction
    to this article was published on 27 September 2023 This article has been updated
    Abstract Over the past two decades, hyperspectral imaging has become popular for
    non-destructive assessment of food quality, safety, and crop monitoring. Imaging
    delivers spatial information to complement the spectral information provided by
    spectroscopy. The key challenge with hyperspectral image data is the high dimensionality.
    Each image captures hundreds of wavelength bands. Reducing the number of wavelengths
    to an optimal subset is essential for speed and robustness due to the high multicollinearity
    between bands. However, there is yet to be a consensus on the best methods to
    find optimal subsets of wavelengths to predict attributes of samples. A systematic
    review procedure was developed and applied to review published research on hyperspectral
    imaging and wavelength selection. The review population included studies from
    all disciplines retrieved from the Scopus database that provided empirical results
    from hyperspectral images and applied wavelength selection. We found that 799
    studies satisfied the defined inclusion criteria and investigated trends in their
    study design, wavelength selection, and machine learning techniques. For further
    analysis, we considered a subset of 71 studies published in English that incorporated
    spatial/texture features to understand how previous works combined spatial features
    with wavelength selection. This review ranks the wavelength selection techniques
    from each study to generate a table of the comparative performance of each selection
    method. Based on these findings, we suggest that future studies include spatial
    feature extraction methods to improve the predictive performance and compare them
    to a broader range of wavelength selection techniques, especially when proposing
    novel methods. Similar content being viewed by others A commentary review on the
    use of normalized difference vegetation index (NDVI) in the era of popular remote
    sensing Article Open access 31 May 2020 Feature selection techniques for machine
    learning: a survey of more than two decades of research Article 01 December 2023
    Remote sensing techniques: mapping and monitoring of mangrove ecosystem—a review
    Article Open access 17 July 2021 Hyperspectral imaging is emerging as one of the
    most popular non-destructive approaches for assessing food quality, and safety
    [1, 2]. Hyperspectral imaging performs digital imaging from spatial information
    augmented with spectral information based on spectroscopy. Exceeding the predictive
    performance of digital RGB imaging and human visual examination [3], spectral
    and spatial information captured in hyperspectral images allows for highly accurate
    sensory and chemical attribute prediction. This technology has enabled applications
    such as predicting the soluble solid content of kiwifruit [4] and classifying
    brands of Cheddar cheeses [5]. Hyperspectral images capture hundreds of contiguous
    narrow wavelengths, typically only a few nanometers wide, compared with the 5-50
    broad bands captured by multispectral sensors. The captured images consider the
    spatial context of the samples, which is not possible with single point sampling
    of spectroscopy [6]. Figure 1 illustrates the differences between these modalities.
    Using multivariate analysis, important information can be extracted from the observed
    reflectance images to predict attributes of the target sample [7]. Multivariate
    analysis establishes statistical or mathematical relationships between samples
    and their chemical attributes [8]. Predicting continuous measurements is a regression
    problem (e.g., total soluble solids of fruits [9]), whereas assigning a discrete
    class to each observation is a classification problem (e.g., identifying the geographical
    origin [2]). Most hyperspectral imaging studies in food science typically regress
    a target variable or classify a sample [10] based on the rich spatial and spectral
    information delivered by hyperspectral sensors. Fig. 1 Comparison of RGB channel
    digital imaging (left) to hyperspectral imaging (middle) and spectroscopy (right).
    Hyperspectral images capture hundreds of wavelength channels across the electromagnetic
    spectrum Full size image High dimensionality is the main challenge when analyzing
    hyperspectral data. Many applications of hyperspectral sensors require low-cost
    real-time (online) decision-making, such as sensors for unmanned aerial vehicles
    (UAVs) [11], assessment of food safety on processing lines [12], and guided surgery
    [13]. The high cost and low acquisition speed of hyperspectral sensors render
    these applications infeasible. High dimensionality also leads to models that overfit
    redundant features or noise [14], a phenomenon known as the Hughes phenomenon
    [15]. Dimensionality reduction is a way to address all three of these issues.
    Two methods exist for dimensionality reduction: projection-based methods and wavelength
    (feature) selection. Projection-based methods require capturing all the wavelengths,
    whereas wavelength selection reduces the required wavelengths. Based on the selected
    wavelengths, multispectral sensors can be designed that are cheaper, faster, and
    more robust than the original hyperspectral sensors [3]. Wavelength selection
    is essential for rapid and robust models [7, 16]. However, there is no consensus
    regarding which wavelength selection techniques provide the best predictive performance,
    as the results vary among studies [1, 6, 7, 17,18,19,20]. Previous reviews on
    hyperspectral imaging have comprehensively assessed the applications of hyperspectral
    imaging and spectroscopy in individual disciplines. For food science applications,
    reviews have surveyed applications in muscle foods [3, 12, 20, 21], seafood [22,
    23], fruits [6, 9, 24], or plant foods and vegetation [19, 25]. Other reviews
    have also investigated methods for making sense of hyperspectral image data, such
    as data mining [10], machine learning techniques [26], and deep learning [8].
    Previous reviews have only discussed a small set of feature selection methods
    and have not covered all commonly used methods [2, 10, 12, 19, 20, 24, 25, 27,28,29].
    Our review comprehensively surveys wavelength selection techniques across all
    applications of hyperspectral imaging that apply wavelength selection to provide
    a detailed answer to the best and most common wavelength selection methods. Hyperspectral
    imaging is unique compared with other spectroscopic techniques because the images
    produced include the spatial context of each pixel. Spatial features are an element
    of hyperspectral image analysis that is often disregarded in studies. Data fusion
    refers to how different feature modalities are combined. Selecting the best data
    fusion method is vital for maximizing the benefits of including spatial information.
    Although models based on spectral information are often sufficient for high classification
    or prediction accuracy, including spatial data has been shown to further increase
    the predictive performance [26, 30]. Previously published reviews discussed the
    importance of considering both spectral and spatial features to improve the reliability
    and prediction accuracy of models  [22]. However, a review has yet to explore
    the best practices for considering both the spectral and spatial features. Our
    study comprehensively reviews spectral and spatial feature selection techniques
    in hyperspectral imaging across disciplines to inform the use of spatial features
    in future investigations. This review provides a complementary guide to discipline-specific
    reviews by making the following contributions: A comprehensive review of the best
    and most common wavelength selection techniques for hyperspectral imaging applications.
    A comprehensive review of previous studies incorporating spatial features from
    optimal wavelengths. An overview of the typical sample sizes and targeted wavelength
    ranges of hyperspectral imaging studies using wavelength selection. Methods Systematic
    reviews are essential to quantify the usefulness of techniques and practices across
    all studies in a research area. This systematic review followed the Preferred
    Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework developed
    by David Moher [31] to survey hyperspectral imaging research that applied wavelength
    selection techniques to reduce dimensionality and the subset of studies that investigated
    spatial features. PRISMA reviews consist of four steps: identification, screening,
    eligibility, and inclusion criteria. This section describes the methodology followed
    in each step. Research questions and conceptual framework This study examined
    research related to wavelength selection for hyperspectral images by following
    the PRISMA framework on publications retrieved from the Scopus database. The first
    stage of research questions focuses on three areas: application area, study design,
    and methods (i.e., feature selection and learning). We also included an extended
    second stage of questions for the subset of studies that extracted spatial features.
    This study answers the following research questions: Standard hyperspectral study
    design (Section 2.2) RQ 1: How many samples are acquired in hyperspectral imaging
    studies? RQ 2: What wavelength ranges are most common for hyperspectral image
    analysis? Feature selection and learning (Section 2.3) RQ 3: What are the most
    common wavelength selection algorithms? RQ 4: Which wavelength selection algorithms
    provide the best predictive performance? RQ 5: Which learning algorithms are most
    common after wavelength selection? Spatial features (Section 2.4) RQ 6: What are
    the most common spatial features, and what parameters are used to extract these
    features? RQ 7: How are spectral and spatial features combined for learning models?
    RQ 8: How do studies select representative images to extract spatial features?
    RQ 9: Does combining spectral and spatial features improve the predictive performance,
    and which type of feature performs best individually? Selection criteria and search
    strategy The Scopus database was the primary source of publications. Scopus provides
    a wide selection of studies with more coverage than other academic databases such
    as Web of Science [32]. Scopus integrates more sources, particularly conference
    proceedings, and is still manually curated, unlike Google Scholar. 1 Our search
    included abstracts because not all publications mentioned wavelength selection
    within the title or keywords. We searched the titles, keywords, and abstracts
    of publications in the Scopus database using the following search string: (( “hyperspectral
    imaging” OR“spectroscopic imaging” OR “chemical imaging” OR “imaging spectroscopy”
    ) AND( “wavelength” OR “waveband” OR “wave band” OR “wave length” ) AND ( “selection”
    OR “selected” )) While the search string included multiple synonyms for hyperspectral
    imaging and wavelength selection, this does not have the effect of excluding studies
    missing “wavelength selection”, meaning that the search results may include unwanted
    studies. Studies were manually included and excluded based on the criteria listed
    in Table 1. The analysis was limited to a subset of hyperspectral imaging studies
    with wavelength selection to reduce the dimensionality. We did not exclude studies
    based on language (e.g., English, Chinese, and French) or year of publication.
    We only excluded studies based on publication type (e.g., conference or peer-reviewed
    journal) if the publication lacked a full methodology (e.g., published abstracts).
    Table 1 Selection criteria for the review process Full size table Study selection
    process The initial search stage of the literature review returned 1063 studies.
    The articles were retrieved on the 14th of October 2021. Our review was then updated
    with 166 more recent studies on the 13th of December 2022, resulting in a total
    of 1229 studies. The selection criteria defined in Table 1 were applied to reduce
    the total number of studies. Figure 2 shows a flowchart of the study’s selection
    process. Duplicate studies (n = 2) and conference announcements (n = 2) were excluded
    resulting in 1,225 records screened. Next, we set aside 17 reviews before screening
    the full-text results, leaving 1208 publications. Reviews were excluded because
    they did not provide empirical results comparing wavelength selection methods
    or follow a methodology to analyze the images. Fig. 2 Study selection process
    flow chart. This details the database search, the screening process and the number
    of studies included for two parts of the review Full size image The selection
    criteria addressed hyperspectral imaging, wavelength selection, and empirical
    results. This study defined hyperspectral imaging as acquiring images with a hyperspectral
    sensor capturing at least 90 wavelengths. Hyperspectral imaging is commonly defined
    in the literature as 100 wavelengths or more. We applied leniency to include studies
    that initially captured over 100 wavelengths but removed noisy bands during their
    analysis, reducing the total number of wavelengths to between 90 and 100. The
    second criterion is wavelength selection, which is defined as algorithm-based
    feature selection to select a subset of optimal wavelengths without limiting the
    size (such as restricting the search to a single band or pair of bands). These
    distinctions excluded studies that could perform a brute-force search of all wavelength
    combinations to select wavelengths. We excluded non-algorithmic and expert knowledge-based
    wavelength selection methods, such as manually selected band ratios. However,
    we included methods in which humans needed to manually interpret charts to select
    wavelengths (e.g., principal component plots or derivative curves). The final
    criterion was empirical results: each study required a transparent methodology
    incorporating selected wavelengths with machine learning or other statistical
    methods for hyperspectral image analysis. Each study was required to provide clear
    experimental results. This criterion excluded studies proposing novel hyperspectral
    systems or sensors without experimentation and studies with very few details available
    to answer our research questions. We screened the full texts of 1208 publications
    selected to establish which studies met our inclusion criteria (Table 1). As the
    lack of details in the abstract was not an exclusion criterion, all the studies
    required full-text screening. In this stage, 395 studies were eliminated. Of the
    eliminated studies, 156 did not apply an appropriate wavelength selection technique,
    while 100 did not investigate hyperspectral images. Finally, 14 studies were inaccessible
    to the researchers and associates. We contacted the original authors of 13 studies
    through ResearchGate. 2 Fourteen of the 16 originally inaccessible did not respond
    in time for this publication. The first stage of this review included 799 studies.
    A complete list of these publications is available in the Supplementary Material
    of this manuscript. The second stage of this review included 62 English studies
    that extracted spatial features from hyperspectral images to answer the spatial
    feature research questions defined in Section 1.1. The entire study population
    (n = 799) and the subset that utilized spatial features (n = 71) were the two
    groups of studies investigated. Encoding of the studies A single reviewer manually
    checked whether each study met our inclusion criteria and recorded all relevant
    information in a Google form. 3 The reviewer then completed multiple passes, checking
    one attribute at a time across all the studies to check the information collected,
    and our inclusion criteria were applied consistently. Google Translate was sufficient
    to extract information from non-English studies during the first stage of the
    review. The second stage was limited to studies written in English to avoid misrepresentation
    of information due to incorrect translations. The data collection process was
    manual, as some studies were not readable using automation tools. The language,
    year of publication, and broad application area (e.g., meat science or medical
    imaging) of each study were collected for demographic and application area analyses.
    The wavelength range and sample size were collected from each study to answer
    the hyperspectral study design research questions. The effective wavelength ranges
    were collected after the authors discarded noisy bands (if applicable) of all
    hyperspectral systems investigated. The sample size was recorded as the number
    of unique objects imaged in the study after the outlying samples were excluded
    (if applicable). The collected publications contained over 200 wavelength selection
    techniques and learning algorithms. From each study, we collected a comma-separated
    list of the techniques for wavelength selection and machine learning and an ordered
    list of the best wavelength selection methods based on their performance. Performance
    was defined as the accuracy of predicting the class or continuous measurement
    of interest. The performance of the sets of optimal wavelengths was determined
    based on the prediction set (testing set) accuracy of the best learning model
    for this subset. When there was a tie, the validation set followed by the calibration
    set accuracy acted as the tiebreaker. The methods were ranked based on their performance
    on the prediction set when one method clearly outperformed a subset of the others.
    If no method decisively outperformed the others, or if there was no comparison
    between multiple methods, we recorded the best method as inconclusive. Many studies
    have applied only a single wavelength selection method. In this case, the best
    method was recorded as only one. The results were tabulated with Python scripts,
    ranking which wavelength selection methods performed better based on the number
    of comparisons with other methods. We only included wavelength selection techniques
    applied in seven or more studies, and binned all others into the Other category.
    For example, if one study applied genetic algorithms (GA), successive projections
    algorithm (SPA), and regression coefficients (RC) to select an optimal subset,
    and they were ranked as such, we recorded three comparisons: GA outperformed SPA,
    GA outperformed RC, and SPA outperformed RC. We tabulated the number of studies
    that utilized each learning algorithm using Python scripts based on the collected
    comma-separated lists. Many studies have tested multiple variants of the same
    family of algorithms, such as random forests and decision trees. For more valuable
    insights, we counted the number of studies as a measure of popularity rather than
    the number of occurrences of each algorithm, meaning a study comparing two versions
    of support vector machines counted as a single observation. We recorded details
    from a subset of studies that extracted spatial features for the final research
    questions related to spatial features using a separate Google form. We recorded
    a list of the extracted spatial features, the parameters of these feature extractors,
    the performance of spectral features compared to spatial features, the performance
    of data fusion compared to individual models, how representative feature images
    were selected to extract spatial features, and how spectral-spatial features were
    fused. We recorded whether the data fusion models outperformed the individual
    models as yes, no, or inconclusive. The best individual set of features was recorded
    as spectral features, spatial features, not compared individually, or inconclusive.
    Both were determined based on the accuracy on the prediction set of the best model.
    All studies required details regarding wavelength selection techniques to meet
    our inclusion criteria. Not all the other attributes collected were compulsory.
    For each research question, we ignored studies in which the related attributes
    of interest were not stated or unclear. Only studies investigating spatial features
    were gathered to answer the spatial feature research questions. Since the information
    gathered is essential for the reproducibility of each study, missing data could
    indicate lower publication quality. A spreadsheet stored the information collected
    by this review and generated simple statistics (e.g., counts). Python scripts
    extracted other information required to answer questions (e.g., comparisons of
    methods and the wavelength range graph) and were used to generate tables and figures.
    The Python libraries required were Pandas 4 for reading the data, 5 and Matplotlib
    6 for creating figures. Biases in this study This review may introduce bias by
    including several studies from prevalent authors or research groups. Some research
    groups may follow the same methodology for multiple hyperspectral imaging applications.
    This imbalance may skew the conclusions on which wavelength selection, learning
    algorithms, camera models, and other attributes of interest are the most popular
    for hyperspectral imaging applications. No methods explored the causes of heterogeneity
    among the study results because each study acquired different datasets, and no
    control groups were available. No method assessed the risk of bias due to missing
    results, the robustness of results, or the certainty of the body of evidence for
    an outcome. There is a significant imbalance in the occurrence of many wavelength
    selection techniques. Some wavelength selection techniques have been utilized
    in a small set of studies (e.g., LASSO [33]), and others have been consistently
    applied across hundreds of studies (i.e., SPA [34], and CARS [35]). We limited
    the analysis to methods applied in more than five studies to reduce the impact
    of less common techniques. Results The initial search returned 1229 publications.
    After applying the selection criteria, 799 publications were selected as relevant
    and analyzed, as described in the following subsections. Although all 799 studies
    informed the conclusions of this review, not all were cited here. The complete
    list of included and excluded studies with the reason for their exclusion is available
    in the Supplementary Material. Demographic characteristics of research studies
    Fig. 3 Number of studies included in this review between 2000 and 2022. The number
    of studies increases over time. There has been a decline in recent years due to
    the COVID-19 pandemic limiting research Full size image The reviewed studies included
    734 journal articles and 65 conference papers based on the classification provided
    by Scopus. Of the total 799 studies, the publication language included 646 studies
    in English, 149 in Chinese, and four published in other languages (German, Spanish,
    French, and Persian). An analysis of publications per year showed an increase
    in hyperspectral imaging studies with wavelength selection (Figure 3). This increase
    is likely due to the increased accessibility of affordable hyperspectral sensors.
    The COVID-19 pandemic may have restricted access to research laboratories, causing
    a slight decline in research outputs since 2020. Standard hyperspectral study
    design Sample sizes The first aspect of designing a hyperspectral imaging study
    examined in this review is the number of samples required (RQ 1). It is essential
    to ensure that each study has an adequate number of samples for training the models
    and a representative set of unseen samples for validation. To compare the sample
    sizes across the population of hyperspectral imaging studies included in this
    review, we must define a single sample. The number of images cannot represent
    the sample sizes because some may contain multiple samples (e.g., multiple maize
    kernels or grains of rice). The number of subjects purchased or retrieved may
    not be reliable since researchers can obtain measurements from multiple subsamples
    (e.g., measuring the tenderness of broiler breast fillets [36]). Therefore, we
    determined the number of samples as the number of individual samples of interest
    with the corresponding reference values or classes. The box and whisker plot shown
    in Fig. 4 summarizes this data on a logarithmic scale due to the skew in observations;
    the median number of samples was 180, with the middle 50% between 105 and 300
    samples. The mean was 424.04 samples, and the maximum and minimum were 19,000
    and 1, respectively, indicating a clear skew in the data due to outliers. Some
    studies had as few samples as a single image, such as studies using the Indian
    pines dataset [37, 38] for land use classification. Studies with thousands of
    samples typically investigated low-cost, easy-to-image subjects, such as oats,
    maize kernels, or other types of seeds [39,40,41,42,43,44]. The study with the
    second highest number of samples (15,000) collected multiple years of data to
    detect diseases affecting grape cultivars [43]. Fig. 4 Number of samples (log
    scale) encountered in hyperspectral studies that applied wavelength selection.
    The median number of samples was 180, with the middle 50% between 105 and 300
    samples. The log scale was required for the outliers, which included one study
    using a single image and another using 19,000 samples Full size image Some studies
    acquired images of the same subjects across multiple timestamps to see how damage
    symptoms developed over time [45] or to compare before and after treatment [46,
    47]. Sampling like this increases the number of observations and sample variation.
    However, this is possible only when there is no destructive analysis to retrieve
    the ground-truth reference values from a sample. In these studies, we counted
    the number of imaged samples rather than the number of unique samples. The chemical
    analysis of samples is often destructive. Depending on the amount of material
    required for this analysis, measuring multiple attributes of each sample may not
    always be possible [48,49,50]. For classification, a single sample could be subdivided
    into regions with multiple classes, such as those for detecting white stripping
    on chicken fillets [51], where multiple affected areas were visible from a single
    sample. Another example is where multiple defective and unaffected regions are
    visible in the same image. Researchers studying the spectral response of damage
    to fruit samples typically defined multiple regions of interest for the damaged
    and unaffected areas [52, 53]. In conclusion, most studies captured between 105
    and 300 samples, but the differences in methodologies between study types made
    recording the sample sizes difficult. Wavelength ranges The second aspect of the
    study design investigated in this review is how to select a suitable wavelength
    range to capture hyperspectral images (RQ 2). Figure 5 shows the wavelength ranges
    studied in the 681 studies that clearly indicated their sensor range. Based on
    the sensors available on the market, the three most common ranges were visible/near-infrared
    (VIS/NIR: 400–1000 nm), near-infrared (NIR: 900–1700 nm), and shortwave infrared
    (SWIR: 900–2500 nm). The specific range of interest depends on the application.
    For example, predicting the attributes of samples correlated with moisture is
    more suitable in an infrared range where water absorbs more light, but where pigments
    are correlated with the target attribute, a sensor focusing on visible light is
    more appropriate. Most studies focused on one of these ranges, and only 50 investigated
    multiple ranges. When researchers combine multiple sensors (to examine the spectral
    features over a more extensive wavelength range), the authors extracted the mean
    spectra from each sensor and concatenated the vectors to form a single contiguous
    spectral vector that represented the sample. The researchers then fitted a model
    to the selected wavelengths from the combined spectra. No study in this review
    extracted extended pixel-wise spectra by combining data from multiple sensors
    because they could not find corresponding pixels between sensors. Fig. 5 Histogram
    of the number of studies investigating each wavelength range Full size image Figure
    5 shows a spike around 900–1000 nm in the wavelength range of interest due to
    the overlap between VIS/NIR and other sensors. The VIS/NIR range was the most
    common due to the accessibility of these sensors [54], and the lower cost of the
    detector coatings for this wavelength range [7]. There is no significant difference
    in the choice of sensors based on the sample type between these different ranges.
    The proportion number of studies looking at the SWIR and VIS/NIR is approximately
    the same for meat products as for fruit and vegetable-related studies, as shown
    by the lines on the plot. Feature selection and machine learning Wavelength selection
    Previous reviews of wavelength selection techniques have divided the field into
    three categories based on how the methods find optimal features [29]. These categories
    are filter, embedded, and wrapper methods. Filter methods apply a threshold to
    a feature importance score to select the best wavelengths. Examples of filter
    methods include regression coefficients and variable importance in projection
    (VIP). Embedded methods integrate learning and feature selection, such as LASSO
    regression [33] and decision trees. Wrapper methods are the most popular category,
    and they operate by iteratively updating the wavelength subsets and fitting models
    to evaluate the performance. Wrapper methods include successive projection algorithms
    (SPA) [34] and genetic algorithms [55]. We also propose a fourth category of techniques,
    manual selection methods, in which an algorithm provides an output that is interpretable
    by experts to select the key wavelengths. Principal component analysis loading
    plots are an example of this type of method. Researchers typically determine points
    at the peaks/troughs of a plot as the important wavelengths. We extend this categorization
    to two other categories of wavelength selection methods: concatenated and interval-based
    methods. Concatenated methods string multiple selection algorithms together to
    select important features from previously selected subsets, and interval-based
    methods select an informative interval of wavelengths, often of a manually determined
    width. Concatenated methods can combine the advantages of multiple algorithms,
    such as UVE and SPA. UVE has problems with multicollinearity, and SPA may select
    uninformative variables. UVE-SPA first removes uninformative wavelengths and SPA
    removes variables with the least multicollinearity [16]. In Table 2, we have categorized
    the most popular wavelength selection techniques into these categories and cited
    recent English studies that employed each technique (RQ 3). Table 2 An overview
    of wavelength selection techniques ranked by popularity Full size table Tables
    3 provide an exhaustive comparison of wavelength selection techniques applied
    in the area against each other (RQ 4). These tables present the number of studies
    in which the selected method (rows) outperformed the comparison method (columns).
    Where the table reads “Only one”, it counts the number of studies where the comparison
    method was the only wavelength selection method. The “Inconclusive” row presents
    the number of studies that applied the wavelength selection technique, where the
    ranking of each method was unclear. Finally, the “Not compared” row provides the
    number of studies using the method that did not compare it to the other methods.
    From these tables, we find that there is a clear set of common methods: successive
    projections algorithm (SPA) [34], competitive adaptive reweighted sampling (CARS)
    [35], regression coefficients (RC) [60], and principal component analysis (PCA)
    loadings. Among these techniques, CARS was most often the best method. CARS performed
    better than SPA in 50 of 76 comparisons, where one was better than the other.
    Researchers in 122 studies manually examined the peaks and troughs in the PCA
    loadings plot to select the key wavelengths. 79 studies applied this as the only
    wavelength selection technique, compared with 59 studies that compared it with
    other methods. PCA loadings outperformed SPA in four out of 14 comparisons, CARS
    in one out of 6 comparisons, and regression coefficients in three out of seven
    comparisons, where one method was better. PCA loadings are among the worst-performing
    techniques despite being among the most popular methods. The most common and consistent
    methods found in this review were the CARS and SPA methods. The concatenated methods
    and variable importance in projection (VIP) are the only comparison methods in
    which the CARS algorithm did not perform well. Concatenated and interval-based
    methods, UVE and CARS outperformed SPA, whereas SPA performed well in all other
    comparisons. Regression Coefficients were the sole wavelength selection method
    used in 89 studies, and in 57 studies where researchers compared it to others,
    it did not perform well. The performance of regression coefficients is demonstrated
    by comparisons with genetic algorithms (best in two out of seven comparisons),
    CARS (best in five out of 16 comparisons), and SPA (best in 11 out of 30 comparisons).
    Regression coefficients, as with PCA loadings, is a simple feature selection method,
    making it more popular than novel methods that are not available in standard libraries
    for different programming languages or built into commercial software. For the
    two concatenated methods utilized in more than five studies, there were very few
    comparisons with the other techniques. Studies that included these methods often
    compared them to other concatenated methods [129, 130]. CARS-SPA and UVE-SPA performed
    well but were only applied in 20 and 15 studies, respectively. CARS-SPA had the
    highest performance, outperforming SPA in ten out of 15 comparisons and an equal
    performance against CARS (better in eight out of 16 comparisons). UVE-SPA similarly
    performed well but underperformed other concatenated approaches, such as CARS-SPA
    (best in two out of five comparisons). UVE-SPA was outperformed by CARS and UVE
    individually (best in two out of nine and one out of seven comparisons, respectively).
    The most common interval-based approaches encountered are interval-partial least
    squares (iPLS) [89], interval-VISSA (iVISSA) [103], Synergy interval partial least
    squares (siPLS) [123], and interval random frog (iRF) [105] were often compared
    to each other. These methods divide the full spectrum into equidistant partitions
    and fit regression models to the intervals [91]. There were not enough comparisons
    of these methods to conclude which interval-based method was the best, and their
    performance against single-feature methods was generally poor. Many studies have
    proposed a novel wavelength selection method and compared it to a small set of
    the most common approaches while claiming superior performance. Comparisons with
    a broader range of techniques over multiple datasets are required to benchmark
    their performance. Since five or fewer studies applied each of these methods,
    we grouped them methods into the “other” column of Table 3. To the best of our
    knowledge, no standardized benchmark datasets are available for wavelength selection.
    Other popular methods for wavelength selection include genetic algorithms [55],
    random frog [70], uninformative variable elimination [67], and variable importance
    in projection [60]. Genetic algorithms performed well, but it is not easy to compare
    to this algorithm because many hyperparameters must be manually set. Machine learning
    methods The final step in model creation is to fit the model to the selected wavelengths.
    Here, we list the most common machine learning and statistical models applied
    to the selected wavelengths (RQ 5). Partial least squares (PLS) is the most common
    learning algorithm for analyzing hyperspectral data after wavelength selection.
    A total of 470 studies chose a variant of PLS. The most common variants are partial
    least squares regression (PLSR) for regression and partial least squares discriminant
    analysis (PLS-DA) for classification. Previous reviews have reported the high
    utilization of PLSR and PLS-DA [131]. Instead of directly operating on the variables,
    PLS extracts a set of latent variables with the best predictive performance [21].
    Seventy-four studies applied a more straightforward multiple linear regression
    (MLR) that fits a simple linear equation to the observed data. MLR struggles with
    high multicollinearity between wavelengths [132], which makes prior wavelength
    selection important. The advantage of MLR is the interpretability of the results,
    whereas the meaning of the latent variables or principle components is unclear.
    The second most common algorithm was support vector machines (SVM) [133] in 333
    studies, the most common variants of which were least squares support vector machines
    (LS-SVM) and support vector regression (SVR). Table 3 Comparative performance
    of wavelength selection algorithms Full size table Various studies applied variants
    of artificial neural network (ANN) architectures, such as backpropagation neural
    networks (BPNN) [134], extreme learning machines (ELM) [135], stacked autoencoders
    (SAE), and convolutional neural networks (CNN) [136]. These machine learning approaches
    apply a series of processing layers to extract higher-level features and are often
    referred to as deep learning approaches. Non-linear methods such as ANNs and SVMs
    are valuable for modelling complex relationships between dependent and independent
    variables. These non-linear approaches have a higher computational complexity
    [10]. Only a few studies have applied deep learning to hyperspectral imaging for
    food applications because of the time and cost requirements for gathering large
    datasets with corresponding reference (ground-truth) measurements [8]. Few studies
    (n = 20) have combined convolutional neural networks with wavelength selection
    despite their popularity in computer vision outside hyperspectral imaging, but
    these techniques have become more common since 2022 [57, 137,138,139]. Due to
    the challenges of big data and small sample sizes, CNN approaches typically apply
    one-dimensional filters to the spectral response rather than two- or three-dimensional
    filters to the hyperspectral image. Table 4 displays the counts for each learning
    algorithm found by this study. Table 4 Machine learning models with wavelength
    selection sorted by popularity Full size table Decision trees were found in 67
    studies, including variants such as random forests and classification and regression
    trees (CART). K-nearest neighbors (KNN) is a simple method for classifying samples
    based on their distance from other labelled samples. Because of the number of
    neighborhood comparisons required, KNN becomes more computationally expensive
    with larger datasets. Projection methods such as linear discriminant analysis
    (LDA) and principal component analysis (PCA) distinguish classes within data by
    projecting the remaining wavelengths onto new axes to maximize variance and class
    separability. Principal component regression (PCR) is applied instead of PCA for
    regression analysis. Fifty-three studies utilized LDA compared with 26 studies
    that chose PCA instead. Of the 799 studies surveyed, 77 compared learning algorithms
    that did not fit these categories. Spatial features Spatial descriptors (image
    texture descriptors) provide information about the spatial arrangement of pixels,
    whereas spectral information describes how light interacts with samples. As discussed
    earlier, many reviews acknowledged that including spatial features as independent
    modelling variables helps to improve the predictive performance of models [22].
    Spatial information includes statistical features that summarize the distribution
    of intensities, such as statistical moments, and higher-order spatial descriptors
    (statistics) describe the distribution of groups of pixels, such as pairs for
    grey-level co-occurrence matrices. Spatial features may also include local neighborhood
    operations, such as local binary patterns (LBP) [140], or shape features describing
    the shape of the regions of interest. This section describes the different techniques
    for extracting spatial features from hyperspectral images (RQ 6), approaches to
    select feature images (RQ 8), the predictive performance of models incorporating
    spatial features (RQ 9), and how studies combine spatial features with spectral
    features (RQ 7). Table 5 Comparison between the 71 studies included in this review
    sorted chronologically, using spatial features with hyperspectral imaging and
    wavelength selection Full size table Spatial features descriptors A total of 71
    out of the 799 studies selected for review included spatial features and wavelength
    selection for hyperspectral image analysis. Table 5 provides a breakdown of these
    studies, answering RQ 6. The most popular spatial features were the grey-level
    co-occurrence matrices (GLCM) texture descriptors found in 41 out of 71 studies.
    GLCM descriptors (also known as Haralick features) describe the two-dimensional
    feature image by creating a matrix of the probability of joint occurrences of
    pixel grey level values at a given angle (\\(\\theta\\)) and distance (d) [202].
    Haralick et al. [202, 203] described 14 features to summarize GLCM matrices into
    texture descriptors. Studies such as Clausi [204] and Soh and Tsatsoulis [205]
    further extended this set of features. The five most common features encountered
    were: contrast/inertia moment (n = 38), energy/uniformity/angular second moment
    (n = 38), correlation (n = 36), homogeneity/inverse difference moment (n = 35),
    and entropy (n = 19). Four studies investigated features beyond this set of five
    core features [171, 178, 184, 194], but two did not provide sufficient information
    on the extracted features. One of the earliest studies combining spatial features
    with wavelength selection [194] extracted 22 GLCM features, including multiple
    methods for selecting the feature image. However, these studies did not examine
    the effectiveness of each feature. The main parameters of the GLCM are the angle
    and distance used to measure pixel correlation. Typically, studies chose four
    angles 0°, 45°, 90°, and 135° describing the relationships diagonally, vertically,
    and horizontally to create GLCM matrices. Different angles and distances create
    different matrices. Eleven studies averaged the descriptors over the four orientations
    to produce a rotation-invariant descriptor. Many studies did not state whether
    they averaged over multiple angles (n = 16), did not average over angles (n =
    8), or only employed a single angle (n = 5). For the distance parameter, the typical
    approach is to create matrices based on a one-pixel distance. Few studies have
    examined a broader range of distances, such as one to ten pixels [201] or one
    to five pixels [164, 190]. Because we can fix the distance between the samples
    and sensors in controlled environments, a single distance should be sufficient.
    Future studies should be conducted using multiple distances to determine the best
    effect. The second most popular spatial feature extraction method was the grey-level
    gradient co-occurrence matrix (GLGCM) [206] (n = 11). Similar to GLCM, GLGCM captures
    second-order statistics about the spatial image content by describing the grey-level
    gradients of the images. The GLGCM matrix represents the relative frequency of
    occurrence between a pixel with a given grey value and gradient. The data fusion
    model outperformed the single modality models for all studies extracting GLGCM
    features that compared fusion models to individual models (n = 10). GLGCM has
    a much larger set of features than GLCM, describing the dominance and distribution
    of gradients and the entropy, average, standard deviation, asymmetry, and nonuniformity
    of both the pixel gradients and grey level. The popularity of GLCM and GLGCM was
    consistent with previous reviews [1, 14]. Another spatial feature extraction approach
    is local binary patterns (LBP) [169, 175, 182] where each pixel was compared to
    its horizontal, vertical, and diagonal neighbors to give an eight-digit binary
    code [140]. Each position in the binary code describes whether the neighboring
    pixel value is larger (1) or smaller (0) than the target pixel. There were mixed
    results from the LBP features for hyperspectral images. One study extracted LBP
    features from RGB images to classify freezer-burnt salmon with no data fusion
    and found that the spectral-based model performed better for discriminating classes
    [175]. Another study found that GLGCM and GLCM significantly outperformed LBP
    without data fusion [169]. GLCM, GLGCM, and LBP provide second-order statistics
    based on the relationships between pixel pairs [207]. SSome studies have extracted
    first-order statistics, such as histogram descriptors, based on the histogram
    of intensities in the feature image. Histogram statistics summarize the grey value
    distribution of the image without considering the spatial interactions between
    pixels. The most common features extracted from these grey-level histograms are
    mean, uniformity, entropy, standard deviation, and third-moment (skewness) [186,
    195, 208], as well as kurtosis, energy, smoothness, contrast, consistency, and
    roughness [141, 149, 154, 178, 180]. One study applied feature selection to the
    precomputed spatial features and found that the selection algorithm did not select
    histogram-based features for beef tenderness forecasting [182]. Other studies
    found that histogram statistics improved the model accuracy more than GLCM features
    [178] or shape descriptors [186]. The GLCM features significantly outperformed
    histogram statistics in another study [51]. Five studies investigated Gabor filters
    for extracting spatial features from hyperspectral images [146, 165, 170, 182,
    191]. Gabor filters, which are linear filters used for texture analysis, apply
    convolutions with kernels generated by combining the Gaussian and sinusoidal terms.
    The results are often averaged over multiple angles to obtain rotationally invariant
    descriptors [146, 165]. Prior to 2020, only one study extracted spatial features
    with a convolutional neural network (CNN) [153]. This CNN architecture utilized
    two branches: one extracted spectral features with a one-dimensional CNN and the
    second extracted spatial features with a two-dimensional CNN. The model concatenated
    the extracted feature vectors for classification into a single feature vector.
    Feature importance was evaluated using the weights learned by the CNN in the first
    layer to select the optimal wavelengths. Since then, more advanced two-dimensional
    CNN architectures have been used to utilize the spatial information within images
    [84, 143]. One study applied the YOLOv3 algorithm [209], a popular object detection
    algorithm, to detect defects on apples [139]. The final common spatial feature
    extraction approaches accounted for were morphological/shape features. These approaches
    described the contour of the region of interest encompassing the sample with shape
    features. Shape features require only the ROI mask of the sample. The typical
    features extracted were the area, perimeter, major- and minor-axis lengths, and
    eccentricity. The studies that extracted shape features had good results for discriminating
    the varieties of seeds [197] and black beans [181] because of the correlation
    between shape and variety. The accuracy of the models trained on spectral features
    in both cases outperformed spatial features, and the data fusion model further
    improved the results. Other studies have applied edge detection algorithms to
    detect cracks and morphological features to detect scattered egg yolk [142]. Other
    less frequently used spatial features extracted include grey-level run-length
    matrix analysis [183], wide line detectors [146, 191], and wavelet-based transformations,
    such as Discrete Wavelet Transform (DWT) [96, 192], and Discrete cosine transformation
    [172]. Selecting feature images Each feature extraction approach requires an image
    to form the basis of spatial feature descriptors. The most common methods extract
    features from the principal component (PC) score images of a full hyperspectral
    image or pre-selected wavelengths. Selecting the optimal wavelengths before extracting
    spatial features is less computationally intensive. Preselecting optimal wavelengths
    assumes that wavelengths with important spectral information also carry important
    spatial information. Selecting the optimal wavelengths before extracting the spatial
    features was employed in 48 studies. Another approach is to extract spatial features
    from each wavelength within the hyperspectral images and then apply feature selection
    to reduce the features to a set of optimal features from a reduced number of bands
    (post-selected). Two studies directly selected bands with important spatial features
    by extracting them from all wavelengths [146, 188]. Other studies have applied
    feature selection to select informative features from a selected or full feature
    set [144, 165, 173, 191]. Twenty-five studies selected PC score images as feature
    images. Although extracting texture descriptors across the entire spectrum is
    possible, this increases the computational complexity, making it infeasible in
    real-time systems. A single study utilised spatial features from all available
    wavelengths without feature selection [177], and five studies created either RGB
    images from the hyperspectral sensor or captured RGB images from a secondary camera
    [84, 142, 166, 175, 201]. The selected feature images are listed in the feature
    image column of Table 5. Data fusion There are three levels of data fusion for
    combining features from different feature spaces [210]. Pixel-level fusion integrates
    multiple modalities as co-registered inputs into a single feature extraction model,
    feature-level fusion combines features after extracting characteristic features,
    and decision-level fusion combines multiple model outputs after fitting models
    to the extracted features. Almost all studies that considered spatial features
    combined spectral and spatial features with feature-level fusion, with some CNN-based
    models integrating spectral-spatial information at the pixel level [84, 139].
    They first extracted features from the images to combine spectral and spatial
    features before fitting the models to the fused feature vector. It is possible
    to apply decision-level fusion by training independent models on the spectral
    and spatial features and then using a meta-model to the decisions of the separate
    models. Models trained after feature-level fusion can incorporate interactions
    between individual features, which decision-level fusion models cannot. Feature-level
    fusion avoids the computational burden of high-dimensional feature extraction
    in pixel-level fusion. Pixel-level fusion is feasible when data from two hyperspectral
    sensors with different wavelength ranges are fused. All the studies that merged
    data from multiple sensors extracted the mean spectra from each sensor’s ROI and
    combined them into a single spectrum for modelling, which would also be considered
    feature-level fusion. However, regions of interest could be registered together
    to combine multiple wavelength ranges for each pixel. Mean normalization is often
    applied to features of different modalities (spectral and various types of spatial
    descriptors) to overcome problems caused by disparities between features [154,
    165, 171, 180, 183, 185, 189]. Normalization rescales each feature in the training
    set (calibration) by its mean and standard deviation, giving all features a comparable
    magnitude. Performance Thirty-seven studies that included spatial features within
    their feature sets found that the predictive performance of models trained on
    spectral features alone outperformed those trained solely on spatial features
    (Table 5). Only five studies discovered that spatial characteristics were more
    helpful for predicting attributes than spectral features, and 29 studies failed
    to compare the two approaches in a way that allowed us to determine which model
    was the most effective. However, the selected model also affected the results.
    The performance metric that we used was the accuracy of the best model. When combining
    spectral and spatial features, we found that most studies (n = 40) performed better
    by combining spectral and spatial features than by using spatial or spectral features
    individually. The individual model outperformed the data fusion model in six studies.
    Finally, 25 studies did not compare the individual models to the spectral-spatial
    fusion models. In these outlying studies, spatial features tended to be more appropriate
    for the problem because they corresponded to clearly visible texture indicators.
    For example, pork marbling can be detected using a wide-line detector to predict
    intramuscular fat [146]. Other studies related to pork meat assessment found that
    Gabor and GLGCM features outperformed spectral features for prediction of the
    total volatile base nitrogen content (TVB-N) [165] and freshness prediction [187],
    respectively. For the classification of tea varieties [167] and prediction of
    the water content of tea leaves [196], the GLCM features outperformed the spectral
    features. These studies showed that spatial features could be more valuable than
    spectral features of hyperspectral images in predicting the attributes of interest.
    Studies have likely found worse performance from data fusion models because they
    did not include spatial features correlated with the prediction variable. One
    study observed a significant drop in performance between the calibration set and
    the prediction set for classifying maize seed quality, possibly indicating that
    the model was overfitting to the data [154]. Overfitting occurs when the model
    identifies trends in the calibration set that are less prevalent in the prediction
    dataset. Another study reported a similar predictive performance between spectral
    and data fusion models for predicting the tenderness of salmon [190]. GLCM features
    did not help predict chilling injury classes of green jujubes [162]. In these
    studies, the spatial features may have been inappropriate for the particular problem,
    or the wavelengths selected based on the spectral data may have been unsuitable.
    Discussion This review systematically surveyed studies that applied wavelength
    selection to hyperspectral imaging and found that food quality and safety accounted
    for most of the applications. Popular subjects included pork, apples, maize kernels,
    wheat, and potatoes, and common attributes of interest included moisture content,
    variety, and adulteration. Standard hyperspectral study design Surveying the number
    of samples in each study showed that hyperspectral imaging studies utilized significantly
    fewer samples than other machine learning applications in computer vision. The
    main reason for this is the cost and time required to acquire sample images. Due
    to the lower accessibility of sensors, it is not possible to use crowd-source
    training data such as popular RGB image datasets [211]. Condensing each sample
    to a single mean reflectance spectrum reduces spatial dimensionality at the cost
    of losing potentially useful spatial information. While deep learning has become
    the most popular approach for many computer vision tasks, typical datasets for
    deep learning, such as object detection or image classification datasets, often
    have millions of annotated RGB images. With more cost-effective hyperspectral
    sensors, researchers can create larger hyperspectral imaging datasets and apply
    deep learning models that reduce the feature set to find reliable trends with
    fewer observations. This review also concludes that VIS/NIR range sensors are
    the most common for hyperspectral imaging applications with wavelength selection.
    VIS/NIR sensors are cheaper and provide adequate information for most studies.
    Future studies should consult prior works to determine which wavelength range
    suits their application. Feature selection and machine learning Wavelength selection
    Given the current state of the literature, determining which feature selection
    technique yields the best wavelengths remains highly application dependent. Many
    confounding factors affected the accuracy of the models. Studies that have evaluated
    multiple wavelength selection methods and multiple classifiers may have one wavelength
    selection technique that performs best with one classifier and another performing
    best with a different classifier. Each study may differ in the implementation
    of each algorithm, experimental conditions, and datasets. The specific implementation
    may differ for methods with multiple hyperparameters, such as genetic algorithms,
    which require a fixed population size, genetic operator probability, number of
    generations, and a fitness function. Many studies have proposed novel methods
    to select features but have not comprehensively compared the most common wavelength
    selection methods. We suggest that future studies compare the most common related
    methods to provide a more accurate benchmark for new feature selection methods
    on a particular dataset. The best-performing and most popular methods are the
    SPA, CARS, and genetic algorithm methods. This review does not consider the number
    of wavelengths selected by each method. UVE often uses hundreds of wavelengths
    with high multicollinearity. Creating multispectral models for real-time analysis
    of hundreds of influential bands is not feasible. We also did not consider the
    differences between the interval-based and individual-feature approaches. Creating
    a multispectral model from a small subset of features may benefit from informative
    intervals instead of informative narrow wavelengths because some spectral features
    are visible over several adjacent wavelengths. Machine learning Our review concluded
    that there are fewer regression or classification methods than wavelength selection
    techniques. The most common methods are based on Support Vector Machines (SVM)
    or Partial Least Squares (PLS). We did not compare the effectiveness of these
    machine learning algorithms. This review attempts to gauge their popularity in
    literature. Partial least squares, decision trees, and multiple linear regression
    methods are explainable approaches compared to neural network approaches but are
    limited in extracting features. Wavelength selection for hyperspectral imaging
    studies follows the process of first extracting features from sample images, applying
    feature selection, and applying a regression or classification algorithm. This
    process limits the search space of models and contradicts the integrated approach
    of deep learning, where a model learns to extract meaningful features and applies
    regression or classification. This review found no algorithm combining feature
    extraction, selection, and regression (or classification) steps. The only similar
    example we found was the two-branch CNN approach by Liu et al., which extracted
    spectral and spatial features independently [153]. Spatial features Our review
    found that GLCM and GLGCM were the most common methods for extracting spatial
    features. These handcrafted features require high levels of domain knowledge and
    may only be beneficial for certain problems. GLCM and GLGCM improved the accuracy
    of the learning algorithms in studies that incorporated them. Features learned
    by deep learning methods have dominated image classification of RGB images over
    the last decade [212]. Learning spatial features is complicated when considering
    the high number of channels in hyperspectral imaging (extensive fea- ture space),
    small sample sizes, and simultaneous challenge of selecting important wavelengths
    to enable online (real-time) applications. The number of spatial features available
    exacerbates the high dimensionality of wavelength selection. GLCM alone has 14
    standard texture descriptors per angle, distance, and wavelength. The feature
    space can quickly grow to extremely high dimensions with many combinations. Many
    studies have limited the feature image set to the selected wavelengths. Experimenting
    with enough features almost guarantees that at least one will correlate with the
    reference attributes in the training set, but this trend may not apply to the
    validation data. Very few studies have selected important spatial features independent
    of the spectral features. Huang et al. [188] showed that the spatial features
    can be plotted over the entire wavelength range to form a continuous curve, similar
    to the standard approach to mean reflectance. Other types of features also share
    a high correlation between adjacent wavelengths. Many studies have extracted spatial
    features from the Principal Component score images and spectral features using
    wavelength selection. However, generating a PC score image requires a complete
    set of wavelength bands. All wavelengths would be needed to extract spatial features
    making wavelength reduction redundant and creating a lower-cost multispectral
    system impossible. The most common approach is to pre-select wavelengths based
    on the spectral features. Most studies have revealed that spectral features are
    superior to spatial descriptors for the classification or prediction of attributes.
    Models incorporating both spectral and spatial data had higher predictive performance
    than individual features alone. The two main limitations of studies investigating
    spatial features are that they assume that wavelengths with the most meaningful
    spectral information also have the most meaningful spatial information. In most
    cases, the set of available spatial features is limited to a set selected by the
    authors. More work is needed to create flexible feature selection techniques and
    test them on larger sets of spatial features. Limitations Due to the restrictions
    of our inclusion criteria used in this review, such as only including studies
    with wavelength selection, some aspects of the studies investigated in this review
    may not represent all hyperspectral imaging research. We did not compare spatial
    feature extraction methods for hyperspectral imaging rather than wavelength selection.
    Studies may also have been missing from this review. They were either unavailable
    on Scopus, or the search string did not retrieve them. We mitigated this as much
    as possible by searching titles, keywords, and abstracts for multiple synonyms
    of “wavelength” and “selection”. However, we found some relevant studies that
    did not mention these keywords in their abstracts. Searching for an acronym for
    each wavelength selection technique is not feasible. Conclusion This review included
    799 hyperspectral imaging studies from Scopus from the 1229 studies collected.
    This review excluded studies that did not investigate hyperspectral imaging and
    wavelength selection or did not provide a transparent methodology and experimentation
    results. We analyzed these studies to understand the methods for feature selection,
    machine learning, and spectral-spatial feature fusion. Regarding the design of
    wavelength selection studies, this review found that hyper- spectral image analysis
    studies tended to have only a small number of samples, with the studies included
    in this review having a median sample size of 180. We investigated the wavelength
    ranges for studies that applied wavelength selection and found three commonly
    investigated regions of the electromagnetic spectrum: namely, 400–1000 nm, 900–1700
    nm, and 900–2500 nm. Many studies have applied an extensive range of wavelength
    selection algorithms. Although the comparative performance of the wavelength selection
    techniques is not an objective of most studies in hyperspectral imaging, we recommend
    that future work should apply a range of different techniques rather than just
    a single technique. We recommend the popular SPA, CARS, and genetic algorithms
    wavelength selection methods as benchmarks for future wavelength selection studies.
    Concatenated methods, such as CARS-SPA and UVE-CARS tend to provide features that
    lead to more accurate models. However, the number of studies that applied each
    of these techniques is not high enough to conclude whether they are better than
    other techniques. Any newly introduced wavelength selection method should be compared
    to a wide range of best-performing methods because there is no clear best selection
    method for all applications. The most common learning algorithms were partial
    least squares, support vector machines, and artificial neural networks. These
    wavelength selection and modelling steps were not integrated into the same solution
    because combining feature reduction with feature extraction and learning remains
    challenging. This review found that spectral features were more informative than
    spatial features in most studies employing spatial features, whereas combining
    both feature types increased predictive performance. Spectral and spatial features
    are typically extracted independently and fused using feature-level fusion. The
    most common method is to select wavelengths with important spectral information
    and extract spatial features from each selected wavelength. GLCM features were
    the most common texture descriptor combined with wavelength selection and were
    applied in more than half of the studies that considered spatial features. There
    is a need for more flexible feature selection and extraction methods, further
    investigation of spatial features, and publicly available large-scale hyperspectral
    image datasets. Flexibility can arise from flexible intervals of wavelengths or
    more flexible methods for extracting informative spatial features independent
    of spectral features. Change history 27 September 2023A Correction to this paper
    has been published: https://doi.org/10.1007/s11694-023-02148-4 Notes https://scholar.google.co.nz/.
    https://researchgate.net/. https://docs.google.com/forms. https://pandas.pydata.org/.
    https://github.com/WestHealth/pyvis. https://matplotlib.org/. References J. Ma,
    D.-W. Sun, H. Pu, J.-H. Cheng, Q. Wei, Advanced techniques for hyperspectral imaging
    in the food industry: principles and recent applications. Annu. Rev. Food Sci.
    Technol. 10(1), 197–220 (2019). https://doi.org/10.1146/annurev-food-032818-121155
    Article   CAS   PubMed   Google Scholar   L. Feng, B. Wu, S. Zhu, Y. He, C. Zhang,
    Application of visible/infrared spectroscopy and hyperspectral imaging with machine
    learning techniques for identifying food varieties and geographical origins. Front.
    Nutr. (2021). https://doi.org/10.3389/fnut.2021.680357 Article   PubMed   PubMed
    Central   Google Scholar   A.Y. Khaled, C.A. Parrish, A. Adedeji, Emerging nondestructive
    approaches for meat quality and safety evaluation–a review. Compr. Rev. Food Sci.
    Food Saf. 20(4), 3438–3463 (2021). https://doi.org/10.1111/1541-4337.12781 Article   PubMed   Google
    Scholar   L. Xu, X. Wang, H. Chen, B. Xin, Y. He, P. Huang, Predicting internal
    parameters of kiwifruit at different storage periods based on hyperspectral imaging
    technology. J. Food Meas. Charact. 16(5), 3910–3925 (2022). https://doi.org/10.1007/s11694-022-01477-0
    Article   Google Scholar   T. Lei, X.-H. Lin, D.-W. Sun, Rapid classification
    of commercial cheddar cheeses from different brands using PLSDA, LDA and SPA–LDA
    models built by hyperspectral data. J. Food Meas. Charact. 13(4), 3119–3129 (2019).
    https://doi.org/10.1007/s11694-019-00234-0 Article   Google Scholar   Y. He, Q.
    Xiao, X. Bai, L. Zhou, F. Liu, C. Zhang, Recent progress of nondestructive techniques
    for fruits damage inspection: a review. Crit. Rev. Food Sci. Nutr. 62(20), 1–19
    (2021). https://doi.org/10.1080/10408398.2021.1885342 Article   Google Scholar   M.
    Kamruzzaman, D.-W. Sun, Introduction to hyperspectral imaging technology, in Computer
    Vision Technology for Food Quality Evaluation, 2nd edn., ed. by D.W. Sun (Academic
    Press, London, 2016), pp.111–139 Chapter   Google Scholar   L. Zhou, C. Zhang,
    F. Liu, Z. Qiu, Y. He, Application of deep learning in food: a review. Compr.
    Rev. Food Sci. Food Saf. 18(6), 1793–1811 (2019). https://doi.org/10.1111/1541-4337.12492
    Article   PubMed   Google Scholar   J.-L. Li, D.-W. Sun, J.-H. Cheng, Recent advances
    in nondestructive analytical techniques for determining the total soluble solids
    in fruits: a review. Compr. Rev. Food Sci. Food Saf. 15(5), 897–911 (2016). https://doi.org/10.1111/1541-4337.12217
    Article   PubMed   Google Scholar   Q. Dai, D.-W. Sun, Z. Xiong, J.-H. Cheng,
    X.-A. Zeng, Recent advances in data mining techniques and their applications in
    hyperspectral image processing for the food industry. Compr. Rev. Food Sci. Food
    Saf. 13(5), 891–905 (2014). https://doi.org/10.1111/1541-4337.12088 Article   Google
    Scholar   J. Zhang, T. Cheng, W. Guo, X. Xu, H. Qiao, Y. Xie, X. Ma, Leaf area
    index estimation model for UAV image hyperspectral data based on wavelength variable
    selection and machine learning methods. Plant Methods (2021). https://doi.org/10.1186/s13007-021-00750-5
    Article   PubMed   PubMed Central   Google Scholar   J.-H. Cheng, B. Nicolai,
    D.-W. Sun, Hyperspectral imaging with multivariate analysis for technological
    parameters prediction and classification of muscle foods: a review. Meat Sci.
    123, 182–191 (2017). https://doi.org/10.1016/j.meatsci.2016.09.017 Article   PubMed   Google
    Scholar   G. Lu, D. Wang, X. Qin, L. Halig, S. Muller, H. Zhang, A. Chen, B.W.
    Pogue, Z.G. Chen, B. Fei, Framework for hyperspectral image processing and quantification
    for cancer detection during animal tumor surgery. J. Biomed. Opt. 20(12), 126012–126012
    (2015). https://doi.org/10.1117/1.JBO.20.12.126012 Article   PubMed   PubMed Central   Google
    Scholar   G. Özdoğan, X. Lin, D.-W. Sun, Rapid and noninvasive sensory analyses
    of food products by hyperspectral imaging: Recent application developments. Trends
    Food Sci. Technol. 111(2), 151–165 (2021). https://doi.org/10.1016/j.tifs.2021.02.044
    Article   CAS   Google Scholar   G. Hughes, On the mean accuracy of statistical
    pattern recognizers. IEEE Trans. Inf. Theory 14(1), 55–63 (1968). https://doi.org/10.1109/TIT.1968.1054102
    Article   Google Scholar   D. Wu, D.-W. Sun, Advanced applications of hyperspectral
    imaging technology for food quality and safety analysis and assessment: a review—part
    I: fundamentals. Innov. Food Sci. Emerg. Technol. 19, 1–14 (2013). https://doi.org/10.1016/j.ifset.2013.04.014
    Article   CAS   Google Scholar   X. Lin, J.-L. Xu, D.-W. Sun, Evaluating drying
    feature differences between ginger slices and splits during microwave-vacuum drying
    by hyperspectral imaging technique. Food Chem. (2020). https://doi.org/10.1016/j.foodchem.2020.127407
    Article   PubMed   Google Scholar   J.-H. Cheng, J.H. Qu, D.-W. Sun, X.A. Zeng,
    Visible/near-infrared hyperspectral imaging prediction of textural firmness of
    grass carp (Ctenopharyngodon idella) as affected by frozen storage. Food Res.
    Int. 56, 190–198 (2014). https://doi.org/10.1016/j.foodres.2013.12.009 Article   Google
    Scholar   A. Hennessy, K. Clarke, M. Lewis, Hyperspectral classification of plants:
    a review of waveband selection generalisability. Remote Sens. (2020). https://doi.org/10.3390/RS12010113
    Article   Google Scholar   H. Pu, M. Kamruzzaman, D.-W. Sun, Selection of feature
    wavelengths for developing multispectral imaging systems for quality, safety and
    authenticity of muscle foods-a review. Trends Food Sci. Technol. 45(1), 86–104
    (2015). https://doi.org/10.1016/j.tifs.2015.05.006 Article   CAS   Google Scholar   T.-T.
    Pan, D.-W. Sun, J.-H. Cheng, H. Pu, Regression algorithms in hyperspectral data
    analysis for meat quality detection and evaluation. Compr. Rev. Food Sci. Food
    Saf. 15(3), 529–541 (2016). https://doi.org/10.1111/1541-4337.12191 Article   PubMed   Google
    Scholar   J.-H. Cheng, D.-W. Sun, Hyperspectral imaging as an effective tool for
    quality analysis and control of fish and other seafoods: current research and
    potential applications. Trends Food Sci. Technol. 37(2), 78–91 (2014). https://doi.org/10.1016/j.tifs.2014.03.006
    Article   CAS   Google Scholar   S. Ghidini, M.O. Varrà, E. Zanardi, Approaching
    authenticity issues in fish and seafood products by qualitative spectroscopy and
    chemometrics. Molecules 24(9), 1812 (2019) Article   CAS   PubMed   PubMed Central   Google
    Scholar   H. Wang, J. Peng, C. Xie, Y. Bao, Y. He, Fruit quality evaluation using
    spectroscopy technology: a review. Sensors (Switzerland) 15(5), 11889–11927 (2015).
    https://doi.org/10.3390/s150511889 Article   Google Scholar   W.-H. Su, D.-W.
    Sun, Multispectral imaging for plant food quality analysis and visualization.
    Compr. Rev. Food Sci. Food Saf. 17(1), 220–239 (2018). https://doi.org/10.1111/1541-4337.12317
    Article   PubMed   Google Scholar   D. Saha, A. Manickavasagan, Machine learning
    techniques for analysis of hyperspectral images to determine quality of food products:
    a review. Current Res. Food Sci. 4, 28–44 (2021). https://doi.org/10.1016/j.crfs.2021.01.002
    Article   CAS   Google Scholar   K. Wang, H. Pu, D.-W. Sun, Emerging spectroscopic
    and spectral imaging techniques for the rapid detection of microorganisms: an
    overview. Compr. Rev. Food Sci. Food Saf. 17(2), 256–273 (2018). https://doi.org/10.1111/1541-4337.12323
    Article   PubMed   Google Scholar   D. Liu, D.-W. Sun, X.-A. Zeng, Recent advances
    in wavelength selection techniques for hyperspectral image processing in the food
    industry. Food Bioprocess Technol. 7(2), 307–323 (2014). https://doi.org/10.1007/s11947-013-1193-6
    Article   Google Scholar   T. Mehmood, K.H. Liland, L. Snipen, S. Sæbø, A review
    of variable selection methods in partial least squares regression. Chemom. Intell.
    Lab. Syst. 118, 62–69 (2012). https://doi.org/10.1016/j.chemolab.2012.07.010 Article   CAS   Google
    Scholar   J.-H. Cheng, D.-W. Sun, Data fusion and hyperspectral imaging in tandem
    with least squares-support vector machine for prediction of sensory quality index
    scores of fish fillet. LWT 63(2), 892–898 (2015). https://doi.org/10.1016/j.lwt.2015.04.039
    Article   CAS   Google Scholar   M.J. Page, D. Moher, P.M. Bossuyt, I. Boutron,
    T.C. Hoffmann, C.D. Mulrow, L. Shamseer, J.M. Tetzlaff, E.A. Akl, S.E. Brennan,
    R. Chou, J. Glanville, J.M. Grimshaw, A. Hróbjartsson, M.M. Lalu, T. Li, E.W.
    Loder, E. Mayo-Wilson, S. McDonald, L.A. McGuinness, L.A. Stewart, J. Thomas,
    A.C. Tricco, V.A. Welch, P. Whiting, J.E. McKenzie, Prisma 2020 explanation and
    elaboration: updated guidance and exemplars for reporting systematic reviews.
    BMJ (2021). https://doi.org/10.1136/bmj.n160 Article   PubMed   PubMed Central   Google
    Scholar   M.E. Falagas, E.I. Pitsouni, G.A. Malietzis, G. Pappas, Comparison of
    pubmed, scopus, web of science, and google scholar: strengths and weaknesses.
    FASEB J. 22(2), 338–342 (2008). https://doi.org/10.1096/fj.07-9492LSF Article   CAS   PubMed   Google
    Scholar   R. Tibshirani, Regression shrinkage and selection via the lasso. J.
    R. Stat. Soc. 58(1), 267–288 (1996) Google Scholar   M.C.U. Araújo, T.C.B. Saldanha,
    R.K.H. Galvão, T. Yoneyama, H.C. Chame, V. Visani, The successive projections
    algorithm for variable selection in spectroscopic multicomponent analysis. Chemom.
    Intell. Lab. Syst. 57(2), 65–73 (2001). https://doi.org/10.1016/S0169-7439(01)00119-8
    Article   Google Scholar   H. Li, Y. Liang, Q. Xu, D. Cao, Key wavelengths screening
    using competitive adaptive reweighted sampling method for multivariate calibration.
    Anal. Chim. Acta 648(1), 77–84 (2009). https://doi.org/10.1016/j.aca.2009.06.046
    Article   CAS   PubMed   Google Scholar   H. Jiang, S.-C. Yoon, H. Zhuang, W.
    Wang, K.C. Lawrence, Y. Yang, Tenderness classification of fresh broiler breast
    fillets using visible and near-infrared hyperspectral imaging. Meat Sci. 139,
    82–90 (2018). https://doi.org/10.1016/j.meatsci.2018.01.013 Article   PubMed   Google
    Scholar   Q. Li, F.K. Kit Wong, T. Fung, Comparison feature selection methods
    for subtropical vegetation classification with hyperspectral data. International
    Geoscience and Remote Sensing Symposium (IGARSS) 3693–3696 (2019). https://doi.org/10.1109/IGARSS.2019.8898541
    J. Tschannerl, J. Ren, J. Zabalza, S. Marshall, Segmented autoencoders for unsupervised
    embedded hyperspectral band selection. Proceedings - European Workshop on Visual
    Information Processing, EUVIP 2018-November (2019). https://doi.org/10.1109/EUVIP.2018.8611643
    J. Zhang, L. Dai, F. Cheng, Classification of frozen corn seeds using hyperspectral
    Vis/NIR reflectance imaging. Molecules (2019). https://doi.org/10.3390/molecules24010149
    Article   PubMed   PubMed Central   Google Scholar   Y. Zhao, S. Zhu, C. Zhang,
    X. Feng, L. Feng, Y. He, Application of hyperspectral imaging and chemometrics
    for variety classification of maize seeds. RSC Adv. 8(3), 1337–1345 (2018). https://doi.org/10.1039/c7ra05954j
    Article   CAS   PubMed   PubMed Central   Google Scholar   S. Zhu, L. Zhou, P.
    Gao, Y. Bao, Y. He, L. Feng, Near-infrared hyperspectral imaging combined with
    deep learning to identify cotton seed varieties. Molecules (2019). https://doi.org/10.3390/molecules24183268
    Article   PubMed   PubMed Central   Google Scholar   N. Wu, Y. Zhang, R. Na, C.
    Mi, S. Zhu, Y. He, C. Zhang, Variety identification of oat seeds using hyperspectral
    imaging: Investigating the representation ability of deep convolutional neural
    network. RSC Adv. 9(22), 12635–12644 (2019). https://doi.org/10.1039/c8ra10335f
    Article   CAS   PubMed   PubMed Central   Google Scholar   Z. Gao, L.R. Khot,
    R.A. Naidu, Q. Zhang, Early detection of grapevine leafroll disease in a red-berried
    wine grape cultivar using hyperspectral imaging. Comput. Electron. Agric. (2020).
    https://doi.org/10.1016/j.compag.2020.105807 Article   Google Scholar   S.R. Delwiche,
    I.T. Rodriguez, S.R. Rausch, R.A. Graybosch, Estimating percentages of fusarium-damaged
    kernels in hard wheat by near-infrared hyperspectral imaging. J. Cereal Sci. 87,
    18–24 (2019). https://doi.org/10.1016/j.jcs.2019.02.008 Article   Google Scholar   B.-H.
    Zhang, W.-Q. Huang, J.-B. Li, C.-J. Zhao, C.-L. Liu, D.-F. Huang, L. Gong, Detection
    of slight bruises on apples based on hyperspectral imaging and MNF transform.
    Spectrosc. Spectral Anal. 34(5), 1367–1372 (2014). https://doi.org/10.3964/j.issn.1000-0593(2014)05-1367-06
    Article   CAS   Google Scholar   D.F. Barbin, D.-W. Sun, C. Su, NIR hyperspectral
    imaging as non-destructive evaluation tool for the recognition of fresh and frozen-thawed
    porcine longissimus dorsi muscles. Innov. Food Sci. Emerg. Technol. 18, 226–236
    (2013). https://doi.org/10.1016/j.ifset.2012.12.011 Article   CAS   Google Scholar   C.Q.
    Xie, X.L. Li, P.C. Nie, Y. He, Application of time series hyperspectral imaging
    (TS-HSI) for determining water content within tea leaves during drying. Trans.
    ASABE 56(6), 1431–1440 (2013). https://doi.org/10.13031/trans.56.10243 Article   Google
    Scholar   J. Long, J. Yang, J. Peng, L. Pan, K. Tu, Detection of moisture and
    carotenoid content in carrot slices during hot air drying based on multispectral
    imaging equipment with selected wavelengths. Int. J. Food Eng. 17(9), 727–735
    (2021). https://doi.org/10.1515/ijfe-2021-0127 Article   CAS   Google Scholar   A.
    Iqbal, D.-W. Sun, P. Allen, Prediction of moisture, color and pH in cooked, pre-sliced
    turkey hams by NIR hyperspectral imaging system. J. Food Eng. 117(1), 42–51 (2013).
    https://doi.org/10.1016/j.jfoodeng.2013.02.001 Article   CAS   Google Scholar   S.
    Wang, A.K. Das, J. Pang, P. Liang, Artificial intelligence empowered multispectral
    vision based system for non-contact monitoring of large yellow croaker (Larimichthys
    crocea) fillets. Foods (2021). https://doi.org/10.3390/foods10061161 Article   PubMed   PubMed
    Central   Google Scholar   H. Jiang, S.-C. Yoon, H. Zhuang, W. Wang, Y. Li, Y.
    Yang, Integration of spectral and textural features of visible and near-infrared
    hyperspectral imaging for differentiating between normal and white striping broiler
    breast meat. Spectrochim. Acta Part A 213, 118–126 (2019). https://doi.org/10.1016/j.saa.2019.01.052
    Article   CAS   Google Scholar   H. Wang, R. Hu, M. Zhang, Z. Zhai, R. Zhang,
    Identification of tomatoes with early decay using visible and near infrared hyperspectral
    imaging and image-spectrum merging technique. J. Food Process Eng. (2021). https://doi.org/10.1111/jfpe.13654
    Article   Google Scholar   W. Tan, L. Sun, F. Yang, W. Che, D. Ye, D. Zhang, B.
    Zou, The feasibility of early detection and grading of apple bruises using hyperspectral
    imaging: Early detection and grading of apple bruises. J. Chemom. 32(10), 3067
    (2018). https://doi.org/10.1002/cem.3067 Article   CAS   Google Scholar   C.-H.
    Feng, Y. Makino, M. Yoshimura, F.J. Rodríguez-Pulido, Real-time prediction of
    pre-cooked japanese sausage color with different storage days using hyperspectral
    imaging. J. Sci. Food Agric. 98(7), 2564–2572 (2018). https://doi.org/10.1002/jsfa.8746
    Article   CAS   PubMed   Google Scholar   D.E. Goldberg, J.H. Holland, Genetic
    algorithms and machine learning. Mach. Learn. 3(2), 95–99 (1988). https://doi.org/10.1023/A:1022602019183
    Article   Google Scholar   H.-J. He, Y. Chen, G. Li, Y. Wang, X. Ou, J. Guo, Hyperspectral
    imaging combined with chemometrics for rapid detection of talcum powder adulterated
    in wheat flour. Food Control (2023). https://doi.org/10.1016/j.foodcont.2022.109378
    Article   Google Scholar   R. Qiu, Y. Zhao, D. Kong, N. Wu, Y. He, Development
    and comparison of classification models on Vis-NIR hyperspectral imaging spectra
    for qualitative detection of the Staphylococcus aureus in fresh chicken breast.
    Spectrochim. Acta A285, 121838 (2023) Article   Google Scholar   X. Li, M. Cai,
    M. Li, X. Wei, Z. Liu, J. Wang, K. Jia, Y. Han, Combining Vis-NIR and NIR hyperspectral
    imaging techniques with a data fusion strategy for the rapid qualitative evaluation
    of multiple qualities in chicken. Food Control (2023). https://doi.org/10.1016/j.foodcont.2022.109416
    Article   PubMed   Google Scholar   Y. Li, Y. Yin, H. Yu, Y. Yuan, Fast detection
    of water loss and hardness for cucumber using hyperspectral imaging technology.
    J. Food Meas. Charact. 16(1), 76–84 (2022). https://doi.org/10.1007/s11694-021-01130-2
    Article   Google Scholar   S. Wold, M. Sjöström, L. Eriksson, PLS-regression:
    a basic tool of chemometrics. Chemom. Intell. Lab. Syst. 58, 109–130 (2001) Article   CAS   Google
    Scholar   Y. Wang, H. He, S. Jiang et al., Nondestructive determination of IMP
    content in chilled chicken based on hyperspectral data combined with chemometrics.
    Int. J. Agric. Biol. Eng. 15(1), 277–284 (2022). https://doi.org/10.25165/j.ijabe.20221501.6612
    Article   Google Scholar   Z. Yuan, Y. Ye, L. Wei, X. Yang, C. Huang, Study on
    the optimization of hyperspectral characteristic bands combined with monitoring
    and visualization of pepper leaf spad value. Sensors (2022). https://doi.org/10.3390/s22010183
    Article   PubMed   PubMed Central   Google Scholar   Q. Thien Pham, N.-S. Liou,
    The development of on-line surface defect detection system for jujubes based on
    hyperspectral images. Comput. Electron. Agric. (2022). https://doi.org/10.1016/j.compag.2022.106743
    Article   Google Scholar   M. Shiddiq, H. Herman, D.S. Arief, E. Fitra, I.R. Husein,
    S.A. Ningsih, Wavelength selection of multispectral imaging for oil palm fresh
    fruit ripeness classification. Appl. Opt. 61(17), 5289–5298 (2022). https://doi.org/10.1364/AO.450384
    Article   PubMed   Google Scholar   B. Li, F. Zhang, Y. Liu, H. Yin, J. Zou, A.
    Ou-yang, Quantitative study on impact damage of yellow peach based on hyperspectral
    image information combined with spectral information. J. Mol. Struct. (2023).
    https://doi.org/10.1016/j.molstruc.2022.134176 Article   Google Scholar   S. Sharma,
    K.C. Sumesh, P. Sirisomboon, Rapid ripening stage classification and dry matter
    prediction of durian pulp using a pushbroom near infrared hyperspectral imaging
    system. Meas. J. Int. Meas. Confeder. (2022). https://doi.org/10.1016/j.measurement.2021.110464
    Article   Google Scholar   V. Centner, D.-L. Massart, O.E. Noord, S. Jong, B.M.
    Vandeginste, C. Sterna, Elimination of uninformative variables for multivariate
    calibration. Anal. Chem. 68(21), 3851–3858 (1996). https://doi.org/10.1021/ac960321m
    Article   CAS   PubMed   Google Scholar   B. Li, F. Zhang, Y. Liu, H. Yin, J.
    Zou, A. Ou-Yang, Quantitative study of impact damage on yellow peaches based on
    reflectance, absorbance and kubelka-munk spectral data. RSC Adv. 12(43), 28152–28170
    (2022). https://doi.org/10.1039/d2ra04635k Article   CAS   PubMed   PubMed Central   Google
    Scholar   P. Xu, Y. Zhang, Q. Tan, K. Xu, W. Sun, J. Xing, R. Yang, Vigor identification
    of maize seeds by using hyperspectral imaging combined with multivariate data
    analysis. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104361
    Article   PubMed   PubMed Central   Google Scholar   H.-D. Li, Q.-S. Xu, Y.-Z.
    Liang, Random frog: an efficient reversible jump Markov chain Monte Carlo-like
    approach for variable selection with applications to gene selection and disease
    classification. Anal. Chim. Acta 740, 20–26 (2012). https://doi.org/10.1016/j.aca.2012.06.031
    Article   CAS   PubMed   Google Scholar   C. Liu, Z. Chu, S. Weng, G. Zhu, K.
    Han, Z. Zhang, L. Huang, Z. Zhu, S. Zheng, Fusion of electronic nose and hyperspectral
    imaging for mutton freshness detection using input-modified convolution neural
    network. Food Chem. (2022). https://doi.org/10.1016/j.foodchem.2022.132651 Article   PubMed   Google
    Scholar   T. Cheng, P. Li, J. Ma, X. Tian, N. Zhong, Identification of four chicken
    breeds by hyperspectral imaging combined with chemometrics. Processes (2022).
    https://doi.org/10.3390/pr10081484 Article   Google Scholar   G. Xuan, C. Gao,
    Y. Shao, Spectral and image analysis of hyperspectral data for internal and external
    quality assessment of peach fruit. Spectrochim. Acta Part A (2022). https://doi.org/10.1016/j.saa.2022.121016
    Article   Google Scholar   M. Kamruzzaman, D. Kalita, M.T. Ahmed, G. ElMasry,
    Y. Makino, Effect of variable selection algorithms on model performance for predicting
    moisture content in biological materials using spectral data. Anal. Chim.Acta
    (2022). https://doi.org/10.1016/j.aca.2021.339390 Article   PubMed   Google Scholar   H.
    Song, S.-R. Yoon, Y.-M. Dang, J.-S. Yang, I.M. Hwang, J.-H. Ha, Nondestructive
    classification of soft rot disease in napa cabbage using hyperspectral imaging
    analysis. Sci. Rep. (2022). https://doi.org/10.1038/s41598-022-19169-6 Article   PubMed   PubMed
    Central   Google Scholar   RMd. Saleh, B. Kulig, A. Arefi, O. Hensel, B. Sturm,
    Prediction of total carotenoids, color, and moisture content of carrot slices
    during hot air drying using non-invasive hyperspectral imaging technique. J. Food
    Process. Preserv. (2022). https://doi.org/10.1111/jfpp.16460 Article   Google
    Scholar   R. Yuan, G. Liu, J. He, G. Wan, N. Fan, Y. Li, Y. Sun, Classification
    of lingwu long jujube internal bruise over time based on visible near-infrared
    hyperspectral imaging combined with partial least squares-discriminant analysis.
    Comput. Electron. Agric. (2021). https://doi.org/10.1016/j.compag.2021.106043
    Article   Google Scholar   M. Gabrielli, V. LançSon-Verdier, P. Picouet, C. Maury,
    Hyperspectral imaging to characterize table grapes. Chemosensors (2021). https://doi.org/10.3390/chemosensors9040071
    Article   Google Scholar   T. Wu, J. Yu, J. Lu, X. Zou, W. Zhang, Research on
    inversion model of cultivated soil moisture content based on hyperspectral imaging
    analysis. Agriculture (Switzerland) 10(7), 1–14 (2020). https://doi.org/10.3390/agriculture10070292
    Article   Google Scholar   Y.-H. Yun, W.-T. Wang, B.-C. Deng, G.-B. Lai, X.-B.
    Liu, D.-B. Ren, Y.-Z. Liang, W. Fan, Q.-S. Xu, Using variable combination population
    analysis for variable selection in multivariate calibration. Anal. Chim. Acta
    862, 14–23 (2015). https://doi.org/10.1016/j.aca.2014.12.048 Article   CAS   PubMed   Google
    Scholar   L. Shi, L. Li, F. Zhang, Y. Lin, Nondestructive detection of panax notoginseng
    saponins by using hyperspectral imaging. Int. J. Food Sci. Technol. 57(7), 4537–4546
    (2022). https://doi.org/10.1111/ijfs.15790 Article   CAS   Google Scholar   Z.
    Guo, J. Zhang, C. Ma, X. Yin, Y. Guo, X. Sun, C. Jin, Application of visible-near-infrared
    hyperspectral imaging technology coupled with wavelength selection algorithm for
    rapid determination of moisture content of soybean seeds. J. Food Compos. Anal.
    (2023). https://doi.org/10.1016/j.jfca.2022.105048 Article   Google Scholar   P.
    Zhang, H. Ji, H. Wang, Y. Liu, X. Zhang, C. Ren, Quantitative evaluation of impact
    damage to apples using NIR hyperspectral imaging. Int. J. Food Prop. 24(1), 457–470
    (2021). https://doi.org/10.1080/10942912.2021.1900240 Article   CAS   Google Scholar   J.
    Onmankhong, T. Ma, T. Inagaki, P. Sirisomboon, S. Tsuchikawa, Cognitive spectroscopy
    for the classification of rice varieties: a comparison of machine learning and
    deep learning approaches in analysing long-wave near-infrared hyperspectral images
    of brown and milled samples. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104100
    Article   Google Scholar   J. Wang, L. Yan, F. Wang, S. Qi, SVM classification
    method of waxy corn seeds with different vitality levels based on hyperspectral
    imaging. J. Sens. (2022). https://doi.org/10.1155/2022/4379317 Article   Google
    Scholar   Y.-H. Yun, W.-T. Wang, M.-L. Tan, Y.-Z. Liang, H.-D. Li, D.-S. Cao,
    H.-M. Lu, Q.-S. Xu, A strategy that iteratively retains informative variables
    for selecting optimal variable subset in multivariate calibration. Anal. Chim.
    Acta 807, 36–43 (2014). https://doi.org/10.1016/j.aca.2013.11.032 Article   CAS   PubMed   Google
    Scholar   D. Saha, T. Senthilkumar, S. Sharma, C.B. Singh, A. Manickavasagan,
    Application of near-infrared hyperspectral imaging coupled with chemometrics for
    rapid and non-destructive prediction of protein content in single chickpea seed.
    J. Food Compos. Anal. (2023). https://doi.org/10.1016/j.jfca.2022.104938 Article   Google
    Scholar   Z. Sun, H. Pan, M. Zuo, J. Li, L. Liang, C.-T. Ho, X. Zou, Non-destructive
    assessment of equivalent umami concentrations in salmon using hyperspectral imaging
    technology combined with multivariate algorithms. Spectrochim. Acta Part A (2023).
    https://doi.org/10.1016/j.saa.2022.121890 Article   Google Scholar   L. Nørregard,
    A. Saudland, J. Wagner, J.P. Nielsen, L. Munck, S.B. Engelsen, Interval partial
    least-squares regression (iPLS): a comparative chemometric study with an example
    from near-infrared spectroscopy. Appl. Spectrosc. 54(3), 413–419 (2000). https://doi.org/10.1366/0003702001949500
    Article   Google Scholar   J. Florián-Huamán, J.P. Cruz-Tirado, D. FernandesBarbin,
    R. Siche, Detection of nutshells in cumin powder using NIR hyperspectral imaging
    and chemometrics tools. J. Food Compos. Anal. (2022). https://doi.org/10.1016/j.jfca.2022.104407
    Article   Google Scholar   A. López-Maestresalas, C. Lopez-Molina, G.A. Oliva-Lobo,
    C. Jarén, J.I. Galarreta, C.M. Peraza-Alemán, S. Arazuri, Evaluation of near-infrared
    hyperspectral imaging for the assessment of potato processing aptitude. Front.
    Nutr. (2022). https://doi.org/10.3389/fnut.2022.999877 Article   PubMed   PubMed
    Central   Google Scholar   G. Kim, H. Lee, I. Baek, B.-K. Cho, M.S. Kim, Quantitative
    detection of benzoyl peroxide in wheat flour using line-scan short-wave infrared
    hyperspectral imaging. Sens. Actuators B (2022). https://doi.org/10.1016/j.snb.2021.130997
    Article   Google Scholar   B. Wang, J. He, S. Zhang, L. Li, Nondestructive prediction
    and visualization of total flavonoids content in Cerasus humilis fruit during
    storage periods based on hyperspectral imaging technique. J. Food Process. Eng.
    (2021). https://doi.org/10.1111/jfpe.13807 Article   Google Scholar   M.M.A. Chaudhry,
    M.L. Amodio, J.M. Amigo, M.L.V. Chiara, F. Babellahi, G. Colelli, Feasibility
    study for the surface prediction and mapping of phytonutrients in minimally processed
    rocket leaves (Diplotaxis tenuifolia) during storage by hyperspectral imaging.
    Comput. Electron. Agric. (2020). https://doi.org/10.1016/j.compag.2020.105575
    Article   Google Scholar   X. Ye, S. Abe, S. Zhang, Estimation and mapping of
    nitrogen content in apple trees at leaf and canopy levels using hyperspectral
    imaging. Precision Agric. 21(1), 198–225 (2020). https://doi.org/10.1007/s11119-019-09661-x
    Article   Google Scholar   K. Song, S.-H. Wang, D. Yang, T.-Y. Shi, Combination
    of spectral and image information from hyperspectral imaging for the prediction
    and visualization of the total volatile basic nitrogen content in cooked beef.
    J. Food Meas. Charact. 15(5), 4006–4020 (2021). https://doi.org/10.1007/s11694-021-00983-x
    Article   Google Scholar   H. Jiang, Y. Hu, X. Jiang, H. Zhou, Maturity stage
    discrimination of Camellia oleifera fruit using visible and near-infrared hyperspectral
    imaging. Molecules (2022). https://doi.org/10.3390/molecules27196318 Article   PubMed   PubMed
    Central   Google Scholar   D. Fu, Q. Wang, M. Ma, Nondestructive detection of
    egg freshness fusion index during storage based on hyperspectral imaging. ACM
    Int. Conf. Proc. Ser. (2020). https://doi.org/10.1145/3453187.3453379 Article   Google
    Scholar   X. Zheng, Y. Li, W. Wei, Y. Peng, Detection of adulteration with duck
    meat in minced lamb meat by using visible near-infrared hyperspectral imaging.
    Meat Sci. 149, 55–62 (2019). https://doi.org/10.1016/j.meatsci.2018.11.005 Article   CAS   PubMed   Google
    Scholar   A.M. Rady, D.E. Guyer, I.R. Donis-González, W. Kirk, N.J. Watson, A
    comparison of different optical instruments and machine learning techniques to
    identify sprouting activity in potatoes during storage. J. Food Meas. Charact.
    14(6), 3565–3579 (2020). https://doi.org/10.1007/s11694-020-00590-2 Article   Google
    Scholar   N. Ekramirad, A.Y. Khaled, L.E. Doyle, J.R. Loeb, K.D. Donohue, R.T.
    Villanueva, A.A. Adedeji, Nondestructive detection of codling moth infestation
    in apples using pixel-based NIR hyperspectral imaging with machine learning and
    feature selection. Foods (2022). https://doi.org/10.3390/foods11010008 Article   Google
    Scholar   I. Baek, C. Mo, C. Eggleton, S.A. Gadsden, B.-K. Cho, J. Qin, D.E. Chan,
    M.S. Kim, Determination of spectral resolutions for multispectral detection of
    apple bruises using visible/near-infrared hyperspectral reflectance imaging. Front.
    Plant Sci. (2022). https://doi.org/10.3389/fpls.2022.963591 Article   PubMed   PubMed
    Central   Google Scholar   B.-C. Deng, Y.-H. Yun, P. Ma, C.-C. Lin, D.-B. Ren,
    Y.-Z. Liang, A new method for wavelength interval selection that intelligently
    optimizes the locations, widths and combinations of the intervals. Analyst 140,
    1876–1885 (2015). https://doi.org/10.1039/C4AN02123A Article   CAS   PubMed   Google
    Scholar   J. Hao, F. Dong, Y. Li, S. Wang, J. Cui, Z. Zhang, K. Wu, Investigation
    of the data fusion of spectral and textural data from hyperspectral imaging for
    the near geographical origin discrimination of wolfberries using 2D-CNN algorithms.
    Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104286
    Article   Google Scholar   Y.H. Yun, H.D. Li, L.R. Wood, W. Fan, J.J. Wang, D.S.
    Cao, Q.S. Xu, Y.Z. Liang, An efficient method of wavelength interval selection
    based on random frog for multivariate spectral calibration. Spectrochim. Acta
    Part A. 111, 31–6 (2013). https://doi.org/10.1016/j.saa.2013.03.083 Article   CAS   Google
    Scholar   I. Noda, Recent advancement in the field of two-dimensional correlation
    spectroscopy. J. Mol. Struct. (2008). https://doi.org/10.1016/j.molstruc.2007.11.038
    Article   Google Scholar   H. Jiang, X. Jiang, Y. Ru, Q. Chen, J. Wang, L. Xu,
    H. Zhou, Detection and visualization of soybean protein powder in ground beef
    using visible and near-infrared hyperspectral imaging. Infrared Phys. Technol.
    (2022). https://doi.org/10.1016/j.infrared.2022.104401 Article   Google Scholar   H.
    Jiang, X. Jiang, Y. Ru, Q. Chen, X. Li, L. Xu, H. Zhou, M. Shi, Rapid and non-destructive
    detection of natural mildew degree of postharvest camellia oleifera fruit based
    on hyperspectral imaging. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104169
    Article   Google Scholar   W. Cai, Y. Li, X. Shao, A variable selection method
    based on uninformative variable elimination for multivariate calibration of near-infrared
    spectra. Chemom. Intell. Lab. Syst. 90(2), 188–194 (2008). https://doi.org/10.1016/j.chemolab.2007.10.001
    Article   CAS   Google Scholar   B. Sturm, S. Raut, B. Kulig, J. Münsterer, K.
    Kammhuber, O. Hensel, S.O.J. Crichton, In-process investigation of the dynamics
    in drying behavior and quality development of hops using visual and environmental
    sensors combined with chemometrics. Comput. Electron. Agric. 175, 96 (2020). https://doi.org/10.1016/j.compag.2020.105547
    Article   Google Scholar   X. Wei, J. He, S. Zheng, D. Ye, Modeling for SSC and
    firmness detection of persimmon based on NIR hyperspectral imaging by sample partitioning
    and variables selection. Infrared Phys. Technol. (2020). https://doi.org/10.1016/j.infrared.2019.103099
    Article   Google Scholar   B. Li, Z. Han, Q. Wang, A. Yang, Y. Liu, Detection
    of skin defects in loquats based on grayscale features combined with reflectance,
    absorbance, and kubelka-munk spectra. J. Chemometr. (2022). https://doi.org/10.1002/cem.3449
    Article   Google Scholar   B.-C. Deng, Y.-H. Yun, Y.-Z. Liang, L.-z Yi, A novel
    variable selection approach that iteratively optimizes variable space using weighted
    binary matrix sampling. Analyst 139, 4836–4845 (2014). https://doi.org/10.1039/C4AN00730A
    Article   CAS   PubMed   Google Scholar   L. Fu, J. Sun, S. Wang, M. Xu, K. Yao,
    X. Zhou, Nondestructive evaluation of Zn content in rape leaves using MSSAE and
    hyperspectral imaging. Spectrochim. Acta Part A (2022). https://doi.org/10.1016/j.saa.2022.121641
    Article   Google Scholar   Y. Wang, Y. Zhang, Y. Yuan, Y. Zhao, J. Nie, T. Nan,
    L. Huang, J. Yang, Nutrient content prediction and geographical origin identification
    of red raspberry fruits by combining hyperspectral imaging with chemometrics.
    Front. Nutr. (2022). https://doi.org/10.3389/fnut.2022.980095 Article   PubMed   PubMed
    Central   Google Scholar   T. An, S. Yu, W. Huang, G. Li, X. Tian, S. Fan, C.
    Dong, C. Zhao, Robustness and accuracy evaluation of moisture prediction model
    for black tea withering process using hyperspectral imaging. Spectrochim. Acta
    Part A (2022). https://doi.org/10.1016/j.saa.2021.120791 Article   Google Scholar   C.X.
    Garzon-Lopez, E. Lasso, Species classification in a tropical alpine ecosystem
    using UAV-borne RGB and hyperspectral imagery. Drones 4(4), 1–18 (2020). https://doi.org/10.3390/drones4040069
    Article   Google Scholar   S. Chang, U. Lee, J.-B. Kim, Y.D. Jo, Application of
    3D-volumetric analysis and hyperspectral imaging systems for investigation of
    heterosis and cytoplasmic effects in pepper. Sci. Horticult. (2022). https://doi.org/10.1016/j.scienta.2022.111150
    Article   Google Scholar   I. Guyon, J. Weston, S. Barnhill, V. Vapnik, Gene selection
    for cancer classification using support vector machines. Mach. Learn. 46(1), 389–422
    (2002). https://doi.org/10.1023/A:1012487302797 Article   Google Scholar   A.
    Viinikka, P. Hurskainen, S. Keski-Saari, S. Kivinen, T. Tanhuanpää, J. Mäyrä,
    L. Poikolainen, P. Vihervaara, T. Kumpula, Detecting European aspen (Populus tremula,
    L.) in boreal forests using airborne hyperspectral and airborne laser scanning
    data. Remote Sens. (2020). https://doi.org/10.3390/RS12162610 Article   Google
    Scholar   J. Mohite, S. Sawant, R. Agarwal, A. Pandit, S. Pappula, Detection of
    crop water stress in maize using drone based hyperspectral imaging. Int. Geosci.
    Remote Sens. Symp. (IGARSS) 2022–July, 5957–5960 (2022). https://doi.org/10.1109/IGARSS46834.2022.9884686
    Article   Google Scholar   A.U.G. Sankararao, P. Rajalakshmi, S. Kaliamoorthy,
    S. Choudhary, Water stress detection in pearl millet canopy with selected wavebands
    using UAV based hyperspectral imaging and machine learning. 2022 IEEE Sensors
    Applications Symposium (SAS), Sundsvall, Sweden, 1–6 (2022) https://doi.org/10.1109/SAS54819.2022.9881337
    L. Munck, J.P. Nielsen, B. Møller, S. Jacobsen, I. Søndergaard, S.B. Engelsen,
    L. Nørgaard, R. Bro, Exploring the phenotypic expression of a regulatory proteome-altering
    gene by spectroscopy and chemometrics. Anal. Chim. Acta 446(1–2), 171–186 (2001)
    CAS   Google Scholar   J. Shi, W. Chen, X. Zou, Y. Xu, X. Huang, Y. Zhu, T. Shen,
    Detection of triterpene acids distribution in loquat (Eriobotrya japonica) leaf
    using hyperspectral imaging. Spectrochimica Acta - Part A: Molecular and Biomolecular
    Spectroscopy 188, 436–442 (2018). https://doi.org/10.1016/j.saa.2017.07.023 Article   CAS   PubMed   Google
    Scholar   A. Hassanzadeh, F. Zhang, J. Van Aardt, S.P. Murphy, S.J. Pethybridge,
    Broadacre crop yield estimation using imaging spectroscopy from unmanned aerial
    systems (UAS): A field-based case study with snap bean. Remote Sensing 13(16)
    (2021). https://doi.org/10.3390/rs13163241 E. Bonah, X. Huang, J.H. Aheto, R.
    Yi, S. Yu, H. Tu, Comparison of variable selection algorithms on Vis-NIR hyperspectral
    imaging spectra for quantitative monitoring and visualization of bacterial foodborne
    pathogens in fresh pork muscles. Infrared Physics and Technology 107 (2020). https://doi.org/10.1016/j.infrared.2020.103327
    W. Liu, S. Zeng, G. Wu, H. Li, F. Chen, Rice seed purity identification technology
    using hyperspectral image with lasso logistic regression model. Sensors 21(13)
    (2021). https://doi.org/10.3390/s21134384 N.H. Samrat, J.B. Johnson, S. White,
    M. Naiker, P. Brown, A rapid non-destructive hyperspectral imaging data model
    for the prediction of pungent constituents in dried ginger. Foods 11(5) (2022).
    https://doi.org/10.3390/foods11050649 Q. Wang, Y. Liu, Q. Xu, J. Feng, H. Yu,
    Identification of mildew degrees in honeysuckle using hyperspectral imaging combined
    with variable selection. Journal of Food Measurement and Characterization 13(3),
    2157–2166 (2019). https://doi.org/10.1007/s11694-019-00136-1 Article   Google
    Scholar   Q. Wang, Y. Liu, X. Gao, A. Xie, H. Yu, Potential of hyperspectral imaging
    for nondestructive determination of chlorogenic acid content in flos lonicerae.
    Journal of Food Measurement and Characterization 13(4), 2603–2612 (2019). https://doi.org/10.1007/s11694-019-00180-x
    Article   Google Scholar   D. Wu, D.-W. Sun, Advanced applications of hyperspectral
    imaging technology for food quality and safety analysis and assessment: A review
    - part ii: Applications. Innov. Food Sci. Emerg. Technol. 19, 15–28 (2013). https://doi.org/10.1016/j.ifset.2013.04.016
    Article   CAS   Google Scholar   D. Wu, S. Wang, N. Wang, P. Nie, Y. He, D.-W.
    Sun, J. Yao, Application of time series hyperspectral imaging (TS-HSI) for determining
    water distribution within beef and spectral kinetic analysis during dehydration.
    Food Bioprocess Technol. 6(11), 2943–2958 (2013). https://doi.org/10.1007/s11947-012-0928-0
    Article   Google Scholar   C. Cortes, V. Vapnik, Support-vector networks. Machine
    Learning 20(3 N - 1573-0565), 273–297 (1995) https://doi.org/10.1007/BF00994018
    D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning representations by back-propagating
    errors. Nature 323(6088), 533–536 (1986). https://doi.org/10.1038/323533a0 Article   Google
    Scholar   G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: theory
    and applications. Neurocomputing 70(1), 489–501 (2006). I.J. Goodfellow, Y. Bengio,
    A. Courville, Deep Learning. MIT Press, Cambridge, MA, USA (2016). http://www.deeplearningbook.org
    Y. Wang, F. Xiong, Y. Zhang, S. Wang, Y. Yuan, C. Lu, J. Nie, T. Nan, B. Yang,
    L. Huang, J. Yang, Application of hyperspectral imaging assisted with integrated
    deep learning approaches in identifying geographical origins and predicting nutrient
    contents of coix seeds. Food Chemistry 404 (2023). https://doi.org/10.1016/j.foodchem.2022.134503
    B. Jin, H. Qi, L. Jia, Q. Tang, L. Gao, Z. Li, G. Zhao, Determination of viability
    and vigor of naturally-aged rice seeds using hyperspectral imaging with machine
    learning. Infrared Physics and Technology 122 (2022). https://doi.org/10.1016/j.infrared.2022.104097
    Q. Pang, W. Huang, S. Fan, Q. Zhou, Z. Wang, X. Tian, Detection of early bruises
    on apples using hyperspectral imaging combining with YOLOv3 deep learning algorithm.
    Journal of Food Process Engineering 45(2) (2022). https://doi.org/10.1111/jfpe.13952
    T. Ojala, M. Pietikäinen, D. Harwood, A comparative study of texture measures
    with classification based on featured distributions. Pattern Recogn. 29(1), 51–59
    (1996). https://doi.org/10.1016/0031-3203(95)00067-4 Article   Google Scholar   Z.
    Wang, W. Huang, X. Tian, Y. Long, L. Li, S. Fan, Rapid and non-destructive classification
    of new and aged maize seeds using hyperspectral image and chemometric methods.
    Frontiers in Plant Science 13 (2022). https://doi.org/10.3389/fpls.2022.849495
    K. Yao, J. Sun, C. Chen, M. Xu, X. Zhou, Y. Cao, Y. Tian, Non-destructive detection
    of egg qualities based on hyperspectral imaging. Journal of Food Engineering 325
    (2022). https://doi.org/10.1016/j.jfoodeng.2022.111024 R.D. Logan, B. Scherrer,
    J. Senecal, N.S. Walton, A. Peerlinck, J.W. Sheppard, J.A. Shaw, Assessing produce
    freshness using hyperspectral imaging and machine learning. Journal of Applied
    Remote Sensing 15(3) (2021). https://doi.org/10.1117/1.JRS.15.034505 S. Feng,
    Y. Cao, T. Xu, F. Yu, D. Zhao, G. Zhang, Rice leaf blast classification method
    based on fused features and one-dimensional deep convolutional neural network.
    Remote Sensing 13(16) (2021). https://doi.org/10.3390/rs13163207 S. Weng, B. Guo,
    Y. Du, M. Wang, P. Tang, J. Zhao, Feasibility of authenticating mutton geographical
    origin and breed via hyperspectral imaging with effective variables of multiple
    features. Food Anal. Methods 14(4), 834–844 (2021). https://doi.org/10.1007/s12161-020-01940-y
    Article   Google Scholar   C.T. Kucha, L. Liu, M. Ngadi, C. Gariépy, Assessment
    of intramuscular fat quality in pork using hyperspectral imaging. Food Engineering
    Reviews 13(1), 274–289 (2021). https://doi.org/10.1007/s12393-020-09246-9 Article   CAS   Google
    Scholar   M.K. Behera, K.M.S. Kishore, S. Chakravarty, Classification of soil
    and prediction of total nitrogen content present in soil by using hyperspectral
    imaging. Lecture Notes in Networks and Systems 202 LNNS, 337–345 (2021). https://doi.org/10.1007/978-981-16-0695-3_33
    Article   Google Scholar   C. Wang, S. Wang, X. He, L. Wu, Y. Li, J. Guo, Combination
    of spectra and texture data of hyperspectral imaging for prediction and visualization
    of palmitic acid and oleic acid contents in lamb meat. Meat Science 169 (2020).
    https://doi.org/10.1016/j.meatsci.2020.108194 H. Zhang, S. Zhang, Y. Chen, W.
    Luo, Y. Huang, D. Tao, B. Zhan, X. Liu, Non-destructive determination of fat and
    moisture contents in salmon (salmo salar) fillets using near-infrared hyperspectral
    imaging coupled with spectral and textural features. Journal of Food Composition
    and Analysis 92 (2020). https://doi.org/10.1016/j.jfca.2020.103567 D. Zhang, G.
    Chen, H. Zhang, N. Jin, C. Gu, S. Weng, Q. Wang, Y. Chen, Integration of spectroscopy
    and image for identifying fusarium damage in wheat kernels. Spectrochimica Acta
    - Part A: Molecular and Biomolecular Spectroscopy 236 (2020). https://doi.org/10.1016/j.saa.2020.118344
    J.H. Aheto, X. Huang, X. Tian, Y. Ren, B. Ernest, E.A. Alenyorege, C. Dai, T.
    Hongyang, Z. Xiaorui, P. Wang, Multi-sensor integration approach based on hyperspectral
    imaging and electronic nose for quantitation of fat and peroxide value of pork
    meat. Anal. Bioanal. Chem. 412(5), 1169–1179 (2020). https://doi.org/10.1007/s00216-019-02345-5
    Article   CAS   PubMed   Google Scholar   H. Lin, Z. Wang, W. Ahmad, Z. Man, Y.
    Duan, Identification of rice storage time based on colorimetric sensor array combined
    hyperspectral imaging technology. Journal of Stored Products Research 85 (2020).
    https://doi.org/10.1016/j.jspr.2019.101523 Y. Liu, S. Zhou, W. Han, W. Liu, Z.
    Qiu, C. Li, Convolutional neural network for hyperspectral data analysis and effective
    wavelengths selection. Anal. Chim. Acta 1086, 46–54 (2019). https://doi.org/10.1016/j.aca.2019.08.026
    Article   CAS   PubMed   Google Scholar   C. Xia, S. Yang, M. Huang, Q. Zhu, Y.
    Guo, J. Qin, Maize seed classification using hyperspectral image coupled with
    multi-linear discriminant analysis. Infrared Physics and Technology 103 (2019).
    https://doi.org/10.1016/j.infrared.2019.103077 S. Jia, H. Li, X. Wu, Q. Li, Laboratory-based
    hyperspectral image analysis for the classification of soil texture. Journal of
    Applied Remote Sensing 13(4) (2019). https://doi.org/10.1117/1.JRS.13.046508 D.
    Tao, Z. Wang, G. Li, L. Xie, Sex determination of silkworm pupae using Vis-NIR
    hyperspectral imaging combined with chemometrics. Spectrochimica Acta - Part A:
    Molecular and Biomolecular Spectroscopy 208, 7–12 (2019). https://doi.org/10.1016/j.saa.2018.09.049
    Article   CAS   PubMed   Google Scholar   K. Tan, R. Wang, M. Li, Z. Gong, Discriminating
    soybean seed varieties using hyperspectral imaging and machine learning. Journal
    of Computational Methods in Sciences and Engineering 19(4), 1001–1015 (2019).
    https://doi.org/10.3233/JCM-193562 Article   Google Scholar   D. Tao, Z. Wang,
    G. Li, L. Xie, Simultaneous species and sex identification of silkworm pupae using
    hyperspectral imaging technology. Spectrosc. Lett. 51(8), 446–452 (2018). https://doi.org/10.1080/00387010.2018.1503602
    Article   CAS   Google Scholar   Y. Wang, X. Hu, Z. Hou, J. Ning, Z. Zhang, Discrimination
    of nitrogen fertilizer levels of tea plant (camellia sinensis) based on hyperspectral
    imaging. J. Sci. Food Agric. 98(12), 4659–4664 (2018). https://doi.org/10.1002/jsfa.8996
    Article   CAS   PubMed   Google Scholar   J. Lu, M. Zhou, Y. Gao, H. Jiang, Using
    hyperspectral imaging to discriminate yellow leaf curl disease in tomato leaves.
    Precision Agric. 19(3), 379–394 (2018). https://doi.org/10.1007/s11119-017-9524-7
    Article   Google Scholar   Y. Sun, K. Wei, Q. Liu, L. Pan, K. Tu, Classification
    and discrimination of different fungal diseases of three infection levels on peaches
    using hyperspectral reflectance imaging analysis. Sensors (Switzerland) 18(4)
    (2018). https://doi.org/10.3390/s18041295 H. Lu, X. Yu, L. Zhou, Y. He, Selection
    of spectral resolution and scanning speed for detecting green jujubes chilling
    injury based on hyperspectral reflectance imaging. Applied Sciences (Switzerland)
    8(4) (2018). https://doi.org/10.3390/app8040523 J. Xiong, R. Lin, R. Bu, Z. Liu,
    Z. Yang, L. Yu, A micro-damage detection method of litchi fruit using hyperspectral
    imaging technology. Sensors (Switzerland) 18(3) (2018). https://doi.org/10.3390/s18030700
    B. Jia, W. Wang, S.-C. Yoon, H. Zhuang, Y.-F. Li, Using a combination of spectral
    and textural data to measure water-holding capacity in fresh chicken breast fillets.
    Applied Sciences (Switzerland) 8(3) (2018). https://doi.org/10.3390/app8030343
    T. Guo, M. Huang, Q. Zhu, Y. Guo, J. Qin, Hyperspectral image-based multi-feature
    integration for TVB-N measurement in pork. J. Food Eng. 218, 61–68 (2018). https://doi.org/10.1016/j.jfoodeng.2017.09.003
    Article   CAS   Google Scholar   R. Khodabakhshian, B. Emadi, Application of Vis/SNIR
    hyperspectral imaging in ripeness classification of pear. Int. J. Food Prop. 20,
    3149–3163 (2018). https://doi.org/10.1080/10942912.2017.1354022 Article   CAS   Google
    Scholar   J. Ning, J. Sun, S. Li, M. Sheng, Z. Zhang, Classification of five chinese
    tea categories with different fermentation degrees using visible and near-infrared
    hyperspectral imaging. Int. J. Food Prop. 20, 1515–1522 (2017). https://doi.org/10.1080/10942912.2016.1233115
    Article   CAS   Google Scholar   H. Zhu, B. Chu, C. Zhang, F. Liu, L. Jiang, Y.
    He, Hyperspectral imaging for presymptomatic detection of tobacco disease with
    successive projections algorithm and machine-learning classifiers. Scientific
    Reports 7(1) (2017). https://doi.org/10.1038/s41598-017-04501-2 H.-H. Wang, S.-L.
    Zhang, K. Li, S.-S. Cheng, M.-Q. Tan, X.-H. Tao, X. Zhang, Non-destructive detection
    of ready-to-eat sea cucumber freshness based on hyperspectral imaging. Spectroscopy
    and Spectral Analysis 37(11), 3632–3640 (2017). https://doi.org/10.3964/j.issn.1000-0593(2017)11-3632-09
    Article   CAS   Google Scholar   S. Zeng, L. Chen, L. Jiang, C. Gao, Hyperspectral
    imaging technique based on Geodesic K-medoids clustering and Gabor wavelets for
    pork quality evaluation. International Journal of Wavelets, Multiresolution and
    Information Processing 15(6) (2017). https://doi.org/10.1142/S0219691317500667
    Y. Fan, T. Wang, Z. Qiu, J. Peng, C. Zhang, Y. He, Fast detection of striped stem-borer
    (chilo suppressalis walker) infested rice seedling based on visible/near-infrared
    hyperspectral imaging system. Sensors (Switzerland) 17(11) (2017). https://doi.org/10.3390/s17112470
    D. Yang, D. He, A. Lu, D. Ren, J. Wang, Combination of spectral and textural information
    of hyperspectral imaging for the prediction of the moisture content and storage
    time of cooked beef. Infrared Physics and Technology 83, 206–216 (2017). https://doi.org/10.1016/j.infrared.2017.05.005
    Article   Google Scholar   J.-H. Cheng, D.-W. Sun, Q. Wei, Enhancing visible and
    near-infrared hyperspectral imaging prediction of TVB-N level for fish fillet
    freshness evaluation by filtering optimal variables. Food Anal. Methods 10(6),
    1888–1898 (2017). https://doi.org/10.1007/s12161-016-0742-9 Article   Google Scholar   J.
    Ma, D.-W. Sun, H. Pu, Model improvement for predicting moisture content (MC) in
    pork longissimus dorsi muscles under diverse processing conditions by hyperspectral
    imaging. J. Food Eng. 196, 65–72 (2017). https://doi.org/10.1016/j.jfoodeng.2016.10.016
    Article   Google Scholar   J.-L. Xu, D.-W. Sun, Identification of freezer burn
    on frozen salmon surface using hyperspectral imaging and computer vision combined
    with machine learning algorithm [identification de la brûlure de congélation sur
    la surface du saumon congelé en utilisant l’imagerie hyperspectrale et la vision
    par ordinateur combinée avec l’algorithme d”’apprentissage automatique]. International
    Journal of Refrigeration 74, 149–162 (2017). https://doi.org/10.1016/j.ijrefrig.2016.10.014
    Zhao, Y.R., Yu, K.Q., Feng, C., Cen, H.Y., He, Y.: Early detection of aphid (Myzus
    persicae) infestation on chinese cabbage by hyperspectral imaging and feature
    extraction. Transactions of the ASABE 60(4), 1045–1051 (2017) https://doi.org/10.13031/trans.11886
    S. Fan, B. Zhang, J. Li, C. Liu, W. Huang, X. Tian, Prediction of soluble solids
    content of apple using the combination of spectra and textural features of hyperspectral
    reflectance imaging data. Postharvest Biol. Technol. 121, 51–61 (2016). https://doi.org/10.1016/j.postharvbio.2016.07.007
    Article   Google Scholar   H. Ma, H.-Y. Ji, W.S. Lee, Identification of the citrus
    greening disease using spectral and textural features based on hyperspectral imaging.
    Spectroscopy and Spectral Analysis 36(7), 2344–2350 (2016). https://doi.org/10.3964/j.issn.1000-0593(2016)07-2344-07
    Article   CAS   PubMed   Google Scholar   U. Khulal, J. Zhao, W. Hu, Q. Chen,
    Nondestructive quantifying total volatile basic nitrogen (TVB-N) content in chicken
    using hyperspectral imaging (HSI) technique combined with different data dimension
    reduction algorithms. Food Chem. 197, 1191–1199 (2016). https://doi.org/10.1016/j.foodchem.2015.11.084
    Article   CAS   PubMed   Google Scholar   M. Huang, C. He, Q. Zhu, J. Qin, Maize
    seed variety classification using the integration of spectral and image features
    combined with feature transformation based on hyperspectral imaging. Applied Sciences
    (Switzerland) 6(6) (2016). https://doi.org/10.3390/app6060183 J. Sun, S. Jiang,
    H. Mao, X. Wu, Q. Li, Classification of black beans using visible and near infrared
    hyperspectral imaging. Int. J. Food Prop. 19(8), 1687–1695 (2016). https://doi.org/10.1080/10942912.2015.1055760
    Article   Google Scholar   G.K. Naganathan, K. Cluff, A. Samal, C.R. Calkins,
    D.D. Jones, R.L. Wehling, J. Subbiah, Identification and validation of key wavelengths
    for on-line beef tenderness forecasting. Trans. ASABE 59(3), 769–783 (2016). https://doi.org/10.13031/trans.59.11034
    Article   Google Scholar   L. Wang, D.-W. Sun, H. Pu, Z. Zhu, Application of hyperspectral
    imaging to discriminate the variety of maize seeds. Food Anal. Methods 9(1), 225–234
    (2016). https://doi.org/10.1007/s12161-015-0160-4 Article   Google Scholar   C.
    Xie, Y. Shao, X. Li, Y. He, Detection of early blight and late blight diseases
    on tomato leaves using hyperspectral imaging. Scientific Reports 5 (2015). https://doi.org/10.1038/srep16564
    Z. Xiong, D.-W. Sun, H. Pu, Z. Zhu, M. Luo, Combination of spectra and texture
    data of hyperspectral imaging for differentiating between free-range and broiler
    chicken meats. LWT 60(2), 649–655 (2015). https://doi.org/10.1016/j.lwt.2014.10.021
    Article   CAS   Google Scholar   Y. Cao, C. Zhang, Q. Chen, Y. Li, S. Qi, L. Tian,
    Y. Ren, Identification of species and geographical strains of sitophilus oryzae
    and sitophilus zeamais using the visible/near-infrared hyperspectral imaging technique.
    Pest Manag. Sci. 71(8), 1113–1121 (2015). https://doi.org/10.1002/ps.3893 Article   CAS   PubMed   Google
    Scholar   J. Ma, H. Pu, D.-W. Sun, W. Gao, J.-H. Qu, K.-Y. Ma, Application of
    Vis-NIR hyperspectral imaging in classification between fresh and frozen-thawed
    pork longissimus dorsi muscles. Int. J. Refrig. 50, 10–18 (2015). https://doi.org/10.1016/j.ijrefrig.2014.10.024
    Article   Google Scholar   M. Huang, Y. Ma, Y. Li, Q. Zhu, G. Huang, P. Bu, Hyperspectral
    image-based feature integration for insect-damaged hawthorn detection. Anal. Methods
    6(19), 7793–7800 (2014). https://doi.org/10.1039/c4ay01246a Article   CAS   Google
    Scholar   D. Liu, H. Pu, D.-W. Sun, L. Wang, X.-A. Zeng, Combination of spectra
    and texture data of hyperspectral imaging for prediction of pH in salted meat.
    Food Chem. 160, 330–337 (2014). https://doi.org/10.1016/j.foodchem.2014.03.096
    Article   CAS   PubMed   Google Scholar   H.-J. He, D. Wu, D.-W. Sun, Potential
    of hyperspectral imaging combined with chemometric analysis for assessing and
    visualising tenderness distribution in raw farmed salmon fillets. J. Food Eng.
    126, 156–164 (2014). https://doi.org/10.1016/j.jfoodeng.2013.11.015 Article   Google
    Scholar   H. Huang, L. Liu, M.O. Ngadi, C. Gariépy, S.O. Prasher, Near-infrared
    spectral image analysis of pork marbling based on Gabor filter and wide line detector
    techniques. Appl. Spectrosc. 68(3), 332–339 (2014). https://doi.org/10.1366/13-07242
    Article   CAS   PubMed   Google Scholar   H. Pu, D.-W. Sun, J. Ma, D. Liu, J.-H.
    Cheng, Using wavelet textural features of visible and near infrared hyperspectral
    image to differentiate between fresh and frozen–thawed pork. Food Bioprocess Technol.
    7(11), 3088–3099 (2014). https://doi.org/10.1007/s11947-014-1330-x Article   Google
    Scholar   X. Wei, F. Liu, Z. Qiu, Y. Shao, Y. He, Ripeness classification of astringent
    persimmon using hyperspectral imaging technique. Food Bioprocess Technol. 7(5),
    1371–1380 (2014). https://doi.org/10.1007/s11947-013-1164-y Article   Google Scholar   D.
    Wu, D.-W. Sun, Y. He, Novel non-invasive distribution measurement of texture profile
    analysis (TPA) in salmon fillet by using visible and near infrared hyperspectral
    imaging. Food Chem. 145, 417–426 (2014). https://doi.org/10.1016/j.foodchem.2013.08.063
    Article   CAS   PubMed   Google Scholar   Q. Chen, Y. Zhang, J. Zhao, Z. Hui,
    Nondestructive measurement of total volatile basic nitrogen (TVB-N) content in
    salted pork in jelly using a hyperspectral imaging technique combined with efficient
    hypercube processing algorithms. Anal. Methods 5(22), 6382–6388 (2013). https://doi.org/10.1039/c3ay40436f
    Article   CAS   Google Scholar   H. Yong-Guang, C. Pei-Pei, L. Ping-Ping, Determination
    of water content in de-enzyming green tea leaves based on hyper-spectral imaging.
    Inf. Technol. J. 12(22), 6729–6734 (2013). https://doi.org/10.3923/itj.2013.6729.6734
    Article   Google Scholar   J. Gao, X. Li, F. Zhu, Y. He, Application of hyperspectral
    imaging technology to discriminate different geographical origins of Jatropha
    curcas, L. seeds. Comput. Electron. Agric. 99, 186–193 (2013). https://doi.org/10.1016/j.compag.2013.09.011
    Article   Google Scholar   X. Zhang, F. Liu, Y. He, X. Li, Application of hyperspectral
    imaging and chemometric calibrations for variety discrimination of maize seeds.
    Sensors (Switzerland) 12(12), 17234–17246 (2012). https://doi.org/10.3390/s121217234
    Article   CAS   Google Scholar   Y. Tian, T. Li, L. Zhang, X. Zhang, Diagnosis
    method of cucumber downy mildew with NIR hyperspectral imaging. Proceedings of
    SPIE - The International Society for Optical Engineering 8002 (2011). https://doi.org/10.1117/12.901527
    R. Gosselin, D. Rodrigue, C. Duchesne, A hyperspectral imaging sensor for on-line
    quality control of extruded polymer composite products. Comput. Chem. Eng. 35(2),
    296–306 (2011). https://doi.org/10.1016/j.compchemeng.2010.07.020 Article   CAS   Google
    Scholar   G. ElMasry, N. Wang, A. ElSayed, M. Ngadi, Hyperspectral imaging for
    nondestructive determination of some quality attributes for strawberry. J. Food
    Eng. 81(1), 98–107 (2007). https://doi.org/10.1016/j.jfoodeng.2006.10.016 Article   CAS   Google
    Scholar   R.M. Haralick, Statistical and structural approaches to texture. Proc.
    IEEE 67(5), 786–804 (1979). https://doi.org/10.1109/PROC.1979.11328 Article   Google
    Scholar   R.M. Haralick, K. Shanmugam, I. Dinstein, Textural features for image
    classification. IEEE Trans. Syst. Man Cybern. SMC–3(6), 610–621 (1973). https://doi.org/10.1109/TSMC.1973.4309314
    Article   Google Scholar   D.A. Clausi, An analysis of co-occurrence texture statistics
    as a function of grey level quantization. Can. J. Remote. Sens. 28(1), 45–62 (2002).
    https://doi.org/10.5589/m02-004 Article   Google Scholar   L.-K. Soh, C. Tsatsoulis,
    Texture analysis of SAR sea ice imagery using gray level co-occurrence matrices.
    IEEE Trans. Geosci. Remote Sens. 37(2), 780–795 (1999). https://doi.org/10.1109/36.752194
    Article   Google Scholar   S.W.-C. Lam, Texture feature extraction using gray
    level gradient based co-occurence matrices. In: 1996 IEEE International Conference
    on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929),
    vol. 1, pp. 267–2711 (1996). https://doi.org/10.1109/ICSMC.1996.569778 A. Ramola,
    A.K. Shakya, D. Van Pham, Study of statistical methods for texture analysis and
    their modern evolutions. Engineering Reports 2(4), 12149 (2020). https://doi.org/10.1002/eng2.12149
    Article   Google Scholar   X. Chu, R. Li, H. Wei, H. Liu, Y. Mu, H. Jiang, Z.
    Ma, Determination of total flavonoid and polysaccharide content in anoectochilus
    formosanus in response to different light qualities using hyperspectral imaging.
    Infrared Physics and Technology 122 (2022). https://doi.org/10.1016/j.infrared.2022.104098
    J. Redmon, A. Farhadi, YOLOv3: An Incremental Improvement. (2018). arxiv:1804.02767
    C. Pohl, J.L. Van Genderen, Review article multisensor image fusion in remote
    sensing: concepts, methods and applications. Int. J. Remote Sens. 19(5), 823–854
    (1998) Article   Google Scholar   J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li,
    L. Fei-Fei, Imagenet: A large-scale hierarchical image database. In: 2009 IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.5206848
    J. Chai, H. Zeng, A. Li, E.W.T. Ngai, Deep learning in computer vision: A critical
    review of emerging techniques and application scenarios. Machine Learning with
    Applications 6(2021). https://doi.org/10.1016/j.mlwa.2021.100134 Article   Google
    Scholar   Download references Acknowledgements Not applicable. Funding Open Access
    funding enabled and organized by CAUL and its Member Institutions Author information
    Authors and Affiliations School of Computer Science, University of Auckland, 38
    Princes Street, Auckland CBD, Auckland, 1010, New Zealand Mitchell Rogers, Martin
    Urschler & Patrice Delmas Institute for Medical Informatics, Statistics and Documentation,
    Medical University Graz, 8010, Graz, Austria Martin Urschler DGA/DT/TA/EMOO, 47
    rue Saint Jean, 31130, Balma, France Jacques Blanc-Talon Corresponding author
    Correspondence to Mitchell Rogers. Additional information Original article has
    been corrected to update affiliation. Supplementary Information Below is the link
    to the electronic supplementary material. Supplementary file1 (PDF 791 KB) Supplementary
    file2 (XLSX 247 KB) Rights and permissions Open Access This article is licensed
    under a Creative Commons Attribution 4.0 International License, which permits
    use, sharing, adaptation, distribution and reproduction in any medium or format,
    as long as you give appropriate credit to the original author(s) and the source,
    provide a link to the Creative Commons licence, and indicate if changes were made.
    The images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Rogers, M., Blanc-Talon,
    J., Urschler, M. et al. Wavelength and texture feature selection for hyperspectral
    imaging: a systematic literature review. Food Measure 17, 6039–6064 (2023). https://doi.org/10.1007/s11694-023-02044-x
    Download citation Received 08 May 2023 Accepted 01 July 2023 Published 17 August
    2023 Issue Date December 2023 DOI https://doi.org/10.1007/s11694-023-02044-x Share
    this article Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Hyperspectral imaging Multispectral imaging Wavelength selection
    Sensors Spatial features Food science Computer vision Agriculture Use our pre-submission
    checklist Avoid common mistakes on your manuscript. Sections Figures References
    Abstract Methods Results Discussion Conclusion Change history Notes References
    Acknowledgements Funding Author information Additional information Supplementary
    Information Rights and permissions About this article Advertisement Discover content
    Journals A-Z Books A-Z Publish with us Publish your research Open access publishing
    Products and services Our products Librarians Societies Partners and advertisers
    Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy
    choices/Manage cookies Your US state privacy rights Accessibility statement Terms
    and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic
    Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024
    Springer Nature"'
  inline_citation: '>'
  journal: Journal of Food Measurement and Characterization
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Wavelength and texture feature selection for hyperspectral imaging: a systematic
    literature review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mishra S.
  - Volety D.R.
  - Bohra N.
  - Alfarhood S.
  - Safran M.
  citation_count: '0'
  description: Today, there is a visible reduction in productivity of common crops
    like rice and wheat in agricultural sector. At the same time, millet crops have
    emerged as an alternative to counter global food security issues because of their
    adaptability and nutritional value that can combat malnutrition. Also, because
    of its low moisture requirement and tolerance to extreme climatic conditions,
    it is perceived as a stable cereal. But its cultivation is affected by diseases
    like rust and blast, thereby causing harm to the farming economy. Conventional
    operational methods of disease detection require regular manual intervention which
    are also costly. The correctness of expert suggestions is also under scrutiny.
    Thus, a sustainable, robust and low cost modern approach for millet crop monitoring
    and disease detection is required. This research aims to develop a smart and sustainable
    framework by integrating Internet of things (IoT) and deep learning (DL). In the
    presented framework, a sensory module based automated crop health data gathering
    system with an improved deep learning-assisted intelligent disease recognition
    model is developed for the millet crop. Sensors collect data from millet farmland
    transfer the crop data readings to the cloud server for storage and Raspberry
    Pi for further detection. Any abnormality in reading leads to an alert notification
    communicated to the farmer. A hybrid predictive model, Customized Convolutional
    neural network (Customized-CNN) model works with the Raspberry Pi to predict the
    presence of blast and rust disease symptoms in millet. The proposed sustainable
    model is implemented, and it generates productive outcomes. The recorded accuracy,
    precision, recall, and f-score values with the customized CNN model were 98.8%,
    98.2%, 97.4%, and 97.7%, respectively. The training and testing delays were only
    67 s and 88 s, respectively. A sample of yearly sensor readings was generated
    to show the accuracy and reliability of the model's data collection. Also, the
    model proved to be scalable, as it gave noteworthy performance with diverse crops.
    The evaluation shows the reliability of the model, and it can be used by farming
    societies to enhance millet cereal yield in a cost-effective way.
  doi: 10.1016/j.aej.2023.10.041
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related works 3. Materials and methods
    4. Results and discussion 5. Conclusion Declaration of Competing Interest Acknowledgement
    References Show full outline Figures (6) Tables (7) Table 1 Table 2 Table 3 Table
    4 Table 5 Table 6 Show all tables Alexandria Engineering Journal Volume 83, 15
    November 2023, Pages 298-306 Original Article A smart and sustainable framework
    for millet crop monitoring equipped with disease detection using enhanced predictive
    intelligence Author links open overlay panel Sushruta Mishra a, Dayal Rohan Volety
    a, Navdeep Bohra b, Sultan Alfarhood c, Mejdl Safran c Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.aej.2023.10.041 Get rights and content Under
    a Creative Commons license open access Abstract Today, there is a visible reduction
    in productivity of common crops like rice and wheat in agricultural sector. At
    the same time, millet crops have emerged as an alternative to counter global food
    security issues because of their adaptability and nutritional value that can combat
    malnutrition. Also, because of its low moisture requirement and tolerance to extreme
    climatic conditions, it is perceived as a stable cereal. But its cultivation is
    affected by diseases like rust and blast, thereby causing harm to the farming
    economy. Conventional operational methods of disease detection require regular
    manual intervention which are also costly. The correctness of expert suggestions
    is also under scrutiny. Thus, a sustainable, robust and low cost modern approach
    for millet crop monitoring and disease detection is required. This research aims
    to develop a smart and sustainable framework by integrating Internet of things
    (IoT) and deep learning (DL). In the presented framework, a sensory module based
    automated crop health data gathering system with an improved deep learning-assisted
    intelligent disease recognition model is developed for the millet crop. Sensors
    collect data from millet farmland transfer the crop data readings to the cloud
    server for storage and Raspberry Pi for further detection. Any abnormality in
    reading leads to an alert notification communicated to the farmer. A hybrid predictive
    model, Customized Convolutional neural network (Customized-CNN) model works with
    the Raspberry Pi to predict the presence of blast and rust disease symptoms in
    millet. The proposed sustainable model is implemented, and it generates productive
    outcomes. The recorded accuracy, precision, recall, and f-score values with the
    customized CNN model were 98.8%, 98.2%, 97.4%, and 97.7%, respectively. The training
    and testing delays were only 67 s and 88 s, respectively. A sample of yearly sensor
    readings was generated to show the accuracy and reliability of the model’s data
    collection. Also, the model proved to be scalable, as it gave noteworthy performance
    with diverse crops. The evaluation shows the reliability of the model, and it
    can be used by farming societies to enhance millet cereal yield in a cost-effective
    way. Previous article in issue Next article in issue Keywords Deep learningImage
    processingInternet of thingsPlant diseasesMilletSustainabilityPredictive learning
    1. Introduction Crop diseases are a serious threat to productivity and sustainable
    development in agriculture. Agriculture is the first act that has helped humanity
    thrive and prosper. Today, the food and agriculture industry is a major priority
    in the world due to the growing number of people and, therefore, the increasing
    demand for food to sustain their lives. Farming is the primary source of food
    and fuel, contributing to the country''s financial development. By 2030, there
    should be reasonable food-building systems and durable farming methods that increase
    overall production. It should also conserve the environment, protect the crops,
    and make them durable from different natural disasters such as changes in temperature,
    extreme climate, droughts, and other factors that further improve the soil quality.
    About 70 % of the total population is dependent on farming in some way or another.
    Pearl millet is the sixth most essential crop for any country after any other
    major crops. Pennisetum glaucum has the ability to resist any climate change,
    and the crop could be very vital to lessening the damaging results of temperature
    change. It also has the capacity to extend the income and food problems of areas
    with a lack of water. The automated identification of plant diseases may be a
    main topic in the field of farming. Furthermore, the primary and early detection
    of plant illnesses undoubtedly affects crop production and profit [1]. Tomorrow,
    global warming may increase because of humans, which affect crop production through
    pests [2] and insects [3]. The government has declared millet ‘Nutri Cereals’
    for various purposes such as production, trade, and consumption [4]. This crop
    is adaptable to climate variations due to its low consumption of water, steadiness
    at soaring temperatures, and lack of water-resistant capability. Hence, millet’s
    ability can be utilized for food insecurities. Crop infections are a significant
    problem in farming and affect both quality and quantity. The production of millet
    is mainly influenced by rust and blast disease [5]. Such diseases pose a considerable
    threat to agricultural sustainability and impact farmers'' economic standards
    [6], [7]. Hence, a system must be introduced that would eliminate the diseases
    quicker and easier. Today, agribusiness are looking forward to artificial intelligence
    and the internet of things to make agriculture more efficient and sustainable.
    The new research on various methodologies has been applied to different sectors,
    which has led to better results in the areas of computer vision and Machine Learning[ML].
    The most recent work that has been introduced is the application of DL in the
    field using computers in agriculture. Various remote sensing techniques through
    the Internet of Things and sensors help farmers monitor crop states at different
    phases. Sensors and modern predictive methods have proven their ability to automate
    data storage, data collection, and analysis of accumulated data samples to facilitate
    precise forecasting. With the immediate method to prevent environmental degradation,
    increase profit, and reduce waste, farmers are increasingly using more effective
    crop management solutions that are supported by controlling technologies and optimization
    derived from IoT. Through IoT and IoT sensing approaches, sustainable energy-related
    research can bring about a revolution in this area. The excessive use of IoT devices
    and electronic components such as cameras, drones, and various sensors helps significantly.
    Sustainable energy systems can address challenges in strength protection for the
    community, with a minimum of an alternate approach to surroundings and subcultures.
    By using IoT and sensors to collect data about their plants and then further filtering
    such information with various methodologies such as machine learning and deep
    learning Agribusinesses are able to keep an eye on the condition of their crops
    in the agriculture field. These technologies are much more than helping farmers
    make agriculture more sustainable; they are also making it more profitable and
    productive. DL and the Internet of Things framework are presented in the paper,
    Blast and Rust Prediction in Pearl Millet. The combination of IoT sensors with
    DL is used to construct a system for disease recognition in pearl millet. The
    challenge is to develop an accurate and efficient disease prediction system for
    rust and blast in millet and a monitoring system that overcomes the limitations
    of traditional methods. In this research, a sensory module-based automated crop
    health data gathering system with an improved deep learning-assisted intelligent
    disease recognition model is presented for the millet crop. The framework automatically
    collects the images and other parametric data present on the millet farmland and
    further sends the collected data to the cloud for storage and Raspberry Pi for
    further classification into rust and blast diseases. The novel Customized-CNN
    model functions with the Raspberry Pi to detect the presence of blast or rust
    diseases in millet. Any abnormality observed in readings from the sensors,an alert
    that is communicated to the farmer. The motivation behind this research lies in
    the need for a robust solution that addresses the challenges faced due to crop
    destruction by diseases. Traditional methods have fallen short of providing the
    accuracy and agility required to meet the demands of modern farming. The proposed
    work fills this critical gap by combining cutting-edge technology in the form
    of deep learning with an Internet of Things (IoT) sensor network to revolutionize
    millet disease prediction and management. Major highlights of the research study
    include the following: • The work presents the current scenario and the concerned
    literature review undertaken in the context of millet crop production as well
    as its disease monitoring. • Based on existing research gaps, a novel methodology
    for millet crop tracking and disease prediction using smart sensory computing
    based predictive intelligence is presented. The model uses a camera module to
    acquire images and cloud storage for effective data accessibility. • The IoT unit
    equipped with crop health tracking sensors allows real-time data collection and
    timely notifications to farmers about any variation in crop conditions. Further
    using a Customized-CNN model on preprocessed data, the occurrence of millet disease
    is accurately detected. • The implementation outcome using various performance
    metrics appears promising which makes it scalable and sustainable. It can assist
    the farming society to accurately track the growth of crops. 2. Related works
    Rational farming has been a popular research area for the past few decades. It
    can be achieved through an effective integration of IoT-based crop health monitoring
    and machine learning-enabled disease detection. Various works concerning millet
    crop tracking are discussed in the following two subsections. 2.1. Predictive
    approach for crop disease prediction In [8], a classification model using CNN
    was presented to distinguish between good-quality leaves and many other diseased
    leaves of various crops. Analysis in [9] found that an important activity for
    raising farming output is the early diagnosis and restriction of plant disorders,
    especially in the early phases of onset. The authors in [10] developed a deep
    neural network for millet disease prediction using transfer learning for prediction.
    Using a superior-quality camera, pictures of rice crop leaves are taken for real-time
    applications [11]. The research process made use of a dataset that included pictures
    of both healthy and sick leaves. Semi-supervised learning is utilized in the paper
    [12] to categorize images using Generative Adversarial Networks. In this kind
    of learning, the discriminator is converted into a multi-class classifier, and
    the generator is solely used for discriminator training. In the paper [13], it
    was found that current disease examination processes frequently result in unreliable
    categorization results, which have recently reduced rice yields. In [14], two
    prevalent illnesses were identified using banana photos from a PlantVillage dataset.
    The experiment was undertaken on diverse kinds of photo scans, with a cumulative
    number of around 4000 images that were shrunk to 60 × 60 pixels. The model''s
    accuracy ranged from 92 to 99 % after numerous training sessions using various
    ratios of the train-to-test split. It was built on the LeNet architecture. A comparable
    method shown in [15] started with 1053 photos of apple leaves and then used preprocessing
    to create 13,689 synthetic images. Five distinct CNN models, as well as a neural
    network and SVM, were tested in the experiment. The modified AlexNet recorded
    the best outcome, identifying various kinds of apple leaf abnormalities with an
    accuracy of 97.62 %. Red-green–blue (RGB) images of rice leaves were utilized
    by authors in [16] to create a novel image processing model that could identify
    and categorize illnesses in rice plants. A straightforward and efficient Naive
    Bayes (NB) classifier was then used for classification. In a publication by Sue
    Han Lee [17], the model is trained using the Caffe framework and the back-propagation
    process. The deconvolutional network was one of the methods used to comprehend
    the inner workings of the CNN model and visualize the chosen filters. By deconvolutional
    and unpooling down to the input image pixel, the deconvolutional network offers
    a function that enables us to see the feature map at each layer. In another paper,
    a system for identifying plants is suggested. Using a hybridized gray level co-occurrence
    matrix (GLCM), a discrete wavelet transform (DWT), and the scan investigate filter
    target (SIFT) model, Devi et al. [18] applied image processing to damaged areas
    of rice leaves. Numerous classifiers, including k-nearest neighbour algorithms,
    back propagation neural networks, multi-class support vector machines (SVMs),
    and naive Bayes, have also been used. The advanced neural network (NN) is suggested
    in this research [19] as a means of processing hyper spectral data. NN mechanisms,
    types, models, and classifiers using various algorithms are reviewed. The present
    state of imaging and non-imaging of the hyper spectral data is highlighted in
    this work. Table 1 summarizes the related works on machine learning-based crop
    disease diagnosis (Table 2). Table 1. Related works on predictive approach for
    crop disease detection. Authors Objectives Methodology Benefits Kamlesh Golhani
    et al., 2018 Neural networks method to detect various diseases in plant Advanced
    neural networks are used to detect plant diseases using hyperspectral data with
    a focus on plants. NNs are used to classify hyperspectral dataset in studying
    with a accuracy of 91 %. E. Pantazi et al., 2019 Automated leaf disease detection
    For feature extraction, use Local Binary Patterns (LBPs), and for classification,
    use One Class Classification. By taking images from the farmland in particular
    conditions crop health can be determined. Al-Amin,et. al, 2019 Potato disease
    identification by its leave Deep CNN model is used in the study. Late blight,
    early blight diseases can be predicted along with the healthy potato with accuracy
    of 97. 43 %. Vishal Pallagani et al., 2019 DL based methodology for correct recognizance
    of infected crops using smart Agriculture Deep convolutional neural network (DNN)
    With the highest degree of confidence, the model has been able to forecast 38
    distinct diseases. T. Islam et al., 2018 A modern technique detect diseases in
    rice plants The state of the art Naive Bayes algorithm has been used. This technique
    is quicker because it makes use of one characteristic ie. RGB. S. Ramesh et al.,
    2020 Detection of paddy leaf diseases. Java language used in deep neural networks.
    According to the established technique, the accuracy was good, coming up at 97.6
    % for the afflicted blast disease, 95.78 % for the bacterial blight, and 90.57
    % for the typical leaf image. F. T. Pinki, et al 2017 Content based paddy leaf
    disease recognition Disease detection in paddy crop and providing solution using
    SVM. It predicts the three paddy leaf diseases Leaf blast, Brown spot and Bacterial
    blight and solution to each problem is provided such as chemicals. Amara, J et
    al 2017 A DL based framework for diseases in banana tree detection This method
    uses LeNet architecture as a CNN to classify disease. Upon several experimentations
    the system was able to find good classification results with a accuracy of 98.
    61 % Lee, Sue Han, et al., 2017 Approach used by DL to extract information from
    an image learns it feature. It makes use of CNN to understand different aspects
    based on a De-Convolutional Network (DN). CNN provides better resolution images
    as compared to other custom algorithms. Marko Arsenovic et al., 2016 Deep Neural
    Networks Based algorithm is used to detect diseases in plants. Detection diseases
    in plants using leaf image classification by using neural networks The model gave
    a exceptional result of precision of 91 % and 97 % for different class,The average
    was about 97. 2 %. P. N. Gayathri, et al., 2019 Diseases detection in Rice plant
    leaves in the southern part of india. K nearest neighborhood neural network, back
    propagation neural network, Support vector Machine to categorize plants which
    are infected or not. It is seen that multi class support vector machines gave
    better accuracy of 97. 83 %. Umapathy Eaganathan et al., 2014 Spotting of Sugarcane
    Leaf Scorch Disease K-means Clustering and Segmentation The overall accuracy using
    K-NN Classifier is 95 % and texture analysis can be used for feature extraction
    Table 2. Summary of related works in crop monitoring using IoT. Authors Summary
    Methodology Benefits Torres-Sospedra et al., 2020 A support system based on vineyard
    is used for monitoring mildew disease Senviro a detailed platform which monitors
    crops in farmland. The proposed methodology helps detect and treat downy mildew
    disease. Yang C et al, 2020 Crop Disease Detection Smart and precison agriculture
    using iot remote sensing Disease detection and site-specific management using
    remote sensing and variable rate technology. Khattab, A et al., 2019 An Internet
    of Things and sensors monitoring system to detect plant disease in early stage
    Agricultural monitoring system using wireless sensor network The presented framework
    predicted the diseases in potato and tomato at a very high accuracy and precision
    Saha, A. K et al., 2018 Improves the crop production using IOT based devices such
    as drones. Internet of Things,Unmanned Aerial Vehicle(UAV); RGB-D sensor; With
    the proposed model the drones will help the farmers at every step of production
    which would increase the yield of crops. K. Lokesh Krishna et al., 2017 Application
    Smart-Agriculture implementation using IOT. Various IOT devices such as mobile
    robot is designed and implmented for performing various tasks in the farm. This
    method controls the various needs for plant growth. It is highly easier to use
    as compared to other complex methods. Thorat, S. A et al., 2017 Early detection
    of grapes diseases Wireless Sensor Network, Vineyard, Hidden Markov Model, Zig-Bee
    By using the proposed model the quantity and quality of production increases and
    accurate information about various chemicals to be sprayed be delivered to the
    farmer. Riskiawan,et al., 2019 Plant and crop can be protected from various threats
    by using the approach of smart agriculture IoT, big data, and deep learning Early
    recognition of disease can be done using smart agriculture which stops further
    spreading of disease in various part of plant. Sankararao, Adduru UG, et al 2021
    Water stress detection using UAV based hyperspectral imaging Canopy with Selected
    Wavebands using UAV Based Hyperspectral Imaging The result of the proposed approach
    shows it recognizes the water stress level in the millet crops. Shi Y et al.,
    2015 IOT application to monitor plant diseases, pests and insects IOT,Pest monitoring
    system, sensor nodes This system provides a way to access and use various agricultural
    information of the farm. 2.2. Smart IoT based crop monitoring and modeling The
    IoT era has revolutionized our world. Precision agriculture is considered an eco-friendly,
    sustainable, and profitable mode to improve agriculture production and quality.
    To monitor plants, methods for infection detection require an enormous amount
    of data and information. To deal with such data, first analyze each cluster with
    other techniques individually to check and show each result. [20]. In the paper,
    author Sai Kirthi Pilli [21] stated that the AGROBOT can test cotton using high-resolution
    images where higher-resolution images are used. It can help to make a decision
    about their farmland by themselves or by using some other means. The authors developed
    a wireless robot [22] using the Internet of Things to do a lot of work, which
    increases the sustainability of crops. This robot takes all the data from various
    devices that are connected to it and takes action accordingly. They also save
    the crops from various insects and pests, which can also be done by using chemicals.
    In [23], the authors developed an Internet of Things technology that uses a wireless
    IOT sensor to gather data from agricultural land. The information from the gadgets
    is utilized to treat illnesses. The authors of [24] created a system for yard
    mildew prediction. The findings are a crucial piece of evidence for phytosanitary
    protection. The Hidden Markov model was used by authors in [25] to monitor a closed-circuit
    camera prototype to detect grape disorders. Numerous sensors are placed on agricultural
    land to gather information about temperature and moisture. Then, Zig-Bee is used
    to transmit these data clusters to a server. Ying et al. [26] developed a technique
    for remote sensing and precision agriculture that is used to find and identify
    illnesses in a variety of crops. It provides experts in various fields, such as
    growers, consultants, and farm equipment, and various experts, such as dealers
    of chemicals, to provide the correct way to deal with such situations. Shi et
    al. [27] mentioned that Internet of Things methods can provide different techniques
    to control and manage pests and insects in farmland based on the IOT framework.
    Markovic et al. [28] presented an Internet of Things-based system for farmers
    that would help them secure their crops even from a distinct location and adjust
    the needs of crops as required. This system is very low-cost and user-friendly,
    which enables farmers to take the production process on a larger scale. Wang et
    al. [29] made a model for crop disease recognition. In this method, various algorithms
    are used to predict the parameters that are required by the crop. Murphy et al.
    [30] The ability of artificial intelligence algorithms can be beneficial in detecting
    disease early. The authors of [31] presented a way to integrate IoT devices using
    Raspberry Pi modules to improve production. Table 1 shows the related existing
    studies undertaken in the plant crop diseases detection. However it is observed
    that most of the existing systems were not able to provide real time automated
    capturing of images which affected the overall prediction efficiency. Many works
    focused on estimating only one type of plant disease while ignoring multi disease
    detection in plants. Also, the existing models are not so robust and reliable
    as they focus on monitoring very limited crops. Apart from these constraints,
    none of the work were responsive framework. Most of the deep learning models earlier
    deployed for disease detection have restricted accuracy with high computational
    delay to generate results. In addition, the internal structural complexity of
    these models are quite high which is not suitable for timely and quick prediction.
    Another gap is the generalization of machine learning models across various use
    cases in agriculture fields. One major constraint observed in the usage of IoT
    for predicting discrepancy in crops is the inadequate interpretation of appropriate
    design and arrangement of smart sensors in agricultural deployment. Thus the development
    of practical techniques for managing the enormous amounts of sensor data generated
    by IoT enabled agricultural equipment is a major issue that requires research.
    The practical use of IoT based plant disease prediction depends on the development
    of scalable and effective algorithms for data preprocessing, feature extraction,
    and disease prediction. Some existing studies also underestimated vital parameter
    like current soil moisture content and regular temperature readings needed to
    track crop health. Thus, developing a more non-interrupted, robust, cost effective
    and reliable plant disease assessment model disease prediction in resource constrained
    environment is the need of the hour for widespread adoption. 3. Materials and
    methods This section presents the proposed methodology in context to millet crop
    monitoring and its disease detection. Also, the dataset used for the study is
    highlighted here. 3.1. Dataset used in study Several hardware elements, such as
    sensors and cameras, are fixed on the farmland in order to collect data. Images
    of the crops that had rust or blast infections were taken in order to identify
    the diseases. For training the model, the researchers have used the blast and
    rust dataset publicly available on Kaggle [32]. A total of 3000 images of the
    millet crop that was found to have blast and rust were selected from the dataset.
    Table 3 summarizes the overall distribution of millet crop samples. Table 3. Distribution
    of Millet crop dataset. Diseases of Millet Total Images Images in training data
    set Images in testing data set Rust 1500 1050 450 Blast 1500 1050 450 A further
    70 %–30 % split of the sample was made into training and testing groups. In a
    70:30 split, 70 % of the dataset is allocated for training the model and 30 %
    for testing its performance. A smaller testing set can reduce the risk of over
    fitting, where the model performs well on training data but poorly on unseen data.
    A larger training set helps the model generalize better to unseen samples. Also,
    authors performed several experiments with the dataset division, for example,
    80:20 and 60:40. It was found that the efficiency with 70:30 ratio generated the
    most optimal outcome. Also, small test sets required fewer computational resources
    for evaluation, which is an advantage in a resource constrained environment like
    agriculture. Fig. 1 shows some samples of millet crop images accumulated from
    the field for analysis. Download : Download high-res image (186KB) Download :
    Download full-size image Fig. 1. Millet crop image samples collected from field.
    3.2. Proposed methodology In this section, the novel framework and the workflow
    model for millet crop disease assessment are presented. The system used for the
    experimental evaluation has the NVIDIA GeForce GTX 1650ti GPU. It runs Windows
    10 64-bit and has PyTorch 1. 9. 1′s deep learning framework, which uses CUDA 12.
    1 and cuDNN 8. 9. 0 to speed up the training of network models on the GPU. This
    study involves a two procedure based approach for millet crop disease assessment.
    Smart IoT sensors with cloud storage are used to track the overall health of millet
    crops by continuously monitoring their temperature, humidity, and moisture levels.
    In case of health parameters discrepancies, farmers are notified. The second procedure
    addresses the millet disease classification into blast and rust types by using
    an advanced deep learning model. The proposed workflow model is observed in Fig.
    2. Download : Download high-res image (255KB) Download : Download full-size image
    Fig. 2. Proposed Methodology for Millet disease monitoring and prediction. A camera
    system is installed to capture images of the crops at regular intervals, while
    temperature, humidity, and soil moisture sensors are embedded in agricultural
    land. The captured images are stored in a cloud storage service, Amazon S3 for
    easy access and organization. To improve the quality of images and eliminate noisy
    features, pre-processing techniques like resizing, normalization, noise reduction,
    and image enhancement are applied. A dataset is standardized by extracting the
    pre-processed images and corresponding disease labels, which are then divided
    into training and testing sets. The data is shuffled to ensure randomness in both
    sets. Further a hybrid deep learning technique, the Customized-CNN model is chosen
    for disease prediction. The architecture of the selected Customized-CNN model
    is designed, and the model is trained using the accumulated dataset. Hyperparameters
    are fine-tuned, and the model is validated using the testing dataset to assess
    its accuracy and generalization capabilities. In parallel, the IoT based sensory
    unit is integrated into the framework. Table 4 shows the IoT components used in
    the research. The GPS unit integrated with sensory modules is fruitful in tracking
    the position of plant abnormality. Also determining the location of land where
    pesticides and water are needed is significant. The temperature, humidity, and
    soil moisture sensors are connected to a Raspberry Pi. A program is designed to
    collect sensor data, which is stored locally on the Raspberry Pi. The collected
    sensor data is then transferred to a cloud storage service, Amazon S3 for secure
    storage. Table 4. IoT sensors configuration. Sensors Model Purpose Configuration
    Temperature and Humidity DHT11 Temperature and humidity are measured using the
    sensor. • Temperature Range: −20 °C to 50 °C Humidity Range: 20 % to 80 % RH (Relative
    Humidity) Soil Moisture LM358 The sensor measures the water content the in soil.
    Range of 0 % (dry) to 100 % (saturated soil). Raspberry PI Raspberry PI-3 Data
    collected from sensors are received by Raspberry PI for further processing. Raspberry
    Pi 3 has 40 GPIO pins that can be used to connect and interface with sensors,
    including DHT11 and LM358. A notification system is implemented to alert farmers
    about significant changes in soil moisture, temperature, or other relevant parameters.
    Change in climatic variables increases the vulnerability of disease occurrence
    in millets. Also, the oospores of the soil act as the main point of infection
    in the lower part of the crop. Thus, sensory units embedded to be used for regular
    soil tracking may recognize oospore''s presence thereby helping to detect the
    risk at an initial phase. The Customized-CNN enabled predictive module collaborates
    with the Raspberry Pi to function in a synchronized way to enable precise forecasting
    and notifying farmers regarding plant risks and other changes in agricultural
    land. A novel Customized-CNN model is used for the suggested strategy because
    CNN produces the highest accuracy even with a limited dataset. A deep neural convolution
    network has been advanced in the suggested way to trip over rust and blast. Using
    3 RGB channels and a 128 by 128 length entry, a bespoke Convolutional neural network
    is built, and the results are seen using a 3 by 3 length convolution filter. Following
    the application of the convolution filter, the image''s dimensions were reduced
    to 126 × 126 with a depth of 32 × 32 filters, and a max-pool layer with a length
    of 2 × 2 filters was added. Following the convolution layer max-pooling procedure,
    the input size was further decreased to 63x63. The same method is applied repeatedly
    until the snap pictures'' dimensions are 28x28. The operations of a typical fully
    connected artificial neural network structure are then performed by flattening
    the two-dimensional matrix into a column vector. On a single totally linked layer,
    the activation function is “relu”. Batch normalization is also carried out to
    hasten the model''s training and steady its learning process. So as to enhance
    the model on validation data and reduce over fitting during training, dropout
    function is also used. Table 5 highlights the proposed model parameters configuration.
    Table 5. Customized-CNN parameters. Layers Layers Operation Feature Map No. Feature
    Map size Kernel size Parameters C1 Convolution 32 128x128 3x3 (32 * 3 * 3 + 32)
    = 320 (Weights) + 32 (Biases) = 352 S1 Max-pooling 32 126x126 2x2 0 C2 Convolution
    16 63x63 3x3 (16 * 3 * 3 * 32 + 16) = 4640 (Weights) + 16 (Biases) = 4656 S2 Max-pooling
    16 61x61 2x2 0 C3 Convolution 8 30x30 3x3 (8 * 3 * 3 * 16 + 8) = 1160 (Weights)
    + 8 (Biases) = 1168 S3 Max-poolling 8 28x28 2x2 0 FC Flatten Layer 8 N/A N/A 0
    S3 Max-pooling 128 2 × 2 8 × 8 0 FC Fully connected layer 128 1X1 N/A (128 * 2
    + 2) = 258 (Weights) + 2 (Biases) = 260 parameters FC Output 3 1X1 N/A 0 The Customized-CNN
    model used in this work is fine-tuned with its vital parameters. The preset values
    of its parameters are highlighted in Table 6. Table 6. Parameters of model training.
    Parameters Value Learning-Rate 0.001 Pool-size(Max pooling) (3,3) Validation-Steps
    50 Number of Epochs 500 Num-Classes 2 Optimizer ADAM Dropout Rate 0.3 Batch Size
    32 Activation Function ReLU(hidden layer) Softmax(output layer) The important
    relevant equations applied in the proposed Customized-CNN model are highlighted
    here. Eq. (1) shows the Convolutional layer. (1) where C tells the output,I is
    the input image and K is the convolution kernel(filter). It takes an input image
    (I) and convolves it with a convolution kernel (K) to produce the output feature
    map (C) output. In the context of millet disease, this equation is at the core
    of how the model learns to detect disease-related patterns or features. Equation
    (2) denotes the max pooling layer used in the model. (2) where Y is the pooled
    output, X is the input, and s is the stride. Pooling layers are used in CNNs to
    reduce the spatial dimensions of feature maps.The input (X) is down-sampled to
    produce the pooled output (Y) using a specified pooling function with a given
    stride (s).It helps reduce complexity, over fitting. In millet disease detection,
    it can help capture essential disease-related features in a more compact form.
    (3) The activation function ‘relu’ is used in this model as shown in equation
    (3). If the function receives any negative input, it returns 0; however, if the
    function receives any positive value ×, it returns that value. As a result, the
    output has a range of 0 to infinite. The batch normalization of the model is shown
    in equation (4). (4) where y is the normalized output,x is the input, is the mean,
    is the variance, is the scale factor, is the shift factor, is constant. Batch
    normalization is used to normalize the activation of neural network layers during
    training. It reduces internal covariate shift, improving convergence, and enabling
    faster training. Eq. (5) represents the back propagation method used in the neural
    network. (5) The is the standard back propagation term, is the error gradient
    is the learning rate.The is the momentum part, where is the momentum rate. This
    represents the update rule for adjusting the network''s weights (w) during the
    training process. Back propagation is essential for optimizing the model''s parameters
    to minimize prediction errors. 4. Results and discussion In this section, we discuss
    and contrast the novel framework with alternative combinations of related CNN
    classifiers using different validation indicators. Here we evaluate the model
    using precision, recall, accuracy, and training time. It provides a complete evaluation
    of the model''s effectiveness. These metrics are derived from four confusion matrix
    variables. True Positives (TP) denotes the samples which are correctly classified.
    False Positives (FP) counts the number of samples incorrectly classified. True
    Negatives (TN) shows the sample count that are correctly classified showing the
    absence of disease. False Negatives (FN) denotes the number of data instances
    which are incorrectly classified showing the disease to be absent. Accuracy is
    the ratio of cases that were successfully predicted to all instances denotes the
    accuracy rate of prediction as shown in Eq. (6). (6) Precision tells the percent
    of correctly predicted good outcomes to all positive outcomes as shown in Eq.
    (7). It evaluates ability of the framework to detect positive occurrences. (7)
    Recall as shown in Eq. (8) represents the proportion of precisely recognized positive
    samples with all positive samples is the recall factor. It evaluates how well
    the model can identify each positive event. (8) F1 score suggests the harmonic
    average of precision and recall. It gives an accurate measurement that considers
    both recall and precision. The Eq. (9) shows the F1 score. (9) The novel Customized-CNN
    model used in the study for millet disease prediction is trained using data gathered
    from sensors placed around the farm. The model''s performance is validated by
    repeated experimentation to produce the best results. The proposed model was compared
    with other existing models like VGG16, VGG19, Resnet, and others. The validation
    outcome attained through the proposed model was very promising. Accuracy, precision,
    recall and f-score values noted using the Customized-CNN model were 98.8 %, 98.2
    %, 97.4 %, and 97.7 % respectively. Fig. 3 denotes the overall outcome analysis.
    Download : Download high-res image (469KB) Download : Download full-size image
    Fig. 3. Evaluation metrics analysis of proposed model with other computational
    models. Fig. 4 denotes the computational delay evaluation of our approach with
    existing deep learning methods. The computational delay is evaluated using both
    training delay and testing delay. It is observed that Customized-CNN model records
    the least execution time as compared to others. While the training is just 67
    s, the testing execution time incurred is also only 88 s. VGG-19 model records
    the highest computational training and testing delay of 106 s and 125 s respectively.
    Download : Download high-res image (341KB) Download : Download full-size image
    Fig. 4. Computational delay analysis of proposed model with other predictive models.
    The sensor readings analysis of different soil health parameters using the proposed
    framework is also taken into account. As seen in Fig. 5, the monthly mean readings
    of temperature, humidity, and soil moisture level for a single year are captured
    and highlighted. The average yearly temperature, humidity, and moisture level
    noted through sensory readings are 25.8, 63.6, and 30.6 respectively. As seen
    in the figure, there is very little deviation from normal. Except for the months
    of June, July, and October, the sensory data for other months suggest that the
    millet crop got the required amount of temperature, humidity, and moisture throughout
    the year for its growth. The sensor data are also notified to the farmer in case
    any abnormal deviation is noted. Download : Download high-res image (676KB) Download
    : Download full-size image Fig. 5. Soil health metrics readings data on an yearly
    basis using integrated sensors. The scalability of a new predictive framework
    also depends upon its diverse behavior toward different scenarios. In our study,
    the scalability is validated by evaluating the model against performance metrics
    by considering various other crops like rice, maize, wheat, soybean, and groundnut.
    The outcome of the evaluation was quite promising as shown in Fig. 6. The mean
    accuracy, precision, recall, and f-score values obtained were 97.36 %, 96.56 %,
    94.82 %, and 95.78 % respectively. Download : Download high-res image (385KB)
    Download : Download full-size image Fig. 6. Evaluation metrics analysis of proposed
    model in context to other crops. As seen in Table 7, a comparative analysis of
    the proposed model is performed with models highlighted in the literature review
    section. The outcome is summarized in context to accuracy, precision, recall,
    and f-score metrics. It is observed that the proposed model outperforms other
    existing models in terms of all metrics. Table 7. Evaluation analysis of proposed
    model with models discussed in literature survey. Models Accuracy(%) Precision(%)
    Recall (%) F-Score (%) Ramesh et al.[11] 97.9 92.8 96.86 96.86 Pinki et al. [13]
    97.82 91.7 96.74 95.27 Amara et al. [14] 94.4 94.79 94.4 94.62 Lee et al. [17]
    96.3 94.6 93.2 93.4 Smith et al. [21] 89 87.5 88.3 87.2 Gayathri et al. [18] 97.83
    96.3 95.5 95.9 Umapathy et al. [10] 95 94.2 90.1 92.85 Proposed model 98.8 98.2
    97.4 97.7 The proposed model for millet crop assessment is developed in this study
    and is evaluated using different parameters. The predicted values for accuracy,
    precision, recall, and f-score with the novel Customized-CNN model were extremely
    satisfactory. One of the main reasons for the proposed model to outperform others
    is its high accuracy. High accuracy means that the proposed model can accurately
    identify the millet disease with a low error rate, hence providing reliable results.
    The proposed model strikes a balance between complexity and performance. While
    deep learning models can become highly complex with many layers and parameters,
    the proposed model achieves exceptional accuracy without any unnecessary complexity.
    The proposed model has been trained after hyperparameter tuning to optimize CNN
    performance. Adjusting parameters like learning rate, batch size, and number of
    layers or filters has resulted in a better and simpler model as compared to other
    existing models. The proposed model achieves high accuracy while using all the
    resources efficiently and with optimal computational resources. The overall computational
    execution delay with the model was also very low. The recorded crop health sensory
    readings data over a period of a single year was generated and its correctness
    was validated. Further, the model proved to be scalable as it gave an accurate
    estimation with other crops too. 5. Conclusion Effective smart monitoring of millet
    crops, along with disease prediction using predictive intelligence, is studied.
    IoT sensors for temperature, humidity, and soil moisture level were used to keep
    track of crop health. A novel customized CNN model was applied to the millet crop
    data to predict the likelihood of the occurrence of disease on the millet crop
    based on aggregated sensory readings and help in classifying the type of disease.
    Traditional farming is being replaced with modern, sustainable farming in this
    work. This makes it robust and advantageous economically. The proposed sustainable
    framework, upon implementation, generated optimum outcomes. The observed accuracy,
    precision, recall, and f-score values with the predictive model were 98.8 %, 98.2
    %, 97.4 %, and 97.7 %, respectively. The training delay was only 67 s, and the
    testing delay noted was also just 88 s. The mean temperature, humidity, and moisture
    level noted through sensory readings on a yearly basis were 25.8, 63.6, and 30.6,
    respectively. The generated average accuracy, precision, recall, and f-score values
    for other diverse crops were 97.36 %, 96.56 %, 94.82 %, and 95.78 %, respectively,
    which made it very scalable. Thus, the proposed sustainable model for millet crop
    assessment is definitely worthy, as it gave excellent results. It can assist the
    agricultural community and farmers in reliable tracking of millet crop growth
    and the occurrence of any disease symptoms. In the future, this research can be
    improved even more by integrating a drone service into the camera module to take
    photographs of the fields from different angles to give a more precise outcome
    in remote settings.The existing system can further be upgraded to develop a user-friendly
    Android or iOS mobile application that allows farmers to access real-time data
    from the IoT sensors and receive disease alerts.The system can further be extended
    to integrate the irrigation control system with the existing IoT sensors to automate
    the watering of crops as required. Declaration of Competing Interest The authors
    declare that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgement
    This research is funded by the Researchers Supporting Project Number (RSPD2023R890),
    King Saud University, Riyadh, Saudi Arabia. References [1] P.K. Sethy, N.K. Barpanda,
    A.K. Rath, S.K. Behera Using a support vector machine identified a deep feature-based
    rice leaf disease Comput. Electron. Agric., 175 (2020), Article 105527 View PDFView
    articleView in ScopusGoogle Scholar [2] A. Dutta, C. Misra, R.K. Barik, S. Mishra
    Enhancing mist assisted cloud computing toward secure and scalable architecture
    for smart healthcare International Conference on Advanced Communication and Computational
    Technology, Springer Nature Singapore, Singapore (2019), pp. 1515-1526 Google
    Scholar [3] S. Savary, L. Willocquet, S. J. Pethybridge, P. Esker, N. McRoberts,
    A. Nelson, The global burden of pathogens and pests on major food crops, Nat.
    Ecol. Evol. 3(3) (2019) 430-439, doi:10. 1038/s41559-018-0793-y. Google Scholar
    [4] S. Sahoo, S. Mishra, B. Panda, N. Jena Building a new model for feature optimization
    in agricultural sectors 2016 3rd International Conference on Computing for Sustainable
    Global Development (INDIACom), IEEE (2016), pp. 2337-2341 View in ScopusGoogle
    Scholar [5] A.K. Jukanti, C.L.L. Gowda, K.N. Rai, V.K. Manga, R.K. Bhatt world-feeding
    crops 11. In the dry and semi-arid tropics, pearl millet (Pennisetum glaucum L.)
    is a significant source of nutrition, health, and food security Food Secure.,
    8 (2016), pp. 307-329 CrossRefView in ScopusGoogle Scholar [6] S. Chakraborty,
    S. Mishra, A smart farming-based recommendation system using collaborative machine
    learning and image processing, in: Cognitive Informatics and Soft Computing: Proceeding
    of CISC 2021, Springer Nature Singapore, Singapore, 2022, pp. 703–716. Google
    Scholar [7] S.K. Mohapatra, S. Mishra, H.K. Tripathy, A. Alkhayyat A sustainable
    data-driven energy consumption assessment model for building infrastructures in
    resource constraint environment Sust. Energy Technol. Assess., 53 (2022), Article
    102697 Google Scholar [8] Srdjan Sladojevic, Marko Arsenovic, Andras Anderla,
    Dubravko Culibrk, Darko Stefanovic, Deep Neural Networks Based Recognition of
    Plant Diseases by Leaf Image Classification, Computational Intelligence and Neuroscience,
    vol. 2016, Article ID 3289801, 11 pages, 2016. https://doi.org/10.1155/2016/3289801.
    Google Scholar [9] X.E. Pantazi, D. Moshou, A.A. Tamouridou Automated leaf disease
    detection in different crop species through image features analysis and One Class
    Classifiers Comput. Electron. Agric., 156 (2019), pp. 96-104 View PDFView articleView
    in ScopusGoogle Scholar [10] S. Coulibaly, et al. Deep neural networks with transfer
    learning in millet crop images Comput. Ind., 108 (2019), pp. 115-120 View PDFView
    articleView in ScopusGoogle Scholar [11] S. Ramesh, D. Vydeki Recognition and
    classification of paddy leaf diseases using optimised deep neural network with
    java algorithm Inform. Process. Agric., 7 (2) (2020), pp. 249-260 View PDFView
    articleView in ScopusGoogle Scholar [12] G. Olmschenk, H. Tang, Z. Zhu Crowd counting
    with minimal data using generative adversarial networks for multiple target regression
    IEEE Winter Conference on Applications of Computer Vision (WACV), 2018 (2018),
    pp. 1151-1159 CrossRefView in ScopusGoogle Scholar [13] F.T. Pinki, N. Khatun,
    S.M. Islam, Content based paddy leaf disease recognition and remedy prediction
    using support vector machine, in: 2017 20th International Conference of Computer
    and Information Technology (ICCIT), 2017, pp. 1–5. Google Scholar [14] J. Amara,
    B. Bouaziz, A. Algergawy A Deep Learning-based Approach for Banana Leaf Diseases
    Classification Datenbanksysteme für Business, Technologie und Web (2017) Google
    Scholar [15] V. Pallagani, V. Khandelwal, B. Chandra, V. Udutalapally, D. Das,
    S.P. Mohanty, dCrop: a deep-learning based framework for accurate prediction of
    diseases of crops in smart agriculture, in: 2019 IEEE International Symposium
    on Smart Electronic Systems (iSES) (Formerly iNiS), 2019, pp. 29–33. Google Scholar
    [16] M. Brahimi, M. Arsenovic, S. Laraba, S. Sladojevic, K. Boukhalfa, A. Moussaoui
    Deep Learning for Plant Diseases: Detection and Saliency Map Visualisation Human
    and Machine Learning (2018) Google Scholar [17] S.H. Lee, C. Chan, S.J. Mayo,
    P. Remagnino How deep learning extracts and learns leaf features for plant classification
    Pattern Recogn., 71 (2017), pp. 1-13 View PDFView articleGoogle Scholar [18] T.
    Devi, P.N. Gayathri Image processing based rice plant leaves diseases in Thanjavur
    Tamilnadu Clust. Comput., 6 (2019), pp. 1-14 Google Scholar [19] K. Golhani, S.K.
    Balasundram, G. Vadamalai, B. Pradhan, A review of neural networks in plant disease
    detection using hyperspectral data. Information Processing in Agriculture, 2018.
    Google Scholar [20] Basori, Ahmad Hoirul, Mansur, Andi Besse Firdausiah and Riskiawan,
    Hendra Yufit, SMARF: Smart Farming Framework Based on Big Data, IoT and Deep Learning
    Model for Plant Disease Detection and Prevention. Applied Computing to Support
    Industry: Innovation and Technology 1174 (2020) 44-56. ISSN 1865-0929. Google
    Scholar [21] Smith, Bharathiraja Nallathambi, Sai Kirthi Pilli, eAGROBOT- A Robot
    for Early Crop Disease Detection Using Image Processing, by Jessy George and Vivek
    Diwanji was published in ICECS in February 2015. Google Scholar [22] K. L. Krishna,
    O. Silver, W. F. Malende and K. Anuradha, “Internet of Things application for
    implementation of smart agriculture system,” 2017 International Conference on
    I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Palladam, India,
    2017, pp. 54-59, doi: 10.1109/I-SMAC.2017.8058236. Google Scholar [23] Khairy,
    S.E. Habib, Y. Fahmy, A. Khattab, H. Ismail, S. Zayan, M. M. Early plant disease
    forecasting using an IoT-based cognitive monitoring system. Comput. Electron.
    Agric. 166 (2019) 105028. Google Scholar [24] S. Riles, J. Torres-Sospedra, Belmonte,
    F.J. Zarazaga-Soria, A. González-Pérez, J. Huerta, Creation of an open sensorized
    platform for smart agriculture: Mildew disease monitoring for a vineyard support
    system. Maintain Computing and Informatics Systems 2020, 28, 100309. Google Scholar
    [25] S.S. Patil, S.A. Thorat Early detection of grapes diseases using machine
    learning and IoT Second International Conference on Cognitive Computing and Information
    Processing (CCIP), 2016 (2016), pp. 1-5 Google Scholar [26] K. Prema, M.B. Carmel
    Smart Farming: IoT-based plant leaf disease detection and prediction using deep
    neural network with image processing Int. J. Innov. Technol. Explor. Eng. (2019),
    pp. 3081-3083 View in ScopusGoogle Scholar [27] C. Yang A practical application
    example of remote sensing and precision agricultural technologies for crop disease
    detection and management Technology, 6 (5) (2020), pp. 528-532 View PDFView articleView
    in ScopusGoogle Scholar [28] Y. Shi, Z. Wang, X. Wang, S. Zhang, Internet of Things
    Application to Monitoring Plant Disease and Insect Pests, 2015. Google Scholar
    [29] X. F. Wang, Z. Wang, S.W. Zhang, Y. Shi, Monitoring and discrimination of
    plant disease and insect pests based on agricultural IOT. In: 4th International
    Conference on Information Technology and Management Innovation, 2015, pp. 112–115.
    Atlantis Press. Google Scholar [30] D. Markovic, R. Koprivica, U. Pesovic, S.
    Randic Application of IoT in monitoring and controlling agricultural production
    Acta Agriculturae Serbica, 20 (2015), pp. 145-153, 10.5937/AASer1540145M Google
    Scholar [31] A. Nadeem, A. Basit, S. Azfar A review of pest detection and management
    methods employing wireless sensor networks J. Entomol Zool. Stud., 3 (2015), pp.
    92-99 Google Scholar [32] N. Kundu, G. Rani, V.S. Dhaka, K. Gupta, S.C. Nayak,
    S. Verma, M.F. Ijaz, M. Woźniak IoT and interpretable machine learning based framework
    for disease prediction in pearl millet Sensors, 21 (2021), p. 5386, 10.3390/s21165386
    View in ScopusGoogle Scholar Cited by (0) © 2023 THE AUTHORS. Published by Elsevier
    BV on behalf of Faculty of Engineering, Alexandria University. Recommended articles
    Evaluation of 5G techniques affecting the deployment of smart hospital infrastructure:
    Understanding 5G, AI and IoT role in smart hospital Alexandria Engineering Journal,
    Volume 83, 2023, pp. 335-354 Arun Kumar, …, Monthippa Uthansakul View PDF An empirical
    survey of topologies, evolution, and current developments in multilevel inverters
    Alexandria Engineering Journal, Volume 83, 2023, pp. 148-194 G. Ezhilarasan, …,
    Stanislav Misak View PDF Photocatalytic wastewater treatment and disinfection
    using green ZnO-NP synthesized via extract Alexandria Engineering Journal, Volume
    83, 2023, pp. 113-121 Ibrahim Hotan Alsohaimi, …, Amr Mohammad Nassar View PDF
    Show 3 more articles Article Metrics Captures Readers: 15 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Alexandria Engineering Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A smart and sustainable framework for millet crop monitoring equipped with
    disease detection using enhanced predictive intelligence
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Almasoud A.S.
  - Mengash H.A.
  - Saeed M.K.
  - Alotaibi F.A.
  - Othman K.M.
  - Mahmud A.
  citation_count: '0'
  description: Recently, the usage of remote sensing (RS) data attained from unmanned
    aerial vehicles (UAV) or satellite imagery has become increasingly popular for
    crop classification processes, namely soil classification, crop mapping, or yield
    prediction. Food crop classification using RS images (RSI) is a significant application
    of RS technology in agriculture. It involves the use of satellite or aerial imagery
    to identify and classify different types of food crops grown in a specific area.
    This information can be valuable for crop monitoring, yield estimation, and land
    management. Meeting the criteria for analyzing these data requires increasingly
    sophisticated methods and artificial intelligence (AI) technologies provide the
    necessary support. Due to the heterogeneity and fragmentation of crop planting,
    typical classification approaches have a lower classification performance. However,
    the DL technique can detect and categorize crop types effectively and has a stronger
    feature extraction capability. In this aspect, this study designed a new remote
    sensing imagery data analysis using the marine predators algorithm with deep learning
    for food crop classification (RSMPA-DLFCC) technique. The RSMPA-DLFCC technique
    mainly investigates the RS data and determines the variety of food crops. In the
    RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the feature
    extraction process. The MPA is applied for the optimal hyperparameter selection
    process in order to optimize the accuracy of SimAM-EfficientNet architecture.
    MPA, inspired by the foraging behaviors of marine predators, perceptively explores
    hyperparameter configurations to optimize the hyperparameters, thereby improving
    the classification accuracy and generalization capabilities. For crop type detection
    and classification, an extreme learning machine (ELM) model can be used. The simulation
    analysis of the RSMPA-DLFCC technique is performed on two benchmark datasets.
    The extensive analysis of the results portrayed the higher performance of the
    RSMPA-DLFCC approach over existing DL techniques.
  doi: 10.3390/biomimetics8070535
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Biomimetics All Article Types Advanced   Journals
    Biomimetics Volume 8 Issue 7 10.3390/biomimetics8070535 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editors
    Francesco Visentin Junzhi Yu Ali Leylavi Shoushtari Subscribe SciFeed Recommended
    Articles Related Info Links More by Authors Links Article Views 1095 Table of
    Contents Abstract Introduction Literature Review The Proposed Model Results Analysis
    Conclusions Author Contributions Funding Institutional Review Board Statement
    Data Availability Statement Conflicts of Interest References Altmetric share Share
    announcement Help format_quote Cite question_answer Discuss in SciProfiles thumb_up
    Endorse textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Remote Sensing Imagery Data Analysis Using Marine Predators Algorithm with Deep
    Learning for Food Crop Classification by Ahmed S. Almasoud 1, Hanan Abdullah Mengash
    2, Muhammad Kashif Saeed 3,*, Faiz Abdullah Alotaibi 4, Kamal M. Othman 5 and
    Ahmed Mahmud 6 1 Department of Information Systems, College of Computer and Information
    Sciences, Prince Sultan University, Riyadh 11586, Saudi Arabia 2 Department of
    Information Systems, College of Computer and Information Sciences, Princess Nourah
    bint Abdulrahman University, Riyadh 11671, Saudi Arabia 3 Department of Computer
    Science, Applied College, Muhayil, King Khalid University, Abha 61421, Saudi Arabia
    4 Department of Information Science, College of Humanities and Social Sciences,
    King Saud University, Riyadh 11437, Saudi Arabia 5 Department of Electrical Engineering,
    College of Engineering and Islamic Architecture, Umm Al-Qura University, Makkah
    21955, Saudi Arabia 6 Research Center, Future University in Egypt, New Cairo 11835,
    Egypt * Author to whom correspondence should be addressed. Biomimetics 2023, 8(7),
    535; https://doi.org/10.3390/biomimetics8070535 Submission received: 27 September
    2023 / Revised: 21 October 2023 / Accepted: 31 October 2023 / Published: 10 November
    2023 (This article belongs to the Special Issue Biomimetics in Agri-Food: From
    Preliminary Design to Field Applications) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Recently, the usage of remote sensing (RS) data
    attained from unmanned aerial vehicles (UAV) or satellite imagery has become increasingly
    popular for crop classification processes, namely soil classification, crop mapping,
    or yield prediction. Food crop classification using RS images (RSI) is a significant
    application of RS technology in agriculture. It involves the use of satellite
    or aerial imagery to identify and classify different types of food crops grown
    in a specific area. This information can be valuable for crop monitoring, yield
    estimation, and land management. Meeting the criteria for analyzing these data
    requires increasingly sophisticated methods and artificial intelligence (AI) technologies
    provide the necessary support. Due to the heterogeneity and fragmentation of crop
    planting, typical classification approaches have a lower classification performance.
    However, the DL technique can detect and categorize crop types effectively and
    has a stronger feature extraction capability. In this aspect, this study designed
    a new remote sensing imagery data analysis using the marine predators algorithm
    with deep learning for food crop classification (RSMPA-DLFCC) technique. The RSMPA-DLFCC
    technique mainly investigates the RS data and determines the variety of food crops.
    In the RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the
    feature extraction process. The MPA is applied for the optimal hyperparameter
    selection process in order to optimize the accuracy of SimAM-EfficientNet architecture.
    MPA, inspired by the foraging behaviors of marine predators, perceptively explores
    hyperparameter configurations to optimize the hyperparameters, thereby improving
    the classification accuracy and generalization capabilities. For crop type detection
    and classification, an extreme learning machine (ELM) model can be used. The simulation
    analysis of the RSMPA-DLFCC technique is performed on two benchmark datasets.
    The extensive analysis of the results portrayed the higher performance of the
    RSMPA-DLFCC approach over existing DL techniques. Keywords: remote sensing images;
    deep learning; crop classification; machine learning; computer vision 1. Introduction
    Recent developments in remote sensing (RS) data and technologies deliver the ability
    of highly accessible, cheap and real time advantages [1]. In recent years, a massive
    quantity of global coverage RS images have been openly available [2]. In particular,
    Landsat 8 satellite offers high-resolution multispectral datasets including wealthy
    data on agricultural vegetation development which is easily accessible. It allows
    us to examine the vegetation growth and forecast the changes over time from past
    to present [3]. RS is an effective data collection technology, and it is broadly
    employed in agriculture, for example, to monitor crop conditions, crop distribution,
    and to predict upcoming food production under various situations [4]. Though current
    agricultural RSs generally use sensors from satellite environments like Landsat
    and MODIS, they combine and integrate the data acquired from the aerial or ground-based
    sensors [5,6]. Even if satellite-borne sensors cover a larger range from a local
    to a national scale, precision agriculture needs remotely sensed data with high
    efficiency, knowledge, and high resolution to sufficiently study crop conditions,
    hence giving support to national food provision security. Aerial or airborne RS
    that uses classical aerial photography taken from aircraft, light aircraft or
    unmanned aerial vehicles (UAVs) as its platform, and gets a higher ground resolution
    of a few centimeters than the satellite image resolution of a few to hundreds
    of meters provides two important advantages: Primarily, significant biochemical
    and biophysical variables can be calculated finely at most of the levels of an
    individual plant, and its images are without mixed pixel effects. Next, important
    phases of crop development can be finely noticed with the use of active and current
    crop height created by classical aerial triangulation technology [7]. Additionally,
    the highly accurate cropland mask, crop-specific categorization and circulation
    gained from airborne sensors provide extra training and validation data for satellite
    observation and additionally increase the respective outcome. Successful integration
    of various sensor sources, wavebands, and time-stamped RS images gives extensive
    feature data about crops [8]. Thus, it is a reasonable and significant study to
    discover the crop classification based on RS images. Classical RS-based image
    classification procedures of ML were slowly used in the classification and detection
    of RS images. These models can be classified as supervised and unsupervised classes.
    The first holds minimum distance, maximum likelihood, and support vector machine
    (SVM). In this phase, the SVM is extensively applied in RS image classification,
    even though few problems exist. DL, referring to a deep neural network, is a type
    of ML technique, and because of its data expression and dominant feature extraction
    capability, it has been widely adopted. Over the years, the identification rate
    of DL on most classical identification processes has enhanced considerably [9].
    Numerous studies have exhibited that DL can extract features from RS imagery and
    enhance the classifier performance. This article develops a remote sensing imagery
    data analysis using the marine predators algorithm with deep learning for food
    crop classification (RSMPA-DLFCC) method. The RSMPA-DLFCC technique mainly investigates
    the RS data and determines the variety of food crops. In the RSMPA-DLFCC technique,
    the SimAM-EfficientNet model is utilized for the feature extraction process. The
    MPA is applied for the optimal parameter selection to optimize the accuracy of
    SimAM-EfficientNet architecture. MPA, inspired by the foraging behaviors of marine
    predators, perceptively explores hyperparameter configurations to optimize the
    hyperparameters, thereby improving the classification accuracy and generalization
    capabilities. For crop type detection and classification, an extreme learning
    machine (ELM) model can be used. The simulation analysis of the RSMPA-DLFCC method
    takes place on the UAV image dataset. The rest of the paper is organized as follows.
    Section 2 provides the related works and Section 3 offers the proposed model.
    Then, Section 4 gives the result analysis and Section 5 concludes the paper. 2.
    Literature Review Kwak and Park [10] examined self training with domain adversarial
    networks (STDAN) to classify crop types. The main function of STDAN is to integrate
    adversarial training for improving spectral discrepancy issues with self training
    in order to create novel trained data in the targeted field, utilizing present
    ground truth details. In [11], a unique structure based on deep CNN (DCNN) and
    the dual attention module (DAM) makes utilization of the Sentinel 2 time series
    dataset which was projected for crop identification. Fresh DAM was applied to
    the removal of enlightened deep features using the advantages of spatial and spectral
    features of Sentinel 2 datasets. Reedha et al. [12] targeted the design of attention-related
    DL networks in a significant technique to state the earlier mentioned complications
    regarding weeds and crop detection with drone systems. The objective is to inspect
    visual transformers (ViT) and implement them in the identification of plants in
    UAV images. In [13], the results of accurate recognition were tested to associate
    the phenology of vegetation products by time series of Landsat8, digital elevation
    model (DEM), and Sentinel 1. Next, based on the agricultural phenology of crops,
    radar Sentinel1 and optical Landsat8 time-series data with DEM were used to enhance
    the performance classification. Sun et al. [14] proposed a technique for attaining
    deduction of fine-scale crops by combining RS information from different satellite
    images by construction of chronological scale crop features inside the parcels
    employing Sentinel 2A, Gaofen-6, and Landsat 8. The authors adopted a feature-equivalent
    technique to fill in the missing values in the time series feature-building methods
    to prevent problems with unidentified crops. Li et al. [15] introduced a scale
    sequence object-based CNN (SS-OCNN) that identifies images at the object phase
    by taking segmented object crop parcels as the primary unit of analysis, therefore
    providing the limits between crop parcels that were defined precisely. Next, the
    segmented object was identified utilizing the CNN approach combined with an automated
    generating scale structure of input patch sizes. Zhai et al. [16] examined the
    contribution of the data to rice planting area mapping. Specifically, the introduction
    of the red-edge band was to build a red-edge agricultural index derived from Sentinel
    2 data. C band quad pol Radar sat 2 data was also utilized. The authors employed
    the random forest technique and finally collaborated with radar and optical data
    to plot rice-planted regions. In [17], the authors designed an enhanced crop planting
    structure to plot the structure for rainy and cloudy regions using collective
    optical data and SAR data. First, the author removed geo parcels from optical
    images with high dimensional resolution. Next, the authors made an RNN-based classification
    appropriate for remote detecting images on a geo parcel scale. 3. The Proposed
    Model This manuscript offered the development of automated food crop classification
    using the RSMPA-DLFCC technique. The RSMPA-DLFCC technique mainly investigates
    the RS data and determines different types of food crops. In the RSMPA-DLFCC technique,
    three major phases of operations are involved, namely the SimAM-EfficientNet feature
    extractor, MPA-based hyperparameter tuning, and ELM classification. Figure 1 represents
    the entire process of the RSMPA-DLFCC approach. Figure 1. Overall process of RSMPA-DLFCC
    algorithm. 3.1. Feature Extraction Using SimAM-EfficientNet Model The RSMPA-DLFCC
    technique applies the SimAM-EfficientNet model to derive feature vectors. A novel
    CNN called EfficientNet was launched by Google researchers [18]. The study uses
    a multi-dimensional hybrid method scaling model making them consider the speed
    and accuracy of the model even though the existing network has advanced considerably
    in speed and accuracy. Through compound scaling factors, ResNet raises the network
    depth to optimize the performance. By improving accuracy and ensuring speed, EfficientNet
    balances the network depth, width, and resolution. EfficientNet-B0 is the initial
    EfficientNet model. The most basic model B0 is: concerning resolution, layers,
    and channels, B1-B7 overall of 7 models adapted from B0. Many existing attention
    modules generate 1D or 2D weights. Next, the weights created are extended for
    channel and spatial attention. Generally, the present attention module faces the
    two subsequent challenges. The former is the attention module could extract features
    through channel and space that results in the flexibility of attention weight.
    Moreover, CNN is influenced by a series of factors and has a complex structure.
    SimAM considers these spaces and channels in contrast to them. Without adding
    parameters, it presents 3D attention weights to the original network. Based on
    neuroscience theory, an energy function can be defined and, in turn, derive a
    solution that converges faster. This operation is executed in ten lines of code.
    An additional benefit of SimAM is that it prevents excessive adjustment to the
    network architecture. Hence, SimAM is lightweight, more flexible, and modular.
    In numerous instances, SimAM is better than the conventional CBAM and SE attention
    models. Figure 2 illustrates the architecture of SimAM-EfficientNet. Figure 2.
    Architecture of SimAM-EfficientNet. The SimAM model defines an energy function
    and looks for important neurons. It adds regular terms and uses binary labels.
    At last, the minimal energy is evaluated by the following expression: 𝑒 ∗ 𝑡 =(4(𝜆+
    𝜎 2 ))/((𝑡−𝑢 ) 2 +2 𝜎 2 +2𝜆)  (1) 𝑢 𝑡 = 1 𝑀−1 ∑ 𝑖=1 𝑀−1 𝑥 𝑖 , 𝜎 2 𝑡 = 1 𝑀−1 ∑
    𝑖=1 𝑀−1 ( 𝑥 𝑖 − 𝑢 𝑡 ) 2   (2) where 𝜇 𝑡 and  𝜎 2 𝑡 are the mean and variance of
    each neuron.  𝑡 is the target neuron. 𝜆 indicates the regularization coefficient.
    Using 𝑀=𝐻×𝑊 , the neuron count on that channel is attained. Finally, the dissimilarity
    between neurons and peripheral neurons is associated with the energy used. The
    implication of all the neurons is evaluated by 1/ 𝑒 ∗ . The scaling operator is
    used to refine the feature and it can be formulated as follows: 𝑋=𝑋·𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (1/𝐸)  (3)
    The 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 function is used to limit the size of the 𝐸 value. In Equation (3),
    𝐸 group each 𝑒 across the channel and spatial sizes. EfficientNet-B0 has a total
    of nine phases. The initial phase is 3×3 convolutional layers. The second to the
    eighth phases are MBConv, which is the building block of these network models.
    The last phase is made up of a pooling layer, a 1×1 convolutional layer, and the
    FC layer. MBConv has five different parts. The initial part is a 1×1 convolutional
    layer. The next part is a depth-wise convolution layer. The third part is the
    SE attention mechanism. The fourth part is a 1×1 convolutional layer for reduction
    dimension. Lastly, the dropout layer lessens the over-fitting problem. After the
    first convolutional layer, the SimAM module was added to increase channel and
    spatial weights. The original EfficientNet comprises the SE attention mechanism.
    The SimAM-EfficientNet is made up of seven SimAM-MBConv models, one FC layer,
    two convolution layers, and one pooling layer. At first, the images with 224×224×3
    dimensions are ascended by the 3×3 convolution layers. The dimensions of the images
    obtained with features are 112×112×32 . Next, the image features are extracted
    by the SimAM-Conv. The connection will be deactivated when both SimAM-Convs are
    the same, and the input will connect. The FC layer is utilized for classification
    and the original channel is restored after 𝑎 1×1 point-wise convolutional layer.
    3.2. Hyperparameter Tuning Using MPA For the optimal hyperparameter selection
    process, the MPA is applied. The MPA is derived from the foraging tactics of the
    ocean predator [19]. MPA is a population-based metaheuristic approach. The optimization
    technique begins with the arbitrary solution. 𝑋 0 = 𝑋 min  +𝑟𝑎𝑛𝑑( 𝑋 max  − 𝑋 min  )
    (4) where 𝑋 min  and 𝑋 max denotes the lower and upper boundaries, and 𝑟𝑎𝑛𝑑 is
    a randomly generated integer in the range [0,1] . In the MPA, Prey and Elite are
    two different matrices with similar dimensions. The optimum solution is selected
    as the fittest predator while creating the Elite matrix. The finding of and search
    for prey is checked through these matrices. 𝑋 → 𝐼 indicates the dominant predator
    vector, 𝑛 is the searching agent, and 𝑑 , the dimension. Both prey and predator
    are the search agents. 𝐸𝑙𝑖𝑡𝑒= ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ 𝑋 𝐼 1,1 𝑋 𝐼 2,1 ⋮ ⋮ 𝑋 𝐼 𝑛,1 𝑋
    𝐼 1,2 𝑋 𝐼 2,2 ⋮ ⋮ 𝑋 𝐼 𝑛,2 … … ⋮ ⋮ … 𝑋 𝐼 1,𝑑 𝑋 𝐼 2,𝑑 ⋮ ⋮ 𝑋 𝐼 𝑛,𝑑 ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥
    ⎥ ⎥ 𝑛𝑥𝑑      (5) where 𝑡ℎ𝑒  𝑗 𝑡ℎ dimension of 𝑖 𝑡ℎ prey is represented as 𝑋 𝑖,𝑗
    . The optimization method is connected to both matrices. Predator uses these matrices
    for updating the position. 𝑃𝑟𝑒𝑦= ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ 𝑋 1,1 𝑋 2,1 𝑋 3,1 ⋮ ⋮ 𝑋
    𝑛,1 𝑋 1,2 𝑋 2,2 𝑋 3,1 ⋮ ⋮ 𝑋 𝑛,2 … … … ⋮ ⋮ … 𝑋 1,𝑑 𝑋 2,𝑑 𝑋 3,𝑑 ⋮ ⋮ 𝑋 𝑛,𝑑 ⎤ ⎦ ⎥
    ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 𝑛𝑥𝑑 (6) In the MPA, there are three stages discussed in detail.
    Phase 1 occurs if <(( Max − 𝐼𝑡𝑒𝑟)/3) . 𝐼𝑡𝑒𝑟 and Max − 𝐼𝑡𝑒𝑟  denote the existing
    and maximal iteration counter. 𝑃 shows the constant number with the value of 0.5.
    The appropriate tactic is one where the predator should stop. In Equation (7)
    of stage 1, vector 𝑅 𝐵 portrays the Brownian motion and uniformly distributed
    random number in [0,1]. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → ⊗( 𝐸𝑙𝑖𝑡𝑒   
        i − 𝑅 → B ⊗ 𝑃𝑟𝑒𝑦        𝑖 )𝑖=1, …𝑛   𝑃𝑟𝑒𝑦        i = 𝑃𝑟𝑒𝑦
           i +𝑃· 𝑅 → ⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (7) Phase 2 realized
    if (( Max − 𝐼𝑡𝑒𝑟)/3)<𝐼𝑡𝑒𝑟<((2 Max − 𝐼𝑡𝑒𝑟)/3 . Once the prey movement is Lévy,
    then the predator movement should be Brownian. The prey is responsible for exploitation,
    and the predator is responsible for exploration. The multiplication of 𝑅 → 𝐿 and
    𝑃𝑟𝑒𝑦 represent the prey movement, and the prey movement can be exemplified by
    adding the 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒 to the prey position. The 𝑅 → 𝐿 vector is a random number
    representing Lévy motion. CF denotes an adaptive parameter. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒 for the predator
    movement can be controlled by the CF. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → L
    ⊗( 𝐸𝑙𝑖𝑡𝑒        i − 𝑅 → L ⊗ 𝑃𝑟𝑒𝑦        i )𝑖=1, …𝑛/2 𝑃𝑟𝑒𝑦   
        i = 𝑃𝑟𝑒𝑦        i +𝑃· 𝑅 → ⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (8)
    𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → B ⊗( 𝑅 → B ⊗ 𝐸𝑙𝑖𝑡𝑒        i − 𝑃𝑟𝑒𝑦
           𝑖 )𝑖= 𝑛 2 , …𝑛 𝑃𝑟𝑒𝑦        i = 𝐸𝑙𝑖𝑡𝑒        i +𝑃·𝐶𝐹⊗
    𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 𝐶𝑃= (1− 𝐼𝑡𝑒𝑟 𝑀𝑎 𝑥 − 𝐼𝑖𝑒𝑟 ) (2 𝑙𝑡𝑒𝑟 𝑀𝑎 𝑥 − 𝐼𝑡𝑒𝑟
    )   (9) Phase 3 occurs If >((2 Max − 𝐼𝑡𝑒𝑟)/3) . As the optimum strategy, the predator
    movement is Lévy. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → L ⊗( 𝑅 → L ⊗ 𝐸𝑙𝑖𝑡𝑒 
          i − 𝑃𝑟𝑒𝑦        i )𝑖=1, …𝑛 𝑃𝑟𝑒𝑦        i = 𝐸𝑙𝑖𝑡𝑒  
         i +𝑃·𝐶𝐹⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (10) The factors including
    fish aggregating devices (FADs) or eddy formation may affect the predator strategy
    are called the FADs effect. 𝑟 is a randomly generated value within [0,1].  𝑈 →
    shows the 𝑏𝑖𝑛𝑎𝑟𝑦 vector with an array of 0 and 1. 𝑟1 and 𝑟2 depict the random
    indexes of prey matrices. 𝑋 → min and 𝑋 → 𝑚𝑎𝑥 denote the lower and upper boundaries
    of the dimension. 𝑃𝑟𝑒𝑦        i = ⎧ ⎩ ⎨     𝑃𝑟𝑒𝑦        i +𝐶𝑃[
    𝑋 → 𝑚𝑖𝑛 + 𝑅 → ⊗( 𝑥 → 𝑚𝑎𝑥 − 𝑋 → 𝑚𝑖𝑛 )]⊗ 𝑈 → , 𝑟≤𝐹𝐴𝐷𝑠 𝑃𝑟𝑒𝑦        i +[𝐹𝐴𝐷𝑠(1−𝑟)+𝑟](
    𝑃𝑟𝑒𝑦        i − 𝑃𝑟𝑒𝑦        i ), 𝑟>𝐹𝐴𝐷𝑠   (11) The fitness selection
    is a major factor in the MPA technique. An encoded solution is used for evaluating
    the outcome of the solution candidate. The accuracy values are the foremost conditions
    used to design an FF. 𝐹𝑖𝑡𝑛𝑒𝑠𝑠= max (𝑃) (12) 𝑃= 𝑇𝑃 𝑇𝑃+𝐹𝑃 (13) where 𝑇𝑃 and 𝐹𝑃 represent
    the true and false positive values. 3.3. Classification Using ELM Model The ELM
    algorithm is applied for the automated detection and classification of food crops.
    The ELM model is used to generate the weight between the hidden and the input
    layers at random, and during the training process, it does not need to be adjusted
    and only needs to set the number of HL neurons in order to attain an optimum result
    [20]. Assume 𝑁 arbitrary sample (𝑋, 𝑡) , where 𝑋 𝑗 =[ 𝑥 𝑗1 ,  𝑥 𝑗2 … 𝑥 𝑗𝑛 ] 𝑇
    ∈ 𝑅 𝑛 , 𝑡 𝑖 =[ 𝑡 𝑖1 ,  𝑡 𝑖2 … 𝑡 𝑖𝑚 ] 𝑇 ∈ R is formulated by ∑ 𝑖=1 𝐿 𝛽 𝑖 𝑔( 𝑊 𝑖
    ⋅ 𝑋 𝑗 + 𝑏 𝑖 )= 𝑡 𝑗 ,𝑗=1,…,𝑁 (14) The weight of 𝑖 𝑡ℎ  neurons in the input layer
    and HL is 𝑊 𝑖 = [ 𝑤 𝑖1 ,  𝑤 𝑖2 … 𝑤 𝑖𝑛 ] 𝑇 , chosen at random. The resultant weight
    is  𝛽 𝑖 , and the learning objective is to obtain the fittest 𝛽 𝑖 . The 𝑗 𝑡ℎ input
    vector is  𝑋 𝑗 . The inner product of 𝑊 𝑖 and 𝑋 𝑗 is 𝑊 𝑖 ⋅ 𝑋 𝑗 . The bias of 𝑖
    𝑡ℎ  HL neuron is 𝑏 𝑖 . The set non-linear activation function is 𝑔(𝑥) . The output
    vector of the 𝑖 𝑡ℎ neurons is 𝑔( 𝑊 𝑖 ⋅ 𝑋 𝑗 + 𝑏 𝑖 ) . The target vector attained
    from the 𝑗 𝑡ℎ input vector is 𝑡 𝑗 . It can be represented in the matrix form:
    𝐻𝛽=𝑇 𝐻( 𝑊 1 , …,  𝑊 𝐿 ,  𝑏 1 , …,  𝑏 𝐿 , …,  𝑋 1 , …,  𝑋 𝐿 ) = ⎡ ⎣ ⎢ ⎢ ⎢ 𝑔( 𝑊
    1 ⋅ 𝑋 1 + 𝑏 1 ) ⋮ 𝑔( 𝑊 1 ⋅ 𝑋 𝑁 + 𝑏 1 ) … ⋱ … 𝑔( 𝑊 𝐿 ⋅ 𝑋 𝑁 + 𝑏 1 ) ⋮ 𝑔( 𝑊 𝐿 ⋅ 𝑋
    𝑁 + 𝑏 1 ) ⎤ ⎦ ⎥ ⎥ ⎥ 𝛽= ⎡ ⎣ ⎢ ⎢ ⎢ 𝛽 𝑇 1 ⋮ 𝛽 𝑇 𝐿 ⎤ ⎦ ⎥ ⎥ ⎥  𝑎𝑛𝑑 𝑇= ⎡ ⎣ ⎢ ⎢ ⎢ 𝑇 𝑇
    1 ⋮ 𝑇 𝑇 𝐿 ⎤ ⎦ ⎥ ⎥ ⎥   (15) The output of the HL node is  𝐻 , the output weight
    is  𝛽 , and the desired output is 𝑇 . The following equation is used to get 𝑊
    ̂ 𝑖 , 𝛽 ̂ 𝑖 ,  𝑏 ̂ 𝑖 as follows: ‖𝐻( 𝑊 ̂ 𝑖 , 𝑏 ̂ 𝑖 ) 𝛽 ̂ i −𝑇‖= min 𝑊,𝑏,𝛽 ‖𝐻(
    𝑤 𝑖 ,  𝑏 𝑖 ) 𝛽 𝑖 −𝑇‖, 𝑖=1,…,𝐿 (16) As shown in Equation (17), this corresponds
    to minimalizing the loss function, 𝐸= ∑ 𝑗=1 𝑁 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ∑ 𝑖=1 𝐿 𝛽 𝑖 𝑔( 𝑊 𝑖
    ⋅ 𝑋 𝑗 + 𝑏 𝑖 )− 𝑡 𝑗 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ 2 (17) Since the HL offset and the input weight
    𝑊 𝑖 are determined randomly, then the output matrix of HL is also defined. As
    shown in Equation (18), the training purpose is transmuted into resolving a linear
    formula 𝐻𝛽=𝑇 : 𝛽 ̂ = 𝐻 + 𝑇 (18) where the optimum output weight is 𝛽 ̂ . The Moore–Penrose
    generalized the inverse of 𝐻 matrix is 𝐻 + , and it is shown that the norm of
    the obtained solution is unique and minimal. Thus, ELM has better robustness and
    generalization. 4. Results Analysis The proposed model is simulated using the
    Python 3.8.5 tool. The proposed model is experimented on PC i5-8600k, GeForce
    1050Ti 4 GB, 16 GB RAM, 250 GB SSD, and 1 TB HDD. The food crop classification
    performance of the RSMPA-DLFCC system is validated on the UAV image dataset [21],
    comprising 6450 samples with six classes. For experimental validation, we have
    used 80:20 and 70:30 of training (TR)/testing (TS) set. Figure 3 demonstrates
    the confusion matrices produced by the RSMPA-DLFCC technique under 80:20 and 70:30
    of the TR phase/TS phase. The experimental values specified the efficient recognition
    of all six classes. Figure 3. Confusion matrices of (a,b) 80:20 of TR phase/TS
    phase and (c,d) 70:30 of TR phase/TS phase. In Table 1 and Figure 4, the food
    crop classification analysis of the RSMPA-DLFCC methodology is calculated at 80:20
    of the TR phase/TS phase. The observational data specified that the RSMPA-DLFCC
    system properly categorizes seven types of crops. With 80% of the TR phase, the
    RSMPA-DLFCC technique offers an average 𝑎𝑐𝑐 𝑢 𝑦 of 98.12%, 𝑝𝑟𝑒 𝑐 𝑛 of 93.23%,
    𝑟𝑒𝑐 𝑎 𝑙 of 90.76%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.89%, and MCC of 90.77%. Additionally, with 20%
    of TS phase, the RSMPA-DLFCC method offers an average 𝑎𝑐𝑐 𝑢 𝑦 of 98.22%, 𝑝𝑟𝑒 𝑐
    𝑛 of 93.06%, 𝑟𝑒𝑐 𝑎 𝑙 of 90.42%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.57%, and MCC of 90.56%, respectively.
    Figure 4. Average of RSMPA-DLFCC algorithm at 80:20 of TR phase/TS phase. Table
    1. Food crop classifier outcome of RSMPA-DLFCC algorithm at 80:20 of TR phase/TS
    phase. In Table 2 and Figure 5, the food crop classification analysis of the RSMPA-DLFCC
    technique is calculated at 70:30 of TR Phase/TS Phase. The experimental values
    indicate that the RSMPA-DLFCC technique appropriately categorizes seven types
    of crops. With 70% of the TR phase, the RSMPA-DLFCC algorithm offers an average
    𝑎𝑐𝑐 𝑢 𝑦 of 97.98%, 𝑝𝑟𝑒 𝑐 𝑛 of 91.79%, 𝑟𝑒𝑐 𝑎 𝑙 of 88.64%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 90.02%, and
    MCC of 88.90%, respectively. In addition, with 30% of TS phase, the RSMPA-DLFCC
    system offers average 𝑎𝑐𝑐 𝑢 𝑦 of 98.07%, 𝑝𝑟𝑒 𝑐 𝑛 of 92.13%, 𝑟𝑒𝑐 𝑎 𝑙 of 90.13%,
    𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.06%, and MCC of 89.92%, correspondingly. Figure 5. Average of RSMPA-DLFCC
    algorithm at 70:30 of TR phase/TS phase. Table 2. Food crop classifier outcome
    of RSMPA-DLFCC algorithm at 70:30 of TR phase/TS phase. To calculate the performance
    of the RSMPA-DLFCC methodology on 80:20 of TR Phase/TS Phase, TR and TS 𝑎𝑐𝑐 𝑢
    𝑦 curves are defined, as shown in Figure 6. The TR and TS 𝑎𝑐𝑐 𝑢 𝑦 curves demonstrate
    the performance of the RSMPA-DLFCC technique over numerous epochs. The figure
    offers the details about the learning task and generalization capabilities of
    the RSMPA-DLFCC system. With a rise in epoch count, it is observed that the TR
    and TS 𝑎𝑐𝑐 𝑢 𝑦 curves attained are enhanced. It is noted that the RSMPA-DLFCC
    approach enriches testing accuracy that has the ability to identify the patterns
    in the TR and TS data. Figure 6. Accuy curve of RSMPA-DLFCC algorithm at 80:20
    of TR phase/TS phase. Figure 7 illustrates an overall TR and TS loss value of
    the RSMPA-DLFCC methodology on 80:20 of TR Phase/TS Phase over epochs. The TR
    loss shows the model loss acquired reduces over epochs. Mainly, the loss values
    are decreased as the model adapts the weight to diminish the predicted error on
    the TR and TS data. The loss analysis illustrates the level where the model is
    fitting the training data. It is evidenced that the TR and TS loss is progressively
    minimized and described that the RSMPA-DLFCC technique effectively learns the
    patterns revealed in the TR and TS data. It is also observed that the RSMPA-DLFCC
    methodology modifies the parameters for reducing the difference between the predicted
    and actual training labels. Figure 7. Loss curve of RSMPA-DLFCC algorithm at 80:20
    of TR phase/TS phase. The PR curve of the RSMPA-DLFCC approach on 80:20 of TR
    phase/TS phase, illustrated by plotting precision against recall as described
    in Figure 8, confirms that the RSMPA-DLFCC technique achieves improved PR values
    under all classes. The figure represents that the model learns to identify different
    class labels. The RSMPA-DLFCC achieves improved effectiveness in the recognition
    of positive samples with reduced false positives. Figure 8. PR curve of RSMPA-DLFCC
    algorithm at 80:20 of TR/TS phase. The ROC analysis, provided by the RSMPA-DLFCC
    system on 80:20 of TR phase/TS phase demonstrated in Figure 9, has the ability
    the differentiate between class labels. The figure shows valuable insights into
    the trade-off between the TPR and FPR rates over dissimilar classification thresholds
    and differing numbers of epochs. It introduces the accurately predicted performance
    of the RSMPA-DLFCC methodology on the classification of various classes. Figure
    9. ROC curve of RSMPA-DLFCC algorithm at 80:20 of TR/TS phase. In Table 3, detailed
    comparative results of the RSMPA-DLFCC technique are demonstrated with current
    models [22,23]. Figure 10 investigates a comparative analysis of the RSMPA-DLFCC
    with recent approaches in terms of 𝑎𝑐𝑐 𝑢 𝑦 . The experimental values highlighted
    that the RSMPA-DLFCC technique reaches an increased 𝑎𝑐𝑐 𝑢 𝑦  of 98.22%, whereas
    the SBODL-FCC, DNN, AlexNet, VGG-16, ResNet, and SVM models obtain decreased 𝑎𝑐𝑐
    𝑢 𝑦 values of 97.43%, 86.23%, 90.49%, 90.35%, 87.70%, and 86.69%, respectively.
    Figure 10. Accuy Comparative outcome of RSMPA-DLFCC algorithm with other systems.
    Table 3. Comparative outcome of RSMPA-DLFCC with other systems. Figure 11 investigates
    a comparative analysis of the RSMPA-DLFCC system with recent techniques, with
    respect to 𝑝𝑟𝑒 𝑐 𝑛 and  𝑟𝑒𝑐 𝑎 𝑙 . The observational data highlighted that the
    RSMPA-DLFCC system attains a raised 𝑃𝑟𝑒 𝑐 𝑛  of 93.06%, while the SBODL-FCC, DNN,
    AlexNet, VGG-16, ResNet, and SVM methods obtain reduced 𝑝𝑟𝑒 𝑐 𝑛 values of 89.02%,
    86.11%, 87.68%, 85.28%, 86.42%, and 87.99%, correspondingly. In addition, the
    RSMPA-DLFCC system attains  𝑟𝑒𝑐 𝑎 𝑙 values of 90.42% whereas SBODL-FCC, DNN, AlexNet,
    VGG-16, ResNet, and SVM systems get decreased 𝑟𝑒𝑐 𝑎 𝑙 values of 85.03%, 84.39%,
    81.7%, 81.35%, 81.18%, and 83.61%, respectively. These experimental data indicated
    that the RSMPA-DLFCC methodology reaches the maximum food crop classification
    process. Figure 11. Comparative outcome of RSMPA-DLFCC algorithm with other systems.
    5. Conclusions This manuscript offered the development of automated food crop
    classification using the RSMPA-DLFCC technique. The RSMPA-DLFCC technique mainly
    investigates the RS data and determines different types of food crops. In the
    RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the feature
    extraction process. The MPA is applied for the optimum hyperparameter selection
    in order to optimize the accuracy of SimAM-EfficientNet architecture. The simulation
    analysis of the RSMPA-DLFCC method takes place on benchmark UAV image dataset.
    The widespread result analysis portrayed the higher performance of the RSMPA-DLFCC
    approach over existing DL models, with a maximum accuracy of 98.22%. In future
    work, real-time remote sensing data will be a priority, enabling the model to
    adapt dynamically to changing crop conditions and emerging threats. Moreover,
    future work can focus on the integration of multi-modal data sources, such as
    thermal imaging or hyperspectral data, and will broaden the scope of crop classification,
    providing a more comprehensive understanding of crop health and types. Finally,
    field tests can be performed to assess the real-world performance and accuracy
    of the RSMPA-DLFCC technique in diverse agricultural settings and will be essential
    for its practical deployment and validation. Author Contributions Conceptualization,
    A.S.A. and H.A.M.; methodology, H.A.M.; software, M.K.S.; validation, A.S.A.,
    H.A.M. and M.K.S.; formal analysis, A.M.; investigation, K.M.O.; resources, A.M.;
    data curation, A.S.A.; writing—original draft preparation, A.S.A., H.A.M., M.K.S.,
    K.M.O., F.A.A. and A.M.; writing—review and editing, H.A.M., F.A.A. and M.K.S.;
    visualization, F.A.A.; supervision, H.A.M.; project administration, M.K.S.; funding
    acquisition, H.A.M. and M.K.S. All authors have read and agreed to the published
    version of the manuscript. Funding The authors extend their appreciation to the
    Deanship of Scientific Research at King Khalid University for funding this work
    through large group Research Project under grant number (RGP2/117/44). Princess
    Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2023R114),
    Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia. Research Supporting
    Project number (RSPD2023R838), King Saud University, Riyadh, Saudi Arabia. This
    study is partially funded by the Future University in Egypt (FUE). Institutional
    Review Board Statement Not applicable. Data Availability Statement Data sharing
    is not applicable to this article as no datasets were generated during the current
    study. Conflicts of Interest The authors declare that they have no conflicts of
    interest. The manuscript was written with contributions of all authors. All authors
    have given approval to the final version of the manuscript. References Joshi,
    A.; Pradhan, B.; Gite, S.; Chakraborty, S. Remote-Sensing Data and Deep-Learning
    Techniques in Crop Mapping and Yield Prediction: A Systematic Review. Remote Sens.
    2023, 15, 2014. [Google Scholar] [CrossRef] Bouguettaya, A.; Zarzour, H.; Kechida,
    A.; Taberkit, A.M. Deep learning techniques to classify agricultural crops through
    UAV imagery: A review. Neural Comput. Appl. 2022, 34, 9511–9536. [Google Scholar]
    [CrossRef] [PubMed] Zhao, H.; Duan, S.; Liu, J.; Sun, L.; Reymondin, L. Evaluation
    of five deep learning models for crop type mapping using sentinel-2 time se ries
    images with missing information. Remote Sens. 2021, 13, 2790. [Google Scholar]
    [CrossRef] Orynbaikyzy, A.; Gessner, U.; Conrad, C. Crop type classification using
    a combination of optical and radar remote sensing data: A review. Int. J. Remote
    Sens. 2019, 40, 6553–6595. [Google Scholar] [CrossRef] de Azevedo, R.P.; Dallacort,
    R.; Boechat, C.L.; Teodoro, P.E.; Teodoro, L.P.R.; Rossi, F.S.; Correia Filho,
    W.L.F.; Della-Silva, J.L.; Baio, F.H.R.; Lima, M.; et al. Remotely sensed imagery
    and machine learning for mapping of sesame crop in the Brazilian Midwest. Remote
    Sens. Appl. Soc. Environ. 2023, 32, 101018. [Google Scholar] [CrossRef] Wang,
    L.; Wang, J.; Liu, Z.; Zhu, J.; Qin, F. Evaluation of a deep-learning model for
    multispectral remote sensing of land use and crop classification. Crop J. 2022,
    10, 1435–1451. [Google Scholar] [CrossRef] Dash, R.; Dash, D.K.; Biswal, G.C.
    Classification of crop based on macronutrients and weather data using machine
    learning techniques. Results Eng. 2021, 9, 100203. [Google Scholar] [CrossRef]
    Kuang, X.; Guo, J.; Bai, J.; Geng, H.; Wang, H. Crop-Planting Area Prediction
    from Multi-Source Gaofen Satellite Images Using a Novel Deep Learning Model: A
    Case Study of Yangling District. Remote Sens. 2023, 15, 3792. [Google Scholar]
    [CrossRef] Suchi, S.D.; Menon, A.; Malik, A.; Hu, J.; Gao, J. Crop identification
    based on remote sensing data using machine learning approaches for fresno county,
    California. In Proceedings of the 2021 IEEE Seventh International Conference on
    Big Data Computing Service and Applications (BigDataService), Oxford, UK, 23–26
    August 2021; pp. 115–124. [Google Scholar] Kwak, G.H.; Park, N.W. Unsupervised
    domain adaptation with adversarial self-training for crop classification using
    remote sensing images. Remote Sens. 2022, 14, 4639. [Google Scholar] [CrossRef]
    Seydi, S.T.; Amani, M.; Ghorbanian, A. A dual attention convolutional neural network
    for crop classification using time-series Sentinel-2 imagery. Remote Sens. 2022,
    14, 498. [Google Scholar] [CrossRef] Reedha, R.; Dericquebourg, E.; Canals, R.;
    Hafiane, A. Transformer neural network for weed and crop classification of high
    resolution UAV images. Remote Sens. 2022, 14, 592. [Google Scholar] [CrossRef]
    Kordi, F.; Yousefi, H. Crop classification based on phenology information by using
    time series of optical and synthetic-aperture radar images. Remote Sens. Appl.
    Soc. Environ. 2022, 27, 100812. [Google Scholar] [CrossRef] Sun, Y.; Yao, N.;
    Luo, J.; Leng, P.; Liu, X. A spatiotemporal collaborative approach for precise
    crop planting structure mapping based on multi-source remote-sensing data. Int.
    J. Remote Sens. 2023, 1–17. [Google Scholar] [CrossRef] Li, H.; Zhang, C.; Zhang,
    Y.; Zhang, S.; Ding, X.; Atkinson, P.M. A Scale Sequence Object-based Convolutional
    Neural Network (SS-OCNN) for crop classification from fine spatial resolution
    remotely sensed imagery. Int. J. Digit. Earth 2021, 14, 1528–1546. [Google Scholar]
    [CrossRef] Zhai, P.; Li, S.; He, Z.; Deng, Y.; Hu, Y. Collaborative mapping rice
    planting areas using multisource remote sensing data. In Proceedings of the 2021
    IEEE International Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium,
    11–16 July 2021; pp. 5969–5972. [Google Scholar] Sun, Y.; Luo, J.; Wu, T.; Zhou,
    Y.N.; Liu, H.; Gao, L.; Dong, W.; Liu, W.; Yang, Y.; Hu, X.; et al. Synchronous
    response analysis of features for remote sensing crop classification based on
    optical and SAR time-series data. Sensors 2019, 19, 4227. [Google Scholar] [CrossRef]
    [PubMed] You, H.; Lu, Y.; Tang, H. Plant disease classification and adversarial
    attack using SimAM-EfficientNet and GP-MI-FGSM. Sustainability 2023, 15, 1233.
    [Google Scholar] [CrossRef] Baştemur Kaya, C. A Novel Hybrid Method Based on the
    Marine Predators Algorithm and Adaptive Neuro-Fuzzy Inference System for the Identification
    of Nonlinear Systems. Symmetry 2023, 15, 1765. [Google Scholar] [CrossRef] Zhou,
    S.; Tan, B. Electrocardiogram soft computing using hybrid deep learning CNN-ELM.
    Appl. Soft Comput. 2020, 86, 105778. [Google Scholar] [CrossRef] Rineer, J.; Beach,
    R.; Lapidus, D.; O’Neil, M.; Temple, D.; Ujeneza, N.; Cajka, J.; Chew, R. Drone
    Imagery Classification Training Dataset for Crop Types in Rwanda. Version 1.0,
    Radiant MLHub. 2021. Available online: https://mlhub.earth/data/rti_rwanda_crop_type
    (accessed on 13 June 2023). Ahmed, M.A.; Aloufi, J.; Alnatheer, S. Satin Bowerbird
    Optimization with Convolutional LSTM for Food Crop Classification on UAV Imagery.
    IEEE Access 2023, 11, 41075–41083. [Google Scholar] [CrossRef] Chew, R.; Rineer,
    J.; Beach, R.; O’Neil, M.; Ujeneza, N.; Lapidus, D.; Miano, T.; Hegarty-Craver,
    M.; Polly, J.; Temple, D.S. Deep neural networks and transfer learning for food
    crop identification in UAV images. Drones 2020, 4, 7. [Google Scholar] [CrossRef]
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all
    publications are solely those of the individual author(s) and contributor(s) and
    not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility
    for any injury to people or property resulting from any ideas, methods, instructions
    or products referred to in the content.  © 2023 by the authors. Licensee MDPI,
    Basel, Switzerland. This article is an open access article distributed under the
    terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Almasoud, A.S.; Mengash, H.A.; Saeed, M.K.;
    Alotaibi, F.A.; Othman, K.M.; Mahmud, A. Remote Sensing Imagery Data Analysis
    Using Marine Predators Algorithm with Deep Learning for Food Crop Classification.
    Biomimetics 2023, 8, 535. https://doi.org/10.3390/biomimetics8070535 AMA Style
    Almasoud AS, Mengash HA, Saeed MK, Alotaibi FA, Othman KM, Mahmud A. Remote Sensing
    Imagery Data Analysis Using Marine Predators Algorithm with Deep Learning for
    Food Crop Classification. Biomimetics. 2023; 8(7):535. https://doi.org/10.3390/biomimetics8070535
    Chicago/Turabian Style Almasoud, Ahmed S., Hanan Abdullah Mengash, Muhammad Kashif
    Saeed, Faiz Abdullah Alotaibi, Kamal M. Othman, and Ahmed Mahmud. 2023. \"Remote
    Sensing Imagery Data Analysis Using Marine Predators Algorithm with Deep Learning
    for Food Crop Classification\" Biomimetics 8, no. 7: 535. https://doi.org/10.3390/biomimetics8070535
    Article Metrics Citations No citations were found for this article, but you may
    check on Google Scholar Article Access Statistics Article access statistics Article
    Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0 250
    500 750 1000 1250 For more information on the journal statistics, click here.
    Multiple requests from the same IP address are counted as one view.   Biomimetics,
    EISSN 2313-7673, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Biomimetics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Remote Sensing Imagery Data Analysis Using Marine Predators Algorithm with
    Deep Learning for Food Crop Classification
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Thakur A.
  - Venu S.
  - Gurusamy M.
  citation_count: '3'
  description: Agriculture represents an essential aspect of human existence, providing
    the sustenance necessary for survival in the form of food and various other products.
    Additionally, it serves as a foundational pillar of economic development, offering
    employment and income opportunities to countless individuals. The incorporation
    of machine vision in agriculture has emerged as a crucial technology, enabling
    farmers to automate various tasks, such as crop monitoring and yield prediction
    using cameras and image processing techniques and even providing real-time information
    on crop maturity for harvest planning. This not only helps increase efficiency
    and productivity but also provides valuable insights for precision agriculture,
    enabling more quick and informed decision-making and ultimately leading to improved
    crop yields and financial returns. This scholarly work conducts a thorough examination
    of the various components that comprise a machine vision system, specifically
    delving into the techniques of image acquisition, processing, and classification.
    It also explores the methods employed within each of these techniques, and how
    the combination of such processes is used to perform various agricultural activities
    such as weeding, seeding, harvesting, fruit counting, overlapping, sorting, etc.
    Furthermore, it aims to guide on how the knowledge gained can be applied to build
    practical machine vision systems for agriculture. Additionally, this work highlights
    the research gaps, address current problems that can be solved and its potential
    for future advancement in the field of agriculture, and guides readers towards
    promising areas for future endeavours.
  doi: 10.1016/j.compag.2023.108146
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Machine vision system
    3. Application using the mentioned techniques in various domains 4. Conclusion
    Declaration of Competing Interest Data availability References Further reading
    Show full outline Cited by (4) Figures (26) Show 20 more figures Tables (12) Table
    1 Table 2 Table 3 Table 4 Table 5 Table 6 Show all tables Computers and Electronics
    in Agriculture Volume 212, September 2023, 108146 Review An extensive review on
    agricultural robots with a focus on their perception systems Author links open
    overlay panel Abhishek Thakur, Sangeeth Venu, Muralimohan Gurusamy Show more Add
    to Mendeley Share Cite https://doi.org/10.1016/j.compag.2023.108146 Get rights
    and content Highlights • Extensive review has been done related to the perception
    system utilized in autonomous agriculture robots. • The study will help the researchers
    know about the recent trends in agricultural robots. Abstract Agriculture represents
    an essential aspect of human existence, providing the sustenance necessary for
    survival in the form of food and various other products. Additionally, it serves
    as a foundational pillar of economic development, offering employment and income
    opportunities to countless individuals. The incorporation of machine vision in
    agriculture has emerged as a crucial technology, enabling farmers to automate
    various tasks, such as crop monitoring and yield prediction using cameras and
    image processing techniques and even providing real-time information on crop maturity
    for harvest planning. This not only helps increase efficiency and productivity
    but also provides valuable insights for precision agriculture, enabling more quick
    and informed decision-making and ultimately leading to improved crop yields and
    financial returns. This scholarly work conducts a thorough examination of the
    various components that comprise a machine vision system, specifically delving
    into the techniques of image acquisition, processing, and classification. It also
    explores the methods employed within each of these techniques, and how the combination
    of such processes is used to perform various agricultural activities such as weeding,
    seeding, harvesting, fruit counting, overlapping, sorting, etc. Furthermore, it
    aims to guide on how the knowledge gained can be applied to build practical machine
    vision systems for agriculture. Additionally, this work highlights the research
    gaps, address current problems that can be solved and its potential for future
    advancement in the field of agriculture, and guides readers towards promising
    areas for future endeavours. Previous article in issue Next article in issue Keywords
    Precision agricultureMachine learningImage processingComputer visionSmart farmingAgricultural
    automationDeep learning 1. Introduction Agriculture is the science or practice
    of cultivating land to grow crops and raise livestock. An agricultural system
    is a combination of machines and equipment used to carry out these tasks. The
    agricultural system encompasses various activities such as harvesting, planting,
    weeding, picking, and sorting. These operations often present challenges such
    as identifying good and bad crops and addressing the efficiency and effectiveness
    of these tasks. All agricultural systems require human involvement to varying
    degrees, but this can also lead to issues such as fatigue, labour shortages, and
    adverse working conditions, which can negatively impact efficiency, increase costs,
    and lead to accidents and longer working hours. To offer sustainable agriculture
    it is entirely necessary to introduce modern technology such as blockchain, big
    data, machine vision, IOT, machine learning to meet future agricultural needs
    (Cravero et al., 2022). The global population is nearly 350 crore people and the
    land currently in use is just not enough to feed the people. According to the
    food and agriculture organization forecast, global food production should increase
    to 70% as the population of the world will reach about 9.1 billion by 2050 (Unitednations,
    2021, UNICEF, 2021). As per the findings of a study conducted by Roser et al.
    (Roser, 2013), there has been an increase in individuals abandoning agriculture
    as their primary activity over the past decade, owing to a decline in income within
    the agricultural sector. In the next 10 to 20 years agriculture will suffer a
    lack of manpower but with a predicted increase in the consumption of food. Considering
    these labour-related difficulties, agriculture is moving towards the incorporation
    of modern machinery that is based on automation and robotics. These systems require
    minimal human intervention and can perform operations with a high degree of efficiency,
    thereby increasing productivity. With the adoption of such systems, a team of
    as few as four people can manage a large agricultural field, which would traditionally
    require 50 individuals using conventional agricultural practices. The central
    component in such robotics and automation systems is machine vision, which empowers
    these machines to operate incessantly in various domains, executing tasks with
    unparalleled efficiency that surpasses human capabilities, even during extended
    periods of time. Seeding, harvesting, and pesticide spraying are some labour-intensive
    agricultural activities that can be performed using very minimal labour in a more
    efficient and hassle-free manner using robots equipped with vision systems. A
    machine vision system is a complex process combining various steps that can extract
    high-value information from a camera source and can bring meaningful results (Brill
    et al., 2020). This survey paper mainly focuses on advances in the machine vision
    systems in agriculture and briefly explains the processes that are currently used
    for the image acquisition system, image processing, and image classifications.
    Moreover, this paper discusses the techniques of machine vision being used in
    different agricultural processes, such as harvesting, seeding, weeding, spraying,
    pick and place operations, and pathfinding for various autonomous vehicles. This
    paper will also elaborate on the various methodologies, algorithms, and operating
    principles used for different agricultural process-specific robots. The contributions
    of this work are significant in several aspects. Firstly, this scholarly examination
    provides a comprehensive overview of the incorporation of machine vision systems
    in agriculture, highlighting its importance in addressing the challenges faced
    by the industry. Secondly, this work delves into the technical details of machine
    vision systems, focusing on image acquisition, processing, and classification
    techniques. By exploring the methods employed within each of these techniques,
    readers gain a deep understanding of how machine vision systems operate in the
    agricultural context. This knowledge can be applied to develop practical machine
    vision systems for various agricultural processes, including weeding, seeding,
    harvesting, fruit counting, sorting, and more. Furthermore, this work highlights
    the current research gaps and addresses existing problems that can be solved through
    advancements in machine vision technology. By disseminating this knowledge, it
    contributes to the ongoing progress of agricultural practices, ultimately benefiting
    farmers, consumers, and the global population. Overall, this work serves as a
    valuable resource for researchers, industry professionals, and policymakers interested
    in the implementation of machine vision systems in agriculture. 2. Machine vision
    system A machine vision system refers the computer’s ability to perceive environment
    like human vision (Nixon and Aguado, 2020). The human eye is limited in its ability
    to perceive light, with a responsiveness range of 390 nm to 770 nm (Dattner and
    Bohn, 2016). However, cameras can detect a broader range of wavelengths, including
    infrared, X-rays. This expands the range of for machine vision. Typically, agriculture
    machine vision can be used for numerous processes such as Weeding, Pesticide spraying,
    Harvesting, Sorting, Seeding, Counting, Yield estimation, Pick and place operations,
    Path-finding, Arranging, and Soil classification with the help of complex interrelated
    systems of devices in which singular or several cameras of same or different types
    are used to acquire images of interest, an illuminating light source provided
    on the system for sufficient lighting over the object for grabbing high-resolution
    image using singular or multiple digital cameras, micro-controllers or microprocessors
    either embedded on the mechanical system or a faraway computer by the use of wireless
    communication. The software processes perform various operations over the camera
    images. Thereafter, the decision-making algorithm provides predictive results.
    Finally, the result obtained after processing are used as signals to actuate the
    mechanical system, such as robots, rovers, manipulators, etc. The MVS system can
    be generalized into three categories (Lee-Post, 2003) as mentioned below. • Image
    acquisition: Image acquisition involves the utilization of a variety of cameras,
    illumination sources, and equipment to acquire the necessary data and high-resolution
    images essential for a particular application. • Image processing: Image processing
    is a crucial step in the overall machine vision system. The raw data obtained
    is subject to processing prior to classification or training. This step encompasses
    transformation, analysis, cleaning, filling of missing data, image correction
    techniques, noise filtering. • Image Classification: This step of image processing
    is crucial for comprehending the image. The image processing data is classified
    according to the required characteristics for training, testing, and validation.
    This data is then aggregated for the application of machine learning or deep learning
    algorithms. 2.1. Image acquisition To achieve optimal image acquisition by a machine,
    two essential components are utilized: an illuminating source and a specific type
    of camera. By combining these elements, a wide range of acquisition methods can
    be employed, leading to diverse and efficient imaging outcomes. This system emulates
    human vision by perceiving and recognizing the images. As humans perceive an image
    through the light reflected from an object, illumination plays a critical role
    in image acquisition, as adjustments to the light can reveal different features
    of the object of interest. Images are captured by cameras with charge-coupled
    devices (CCD) and complementary metal–oxide–semiconductor (CMOS) sensors, and
    the quality of illumination can significantly impact the texture and features
    of the object of interest (Mahajan et al., 2015). All these factors contribute
    in acquiring necessary information. The first step is the selection of an appropriate
    technique that can capture high-quality images in the form of an array of numerical
    data that can be used to synthesize information by the computer (Mishra et al.,
    (2017)). Fig. 1 shows the process of image acquisition using a camera attached
    to the lower side of the robot and an illumination source to light the object.
    The images are subsequently transmitted over a wired or wireless network. There
    are various approaches to obtain high resolution images that can be used for agricultural
    purposes. The following subsections detail the wide range of wavelengths that
    can be perceived by the camera, as well as various techniques and illumination
    sources. Download : Download high-res image (99KB) Download : Download full-size
    image Fig. 1. Processes involved for Machine vision systems. 2.1.1. Monocular
    stereo camera system This is the basic low-cost technique using a single camera
    to capture images. Fig. 2 illustrates a monocular camera scheme utilized in autonomous
    mobile vehicles. Pravakar Roy et al. (Roy and Isler, 2016) used a monocular vision
    system to gather images to obtain effective information regarding apple fruit
    count and size in a dense cluster. Khan et al. (Khan and Debnath, 2019) employed
    a black and white camera in the MAGALI project to detect fruits based on their
    features. Zhao et al. (Zhao et al., 2005) replaced a black and white camera with
    Red Green Blue (RGB) cameras which provided colour contrast and reported an accuracy
    of 90% in identifying apples based on colour and texture features. Chen et al.
    (Chen et al., 2020) used a monocular RGB camera for positioning sweet peppers
    with the aid of a Visual Geometric Group network with 16 layers (VGG-16) and showed
    an average error of 18.3 % in pixels with respect to the position of the fruit
    centre over the fruit radius. Due to variable illumination and reflected light
    from neighbouring fruits and other background noise it is difficult to detect
    citrus fruit effectively. Using monocular camera system J.J. Zhuang et al. (Zhuang
    et al., 2018) developed a method for citrus fruit detection. Zhao et al. (Zhao
    et al., 2016) reported the disadvantage of a monocular setup that it only provided
    2D information and there were high chances of illumination changes affecting the
    results. Download : Download high-res image (86KB) Download : Download full-size
    image Fig. 2. A basic system mentioning monocular machine vision system over a
    robot. 2.1.2. Binocular stereo vision system Binocular stereo-vision systems are
    well recognized as effective 3D vision systems. Compared to a 2D vision system,
    they provide a detailed representation of reality. The third axis obtained in
    a stereo system is used to measure image depth. It can also be used to determine
    the geometric properties of an object, such as dimensions and miscellaneous measurements.
    In Fig. 3, dual camera stereo systems are mounted on the top of a moving vehicle
    (Lin et al., 2008). Download : Download high-res image (78KB) Download : Download
    full-size image Fig. 3. Visualization of a Binocular vision system over a moving
    cart. Two cameras are placed at a variable distance from each other on the same
    plane, while keeping the height constant. They simultaneously capture two images,
    from two different perspectives, and generate a 3D view (Hong et al., 2018, Kim
    and Lee, 2015). Fig. 4 depicts generation of 3D perspective from two cameras.
    Download : Download high-res image (57KB) Download : Download full-size image
    Fig. 4. Binocular system working, visualization of 3d perspective. Corado costa
    et al. (Costa, 2019) developed a methodology for path guidance for a vehicle by
    making a 3D construct of the environment and providing distance information in
    veritable scenes to determine obstacles or the multiple branches overhanging and
    branch splitting of the hazel trees. Using depths of images, Subhi et al. (Subhi
    and Ali, 2018) provided a method to estimate the volume and mass of food. In addition,
    they proposed several food grading techniques based on stereo vision. For instance,
    for precision agriculture, stereo vision was used to develop a three-dimensional
    terrain map, a compact stereo camera was used on a mobile moving platform to construct
    3d map of the terrain and showed the level of accuracy and detail by stereo perceptions
    (Rovira-Más et al., 2008). Two cameras, as illustrated in Fig. 4, are separated
    by an angular distance allowing them to take images of the same object from different
    angles as shown by Zhao et al. (Zhao et al., 2016). Andersen et al. (Andersen
    et al., 2005) used stereo vision for analyzing geometric attributes of the plants
    like total leaf area and height. A simulated annealing method was used for considering
    the neighboring pixels for the stereo matching process. Wang et al. (Wang et al.,
    2016) applied stereo vision for the localization of litchi, then wavelet transform
    was applied to a pair of images to normalize the illumination and K-means clustering-based
    algorithm was used to differentiate litchi from its background. Xiang et al. (Xiang
    et al., 2014) utilized stereo imaging for acquiring the depth map of clustered
    tomatoes and categorized overlapping and adhering regions after denoising the
    depth map. 2.1.3. Remote sensing imaging Remote sensing is the method used for
    measuring electromagnetic radiations reflected or emitted from a body. The instrument
    records EM radiations measured in either the UV spectrum or visible spectrum or
    infrared spectrum. The devices that are used for this purpose can be hand-held,
    ground based or aerial by means of drones, aircraft, satellites, and balloons.
    The apparatus carries a digital camera, video system and radiometers. The data
    can be stored or can be viewed directly based on a wireless network (AGRIOS, 2005).
    Through Fig. 5, khunal et al. (Khanal, (2017,3)) displays how remote sensing is
    used to detect changes in crop and soil conditions by detecting changes in surface
    reflectance through multiple images taken over a period. The field of remote sensing
    has evolved over decade. Today remote sensing is used for providing data from
    energy reflected, transmitted, or emitted from all types of EM spectrums. It is
    profoundly used in agricultural properties, determination of vegetation covers,
    land use, topography (Estes et al., 2001). Download : Download high-res image
    (318KB) Download : Download full-size image Fig. 5. Remote sensing working. Fig.
    6 displays a soil map created through remote sensing, measuring critical soil
    parameters such as soil organic matter (SOM), soil texture, soil pH level, and
    moisture content (Shustova, 2022). Precision agriculture nowadays uses remote
    sensing for gathering coordinated information of the ground target more accurately
    to provide wheat field yield density, area selection for the destruction of beat
    acres, and nitrogen application recommendations (Seelan et al., 2003). Bathany
    sleep et al. (Sleep et al., 2021) used visible near-infrared absorbance spectroscopy
    for soil pH determination. Whereas to determine the soil fertility near-infrared
    was used by Munawar et al. (Munawar et al., (2021)). Zheang zhu et al. (Zhou et
    al., 2021) measured the levels of crop water stress using thermal imaging in remote
    sensing in precision agriculture, moreover, provided a brief description of applications
    for various crop and segmentation strategies. Download : Download high-res image
    (357KB) Download : Download full-size image Fig. 6. Ph value of soil through remote
    sensing(Shustova, 2022. 2.1.4. Hyper-Spectral imaging An object’s spatial and
    spectral characteristics are measured by capturing images of an object over various
    wavelengths. Typically visible, near infrared, and infrared wavelengths are used
    for capturing images. However, the wavelengths can range from long-wave infrared
    to ultra-violet (“Hyperspectral Viewer”, 2022). Because of spectral resolution
    limits, the accuracy of the returned variables is frequently restricted and early
    signs of crop stressors cannot be adequately recognized. Lu et al. (Lu et al.,
    2020) have proposed that hyperspectral photographs containing hundreds of bands
    can record more detailed spectral responses, making them more suited for identifying
    tiny differences as shown in Fig. 7. The disadvantage of hyperspectral systems
    is their high cost, as well as the fact that modelling and data processing is
    time-consuming as depicted by Dale et al. (Dale et al., 2013). Manjunath et al.
    (Manjunath et al., 2011) distinguished between ornamental crops and pulses using
    the data collected from spectral imaging. Kumar et al. (Kumar et al., 2013) showed
    that the spectral reflectance of a canopy infested by aphids and a healthy mustard
    crop was different at near-infrared wavelength. Download : Download high-res image
    (155KB) Download : Download full-size image Fig. 7. Hyperspectral scheme vs multispectral
    scheme (Giannoni et al., 2018). 2.1.5. Multi-Spectral imaging A multi-spectral
    image is a combination of multi-layer of images of a single scene, with every
    single layer of a different wavelength as shown in Fig. 8. Multispectral imaging
    can perfectly detect radiations over the given wavelength bands (Nicolis and Gonzalez,
    2021): • Blue: 450–515. 520 nm • Near-infrared (NIR): 750–900 nm • Green: 515.
    520–590. 600 nm • Red: 600–630–680. 690 nm • Short infrared Download : Download
    high-res image (59KB) Download : Download full-size image Fig. 8. Wavelength band
    for Hyper-spectral and Multispectral imaging. The limitation of hyperspectral
    imaging is its complexity in computing data due to high-resolution levels. On
    the other hand, multispectral imaging is computationally less complex due to low-resolution
    levels. Honrado et al. (Honrado et al., 2017) utilized 2 Canon S100 compact digital
    cameras and Near-Infrared- retrofitted cameras for aerial imagery. Ground sensors
    were used to verify the Normalized Difference Vegetation Index (NDVI) obtained
    from the Unmanned Aerial Vehicle (UAV). UAV-based flights provided a high resolution
    and geographic breadth necessary for imaging and targeting heterogeneous cropping
    regimes and multiple-farmer plots. Instead of a ledger-based monitoring system
    this method was proposed for managing and monitoring local agriculture. Abhi et
    al utilized a monochromatic CCD camera equipped with two tunable electro-optical
    filters, one in the visible range (VIS 400 – 720 nm) and the other in the infrared
    range (SNIR 650 – 1100 nm), to capture a sequence of images in stable lighting
    conditions. These images correspond to the energy emitted by targeted features
    within narrow 10 nm bands within the filter range. The accuracy of the acquired
    data was confirmed, and a spectral profile was generated for each visible object.
    By analysing the variations in the amounts of reflected, absorbed, and transmitted
    energy, objects on the ground could be distinguished. Hyperspectral technology
    enabled the construction of images using data acquired at different wavelengths,
    revealing that even objects within the same category, such as deciduous trees,
    differed in the proportions of energy they absorbed, reflected, and transmitted,
    depending on the wavelength (Jasinski et al., 2010). 2.1.6. X-ray X-rays are utilized
    to discern an undesired object, that possesses a higher density than its surrounding
    matrix be it of agricultural or metallic origins. As a result, X-rays can be employed
    to detect contaminants in yield packaging by identifying foreign objects, such
    as metal, glass, calcified bone, and stone, thereby ensuring a higher quality
    output. An X-ray can reveal spatial information about the objects under examination
    and can acquire three-dimensional data. Small variations in density and distinctions
    between materials can be observed through this medium, leading to the detection
    of foreign bodies or for the quality control of fresh produce (Zwiggelaar et al.,
    1996). Most of the x-ray that are being used in the agriculture sector are low
    powered Xray with a magnitude of 50 keV and are known as soft Xray. More profoundly
    being used in crop, soil, grain, tree nut and fruit study (Mathanker et al., 2013).
    In recent years, X-ray technology has become renowned in determining the quality
    of agricultural products, enabling a better understanding of the composition,
    physicochemical characteristic, and internal structure on samples. In meticulous
    detail, X-ray technology can be utilized to scrutinize a wide array of agricultural
    products, including cereals such as wheat, corn, and rice, as well as fruits like
    apples and pears. This method grants a high level of two-dimensional and three-dimensional
    visualization. The technical applications of this technology include the assessment
    of internal quality, examination of micro-structure, and other purposes. Research
    studies have been conducted on various food products and have reported properties
    such as infection, antiinfection, and acidity prediction, as highlighted by Zhe
    Du et al. (Du et al., 2019). 2.1.7. Thermal imaging Using a thermal camera, Stanjnko
    et al. (Stajnko et al., 2004) devised a method for calculating the quantity and
    diameter of apples in an orchard. Based on heat emission as a function of exposure
    time to the sun’s heat. Bulanon et al. (Bulanon et al., 2009) and Slaughter et
    al. (Slaughter and Harrell, 1987) presented a novel experiment that utilized thermal
    infrared imaging in the fruit citrus canopy. A significant temperature differential
    between the fruit and the canopy was discovered at 4:00p.m. Based on the Stephan-Boltzmann
    principle, thermal radiation was employed to determine the temperature of the
    emitting object, and this method can be used to determine the water potential
    in a field, thus allowing for the optimization of irrigation. Optical methods
    can also be employed to identify the propagation of disease throughout the vegetation,
    as illustrated by Hernández-Clemente et al. (Hernández-Clemente et al., (2019)).
    Variations in leaf water content can alter the short-wave infrared spectrum, and
    the thermal infrared band was employed to detect the temperature of the leaf.
    Thermal imaging presents a valuable tool for monitoring environmental factors
    such as drought and rainfall, as well as variations in transpiration rates as
    shown in Fig. 9. Through this technique, it is possible to detect the shaded and
    sun exposed canopy fractions. Furthermore, variations in transpiration are directly
    linked to leaf temperature, can be inferred from changes in the temperature of
    canopy foliage under otherwise similar conditions of solar radiation and wind
    will refer to reduced transpiration (Hulley et al., 2019). Download : Download
    high-res image (291KB) Download : Download full-size image Fig. 9. Thermal imaging
    of a forest through aerial mode, on the left is the normal image while on the
    right is the thermal image. 2.1.8. Orthogonally polarized terahertz (THz)-Wave
    imaging system Yu et al. (Yu et al., 2015) developed a new system for detecting
    unwanted objects on a conveyor belt transporting crops, fruits, or food that moves
    faster than 20 m per minute. The system consisted of an imaging system positioned
    at a right angle that produced four different images, comprising of horizontal
    and vertical polarized images, as well as their respective addition and subtraction
    images. The system can effectively identify foreign objects on a product, such
    as pests, worms, or any other foreign body. Jiang et al. (Jiang et al., 2022)
    mentioned numerous machine learning models applied to terahertz technology for
    data preprocessing, multivariate analysis and terahertz imaging for water content
    measurement in fruits, additives in flour, varieties of rice, nutritional ingredients
    in dietary supplements. 2.1.9. Liquid Crystal tuneable filter (LCTF) Hyperspectral
    imaging can capture many images over a broad range of wavebands that are undetectable
    by the human eye. To ensure dependable and consistent outcomes using this optical
    sensor, it is crucial that the intensity displayed by the objects in the various
    spectral images remains unaffected by differences in the system''s sensitivity
    for different wavelengths. The spectral efficiency of the acquisition devices
    and the spectral emission of the lighting system can vary across the spectrum
    and the images. If the system is not accurately calibrated and corrected, these
    variations may affect the results obtained. Therefore, it is important to calibrate
    and correct hyperspectral imaging systems to achieve accurate and reliable outcomes.
    This process can be quite onerous, so to simplify it, numerous Liquid Crystal
    Tunable Filters (LCTF) devices are utilized to capture images across a wide range
    of spectral wavelengths. As demonstrated by Gomez Sanchis et al. (Gómez-Sanchis
    et al., 2014) a methodology was proposed for detecting decay in citrus fruits
    utilizing two hyperspectral systems in conjunction with two Liquid Crystal Tunable
    Filters (LCTF) for the image acquisition of spherical fruits. The system was capable
    of accurately identifying 98% of the pixels of rotten or non-rotten areas, and
    with a 95% success rate in determining the condition of the fruits. 2.2. Image
    processing The utilization of image processing has risen dramatically in recent
    years to acquire accurate data for computer processing, enhance understanding
    and reduce computational costs. As a result, the scope of applications for image
    processing has expanded exponentially, encompassing fields such as military, agriculture,
    remote sensing, medicine, geology, and even space and interstellar research. Image
    processing is a technique in which digital images are manipulated to suit specific
    requirements by utilizing a digital computer. The goal of this process can be
    to clean, extract, classify, or identify data from the region of interest (ROI)
    (Da Silva and Mendonça, 2005). In agriculture, image processing can be used to
    identify diseased plant or crop, or fruit by analysing the image, and its pixel
    density. The initial step in this process is to acquire images of both the diseased
    and healthy plants using a machine vision camera. Image pre-processing techniques
    are then applied to the acquired images. Following pre-processing, the image is
    divided into distinct segments. A variety of segmentation techniques, such as
    threshold-based, edge-based, region-based, clustering-based, and artificial neural
    network-based methods, are then applied to the image to determine the nature and
    type of disease. Suganya et al. (Suganya et al., 2019) classified plants based
    on the disease incurred using machine vision. This process also is known as digital
    image processing, and it does not involve interpreting the content of the image
    or changing its meaning by any alteration. Image processing is thus classified
    into three different levels, as can be depicted from Fig. 10, and are explained
    below as low-level processing, intermediate-level processing, and high-level processing.
    Download : Download high-res image (129KB) Download : Download full-size image
    Fig. 10. Various machine learning algorithms used in machine vision systems. 2.2.1.
    Low-level processing Low-level image processing techniques are employed for the
    preliminary treatment of the data. As the images acquired by the camera cannot
    be synchronized with the application. Insufficient light, spatial effects, low
    focus, unwanted graining, and low resolution can impact the image. Due to such
    problems, images are manipulated with techniques such as noise reduction, contrast
    enhancement, image sharpening, and others to improve its quality based on the
    requirement (Senni et al., 2014). Low-level processing is used to create a perceptually
    good smoothing process by applying various techniques to make images compatible
    with a segmentation process, intermediate-level processing, and formulating a
    meaningful scale space for images (Sochen et al., 1998). Detecting a feature is
    a low-level image processing operation that is usually performed as the first
    operation on an image feature. A feature is a region of interest in an image.
    Mustafa et al. (Mustafa et al., 2008) used feature detection to check the ripeness
    of the banana and its size. Image restoration is another type of low-level processing
    technique used for enhancing the image. It involves restoring the image from a
    degraded version such as blur or noise that can be caused by atmospheric turbulence
    during wireless transfer and any other stochastic phenomena (Reeves, 2014). Senni
    et al. (Senni et al., 2014) proposed an on-line Thermography Non-Destructive Testing
    method to detect whether biscuits were contaminated with foreign objects. During
    the image pre-processing step, the authors applied a 2D low pass-filter, a focusing
    filter, and a 2D Wiener filter in the frequency domain to reduce the Additive
    White Gaussian Noise, image blurring, and drag effects problems in the raw images.
    2.2.2. Intermediate level processing Mid-level image processing is used to segregate
    the information from the pre-processed image to obtain the feature from the target
    data. It reduces the computational cost and time by narrowing the approach. It
    generally involves operations like image segmentation, image description, object
    recognition, and image transformation. This level of processing usually has images
    as an input and extracted attributes as an output. Image segmentation is also
    used for classifying or grouping images-based similarities and dissimilarities
    appointing the use of techniques based on thresholding, colour, and clustering.
    Its function is to segregate target data from extraneous information, hence reducing
    computation time and increasing accuracy. Vijai Singh et al. (Singh and Misra,
    2017) demonstrated a genetic-based classification algorithm for detecting and
    classifying plant leaf diseases in banana, bean, jackfruit, lemon, mango, potato,
    tomato, and sapota species. Early detection of damage on peaches due to bruises
    is a challenging task; it determines the quality of peaches. Watershed segmentation
    is a region-based technique that takes into consideration topographic features
    such as mountains, valleys, and basins to segment objects. Several efforts using
    short wave NIR were more suitable, but Li et al. (Leiva-Valenzuela and Aguilera,
    2013) devised an improved watershed segmentation algorithm based on morphological
    gradient reconstruction and marker extraction and used this method to segment
    bruised regions on peaches. Jaime et al. (Giménez-Gallego et al., 2020) used image-based
    system to segment various tree leaves based on a vector machine model coupled
    with deep learning for image segmentation. Ni et al. (Ni et al., 2018) used OTSU
    method after the segmentation process to detect apples in a complex environment
    for an apple-picking robot using a vision system. Momin et al. (Momin et al.,
    2017) used a machine vision system to grade mangoes based on geometry and their
    shape. In this system, an 8-bit XGA camera was used to capture the mango images.
    Moreover, to distinguish mangoes, various intermediate processing techniques were
    used such as image segmentation techniques, which include thresholding and pattern
    recognition, to segment the features of mangoes. In addition, it was found that
    mango mass can be well estimated with the projected area by 97%, hence this parameter
    was taken as benchmark for identifying features such as projected area, perimeter,
    diameter, and roundness of the fruits. Finally, a simple filter binarization with
    a median filtering, coupled with median filter was deployed to grade the mangoes
    into three basic categories: small, medium, and large. Pan et al. (Pan et al.,
    2016) utilized a machine vision system to identify chill damage in peaches stored
    at extremely low temperatures. Hyperspectral imaging system and a CCD camera equipped
    with an adjustable light source was used to acquire the images. The data was then
    processed using global thresholding and classified using a multi-layer perceptron
    Artificial Neural Network (MLPANN) to identify the peaches that were affected
    by chill damage. The model was able to achieve 97% accuracy. 2.2.3. High level
    processing High-level processing includes image recognition and image interpretation
    to obtain meaning from a group of recognized objects. In this step, statistical
    methods, machine learning methods, neural networks, or deep learning methods are
    commonly used to classify and identify the target based on the region of interest.
    This type of processing comprises the use of algorithms such as K-nearest neighbour
    (KNN), principal component analysis, Support Vector Machine (SVM), artificial
    neural networks, genetic algorithms or fuzzy logic that can be deployed to interpret
    meaning from processed image data after intermediate level processing (Liu et
    al., 2017) (Janke et al., 2019) (Druzhkov and Kustikova, (2016)). Nguyen et al.
    (Nguyen et al., 2020) used deep learning to precisely locate the paddy field at
    the pixel level throughout the year from the obtained data through remote sensing
    techniques. To detect and predict rice leaf diseases, deep learning support vector
    machines are used moreover for pattern recognition and to improve the recognition
    accuracy. Jiang et al. (Jiang et al., 2020) used a convolutional neural network
    to obtain leaf disease features. Due to their higher probability of occurrence
    after using support vector machines to classify and predict the disease, The model
    was applied over four rice diseases: rice blast, red blight, stripe blight, and
    sheath blight. In smart farms in Chile Quiroz et al. (Quiroz and Alférez, 2020)
    used convolutional neural network for image recognition for the detection of legacy
    blueberries at the rooting stage. The proposed method could identify blueberries
    plant with tray, presence of tray without plant, and no trays. 2.3. Image classification
    2.3.1. Machine learning algorithms Machine learning (ML) refers to a system’s
    ability to acquire and integrate knowledge through large-scale observations and
    to improve and extend itself by learning fresh knowledge and augmenting its database
    instead of being confined to a predetermined set of limited information through
    programming. Machine learning is used for classification, detection of crop, pest,
    weeds, disease on various crops pulses, tiago et al (Domingues et al., 2022) identified
    various uses of machine learning for tomatoes, and factors such as weather, temperature,
    lighting affecting these models. Some of the most prominent techniques of machine
    learning in agriculture as shown in Fig. 11 are based on supervised as well as
    unsupervised learning and reinforcement machine learning, the system includes
    linear algorithms such as linear regression or logistic regression (LR) and linear
    discriminant analysis (LDA) as well as nonlinear algorithms such as K-nearest
    neighbour (KNN), classification and regression trees (CART), Gaussian naive Bayes
    (NB) and support vector machine such as SVM regression, fuzzy cognitive map learning,
    or fuzzy clustering based approaches (Mupangwa et al., 2020) (Rehman et al., 1036).
    ML techniques vary in their accuracy and precision levels based on the application.
    Thus, selection of an algorithm based on the application is a critical step. Table
    1 illustrates the utilization of various algorithmic types, including their respective
    crops and intended purposes. Model can be made based on factors that affect agriculture
    will vary with climate, geographic zone, soil type, crop, and disease vulnerability
    (Domingues et al., 2022). Christos el al. (Chaschatzis et al., 2022) used YOLOv5
    and ResNet architecture on a novel dataset of sweet cherries to detection infected
    leaves and branches indicating the stages of disease. Larrera et al. (Larrea-Gallegos
    and Vázquez-Rowe, 2022) generated a deforestation prediction model using machine
    learning algorithms which can help in identifying crop plantation areas. Download
    : Download high-res image (104KB) Download : Download full-size image Fig. 11.
    Various machine learning algorithms supervised and unsupervised used in machine
    vision systems. Table 1. Application of various supervised machine learning algorithms
    for agricultural practices. Algorithm type Crop Feature Application Reference
    Naïve Bayes Apple Spectral feature Bruise and cultivar detection (Siedliska et
    al., 2014) Cucumber Spectral and textural features Injury based grading (Cen et
    al., 2016) Wheat Spectral features Removing contaminant from grain (Ravikanth
    et al., 2015) Cereal Colour features Vegetation Segmentation (Laursen et al.,
    2014) Citrus Textural features Leaf disease identification (Bandi et al., 2013)     Discriminant
    analysis algorithm Bell pepper Colour features Grading based on colour and defects
    (Shearer and Payne, 1990) Olives Colour features Grading based on wrinkle detection
    (Puerto et al., 2015) Pomegranate arils Colour and shape features Fruit grading
    and sorting (Blasco et al., 2009) White radish Spectral features Hollowness detection
    arils and membrane pieces (Pan et al., 2017)  SVM  Potato Colour and texture feature
    Diseases Classification (Islam et al., 2017) Chilli Colour, shape and moment invariant
    features Crop and weed classification (Ahmed et al., 2012) Rose, Bean, Lemon and
    Banana Texture features Leaf disease (Singh and Misra, 2017) Pomegranate Colour
    and texture features Leaf disease (Sannakki et al., 2013) k-Nearest Neighbour
    Blueberry Colour features Stages of maturity (Li et al., 2014) Rapeseed Textural
    features Seed type identification (Kurtulmus and Ünal, ,2015) Wheat and Barley
    grains Colour, shape, and textural features Identification based on physical characteristics
    (Guevara-Hernandez and Gil, 2011) Apple Spectral features Surface defect (Miller
    et al., 1998) 2.3.2. Deep learning algorithms Deep neural networks are a powerful
    sub-category of machine learning algorithms implemented by stacking layers of
    user-defined neural networks along the depth and width of small architectures
    (Mahmood et al., 2017). Andreas et al. (Kamilaris and Prenafeta-Boldú, 2018) identified
    16 major areas in total where deep learning techniques are most profoundly being
    used in agriculture, they are identification of weeds, land cover classification,
    plant recognition, fruit counting, and crop type classification. Most prominently
    techniques used in decreasing order are CNN, HistNN then CNN + linear regression,
    CNN + vgg16 and some RNN based models. 3. Application using the mentioned techniques
    in various domains 3.1. Weeding Weeds are plants that are considered undesirable
    in a crop. There are several reasons why weed is harmful for its neighbouring
    plants. Weeds reduce crop yields, interfere with the harvest, support pathogens
    and insect pests, and contaminate seeds (Cloutier and Leblanc, 2001). Thus, it
    becomes entirely necessary to control weeds for crop growth. The most effective
    way is to use a machine vision system, due to its higher accuracy and precision.
    J. Blasco et al. (Blasco et al., 2002) developed a non-chemical-based weed control
    method using two monochromatic machine vision cameras, one for capturing the image
    for weed identification and second for the alignment and optimization of trajectory
    of the weeding tool, weeds were eliminated using electric discharge. Chung-Liang
    Chang et al. (Chang et al., 2021) fabricated a 4-wheel rover with a mechanical
    claw rake weeding tool, as depicted in Fig. 12. One digital camera underneath
    the rover captured images. RGB colour layer and deep learning algorithm yolov3
    was used to determine the weeds. The setup detected weeds at 20 m/s with more
    than 90% accuracy. Download : Download high-res image (69KB) Download : Download
    full-size image Fig. 12. Machine vision system for a weeder robot. B.chen et al.
    (Chen et al., 2003) developed an inexpensive machine vision system for determining
    the travel directrix for a micro weeding robot for paddy fields. A macro camera
    with ccd embedded sensor that revolved 360 degrees at different heights captured
    the images. The target image was obtained by differentiating blue and black pixels
    of a binary image through the Hough transform (PKPHT) for image processing. Le
    zhang et al. (Zhang et al., 2021) developed a quadratic transversal algorithm
    for weed path planning. A mobile robot with a digital camera, a depth camera,
    a universal arm, GPS, and lidar was used. Faster R-CNN was used to recognize corn
    and weeds. Moreover, the depth camera converted the 2D acquired image into 3D
    for multitarget depth ranging and optimized path planning. A multi-camera approach
    was taken by Wu et al. (Wu et al., 2021), the system removed weeds by classifying
    plants and weeds. Multiple tactics were taken by Kunz et al. to manage weeds in
    sugar beet, maize, and soybean in order to lift weeding efficiency. Machine learning
    in precision agriculture mainly focuses on the valuation of the accuracy of object
    detection, while other real-world challenges are there to overcome. To overcome
    three challenges in the real-world deployment of precision weeding, Adrain et
    al. (Gomez et al., 2021) devised custom ML based algorithms, and five cameras
    to highlight weeds. Shanwen Zhang et al. (Zhang et al., 2021) proposed a novel
    weed species recognition system that uses digital image processing and pattern
    recognition. A CMOS digital camera captured the images. A combined system of grabcut,
    adaptive fuzzy dynamics, k means algorithm and sparse representation classification
    was developed to identify species. Bingrui Xu et al. (Xu et al., 2021) developed
    a machine vision method for seeding and weeding in a cornfield. An ultra-green
    feature algorithm was used to segment corn and land, OTSU was used to differentiate
    weeds from corn plants, and column pixel projection method was used to identify
    row positions. Chung-Liang Chang et al. (Chang and Lin, 2018) developed an agri-rover
    with a machine vision system for weeding and variable rate irrigation scheme.
    A rather simple HSV model was used for image processing, meanwhile for the purpose
    of irrigation, data of the wet distribution area of the surface soil was used.
    The moisture content data was provided to the fuzzy controller, to actuate variable
    irrigation to save water. Fig. 13 depicts a flow chart of weed detection in terms
    of image, with weed detection in (d) part. Download : Download high-res image
    (127KB) Download : Download full-size image Fig. 13. Process for weed detection.
    (a) Original image; (b) output channel; (c) feature extraction; (d) marking of
    weed position of the plant (Gomez et al., 2021). Fedrico et al. (Pallottino et
    al., 2018) developed an RGB retrofit kit for carrying out mechanical weeding in
    row crops with a variable degree of weed manifestation to benefit economically
    and to eliminate the use of chemicals. To identify weeds, the developed kit includes
    a RGB-charged CCD camera with shape analysis and colorimetric KNN clustering.
    Chung- Lyndon et al. (Smith et al., 2018) presented an innovative 3D and 2D approaches
    for tasks like weeding, harvesting, and various other agricultural activities.
    Michel et al. (Spaeth et al., 2020) devised a smart method for harrowing for integrated
    weed control to reduce herbicide use. Machine vision system was employed to adjust
    the soil treatment intensity. Two machine vision cameras were used, one at the
    front and another at the rear. By considering the difference between the crop
    soil coverage and comparing it with the set threshold value, tine angle was adjusted.
    A highly selective weeding control within crop rows requires a very accurate and
    precise guidance mechanism. Zhang et al. (Zhang et al., 2013) used a machine vision
    system to identify the crop plant and to perform a row shift mechanism to prevent
    crop damage. A swing type intra row weeding hoe actuated the weeding process;
    its navigation was done with the help of extended Hough transformation. Meng et
    al. (Meng et al., 2015) devised a machine vision system for weeding. The system
    used grey image, binarization processing and fuzzy algorithm to operate the weeding
    machine. For the detection of weeds in lettuce field Lydia et al. (Elstone et
    al., 2020) used red, green, and near infrared reflectance combined with size differentiation
    method. A wide angle RGB camera and a six led illumination system were all mounted
    on a tractor and tested on the field. Moreover, an optimization algorithm was
    also used. Wang et al. (Wang et al., 2020) developed a method to detect weeds
    using semantic segmentation technique and an encoder decoder architecture using
    deep neural network. Moreover, outdoor illumination is naturally uncontrolled
    and produce distortions in image. To eliminate this, image enhancement methods
    were used. A combination of enhancement techniques such as HE, PS-AC, DPE, MILIATO
    CNN and image representation techniques obtained an accurate model. Wu et al.
    (Wu et al., 2019) reported that the advanced machineries like weeds mowers or
    spray machines introduced to reduce human effort were proved ineffective during
    inter-row and intra-row weed removal operation due to errors thus damaging the
    healthy crops. Attempts were made to design autonomous robots that could quickly
    and efficiently manoeuvre between rows of crops for weed detection through machine
    vision as shown in Fig. 14. Chang et al. (Chang and Lin, 2018) discussed about
    the vision-based classification approaches commonly used in agricultural robots
    to detect and remove weeds growing between crops rows. The advent of Deep Learning
    algorithms like convolutional neural networks (CNN) has enabled agricultural robots
    to detect weeds even better than humans (Ralph, (2022)). The CNN algorithm gets
    real-time video feed from a calibrated camera attached to the robot to perform
    the weed detection. At times, multiple CNNs are deployed on a robot to do a specific
    task to produce more accurate results. Furthermore, CNN architectures like (Redmon
    et al., 2016) YOLO, (Liu et al., 2016) SSD, (Howard et al., 2017) MobileNet that
    are trained on large data sets such as (Lin et al., 2014) COCO, (Deng et al.,
    2009) ImageNet, (Kuznetsova et al., 2020) and Google Open Images enable researchers
    to achieve greater accuracy on their custom datasets. Chang et al. (Chang and
    Lin, 2018) and Blasco et al. (Blasco et al., 2002) have reported that RGB (Red
    Green Blue) format images were used as input for detection purposes. Obtained
    images can be further processed and converted to hue saturation value (HSV) scale
    to enhance results. Wu et al. (Wu et al., 2019) suggested that RGB + NIR (Red
    Green Blue + Near-infrared) formatted images can improve classification accuracy.
    Adaptive threshold and Image Segmentation were the commonly used algorithms to
    infer patterns from the data to perform the classification function (Chang and
    Lin, 2018) (Blasco et al., 2002). Recently, CNN has started to replace these traditional
    computer vision algorithms its capability to generalize the data, increasing robot
    predicting efficiency even in dynamic environments (Wu et al., 2019). Though most
    robots get their input feed from a single monocular camera, advanced models are
    provided with multiple cameras either to obtain the depth values or track objects.
    Extended Kalman Filter (EKF) is utilized to track weeds with great precision (Wu
    et al., 2019). Since removing a healthy plant is considered as a significant loss
    than failing to remove a weed plant, a naive Bayes classifier is used to classify
    healthy crops from weeds. Chiu et al. (Chiu et al., 2020) created a large image
    dataset for weed detection for vast fields, the dataset includes RGB and NRG images
    for developing effective algorithm to identify large weed areas. Download : Download
    high-res image (233KB) Download : Download full-size image Fig. 14. Multi-camera
    detection and tracking system to detect weeds (Wu et al., 2019). Javaid et al.
    (Wani et al., 2022) mentioned 8 data set for plant disease containing data of
    numerous crops namely bell pepper, apple, peach, squash, strawberry, rice, beans.
    And in-depth disease types of disease occurrence in potato, apple, tomato and
    rice and algorithms for image processing have also been provided. Liu et al (Liu
    et al., 2022) used machine learning for prediction of tea leaf disease that is
    Exobasidium vexans using multiple regression model by correlating disease density
    with temperature, humidity, and rainfall for IOT based application. Akhter et
    al. (Akhter and Sofi, 2022) developed a prediction model to detect apple disease
    scab in apple orchards of Kashmir valley using temperature, humidity, barometric
    pressure, ambient light sensor, and dual-axis accelerometer. This work was deployed
    using IOT devices to monitor timely and precisely. 3.2. Harvesting Harvesting
    is a process of gathering useful plant parts either manually or with a machine.
    Machine vision systems can be used to automate such processes (Chen et al., 2019,
    Ladaniya, 2008). With the help of a machine vision system, Henry et al. (Williams
    et al., 2019) designed a kiwi harvesting multi-arm robot. The system operated
    autonomously below the web to detect the hanging kiwi in orchards, with eight
    stereo colour cameras facing the hanging kiwis to measure the depth of field and
    to detect fruits. From Fig. 15, the trained neural network can successfully detect
    kiwi fruit where blue depicts the kiwi fruit’s calyx while, green represents the
    canes, and red represents wires. The autonomous system could detect, grab, and
    pick the hanging kiwi from the represented network as shown in Fig. 16. Download
    : Download high-res image (213KB) Download : Download full-size image Fig. 15.
    Kiwi fruit and environment detection through trained neural network(Williams et
    al., 2019). Download : Download high-res image (261KB) Download : Download full-size
    image Fig. 16. Kiwi fruit robotic harvesting unit(Williams et al., 2019). To eliminate
    the hand harvest of broccoli, which is considered as a tedious task and comprises
    of 35% the cost of production, Pieter et al. (Blok et al., 2016) developed an
    autonomous selective harvester for broccoli with machine vision system. Colour
    based and texture-based image segmentation was used to differentiate broccoli
    from the background. An RGB camera based on a CCD sensor was used for image acquisition,
    a boxed lighting mechanism to eliminate external light interference, and the GrabCut
    algorithm to separate broccoli from the background in an image. Zhang et al. (Zhang
    et al., 2020) used a machine vision system for developing navigational path for
    a combined rice harvester. Moreover, it was identified in statistical findings,
    that the Cr component of YCbCr colour space possessed less intra-regional difference
    while the inter-regional difference was very high, making it easier to identify
    rice boundaries. Guru et al. (Guru et al., 2012) developed a machine vision system
    to classify tobacco leaves and to harvest them. The model developed consisted
    of three stages that are image segmentation, feature extraction, and classification.
    CIEL*a*b colour model was used for extracting the leaf image from the background
    because of its high efficiency and kNN for classification. Suraj et al. (Amatya
    et al., 2017) developed a system for cherry harvesting that involves shaking the
    plant. A precise mechanism was developed for shaking of stem, RGB images were
    used to identify the location of shaking and 3d camera for calculating the depth,
    the cherry trees were arranged in Y-trellis and vertical trellis systems and this
    system takes in consideration branches that are partially visible, not visible,
    and the branch section not satisfying the equation. With that, we can predict
    the number of shakes. For robotic harvesting system of eggplants, Hayashi et al.
    (Hayashi et al., (2002)) developed a RGB based system using ccd camera to identify
    the plants. The system was mounted on the end effector situated on a moving platform.
    They designed a fuzzy feedback control for the manipulator. As each system requires
    a distinguish machine vision algorithm for detection, the system used a colour
    morphological based detection algorithm. An end effector suction pad and rubber
    actuated mechanism used for grabbing, and a scissor mechanism with a guided bar
    was used for harvesting. Nasirahmadi et al. (Nasirahmadi et al., 2021) developed
    a system that could detect damage on sugar beet during the time of its harvesting
    using machine vision system coupled with deep learning. A machine vision models
    can be affected by various factors such as lighting condition, level of noise,
    quality of the apparatus for image acquisition. To make the approach robust, it
    must be coupled with deep learning techniques for external damage detection through
    2d digital images. The cameras installed over the high-speed cleaning turbine,
    and high-speed cameras were used to eliminate blur. As Cracks, breakage, and surface
    abrasion were considered as damage for the work. RCNN and RFCN were used for damage
    detection. Kuznetsova et al. (Kuznetsova et al., 2020) designed a vision system
    with YOLOv3 algorithm for detecting apples in orchards. The algorithm was able
    to detect an apple with an average time of 19 ms and reported 7.8% objects were
    mistaken for apples and 9.2% were not recognized. The same system can also be
    applied to orange harvesting robots. The machine vision developed by Financial
    University (Kuznetsova et al., 2020) utilized two stationary Sony Alpha ILCE-7RM2
    cameras with Sony FE24-240 mm f/3.5–6.3OSS lenses and one Logitech Webcam C930e
    camera, mounted on the second movable shoulder of the manipulator before the grip.
    The two Sony cameras took a general far-view shot of the canopy to detect the
    apples and decide the optimal route for the manipulator, and the Logitech camera
    positioned the grip appropriately for the harvesting process. To improve the quality
    of detection, the images were pre-processed by increasing the contrast through
    histogram normalization. Contrast limited adaptive histogram alignment (CLAHE)
    with 4 × 4 grid size and clip limit set to 3 was applied with the median filter
    of 3 × 3 kernel which gave a slight blur. The morphological opening with a flat
    5 × 5 square structuring element was used to thicken the borders. Due to this
    pre-processing, the effects of shadows, glare, minor damages of apples, and thick
    branches overlapping the apples were avoided. Unfortunately, the algorithm detected
    yellow leaves as apples, but this was prevented by disregarding objects whose
    ratio of the greater side of the circumscribed rectangle to the smaller one was
    more than 3. To avoid recognizing the gap between the leaves for apples during
    the post-processing, the objects whose area of the circumscribed rectangle was
    less than the threshold were discarded. However, many apples in the canopy images
    remained undetected, that can be seen in the close-up pictures where the small
    apples are smaller than the anchor box. To tackle this, increasing the number
    of anchor boxes was not a viable since it increased the computation time. So,
    the images were cut into k2 parts (k = number of times the apple in a canopy is
    smaller than the anchor box). k = 3 was chosen since larger k values will require
    higher resolution images, and higher k values produced worse results (Fig. 17)
    to detect and decrease the computation time. Kailasam et al. (Kailasam et al.,
    2022) developed an IOT based crop maintenance system, it can early detect plant
    diseases with the help of threshold segmentation and random forest classification.
    Download : Download high-res image (217KB) Download : Download full-size image
    Fig. 17. 48 apples found in far-view after pre-processing (Kuznetsova et al.,
    2020). 3.3. Fruit counting and yield estimation Detecting and counting individual
    fruits in a plant is the preliminary step in the pre-harvesting phase called yield
    estimation. Mekhalfi et al. (Mekhalfi et al., 2020) utilized the total number
    of fruits as a parameter to estimate the logistic factors like transport, labour
    requirements, current demand in the market, etc. To benefit farmers economically
    by providing them insight to plan and organize the facilities required for harvesting.
    Mekhalfi et al. (Mekhalfi et al., 2020) Dorj et al. (Dorj et al., 2013) and Zhou
    et al. (Zhou et al., 2012) performed background subtraction in the captured image
    as the first step before image processing, to reduce unwanted noise like background
    leaves and branches such that it does not add false positives during the detection
    stage. Mekhalfi et al. (Mekhalfi et al., 2020) obtained the input image in the
    RGB format and converted it to the (Lightness, Channel a, and Channel b) LAB space
    format to remove noise. A foreground-background is generated from this LAB image’s
    “A” channel using thresholding (Otsu, 1979). Further, a morphological image processing
    technique called dilation enhances the mask, which is applied to the actual image
    to detect the fruits. The significant challenges for the detection system were
    to detect fruits of different shapes, orientations and brightnesses. To solve
    this issue Mekhalfi et al. (Mekhalfi et al., 2020) focused on counting kiwi fruits
    and proposed a technique to focus on the tip of the fruit rather than the whole
    fruit during the detection phase using the Viola-Jones algorithm. This method
    gave 6% and 15% error rates at two different kiwifruit orchards during their test
    trials. Dorj et al. (Dorj et al., 2013) have discussed various techniques for
    tangerine detection and counting. Initially, the background noise from the acquired
    RGB image is manually removed and the image is divided into 72 sub-images. The
    sub-images are converted to the YCbCr (Green (Y), Blue (Cb), and Red (Cr)) colour
    space. An image histogram is computed on the Cb (Blue) chrominance component followed
    by the thresholding operation. The number of connected objects is counted to determine
    the number of fruits. Zhou et al. (Zhou et al., 2012) discussed on counting and
    detecting apple fruits in an orchard, particularly in June drop, when a large
    and unpredictable number of fruits might drop as an alternative to the yield estimate
    during the flowering stage as performed by Aggelopoulou et al. (Aggelopoulou et
    al., 2011). Images were acquired when the fruits were light green immediately
    after the drop period during early summer. A white drape was placed behind the
    target trees to remove background noise like leaves, sky, and branches, and a
    red sphere was placed on top of the tree for size calibration of the apples. At
    the end of June, the fruits started turning red, and on the R-B (Red – Blue) scale,
    the R-value was greater for the apples when compared to their surroundings. Thus,
    a threshold value of R-B greater than 40 was chosen to mask the fruits, and anything
    below this will have its pixel value set to zero. The algorithm erratically generated
    a few false positives as it labelled the leaves as fruits, and therefore a threshold
    value of less than 20 was set in the G-R colour scale to improve the prediction
    accuracy. Both thresholds were combined to make predictions on the final images.
    A connected components algorithm was used to count the fruits from the generated
    mask. To prevent miscalculations in cases of overlapped fruits in a cluster, the
    connected domain area of the apples was compared with that of the red sphere used
    for calibration. If the area exceeded 400 pixels, it would be counted as two fruits,
    whereas if the area was less than 30 pixels, it would be counted as just one fruit.
    Bini et al. (Bini et al., 2022) used colour thresholding combined with a median
    filter to calculate tomato yield; the r2 for the algorithm was found to be 0.98
    in comparison to manual picking. Amanda et al. (Jacques et al., 2018) developed
    a machine vision system for real-time detection of shallot onions while harvesting,
    using colour thresholding and the Otsu thresholding selection method for detecting
    and counting onions. Amatya et al. (Amatya et al., 2016) developed a method for
    locating shaking position for automated cherry harvesting based on branch and
    cherry pixel location RGB and stereo camera. The overall root mean squared error
    for estimating the distance to desired shaking point was 0.064 m. 3.4. Overlapping
    of fruits The identification of the object of interest and the accuracy of fruit
    detection by machine vision can be significantly hindered by the presence of overlapping
    conditions such as mixed branches or neighbouring fruits, which can obscure the
    region of interest as shown in Fig. 18. Guo et al. (Guo et al., (2019)) developed
    a method to detect overlapped lychee fruit by using monocular machine vision.
    A combination of techniques such as contrast limited adaptive histogram equalization
    (CLAHE), red/blue chromatic mapping, Otsu thresholding and morphology operations
    were used to segment foreground images of lychee. A three-point circle-based process
    extracted each lychee from its overlaid cluster, and precision, recall, and f1
    score were obtained using local binary pattern support vector machine. The result
    provided enhance detection rate for the lychee. Winter jujube is a fruit that
    occurs in China, Zhiheng et al. (Lu et al., 2021) developed a method using hand
    based and Yolov3 to differentiate and detect overlapping fruits. Moreover, it
    was found that yolov3 detection percentage was 5% greater than thresholding. While
    harvesting apples, the orchard environment becomes noisy in terms of overlaps,
    thus making it difficult for the image processing unit to segment the apples from
    the image, Meng et al. (Meng and Wang, 2015) used boundary tracking and some image
    segmentation techniques to recognise and extract overlapping apple from the environment.
    Firstly, image segmentation removed the background on bases of colour (YUV colour
    space) thereafter for boundary tracking two processes were performed to determine
    the boundary of the fruit to extract each apple from its cluster, a run length
    encoding was used to label every region of the binary image and area was calculated.
    The noise area was eliminated by averaging. Download : Download high-res image
    (147KB) Download : Download full-size image Fig. 18. Segmentation of Overlapped
    fruits over each other, leaves, branches (Zeng et al., (2009). Secondly, small
    holes were compensated using seed filling method. To separate boundary, pixel
    wise calculation was done and mathematical calculations to determine the boundary
    of the apple. Several methods have been developed for the daytime, Longsheng et
    al. (Fu et al., 2015) developed a method to recognize kiwifruit in night condition.
    Artificial lighting methods ranging from 30 to 50 Lx, a RG colour method, and
    a method for image processing using canny operator was deployed to detect the
    boundary of the fruit and OTSU thresholding to properly detect the fruit. To differentiate
    merged boundaries and individual fruits, a Hugh transformation was done to depict
    the most elliptical shape. Bazame et al. (Bazame et al., 2021) developed a method
    to differentiate coffee fruits from a video graphic image where fruits were getting
    discharged from the conveyor harvester. A 23-layer neural network structure comprising
    convolutional, maxpool, yolo and up sampling was used for the task to achieve
    high processing speed. A CMOS camera supported by a 21w LED system to capture
    images and the camera was stabilized by the gimbal. The model''s accuracy was
    somewhere between 80% and 86%. Jia et al. (Jiao et al., 2020) proposed a robot
    vision detector based on the Mask Region Convolutional Neural Network (Mask R-CNN)
    as shown in Fig. 19. The input parameters for feature extraction were drastically
    reduced by Residual Network and Densely Connected Convolutional Networks (DenseNet).
    The feature maps were used as input for the Region Proposal Network (RPN) to generate
    the region of interest (ROI). With a full convolution network (FCN), the mask
    was generated to identify the apple region. Further, Jia et al. (Jiao et al.,
    2020) reported a precision rate of 97.31% and a recall rate of 95.70%. Images
    were captured using a camera with a 6000 × 4000-pixel resolution in different
    lighting conditions. The images were pre-processed by altering the orientation
    and contrast in order to prevent over-fitting and also to make the dataset more
    diverse. Since robots usually move, the captured images may be blurred, and to
    mitigate this problem, the length was set to 512 pixels. The width was also changed
    accordingly to maintain a constant aspect ratio, which would help train the algorithm
    for real-world problems. ResNet was used to train a deeper CNN network, which
    would not be possible with an increase in training due to large number of layers
    of deep convolutional network. Since the image resolution gets lower in deeper
    layers, DenseNet is implemented to improve the backbone network. Thereafter a
    robust marker-controlled watershed transform algorithm to automatically perform
    the accurate segmentation of overlapping plant fruits. The small-sized tomatoes
    are green in nature and are difficult to detect and distinguish from the green
    background, sun et al. (Sun et al., 2020) provided an improved feature pyramid
    network for tomato organ recognition to tackle this problem. The algorithm could
    predict with an accuracy of 99.95%. Download : Download high-res image (128KB)
    Download : Download full-size image Fig. 19. Model of improved mask R-CNN (Jiao
    et al., 2020). 3.5. Sorting Zhiheng et al. (Zhang et al., 2018) used of yolov3
    model to detect fruit with a 96% accuracy rate. The algorithm performed grading
    operation on the fruit with an accuracy of 97.28%. When done manually, sorting
    struggles to maintain consistency and uniformity. To eliminate such problems,
    Nandi et al. (Nandi et al., 2013) developed a machine vision-based system for
    grading and sorting mango fruits using fuzzy logic. Four batches of mangoes were
    considered for a blue colour conveyor belt for getting more accuracy with RGB
    model and 120Lx light using CCD camera. To remove the noise, a simple median filter
    was used, while to maintain ease in computation, an additional pseudo-median filter
    was deployed. Since the boundary of the mangoes is not complex, a graph contour
    tracking method based on chain code was used to detect the boundary of the mangoes.
    Some alignment techniques were employed to align the mangoes, such that obtaining
    the apex and stalk region of the mango. To determine the maturity of mangoes and
    sort them accordingly, a RGB averaging and gaussian mixture model was used. It
    was found after comparing the result with manual sorting and grading that the
    machine vision system was highly accurate. The variation of detection of mangoes
    from manual and machine vision system was not more than 2 %. The performance data
    is given below. Similar work was done by Khaled et al. (Mohi-Alden et al., 2022)
    to grade bell peppers based on 5 different by datasets, DCNN was used, ResNet50
    of DCNN was replaced with classifiers for global average pooling layer, the dense
    layer, batch normalization, and the dropout layer. Rokunuzzaman et al. (Rokunuzzaman
    and Jayasuriya, 2013) developed a low-cost machine vision system to sort tomatoes
    for 3 defects that occurs on tomato, first blossom end rot, second cracked and
    third calyx, differentiation was done on the base of thresholding techniques over
    a conveyor belt and sorting was done with a push mechanism that pushed the defected
    tomato for image processing. Red green and hue levels were considered, for identifying
    first defect red colour was used, for second and third green colour and parameter-
    based differentiation was considered, blob extraction was done to label the segmented
    images, all the processed data was then fed to a neural network to sort the defected
    tomato. Comparative research has been done to identify and grade tomatoes. The
    maximum accuracy obtained was 80.50%, while Kumar et al. (Dhakshina Kumar et al.,
    2020) devised a tomato grading and sorting system that provided accuracy of 97.74%,
    using a cascade approach of two SVM classifiers. The system operates in three
    stages: first, to classify tomatoes based on their species by considering shape,
    size, and texture; second, to classify them based on ripe and unripe and finding
    infected regions based on the Gabor wavelet transform; and third, to identify
    defects like black spots, canker, and melanoses based on colour and geometric
    features. 24 features of tomatoes were extracted. To do non-linear classification,
    a support vector machine (SVM) binary classifier was used to find the optimal
    hyper plane to separate two classes. ElMasry et al. (ElMasry et al., 2012) developed
    a method to sort potatoes based on their physical characteristics such as parameter,
    centroid, area, moment of inertia, length, width, and Fourier transform. All the
    perimeters helped differentiate potato based on two shape factors and Fourier
    descriptors. The potatoes were moving on the conveyor, a CCD camera coupled with
    a frame grabber was used to capture data. The grading system could successfully
    grade based on the size of the potatoes. A novel tomato ripening methodology was
    developed by Jang et al. (Ko et al., 2021), results for 5 classes were predicted.
    The method combined convolutional neural network and stochastic decision fusion;
    the workflow can be seen in the Fig. 20. Download : Download high-res image (219KB)
    Download : Download full-size image Fig. 20. The workflow for tomato ripeness
    (Fu et al., 2015). 3.6. Seeding Dong et al. (Dong et al., (2019)) monitored the
    performance of hybrid rice seed sowing using machine vision techniques. The seedlings
    in the pot tray were passed through an illuminated cabinet, as depicted in Fig.
    21. Download : Download high-res image (140KB) Download : Download full-size image
    Fig. 21. Seed detection apparatus used by Dong et al. (Dong et al., 2019). Image
    processing was done using of HLV thresholding model and exploratory methods, The
    algorithm detected missing seedlings. The model’s average accuracy was nearly
    95.8%. Chen et al. (Chen et al., 2019) developed a machine vision system for identifying
    the number of seed particles of rice or carrot. The system employed a mask function
    coupled with a square algorithm to emphasize the area containing relevant information
    while disregarding other areas. The algorithm significantly enhanced the contrast
    and brightness of the image, thereby facilitating binary image processing. The
    system then applied geometric matching and morphological experiments, followed
    by particle analysis, which included a particle algorithm, binarization thresholding,
    and binary particle analysis to arrive at the final detection result. Bai et al.
    (Bai et al., 2021) developed an automated seeding system for missing sweet corn
    detection. Outline extraction and skewness correction were done for image pre-processing,
    to identify missing seeds and precisely identify thresholding, contour extraction
    and minimum area rectangle extraction was done, thereafter the median filter and
    morphological processing was performed. In addition, RGB, HSV and Voting based
    algorithm were also used. A pneumatic seed planter can be depicted in the Fig.
    22, missing seeds through this machine was a problem in Phillipines, Borja et
    al. (Borja et al., 2018) integrated a machine with machine vision inside it to
    prevent missing seeds. Download : Download high-res image (86KB) Download : Download
    full-size image Fig. 22. (a)Prototype machine vision planter, (b) Simplified diagram
    of the seed meter (Kanagasingham et al., 2020). 3.7. Path finding The proper function
    of an autonomous robot or vehicle in an agricultural system is to carry out tasks
    in a specific location without causing damage to the plants, is heavily dependent
    on a guidance system. Machine vision which combines various algorithms and techniques
    is a crucial aspect of this guidance system. Rovira-Mas et al. (Rovira-Más et
    al., 2008) devised a machine vision based automated tractor guidance system that
    used a CCD camera with near infrared filter for driving across the crop rows precisely.
    The vision system output was used to steer the vehicle in a row. The vision system
    employed dynamic thresholding within the selected region of interest to binarize
    the image. Then utilized the midpoint encoding method to convert white pixels
    to a one-pixel width row, and applied a Hough transform subsequently. The data
    obtained was then utilized to guide the tractor through the field. Rovira-Mas
    et al. (Rovira-Más et al., 2008) generated a three-dimensional (3D) terrain map
    using a stereo camera, a localization sensor, and an inertial measurement unit
    (IMU). The stereo camera captures the fields and generates a 3D point cloud, then
    transformed into geodetic coordinates and assembled it to a global field map.
    It was shown that this system could generate 3D field maps with appropriate accuracy
    for field robotics. Sabeethan et al. (Kanagasingham et al., 2020) developed an
    autonomous guided robot that can precisely guide its way through a rice crop row
    without damaging the plant. A digital camera with wide angle lens was used. This
    novel crop row detection algorithm was developed combining the effect of 3D, it
    could predict the path through GNSS path planning, compass bearing selection and
    vision system. Bei he et al. (He et al., 2011) used MVS algorithms for recognition
    of the navigation path for a harvest robot for orchards based on machine vision,
    then created a binary image using OTSU segmentation. thereafter generated line
    using least square method, the intersection points of the lined fruit trees with
    respect to the ground were taken as features which can be depicted from the Fig.
    23. Download : Download high-res image (105KB) Download : Download full-size image
    Fig. 23. (a) The tree trunk area (b) Feature point nodes (c) Navigation line detection
    (Moallem et al., 2017). A novel approach was presented by Opiyo et al. (Opiyo
    et al., 2021) for navigation based on the extraction of medial axis. The feature
    extraction was done through grey scaling, Gabor filters, Principal Component,
    Analysis (PCA), K-mean clustering and medial axis algorithm. Based on that a path
    line was drawn for navigation. To control the guiding, 2 fuzzy logic-based controllers
    were deployed. Since, predicting path using hough transformation consumes time
    while using the least square method give low accuracy, Chen et al. (Chen et al.,
    2021) provided a novel point prediction based on the hough transformation technique
    and an improved grayscale method for path finding in optimum time. Various recent
    works related to the path finding and application have been mentioned by Vrochidou
    et al. (Vrochidou et al., 2022). 3.8. Soil classification Assessing the properties
    of soil, such as its moisture content or nutritional value, is a critical process,
    as the success of farming is contingent upon it. In recent years, various techniques
    based on machine vision systems have been developed to facilitate this purpose.
    Ajdadi et al. (Rahimi-Ajdadi et al., 2018) developed a rapid and non-contact system
    to measure soil water content using machine vision system. Since the colour of
    the soil is the most important factor in determining the moisture level, the colour
    was utilized as a feature for the machine vision system. Moreover, a slit change
    or instability in light can majorly affect the process; thus, to eliminate such
    problem a dome shape chamber was made with a square shape light on its top. 16
    levels of moisture were identified, and the model was developed to distinguish
    between them. Based on the RGB model, nine descriptive statistics were extracted.
    To predict the soil moisture content, two methods were adopted: an adaptive Nero-Fuzzy
    237 inference system (ANFIS) and stepwise multiple regression. Peter et al. (Riegler-Nurscher
    et al., 2020) developed a machine vision system to measure soil roughness and
    to measure tillage of the attached machine during seeding operation, the apparatus
    was attached to a cart with the tractor. The Stereo camera was used to measure
    the soil roughness, depth information helped measure the roughness, large height
    variations were indicative of rough soil and larger aggregate sizes. Four operations
    were carried out: first, with the use of block matching algorithm, stereo matching
    was done, then point cloud processing, followed by parameter estimation, and thereafter
    roughness estimation. A soil trace-based navigation system for tractors using
    machine vision was developed by Kiani et al. (Kiani et al., (2012,5).) for land
    and tilling operations, the information from previous ploughs in the field was
    used as a feature. A CCD colour camera captured the image of the ploughed, subsoiled,
    and furrowed soil then a Hough transform was done to highlight the path. Once
    the significant pattern of the path was obtained, the path was detected for movement
    of the vehicle for the respective three condition. Christos et al. (Chaschatzis
    et al., 2022) integrated different steps of Information Communication Technologies
    (ICT) solutions to assist crop monitoring, livestock management and farm management
    through information sharing for better decision making and practices.(See Table
    2, Table 3, Table 4, Table 5, Table 6, Table 7, Table 8, Table 9, Table 10, Table
    11, Table 12) Table 2. Application of various unsupervised machine learning algorithms
    for agricultural practices. Algorithm type Crop Task Application Reference K-Means
    Clustering Apple Defected pixel segmentation and classification Defects and disease
    detection in fruits (Dubey and Jalal, 2013) Banana Banana Finger and Flaws Segmentation
    Fruit grading (Hu et al., 2014)  Fuzzy clustering Maize Identification Row detection
    (Romeo et al., 2012) Wheat Identification Diseased leaf (Mondal and Kole, 2016)
    Table 3. Application of various Deep learning algorithms for agricultural practices.
    Type of objective Various Deep Algorithms being used Crop yield estimation Author-defined
    CNN, Five-unit LSTM, GatedRecurrent Unit (GRU) Fruit counting Modified Resnet
    CNN, CNN, Linear regression, Combined CNN + Linear Regression, Faster Region-based
    CNN with VGG16 model Soil moisture Deep belief network based macroscopic cellular
    automata (DBNMCA), Stepwise multiple regression, ANN Weed detection PCANet + LMC
    classifiers, Variation of VGG16, DenseNet CNN, Adapted version of Inceptionv3
    + lightweight DCNN + set of Klightweight models as a mixture model (MixDCNN),
    Based on DetectNet CNN, SNN, CNN Crop type classification Author-defined CNN,
    Adapted version of VGG16, three-unit LSTM, CNN + HistNN Plant disease detection
    LeNet CNN, AlexNet CNN Table 4. Various Weed, Pest and Disease detection using
    machine vision. Crop Algorithm Disease/Weed/Pest type Reference Peanut field Em
    Yolo tiny Rust disease (Zhang et al., 2022) Onion, Soybean, Corn, Beans, and Rice
    RGB based Row detection based (Terra et al., 2021) Peach orchards Colour-depth
    fusion segmentation method Spray row detection (Gao et al., 2020) Pear Segnet
    Fruit (Kim et al., 2020) Lettuce Non-linear Bayesian Discriminant analysis, Color
    thresholding Weed (Blasco et al., 2002) Sugar beat Discriminant analysis Leaves
    occlusion and their overlaps (Jafari et al., 2006) Wild Blueberry Linear and quadratic
    classifiers (DM-HSIS) Goldenrod weed spot (Rehman et al., 2019) Sugar beet Principal
    component analysis, ANN and SVM Shape features including moment invariants and
    Fourier descriptors (Bakhshipour and Jafari, 2018) Maize Random Forest Convolvulus
    arvensis, Rumex, Cirsium arvense (Gao et al., 2018) Soybean CNN Cephalanoplos,
    Digitaria, Bindweed (Tang et al., 2017) Pea MLC, VI, and SAM Diplotaxis spp. (Castro
    et al., 2012) Sugarcane SVM Sugar cane borer disease (Huang et al., 2018) Bell
    peppers PCA-based Algorithm, CV Algorithms Tomato spotted wilt virus, Powdery
    mildew (Schor et al., 2016) Grape BP Networks Classifier Grape downy mildew, Grape
    powdery mildew (Wang et al., 2012) Paddy GeneticAlgorithm, Rule Generation Algorithm
    Bacterial blight, Leaf brown spot, Rice blast, Sheath rot pest (Phadikar et al.,
    2013) Strawberry greenhouse SVM Thysanoptera pest (Ebrahimi et al., 2017) Green
    leaves Multispectral MVS (NIR, UV, VISIBLE) Invertebrate pests (Liu and Chahl,
    2018) Crop CNN 33 class insects (Kasinathan et al., 2021) Rice k-means, ANN Blast
    disease (Ramesh and Vydeki, 2018) Apple leaves YOLOv4 Venturia inaequalis, Gymnosporangium
    juniperi-virginianae (Roy and Bhaduri, 2021) Table 5. Machine vision for harvesting
    for various crop types. Type of crop Application Features Algorithm Reference
    Kiwi Novel multi arm robot FCN-8S for features Stereo point matching, Kiwifruit
    clustering, Scheduling algorithm (Williams et al., 2019) Broccoli Autonomous selective
    harvester Area of connected textures Color based segmentation, Dice similarity
    coefficient (Blok et al., 2016) Rice Combine harvester YCbCr and target demarcation
    Hierarchical clustering method and polynomial fitting method (Zhang et al., 2020)
    Tobacco leaf – GLTP (Gray Level Local Texture Patterns), LBP (Local Binary Pattern)
    and LBPV (Local Binary Pattern Variance) K-Nearest Neighbour (K-NN) based on Euclidean
    (Guru et al., 2012) EGG plants Intelligent rover-based robot Color characteristics
    and morphological features Logical operations (Nasirahmadi et al., 2021) Harvesting
    time for fresh tea leaves Guiding rail Color characteristic and geometric properties
    B-G algorithm, Median filter algorithm, Otsu algorithm, Bayesian discriminant
    principle (Zhang et al., 2019) Date fruit – Type, Maturity, Harvesting decision
    AlexNet and VGGNet (Altaheri et al., 2019) Tomato – Geometric, Random noise CNN
    (Zhang et al., 2018) Table 6. Machine vision for Overlapping fruits detection.
    Crop Algorithm Accuracy Reference Lychee overlapping CLAHE, Red/blue chromatic
    mapping, Otsu thresholding, Relative position relation, Three-point circle, LBP-SVM
    87% (Guo et al., 2019) Winter jijube overlapping YOLOv3 97.28% (Lu et al., 2021)
    Kiwi overlapping detection at night OTSU thresholding and hough transformation
    _ (Fu et al., 2015) Overlapping fruit segmentation Novel marker-controlled watershed
    transform Accuracy nearly 99% (Zeng et al., 2009) Small size immature tomato Improved
    featured pyramid network Accuracy 99.5% (Sun et al., 2020) Overlapping citrus
    Mask R-CNN, Convex shell algortihtm, Shi-Tomasi corner detection algorithm, least
    squares fitting method – (Longye et al., 2019) Grapes Phenotypic characteristics
    Edge detection, Contour fitting improved HED (Miao et al., 2021) Table 7. Performance
    table (Nandi et al., 2013). Variety Q1 Q2 Q3 Q4 Expert Systems Expert System Expert
    System Expert System KU 91.2 90.4 89.4 88.7 90.3 89.4 90.2 89.4 AM 90.7 90.1 90.0
    89.1 88.9 88.2 90.1 89.4 SO 90.2 89.7 90.7 89.1 90.0 88.3 89.8 89.5 LA 91.1 90.5
    90.9 88.6 89.2 88.8 91.0 89.7 HI 90.1 89.4 90.6 90.0 90.4 89.1 91.3 90.0 Table
    8. Machine vision for sorting and grading. Application Algorithm Accuracy Reference
    Grading and sorting tomato Cascading SVM classifier, Binary classifiaction, Gabor
    wavelet transformation Average accuracy 97.74 % (Dhakshina Kumar et al., 2020)
    Sorting irregular potatoes Fourier transform, Stepwise linear discriminant analysis
    Accuracy of 96.6% for inline and 100% for perfectly shaped (ElMasry et al., 2012)
    Sorting and grading mangoes Gaussian mixture model, Size from binary image, fuzzy
    Logic Accuracy ranging from 88% to nearly 92% (Nandi et al., 2013) Sorting of
    bell peppers Modified DCNN Accuracy of 96.6% (Mohi-Alden et al., 2022) Tomato
    ripeness-based sorting Combined ConvNet of YoloV3 and SDF Accuracy 96% (Ko et
    al., 2021) Potato grading based on size Color space conversion, Partial least
    squares discriminant analysis Accuracy 86% (Islam et al., 2021) Carrot shape-based
    sorting HOG descriptor, KNN, K Fold, CNN – (Sharma et al., 2021) Infrared based
    pineapple grading Otsu thresholding, LDA, QDA, SVM, KNN, Decision tree, naive
    bayes Maximum accuracy with SVM (Mohd Ali et al., 2022) Golden apple grading based
    on thermal imaging SVM, KNN, MLPs 92.50% (Moallem et al., 2017) Table 9. Machine
    vision systems for seeding. Application Algorithm Accuracy Reference Missing sweetcorn
    detection Thresholding, Contour Extraction, Minimum area Rectangle extraction,
    Median filter, Morphological Processing Average accuracy 98 % (Bai et al., 2021)
    Hybrid Rice Pot-Tray Sowing Threshold segmentation & exploratory analysis method
    Average accuracy 95.68% (Dong et al., 2019) Missed seeding in pneumatic corn planter
    detection RGB, OTSU thresholding – (Borja et al., 2018) Table 10. Various vision
    systems for path finding. Working Principle Algorithm Application Reference To
    steer vehicle Binarization, Dynamic thresholding Over Rough Terrain (Rovira-Más
    et al., 2008) Navigation Path for apple orchard OTSU segmentation, region segmentation
    Ground and tree intersection points (Opiyo et al., 2021) Autonomous navigation
    Gabor filters, Principal Component, Analysis (PCA), K-mean clustering and medial
    axis algorithm Removing environment except the path (Chen et al., 2019) Navigation
    for green house cucumber robot Novel prediction point hough transform, Improved
    grayscale Based on point prediction (Chen et al., 2021) Under-plant canopy robot
    Extended Kalman Filter, CNN, ResNet-18, ImageNet Crop row vanishing point, and
    distance ratio (Sivakumar et al., ,2021) Monocular guidance Level-adjustment thresholding
    to discriminate rows from gaps, an averaging technique using a viewport to locate
    rows, rregression analysis to fit the best line 0.020 m accuracy at a speed of
    1 m/s (Billingsley and Schoenfisch, 1997) Tractor localization for vineyard Extended
    Kalman Filter (EKF), adaptive data selection, Lower part of mage cropped, green
    plane, extraction from cropped image, thresholding to, extract path plane, filtering
    to remove noise, centroid of path plane determination Tested on a GEARs Surface
    Mobility Platform in a laboratory setting and in a peach orchard (Corno et al.,
    2021) Adaptive vision navigation for smart agri robots SURF for feature extraction
    and matching to obtain feature pairs, confidence density image construction by
    integrating the enhanced elevation image and the corresponding binarized crop
    row image Tested on a smart agricultural robot manufactured in Shanghai, China
    on S-type and O-type in-lab simulated crop plant leaves paths (Zhang et al., 2013)
    Table 11. Machine vision systems for soil classification. Application Feature
    Algorithm Reference Moisture levels RGB Nero fuzzy 237, Stepwise multiple regression
    (Rahimi-Ajdadi et al., 2018) Soil roughness Block matching algorithm, RGB - (Riegler-Nurscher
    et al., 2020) Soil 3d reconstruction Stereo binocular method Shift algorithm and
    depth mapping (Chaschatzis et al., 2022) Rapidly predicting SOM Color space transformation
    K means clustering (Azizi et al., 2021) SOM and SMC Histogram analysis, colour
    space conversion ANN, Cubist, Exponential GPR (Taneja et al., 2021) Table 12.
    Machine vision systems for pesticide spraying. Crop Algorithm Disease Reference
    Apple leaf Mask R-CNN Rust disease (Storey et al., 2022) Onion, Soybean, Corn,
    Beans, and Rice RGB based row detection based (Terra et al., (2021,5)) Pear orchards
    Colour-depth fusion segmentation method Spray row detection (Gao et al., 2020)
    Tulip Multispectral, RGB Tulip breaking virus (Polder et al., 2012) Leaf spot
    and white flies Surf algorithm Disease (Dhumale and Bhaskar, 2021) Pear SegNet
    Fruit (Kim et al., 2020) Azizi et al. (Azizi et al., 2021) estimated the soil
    surface roughness using a stereo approach in which they reconstructed the soil
    in 3d using disparity calculation with the equations one and two mentioned below.
    It was used over two different images obtained from binocular method. The coordinates
    between two points were determined by sift algorithm, the workflow of their method
    can be seen from Fig. 24. (1) (2) Download : Download high-res image (119KB) Download
    : Download full-size image Fig. 24. Workflow for 3d reconstruction of the soil
    surface(Chen et al., 2021). A novel method was developed by Gorthi et al. (Gorthi
    et al., 2021) for predicting organic matter in the soil based on soil image segmentation
    and a tree-based pipeline optimization tool to determine optimum ML scheme that
    can be used to predict model score with sufficient accuracy. Taneja et al. (Taneja
    et al., 2021) developed a method to determine the soil moisture and soil organic
    matter content using machine vision with a cell phone. In this approach, numerous
    machine learning model such as linear regression, Decision/Regression Trees, Support
    Vector Machines (SVM), Gaussian Process Regression (GPR), random forest and cubist,
    and other models including Artificial Neural Network (ANN) were compared. The
    best score for moisture levels were obtained through exponential gaussian progression
    model and cubist model while for the soil organic model, ANN and cubist provided
    the most accuracy. 3.9. Pesticide spraying The autonomous spraying system incorporates
    two core technologies that are sensing technology for the region of interest detection
    and a robotic spray execution or chemical spray system, this can be observed in
    the Fig. 25 which depicts smart spraying system (Song et al., 2015). Download
    : Download high-res image (165KB) Download : Download full-size image Fig. 25.
    Smart spraying system. To do precise spraying in peach orchards, which was difficult
    due to the dense canopy and complex background Gao et al. (Gao et al., 2020) developed
    a path planning algorithm based on a colour depth vision system. The system intelligently
    uses colour and depth for path planning. In image segmentation, a combination
    of RGB green segmentation with HSV green segmentation on ROI was used. For distance
    segmentation, a k-means algorithm was initially applied, followed by segmentation.
    A combination of both image segmentation and distance segmentation was performed
    to form colour-depth fusion segmentation that provides accurate path planning.
    Kim et al (Kim et al., 2020) deployed a smart pesticide spraying system over a
    moving rover for pear orchards. A SegNet structure was used for the purpose, as
    well as semantic segmentation. Five distinct classes of pears were used to train
    the model. The background environment posed a problem for accurate detection of
    pears, to prevent background noise, depth data was used from the RGB-D camera.
    All this was mounted on the moving rover platform, and the spraying was done with
    the help of 4 nozzles attached on both sides, as can be seen from the Fig. 26.
    In addition, a mapping mechanism was developed considering the nozzle orientation,
    position, and area of spray. Download : Download high-res image (316KB) Download
    : Download full-size image Fig. 26. Moving rover with attached sprayer(red) and
    camera(green) (Kim et al., 2020). (For interpretation of the references to color
    in this figure legend, the reader is referred to the web version of this article.)
    For apple orchards, Storey et al. (Storey et al., 2022) developed an intelligent
    spraying system using a machine vision system to reduce chemical usage and prevent
    unwanted splattering of pesticides during spraying. The system could identify
    apple leaf disease such as rusting and can extend its use to identify other crop
    disease and weeds for reducing the spray chemical usage. A mask R-CNN network
    was used for segmentation, this system was based on backbones (ResNet 50, MobileNet
    v3 layer and MobileNet layer mobile) that were combined for leaf and disease detection,
    this network can precisely detected rust on leaves. At the end of ResNet 50, a
    disease detection confusion matrix was used to detect combinations of rusty and
    healthy leaves. Additionally, most components of the system could also be utilized
    for weed detection, enabling the development of a machine vision system focused
    on weed identification. 4. Conclusion In conclusion, this survey paper has offered
    a comprehensive examination of recent advancements and applications of machine
    vision technology in the field of agriculture. Through a thorough analysis of
    the literature, we have demonstrated the various ways in which machine vision-based
    systems have been utilized, including crop counting, harvesting, pesticide elimination,
    yield prediction, disease detection, and weed identification. Furthermore, we
    have delved into the intricacies of the various processes that comprise a machine
    vision system, such as image acquisition, image processing, and image classification,
    highlighting the specific operations that are performed either alone or are cascaded
    to achieve the specific task. Additionally, we have emphasized the utilization
    of deep learning and CNNs to classify, improve accuracy, and enhance the robustness
    of these systems. The utilization of multi-spectral imaging and hyperspectral
    imaging to extract detailed information about crop health and growth has also
    been explored. To further improve the detection accuracy, we have also discussed
    the use of remote sensing operations to map terrain and the implementation of
    a binocular system. We have also highlighted the integration of machine vision
    systems with other technologies, such as drones, rovers, and robots. It is evident
    that the advancements in machine vision technology have the potential to revolutionize
    precision agriculture, resulting in increased crop yields, decreased costs, and
    more sustainable farming practices. However, it is important to bear in mind the
    limitations of the technology, such as the cost of the equipment and the need
    for proper maintenance and calibration, as well as the fact that the system may
    not be effective in certain environmental conditions. Furthermore, there is potential
    for further research on the integration of machine vision systems with other technologies
    such as IoT and AI to further enhance the capabilities of these systems, as well
    as work on adaptive light systems, different fruit and crops harvesting, real-time
    yield counting, and the development of actuators and manipulators for precision
    cutting of delicate crops. Compliance with Ethical Standards: The authors would
    like to declare that this research does not involve any human participants or
    animals and there is no conflict of interest. Declaration of Competing Interest
    The authors declare that they have no known competing financial interests or personal
    relationships that could have appeared to influence the work reported in this
    paper. Data availability No data was used for the research described in the article.
    References “Hyperspectral Viewer”, 2022 “Hyperspectral Viewer.” Getting Started
    with Hyperspectral Image Processing - MATLAB amp; Simulink. Accessed December
    14, 2022. https://www.mathworks.com/help/images/getting-started-with-hyperspectral-image-analysis.html.
    Google Scholar Aggelopoulou et al., (2011,6). K. Aggelopoulou, D. Bochtis, S.
    Fountas, K. Swain, T. Gemtos, G. Nanos Yield prediction in apple orchards based
    on image processing Precis. Agric., 12 (2011,6.), pp. 448-456 Google Scholar AGRIOS,
    2005 AGRIOS, G. chapter eight - PLANT DISEASE EPIDEMIOLOGY. Plant Pathology (Fifth
    Edition). pp. 265-291(2005), https://www.sciencedirect.com/science/article/pii/B9780080473789500142.
    Google Scholar Ahmed et al., 2012 F. Ahmed, H. Al-Mamun, A. Bari, E. Hossain,
    P. Kwan Classification of crops and weeds from digital images: A support vector
    1065 machine approach Crop Prot., 40 (2012), pp. 98-104 View PDFView articleView
    in ScopusGoogle Scholar Akhter and Sofi, 2022 R. Akhter, S.A. Sofi Precision agriculture
    using IoT data analytics and machine learning Journal of King Saud University-Computer
    and Information Sciences, 34 (8) (2022), pp. 5602-5618 View PDFView articleView
    in ScopusGoogle Scholar Altaheri et al., 2019 H. Altaheri, M. Alsulaiman, G. Muhammad
    Date fruit classification for robotic harvesting in a natural environment using
    deep learning IEEE Access, 7 (2019), pp. 117115-117133 CrossRefView in ScopusGoogle
    Scholar Amatya et al., 2016 Amatya, S., Karkee, M., Gongal, A., Zhang, Q. & Whiting,
    M. Detection of cherry tree branches with full foliage in planar architecture
    for automated sweet-cherry harvesting. Biosystems Engineering. 146 pp. 3-15 (2016),
    https://www.sciencedirect.com/science/article/pii/S1 Special Issue: Advances in
    Robotic Agriculture for Crops. Google Scholar Amatya et al., 2017 S. Amatya, M.
    Karkee, Q. Zhang, M. Whiting Automated Detection of Branch Shaking Locations for
    Robotic Cherry Harvesting Using Machine Vision Robotics, 6 (2017) https://www.mdpi.com/2218-6581/6/4/31
    Google Scholar Andersen et al., 2005 H. Andersen, L. Reng, K. Kirk Geometric plant
    properties by relaxed stereo vision using simulated annealing Computers And Electronics
    In Agriculture., 49 (2005), pp. 219-232 View PDFView articleView in ScopusGoogle
    Scholar Azizi et al., 2021 A. Azizi, Y. Abbaspour-Gilandeh, T. Mesri-Gundoshmian,
    A. Farooque, H. Afzaal Estimation of Soil Surface Roughness Using Stereo Vision
    Approach Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/13/4386 Google Scholar
    Bai et al., 2021 J. Bai, F. Hao, G. Cheng, C. Li Machine vision-based supplemental
    seeding device for plug seedling of sweet corn Computers And Electronics In Agriculture.,
    188 (2021), Article 106345 https://www.sciencedirect.com/science/article/pii/S0168169921003628
    View PDFView articleView in ScopusGoogle Scholar Bakhshipour and Jafari, 2018
    A. Bakhshipour, A. Jafari Evaluation of support vector machine and artificial
    neural networks in weed detection using shape features Computers And Electronics
    in Agriculture., 145 (2018), pp. 153-160 View PDFView articleView in ScopusGoogle
    Scholar Bandi et al., 2013 S. Bandi, A. Varadharajan, A. Chinnasamy Performance
    evaluation of various statistical classifiers in detecting the diseased 1048 citrus
    leaves International Journal Of Engineering Science And Technology., 5 (2013),
    pp. 298-307 Google Scholar Bazame et al., 2021 H. Bazame, J. Molin, D. Althoff,
    M. Martello Detection, classification, and mapping of coffee fruits during harvest
    with computer vision Computers And Electronics in Agriculture., 183 (2021), Article
    106066 https://www.sciencedirect.com/science/article/pii/S01681699210 View PDFView
    articleView in ScopusGoogle Scholar Billingsley and Schoenfisch, 1997 J. Billingsley,
    M. Schoenfisch The successful development of a vision guidance system for agriculture.
    Computers And Electronics In Robotics in Agriculture Agriculture, 16 (1997), pp.
    147-163 https://www.sciencedirect.com/science/article/pii/S0168169996000348, View
    PDFView articleView in ScopusGoogle Scholar Bini et al., 2022 D. Bini, D. Pamela,
    D. Shamia, S. Prince Others Intelligent agrobots for crop yield estimation using
    computer vision Computer Assisted Methods in Engineering And Science., 29 (2022),
    pp. 161-175 View in ScopusGoogle Scholar Blasco et al., 2002 J. Blasco, N. Aleixos,
    J. Roger, G. Rabatel, E. Moltó AE—Automation and emerging technologies: Robotic
    weed control using machine vision Biosyst. Eng., 83 (2002), pp. 149-157 https://www.sciencedirect.com/science/article/pii/S1537511002901091
    View PDFView articleView in ScopusGoogle Scholar Blasco et al., 2009 J. Blasco,
    S. Cubero, J. Gómez-Sanchıs, P. Mira, E. Moltó Development of a machine for the
    automatic sorting of pomegranate (Punica granatum) arils based on computer vision
    J. Food Eng., 90 (2009), pp. 27-34 View PDFView articleView in ScopusGoogle Scholar
    Blok et al., 2016 Blok, P., Barth, R. & Van den Berg, W. Machine vision for a
    selective broccoli harvesting robot. IFAC-PapersOnLine. 49, 66-71 (2016), 5th
    IFAC Conference on Sensing, Control and Automation Technologies for Agriculture
    AGRICONTROL 2016 https://www.sciencedirect.com/science/article/pii/S2405896316315749.
    Google Scholar Borja et al., 2018 A. Borja, R. Amongo, D. Suministrado, J. Pabico
    A machine vision assisted mechatronic seed meter for precision planting of corn
    2018 3rd InternatiOnal COnference On COntrol And Robotics Engineering (ICCRE)
    (2018), pp. 183-187 CrossRefView in ScopusGoogle Scholar Brill et al., 2020 Brill,
    F., Erukhimov, V., Giduthuri, R. & Ramm, S. Chapter 1 - Introduction. OpenVX Programming
    Guide. pp. 1-13(2020), https://www.sciencedirect.com/science/article/pii/B9780128164259000073.
    Google Scholar Bulanon et al., 2009 D. Bulanon, T. Burks, V. Alchanatis Image
    fusion of visible and thermal images for fruit detection Biosyst. Eng., 103 (2009),
    pp. 12-22 View PDFView articleView in ScopusGoogle Scholar Castro et al., 2012
    A. Castro, M. Jurado-Expósito, J. Peña-Barragán, F. López-Granados Airborne multi-spectral
    imagery for mapping cruciferous weeds in cereal and legume crops Precis. Agric.,
    13 (2012), pp. 302-321 CrossRefGoogle Scholar Cen et al., 2016 H. Cen, R. Lu,
    Q. Zhu, F. Mendoza Nondestructive detection of chilling injury in cucumber fruit
    using hyperspectral 1041 imaging with feature selection and supervised classification
    Postharvest Biology And Technology, 111 (2016), pp. 352-361 View PDFView articleView
    in ScopusGoogle Scholar Chang and Lin, 2018 Chang, C. & Lin, K. Smart Agricultural
    Machine with a Computer Vision-Based Weeding and Variable-Rate Irrigation Scheme
    Robotics. 7 (2018), https://www.mdpi.com/2218-6581/7/3/38. Google Scholar Chang
    and Lin, 2018 C. Chang, K. Lin Smart agricultural machine with a computer vision-based
    weeding and variable-rate irrigation scheme Robotics, 7 (2018), p. 38 Google Scholar
    Chang et al., 2021 C. Chang, B. Xie, S. Chung Mechanical Control with a Deep Learning
    Method for Precise Weeding on a Farm Agriculture, 11 (2021) https://www.mdpi.com/2077-0472/11/11/1049
    Google Scholar Chaschatzis et al., 2022 Chaschatzis, C., Lytos, A., Bibi, S.,
    Lagkas, T., Petaloti, C., Goudos, S., ... & Sarigiannidis, P. (2022, June). Integration
    of Information and Communication Technologies in Agriculture for Farm Management
    and Knowledge Exchange. In 2022 11th International Conference on Modern Circuits
    and Systems Technologies (MOCAST) (pp. 1-4). IEEE. Google Scholar Chaschatzis
    et al., 2022 C. Chaschatzis, C. Karaiskou, E.G. Mouratidis, E. Karagiannis, P.G.
    Sarigiannidis Detection and characterization of stressed sweet cherry tissues
    using machine learning Drones, 6 (1) (2022), p. 3 View in ScopusGoogle Scholar
    Chen et al., 2019 Chen, Y., Barzee, T., Zhang, R. & Pan, Z. Chapter 9 - Citrus.
    Integrated Processing Technologies for Food And Agricultural By-Products. pp.
    217-242 (2019), https://www.sciencedirect.com/science/article/pii/B9780128141380000095.
    Google Scholar Chen et al., 2019 Chen, S.X. and Jiao, L. and Xu, H.X. and Xu,
    J.M., L. Research on The Precision Seeding System for Tiny Particle Seed Based
    on Machine Vision Chemical Engineering Transactions. 19 (2019,12). Google Scholar
    Chen et al., 2020 C. Chen, B. Li, J. Liu, T. Bao, N. Ren Monocular positioning
    of sweet peppers: An instance segmentation approach for harvest robots Biosyst.
    Eng., 196 (2020), pp. 15-28 View PDFView articleGoogle Scholar Chen et al., 2021
    J. Chen, H. Qiang, J. Wu, G. Xu, Z. Wang Navigation path extraction for greenhouse
    cucumber-picking robots using the prediction-point Hough transform Computers And
    Electronics In Agriculture., 180 (2021), Article 105911 View PDFView articleView
    in ScopusGoogle Scholar Chen et al., 2003 B. Chen, S. Tojo, K. Watanabe Machine
    Vision for a Micro Weeding Robot in a Paddy Field Biosyst. Eng., 85 (2003), pp.
    393-404 https://www.sciencedirect.com/science/article/pii/S1537511003000783 View
    PDFView articleView in ScopusGoogle Scholar Chiu et al., 2020 Chiu, M., Xu, X.,
    Wei, Y., Huang, Z., Schwing, A., Brunner, R., Khachatrian, H., Karapetyan, H.,
    Dozier, I., Rose, G. & Others Agriculture-vision: A large aerial image database
    for agricultural pattern analysis. Proceedings Of The IEEE/CVF Conference on Computer
    Vision And Pattern Recognition. pp. 2828-2838 (2020). Google Scholar Cloutier
    and Leblanc, 2001 Cloutier, D. & Leblanc, M. Mechanical Weed Control in Agriculture.
    Physical Control Methods in Plant Protection. pp. 191-204 (2001), 10.1007/978-3-662-04584-813.
    Google Scholar Corno et al., 2021 M. Corno, S. Furioli, P. Cesana, S. Savaresi
    Adaptive Ultrasound-Based Tractor Localization for Semi-Autonomous Vineyard Operations
    Agronomy, 11 (2021) https://www.mdpi.com/2073-4395/11/2/287 Google Scholar Costa,
    2019 Costa, C., 1. Consiglio per la ricerca in agricoltura e l’analisi dell’economia
    agraria (CREA), Centro di ricerca Ingegneria e Trasformazioni Agroalimentari,
    Via della Pascolare 16, 00015 Monterotondo scalo, Rome, Italy, Febbi, P., Pallottino,
    F., Cecchini, M.,Figorilli, S., Antonucci, F., Menesatti, P. & 2. Department of
    Agriculture and Forestry Science, Tuscia University, Via S. Camillo deLellis snc,
    01100 Viterbo, Italy Stereovision system for estimating tractors and agricultural
    machines transit area under orchards canopy. Int. J. Agric. Biol. Eng.. 12, 1-5
    (2019). Google Scholar Cravero et al., 2022 A. Cravero, S. Pardo, S. Sepúlveda,
    L. Muñoz Challenges to Use Machine Learning in Agricultural Big Data: A Systematic
    Literature Review Agronomy, 12 (3) (2022), p. 748 CrossRefView in ScopusGoogle
    Scholar Da Silva and Mendonça, 2005 Da Silva, E. & Mendonça, G. 4 - Digital Image
    Processing. The Electrical Engineering Handbook. pp. 891-910(2005), https://www.sciencedirect.com/science/article/pii/B9780121709600500645.
    Google Scholar Dale et al., 2013 L. Dale, A. Thewis, C. Boudry, I. Rotar, P. Dardenne,
    V. Baeten, J. Pierna Hyperspectral imaging applications in agriculture and agro-food
    product quality and safety control: A review Appl. Spectrosc. Rev., 48 (2013),
    pp. 142-159 CrossRefView in ScopusGoogle Scholar Dattner and Bohn, 2016 M. Dattner,
    D. Bohn 20 - Characterization of Print Quality in Terms of Colorimetric Aspects
    Printing On Polymers. (2016), pp. 329-345 https://www.sciencedirect.com/science/article/pii/B9780323374682000208
    View PDFView articleGoogle Scholar Deng et al., 2009 Deng, J., Dong, W., Socher,
    R., Li, L., Li, K. & Fei-Fei, L. Imagenet: A large-scale hierarchical image database.
    2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248-255 (2009).
    Google Scholar Dhakshina Kumar et al., 2020 S. Dhakshina Kumar, S. Esakkirajan,
    S. Bama, B. Keerthiveena A microcontroller-based machine vision approach for tomato
    grading and sorting using SVM classifier Microprocessors And Microsystems., 76
    (2020), Article 103090 View PDFView articleView in ScopusGoogle Scholar Dhumale
    and Bhaskar, 2021 N. Dhumale, P. Bhaskar Smart Agricultural Robot for Spraying
    Pesticide with Image Processing based Disease Classification Technique 2021 International
    Conference on Emerging Smart Computing and Informatics (ESCI) (2021), pp. 604-609
    CrossRefView in ScopusGoogle Scholar Domingues et al., 2022 T. Domingues, T. Brandão,
    J.C. Ferreira Machine Learning for Detection and Prediction of Crop Diseases and
    Pests: A Comprehensive Survey Agriculture, 12 (9) (2022), p. 1350 CrossRefView
    in ScopusGoogle Scholar Dong et al., 2019 W. Dong, X. Ma, H. Li, S. Tan, L. Guo
    Detection of Performance of Hybrid Rice Pot-Tray Sowing Utilizing Machine Vision
    and Machine Learning Approach Sensors (Basel), 19 (12) (2019) Google Scholar Dorj
    et al., 2013 U. Dorj, M. Lee, S. Han A comparative study on tangerine detection,
    counting and yield estimation algorithm International Journal Of Security And
    Its Applications., 7 (1) (2013), pp. 405-412 View in ScopusGoogle Scholar Druzhkov
    and Kustikova, 2016 P. Druzhkov, V. Kustikova A survey of deep learning methods
    and software tools for image classification and object detection. Pattern Recognition
    And Image Analysis, 26 (2016), pp. 9-15, 10.1134/S1054661816010065 View in ScopusGoogle
    Scholar Du et al., 2019 Z. Du, Y. Hu, N. Ali Buttar, A. Mahmood X-ray computed
    tomography for quality inspection of agricultural products: A review Food Sci.Nutrition.,
    7 (2019), pp. 3146-3160 https://onlinelibrary.wiley.com/doi/abs/10.1002/fsn3.1179
    CrossRefView in ScopusGoogle Scholar Dubey and Jalal, 2013 S. Dubey, A. Jalal
    Adapted approach for fruit disease identification using images Image Processing:
    Concepts, Methodologies, Tools, And Applications. (2013), pp. 1395-1409 CrossRefView
    in ScopusGoogle Scholar Ebrahimi et al., 2017 M. Ebrahimi, M. Khoshtaghaza, S.
    Minaei, B. Jamshidi Vision-based pest detection based on SVM classification method
    Computers And Electronics In Agriculture., 137 (2017), pp. 52-58 https://www.sciencedirect.com/science/article/pii/S016816991631136X
    View PDFView articleView in ScopusGoogle Scholar ElMasry et al., 2012 G. ElMasry,
    S. Cubero, E. Moltó, J. Blasco In-line sorting of irregular potatoes by using
    automated computer-based machine vision system J. Food Eng., 112 (2012), pp. 60-68
    https://www.sciencedirect.com/science/article/pii/S0260877412001690 View PDFView
    articleView in ScopusGoogle Scholar Elstone et al., 2020 L. Elstone, K. How, S.
    Brodie, M. Ghazali, W. Heath, B. Grieve High Speed Crop and Weed Identification
    in Lettuce Fields for Precision Weeding Sensors, 20 (2020) https://www.mdpi.com/1424-8220/20/2/455
    Google Scholar Estes et al., 2001 Estes, J., Kline, K. & Collins, E. Remote Sensing.
    International Encyclopedia Of The Social Behavioral Sciences. pp. 13144-13150
    (2001), https://www.sciencedirect.com/science/article/pii/B0080430767025262. Google
    Scholar Fu et al., 2015 Fu, L., Bin, W., Yongjie, C., Shuai, S., Gejima, Y. &
    Kobayashi, T. Kiwifruit recognition at nighttime using artificial lighting based
    on machine vision. Int J Agric Biol Eng. 2015 pp. 52-59 (2015,8). Google Scholar
    Gao et al., 2020 Gao, G., Xiao, K. & Jia, Y. A spraying path planning algorithm
    based on colour-depth fusion segmentation in peach orchards. Com- 1146 puters
    And Electronics In Agriculture. 173 pp. 105412 (2020), https://www.sciencedirect.com/science/article/pii/S0168169920301733.
    Google Scholar Gao et al., 2018 J. Gao, D. Nuyttens, P. Lootens, Y. He, J. Pieters
    Recognising weeds in a maize crop using a random forest machine-learning algorithm
    and near-infrared snapshot mosaic hyperspectral imagery Biosyst. Eng., 170 (2018),
    pp. 39-50 View PDFView articleView in ScopusGoogle Scholar Giannoni et al., 2018
    L. Giannoni, F. Lange, I. Tachtsidis Hyperspectral imaging solutions for brain
    tissue metabolic and hemodynamic monitoring: past, current and future developments
    J. Opt., 20 (2018), Article 044009 CrossRefView in ScopusGoogle Scholar Giménez-Gallego
    et al., 2020 J. Giménez-Gallego, J. González-Teruel, M. Jiménez-Buendía, A. Toledo-Moreo,
    F. Soto-Valles, R. Torres-Sánchez Segmentation of Multiple Tree Leaves Pictures
    with Natural Backgrounds using Deep Learning for Image-Based Agriculture Applications
    Appl. Sci., 10 (2020) https://www.mdpi.com/2076-3417/10/1/202 Google Scholar Gomez
    et al., 2021 A. Gomez, M. Darbyshire, J. Gao, E. Sklar, S. Parsons Towards practical
    object detection for weed spraying in precision agriculture CoRR. abs/2109.11048
    (2021) https://arxiv.org/abs/2109.11048 Google Scholar Gómez-Sanchis et al., 2014
    J. Gómez-Sanchis, D. Lorente, E. Soria-Olivas, N. Aleixos, S. Cubero, J. Blasco
    Development of a hyperspectral computer vision system based on two liquid crystal
    tuneable filters for fruit inspection. application to detect citrus fruits decay
    Food And Bioprocess Technology., 7 (4) (2014), pp. 1047-1056, 10.1007/s11947-013-1158-9
    View in ScopusGoogle Scholar Gorthi et al., 2021 S. Gorthi, R. Swetha, S. Chakraborty,
    B. Li, D. Weindorf, S. Dutta, H. Banerjee, K. Das, K. Majumdar Soil organic matter
    prediction using smartphone-captured digital images: Use of reflectance image
    and image perturbation Biosyst. Eng., 209 (2021), pp. 154-169 https://www.sciencedirect.com/science/article/pii/S1537511021001422
    View PDFView articleView in ScopusGoogle Scholar Guevara-Hernandez and Gil, 2011
    F. Guevara-Hernandez, J. Gil A machine vision system for classification of wheat
    and barley grain kernels Span. J. Agric. Res., 672–680 (2011) Google Scholar Guo
    et al., 2019 Q. Guo, Y. Chen, Y. Tang, J. Zhuang, Y. He, C. Hou, X. Chu, Z. Zhong,
    S. Luo Lychee Fruit Detection Based on Monocular Machine Vision in Orchard Environment
    Sensors (Basel), 19 (2019) Google Scholar Guru et al., 2012 Guru, D., Mallikarjuna,
    P., Manjunath, S. & M. M. Shenoi Machine Vision Based Classification of Tobacco
    Leaves for Automatic Harvesting. Intelligent Automation Soft Computing. 18, 581-590
    (2012). Google Scholar Hayashi et al., 2002 S. Hayashi, K. Ganno, Y. Ishii, I.
    Tanaka Robotic Harvesting System for Eggplants Japan Agricultural Research Quarterly:
    JARQ., 36 (2002,7.), pp. 163-168 Google Scholar He et al., 2011 B. He, G. Liu,
    Y. Ji, Y. Si, R. Gao Auto Recognition of Navigation Path for Harvest Robot Based
    on Machine Vision Computer And ComputIng Technologies In Agriculture IV (2011),
    pp. 138-148 CrossRefView in ScopusGoogle Scholar Hernández-Clemente et al., 2019
    R. Hernández-Clemente, A. Hornero, M. Mottus, J. Penuelas, V. González-Dugo, J.
    Jiménez, L. Suárez, L. Alonso, P. Zarco-Tejada Early Diagnosis of Vegetation Health
    From High-Resolution Hyperspectral and Thermal Imagery: Lessons Learned From Empirical
    Relationships and Radiative Transfer Modelling Current Forestry Reports., 5 (9)
    (2019), pp. 169-183, 10.1007/s40725-019-00096-1 View in ScopusGoogle Scholar Hong
    et al., 2018 S. Hong, A. Ansari, G. Saavedra, M. Martinez-Corral Full-parallax
    3D display from stereo-hybrid 3D camera system Optics And Lasers In Engineering.,
    103 (2018), pp. 46-54 https://www.sciencedirect.com/science/article/pii/S0143816617306267
    View PDFView articleView in ScopusGoogle Scholar Honrado et al., 2017 J. Honrado,
    D. Solpico, C. Favila, E. Tongson, G. Tangonan, N. Libatique UAV imaging with
    low-cost multispectral imaging system for precision agriculture applications 2017
    IEEE Global Humanitarian Technology Conference (GHTC) (2017), pp. 1-7 Google Scholar
    Howard et al., 2017 Howard, A., Zhu, M., Chen, B., Kalenichenko, D., Wang, W.,
    Weyand, T., Andreetto, M. & Adam, H. Mobilenets: Efficient convolutional neural
    networks for mobile vision applications. ArXiv Preprint ArXiv:1704.04861. (2017).
    Google Scholar Hu et al., 2014 M. Hu, Q. Dong, B. Liu, P. Malakar The potential
    of double K-means clustering for banana image segmentation J. Food Process Eng,
    37 (2014), pp. 10-18 View in ScopusGoogle Scholar Huang et al., 2018 T. Huang,
    R. Yang, W. Huang, Y. Huang, X. Qiao Detecting sugarcane borer diseases using
    support vector machine Information Processing In Agriculture., 5 (2018), pp. 74-82
    View PDFView articleView in ScopusGoogle Scholar Hulley et al., 2019 Hulley, G.,
    Ghent, D., Göttsche, F., Guillevic, P., Mildrexler, D. & Coll, C. 3 - Land Surface
    Temperature. Taking The Temperature Of The Earth. pp. 57-127 (2019), https://www.sciencedirect.com/science/article/pii/B9780128144589000034.
    Google Scholar Islam et al., 2017 Islam, M., Dinh, A., Wahid, K. & Bhowmik, P.
    Detection of potato diseases using image segmentation and multiclass support vector
    machine. 2017 IEEE 30th Canadian Conference On Electrical And Computer Engineering
    (CCECE). pp. 1-4 (2017). Google Scholar Islam et al., 2021 M. Islam, A. Rahman,
    M. Rana Potato Grading Based on Size Features by Machine Vision Technique Journal
    Of the Bangladesh Agricultural University., 19 (2021), pp. 528-532 https://jbau.bau.edu.bd/index.php/home/article/view/50
    CrossRefGoogle Scholar Jacques et al., 2018 Jacques, A., Adamchuk, V., Cloutier,
    G., Clark, J. & Miller, C. Development of a machine vision yield monitor for shallot
    onion harvesters. Proceedings Of The 14th International Conference on Precision
    Agriculture June 24–June 27, 2018 Montreal, Quebec, Canada. (2018). Google Scholar
    Jafari et al., 2006 A. Jafari, S. Mohtasebi, H. Jahromi, M. Omid Weed detection
    in sugar beet fields using machine vision Int. J. Agric. Biol., 8 (2006), pp.
    602-605 Google Scholar Janke et al., 2019 J. Janke, M. Castelli, A. Popovicˇ Analysis
    of the proficiency of fully connected neural networks in the process of classifying
    1022 digital images. Benchmark of different classification algorithms on high-level
    image features from convolutional layers Expert Syst. Appl. (2019) https://www.sciencedirect.com/science/article/pii/S0957417419303938
    Google Scholar Jasinski et al., 2010 Jasinski, J., Pietrek, S., Walczykowski,
    P. & Orych, A. Acquisition of spectral reflectance characteristics of land cover
    features based on hyperspectral images.. (2010,1). Google Scholar Jiang et al.,
    2022 Y. Jiang, G. Li, H. Ge, F. Wang, L. Li, X. Chen, Y. Zhang Machine learning
    and application in terahertz technology: A review on achievements and future challenges
    IEEE Access (2022) Google Scholar Jiang et al., 2020 F. Jiang, Y. Lu, Y. Chen,
    D. Cai, G. Li Image recognition of four rice leaf diseases based on deep learning
    and support vector ma- 1029 chine Computers And Electronics. In Agriculture.,
    179 (2020), Article 105824 https://www.sciencedirect.com/science/article/pii/S016816992030795X1030
    View PDFView articleView in ScopusGoogle Scholar Jiao et al., 2020 Y. Jiao, R.
    Luo, Q. Li, X. Deng, X. Yin, C. Ruan, W. Jia Detection and Localization of Overlapped
    Fruits Application in an Apple Harvesting Robot Electronics, 9 (2020) https://www.mdpi.com/2079-9292/9/6/1023
    Google Scholar Kailasam et al., 2022 Kailasam, S., Achanta, S. D. M., Rama Koteswara
    Rao, P., Vatambeti, R., & Kayam, S. (2022). An IoT-based agriculture maintenance
    using pervasive computing with machine learning technique. International Journal
    of Intelligent Computing and Cybernetics, 15(2), 184-197. Google Scholar Kamilaris
    and Prenafeta-Boldú, 2018 A. Kamilaris, F. Prenafeta-Boldú Deep learning in agriculture:
    A survey Computers And Electronics In Agriculture., 147 (2018), pp. 70-90 https://www.sciencedirect.com/science/article/pii/S0168169917308803
    View PDFView articleView in ScopusGoogle Scholar Kanagasingham et al., 2020 S.
    Kanagasingham, M. Ekpanyapong, R. Chaihan Integrating machine vision-based row
    guidance with GPS and compass-based routing to achieve autonomous navigation for
    a rice field weeding robot Precis. Agric., 21 (8) (2020), pp. 831-855, 10.1007/s11119-019-09697-z
    View in ScopusGoogle Scholar Kasinathan et al., 2021 T. Kasinathan, D. Singaraju,
    S. Uyyala Insect classification and detection in field crops using modern machine
    learning techniques Information Processing In Agriculture., 8 (2021), pp. 446-457
    https://www.sciencedirect.com/science/article/pii/S2214317320302067 View PDFView
    articleView in ScopusGoogle Scholar Khan and Debnath, 2019 R. Khan, R. Debnath
    Multi class fruit classification using efficient object detection and recognition
    techniques. International Journal Of Image, Graphics And Signal Process., 11 (2019),
    p. 1 Google Scholar Khanal, 2017 S. Khanal Remote Sensing in precision agriculture
    Ohioline. (3) (2017) https://ohioline.osu.edu/factsheet/fabe-5541 Google Scholar
    Kiani et al., (2012,5). S. Kiani, S. Kamgar, M. Raoufat Machine Vision and Soil
    Trace-based Guidance-Assistance System for Farm Tractors in Soil Preparation Operations.
    Journal Of Agricultural Science, 4 (2012,5.) Google Scholar Kim and Lee, 2015
    Kim, H. & Lee, J. A study on the possibility of implementing a real-time stereoscopic
    3D rendering TV system. Displays. 40 pp. 24-34 (2015), https://www.sciencedirect.com/science/article/pii/S0141938215000608,
    Next Generation TV Systems and Technologies. Google Scholar Kim et al., 2020 Kim,
    J., Seol, J., Lee, S., Hong, S. & Son, H. An Intelligent Spraying System with
    Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards. 2020 IEEE
    International Conference On Robotics And Automation (ICRA). pp. 3923-3929 (2020).
    Google Scholar Ko et al., 2021 K. Ko, I. Jang, J. Choi, J. Lim, D. Lee Stochastic
    Decision Fusion of Convolutional Neural Networks for Tomato Ripeness Detection
    in Agricultural Sorting Systems Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/3/917
    Google Scholar Kumar et al., 2013 J. Kumar, A. Vashisth, V. Sehgal, V. Gupta Assessment
    of aphid infestation in mustard by hyperspectral remote sensing J. Indian Soc.
    Remote Sens., 41 (2013), pp. 83-90 CrossRefView in ScopusGoogle Scholar Kurtulmus
    and Ünal, 2015 Kurtulmus¸, F., Ünal, H. & Others Discriminating rapeseed varieties
    using computer vision and machine learning. (Pergamon-Elsevier,2015). Google Scholar
    Kuznetsova et al., 2020 A. Kuznetsova, T. Maleva, V. Soloviev Using YOLOv3 Algorithm
    with Pre- and Post-Processing for Apple Detection in Fruit-Harvesting Robot Agronomy,
    10 (2020) https://www.mdpi.com/2073-4395/10/7/1016 Google Scholar Kuznetsova et
    al., 2020 A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
    S. Kamali, S. Popov, M. Malloci, A. Kolesnikov & Others The open images dataset
    v4 Int. J. Comput. Vis., 128 (2020), pp. 1956-1981 CrossRefView in ScopusGoogle
    Scholar Ladaniya, 2008 M. Ladaniya 8 - HARVESTING Citrus Fruit. (2008) https://www.sciencedirect.com/science/article/pii/B978012374130150010
    Google Scholar Larrea-Gallegos and Vázquez-Rowe, 2022 G. Larrea-Gallegos, I. Vázquez-Rowe
    Exploring machine learning techniques to predict deforestation to enhance the
    decision-making of road construction projects J. Ind. Ecol., 26 (1) (2022), pp.
    225-239 CrossRefView in ScopusGoogle Scholar Laursen et al., 2014 M. Laursen,
    H. Midtiby, N. Krüger, R. Jørgensen Statistics-based segmentation using a continuous-scale
    naive Bayes approach Computers And Electronics In Agriculture., 109 (2014), pp.
    271-277 https://www.sciencedirect.com/science/article/pii/S01681699140025671047
    View PDFView articleView in ScopusGoogle Scholar Lee-Post, 2003 Lee-Post, A. Computer-Aided
    Manufacturing. Encyclopedia Of Information Systems. pp. 187-203 (2003), https://www.sciencedirect.com/science.
    Google Scholar Leiva-Valenzuela and Aguilera, 2013 G. Leiva-Valenzuela, J. Aguilera
    Automatic detection of orientation and diseases in blueberries using image analysis
    to improve their postharvest storage quality Food Control, 33 (2013), Article
    166173 https://www.sciencedirect.com/science/article/pii/S0956713513001011 Google
    Scholar Li et al., 2014 H. Li, W. Lee, K. Wang Identifying blueberry fruit of
    different growth stages using natural outdoor color images Computers And Electronics
    In Agriculture., 106 (2014), pp. 91-101 View PDFView articleView in ScopusGoogle
    Scholar Lin et al., 2008 Lin, T., Hsiung, Y., Hong, G., Chang, H. & Lu, F. Development
    of a virtual reality GIS using stereo vision. Computers And Electronics In Agriculture.
    63, 38-48 (2008), https://www.sciencedirect.com/science/article/pii/S0168169908000446,
    Special issue on bio-robotics. Google Scholar Lin et al., 2014 Lin, T., Maire,
    M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. & Zitnick, C.
    Microsoft coco: Common objects in context. European Conference on Computer Vision.
    pp. 740-755 (2014). Google Scholar Liu et al., 2022 Z. Liu, R.N. Bashir, S. Iqbal,
    M.M.A. Shahid, M. Tausif, Q. Umer Internet of Things (IoT) and machine learning
    model of plant disease prediction-blister blight for tea plant IEEE Access, 10
    (2022), pp. 44934-44944 CrossRefView in ScopusGoogle Scholar Liu and Chahl, 2018
    H. Liu, J. Chahl A multispectral machine vision system for invertebrate detection
    on green leaves Computers And Electronics in Agriculture., 150 (2018), pp. 279-288
    https://www.sciencedirect.com/science/article/pii/S016816991830142X View PDFView
    articleGoogle Scholar Liu et al., 2016 Liu, W., Anguelov, D., Erhan, D., Szegedy,
    C., Reed, S., Fu, C. & Berg, A. Ssd: Single shot multibox detector. European Conference
    on Computer Vision. pp. 21-37 (2016). Google Scholar Liu et al., 2017 W. Liu,
    Z. Wang, X. Liu, N. Zeng, Y. Liu, F. Alsaadi A survey of deep neural network architectures
    and their applications Neurocomputing, 234 (2017), pp. 11-26 https://www.sciencedirect.com/science/article/pii/S0925231216315533
    View PDFView articleCrossRefView in ScopusGoogle Scholar Longye et al., 2019 Longye,
    X., Zhuo, W., Haishen, L., Xilong, K. & Changhui, Y. Overlapping citrus segmentation
    and reconstruction based on Mask R-CNN model and concave region simplification
    and distance analysis. Journal Of Physics: Conference Series. 1345, 032064 (2019,11),
    10.1088/1742-6596/1345/3/032064. Google Scholar Lu et al., 2020 B. Lu, P. Dao,
    J. Liu, Y. He, J. Shang Recent advances of hyperspectral imaging technology and
    applications in agriculture Remote Sens. (Basel), 12 (2020), p. 2659 CrossRefView
    in ScopusGoogle Scholar Lu et al., 2021 Z. Lu, M. Zhao, J. Luo, G. Wang, D. Wang
    Design of a winter-jujube grading robot based on machine vision Computers And
    Electronics in Agriculture., 186 (2021), Article 106170 https://www.sciencedirect.com/science/article/pii/S0168169921001873
    View PDFView articleView in ScopusGoogle Scholar Mahajan et al., 2015 S. Mahajan,
    A. Das, H. Sardana Image acquisition techniques for assessment of legume quality
    Trends In Food Science Technology., 42 (2015), pp. 116-133 https://www.sciencedirect.com/science/article/pii/S0924224415000023
    View PDFView articleView in ScopusGoogle Scholar Mahmood et al., 2017 Mahmood,
    A., Bennamoun, M., An, S., Sohel, F., Boussaid, F., Hovey, R., Kendrick, G. &
    Fisher, R. Deep Learning for Coral Classification. Handbook Of Neural Computation.
    pp. 383-401 (2017). Google Scholar Manjunath et al., 2011 K. Manjunath, S. Ray,
    S. Panigrahy Discrimination of spectrally-close crops using ground-based hyperspectral
    data J. Indian Soc. Remote Sens., 39 (2011), pp. 599-602 CrossRefView in ScopusGoogle
    Scholar Mathanker et al., 2013 S. Mathanker, P. Weckler, T. Bowser X-ray applications
    in food and agriculture: a review Transactions Of The ASABE (American Society
    Of Agricultural And Biological Engineers), 56 (5) (2013), pp. 1227-1239 View in
    ScopusGoogle Scholar Mekhalfi et al., 2020 M. Mekhalfi, C. Nicolò, I. Ianniello,
    F. Calamita, R. Goller, M. Barazzuol, F. Melgani Vision System for Automatic On-Tree
    Kiwifruit Counting and Yield Estimation Sensors, 20 (2020) https://www.mdpi.com/1424-8220/20/15/4214
    Google Scholar Meng et al., 2015 Meng, Q., Qiu, R., He, J., Zhang, M., Ma, X.
    & Liu, G. Development of agricultural implement system based on machine vision
    and fuzzy control. Computers And Electronics in Agriculture. 112 p. 128-138 (2015),
    https://www.sciencedirect.com/science/article/pii/S016816991Precision Agriculture.
    Google Scholar Meng and Wang, 2015 J. Meng, S. Wang The Recognition of Overlapping
    Apple Fruits Based on Boundary Curvature Estimation 2015 Sixth InternatiOnal COnference
    On Intelligent Systems Design And Engineering ApplicatiOns (ISDEA) (2015), pp.
    874-877 CrossRefView in ScopusGoogle Scholar Miao et al., 2021 Y. Miao, L. Huang,
    S. Zhang A Two-Step Phenotypic Parameter Measurement Strategy for Overlapped Grapes
    under Different Light Conditions Sensors, 21 (2021), p. 4532 CrossRefView in ScopusGoogle
    Scholar Miller et al., 1998 W. Miller, J. Throop, B. Upchurch Pattern recognition
    models for spectral reflectance evaluation of apple blemishes Postharvest Biology
    And Technology., 14 (1998), pp. 11-20 View PDFView articleView in ScopusGoogle
    Scholar Mishra et al., 2017 V. Mishra, S. Kumar, N. Shukla Image acquisition and
    techniques to perform image acquisition SAMRIDDHI : A Journal Of Physical Sciences,
    Engineering And Technology., 9 (2017,7.) Google Scholar Moallem et al., 2017 P.
    Moallem, A. Serajoddin, H. Pourghassem Computer vision-based apple grading for
    golden delicious apples based on surface features Information Processing in Agriculture.,
    4 (2017), pp. 33-40 https://www.sciencedirect.com/science/article/pii/S2214317315300068
    View PDFView articleView in ScopusGoogle Scholar Mohd Ali et al., 2022 M. Mohd
    Ali, N. Hashim, S. Abd Aziz, O. Lasekan Characterisation of Pineapple Cultivars
    under Different Storage Conditions Using Infrared Thermal Imaging Coupled with
    Machine Learning Algorithms Agriculture, 12 (2022) https://www.mdpi.com/2077-0472/12/7/1013
    Google Scholar Mohi-Alden et al., 2022 K. Mohi-Alden, M. Omid, M. Soltani Firouz,
    A. Nasiri Design and evaluation of an intelligent sorting system for bell pepper
    using deep convolutional neural networks Journal Of Food Science., 87 (2022),
    pp. 289-301 https://ift.onlinelibrary.wiley.com/doi/abs/10.1111/1750-3841.15995
    CrossRefView in ScopusGoogle Scholar Momin et al., 2017 M. Momin, M. Rahman, M.
    Sultana, C. Igathinathane, A. Ziauddin, T. Grift Geometry-based mass grading of
    mango fruits us- 1016 ing image processing Information Processing In Agriculture.,
    4 (2017), pp. 150-160 https://www.sciencedirect.com/science/article/pii/S221431731631017
    View PDFView articleView in ScopusGoogle Scholar Mondal and Kole, 2016 D. Mondal,
    D. Kole A time efficient leaf rust disease detection technique of wheat leaf images
    using pearson correlation coefficient and rough fuzzy C-means Information Systems
    Design And Intelligent Applications. (2016), pp. 609-618 CrossRefView in ScopusGoogle
    Scholar Munawar et al., 2021 A. Munawar, Y. Yunus, D. Devianti, P. Satriyo Agriculture
    environment monitoring: rapid soil fertility evaluation by means of near infrared
    spectroscopy. IOP Conference Series: Earth And Environmental Science, 644 (2021),
    Article 012036, 10.1088/1755-1315/644/1/012036 View in ScopusGoogle Scholar Mupangwa
    et al., 2020 W. Mupangwa, L. Chipindu, I. Nyagumbo, S. Mkuhlani, G. Sisito Evaluating
    machine learning algorithms for pre- 1033 dicting maize yield under conservation
    agriculture in Eastern and Southern Africa. SN Appl. Sci., 2 (2020), p. 952, 10.1007/s42452-020-2711-6
    View in ScopusGoogle Scholar Mustafa et al., 2008 Mustafa, N., Fuad, N., Ahmed,
    S., Abidin, A., Ali, Z., Yit, W. & Sharrif, Z. Image processing of an agriculture
    produce: Determination of size and ripeness of a banana. 2008 International Symposium
    On Information Technology. 1 pp. 1-7 (2008). Google Scholar Nandi et al., 2013
    Nandi, C., Tudu, B. & Koley, C. Machine Vision Based Techniques for Automatic
    Mango Fruit Sorting and Grading Based on Maturity Level and Size. Sensing Technology:
    Current Status and Future Trends II. 8 pp. 27-46 (2013,1). Google Scholar Nasirahmadi
    et al., 2021 A. Nasirahmadi, U. Wilczek, O. Hensel Sugar beet damage detection
    during harvesting using different convolutional neural network models Agriculture,
    11 (2021) https://www.mdpi.com/2077-0472/11/11/1111 Google Scholar Nguyen et al.,
    2020 T. Nguyen, T. Hoang, M. Pham, T. Vu, T. Nguyen, Q. Huynh, J. Jo Monitoring
    agriculture areas with satellite images and deep 1027 learning Appl. Soft Comput.,
    95 (2020), Article 106565 https://www.sciencedirect.com/science/article/pii/S1568494620305032
    View PDFView articleView in ScopusGoogle Scholar Ni et al., 2018 Ni, X., Wang,
    X., Wang, S., Wang, S., Yao, Z. & Ma, Y. Structure Design and Image Recognition
    Research of A Picking Device on the Apple Picking Robot. IFAC-PapersOnLine.51,489494(2018),https://www.sciencedirect.com/science/article/pii/S2405896318312801
    IFAC Conference on Bio-Robotics BIOROBOTICS 2018. Google Scholar Nicolis and Gonzalez,
    2021 Nicolis, O. & Gonzalez, C. 19 - Wavelet-based fractal and multifractal analysis
    for detecting mineral deposits using multispectral images taken by drones. Methods
    And Applications In Petroleum And Mineral Exploration And Engineering Geology.
    pp. 295-307 (2021), https://www.sciencedirect.com/science/article/pii/B9780323856171000175.
    Google Scholar Nixon and Aguado, 2020 Nixon, M. & Aguado, A. 1 - Introduction.
    Feature Extraction And Image Processing For Computer Vision (Fourth Edition).
    pp. 1-33 (2020),https://www.sciencedirect.com/science/article/pii/B9780128149768000014.
    Google Scholar Opiyo et al., 2021 S. Opiyo, C. Okinda, J. Zhou, E. Mwangi, N.
    Makange Medial axis-based machine-vision system for orchard robot navigation Computers
    And Electronics in Agriculture., 185 (2021), Article 106153 https://www.sciencedirect.com/science/article/pii/S016816992100171X1\\
    View PDFView articleView in ScopusGoogle Scholar Otsu, 1979 N. Otsu A threshold
    selection method from gray-level histograms IEEE Transactions on Systems, Man,
    And Cybernetics., 9 (1979), pp. 62-66 CrossRefGoogle Scholar Pallottino et al.,
    2018 Pallottino, F., Menesatti, P., Figorilli, S., Antonucci, F., Tomasone, R.,
    Colantoni, A. & Costa, C. Machine Vision Retrofit System for Mechanical Weed Control
    in Precision Agriculture Applications. Sustainability. 10 (2018), https://www.mdpi.com/2071-
    1050/10/7/2209. Google Scholar Pan et al., 2016 Pan, L., Zhang, Q., Zhang, W.,
    Sun, Y., Hu, P. & Tu, K. Detection of cold injury in peaches by hyperspectral
    reflectance imaging and 1018 artificial neural network. Food Chemistry. 192 pp.
    134-141 (2016,2), 10.1016/j.foodchem.2015.06.106. Google Scholar Pan et al., 2017
    L. Pan, Y. Sun, H. Xiao, X. Gu, P. Hu, Y. Wei, K. Tu Hyperspectral imaging with
    different illumination patterns for the 1055 hollowness classification of white
    radish Postharvest Biology And Technology., 126 (2017), pp. 40-49 View PDFView
    articleView in ScopusGoogle Scholar Phadikar et al., 2013 S. Phadikar, J. Sil,
    A. Das Rice diseases classification using feature selection and rule generation
    techniques Computers And Electronics In Agriculture., 90 (2013), pp. 76-85 View
    PDFView articleView in ScopusGoogle Scholar Polder et al., 2012 Polder, G., Heijden,
    G., Doorn, J. & Baltissen, A. Automatic detection of tulip breaking virus (TBV)
    in tulip fields using machine vision. (2012), https://edepot.wur.nl/244613. Google
    Scholar Puerto et al., 2015 D. Puerto, D. Martınez Gila, J. Gámez Garcıa, J. Gómez
    Ortega Sorting olive batches for the milling process using image 1051 processing
    Sensors, 15 (2015), pp. 15738-15754 CrossRefView in ScopusGoogle Scholar Quiroz
    and Alférez, 2020 Quiroz, I. & Alférez, G. Image recognition of Legacy blueberries
    in a Chilean smart farm through deep learning. Computers And Electronics In Agriculture.
    168 pp. 105044 (2020), https://www.sciencedirect.com/science/article/pii/S0168169919312670
    1032. Google Scholar Rahimi-Ajdadi et al., 2018 Rahimi-Ajdadi, F., Abbaspour-Gilandeh,
    Y., Mollazade, K. & Hasanzadeh, R. Development of a novel machine vision procedure
    for rapid and non-contact measurement of soil moisture content. Measurement. 121
    pp. 179-189 (2018), https://www.sciencedirect.com/science/article/pii/S0263224118301593
    1294. Google Scholar Ralph, 2022 R. Ralph Robots using machine vision algorithms
    in agriculture RSIP Vision., 1 (2022) https://rsipvision.com/robots-using-machine-vision-agriculture/
    Google Scholar Ramesh and Vydeki, 2018 Ramesh, S. & Vydeki, D. Rice Blast Disease
    Detection and Classification Using Machine Learning Algorithm. 2018 2nd International
    Conference on Micro-Electronics and Telecommunication Engineering (ICMETE). pp.
    255-259 (2018). Google Scholar Ravikanth et al., 2015 L. Ravikanth, C. Singh,
    D. Jayas, N. White Classification of contaminants from wheat using near-infrared
    hyperspectral 1044 imaging Biosyst. Eng., 135 (2015), pp. 73-86 https://www.sciencedirect.com/science/article/pii/S1537511015000707
    View PDFView articleView in ScopusGoogle Scholar Redmon et al., 2016 Redmon, J.,
    Divvala, S., Girshick, R. & Farhadi, A. You only look once: Unified, real-time
    object detection. Proceedings Of the IEEE Conference On Computer Vision And Pattern
    Recognition. pp. 779-788 (2016). Google Scholar Reeves, 2014 Reeves, S. Chapter
    6 - Image Restoration: Fundamentals of Image Restoration. Academic Press Library
    In Signal Processing: Volume 4. 10044 pp. 165-192 (2014), https://www.sciencedirect.com/science/article/pii/B9780123965011000066.
    Google Scholar Rehman et al., 1036 Rehman, T., Mahmud, M., Chang, Y., Jin, J.
    & Shin, J. Current and future applications of statistical machine learning 1036
    algorithms for agricultural machine vision systems. Computers And Electronics
    In Agriculture. 156 pp. 585-605 (2019), 1037 https://www.sciencedirect.com/science/article/pii/S0168169918304289.
    Google Scholar Rehman et al., 2019 T. Rehman, Q. Zaman, Y. Chang, A. Schumann,
    K. Corscadden Development and field evaluation of a machine vision based in-season
    weed detection system for wild blueberry Computers And Electronics in Agriculture.,
    162 (2019), pp. 1-13 View PDFView articleView in ScopusGoogle Scholar Riegler-Nurscher
    et al., 2020 P. Riegler-Nurscher, G. Moitzi, J. Prankl, J. Huber, J. Karner, H.
    Wagentristl, M. Vincze Machine vision for soil roughness measurement and control
    of tillage machines during seedbed preparation Soil And Tillage Research., 196
    (2020), Article 104351 https://www.sciencedirect.com/science/article/pii/S0167198719300613
    View PDFView articleView in ScopusGoogle Scholar Rokunuzzaman and Jayasuriya,
    2013 M. Rokunuzzaman, H. Jayasuriya Development of a low-cost machine vision system
    for sorting of tomatoes Agric. Eng. Int. CIGR J., 15 (1) (2013), pp. 173-180 View
    in ScopusGoogle Scholar Romeo et al., 2012 J. Romeo, G. Pajares, M. Montalvo,
    J. Guerrero, M. Guijarro, A. Ribeiro Crop row detection in maize fields inspired
    on the 1077 human visual perception Scientific World Journal, 2012 (2012) Google
    Scholar Roser, 2013 Roser, M. Employment in Agriculture. Our World In Data. (2013),
    https://ourworldindata.org/employment-in-agriculture. Google Scholar Rovira-Más
    et al., 2008 Rovira-Más, F., Zhang, Q. & Reid, J. Stereo vision three-dimensional
    terrain maps for precision agriculture. Computers And Electronics In Agriculture.
    60, 133-143 (2008), https://www.sciencedirect.com/science/article/pii/S016816990700172X.
    Google Scholar Roy and Bhaduri, 2021 A. Roy, J. Bhaduri A Deep Learning Enabled
    Multi-Class Plant Disease Detection Model Based on Computer Vision AI., 2 (2021),
    pp. 413-428 https://www.mdpi.com/2673-2688/2/3/26 CrossRefView in ScopusGoogle
    Scholar Roy and Isler, 2016 P. Roy, V. Isler Surveying apple orchards with a monocular
    vision system 2016 IEEE InternatiOnal COnference On AutomatiOn Science And Engineering
    (CASE) (2016), pp. 916-921 CrossRefView in ScopusGoogle Scholar Sannakki et al.,
    2013 Sannakki, S., Rajpurohit, V. & Nargund, V. SVM-DSD: SVM Based diagnostic
    system for the detection of pomegranate leaf diseases. Proceedings Of International
    Conference On Advances In Computing. pp. 715-720 (2013). Google Scholar Schor
    et al., 2016 N. Schor, A. Bechar, T. Ignat, A. Dombrovsky, Y. Elad, S. Berman
    Robotic disease detection in greenhouses: combined detection of powdery mildew
    and tomato spotted wilt virus IEEE Rob. Autom. Lett., 1 (2016), pp. 354-360 View
    in ScopusGoogle Scholar Seelan et al., 2003 Seelan, S., Laguette, S., Casady,
    G. & Seielstad, G. Remote sensing applications for precision agriculture: A learning
    community approach. Remote Sensing Of Environment. 88,157-169(2003), https://www.sciencedirect.com/science/article/pii/S0034425703002360,
    IKONOS Fine Spatial Resolution Land Observation. Google Scholar Senni et al.,
    2014 L. Senni, M. Ricci, A. Palazzi, P. Burrascano, P. Pennisi, F. Ghirelli On-line
    automatic detection of foreign bod- 997 ies in biscuits by infrared thermography
    and image processing J. Food Eng., 128 (2014), pp. 146-156 https://www.sciencedirect.com/science/article/pii/S0260877413006262
    View PDFView articleView in ScopusGoogle Scholar Sharma et al., 2021 R. Sharma,
    A. Agarwal, H. Mamatha Classification of Carrots based on Shape Analysis using
    Machine Learning Techniques 2021 Third International Conference on Intelligent
    Communication Technologies and Virtual Mobile Networks (ICICV) (2021), pp. 1407-1411
    CrossRefView in ScopusGoogle Scholar Shearer and Payne, 1990 S. Shearer, F. Payne
    Color and defect sorting of bell peppers using machine vision Transactions Of
    The ASAE., 33 (1990), pp. 1245-1250 Google Scholar Shustova, 2022 Shustova, A.
    Remote Sensing in agriculture – what are some applications?. Dragonfly Aerospace.
    (2022,5), https://dragonflyaerospace.com/rem938 sensing-in-agriculture-what-are-some-applications/.
    Google Scholar Siedliska et al., 2014 A. Siedliska, P. Baranowski, W. Mazurek
    Classification models of bruise and cultivar detection on the basis of hyperspectral
    imag- 1039 ing data Computers And Electronics In Agriculture., 106 (2014), pp.
    66-74 https://www.sciencedirect.com/science/article/pii/S01681699140014581040
    View PDFView articleView in ScopusGoogle Scholar Singh and Misra, 2017 V. Singh,
    A. Misra Detection of plant leaf diseases using image segmentation and soft computing
    techniques Information Processing In Agriculture., 4 (2017), pp. 41-49 https://www.sciencedirect.com/science/article/pii/S2214317316300154
    View PDFView articleView in ScopusGoogle Scholar Sivakumar et al., 2021 Sivakumar,
    A., Modi, S., Gasparino, M., Ellis, C., Velasquez, A., Chowdhary, G. & Gupta,
    S. Learned Visual Navigation for Under-Canopy Agricultural Robots. (arXiv,2021),
    https://arxiv.org/abs/2107.02792. Google Scholar Slaughter and Harrell, 1987 D.
    Slaughter, R. Harrell Color vision in robotic fruit harvesting Transactions Of
    The ASAE., 30 (1987), pp. 1144-1148 Google Scholar Sleep et al., 2021 B. Sleep,
    S. Mason, L. Janik, L. Mosley Application of visible near-infrared absorbance
    spectroscopy for the determination of soil ph and liming requirements for broad-acre
    agriculture - precision agriculture SpringerLink., 8 (2021) https://link.springer.com/article/10.1007/s11119-021-09834-7
    Google Scholar Smith et al., 2018 L. Smith, W. Zhang, M. Hansen, I. Hales, M.
    Smith Innovative 3D and 2D machine vision methods for analysis of plants and crops
    in the field Comput. Ind., 97 (2018), pp. 122-131 https://www.sciencedirect.com/science/article/pii/S01663615173056631108
    View PDFView articleView in ScopusGoogle Scholar Sochen et al., 1998 N. Sochen,
    R. Kimmel, R. Malladi A general framework for low level vision IEEE Trans. Image
    Process., 7 (1998), pp. 310-318 View in ScopusGoogle Scholar Song et al., 2015
    Y. Song, H. Sun, M. Li, Q. Zhang Technology application of smart spray in agriculture:
    A Review Intelligent Automation Soft Computing., 21 (2015), pp. 319-333 CrossRefView
    in ScopusGoogle Scholar Spaeth et al., 2020 M. Spaeth, J. Machleb, G. Peteinatos,
    M. Saile, R. Gerhards Smart harrowing—adjusting the treatment intensity based
    on machine vision to achieve a uniform weed control selectivity under heterogeneous
    field conditions Agronomy, 10 (2020) https://www.mdpi.com/2073-4395/10/12/1925
    Google Scholar Stajnko et al., 2004 D. Stajnko, M. Lakota, M. Hocˇevar Estimation
    of number and diameter of apple fruits in an orchard during the growing season
    by thermal imaging Computers And Electronics In Agriculture., 42 (2004), pp. 31-42
    View PDFView articleView in ScopusGoogle Scholar Storey et al., 2022 G. Storey,
    Q. Meng, B. Li Leaf Disease Segmentation and Detection in Apple Orchards for Precise
    Smart Spraying in Sustainable Agriculture Sustainability., 14 (2022) https://www.mdpi.com/2071-1050/14/3/1458
    Google Scholar Subhi and Ali, 2018 Subhi, M., Md. Ali, S., Ismail, A. & Othman,
    M. Food volume estimation based on stereo image analysis. IEEE Instrumentation
    Measurement Magazine. 21, 36-43 (2018). Google Scholar Suganya et al., 2019 Suganya,
    E., Sountharrajan, S., Shandilya, S. & Ms, K. IoT in Agriculture Investigation
    on Plant Diseases and Nutrient Level Using Image Analysis Techniques. (2019,1).
    Google Scholar Sun et al., 2020 J. Sun, X. He, M. Wu, X. Wu, J. Shen, B. Lu Detection
    of tomato organs based on convolutional neural network under the overlap and occlusion
    backgrounds Mach. Vis. Appl., 31 (5) (2020), p. 31, 10.1007/s00138-020-01081-6
    Google Scholar Taneja et al., 2021 P. Taneja, H. Vasava, P. Daggupati, A. Biswas
    Multi-algorithm comparison to predict soil organic matter and soil moisture content
    from cell phone images Geoderma, 385 (2021), Article 114863 https://www.sciencedirect.com/science/article/pii/S0016706120326185
    View PDFView articleView in ScopusGoogle Scholar Tang et al., 2017 J. Tang, D.
    Wang, Z. Zhang, L. He, J. Xin, Y. Xu Weed identification based on K-means feature
    learning combined with convolutional neural network Computers And Electronics
    in Agriculture., 135 (2017), pp. 63-70 View PDFView articleView in ScopusGoogle
    Scholar Terra et al., 2021 Terra, F., Nascimento, G., Duarte, G. & Drews-Jr, P.
    Autonomous Agricultural Sprayer using Machine Vision and Nozzle Control. Journal
    Of Intelligent Robotic Systems. 102, 38 (2021,5), 10.1007/s10846-021-01361-x.
    Google Scholar UNICEF, 2021 UNICEF \\& Others The state of food security and nutrition
    in the world 2021. Google Scholar Unitednations, 2021 Unitednations & food and
    agriculture organization, united nations Food (2021) https://www.fao.org/fileadmin/templates/wsfs/docs/Issuespapers/
    Google Scholar Vrochidou et al., 2022 E. Vrochidou, D. Oustadakis, A. Kefalas,
    G. Papakostas Computer Vision in Self-Steering Tractors. Machines., 10 (2022)
    https://www.mdpi.com/2075-1702/10/2/129 Google Scholar Wang et al., 2012 Wang,
    H., Li, G., Ma, Z. & Li, X. Image recognition of plant diseases based on backpropagation
    networks. 2012 5th International Congress on Image and Signal Processing. pp.
    894-900 (2012). Google Scholar Wang et al., 2020 A. Wang, Y. Xu, X. Wei, B. Cui
    Semantic Segmentation of Crop and Weed using an Encoder-Decoder Network and Image
    Enhancement Method under Uncontrolled Outdoor Illumination IEEE Access, 8 (2020),
    pp. 81724-81734 CrossRefView in ScopusGoogle Scholar Wang et al., 2016 C. Wang,
    X. Zou, Y. Tang, L. Luo, W. Feng Localisation of litchi in an unstructured environment
    using binocular stereo vision Biosyst. Eng., 145 (2016), pp. 39-51 View PDFView
    articleView in ScopusGoogle Scholar Wani et al., 2022 J.A. Wani, S. Sharma, M.
    Muzamil, S. Ahmed, S. Sharma, S. Singh Machine learning and deep learning based
    computational techniques in automatic agricultural diseases detection: Methodologies,
    applications, and challenges Arch. Comput. Meth. Eng., 29 (1) (2022), pp. 641-677
    CrossRefView in ScopusGoogle Scholar Williams et al., 2019 H. Williams, M. Jones,
    M. Nejati, M. Seabright, J. Bell, N. Penhall, J. Barnett, M. Duke, A. Scarfe,
    H. Ahn, J. Lim, B. MacDonald Robotic kiwifruit harvesting using machine vision,
    convolutional neural networks, and robotic arms Biosyst. Eng., 181 (2019), pp.
    140-156 https://www.sciencedirect.com/science/article/pii/S153751101830638X View
    PDFView articleView in ScopusGoogle Scholar Wu et al., 2021 Z. Wu, Y. Chen, B.
    Zhao, X. Kang, Y. Ding Review of Weed Detection Methods Based on Computer Vision
    Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/11/3647 Google Scholar Wu
    et al., 2019 Wu, X., Aravecchia, S. & Pradalier, C. Design and implementation
    of computer vision based in-row weeding system. 2019 International Conference
    on Robotics And Automation (ICRA). pp. 4218-4224 (2019). Google Scholar Xiang
    et al., 2014 R. Xiang, H. Jiang, Y. Ying Recognition of clustered tomatoes based
    on binocular stereo vision Computers And Electronics In Agriculture., 106 (2014),
    pp. 75-90 View PDFView articleView in ScopusGoogle Scholar Xu et al., 2021 Xu,
    B., Chai, L. & Zhang, C. Research and application on corn crop identification
    and positioning method based on Machine 1225 vision. Information Processing in
    Agriculture(2021), https://www.sciencedirect.com/science/article/pii/S2214317321000603
    1226. Google Scholar Yu et al., 2015 Yu, X., Endo, M., Ishibashi, T., Shimizu,
    M., Kusanagi, S., Nozokido, T. & Bae, J. Orthogonally polarized terahertz wave
    imaging with real-time capability for food inspection. 2015 Asia-Pacific Microwave
    Conference (APMC). 2 pp. 1-3 (2015). Google Scholar Zeng et al., 2009 Q. Zeng,
    Y. Miao, C. Liu, S. Wang Algorithm based on marker-controlled watershed transform
    for overlapping plant fruit segmentation Opt. Eng., 48 (2009) Google Scholar Zhang
    et al., 2020 Z. Zhang, R. Cao, C. Peng, R. Liu, Y. Sun, M. Zhang, H. Li Cut-Edge
    Detection Method for Rice Harvesting Based on Machine Vision Agronomy, 10 (2020)
    https://www.mdpi.com/2073-4395/10/4/590 Google Scholar Zhang et al., 2013 Zhang,
    C., Zhang, J., Huang, X., Li, N., Chen, Z. & Li, W. System Integration Design
    of Intra-Row Weeding Robot. American Society Of Agricultural And Biological Engineers
    Annual International Meeting 2013, ASABE 2013. 1 (2013,1). Google Scholar Zhang
    et al., 2021 Zhang, L., Li, R., Li, Z., Meng, Y., Liang, J., Fu, L., Jin, X. &
    Li, S. A Quadratic Traversal Algorithm of Shortest Weeding Path Planning for Agricultural
    Mobile Robots in Cornfield. Journal Of Robotics. 2021 pp. 6633139 (2021,2), 10.1155/2021/6633139.
    Google Scholar Zhang et al., 2021 S. Zhang, W. Huang, Z. Wang Combing modified
    Grabcut, K-means clustering and sparse representation classification for weed
    recognition in wheat field Neurocomputing, 452 (2021), pp. 665-674 https://www.sciencedirect.com/science/article/pii/S092523122031910X1100
    View PDFView articleCrossRefGoogle Scholar Zhang et al., 2018 L. Zhang, J. Jia,
    G. Gui, X. Hao, W. Gao, M. Wang Deep learning based improved classification system
    for designing tomato harvesting robot IEEE Access, 6 (2018), pp. 67940-67950 CrossRefView
    in ScopusGoogle Scholar Zhang et al., 2022 H. Zhang, Z. Wang, Y. Guo, Y. Ma, W.
    Cao, D. Chen, S. Yang, R. Gao Weed Detection in Peanut Fields Based on Machine
    Vision Agriculture, 12 (2022) https://www.mdpi.com/2077-0472/12/10/1541 Google
    Scholar Zhang et al., 2019 L. Zhang, H. Zhang, Y. Chen, S. Dai, X. Li, I. Kenji,
    Z. Liu, M. Li Real-time monitoring of optimum timing for harvesting fresh tea
    leaves based on machine vision Int. J. Agric. Biol. Eng., 12 (2019), pp. 6-9 CrossRefView
    in ScopusGoogle Scholar Zhao et al., 2016 Y. Zhao, L. Gong, Y. Huang, C. Liu A
    review of key techniques of vision-based control for harvesting robot Computers
    AndElectronics In Agriculture., 127 (2016), pp. 311-323 View PDFView articleView
    in ScopusGoogle Scholar Zhao et al., 2005 J. Zhao, J. Tow, J. Katupitiya On-tree
    fruit recognition using texture properties and color data 2005 IEEE/RSJ InternatiOnal
    COnference On Intelligent Robots And Systems (2005), pp. 263-268 CrossRefView
    in ScopusGoogle Scholar Zhou et al., 2012 R. Zhou, L. Damerow, Y. Sun, M. Blanke
    Using colour features of cv. ‘Gala’ apple fruits in an orchard in image processing
    to predict yield Precis. Agric., 13 (10) (2012) Google Scholar Zhou et al., 2021
    Z. Zhou, Y. Majeed, G. Diverres Naranjo, E. Gambacorta Assessment for crop water
    stress with infrared thermal imagery in precision agriculture: A review and future
    prospects for deep learning applications Computers And Electronics In Agriculture.,
    182 (2021), Article 106019 https://www.sciencedirect.com/science/article/pii/S0168169921000375
    View PDFView articleView in ScopusGoogle Scholar Zhuang et al., 2018 J. Zhuang,
    S. Luo, C. Hou, Y. Tang, Y. He, X. Xue Detection of orchard citrus fruits using
    a monocular machine vision-based method for automatic fruit picking applications
    Computers And Electronics In Agriculture., 152 (2018), pp. 64-73 https://www.sciencedirect.com/science/article/pii/S0168169918301649
    View PDFView articleView in ScopusGoogle Scholar Zwiggelaar et al., 1996 R. Zwiggelaar,
    C. Bull, M. Mooney X-ray Simulations for Imaging Applications in the Agricultural
    and Food Industries J. Agric. Eng. Res., 63 (1996), pp. 161-170 https://www.sciencedirect.com/science/article/pii/S0021863496900189
    View PDFView articleView in ScopusGoogle Scholar Further reading Zhang, 2021 Z.
    Zhang An Adaptive Vision Navigation Algorithm in Agricultural IoT System for Smart
    Agricultural Robots Comput. Mater. Contin., 66 (2021), pp. 1043-1056 http://www.techscience.com/cmc/v66n1/40496
    View in ScopusGoogle Scholar Cited by (4) Advancements in Utilizing Image-Analysis
    Technology for Crop-Yield Estimation 2024, Remote Sensing Effect of Soil Properties
    and Powertrain Configuration on the Energy Consumption of Wheeled Electric Agricultural
    Robots 2024, Energies Intelligent classifier for various degrees of coffee roasts
    using smart multispectral vision system 2024, Journal of Field Robotics Research
    on Recognition and Localization of Cucumber Based on Complex Environment 2023,
    Research Square View Abstract © 2023 Elsevier B.V. All rights reserved. Recommended
    articles Introduction to the special section on emerging technologies in navigation,
    control and sensing for agricultural robots: Computational intelligence and artificial
    intelligence solutions Computers and Electrical Engineering, Volume 112, 2023,
    Article 109007 Hai Wang, …, Yue Wang View PDF Human–robot collaboration systems
    in agricultural tasks: A review and roadmap Computers and Electronics in Agriculture,
    Volume 204, 2023, Article 107541 George Adamides, Yael Edan View PDF Development,
    analysis, and verification of an intelligent auxiliary beekeeping device mounted
    on a crawler transporter Computers and Electronics in Agriculture, Volume 212,
    2023, Article 108148 Pingan Wang, Xiongzhe Han View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 1 Captures Readers: 54 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An extensive review on agricultural robots with a focus on their perception
    systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bilal A.
  - Liu X.
  - Long H.
  - Shafiq M.
  - Waqar M.
  citation_count: '0'
  description: Farming is cultivating the soil, producing crops, and keeping livestock.
    The agricultural sector plays a crucial role in a country’s economic growth. This
    research proposes a two-stage machine learning framework for agriculture to improve
    efficiency and increase crop yield. In the first stage, machine learning algorithms
    generate data for extensive and far-flung agricultural areas and forecast crops.
    The recommended crops are based on various factors such as weather conditions,
    soil analysis, and the amount of fertilizers and pesticides required. In the second
    stage, a transfer learning-based model for plant seedlings, pests, and plant leaf
    disease datasets is used to detect weeds, pesticides, and diseases in the crop.
    The proposed model achieved an average accuracy of 95%, 97%, and 98% in plant
    seedlings, pests, and plant leaf disease detection, respectively. The system can
    help farmers pinpoint the precise measures required at the right time to increase
    yields.
  doi: 10.32604/cmc.2023.037857
  full_citation: '>'
  full_text: '>

    "Submit LOGIN REGISTER Home Academic Journals Books & Monographs Conferences Language
    Service News & Announcements About Home/ Journals/ CMC/ Vol.76, No.2, 2023/ 10.32604/cmc.2023.037857
    Submit a Paper Propose a Special lssue Table of Content Abstract Introduction
    Related Work Proposed Methodology Experimental Results and Discussion Conclusion
    References Open Access ARTICLE Increasing Crop Quality and Yield with a Machine
    Learning-Based Crop Monitoring System Anas Bilal1,*, Xiaowen Liu1, Haixia Long1,*,
    Muhammad Shafiq2, Muhammad Waqar3 1 College of Information Science Technology,
    Hainan Normal University, Haikou, 571158, China 2 School of Information Engineering,
    Qujing Normal University, Qujing, 655011, China 3 Department of Computer Science,
    COMSATS University, Islamabad, 45550, Pakistan * Corresponding Authors: Anas Bilal.
    Email: ; Haixia Long. Email: Computers, Materials & Continua 2023, 76(2), 2401-2426.
    https://doi.org/10.32604/cmc.2023.037857 Received 18 November 2022; Accepted 19
    June 2023; Issue published 30 August 2023 View Full Text Download PDF Abstract
    Farming is cultivating the soil, producing crops, and keeping livestock. The agricultural
    sector plays a crucial role in a country’s economic growth. This research proposes
    a two-stage machine learning framework for agriculture to improve efficiency and
    increase crop yield. In the first stage, machine learning algorithms generate
    data for extensive and far-flung agricultural areas and forecast crops. The recommended
    crops are based on various factors such as weather conditions, soil analysis,
    and the amount of fertilizers and pesticides required. In the second stage, a
    transfer learning-based model for plant seedlings, pests, and plant leaf disease
    datasets is used to detect weeds, pesticides, and diseases in the crop. The proposed
    model achieved an average accuracy of 95%, 97%, and 98% in plant seedlings, pests,
    and plant leaf disease detection, respectively. The system can help farmers pinpoint
    the precise measures required at the right time to increase yields. Keywords Machine
    learning; computer vision; trends in smart farming; precision agriculture; Agriculture
    4.0 Cite This Article A. Bilal, X. Liu, H. Long, M. Shafiq and M. Waqar, \"Increasing
    crop quality and yield with a machine learning-based crop monitoring system,\"
    Computers, Materials & Continua, vol. 76, no.2, pp. 2401–2426, 2023. BibTex EndNote
    RIS    This work is licensed under a Creative Commons Attribution 4.0 International
    License , which permits unrestricted use, distribution, and reproduction in any
    medium, provided the original work is properly cited. We recommend Identification
    of Crop Diseases Based on Improved Genetic Algorithm and Extreme Learning Machine
    Linguo Li et al., CMC-Computers, Materials & Continua, 2020 Enrichment of Crop
    Yield Prophecy Using Machine Learning Algorithms R. Kingsy Grace et al., Intelligent
    Automation & Soft Computing, 2022 Multimodal Machine Learning Based Crop Recommendation
    and Yield Prediction Model P. S. et al., Intelligent Automation & Soft Computing,
    2022 IoT and Machine Learning Based Stem Borer Pest Prediction Rana Muhammad Nadeem
    et al., Intelligent Automation & Soft Computing, 2022 Deep Transfer Learning Based
    Detection and Classification of Citrus Plant Diseases Shah Faisal et al., CMC-Computers,
    Materials & Continua, 2023 Lettuce have it: Machine learning for cr-optimization
    Phys.org, 2019 Chemists show how bias can crop up in machine learning algorithm
    results Phys.org, 2019 How sensors and big data can help cut food waste by Jean
    Frederic Isingizwe Nturambirwe et al., TechXplore.com, 2020 Impact of dataset
    on the study of crop disease image recognition Yuan Yuan et al., International
    Journal of Agricultural and Biological Engineering, 2022 Robot uses machine learning
    to harvest lettuce by Sarah Collins et al., TechXplore.com, 2019 Powered by Downloads
    Citation Tools 334 View 170 Download 0 Like Related articles Improved VGG Model
    for Road Traffic Sign Recognition Shuren Zhou, Wenlong Liang, Junguo... A Method
    for Improving CNN-Based Image Recognition Using DCGAN Wei Fang, Feihong Zhang,
    Victor... Brake Fault Diagnosis Through Machine Learning Approaches – A Review
    Alamelu Manghai T.M., Jegadeeshwaran... A Comparative Study of Bayes Classifiers
    for Blade Fault Diagnosis in Wind Turbines through Vibration Signals A. Joshuva,
    V. Sugumaran Condition Monitoring of Roller Bearing by K-Star Classifier and K-Nearest
    Neighborhood Classifier Using Sound Signal. Rahul Kumar Sharma, V. Sugumaran,..."'
  inline_citation: '>'
  journal: Computers, Materials and Continua
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Increasing Crop Quality and Yield with a Machine Learning-Based Crop Monitoring
    System
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hashmi M.F.
  - Keskar A.G.
  citation_count: '1'
  description: Machine Learning and Deep Learning for Smart Agriculture and Applications
    delves into the captivating realm of artificial intelligence and its pivotal role
    in transforming the landscape of modern agriculture. With a focus on precision
    agriculture, digital farming, and emerging concepts, this book illuminates the
    significance of sustainable food production and resource management in the face
    of evolving digital hardware and software technologies. Geospatial technology,
    robotics, the Internet of Things (IoT), and data analytics converge with machine
    learning and big data to unlock new possibilities in agricultural management.
    This book explores the synergy between these disciplines, offering cutting-edge
    insights into data-intensive processes within operational agricultural environments.
    From automated irrigation systems and agricultural drones for field analysis to
    crop monitoring and precision agriculture, the applications of machine learning
    are far-reaching. Animal identification and health monitoring also benefit from
    these advanced techniques. One of the book's key focuses is the critical role
    of health monitoring for plants and fruits in achieving sustainable agriculture.
    Plant diseases pose significant financial challenges in the farming industry worldwide.
    By leveraging sophisticated image processing and advanced computer vision techniques,
    automated detection and identification of plant diseases are revolutionized, enabling
    precise and rapid identification while minimizing human effort and labor costs.
    For researchers involved in image processing and computer vision for smart agriculture,
    this book offers invaluable insights. It covers the most important fields of image
    processing in the agricultural domain, encompassing computer vision applications,
    machine learning, and deep learning approaches. From the analysis of agricultural
    data using machine learning to the implementation of bio-inspired algorithms,
    the book explores the breadth and depth of agricultural modernization through
    the lens of AI technologies. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape.
  doi: 10.4018/978-1-6684-9975-7
  full_citation: '>'
  full_text: '>

    "Login Register Language: English Welcome to the InfoSci Platform University of
    Nebraska - Lincoln Database Search Research Tools User Resources Reference Hub1
    Indices1 Machine Learning and Deep Learning for Smart Agriculture and Applications
    Mohamamd Farukh Hashmi, Avinash G. Kesakr Copyright: © 2023 |Pages: 257 ISBN13:
    9781668499757|ISBN10: 1668499754|EISBN13: 9781668499764 DOI: 10.4018/978-1-6684-9975-7
    Cite Book Favorite Full-Book Download Machine Learning and Deep Learning for Smart
    Agriculture and Applications delves into the captivating realm of artificial intelligence
    and its pivotal role in transforming the landscape of modern agriculture. With
    a focus on precision agriculture, digital farming, and emerging concepts, this
    book illuminates the significance of sustainable food production and resource
    management in the face of evolving digital hardware and software technologies.
    Geospatial technology, robotics, the Internet of Things (IoT), and data analytics
    converge with machine learning and big data to unlock new possibilities in agricultural
    management. This book explores the synergy between these disciplines, offering
    cutting-edge insights into data-intensive processes within operational agricultural
    environments. From automated irrigation systems and agricultural drones for field
    analysis to crop monitoring and precision agriculture, the applications of machine
    learning are far-reaching. Animal identification and health monitoring also benefit
    from these advanced techniques. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape. Table of Contents Reset Front
    Materials PDF HTML Title Page PDF HTML Copyright Page PDF HTML Advances in Environmental
    Engineering and Green Technologies (AEEGT) Book Series PDF HTML Preface Chapters
    PDF HTML Chapter 1 Deep Learning Techniques for Smart Agriculture Applications  (pages
    1-23) Ankita Mishra, Sourik Banerjee, Brijendra Singh With an emphasis on the
    rapid and accurate diagnosis of plant and fruit diseases, researchers have been
    looking into sustainable agriculture utilizing cutting-edge deep learning techniques.
    The objective is to show how effective deep... PDF HTML Chapter 2 Review Work
    of Automation Agricultural Robot System Using Machine Learning and Deep Learning  (pages
    24-33) R. Felshiya Rajakumari, M. Siva Ramkumar An occupation based on automatic
    system using machine learning and deep learning techniques is developed in this
    chapter. The agricultural land and automatic systems are worked mutually to defeat
    the concern by integration with solar... PDF HTML Chapter 3 Applications of Deep
    Learning and Machine Learning in Smart Agriculture: A Survey  (pages 34-57) Amrit
    pal Kaur, Devershi Pallavi Bhatt, Linesh Raja Machine learning (ML) and deep learning
    can be used in the smartest way possible to improve productivity in agriculture.
    The Food and Agriculture Organization''s research shows that the crop''s production
    is rising. One of the... PDF HTML Chapter 4 Plant Disease Classification in Segmented
    Images Using Computer Vision  (pages 58-92) Rajashri Roy Choudhury, Piyal Roy,
    Shivnath Ghosh Agriculture productivity has a significant impact on the lives
    of people and economies because of the growing human population. In agriculture,
    plant diseases are a big problem since they result in severe crop losses and financial...
    PDF HTML Chapter 5 Enhancing Tomato Fruit Detection and Counting Through AI-Enabled
    Agricultural Innovations  (pages 93-105) S Gandhimathi Alias Usha This chapter
    aims to enhance tomato fruit detection and counting in agricultural practices
    through AI-enabled innovations. Traditional manual methods for fruit detection
    and counting are labor-intensive and time-consuming. By... PDF HTML Chapter 6
    Harnessing Environmental Intelligence to Enhance Crop Management by Leveraging
    Deep Learning Technique  (pages 106-123) S. Gandhimathi Alias Usha This chapter
    aims to enhance crop management practices by harnessing environmental intelligence
    through the power of deep learning techniques. Efficient and sustainable crop
    management is crucial for meeting the increasing demand for... PDF HTML Chapter
    7 Crop Prediction for Smart Agriculture Using Ensemble of Classifiers  (pages
    124-141) Khushal Kindra, Bhuvaneswari Amma N. G. Nowadays due to the advancement
    in technology, smart agriculture is in the evolving stage. Agricultural farmers
    worldwide commonly utilize the process of cultivating and harvesting crops to
    produce food and fiber. Therefore, crop... PDF HTML Chapter 8 An Integrated Approach
    for Selection and Design of Sustainable Farmers'' Protective-Hat in India  (pages
    142-172) Suchismita Satapathy Normally, the farmers in India are required to work
    in adverse climatic conditions while performing their agricultural activities.
    Though a number of preventive measures are available for the protection of farmers,
    the nominal and... PDF HTML Chapter 9 Smart Crop Protection System From Wild Animals
    Using Artificial Intelligence  (pages 173-191) Shailaja S. Mudengudi, Muktha S.
    Patil, Neetha S. Mudaraddi The first major threat to the farmers is drought. Crop
    vandalization by animals is the second major threat after drought. Crops are vulnerable
    to animals. Therefore, it is very important to monitor the nearby presence of
    animals. The... PDF HTML Chapter 10 GUI-Based End-to-End Deep Learning Model for
    Corn Leaf Disease Classification  (pages 192-213) G. Revathy, J. Jeyabharathi,
    Madonna Arieth, A. Ramalingam Food security is a major problem worldwide. Ensuring
    that the crops produced are both safe and wholesome is crucial not only for people
    as the ultimate consumers of the crops, but also for farmers. Plant diseases are
    responsible for... This content was retracted PDF HTML Chapter 11 A Transfer Learning
    Approach for Detecting Plant Leaf Diseases With Convolutional Neural Networks  (pages
    214-224) P. Valarmathi, N. G. Bhuvaneswari Amma, Vasu Bhasin Agriculture is an
    indispensable sector for the continuity of homo sapiens. In the Indian context
    where agriculture contributes 19.9 percent of GDP and engages almost 54.6 percent
    of the population, it requires great measures to be... Back Materials PDF HTML
    Compilation of References PDF HTML About the Contributors PDF Index View All Books
    Request Access You do not own this content. Please login to recommend this title
    to your institution''s librarian or purchase it from the IGI Global bookstore.
    Username or email:   Password:   Log In   Forgot individual login password? Create
    individual account Research Tools Database Search | Help | User Guide | Advisory
    Board User Resources Librarians | Researchers | Authors Librarian Tools COUNTER
    Reports | Persistent URLs | MARC Records | Institution Holdings | Institution
    Settings Librarian Resources Training | Title Lists | Licensing and Consortium
    Information | Promotions Policies Terms and Conditions     Copyright © 1988-2024,
    IGI Global - All Rights Reserved"'
  inline_citation: '>'
  journal: Machine Learning and Deep Learning for Smart Agriculture and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Machine learning and deep learning for smart agriculture and applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zineb R.
  - Samira K.
  - Larbi K.M.
  citation_count: '0'
  description: DL, short for Deep Learning, is a cutting-edge approach that merges
    advanced techniques in image processing and data analysis with the power of big
    data analysis. Its potential is enormous and has already found practical applications
    in several fields, including autonomous driving, automatic speech recognition,
    medical research, image restoration, natural language processing, and, among others.
    DL has been recently introduced in agriculture showing promising results in solving
    various farming problems like disease detection, automated plant and fruit identification,
    and counting. This study presents a comprehensive review of research using DL
    techniques in farming, including crop monitoring, crop mapping, weed and pest
    detection and management, irrigation, fruit grading, reorganizations of species
    and herbicide identification. Furthermore, different DL techniques applied in
    various fields are analyzed and compared with existing techniques. It was found
    that DL outperforms traditional image processing technology in terms of accuracy,
    both in classification and regression. Additionally, the study suggests that DL
    can be applied beyond detections, classification tasks to yield production, and
    disease segmentation in agriculture.
  doi: 10.1051/e3sconf/202341601035
  full_citation: '>'
  full_text: '>

    "Journals Books Conferences EDPS Account All issuesSeriesForthcomingAbout Search
    Menu All issues Volume 416 (2023) E3S Web Conf., 416 (2023) 01035 Abstract Table
    of Contents Open Access E3S Web of Conferences 416, 01035 (2023) Fatigue Analysis
    of Hybrid Wind Turbine Towers Yuan Wei1,2, Yingjie Li1, Zhaoqi Wu2*, Jinyu Chen1,
    Shaofei Jiang2 and Xianbiao Xiao1 1 State Grid Fujian Electric Power Research
    Institute, Fuzhou 350007, China 2 College of Civil Engineering, Fuzhou University,
    Fuzhou 350108, China * Corresponding author: zhaoqi_wu@fzu.edu.cn Abstract Fatigue
    analysis of hybrid wind turbine towers between cut in wind speed and cut out wind
    speed are demonstrated. Nominal stress method and miner liner accumulated damage
    theory are adopted. Through the analysis of the results, comparative research
    on effect of parameters of aspect ratio, height ratio and unequal legs for fatigue
    properties of hybrid wind turbine towers. The results show that the optimal range
    of aspect ratio of hybrid towers is 1/6~1/4, the optimal range of height ratio
    of hybrid towers is 0.60~0.67. Fatigue analysis of hybrid towers, should select
    the right junction of leeward towers and the S-N curve from EN 1993-1-9.The effect
    of unequal legs for fatigue properties of hybrid towers can be neglected. Key
    words: Wind power generation / Hybrid tower / Fatigue property / Design load spectrum
    / Finite element analysis © The Authors, published by EDP Sciences, 2023 This
    is an Open Access article distributed under the terms of the Creative Commons
    Attribution License 4.0, which permits unrestricted use, distribution, and reproduction
    in any medium, provided the original work is properly cited. Download this article
    in PDF format Table of Contents Article contents AbstractPDF (307.7 KB)References
    Database links NASA ADS Abstract Service Metrics Show article metrics Services
    Same authors - Google Scholar - EDP Sciences database Recommend this article Download
    citation Alert me if this article is corrected Alert me if this article is cited
    Related Articles Research on Mass Imbalance Fault of Wind Turbine Based on Virtual
    Prototype MATEC Web of Conferences 95, 06001 (2017) Fatigue Analysis of Large-scale
    Wind turbine MATEC Web of Conferences 100, 03001 (2017) Foundation Types for Land
    and Offshore Sustainable Wind Energy Turbine Towers E3S Web of Conferences 184,
    01094 (2020)     More Bookmarking Mendeley Reader''s services Email-alert E3S
    Web of Conferences eISSN: 2267-1242 All issues Volume 416 (2023) E3S Web Conf.,
    416 (2023) 01035 Abstract Back to top Mentions légales Contacts Privacy policy
    A Vision4Press website By using this website, you agree that EDP Sciences may
    store web audience measurement cookies and, on some pages, cookies from social
    networks. More information and setup OK"'
  inline_citation: '>'
  journal: E3S Web of Conferences
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Applications of smart agriculture for environmental protection using deep
    learning techniques
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Selea T.
  citation_count: '2'
  description: 'With the increasing volume of collected Earth observation (EO) data,
    artificial intelligence (AI) methods have become state-of-the-art in processing
    and analyzing them. However, there is still a lack of high-quality, large-scale
    EO datasets for training robust networks. This paper presents AgriSen-COG, a large-scale
    benchmark dataset for crop type mapping based on Sentinel-2 data. AgriSen-COG
    deals with the challenges of remote sensing (RS) datasets. First, it includes
    data from five different European countries (Austria, Belgium, Spain, Denmark,
    and the Netherlands), targeting the problem of domain adaptation. Second, it is
    multitemporal and multiyear (2019–2020), therefore enabling analysis based on
    the growth of crops in time and yearly variability. Third, AgriSen-COG includes
    an anomaly detection preprocessing step, which reduces the amount of mislabeled
    information. AgriSen-COG comprises 6,972,485 parcels, making it the most extensive
    available dataset for crop type mapping. It includes two types of data: pixel-level
    data and parcel aggregated information. By carrying this out, we target two computer
    vision (CV) problems: semantic segmentation and classification. To establish the
    validity of the proposed dataset, we conducted several experiments using state-of-the-art
    deep-learning models for temporal semantic segmentation with pixel-level data
    (U-Net and ConvStar networks) and time-series classification with parcel aggregated
    information (LSTM, Transformer, TempCNN networks). The most popular models (U-Net
    and LSTM) achieve the best performance in the Belgium region, with a weighted
    F1 score of 0.956 (U-Net) and 0.918 (LSTM).The proposed data are distributed as
    a cloud-optimized GeoTIFF (COG), together with a SpatioTemporal Asset Catalog
    (STAC), which makes AgriSen-COG a findable, accessible, interoperable, and reusable
    (FAIR) dataset.'
  doi: 10.3390/rs15122980
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals
    Remote Sensing Volume 15 Issue 12 10.3390/rs15122980 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editors Enrico
    Corrado Borgogno Mondino Filippo Sarvia Samuele De Petris Show more... Subscribe
    SciFeed Recommended Articles Related Info Link More by Author Links Article Views
    1763 Citations 2 Table of Contents Abstract Introduction Crop Datasets for ML/DL
    Applications Popular DL Methods for Crop Type Mapping Anomaly Detection The AgriSen-COG
    Dataset Experimental Results Discussion Conclusions Funding Data Availability
    Statement Conflicts of Interest References share Share announcement Help format_quote
    Cite question_answer Discuss in SciProfiles thumb_up Endorse textsms Comment first_page
    settings Order Article Reprints Open AccessArticle AgriSen-COG, a Multicountry,
    Multitemporal Large-Scale Sentinel-2 Benchmark Dataset for Crop Mapping Using
    Deep Learning by Teodora Selea Faculty of Mathematics and Informatics, West University
    of Timisoara, 300223 Timisoara, Romania Remote Sens. 2023, 15(12), 2980; https://doi.org/10.3390/rs15122980
    Submission received: 12 May 2023 / Revised: 3 June 2023 / Accepted: 5 June 2023
    / Published: 7 June 2023 (This article belongs to the Special Issue Remote Sensing
    and Associated Artificial Intelligence in Agricultural Applications) Download
    keyboard_arrow_down     Browse Figures Review Reports Versions Notes Abstract
    With the increasing volume of collected Earth observation (EO) data, artificial
    intelligence (AI) methods have become state-of-the-art in processing and analyzing
    them. However, there is still a lack of high-quality, large-scale EO datasets
    for training robust networks. This paper presents AgriSen-COG, a large-scale benchmark
    dataset for crop type mapping based on Sentinel-2 data. AgriSen-COG deals with
    the challenges of remote sensing (RS) datasets. First, it includes data from five
    different European countries (Austria, Belgium, Spain, Denmark, and the Netherlands),
    targeting the problem of domain adaptation. Second, it is multitemporal and multiyear
    (2019–2020), therefore enabling analysis based on the growth of crops in time
    and yearly variability. Third, AgriSen-COG includes an anomaly detection preprocessing
    step, which reduces the amount of mislabeled information. AgriSen-COG comprises
    6,972,485 parcels, making it the most extensive available dataset for crop type
    mapping. It includes two types of data: pixel-level data and parcel aggregated
    information. By carrying this out, we target two computer vision (CV) problems:
    semantic segmentation and classification. To establish the validity of the proposed
    dataset, we conducted several experiments using state-of-the-art deep-learning
    models for temporal semantic segmentation with pixel-level data (U-Net and ConvStar
    networks) and time-series classification with parcel aggregated information (LSTM,
    Transformer, TempCNN networks). The most popular models (U-Net and LSTM) achieve
    the best performance in the Belgium region, with a weighted F1 score of 0.956
    (U-Net) and 0.918 (LSTM).The proposed data are distributed as a cloud-optimized
    GeoTIFF (COG), together with a SpatioTemporal Asset Catalog (STAC), which makes
    AgriSen-COG a findable, accessible, interoperable, and reusable (FAIR) dataset.
    Keywords: benchmark dataset; crop monitoring; crop detection; deep learning; image
    segmentation; multitemporal analysis; crop classification; agricultural application;
    Sentinel-2; common agricultural policy (CAP) 1. Introduction Artificial intelligence
    (AI) has become a hot topic in the past decade, and since 2015, it has been an
    important method used in the Earth observation (EO) community [1]. The adoption
    of AI techniques—machine learning (ML) and deep learning (DL)—for EO-related use
    cases is a consequence of the increasing volume of publicly available satellite
    data (e.g., Copernicus Sentinels), and thus, a need to process it. Possible use
    cases are land cover and land use, deforestation, urban mapping, and agriculture.
    In this paper, we focus on the latter, particularly on the task of crop type mapping.
    Crop type mapping supports the crop monitoring task, which is helpful for agricultural
    insurance or implementing common agricultural policies (CAP). Applying crop monitoring
    at a global scale can be conducted with the help of satellite data, which offers
    global coverage. Monitoring crops is a crucial factor for further agricultural
    development, as we need a production increase of 60% to serve the demand of current
    population growth (Food and Agriculture Organization (FAO) of the United Nations
    study [2]). However, the increase in yield needs to be sustainable without damaging
    natural resources or the environment (Climate Sustainable Development Goals (SGD)
    [3]). To properly put these two objectives together, global and joint monitoring
    must be established. In the context of AI methods, the crop type mapping use case
    is assimilated with the semantic segmentation task from computer vision (CV),
    described as attaching a class label to every pixel from the image. It can also
    be incorporated into a classification problem by aggregating parcel data and offering
    one label for each parcel. However, as opposed to standard 3-channel (red, green,
    blue) images, satellite data enrich the input by including multitemporal and multichannel
    information. For the selected use case, the temporal data are beneficial for identifying
    crop phenology, and therefore, for better classifying it. Furthermore, the multichannel
    satellite data refine the crop information, as the various spectra capture different
    characteristics. The advancement of AI methods is correlated to the training datasets’
    availability, size, and quality. While other domains (e.g., computer vision) already
    have several benchmarking datasets (e.g., ImageNet [4], CityScape [5]), there
    is still a lack of large-scale, high-quality EO datasets. Even though large volumes
    of satellite data are publicly available, the challenge comes from creating ground
    truth (GT) data. For crop type mapping, GT information for dataset creation may
    be extracted from land parcel identification systems (LPIS). Several European
    countries have made their LPIS information public in the past few years, and therefore,
    there has been an increase in crop-related datasets for the European area. However,
    since the LPIS is created individually by each country with data received from
    farmers, it raises the following problems: no crop naming conventions, different
    languages used, and errors in collecting the crop type for each parcel. Existing
    datasets include information from only one or two EU countries without a standard
    way of storing and distributing the datasets. Datasets that combine knowledge
    from several areas would improve the state-of-the-art methods related to domain
    adaptation. A unified way of accessing the dataset leads to ease of usage and
    faster integration with DL techniques. In addition, there is also a lack of a
    methodology for LPIS processing so that the existing training crop-type datasets
    may be easily extended once new data become available. Moreover, as LPIS is created
    based on farmer data, it is prone to human error, which may lead to incorrect
    labels affecting the performance of the ML/DL model; therefore, the data need
    to be curated. In this paper, we propose AgriSen-COG, a new large-scale crop-type
    mapping dataset with the following characteristics: (1) it is based on publicly
    available data only (Sentinel-2 and LPIS), making it easily extensible; (2) it
    includes data from five different European countries (Austria, Belgium, Spain,
    Denmark, and the Netherlands), targeting the problem of domain adaptation; (3)
    it incorporates an anomaly detection based on autoencoder as preprocessing step
    that lowers the amount of mislabeled information for GT; (4) it is multitemporal
    and multiannual, incorporating crop phenology and seasonal variability; (5) it
    includes pixel-based data to account for the crop parcel’s spatiality; (6) it
    incorporates time-series data for crop classification (when the parcel geometry
    is known). AgriSen-COG is distributed using accessible formats such as COGs, Zarr,
    and Parquet and is indexed under SpatioTemporal Assets Catalogues (STAC). By using
    COGs, we ensure an easy way of accessing the data without the need to download
    the entire dataset. STAC enables a standard way to discover and describe our dataset,
    making it reusable and interoperable. Our main contributions are as follows: We
    created AgriSen-COG, a large-scale benchmark dataset for crop type mapping, including
    the largest number of different European countries (five), designed for AI applications.
    We introduce a methodology for LPIS processing to obtain GT for a crop-type dataset,
    useful for further extensions of the current dataset. We incorporate an anomaly
    detection method based on autoencoders and dynamic time warping (DTW) distance
    as a preprocessing step to identify mislabeled data from LPIS. We experiment with
    popular DL models and provide a baseline, showing the generalization capabilities
    of the models on the proposed dataset across space (multicountry) and time (multitemporal).
    We provide the LPIS preprocessing, anomaly detection, training/testing code, and
    our trained models to enable further development. The remainder of this article
    starts with a review of other datasets for crop type mapping. Next, we present
    popular ML/DL techniques for our selected use case. Furthermore, we continue with
    the proposed methodology for dataset creation and describe how we applied anomaly
    detection to curate the LPIS information. Finally, we present our experimental
    results, followed by a discussion, and summarize our conclusions. 2. Crop Datasets
    for ML/DL Applications Benchmark datasets play a significant role in developing
    ML/DL methods. New methods need to be assessed using the same input dataset to
    eliminate the bias given by learning from different data. The progress in computer
    vision deep learning methods is partially due to numerous large benchmarking datasets
    (e.g., ImageNet [4], MS COCO [6], Cityscape [5]) that offer access to many annotated
    samples. As the interest in ML/DL applied to remote sensing data is increasing,
    so is the demand for large-scale datasets processed to fit various use cases (e.g.,
    land cover, flooding, building detection, agriculture). Figure 1 presents the
    publication date of popular RS datasets, showing a recently increased interest
    in large-scale crop type mapping datasets. Each dataset is characterized by the
    following: (1) the area covered; (2) the time extent for the collected data; (3)
    the source for both input and GT data; (4) the intended use case; (5) the number
    and size of provided patches (the original image size might be too large to fit
    into hardware memory; therefore, the images are cropped in smaller, ready-to-use
    patches). Figure 1. Crop-related datasets’ publication timeline. The first attempts
    to create large-scale datasets for RS scenarios targeted the land cover problem.
    BigEarthNet [7] represents one of the first large-scale RS image archives. It
    targets land-cover multi-classification and uses Sentinel-2 data as input, matched
    with Corine Land Cover (CLC) information for ground truth. It includes data from
    June 2017 to May 2018, covering ten different European regions: Austria, Belgium,
    Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, and Switzerland.
    Since it is a dataset designed to work with computer vision ML/DL models, the
    authors distribute the dataset in patches, resulting in a total of 590,326 nonoverlapping
    patches, with different sizes for each resolution:  120×120  pixels for 10 m band,  60×60  pixels
    for 20 m, and  20×20  for 60 m bands. Also targeting land cover, but from a semantic
    segmentation perspective, is the Sen12MS [8] dataset. Like BigEarthNet, it uses
    Sentinel-2 data, but adds Sentinel-1 and extracts its ground truth information
    from MODIS land cover maps. Sen12MS is distributed in overlapping patches of  256×256
    , with a stride of 128. The dataset includes 180,662 patch triplets (Sentinel-1,
    Sentinel-2, GT) covering a global area and including all meteorological seasons.
    Sen12MS distributes its patches at a resolution of 10m, with upsampled data for
    the 60 m and 20 m resolution bands. As large-scale remote sensing land cover datasets
    appeared, the need to target more specific use cases also emerged. This paper
    focuses on the particular scenario of crop-type mapping. Crop type mapping datasets
    are based on land parcel identification system [9] (LPIS) information to generate
    ground truth data. Therefore, datasets are developed together with the release
    of open access to regions’ parcel information. Compared to a land cover dataset,
    a crop type dataset includes only one category of labels—agricultural fields—but
    with increased granularity. Therefore, each crop parcel is delimited and labeled
    with the corresponding crop type. BreizhCrops [10] is the first large-scale dataset
    for crop classification, covering the Brittany area of France (27,200 km 2 ),
    spanning over the entire years of 2017 and 2018. The proposed dataset provides
    nine broad categories of crops and uses both Sentinel-2 Level-1C and Level-2A
    data to gain better regional coverage. The GT is created from the France’s publicly
    available LPIS database (Registre Parcellaire Graphique—RPG). The authors provide
    mean-aggregated values per band/timeframe over each field parcel, not image patches
    at a pixel level. ZueriCrop [11] is a dataset for crop type mapping based on Sentinel-2
    Level-2A bottom-of-atmosphere images from the Switzerland area (50 km × 48 km
    area). The GT data were extracted from Switzerland’s LPIS (Swiss Federal Office
    for Agriculture), which is not publicly available. The dataset comprises 48 different
    crop classes at the lowest hierarchical level and 5 categories at most at the
    top level. The crops are observed for 2019, resulting in 28,000 patches of  24×24  pixels,
    including 116,000 crop parcels. The Austrian region for crop type mapping is covered
    in the dataset proposed by [12] (TimeSen2Crop). The dataset uses Sentinel-2 bottom-of-atmosphere
    images spanning between September 2017 and August 2018. For GT, the authors used
    the publicly available Austrian LPIS, offering 16 different labels in the end.
    TimeSen2Crop is distributed as a monthly median composite for each tile, and comprises
    around 1 million labeled samples in total. DENETHOR [13] is the first dataset
    for crop type mapping that includes commercial data, namely Planet. It consists
    of three types of input data: Planet (3 m resolution), Sentinel-1, and Sentinel-2.
    DENETHOR covers the area of Northern Germany over two years: 2018 and 2019. Like
    the datasets mentioned above, DENETHOR uses the LPIS from Germany; however, it
    is not a publicly available database. The dataset includes nine crop type labels,
    with 4500 crop fields. Sen4AgriNet [14] is also a crop classification dataset
    based on Sentinel-2 Level-1C and LPIS data. The dataset uses the FAO ICC [15]
    classification for aggregating the LPIS information from France and Catalonia.
    It is the first multicountry and multiyear dataset, comprising two regions: France
    and Catalonia. Sen4AgriNet comprises aggregated data (crop parcel average) and
    pixel-based data (image patches), making it a valuable dataset for crop classification
    and crop segmentation. Sen4AgriNet offers the full spectrum of Sentinel-2 bands,
    preserving the initial spatial resolution. The data are divided into smaller patches
    ( 366×366  pixels for 10 m resolution,  183×183  pixels for 20 m resolution, and  61×61  pixels
    for 60m resolution). The temporal extent includes the years 2019 and 2020, resulting
    in patches with 168 class labels. Sen4AgriNet is distributed using the NetCDF
    format, making it compatible with modern self-describing tools such as Xarray
    [16,17]. AI4Boundaries [18] is the first multicountry crop dataset intended for
    field boundary detection, including 14.8 million parcels. It covers 2019 and incorporates
    seven different regions: Austria, Catalonia, France, Luxembourg, the Netherlands,
    Slovenia, and Sweden. AI4Boundaries offers two complementary datasets. First,
    it uses Sentinel-2 cloud-free monthly composites, with tiles 256 pixels in size,
    including the four 10 m resolution bands. Second, AI4Boundaries provides three
    channels at 1m resolution orthophotos of  512×512  pixels. We described several
    crop-type mapping datasets that include the European region, as they provide particularities
    in creating the ground truth data from LPIS databases. Most previously mentioned
    datasets use the available LPIS information for the selected country. However,
    they lack a methodology for the steps needed to process the data to obtain high-quality
    GT. Extending current datasets is tedious and prone to error tasks without a standard
    way of processing. Furthermore, although several regions are covered (Austria,
    Northern Germany, France, Catalonia, Switzerland), there is no standard in class
    naming and grouping from the initial LPIS. Besides Sen4AgriNet (covering France
    and Catalonia), each dataset only covers one type of data: pixel or parcel aggregate,
    making it difficult to benchmark other results. Additionally, there is no mention
    of possible mislabeled parcels in the original LPIS, which may cause training
    errors later. AgriSen-COG is designed to extend existing datasets (Table 1), complementing
    both temporal and spatial perspectives. Like Sen4AgriNet, we use the FAO ICC crop
    naming conventions to provide a standard and extensible way of labeling the dataset.
    AgriSen-COG includes pixel-level and object-aggregated data for two years (2019
    and 2020), enriching BreizhCrops, TimeSen2Crop, and DENETHOR. Compared to Sen4AgriNet,
    for the Catalonia region, our proposed dataset provides additional granularity
    in crop type label selection paired with additional anomalous label removal; it
    is also based on Sentinel-2 Level-2A data, as opposed to Level-1C as in Sen4AgriNet,
    including more patches from the Catalonia area (5168 patches vs. 4638 patches),
    and incorporates a different parcel aggregation method (barycenter vs mean and
    standard deviation). AgriSen-COG is the first dataset to include information from
    five EU countries. It is also easily integrated with AI4Boundaries for parcel
    boundary refinement or for discovering and creating new crop type datasets. Table
    1. Summary of the main characteristics of popular large-scale datasets for land
    cover and crop type mapping. 3. Popular DL Methods for Crop Type Mapping As the
    crop type mapping problem is frequently assimilated with the semantic segmentation
    task from computer vision (CV), the first DL approaches were heavily based on
    CV models based on classic convolutional neural networks (CNNs). However, satellite
    data pose additional characteristics to the three-channel (red, green, blue—RGB)
    CV image. These distinct properties include larger image size, increased number
    of channels, and the temporal dimension of the data. For the specific task of
    crop type mapping, the near-infrared band proves particularly useful in combination
    with the RGB channels, as it captures vegetation characteristics. The temporal
    dimension is essential for classifying crops because it provides information about
    their growing cycles. The larger size issue is solved by tiling the initial image
    (e.g., 10,980 × 10,980 pixels) into smaller patches (e.g.,  366×366  pixels) that
    fit the network and hardware limitations. Regarding the previously mentioned factors,
    crop type mapping is implemented using two approaches: a semantic segmentation
    problem or a time-series classification task. The first case uses DL networks
    composed entirely of CNNs or in combination with recurrent neural networks (RNNs)
    [19,20]. The second approach benefits from simple RNNs or transformer [21] networks.
    However, a plain CNN network can also be used for time-series classification.
    U-Net [22] is a popular CNN topology with good results on semantic segmentation.
    The main characteristic of U-Net is the “U”-shaped architecture, where information
    from intermediate layers of the encoder part of the network is transferred to
    the decoder. Even though it was built for biomedical images, U-Net has been successfully
    applied on EO-related use cases such as land cover ([23,24,25]), cloud masking
    ([26,27,28]), building detection ([29,30,31]), crop type mapping ([32,33,34]),
    and others. ConvLSTM [35] uses a combination of CNN layers with a particular type
    of RNN, the LSTM cell. The proposed network was designed for spatiotemporal inputs,
    in particular for the use case of precipitation nowcasting. ConvLSTM enhances
    the simple LSTM [36] cell by applying a convolutional layer over the input data.
    Therefore, the network exploits the data’s temporal (LSTM) and spatial (convolution)
    dimensions. This topology is also popular among remote sensing data, in particular
    for a use case that benefits from both spatial and temporal dimensions such as
    land cover ([37,38,39]), soil moisture ([40,41]), solar radiation ([42]), air
    quality [43], and others, including crop type mapping ([44,45,46]). In [47], the
    authors propose a more efficient version of ConvLSTM, named ConvSTAR. It eliminates
    several operations inside the LSTM cell (input and output gates), which results
    in a faster and more stable training process. Crop type mapping may be viewed
    as a time-series classification problem if the spatial dimension is discarded
    and information is aggregated at the parcel level, usually with a mean over a
    parcel’s pixels. The result is a sequence for each crop parcel, with the length
    equal to the number of sensing times for each polygon. Therefore, simple RNNs
    such as LSTMs have been used, for example, with datasets such as BreizhCrop for
    crop classification. The Transformer model [21] is another popular way to deal
    with sequence data. It uses the attention mechanism, which enables the network
    to access any particular step. Transformers are the new state of the art in natural
    language processing, as they require fewer parameters than LSTMs, resulting in
    faster training and more accurate results. TempCNN [48] is a CNN network capable
    of handling sequence data. As opposed to the LSTM and transformer, it was designed
    specifically for crop classification in France. It uses one-dimensional convolutional
    layers applied to satellite image time series (SITS). To prove the validity of
    the proposed dataset, we experiment with the two methods of crop detection: similar
    to semantic segmentation and time-series classification. Therefore, we present
    how AgriSen-COG is a promising dataset for new developments, regardless of the
    approach. To carry this out, we use popular deep-learning models from each category.
    We aim to provide a benchmark and starting point for each technique to help them
    further progress. 4. Anomaly Detection Dataset quality is strongly related to
    the performance of an ML/DL algorithm and directly influences a model’s ability
    to predict the desired result accurately. The garbage in, garbage out principle
    particularly applies when learning from data. Bad-quality data affect the training
    process, leading to longer training time and poor performance. However, when trained
    on well-labeled data, the algorithms are faster and better at discovering the
    patterns in the data. As previously seen, the LPIS requires information directly
    from the farmers. Therefore, it might include errors affecting the quality of
    the generated GT. To assess the quality of the labels in the proposed dataset,
    we treat it as a problem of anomaly detection applied to time series. Since we
    only have a set of labels and we aim to identify outliers, we apply an unsupervised
    anomaly detection method. Dynamic time warping (DTW) is a popular distance used
    for measuring similarities between time series. As opposed to the Euclidian distance,
    it enables the comparison of shifted time series with heterogenous lengths. In
    the case of crop phenology, it is essential to be able to identify similar growing
    cycles, even if they are shifted. DTW has already been applied on several use
    cases that aimed to find outliers or to align a time series: magnetic data [49],
    electric grid [50], livestock activity [51]. DTW is also implemented in combination
    with ML techniques, such as k-nearest neighbors (KNN) [52] or the DL method, with
    autoencoders [53]. In [54], the authors propose a way to compute a barycenter,
    a cluster center that minimizes the DTW distance. DTW has already been used to
    identify crop similarity, mostly for the crop classification task. In [55], the
    authors used the DTW distance to identify outliers in the LPIS data for the area
    of Italy, converting the problem of crop classification to an anomaly detection
    task. The proposed method used the normalized difference vegetation index (NDVI)
    [56] and computes a reference feature trend from the histogram of the available
    parcels. Afterward, several thresholds were calculated based on the true and false
    positive rates. Anomaly detection on crop sites was also performed by [57], where
    the authors used a histogram to identify the anomalous pixels. In [58], the authors
    also integrated an outlier detection step during their dataset preparation stage
    to target the use case of grassland mowing detection over Estonia. For each grassland
    field, the authors computed the NDVI using Sentinel-2 data and applied an anomaly
    detection step to distinguish the outliers introduced by clouds. The outliers
    were identified by considering triplets of consecutive NDVI measurements and identifying
    the triplet that forms a rhombus shape. Moreover, the authors proposed to exclude
    several prediction results with low confidence from the employed deep learning
    network. The idea of anomaly detection in agriculture was also explored in [59].
    The authors performed outlier detection on IoT data using deep learning methods.
    The DTW distance is used as a measure of performance. DTW distance was also used
    in [60] to detect anomalies in satellite sensor time series data acquisition.
    The authors argue that a data-driven method has the advantage of not requiring
    expert knowledge, as in the case of establishing specific thresholds. Another
    plus would also be against model-based approaches, which require an accurate mathematical
    model that is time-consuming to achieve. However, the data-driven method is based
    on the similarity between the time series. The authors combined the DTW distance
    with KNN algorithm to better identify the outliers. The DTW was also applied by
    [61], where it was used to detect outliers in plane traffic control. However,
    the authors use the DTW algorithm to obtain the best sequence alignment, and afterward
    to apply a Euclidian distance to measure the distance between two instances. Autoencoders
    were applied in [62] for the specific task of identifying mislabels on LPIS data
    over the Sevilla (Spain) region. The authors proposed a method using Sentinel-1
    as input data and an autoencoder with 1D convolutional layers in its encoder part.
    The proposed work analysed the LPIS labels from two perspectives: at the parcel
    level and at the class level. Therefore, the authors identified possible mislabeled
    pixels from one parcel, such as entire polygons, which might have received the
    wrong crop label. In the paper, there was no aggregation performed at the parcel
    level, but the convolutional layers were used to extract the temporal feature.
    In order to separate anomalies, the dynamic Otsu [63] thresholding was applied.
    For our proposed dataset, we apply an anomaly detection method using an LSTM-based
    autoencoder as we work with variable-length sequences. Furthermore, as our dataset
    is formed from six regions, we need to reduce the dimensionality of our data.
    Therefore, we apply it at the class level, using our Sentinel-2 data, aiming to
    identify only mislabeled parcels. 5. The AgriSen-COG Dataset The area reflected
    in the proposed dataset covers regions from five EU countries (Austria, Belgium,
    Spain, Denmark, and Netherlands). AgriSen-COG comprises 6,972,485 parcel observations,
    grouped in 41,100 patches of size  366×366  pixels. Each observation is described
    by the parcel’s spatial and temporal characteristics or as a univariate time series,
    with an aggregated version for each polygon. 5.1. Input Satellite Data The dataset
    is based on ESA Sentinel-2 [64] data, as they have the largest spectral and spatial
    resolution out of all free optical satellite optical data, which helps for a more
    precise segmentation. The Sentinel-2 mission is a constellation of two satellites
    in the same orbit, with a phase of 180° from each other. It has a high revisit
    time (5 days) and offers 13 spectral bands at 3 spatial resolutions: 10 m, 20
    m, and 60 m. The Sentinel-2 product is available to users under two processing
    levels: Level 1C—top of atmosphere, and Level 2A—bottom of atmoshepere. The latter
    is derived from Level 1C; as the name suggests, it contains an atmospheric corrected
    image. In addition, Level 2A is delivered with cloud probability data and a scene
    classification mask (SCL), incorporating land cover, cloud, and snow labels. A
    Sentinel-2 orthoimage (tile) corresponds to an area of  100×100  km 2 , distributed
    in UTM/WGS84 projection. 5.2. LPIS—Crop Type Labels Land parcel identification
    systems [9] (LPIS) are designed to record all of the EU’s crop parcel information.
    Common agricultural policy (CAP) uses them to verify agricultural subsidies and
    environmental obligations. LPISs are an essential component of IACS, the primary
    system CAP uses for handling subsidiaries. The system serves several objectives:
    validating the parcel identification information, assessing the eligibility area,
    and aiding in on-the-spot controls conducted for administrative purposes or by
    CAP. LPISs are databases created using information from the farmers and are under
    the governmental administration of each country. Therefore, no naming convention
    makes combining LPIS datasets from different countries difficult. Several EU administrations
    have published their LPIS information in the past few years, accelerating the
    creation of crop-type-mapping-related datasets. 5.3. Dataset Creation Methodology
    Crop type mapping datasets are built upon publicly available data. Several countries
    have already published their LPIS information for the EU region, making it possible
    to create large-scale crop type mapping datasets for ML/DL. In this paper, we
    analyze the available parcel information from all open-source EU crops’ data,
    resulting in the collection, processing, and analysis of five different areas
    (Austria, Belgium, Catalonia, Denmark, and Netherlands). Since there is no standard
    regarding LPIS data, creating a dataset raises several challenges due to the nonuniformity
    of the data. We also propose a detailed and reproducible methodology based on
    the LPIS input’s heterogeneity, which may be applied further to extend the current
    dataset with additional areas. Multiple regions included in an ML/DL dataset for
    crop type mapping can increase a model’s generalization capability or help in
    the domain adaptation method. In this context, a similar processing method is
    crucial for providing consistency among the data. Our dataset creation process
    is divided into three stages: (1) LPIS processing, to obtain a standard crop description
    for each polygon; (2) preparing the rasterized data, to obtain pairs of input
    data; and GT (3) improving dataset quality using anomaly detection. In Figure
    2 and Figure 3, we present the workflow of our methodology and the challenges
    they handle. The final dataset and several intermediate preprocessing steps are
    available for download from our repository. The entire processing code may be
    accessed as Python scripts on the project’s GitHub, and serves as a quickstart
    in modifying or extending the proposed dataset. Figure 2. Overview of the LPIS
    processing workflow. Figure 3. Overview of the rasterization workflow. (1) LPIS
    processing Step 1.1 is represented by LPIS data collection. In the absence of
    a shared database, retrieving each piece of LPIS information is time-consuming
    and performed manually. Most of the LPIS datasets are available on the Ministry
    of Agriculture of each country’s webpage. However, navigation is hampered, as
    most websites use specific acronyms in the original language to denote the files.
    We collected and published the original files involved in the proposed dataset
    for easier access (Output 1.1). The original files helped to test the proposed
    workflow or conduct different analyses/processing. Step 1.2 consists of converting
    the LPIS data to a unified format, which helps with further processing. The lack
    of a standard is visible first in the output format used to deliver the LPIS data.
    We encountered Shapefiles [65], Geopackages [66], Geodatabases [67], and GeoJSON
    [68] files. The output format consistency does not apply even for the same area,
    as there is a distinct format for different years. Therefore, we unified the file
    types, providing the data in two formats (Output 1.2): Geopackage and Parquet.
    Geopackage was chosen due to its popularity among the geocommunity and the integration
    with geotools (e.g., QGIS [69]). We also provided the data in a partitioned Parquet
    [70] format that allows for further distributed processing, which is needed when
    handling a large number of polygons. For our end goal, we require a list of geometrical
    shapes mapped with a label for each polygon. Step 1.2 is crucial in standardizing
    the LPIS data to apply uniform processing algorithms later. Step 1.3 continues
    our standardization by selecting a set of columns of interest, renaming them based
    on a chosen convention way, and translating the corresponding values to English.
    For each LPIS, we considered the crop type, crop group, area, and geometries-related
    columns. We believe crop type and geometries are mandatory fields, as they include
    the preliminary information we need for the proposed dataset. Crop group and area
    information are supplementary materials useful for statistical analysis. If the
    area for each polygon was not provided, we automatically computed it. Next, we
    proceeded with the English translation of the unique labels from the original
    LPIS. For a proper translation, we identified the correct encoding for each original
    LPIS file. The translations were manually corrected for each country to remove
    errors. This was a time-consuming manual process, but it was necessary to align
    the labels among various LPIS systems. Output 1.3 includes the English version
    of the LPIS data, with the same column naming for each country, together with
    a list of the corresponding encodings and translation of each label. Both types
    of information serve as a starting point for further extension of the current
    dataset. Sen4AgriNet also offers translation data for France and Catalonia. However,
    there are differences between our proposed translation and theirs, probably due
    to our translation revision process, which improves the quality of the final output.
    Step 1.4 addresses another issue created by the absence of a standard: distinct
    names for the same crop label class. This problem exists even for the same region
    when analyzing data from different years. We used the FAO Indicative Crop Classification
    (ICC) [15] categories to solve this and map each crop type to a new label. We
    followed the same naming convention as Sen4AgriNet, as we created AgriSen-COG
    to integrate with existing datasets. Furthermore, we believe that using a clear
    standard for crop labeling is helpful in further dataset extensions. FAO ICC uses
    a taxonomy that divides the crop types based on group > class > subclass > order.
    For AgriSen-COG, we chose the innermost ICC label and attached a number code to
    each crop type. There are 168 classes and subclasses in all, making up the custom
    FAO/CLC classification scheme. We incorporated two supplementary classes, Fallow
    land and background, in our final GT. In addition, each region’s resulting file
    (Output 1.4) contains all the upper levels from FAO ICC, including group, class,
    subclass, and order. (2) Preparing rasterized data Step 2.1 starts our rasterization
    process for converting the LPIS data into actual raster data. It consists of finding
    the exact boundaries of each area of interest (AOI). The limits are needed in
    the next step to finding the intersecting Sentinel-2 tiles. One may choose between
    a region/country’s actual border coordinates to retrieve the boundaries or extract
    them from the LPIS file. We tested both approaches and decided to follow the latter.
    Even though the first version is faster, as the border files are publicly available,
    we are only interested in the region with agricultural representation. Therefore,
    we computed the boundaries from the previously generated LPIS. In this way, we
    eliminated from the start all of the Sentinel-2 tiles that do not intersect with
    any crop polygons. Step 2.2 continues with the discovery of the Sentinel-2 tiles
    that will serve as input data for the proposed dataset. To ease the searching
    process, we used the S2 Amazon STAC catalogue (AWS S2 COGs STAC: https://registry.opendata.aws/sentinel-2-l2a-cogs/,
    accessed on 4 June 2023). We conducted STAC searching queries based on each AOI’s
    boundary, cloud percentage, and our dates of interest. Step 2.3 starts the actual
    rasterization of our LPIS information. The advantage of using S2 COGs is that
    we can study a Sentinel-2 tile without needing to download it. We took the unique
    tile regions identified in the previous step and used their bounding boxes to
    map the LPIS polygon on a new raster for each tile. The result (Output 2) was
    a georeferenced array for each tile. These constitute the ground truth data of
    the proposed AgriSen-COG. We used the previously mapped FAO ICC code to generate
    the pixel values for each geometry. The raster images were generated by matching
    each Sentinel-2 tile’s coordinate reference system (CRS). The GT raster was released
    under the following formats: Geotiff [71], as it is a popular geo-format; and
    Zarr [72], to enable distributed processing; and COGs (cloud-optimized Geotiff:
    https://www.cogeo.org, accessed on 4 June 2023), to allow image access without
    downloading the data. (3) Improving dataset quality with Anomaly Detection We
    integrated an identification of mislabeled GT as a preprocessing step for our
    dataset creation. To our knowledge, AgriSen-COG is the first crop type dataset
    to incorporate an anomaly detection step to curate the data. Our goal is to identify
    the mislabeled crop parcels. Therefore, we need to prepare aggregated information
    at the polygon level. The workflow for our data preparation and anomaly detection
    process is described in Figure 4 and Figure 5. Figure 4. Overview of the anomaly
    detection preprocessing workflow. Figure 5. Overview of the anomaly detection
    workflow. Step 3.1 starts our data preparation for the anomaly detection task.
    First, we computed the NDVI index ( 𝑁𝐼𝑅−𝑅𝑒𝑑 𝑁𝐼𝑅+𝑅𝑒𝑑 ) to capture the characteristics
    of our crop vegetation while reducing the multichannel structure to a one-channel
    image. The NDVI was computed for each tile at a pixel level to preserve the temporal
    and spatial dimensions. Step 3.2 continues with cloud masking the NDVI image.
    As the NDVI was later used for anomaly detection, clouds would alter the process
    and include bias in a polygon’s time series. Therefore, we applied a cloud mask
    on each image. We decided to use the SCL mask, already delivered with the Sentinel-2
    product, eliminating the need for another cloud processing algorithm to be added
    to our workflow. The SCL mask offers comparable results to top cloud masking methods
    [73]. From the 12 labels present in the SCL mask, we implemented the cloud- and
    snow-related pixel classes, namely saturated or defective, cast shadows, cloud
    shadows, cloud medium probability, cloud high probability, thin cirrus, and snow
    or ice. Step 3.3 assembles our NDVI time series. Each pixel from our dataset has
    the following properties: (1) a sequence of NDVI values, representing the vegetation
    characteristics captured at different moments; (2) a crop label, describing the
    corresponding crop class (from LPIS); (3) a polygon identifier, assimilated to
    a number given to each polygon from the LPIS data. Initially, the information
    is stored as multiple matrixes of pixels, which are transformed into sequences
    having the values mentioned earlier. Step 3.4 is the final data processing step
    before applying the LSTMAutoencoder for anomaly detection. As we are only interested
    in detecting anomalies at the polygon level, we aggregated the time series corresponding
    to pixels from the same polygon. Possible aggregations include polygon median,
    mean, or computing the barycenter. In our time series, we might have missing data
    for the same polygon due to cloud masking or just missing data from the original
    Sentinel-2 image. The median and mean are more sensitive to the missing data situations,
    as mentioned earlier. Therefore, we computed the barycenter for each polygon,
    capturing the time-related variability of each polygon. The barycenter (Equation
    (1)) was computed using the DTW barycenter averaging (DBA) [54] algorithm. The
    barycenter is a sequence for each polygon that reflects a crop’s growing cycle
    from the respective parcel. 𝑏𝑎𝑟𝑦𝑐𝑒𝑛𝑡𝑒𝑟(𝐷)= min 𝜇 ∑ 𝑥𝜖𝐷 𝐷𝑇𝑊 (𝜇,𝑥) 2 (1) Step 3.5
    starts our anomaly detection process by grouping the barycenter time series for
    each crop type. As in [55,62], we expected most crop labels from the same class
    to be correct, and aimed to identify the outliers only. As we had a large variability
    regarding the number of time-series for each category (from a few hundred to ten
    thousand), we chose an autoencoder network instead of a KNN with DTW. We eliminated
    the dates without input for each time series and used interpolation to fill in
    missing values. Step 3.6 corresponds to the actual model training, as we trained
    a LSTM Autoencoder for each category. The architecture of our models is described
    in Table 2 and is based on the LSTM autoencoders from here (LSTM autoencoders:
    https://github.com/shobrook/sequitur, accessed on 4 June 2023). The proposed network
    follows a classic autoencoder structure composed of encoder and decoder parts.
    In our case, the encoder and decoder use two LSTM layers, followed by a fully
    connected layer at the end of the decoder. Table 2. Architecture of the LSTM autoencoder.
    Step 3.7 consists of passing again through the trained autoencoder model in prediction
    mode to record the prediction error. The autoencoder tries to reconstruct the
    input by minimizing the reconstruction loss. We chose the mean squared error (MSE—Equation
    (2)) loss for our model. We saved the value of the MSE for each sequence and used
    a threshold to identify the anomalies based on it. 𝑀𝑆𝐸= 1 𝑀 ∑ 𝑖=1 𝑀 ( 𝑦 𝑖 − 𝑦
    𝑖 ̂ ) 2 (2) Step 3.8 identifies the outliers based on the MSE loss values determined
    in the previous step. We have an array of prediction loss for each crop type label,
    on which we apply a threshold to separate the regular class from the possible
    abnormalities. Even though the threshold might be chosen by a visual analysis
    of the distribution, in our case, we have more than 50 label types for each country.
    Therefore, we proceeded with a dynamic threshold, as in [62], the Otsu thresholding.
    This technology, which was initially developed to convert gray-level photos into
    black-and-white images, enables the separation of a histogram with two spikes.
    It looks for a binary threshold that yields the least intraclass variance when
    the two groups are averaged. We computed the Otsu thresholding for each category
    and eliminated the crop parcel with higher values than the corresponding threshold
    for each class. The Otsu thresholding is defined in Equation (3), where  𝜔 1 (𝑡)  and  𝜔
    2 (𝑡)  is the empirical probability that the loss is equal or below t, respectively
    above. The variance of normal/abnormal values is reflected in  𝜎 2 1 (𝑡)  and  𝜎
    2 2 (𝑡) . find 𝑡 that minimizes  𝜎 2 (𝑡)= 𝜔 1 (𝑡)∗ 𝜎 2 1 (𝑡)+ 𝜔 2 (𝑡)∗ 𝜎 2 2 (𝑡)
    (3) 5.4. Dataset Description The resulting AgriSen-COG is a multiyear, multicountry
    dataset for crop type mapping. It includes 2019 and 2020 data covering the following
    five areas: Austria, Belgium, Catalonia, Denmark, and the Netherlands. We used
    the corresponding LPIS information for each region, distributed under the Open
    Data Commons Attributions Licence. We selected the years 2019 and 2020, summing
    up to 10.2 M parcels (the detailed distribution of polygons is presented in Table
    3). Each original AOI includes a large and varied number of unique labels (Table
    3), mapped to FAO ICC standard, resulting in, at most, 102 common crop classes,
    including the additional Fallow category. The noncrop pixels are marked with the
    background label. In Figure 6, we present a sample for the proposed dataset from
    all six regions, including both years for the same area. As depicted, we highlight
    the spatial variability and temporal changes included in the AgriSen-COG dataset.
    Our GitHub repository provides a more thorough explanation, examples of data loading
    functions, and graphic demonstrations. Additionally, code samples are offered
    to help people write the logic presented in the creation methodology, regarding
    both LPIS processing and data rasterization. Figure 6. Sample patches from the
    proposed dataset (AgriSen-COG): (a) Austria 2019 33UVP; (b) Austria 2020 33UVP;
    (c) Belgium 2019 31UDS; (d) Belgium 2020 31UDS; (e) Catalonia 2019 31TCF; (f)
    Catalonia 2020 31TCF; (g) Denmark 2019 32UNG; (h) Denmark 2020 32UNG; (i) Netherlands
    2019 31UET; (j) Netherlands 2020 31UET. Table 3. LPIS original information for
    2019–2020 used in AgriSen-COG. The proposed dataset contains two subsets created
    to match the two approaches in crop type mapping: pixel-level patch subset (for
    temporal semantic segmentation) and parcel-level aggregated subset (for time-series
    classification). Both subsets contain data from all five regions, covering two
    years (2019, 2020). Therefore, we enable further research focused on a single
    area or studying how models handle different geographical characteristics. We
    followed the methodology mentioned earlier in creating AgriSen-COG. It relies
    on a total of 62 Sentinel-2 tiles. We selected the tiles that intersected with
    the LPIS bounds of each region and retrieved the Sentinel-2 Level-2A tiles with
    less than 30% cloud percentage. Next, we rasterized the LPIS polygons, following
    the bounding boxes of each tile, but we discarded the parcels with less than 0.1
    ha area. After rasterization, we applied our anomaly detection algorithm and identified
    possible anomalous fields. Therefore, we eliminated the corresponding polygons
    by labeling them as the background class. Ultimately, we divided each tile into
    patches so that our data fit the hardware restrictions. The proposed dataset includes
    the 10 m resolution bands only (red, green, blue, and near-infrared), as they
    provide most of the vegetation-related information and they do not require further
    upsampling to use. Therefore, we chose size 366 × 366 for each patch, an integer
    division with the initial tile size for the 10 m resolution bands. The patch size,
    as mentioned earlier, also makes AgriSen-COG compatible with other datasets, such
    as Sen4AgriNet. From each tile, there is a total of 900 resulting patches. However,
    we discarded the patches that did not include any crop-related polygon, summing
    up to 41,100 patches in AgriSen-COG. Table 4 presents a detailed description of
    the eliminated polygons and patches during each stage. Table 4. AgriSen-COG data
    summary. The patches are saved as Zarr arrays in a format offering a self-describing
    design compatible with Xarray. The selected format is also compatible with a distributed
    processor (like Dask) and is the preferred format for cloud-stored data. The ground
    truth (LPIS masks) data files are stored using the COG format and are available
    on a public S3 bucket. This way, we enrich the existing COGs databases and make
    the proposed data easily accessible (no download needed) and findable (STAC catalogues
    indexes). The aggregated data for each field (the barycenters) constitute the
    proposed time-series dataset, and are distributed in Parquet format to ensure
    a smaller size and distributed processing if needed. The five AOIs comprise around
    62 Sentinel-2 tiles, with the patches dataset summing up to 6,972,485 fields,
    for 2019–2020, with 41,100 patches. 6. Experimental Results This section presents
    our experiments performed on the proposed AgriSen-COG dataset, for 2019 and 2020,
    including all six regions. We created the training, validation, and testing datasets
    using label stratification, with a ratio of 60%-20%-20% for each set. The input
    data were aggregated into a monthly median to lower the number of input timesteps.
    The experiments were based on the four 10 m resolution bands (red, green, blue,
    and nir) within a period of 6 months for each year, from month 4 (April) to 9
    (September). Each AOI has a different number of labels, ranging between 60–80
    of other classes. However, to help the training process, we reduced the number
    of crops and chose 11 common categories: wheat, maize, barley, oats, rapeseed,
    potatoes, peas, rye, sunflower, sorghum, and grapes. The distribution of the selected
    classes is depicted in Figure 7) for both of the proposed datasets. We notice
    that wheat, maize and barley are the dominant crop categories. However, there
    is a major difference given by the dataset type. For example, the number of wheat
    and maize field crops in Austria is larger by more than 50k than in Denmark. However,
    when compared to the actual number of pixels from each parcel, Denmark comprises
    more than 30 M pixels for the respective categories. Figure 7. Selected crop distribution
    for the barycenter time-series and pixel-wise patch datasets. Vertical axis is
    in logarithmic scale. (a) Crop time-series distribution for 2019, (b) Crop time-series
    distribution for 2020, (c) Crop pixel distribution for 2019, (d) Crop pixel distribution
    for 2020. Based on the barycenters computed from NDVI in the anomaly detection
    step, we depict (Figure 8) how the barycenter changes over time for each selected
    AOIs, for two representative crops: wheat and maize. We observe similar variations
    in the crops’ growing cycles for each region, even though they are shifted from
    one year to another. We also encounter differences between the AOIs for the same
    culture. Therefore, the proposed dataset incorporates the real-world challenges
    of crop detection: spatial and temporal variation. Figure 8. Barycenters over
    NDVI for different crops. This work aims to train DL models for two crop-related
    tasks: crop type classification and crop type mapping. Performing this shows how
    the proposed dataset helps handle both scenarios while covering crops’ spatial
    and temporal variability. Therefore, we present the following experimental strategies:
    Experiment Type 1 (anomalies variation): We conduct individual experiments on
    each AOI for a single year, with one model, to highlight the importance of curated
    data labels. Experiment Type 2 (temporal generalization): We conduct individual
    experiments on each AOI using the model trained in Experiment Type 1 and predict
    the instances for 2020. Experiment Type 3 (spatial generalization): We conduct
    several experiments, using data from one year and splitting our data based on
    regions’ similarity in crop patterns. Experiment Type 4 (overall generalization):
    We train on two AOIs for 2019, with different models (LSTM, Transformer, TempCNN,
    U-Net, ConvStar) to see the behavior of the proposed dataset. The crop type mapping
    use case was assimilated to a semantic segmentation problem, and therefore, we
    employed the popular U-Net model. In addition to this, we also used a temporal-designed
    model, the ConvStar, for comparison. The crop type classification approach is
    based on aggregated time series at the parcel level. Previous work included mean-aggregated
    time series; however, we also propose, for comparison, the barycenter series computed
    for each crop polygon. This approach was tested with three types of models for
    time series: LSTM, transformer, and TempCNN. We evaluated our experiments using
    a weighted F1 score as we encounter high-class imbalance. Moreover, we included
    several normalized confusion matrices for further visual analysis. 6.1. Crop Type
    Classification Experiments—Time-Series Classification In this paper, we used the
    barycenters computed for each crop parcel for crop type classification. The barycenters
    were previously also used to eliminate possible anomalous polygons. This set of
    experiments serves as a proof-of-concept on using a different metric for aggregation
    crop parcel information instead of the classic mean, median, or standard deviation.
    We resampled our time series in the proposed experiments to obtain a weekly value.
    By carrying this out, we preserved the crop growth information while still obtaining
    a regular interval for our time-series data. However, the experiments presented
    in this paper used information from month 4 (April) to month 9 (September), as
    it captured the growing cycles of the analyzed cultures. LSTM is the first architecture
    used to assess our barycenter-based dataset. LSTMs are a popular choice in dealing
    with time-series data; we used them for most experiments due to their simplicity.
    The proposed network consists of three bidirectional LSTM layers, with one input
    feature and a hidden size of 128. The result is then passed through two fully
    connected layers, with a ReLU function in between. A final Softmax function is
    applied for classification. The model includes 349 k trainable parameters. The
    experiments using LSTM employ cross entropy as a loss function, the Adam optimizer,
    with a starting learning rate of 0.001 and a learning rate scheduler decrease
    of 0.1 every five epochs. Experiment Type 1 (anomalies variation): The first set
    of experiments aims to highlight how curated data labels impact a model’s performance.
    Using the barycenters for each polygon, we removed the influence given by larger
    parcels from a category and focused only on detecting crop growing patterns. We
    conducted ten experiments, two for each included AOI (Austria, Belgium, Catalonia,
    Denmark, Netherlands), for 2019, using the initial time-series data and the curated
    data after the anomaly detection process. The results of the experiments are presented
    in Table 5, showing the score obtained using the initial version of the dataset
    (v0) and the time series after the anomaly detection process (v1). We observe
    that the model trained with the curated dataset outperforms in the overall score
    for each selected AOI, showing the benefit of training with more trustworthy labels.
    Table 5. Results for the crop type classification experiments. The best results
    are in bold for each AOI. Experiment Type 2 (temporal generalization): On the
    time-series data, the second set of experiments aims to show how the dataset scores
    for the same AOI while being trained on the 2019 time series and using the 2020
    data for testing. We used the same LSTM models trained in the previous experiments;
    however, we considered only the curated version of our proposed dataset. Table
    5 reflects the results obtained for each region using our dataset (v1 2020). For
    most of our areas, we observed a considerable decrease in the overall score, which
    shows the variability of the crops in time, even for the same AOI. Corresponding
    confusion matrices (Figure 9a–j) illustrate a detailed view of the eleven labels
    considered. For Austria, we observed a decrease in correctly identifying crops
    for all the labels except maize. However, for Belgium, we noticed consistency
    for the wheat category and a lack of recognition of the maize crop from 2020.
    Sorghum raises challenges for Belgium in both years. Catalonia results highlight
    a significant difference between the 2019 and 2020 crops, with consistency only
    for the maize and grapes categories. The Denmark area provides the closest results
    between the two years, indicating a low shift between the crop-growing cycles.
    Finally, for the Netherlands, we obtained similar results for the wheat and potatoes
    labels, with a score decrease for the maize crop. The results show that the proposed
    dataset incorporates the challenges of the time variability of crop growing cycles,
    making it suitable for further developing stable crop monitoring methods. Figure
    9. Confusion matrixes for Experiment Type 2 (temporal generalization) and results.
    Experiment Type 3 (spatial generalization). LSTM model trained with 2019 time
    series-dataset and tested with both 2019 and 2020 time series: (a) predicted labels
    Austria 2019; (b) predicted labels Belgium 2019; (c) predicted labels Catalonia
    2019; (d) predicted labels Denmark 2019; (e) predicted labels Netherlands 2019;
    (f) predicted labels Austria 2020; (g) predicted labels Belgium 2020; (h) predicted
    labels Catalonia 2020; (i) predicted labels Denmark 2020; (j) predicted labels
    Netherlands 2020; (k) predicted labels Austria 2019 (Type 3); (l) predicted labels
    Belgium 2019 (Type 3); (m) predicted labels Catalonia 2019 (Type 3); (n) predicted
    labels Denmark 2019 (Type 3); (o) predicted labels Netherlands 2019 (Type 3).
    Experiment Type 3 (spatial generalization): The third set of experiments aims
    to show how the proposed time series can capture crop-growing cycle similarities
    between different regions. We only used the 2019 year for training and testing.
    An LSTM model was trained using three AOIs (Austria, Catalonia, and the Netherlands)
    and tested using the five areas from the proposed dataset. The similarity between
    the Austrian and Dutch crops’ time series, as opposed to Catalonia, is reflected
    in the results presented in Table 5 (v1 2019 Type 3 rows). We observed a significant
    decrease in the scores obtained for the Catalan region. A lower score was obtained
    for the two AOIs (Belgium and Denmark). However, they still performed better than
    Catalonia due to their similarity to Austria and Netherlands. From the confusion
    matrixes (Figure 9k–o), we observe that maize and wheat crops are strongly identified
    in most of the regions. In contrast, for Belgium and Denmark, rapeseed and barley
    are classified with lower accuracy. Experiment Type 4 (model behavior): The last
    experiments present how different models perform when trained with the proposed
    dataset. We experimented with two popular time-series models for the proposed
    barycenter data: the transformer model and the TempCNN. We chose two of our AOIs,
    Denmark and Catalonia, as they present different crop markers. We trained the
    models using the 2019 data and predicted using 2019 and 2020. Based on the results
    shown in Table 5 (v1 Type 4), we observed that TempCNN outperforms the LSTM and
    transformer models for both regions. However, when tested on the 2020 data, the
    TempCNN achieved the lowest performance in Catalonia but an increased score for
    Denmark. 6.2. Crop Mapping Experiments—Semantic Segmentation The crop mapping
    experiments present the second use case of the proposed dataset that uses the
    actual parcel’s spatial and temporal information. As in the previous case, we
    only used data from month 4 (April) to month 9 (September). The crop mapping application
    is practical when a crop’s geometry is unknown. Therefore, the task is to discover
    a crop’s type and the parcel’s geometry. As it resembles the problem of semantic
    segmentation, we used the popular U-Net model to test the utility of the proposed
    dataset. Our experiments were based on the four 10 m resolution bands, resulting
    in four input channels. In addition to this, we also incorporated the crop’s temporal
    information, using a monthly median, with the final input data in the shape of
    [T, C, H, W] (T: timesteps, C: channels, H: height, W: width). As U-Net is not
    designed to support the temporal size, we concatenated the first two dimensions,
    achieving an input image shape of [T × C, H, W]. In the latter part of the proposed
    experiment, we also experimented with two models for temporal semantic segmentation:
    U-Net and ConvStar. We created smaller nonoverlapping chips of  61×61  (H × W)
    for faster training from our initial patches of  366×366  pixels. Therefore, our
    input for training was in the shape of  6×4×61×61  (T × C × H × W). Due to the
    high-class imbalance, given not only by the presence of dominating crops (as with
    the time series) but also due to the size of the pixels from specific parcels,
    we use a weighted negative log-likelihood loss. During the training and testing
    stages, we masked all the pixels that did not correspond to the 11 selected classes
    as background. The Adam optimizer was employed for training, starting with a learning
    rate of 0.001, with a decrease strategy applied to validation loss plateaus. Experiment
    Type 1 (anomaly variation): In the first series of experiments, we analyzed how
    curated data labels affect the performance of a model. As in the previous case,
    we conducted ten experiments in 2019, two for each included AOI (Austria, Belgium,
    Catalonia, Denmark, and the Netherlands), using the initial parcel’s dataset and
    the one that was curated. The results of the experiments are shown in Table 6,
    which displays the score derived with the initial version of the dataset (v0)
    and the time series after the anomaly detection process (v1). As opposed to the
    experiments based on time series, the current results are influenced by the number
    of pixels from each parcel. However, we also observe an improvement in the curated
    dataset for each AOI, even for the proposed pixel-based segmentation. Therefore,
    it shows the value of training with more reliable labels. Table 6. Results for
    the crop type mapping experiments. The best results are in bold for each AOI.
    Experiment Type 2 (temporal generalization): The second set of experiments on
    the time-series data seeks to demonstrate how the dataset performs for the same
    AOI when trained on the 2019 time series and tested with the 2020 data. We employed
    the same U-Net models trained in previous experiments, but we only considered
    the curated version of our proposed dataset. Table 6 reflects the results obtained
    for each region using our dataset (v1 2020). Except for the Netherlands, there
    was a decrease in the overall F1 score. Even though we obtained good accuracy
    and precision on the 2020 dataset, we achieved poor recall, as the models struggled
    to identify actual pixels from each category. The previously mentioned tradeoff
    between precision and recall is visible for each crop category for all AOIs. Corresponding
    confusion matrices (Figure 10) illustrate a detailed view of the eleven labels
    considered. By incorporating the pixel-based characteristic for crop mapping,
    we observe that the models can better classify crops from one year to another.
    The findings demonstrate that the proposed dataset addressed the difficulties
    provided by the temporal unpredictability of crop growth cycles, making it appropriate
    for the continuing development of reliable crop monitoring techniques. Figure
    10. Confusion matrixes for Experiment Type 2 (temporal generalization) and results.
    Experiment Type 3 (spatial generalization). U-Net model trained with 2019 time-series
    dataset and tested with both 2019 and 2020 pixel-based patches: (a) predicted
    labels Austria 2019; (b) predicted labels Belgium 2019; (c) predicted labels Catalonia
    2019; (d) predicted labels Denmark 2019; (e) predicted labels Netherlands 2019;
    (f) predicted labels Austria 2020; (g) predicted labels Belgium 2020; (h) predicted
    labels Catalonia 2020; (i) predicted labels Denmark 2020; (j) predicted labels
    Netherlands 2020; (k) predicted labels Austria 2019 (Type 3); (l) predicted labels
    Belgium 2019 (Type 3); (m) predicted labels Catalonia 2019 (Type 3); (n) predicted
    labels Denmark 2019 (Type 3); (o) predicted labels Netherlands 2019 (Type 3).
    Experiment Type 3 (spatial generalization): The third set of experiments exemplifies
    how the proposed pixel-based dataset captures regional variations in crop-growing
    cycles. Similar to the time-series experiments, we only used the year 2019 for
    training and testing. We trained a U-Net model using three AOIs (Austria, Catalonia,
    and the Netherlands) and tested it using the five areas from the proposed dataset.
    As seen in Table 6), the resemblance between the Austrian and Dutch crops is highlighted,
    with an actual improvement in the overall score for the Netherlands. As seen in
    the corresponding confusion matrixes (Figure 10k–o), the Catalonia region obtains
    a lower score. Even though maize, wheat, and barley crops are identified, the
    sorghum class is mislabeled as wheat and barley. Maize and wheat are also classified
    with a good score for Belgium and Denmark; however, barley, oats, and grapes are
    not recognized in either of the two test areas. Experiment Type 4 (model behavior):
    Our final experiments aim to illustrate how various models perform when trained
    using the dataset. Alongside the popular U-Net for semantic segmentation, we also
    experimented with ConvStar, a model created for temporal semantic segmentation.
    We chose two of our AOIs, Denmark and Catalonia, as they present different crop
    markers. We trained the models using the 2019 data and predicted using 2019 and
    2020. Based on the results shown in Table 6 (v1 Type 4), the temporal U-Net still
    obtains a better overall score than the ConvStar network. In Figure 11, we illustrate
    two test patches from Catalonia and Denmark, representing the patch for both 2019
    and 2020. We observe crop variability in the two samples, where the same parcels
    shifted their crops from rye to wheat (Catalonia) and maize to barley (Denmark).
    For Catalonia, the ConvStar model fails to identify the oats’ culture, mislabeling
    it with rye. However, the yearly changes from rye to wheat for the same parcels
    are better captured for the patch by the ConvStar model, as U-Net wrongly classifies
    rye as wheat. Figure 11. Predicted samples for Catalonia and Denmark. U-Net and
    ConvStar models trained with 2019 data. Tested using 2019 and 2020 data: (a) Catalonia
    generated ground truth (GT) 2019; (b) Catalonia U-Net prediction 2019; (c) Catalonia
    ConvStar prediction 2019; (d) Denmark generated ground truth (GT) 2019; (e) Denmark
    U-Net prediction 2019; (f) Denmark ConvStar prediction 2019; (g) Catalonia generated
    ground truth (GT) 2020; (h) Catalonia U-Net prediction 2020; (i) Catalonia ConvStar
    prediction 2020; (j) Denmark generated ground truth (GT) 2020; (k) Denmark U-Net
    prediction 2020; (l) Denmark ConvStar prediction 2020. Both models can identify
    the wheat and maize class for the Denmark patch but struggle with differentiating
    between the sorghum and barley crops. As of 2020, parcels with maize have been
    replaced with barley, and the models encounter the same problem, therefore misclassifying
    barley as sorghum. However, for the 2020 sample, we also observe that the ConvStar
    model struggles with differentiating between the rye and sorghum categories. 7.
    Discussion Agricultural monitoring implies several crop-related tasks: crop type
    classification, crop mapping, parcel extraction, and crop phenology. The availability
    of Sentinel-2 satellite data enables further development in agricultural monitoring
    due to the global coverage, multispectral bands at different resolutions, and
    temporal dimensionality. As official LPIS data from multiple regions became available,
    we created the AgriSen-COG dataset to fill in current demands for DL datasets
    for agricultural monitoring: multiregion, multiyear, trustworthy labels, and methodology
    to enable extension as new data became available. The first challenge in creating
    AgriSen-COG was label harmonization, as each country uses different naming conventions
    for their crops, and they even differ from year to year. As in [14], we followed
    the official FAO crop naming conventions. The second challenge involved the discovery
    of potential anomalous labels, as the LPIS data are created based on farmers’
    declarations. The anomaly detection process required the computation of a time
    series for each crop parcel and identifying the crops that follow a significantly
    different crop-growing pattern. As this process is performed on all the data,
    we used Sentinel-2 AWS COGs (Sentinel-2 AWS COGs repository: https://registry.opendata.aws/sentinel-2/,
    accessed on 4 June 2023), which enabled us to conduct the required computations
    without downloading a large amount of data. The proposed dataset meets two use-case
    scenarios: crop type classification and crop mapping. In the first scenario, we
    offer the time-series dataset computed during the anomaly detection stage. It
    comprises a barycenter time series for each polygon from all five AOIs: Austria,
    Belgium, Catalonia, Denmark, and the Netherlands, for two years: 2019 and 2020.
    The second scenario is met by our pixel-based patches, of size 366 × 366, which
    incorporate the four Sentinel-2 10 m resolution bands and a ground truth patch
    with all the labels. To show the validity of the proposed dataset, we designed
    four types of experiments, which also aim to highlight the potential of further
    research conducted using AgriSen-COG. Experiment Type 1 compared the performance
    of a popular DL model trained using the original dataset and a model trained using
    the curated dataset. We observed improved performance for both scenarios, showing
    that AgriSen-COG is a trustworthy dataset for crop detection. The second experiment
    (Experiment Type 2) illustrated the importance of having a multiyear dataset for
    crops due to the yearly variability. Our results showed a considerable decrease
    in the performance of the models trained for 2019 and tested with 2020 data. Therefore,
    AgriSen-COG is a dataset that could be used to develop better classification methods
    that better incorporate the yearly temporal shift. The spatial generalization
    experiment (Experiment Type 3) revealed the need for conducting targeted research
    for domain adaptation to enable crop classification for a larger number of regions.
    AgriSen-COG incorporates the most different areas (5) as opposed to existing datasets,
    making it a good benchmark for this task. The final Experiment Type 4 showed that
    AgriSen-COG is not only a dataset that works with simple yet popular DL models
    (LSTM and U-Net). Good performance was also obtained with three other types: transformer,
    TempCNN, and ConvStar, showing the potential of the proposed dataset to be used
    in the development of better DL models. 8. Conclusions This paper proposes AgriSen-COG,
    a new crop type mapping dataset that gathers information from five different areas
    (Austria, Belgium, Catalonia, Denmark, and the Netherlands). To this moment, AgriSen-COG
    is the only dataset that includes information from more than two areas, comprising
    the largest number of tiles and crop polygons (61 tiles and almost 7 M fields).
    We also detail the steps taken to create the dataset (together with code for reproducibility)
    and provide several intermediate results. Both output assets are crucial components
    for a consistent future extension with new regions as the LPIS data become available.
    In addition to this, by offering the output both in geopopular formats (Geopackage
    and Geotiff) and compliant distributed storage formats (Parquet and Zarr), we
    support further advancement in several domains: geoanalysis on crop trends, deep
    learning methods for agricultural monitoring, and big data distributed processing
    adapted to georeferenced data. We also distribute the dataset using COGs, enabling
    the data’s usage without prior download. In addition to this, we describe the
    dataset using a STAC catalogue, making it easier to discover and use. AgriSen-COG
    is the only crop-related dataset that offers a DTW—based barycenter time series
    for each parcel, as opposed to mean or median. By undergoing the anomaly detection
    process, AgriSen-COG is also the only crop-related dataset with curated labels
    for five different regions. Based on these characteristics, the proposed dataset
    may be regarded as a benchmark dataset for further development on agricultural
    monitoring, serving various CAP-related use cases: agricultural insurance, early
    warning risk assessment, or crop yield management. Funding This work was primarily
    funded by a grant from the Romanian Ministry of Research and Innovation, CNCS—UEFISCDI,
    project number PN-III-P4-ID-PCE-2016-0842, within PNCDI III, COCO (PN III-P4-ID-PCE-2020-0407)
    and the HARMONIA project, from the EU Horizon 2020 research and innovation programme
    under agreement No. 101003517. This work has partially supported by the European
    Space Agency through the ML4EO Project (contract number 4000125049/18/NL/CBI),
    Romanian UEFISCD FUSE4DL and by project POC/163/1/3/ “Advanced computa-tional
    statistics for planning and monitoring production environments” (2022–2023). Data
    Availability Statement The data presented in this study are openly available in
    Zenodo at https://doi.org/10.5281/zenodo.7892012 (accessed on 4 June 2023).To
    ease data access for large files, we also provide a Dropbox folder https://www.dropbox.com/sh/5bc55skio0o5xd7/AAAQVG3ZmVGFNvPiltQ9Esqma?dl=0
    (accessed on 4 June 2023) and a public Minio S3 bucket, name: agrisen-cog-v1,
    endpoint: https://s3-3.services.tselea.info.uvt.ro (Set anonymous access for download.
    Accessed on 4 June 2023). Upon request, for further development on Alibaba Cloud,
    access to private Alibaba S3 Bucket can also be granted.The code for preprocessing
    and training is available here: https://github.com/tselea/agrisen-cog.git (accessed
    on 4 June 2023). Conflicts of Interest The authors declare no conflict of interest.
    References Ma, L.; Liu, Y.; Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep learning
    in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm.
    Remote Sens. 2019, 152, 166–177. [Google Scholar] [CrossRef] Sustainable Agriculture|Sustainable
    Development Goals|Food and Agriculture Organization of the United Nations. Available
    online: https://www.fao.org/sustainable-development-goals/overview/fao-and-the-2030-agenda-for-sustainable-development/sustainable-agriculture/en/
    (accessed on 20 October 2022). THE 17 GOALS|Sustainable Development. Available
    online: https://sdgs.un.org/goals#icons (accessed on 20 October 2022). Deng, J.;
    Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical
    image database. In Proceedings of the 2009 IEEE Conference on Computer Vision
    and Pattern Recognition, Miami, FL, USA, 20–25 June 2009; pp. 248–255. [Google
    Scholar] Cordts, M.; Omran, M.; Ramos, S.; Scharwächter, T.; Enzweiler, M.; Benenson,
    R.; Franke, U.; Roth, S.; Schiele, B. The cityscapes dataset. In Proceedings of
    the CVPR Workshop on the Future of Datasets in Vision, Boston, MA, USA, 7–12 June
    2015; Volume 2. [Google Scholar] Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.;
    Perona, P.; Ramanan, D.; Dollár, P.; Zitnick, C.L. Microsoft COCO: Common objects
    in context. In Proceedings of the Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, 6–12 September 2014; Springer: Berlin/Heidelberg, Germany,
    2014; pp. 740–755. [Google Scholar] Sumbul, G.; Charfuelan, M.; Demir, B.; Markl,
    V. Bigearthnet: A large-scale benchmark archive for remote sensing image understanding.
    In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote
    Sensing Symposium, Yokohama, Japan, 28 July–2 August 2019; pp. 5901–5904. [Google
    Scholar] Schmitt, M.; Hughes, L.H.; Qiu, C.; Zhu, X.X. SEN12MS–A Curated Dataset
    of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data
    Fusion. arXiv 2019, arXiv:1906.07789. [Google Scholar] [CrossRef] [Green Version]
    European Court of Auditors. The Land Parcel Identification System: A Useful Tool
    to Determine the Eligibility of Agricultural Land—But Its Management Could Be
    Further Improved; Special Report No 25; Publications Office: Luxembourg, 2016.
    [CrossRef] Rußwurm, M.; Pelletier, C.; Zollner, M.; Lefèvre, S.; Körner, M. BreizhCrops:
    A Time Series Dataset for Crop Type Mapping. ISPRS Int. Arch. Photogramm. Remote.
    Sens. Spat. Inf. Sci. 2020, XLIII-B2-2020, 1545–1551. [Google Scholar] [CrossRef]
    Turkoglu, M.O.; D’Aronco, S.; Perich, G.; Liebisch, F.; Streit, C.; Schindler,
    K.; Wegner, J.D. Crop mapping from image time series: Deep learning with multi-scale
    label hierarchies. arXiv 2021, arXiv:2102.08820. [Google Scholar] [CrossRef] Weikmann,
    G.; Paris, C.; Bruzzone, L. TimeSen2Crop: A Million Labeled Samples Dataset of
    Sentinel 2 Image Time Series for Crop-Type Classification. IEEE J. Sel. Top. Appl.
    Earth Obs. Remote Sens. 2021, 14, 4699–4708. [Google Scholar] [CrossRef] Kondmann,
    L.; Toker, A.; Rußwurm, M.; Camero, A.; Peressuti, D.; Milcinski, G.; Mathieu,
    P.P.; Longépé, N.; Davis, T.; Marchisio, G.; et al. DENETHOR: The DynamicEarthNET
    dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring
    from space. In Proceedings of the Thirty-Fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 2), Virtual, 6–14 December
    2021. [Google Scholar] Sykas, D.; Sdraka, M.; Zografakis, D.; Papoutsis, I. A
    Sentinel-2 Multiyear, Multicountry Benchmark Dataset for Crop Classification and
    Segmentation With Deep Learning. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.
    2022, 15, 3323–3339. [Google Scholar] [CrossRef] Food and Agriculture Organization
    of the United Nations. A System of Integrated Agricultural Censuses and Surveys:
    World Programme for the Census of Agriculture 2010; Food and Agriculture Organization
    of the United Nations: Rome, Italy, 2005; Volume 1. [Google Scholar] Hoyer, S.;
    Hamman, J. xarray: N-D labeled arrays and datasets in Python. J. Open Res. Softw.
    2017, 5, 10. [Google Scholar] [CrossRef] [Green Version] Hoyer, S.; Fitzgerald,
    C.; Hamman, J.; Akleeman; Kluyver, T.; Roos, M.; Helmus, J.J.; Markel; Cable,
    P.; Maussion, F.; et al. xarray: V0.8.0. 2016. Available online: https://doi.org/10.5281/zenodo.59499
    (accessed on 4 June 2023). d’Andrimont, R.; Claverie, M.; Kempeneers, P.; Muraro,
    D.; Yordanov, M.; Peressutti, D.; Batič, M.; Waldner, F. AI4Boundaries: An open
    AI-ready dataset to map field boundaries with Sentinel-2 and aerial photography.
    Earth Syst. Sci. Data Discuss. 2022, 15, 317–329. [Google Scholar] [CrossRef]
    Jordan, M.I. Serial Order: A Parallel Distributed Processing Approach. Technical
    Report, June 1985–March 1986. 1986. Available online: https://doi.org/10.1016/S0166-4115(97)80111-2
    (accessed on 4 June 2023). Rumelhart, D.E.; McClelland, J.L. Learning Internal
    Representations by Error Propagation. In Parallel Distributed Processing: Explorations
    in the Microstructure of Cognition: Foundations; U.S. Department of Energy Office
    of Scientific and Technical Information; MIT Press: Cambridge, MA, USA, 1987;
    pp. 318–362. [Google Scholar] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit,
    J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need.
    Adv. Neural Inf. Process. Syst. 2017. Available online: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
    (accessed on 4 June 2023). Ronneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional
    networks for biomedical image segmentation. In Proceedings of the Medical Image
    Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, 5–9 October 2015; pp. 234–241. [Google Scholar] Rakhlin, A.;
    Davydow, A.; Nikolenko, S. Land cover classification from satellite imagery with
    u-net and lovász-softmax loss. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, Salt Lake City, UT, USA, 18–22 June
    2018; pp. 262–266. [Google Scholar] Solórzano, J.V.; Mas, J.F.; Gao, Y.; Gallardo-Cruz,
    J.A. Land use land cover classification with U-net: Advantages of combining sentinel-1
    and sentinel-2 imagery. Remote Sens. 2021, 13, 3600. [Google Scholar] [CrossRef]
    Wang, J.; Yang, M.; Chen, Z.; Lu, J.; Zhang, L. An MLC and U-Net Integrated Method
    for Land Use/Land Cover Change Detection Based on Time Series NDVI-Composed Image
    from PlanetScope Satellite. Water 2022, 14, 3363. [Google Scholar] [CrossRef]
    Zhang, Z.; Iwasaki, A.; Xu, G.; Song, J. Cloud detection on small satellites based
    on lightweight U-net and image compression. J. Appl. Remote Sens. 2019, 13, 026502.
    [Google Scholar] [CrossRef] Guo, Y.; Cao, X.; Liu, B.; Gao, M. Cloud detection
    for satellite imagery using attention-based U-Net convolutional neural network.
    Symmetry 2020, 12, 1056. [Google Scholar] [CrossRef] Xing, D.; Hou, J.; Huang,
    C.; Zhang, W. Spatiotemporal Reconstruction of MODIS Normalized Difference Snow
    Index Products Using U-Net with Partial Convolutions. Remote Sens. 2022, 14, 1795.
    [Google Scholar] [CrossRef] Ivanovsky, L.; Khryashchev, V.; Pavlov, V.; Ostrovskaya,
    A. Building detection on aerial images using U-NET neural networks. In Proceedings
    of the 2019 24th Conference of Open Innovations Association (FRUCT), Moscow, Russia,
    8–12 April 2019; pp. 116–122. [Google Scholar] Irwansyah, E.; Heryadi, Y.; Gunawan,
    A.A.S. Semantic image segmentation for building detection in urban area with aerial
    photograph image using U-Net models. In Proceedings of the 2020 IEEE Asia-Pacific
    Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS), Jakarta,
    Indonesia, 7–8 December 2020; pp. 48–51. [Google Scholar] Wu, C.; Zhang, F.; Xia,
    J.; Xu, Y.; Li, G.; Xie, J.; Du, Z.; Liu, R. Building damage detection using U-Net
    with attention mechanism from pre-and post-disaster remote sensing datasets. Remote
    Sens. 2021, 13, 905. [Google Scholar] [CrossRef] Wei, S.; Zhang, H.; Wang, C.;
    Wang, Y.; Xu, L. Multi-temporal SAR data large-scale crop mapping based on U-Net
    model. Remote Sens. 2019, 11, 68. [Google Scholar] [CrossRef] [Green Version]
    Fan, X.; Yan, C.; Fan, J.; Wang, N. Improved U-Net Remote Sensing Classification
    Algorithm Fusing Attention and Multiscale Features. Remote Sens. 2022, 14, 3591.
    [Google Scholar] [CrossRef] Li, G.; Cui, J.; Han, W.; Zhang, H.; Huang, S.; Chen,
    H.; Ao, J. Crop type mapping using time-series Sentinel-2 imagery and U-Net in
    early growth periods in the Hetao irrigation district in China. Comput. Electron.
    Agric. 2022, 203, 107478. [Google Scholar] [CrossRef] Shi, X.; Chen, Z.; Wang,
    H.; Yeung, D.Y.; Wong, W.K.; Woo, W.C. Convolutional LSTM network: A machine learning
    approach for precipitation nowcasting. Adv. Neural Inf. Process. Syst. 2015, 2015,
    802–810. [Google Scholar] Hochreiter, S.; Schmidhuber, J. Long Short-term Memory.
    Neural Comput. 1997, 9, 1735–1780. [Google Scholar] [CrossRef] Farooque, G.; Xiao,
    L.; Yang, J.; Sargano, A.B. Hyperspectral image classification via a novel spectral–spatial
    3D ConvLSTM-CNN. Remote Sens. 2021, 13, 4348. [Google Scholar] [CrossRef] Cherif,
    E.; Hell, M.; Brandmeier, M. DeepForest: Novel Deep Learning Models for Land Use
    and Land Cover Classification Using Multi-Temporal and-Modal Sentinel Data of
    the Amazon Basin. Remote Sens. 2022, 14, 5000. [Google Scholar] [CrossRef] Meng,
    X.; Liu, Q.; Shao, F.; Li, S. Spatio–Temporal–Spectral Collaborative Learning
    for Spatio–Temporal Fusion with Land Cover Changes. IEEE Trans. Geosci. Remote
    Sens. 2022, 60, 5704116. [Google Scholar] [CrossRef] Habiboullah, A.; Louly, M.A.
    Soil Moisture Prediction Using NDVI and NSMI Satellite Data: ViT-Based Models
    and ConvLSTM-Based Model. SN Comput. Sci. 2023, 4, 140. [Google Scholar] [CrossRef]
    Park, S.; Im, J.; Han, D.; Rhee, J. Short-term forecasting of satellite-based
    drought indices using their temporal patterns and numerical model output. Remote
    Sens. 2020, 12, 3499. [Google Scholar] [CrossRef] Yeom, J.M.; Deo, R.C.; Adamowski,
    J.F.; Park, S.; Lee, C.S. Spatial mapping of short-term solar radiation prediction
    incorporating geostationary satellite images coupled with deep convolutional LSTM
    networks for South Korea. Environ. Res. Lett. 2020, 15, 094025. [Google Scholar]
    [CrossRef] Muthukumar, P.; Cocom, E.; Nagrecha, K.; Comer, D.; Burga, I.; Taub,
    J.; Calvert, C.F.; Holm, J.; Pourhomayoun, M. Predicting PM2. 5 atmospheric air
    pollution using deep learning with meteorological data and ground-based observations
    and remote-sensing satellite big data. Air Qual. Atmos. Health 2021, 15, 1221–1234.
    [Google Scholar] [CrossRef] Yaramasu, R.; Bandaru, V.; Pnvr, K. Pre-season crop
    type mapping using deep neural networks. Comput. Electron. Agric. 2020, 176, 105664.
    [Google Scholar] [CrossRef] Chang, Y.L.; Tan, T.H.; Chen, T.H.; Chuah, J.H.; Chang,
    L.; Wu, M.C.; Tatini, N.B.; Ma, S.C.; Alkhaleefah, M. Spatial-temporal neural
    network for rice field classification from SAR images. Remote Sens. 2022, 14,
    1929. [Google Scholar] [CrossRef] Ienco, D.; Interdonato, R.; Gaetano, R.; Minh,
    D.H.T. Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land
    cover mapping via a multi-source deep learning architecture. ISPRS J. Photogramm.
    Remote Sens. 2019, 158, 11–22. [Google Scholar] [CrossRef] Turkoglu, M.O.; D’Aronco,
    S.; Wegner, J.D.; Schindler, K. Gating revisited: Deep multi-layer rnns that can
    be trained. arXiv 2019, arXiv:1911.11033. [Google Scholar] [CrossRef] [PubMed]
    Pelletier, C.; Webb, G.I.; Petitjean, F. Temporal convolutional neural network
    for the classification of satellite image time series. Remote Sens. 2019, 11,
    523. [Google Scholar] [CrossRef] [Green Version] Mitra, P.; Akhiyarov, D.; Araya-Polo,
    M.; Byrd, D. Machine Learning-based Anomaly Detection with Magnetic Data. Preprints.org
    2020. [Google Scholar] [CrossRef] Sontowski, S.; Lawrence, N.; Deka, D.; Gupta,
    M. Detecting Anomalies using Overlapping Electrical Measurements in Smart Power
    Grids. In Proceedings of the 2021 IEEE International Conference on Big Data (Big
    Data), Orlando, FL, USA, 15–18 December 2021; pp. 2434–2441. [Google Scholar]
    Wagner, N.; Antoine, V.; Koko, J.; Mialon, M.M.; Lardy, R.; Veissier, I. Comparison
    of machine learning methods to detect anomalies in the activity of dairy cows.
    In Proceedings of the International Symposium on Methodologies for Intelligent
    Systems, Graz, Austria, 23–25 September 2020; pp. 342–351. [Google Scholar] Cover,
    T.; Hart, P. Nearest neighbor pattern classification. IEEE Trans. Inf. Theory
    1967, 13, 21–27. [Google Scholar] [CrossRef] [Green Version] Ballard, D.H. Modular
    learning in neural networks. In Proceedings of the AAAI, Seattle, WA, USA, 13
    July 1987; Volume 647, pp. 279–284. [Google Scholar] Petitjean, F.; Ketterlin,
    A.; Gançarski, P. A global averaging method for dynamic time warping, with applications
    to clustering. Pattern Recognit. 2011, 44, 678–693. [Google Scholar] [CrossRef]
    Avolio, C.; Tricomi, A.; Zavagli, M.; De Vendictis, L.; Volpe, F.; Costantini,
    M. Automatic Detection of Anomalous Time Trends from Satellite Image Series to
    Support Agricultural Monitoring. In Proceedings of the 2021 IEEE International
    Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium, 11–16 July
    2021; pp. 6524–6527. [Google Scholar] Huang, S.; Tang, L.; Hupy, J.P.; Wang, Y.;
    Shao, G. A commentary review on the use of normalized difference vegetation index
    (NDVI) in the era of popular remote sensing. J. For. Res. 2021, 32, 1–6. [Google
    Scholar] [CrossRef] Castillo-Villamor, L.; Hardy, A.; Bunting, P.; Llanos-Peralta,
    W.; Zamora, M.; Rodriguez, Y.; Gomez-Latorre, D.A. The Earth Observation-based
    Anomaly Detection (EOAD) system: A simple, scalable approach to mapping in-field
    and farm-scale anomalies using widely available satellite imagery. Int. J. Appl.
    Earth Obs. Geoinf. 2021, 104, 102535. [Google Scholar] [CrossRef] Komisarenko,
    V.; Voormansik, K.; Elshawi, R.; Sakr, S. Exploiting time series of Sentinel-1
    and Sentinel-2 to detect grassland mowing events using deep learning with reject
    region. Sci. Rep. 2022, 12, 983. [Google Scholar] [CrossRef] [PubMed] Cheng, W.;
    Ma, T.; Wang, X.; Wang, G. Anomaly Detection for Internet of Things Time Series
    Data Using Generative Adversarial Networks With Attention Mechanism in Smart Agriculture.
    Front. Plant Sci. 2022, 13, 890563. [Google Scholar] [CrossRef] Cui, L.; Zhang,
    Q.; Shi, Y.; Yang, L.; Wang, Y.; Wang, J.; Bai, C. A method for satellite time
    series anomaly detection based on fast-DTW and improved-KNN. Chin. J. Aeronaut.
    2022, 36, 149–159. [Google Scholar] [CrossRef] Diab, D.M.; AsSadhan, B.; Binsalleeh,
    H.; Lambotharan, S.; Kyriakopoulos, K.G.; Ghafir, I. Anomaly detection using dynamic
    time warping. In Proceedings of the 2019 IEEE International Conference on Computational
    Science and Engineering (CSE) and IEEE International Conference on Embedded and
    Ubiquitous Computing (EUC), New York, NY, USA, 1–3 August 2019; pp. 193–198. [Google
    Scholar] Di Martino, T.; Guinvarc’h, R.; Thirion-Lefevre, L.; Colin, E. FARMSAR:
    Fixing AgRicultural Mislabels Using Sentinel-1 Time Series and AutoencodeRs. Remote
    Sens. 2022, 15, 35. [Google Scholar] [CrossRef] Otsu, N. A threshold selection
    method from gray-level histograms. IEEE Trans. Syst. Man, Cybern. 1979, 9, 62–66.
    [Google Scholar] [CrossRef] [Green Version] Drusch, M.; Del Bello, U.; Carlier,
    S.; Colin, O.; Fernandez, V.; Gascon, F.; Hoersch, B.; Isola, C.; Laberinti, P.;
    Martimort, P.; et al. Sentinel-2: ESA’s optical high-resolution mission for GMES
    operational services. Remote Sens. Environ. 2012, 120, 25–36. [Google Scholar]
    [CrossRef] PaperdJuly, W. ESRI shapefile technical description. Comput. Stat.
    1998, 16, 370–371. [Google Scholar] Yutzler, J. OGC® GeoPackage Encoding Standard-with
    Corrigendum, Version 1.2. 175. 2018. Available online: https://www.geopackage.org/spec121/
    (accessed on 4 June 2023). Zeiler, M. Modeling Our World: The ESRI Guide to Geodatabase
    Design; ESRI, Inc.: Redlands, CA, USA, 1999; Volume 40. [Google Scholar] Butler,
    H.; Daly, M.; Doyle, A.; Gillies, S.; Hagen, S.; Schaub, T. The Geojson Format.
    Technical Report. 2016. Available online: https://www.rfc-editor.org/rfc/rfc7946
    (accessed on 4 June 2023). Moyroud, N.; Portet, F. Introduction to QGIS. QGIS
    Generic Tools 2018, 1, 1–17. [Google Scholar] Vohra, D. Apache parquet. In Practical
    Hadoop Ecosystem: A Definitive Guide to Hadoop-Related Frameworks and Tools; Apress:
    Berkley, CA, USA, 2016; pp. 325–335. [Google Scholar] Trakas, A.; McKee, L. OGC
    standards and the space community—Processes, application and value. In Proceedings
    of the 2011 2nd International Conference on Space Technology, Athens, Greece,
    15–17 September 2011; pp. 1–5. [Google Scholar] [CrossRef] Durbin, C.; Quinn,
    P.; Shum, D. Task 51-Cloud-Optimized Format Study; Technical Report; NTRS: Chicago,
    IL, USA, 2020. [Google Scholar] Sanchez, A.H.; Picoli, M.C.A.; Camara, G.; Andrade,
    P.R.; Chaves, M.E.D.; Lechler, S.; Soares, A.R.; Marujo, R.F.; Simões, R.E.O.;
    Ferreira, K.R.; et al. Comparison of Cloud cover detection algorithms on sentinel–2
    images of the amazon tropical forest. Remote Sens. 2020, 12, 1284. [Google Scholar]
    [CrossRef] [Green Version] AgrarMarkt Austria InVeKoS Strikes Austria. Available
    online: https://www.data.gv.at/ (accessed on 9 February 2023). Department of Agriculture
    and Fisheries Flemish Government. Available online: https://data.gov.be/en (accessed
    on 9 February 2023). Government of Catalonia Department of Agriculture Livestock
    Fisheries and Food. Available online: https://analisi.transparenciacatalunya.cat
    (accessed on 9 February 2023). The Danish Agency for Agriculture. Available online:
    https://lbst.dk/landbrug/ (accessed on 9 February 2023). Netherlands Enterprise
    Agency. Available online: https://nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/home
    (accessed on 9 February 2023). Disclaimer/Publisher’s Note: The statements, opinions
    and data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the author. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Selea, T. AgriSen-COG, a Multicountry, Multitemporal Large-Scale
    Sentinel-2 Benchmark Dataset for Crop Mapping Using Deep Learning. Remote Sens.
    2023, 15, 2980. https://doi.org/10.3390/rs15122980 AMA Style Selea T. AgriSen-COG,
    a Multicountry, Multitemporal Large-Scale Sentinel-2 Benchmark Dataset for Crop
    Mapping Using Deep Learning. Remote Sensing. 2023; 15(12):2980. https://doi.org/10.3390/rs15122980
    Chicago/Turabian Style Selea, Teodora. 2023. \"AgriSen-COG, a Multicountry, Multitemporal
    Large-Scale Sentinel-2 Benchmark Dataset for Crop Mapping Using Deep Learning\"
    Remote Sensing 15, no. 12: 2980. https://doi.org/10.3390/rs15122980 Note that
    from the first issue of 2016, this journal uses article numbers instead of page
    numbers. See further details here. Article Metrics Citations Scopus   2 Crossref   2
    Google Scholar   [click to view] Article Access Statistics Article access statistics
    Article Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28.
    Mar 0 500 1000 1500 2000 For more information on the journal statistics, click
    here. Multiple requests from the same IP address are counted as one view.   Remote
    Sens., EISSN 2072-4292, Published by MDPI RSS Content Alert Further Information
    Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs
    at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: AgriSen-COG, a Multicountry, Multitemporal Large-Scale Sentinel-2 Benchmark
    Dataset for Crop Mapping Using Deep Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kethineni K.
  - Pradeepini G.
  citation_count: '0'
  description: Plant disease detection is a critical component of crop monitoring
    systems. Computer vision and machine learning (ML) approaches have been seen to
    be cutting-edge in addressing a variety of agricultural issues. After disease
    detection, IOT plays a critical role in protecting agriculture in the absence
    of farmers. In various modernization domains Internet of Things (IoT) is a most
    impressive technology that offers effective and dependable approaches. With the
    help of IOT approaches the involvement of humans are very less due to the automatic
    manage and tracking of farms with the built solutions in IoT. The article covers
    a wide range of innovations related to IoT in farming conditions. The major components
    involved in smart farming in IOT are described. Cloud computing plays a key role
    in future of IOT agricultural due to the heterogeneous and massive amount of data
    gathered by IoT devices. Simultaneously, microcontrollers can expand the capabilities
    of the internet of things (IoT). This survey examines the patterns in research,
    principles, fundamental components of IOT, problems, and application of IOT in
    agriculture. Finally, the discussion will conclude with IOT security questions
    and threats. Along with IOT, machine learning techniques aid in the identification
    of crop diseases, allowing farmers to identify crop production and take appropriate
    action. These developments in harvest forecasting research have been made possible
    in large part by advancements in information technology.
  doi: 10.1063/5.0125643
  full_citation: '>'
  full_text: '>

    "All Content AIP Publishing Portfolio AIP Conference Proceedings                              Advanced
    Search | Citation Search Univ Nebraska Lincoln Lib Sign In HOME BROWSE FOR AUTHORS
    FOR ORGANIZERS ABOUT Volume 2477, Issue 1 24 May 2023 INTERNATIONAL CONFERENCE
    ON ADVANCES IN SIGNAL PROCESSING COMMUNICATIONS AND COMPUTATIONAL INTELLIGENCE
    23–24 July 2021 Hyderabad, India REFERENCES RESEARCH ARTICLE| MAY 24 2023 An overview
    of smart agriculture activities using machine learning and IoT Keerthi Kethineni;
    G. Pradeepini Author & Article Information AIP Conf. Proc. 2477, 030033 (2023)
    https://doi.org/10.1063/5.0125643 Split-Screen PDF Share Tools Plant disease detection
    is a critical component of crop monitoring systems. Computer vision and machine
    learning (ML) approaches have been seen to be cutting-edge in addressing a variety
    of agricultural issues. After disease detection, IOT plays a critical role in
    protecting agriculture in the absence of farmers. In various modernization domains
    Internet of Things (IoT) is a most impressive technology that offers effective
    and dependable approaches. With the help of IOT approaches the involvement of
    humans are very less due to the automatic manage and tracking of farms with the
    built solutions in IoT. The article covers a wide range of innovations related
    to IoT in farming conditions. The major components involved in smart farming in
    IOT are described. Cloud computing plays a key role in future of IOT agricultural
    due to the heterogeneous and massive amount of data gathered by IoT devices. Simultaneously,
    microcontrollers can expand the capabilities of the internet of things (IoT).
    This survey examines the patterns in research, principles, fundamental components
    of IOT, problems, and application of IOT in agriculture. Finally, the discussion
    will conclude with IOT security questions and threats. Along with IOT, machine
    learning techniques aid in the identification of crop diseases, allowing farmers
    to identify crop production and take appropriate action. These developments in
    harvest forecasting research have been made possible in large part by advancements
    in information technology. Topics Microcontroller, Crop production, Information
    technology, Internet of things, Artificial intelligence, Machine learning, Cloud
    computing REFERENCES 1.Sankaran, S.; Mishra, A.; Ehsani, R.; Davis, C. A review
    of advanced techniques for detecting plant diseases. Comput. Electron. Agric.
    2010, 72, 1–13. https://doi.org/10.1016/j.compag.2010.02.007 Google ScholarCrossref   2.Duro,
    D.C.; Franklin, S.E.; Dubé, M.G. A comparison of pixel-based and object-based
    image analysis with selected machine learning algorithms for the classification
    of agricultural landscapes using SPOT-5 HRG imagery. Remote Sens. Environ. 2012,
    118, 259–272. https://doi.org/10.1016/j.rse.2011.11.020 Google ScholarCrossref   3.Esteva,
    A.; Robicquet, A.; Ramsundar, B.; Kuleshov, V.; DePristo, M.; Chou, K.; Cui, C.;
    Corrado, G.; Thrun, S.; Dean, J. A guide to deep learning in healthcare. Nat.
    Med. 2019, 25, 24–29. https://doi.org/10.1038/s41591-018-0316-z Google ScholarCrossref
    PubMed  4.Adhikari, S.P.; Yang, H.; Kim, H. Learning semantic graphics using convolutional
    encoder-decoder network for autonomous weeding in paddy field. Front. Plant Sci.
    2019, 10, 1404. https://doi.org/10.3389/fpls.2019.01404 Google ScholarCrossref
    PubMed  5.Marani, R.; Milella, A.; Petitti, A.; Reina, G. Deep neural networks
    for grape bunch segmentation in natural images from a consumer-grade camera. Precis.
    Agric. 2020, 1–27. Google Scholar  6.Ampatzidis, Y.; Partel, V. UAV-based high
    throughput phenotyping in citrus utilizing multispectral imaging and artificial
    intelligence. Remote Sens. 2019, 11, 410. https://doi.org/10.3390/rs11040410 Google
    ScholarCrossref   7.Saleem, M.H.; Potgieter, J.; Arif, K.M. Plant disease detection
    and classification by deep learning. Plants 2019, 8, 468. https://doi.org/10.3390/plants8110468
    Google ScholarCrossref PubMed  8.Lee, I., & Lee, K. (2015). The Internet of Things
    (IoT): Applications, investments, and challenges for enterprises. Business Horizons,
    58(4), 431–440. https://doi.org/10.1016/j.bushor.2015.03.008 Google ScholarCrossref   9.Chen,
    S., Xu, H., Liu, D., Hu, B., & Wang, H. (2014). A vision of IoT: Applications,
    challenges, and opportunities with china perspective. IEEE Internet of Things
    journal, 1(4), 349–359. https://doi.org/10.1109/JIOT.2014.2337336 Google ScholarCrossref   10.Stočes,
    M., Vaněk, J., Masner, J., & Pavlík, J. (2016). Internet of things (iot) in agriculture-selected
    aspects. Agris on-line Papers in Economics and Informatics, 8(665-2016-45107),
    83–88. Google Scholar  11.Ray, P. P. (2017). Internet of things for smart agriculture:
    Technologies, practices and future direction. Journal of Ambient Intelligence
    and Smart Environments, 9(4), 395–420. https://doi.org/10.3233/AIS-170440 Google
    ScholarCrossref   12.Kamienski, C., Soininen, J. P., Taumberger, M., Dantas, R.,
    Toscano, A., Salmon Cinotti, T. & Torre Neto, A. (2019). Smart water management
    platform: Iot-based precision irrigation for agriculture. Sensors, 19(2), 276.
    https://doi.org/10.3390/s19020276 Google ScholarCrossref PubMed  13.Ojha, T.,
    Misra, S., & Raghuwanshi, N. S. (2015). Wireless sensor networks for agriculture:
    The state-of-the-art in practice and future challenges. Computers and Electronics
    in Agriculture, 118, 66–84. https://doi.org/10.1016/j.compag.2015.08.011 Google
    ScholarCrossref   14.Zhang, X., Zhang, J., Li, L., Zhang, Y., & Yang, G. (2017).
    Monitoring citrus soil moisture and nutrients using an iot based system. Sensors,
    17(3), 447. https://doi.org/10.3390/s17030447 Google ScholarCrossref PubMed  15.Jayaraman,
    P., Yavari, A., Georgakopoulos, D., Morshed, A., & Zaslavsky, A. (2016). Internet
    of things platform for smart farming: Experiences and lessons learnt. Sensors,
    16(11), 1884. https://doi.org/10.3390/s16111884 Google ScholarCrossref PubMed  16.Köksal,
    Ö., & Tekinerdogan, B. (2018). Architecture design approach for IoT-based farm
    management information systems. Precision Agriculture, 1–33. Google Scholar  17.Shabadi,
    L. S., & Biradar, H. B. Design and Implementation of IOT based Smart Security
    and Monitoring for Connected Smart Farming. International Journal of Computer
    Applications, 975, 8887. 18.Minerva, R., Biru, A., & Rotondi, D. (2015). Towards
    a definition of the Internet of Things (IoT). IEEE Internet Initiative (1). Google
    Scholar  19.Chen, X.-Y., & Jin, Z.-G. (2012). Research on key technology andapplications
    for internet of things. Physics Procedia, 33, 561–566. https://doi.org/10.1016/j.phpro.2012.05.104
    Google ScholarCrossref   20.Koshizuka, N., & Sakamura, K. (2010). Ubiquitous ID:
    standards for ubiquitous computing and the Internet of Things. IEEE Pervasive
    Computing, 9(4), 98–101. https://doi.org/10.1109/MPRV.2010.87 Google ScholarCrossref   21.Kushalnagar,
    N., Montenegro, G., & Schumacher, C. (2007). IPv6 over low-power wireless personal
    area networks (6LoWPANs): overview, assumptions, problem statement, and goals.
    Google Scholar  22.Khorov, E., Lyakhov, A., Krotov, A., & Guschin, A. (2015).
    A survey on IEEE 802.11 ah: An enabling networking technology for smart cities.
    Computer Communications, 58, 53–69. https://doi.org/10.1016/j.comcom.2014.08.008
    Google ScholarCrossref   23.Kumar, J. S., & Patel, D. R. (2014). A survey on internet
    of things: Security and privacy issues. International Journal of Computer Applications,
    90(11). Google Scholar  24.Botta, A., De Donato, W., Persico, V., & Pescapé, A.
    (2016). Integration of cloud computing and internet of things: a survey. Future
    generation computer systems, 56, 684–700. https://doi.org/10.1016/j.future.2015.09.021
    Google Scholar  25.Pavón-Pulido, N., López-Riquelme, J. A., Torres, R., Morais,
    R., & Pastor, J. A. (2017). New trends in precision agriculture: a novel cloud-based
    system for enabling data storage and agricultural task planning and automation.
    Precision agriculture, 18(6), 1038–1068. https://doi.org/10.1007/s11119-017-9532-7
    Google ScholarCrossref   26.Zamora-Izquierdo, M. A., Santa, J., Martínez, J. A.,
    Martínez, V., & Skarmeta, A. F. (2019). Smart farming IoT platform based on edge
    and cloud computing. Biosystems engineering, 177, 4–17. https://doi.org/10.1016/j.biosystemseng.2018.10.014
    Google ScholarCrossref   27.Gill, S. S., Chana, I., & Buyya, R. (2017). IoT based
    agriculture as a cloud and big data service: the beginning of digital India. Journal
    of Organizational and End User Computing (JOEUC), 29(4), 1–23. https://doi.org/10.4018/JOEUC.2017100101
    Google ScholarCrossref   28.Kamilaris, A., Kartakoullis, A., & Prenafeta-Boldú,
    F. X. (2017). A review on the practice of big data analysis in agriculture. Computers
    and Electronics in Agriculture, 143, 23–37. https://doi.org/10.1016/j.compag.2017.09.037
    Google ScholarCrossref   29.Liu, X., Zhang, C., Liu, P., Yan, M., Wang, B., Zhang,
    J., & Higgs, R. (2018). Application of Temperature Prediction Based on Neural
    Network in Intrusion Detection of IoT. Security and Communication Networks, 2018.
    Google Scholar  30.Mehra, M., Saxena, S., Sankaranarayanan, S., Tom, R. J., &
    Veeramanikandan, M. (2018). IoT based hydroponics system using Deep Neural Networks.
    Computers and electronics in agriculture, 155, 473–486. https://doi.org/10.1016/j.compag.2018.10.015
    Google ScholarCrossref   31.Navulur, S., & Prasad, M. G. (2017). Agricultural
    management through wireless sensors and internet of things. International Journal
    of Electrical and Computer Engineering, 7(6), 3492. Google Scholar  32.Al-Sarawi,
    S., Anbar, M., Alieyan, K., & Alzubaidi, M. (2017, May). Internet of Things (IoT)
    communication protocols. In 2017 8th International conference on information technology
    (ICIT) (pp. 685–690). IEEE. Google ScholarCrossref   33.CHENG, X. L., & DENG,
    Z. D. (2008). Construction of large-scale wireless sensor network using ZigBee
    specification [J]. Journal on Communications, 11. Google Scholar  34.Brandt, P.,
    Kvakić, M., Butterbach-Bahl, K., & Rufino, M. C. (2017). How to target climate-smart
    agriculture? Concept and application of the consensus-driven decision support
    framework “targetCSA”. Agricultural Systems, 151, 234–245. https://doi.org/10.1016/j.agsy.2015.12.011
    Google ScholarCrossref   35.Talavera, J. M., Tobón, L. E., Gómez, J. A., Culman,
    M. A., Aranda, J. M., Parra, D. T., … & Garreta, L. E. (2017). Review of IoT applications
    in agro-industrial and environmental fields. Computers and Electronics in Agriculture,
    142, 283–297. https://doi.org/10.1016/j.compag.2017.09.015 Google ScholarCrossref   36.de
    Morais, C. M., Sadok, D., & Kelner, J. (2019). An IoT sensor and scenario survey
    for data researchers. Journal of the Brazilian Computer Society, 25(1), 4. https://doi.org/10.1186/s13173-019-0085-7
    Google ScholarCrossref   37.Zhang, X., Zhang, J., Li, L., Zhang, Y., & Yang, G.
    (2017). Monitoring citrus soil moisture and nutrients using an iot based system.
    Sensors, 17(3), 447. https://doi.org/10.3390/s17030447 Google ScholarCrossref
    PubMed  38.Bodake, K., Ghate, R., Doshi, H., Jadhav, P., & Recommendation System
    using Internet of Things. MVP Tarle, B. (2018). Soil based Fertilizer Journal
    of Engineering Sciences, 1(1), 13–19. Google Scholar  39.Zhang, S., Chen, X.,
    & Wang, S. (2014, August). Research on the monitoring system of wheat diseases,
    pests and weeds based on IOT. In 2014 9th International Conference on Computer
    Science & Education (pp. 981–985). IEEE. Google ScholarCrossref   40.Windsperger,
    B., Windsperger, A., Bird, D. N., Schwaiger, H., Jungmeier, G., Nathani, C., &
    Frischknecht, R. (2019). Greenhouse gas emissions due to national product consumption:
    from demand and research gaps to addressing key challenges. International journal
    of environmental science and technology, 16(2), 1025–1038. https://doi.org/10.1007/s13762-018-1743-6
    Google ScholarCrossref   41.Shirsath, D. O., Kamble, P., Mane, R., Kolap, A.,
    & More, R. S. (2017). IoT based smart greenhouse automation using Arduino. International
    Journal of Innovative Research in Computer Science & Technology, 5(2), 234–8.
    Google ScholarCrossref   42.“U.S. Sugar and AgriSource Data Leverage Ingenu’s
    Machine Network to Deliver Innovative Smart Agriculture Solution.” [Online]. Available:
    https://www.ingenu.com/2016/12/us-sugar-and-agrisource-data-leverage-ingenu-machine-network-to-deliver-innovative-smart-agriculture-solution.
    43.Asikainen, M., Haataja, K., & Toivanen, P. (2013, July). Wireless indoor tracking
    of livestock for behavioral analysis. In 2013 9th International Wireless Communications
    and Mobile Computing Conference (IWCMC) (pp. 1833–1838). IEEE. Google ScholarCrossref   44.Ojha,
    T., Misra, S., & Raghuwanshi, N. S. (2015). Wireless sensor networks for agriculture:
    The state-of-the-art in practice and future challenges. Computers and Electronics
    in Agriculture, 118, 66–84. https://doi.org/10.1016/j.compag.2015.08.011 Google
    ScholarCrossref   45.Jawad, H., Nordin, R., Gharghan, S., Jawad, A., & Ismail,
    M. (2017). Energy-efficient wireless sensor networks for precision agriculture:
    A review. Sensors, 17(8), 1781. https://doi.org/10.3390/s17081781 Google ScholarCrossref
    PubMed  46.Elijah, O., Rahman, T. A., Orikumhi, I., Leow, C. Y., & Hindia, M.
    N. (2018). An overview of Internet of Things (IoT) and data analytics in agriculture:
    Benefits and challenges. IEEE Internet of Things Journal, 5(5), 3758–3773. https://doi.org/10.1109/JIOT.2018.2844296
    Google ScholarCrossref   47.Reddy, A.V.N., Krishna, C.P. & Mallick, P.K. An image
    classification framework exploring the capabilities of extreme learning machines
    and artificial bee colony. Neural Comput & Applic 32, 3079–3099 (2020). https://doi.org/10.1007/s00521-019-04385-5
    Google ScholarCrossref   48.Murugan R., devi R.K., Albert A.J., Nayak D.K. (2020)
    An IOT Based Weather Monitoring System to Prevent and Alert Cauvery Delta District
    of Tamilnadu, India. In: Pandian A., Senjyu T., Islam S., Wang H. (eds) Proceeding
    of the International Conference on Computer Networks, Big Data and IoT (ICCBI
    - 2018). ICCBI 2018. Lecture Notes on Data Engineering and Communications Technologies,
    vol 31. Springer, Cham. Google ScholarCrossref   49.Bhimanpallewar, Ratnmala &
    rama narasingarao, Manda. (2020). Alternative approaches of Machine Learning for
    Agriculture Advisory System. 27–31. https://doi.org/10.1109/Confluence47617.2020.9058152.
    Google Scholar  50.lalitha bhavani, B & krishnaveni, G & Malathi, J. (2019). A
    comparative performance analysis of different machine learning techniques. Journal
    of Physics: Conference Series. 1228. 012035. https://doi.org/10.1088/1742-6596/1228/1/012035.
    Google ScholarCrossref   51.Lakshmi Mallika I., D. Venkata Ratnam, Saravana Raman,
    G. Sivavaraprasad, Machine learning algorithm to forecast ionospheric time delays
    using Global Navigation satellite system observations, Acta Astronautica, Volume
    173, 2020, Pages 221–231,ISSN 0094-5765, https://doi.org/10.1016/j.actaastro.2020.04.048
    Google ScholarCrossref   52.Dabbakuti, J. R. K. Kumar & Ch, Bhupati. (2019). Ionospheric
    monitoring system based on the Internet of Things with ThingSpeak. Astrophysics
    and Space Science. 364. https://doi.org/10.1007/s10509-019-3630-0 Google Scholar  This
    content is only available via PDF. PDF ©2023 Authors. Published by AIP Publishing.
    View Metrics Citing Articles Via Google Scholar CrossRef (1) Publish with us -
    Request a Quote! Sign up for alerts Most Read Most Cited Phytochemical analysis
    of bioactive compounds in ethanolic extract of Sterculia quadrifida R.Br. Siswadi
    Siswadi, Grace Serepina Saragih Impact of blockchain technology development on
    industries in the context of entrepreneurial, marketing and management perspectives
    worldwide Ivelina Kulova Design of a 100 MW solar power plant on wetland in Bangladesh
    Apu Kowsar, Sumon Chandra Debnath, et al. Online ISSN 1551-7616 Print ISSN 0094-243X
    Resources For Researchers For Librarians For Advertisers Our Publishing Partners  Explore
    Journals Physics Today Conference Proceedings Books Special Topics Publishers
    pubs.aip.org About User Guide Contact Us Register Help Privacy Policy Terms of
    Use Connect with AIP Publishing Facebook LinkedIn Twitter YouTube © Copyright
    2024 AIP Publishing LLC"'
  inline_citation: '>'
  journal: AIP Conference Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Overview of Smart Agriculture Activities Using Machine Learning and IoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sharma S.
  - Verma K.
  - Hardaha P.
  citation_count: '27'
  description: Agriculture has a significant contribution to the economy. Agricultural
    automation is a major cause of concern and a relatively new phenomenon throughout
    the world. The world’s population is quickly growing, resulting in increased demand
    for food and labor. Farmers’ customary techniques were insufficient to achieve
    these goals. As a result, new automated techniques were developed. These creative
    initiatives met food demands while also providing work opportunities for a large
    number of people. Agriculture has changed as a result of artificial intelligence
    (AI). This strategy has shielded agricultural production from a variety of threats
    such as weather, population growth, labor rights, and food security concerns.
    The major issue of this is the numerous applications of AI in agriculture, such
    as irrigation, weeding, and spraying with different sensors or other ways implanted
    in robots and drones. These technologies limit the use of water, pesticides, and
    herbicides, preserve soil fertility, and help in the effective use of labor, resulting
    in increased output and quality. Many researchers make efforts to gain a quick
    overview of the present state of automation in agriculture, including weeding
    systems using robots and drones. Two automated weeding strategies as well as several
    soil water sensing technologies are explored. The drones are employed for the
    numerous methods for spraying and crop monitoring. In this paper, we also discuss
    how AI should be combined with other technologies and applications of AI for solving
    farming challenges.
  doi: 10.47852/bonviewJCCE2202174
  full_citation: '>'
  full_text: '>

    "Journal of Computational and Cognitive Engineering HOME ABOUT BROWSE CONTRIBUTE
    EDITORIAL BOARD SPECIAL ISSUES Search Register BROWSE Login Home / Archives /
    Vol. 2 No. 2 (2023) / Research Articles Implementation of Artificial Intelligence
    in Agriculture Shivangi Sharma Department of Computer Science and Engineering,
    Lakshmi Narain College of Technology and Sciences, India Kirti Verma Department
    of Engineering Mathematics, Lakshmi Narain College of Technology, India .st0{fill:#A6CE39;}
    .st1{fill:#FFFFFF;} https://orcid.org/0000-0003-4658-9722 Palak Hardaha Department
    of Computer Science and Engineering, Lakshmi Narain College of Technology and
    Sciences, India DOI: https://doi.org/10.47852/bonviewJCCE2202174 Keywords: artificial
    intelligence, herbicide, pesticide, automation, irrigation, machine learning,
    anomaly detection, computer vision, natural language processing, conversational
    AI Abstract Agriculture has a significant contribution to the economy. Agricultural
    automation is a major cause of concern and a relatively new phenomenon throughout
    the world. The world''s population is quickly growing, resulting in increased
    demand for food and labor. Farmers'' customary techniques were insufficient to
    achieve these goals. As a result, new automated techniques were developed. These
    creative initiatives met food demands while also providing work opportunities
    for a large number of people. Agriculture has changed as a result of artificial
    intelligence. This strategy has shielded agricultural production from a variety
    of threats such as weather, population growth, labour rights, and food security
    concerns. The major issue of this is the numerous applications of AI in agriculture,
    such as irrigation, weeding, and spraying with different sensors or other ways
    implanted in robots and drones. These technologies limit the use of water, pesticides,
    and herbicides, preserve soil fertility and help in the effective use of labour,
    resulting in increased output and quality. Many researchers efforts to gain a
    quick overview of the present state of automation in agriculture, including weeding
    systems using robots and drones. Two automated weeding strategies are explored,
    as well as several soil water sensing technologies. The utilization of drones,
    as well as the numerous methods for spraying and crop monitoring that drones employ.
    In this Paper we also discuss how AI should be combined with other technologies
    and applications of AI in solving farming challenges.   Received: 18 February
    2022 | Revised: 2 June 2022 | Accepted: 2 July 2022   Conflicts of Interest The
    authors declare that they have no conflicts of interest to this work. Metrics
    PDF views 3,216 Jul 04 ''22 Jul 07 ''22 Jul 10 ''22 Jul 13 ''22 Jul 16 ''22 Jul
    19 ''22 Jul 22 ''22 Jul 25 ''22 Jul 28 ''22 Jul 31 ''22 Aug 01 ''22 32 daily (first
    30) | monthly  We recommend Revolutionizing Sustainable Energy Production with
    Quantum Artificial Intelligence: Applications in Autonomous Robotics and Data
    Management Luka Baklaga, Green and Low-Carbon Economy, 2023 Thriving in the Entrepreneurial
    Landscape of Sustainability and Intelligent Automation Era Narcisa Roxana Moşteanu,
    Green and Low-Carbon Economy, 2023 Topological Data Analysis of COVID-19 Using
    Artificial Intelligence and Machine Learning Techniques in Big Datasets of Hausdorff
    Spaces Allan Onyango et al., Journal of Data Science and Intelligent Systems,
    2023 AI weed-killing drones are coming for mega farms by Coco Liu, TechXplore.com,
    2023 Robots in the field: farms embracing autonomous technology by Kelvin Chan,
    Phys.org, 2018 Applications of artificial intelligence in power system operation,
    control and planning: a review Utkarsh Pandey et al., Clean Energy, 2023 Powered
    by PDF Published 2022-07-04 How to Cite Sharma, S., Verma, K., & Hardaha, P. (2022).
    Implementation of Artificial Intelligence in Agriculture. Journal of Computational
    and Cognitive Engineering, 2(2), 155–162. https://doi.org/10.47852/bonviewJCCE2202174
    More Citation Formats Issue Vol. 2 No. 2 (2023) Section Research Articles License
    Copyright (c) 2022 Authors This work is licensed under a Creative Commons Attribution
    4.0 International License. Journal Information Editor-in-Chief: Harish Garg Thapar
    Institute of Engineering and Technology, India Frequency: Quarterly Submission
    to final decision: 65 days Acceptance to publication: 15 days Acceptance rate:
    29% eISSN: 2810-9503 pISSN: 2810-9570   © 2024  Bon View Publishing Pte Ltd. Make
    a Submission Announcements JCCE is formally accepted by SCOPUS March 12, 2024
    We are delighted to share with you that Journal of Computational and Cognitive
    Engineering has been accepted by SCOPUS and articles published in this journal
    will be indexed from April, 2024. JCCE Published Volume 3, Issue 1 on February
    23, 2024 February 23, 2024 We are excited to announce that Journal of Computational
    and Cognitive Engineering (JCCE) published Volume 3 Issue 1 on February 23, 2024.
    JCCE Published Volume 2, Issue 4 on November 15, 2023 November 15, 2023 We are
    excited to announce that Journal of Computational and Cognitive Engineering (JCCE)
    published Volume 2 Issue 4 on November 15, 2023. Keywords reliability decision-making
    aggregation operators soft set availability Internet of Things optimal solution
    fuzzy set BERT big data Choquet integral fuzzy logic MADM solar PFDDHM operator
    repairman automation Most Read Implementation of Artificial Intelligence in Agriculture
    3017 A Systematic Review on Intelligent Transport Systems 1141 Spam Detection
    Using Bidirectional Transformers and Machine Learning Classifier Algorithms 1104
    Comparing BERT Against Traditional Machine Learning Models in Text Classification
    863 Machine Learning-Based Intrusion Detection System: An Experimental Comparison
    844 All site content, except where otherwise noted, is licensed under a Creative
    Commons Attribution 4.0 International License. pISSN 2810-9570, eISSN 2810-9503
    | Published by Bon View Publishing Pte Ltd. Member of                   "'
  inline_citation: '>'
  journal: Journal of Computational and Cognitive Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Implementation of Artificial Intelligence in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bah M.D.
  - Hafiane A.
  - Canals R.
  citation_count: '4'
  description: Crop row detection is an important aspect of smart farming. An accurate
    method of crop row detection ensures better navigation of the robot in the field
    as well as accurate weed control between rows, and crop monitoring. Recent deep
    learning approaches have emerged as the indispensable methods for most computer
    vision tasks. Therefore, crop row detection using neural networks is currently
    the most frequently studied approach. However, these methods require a large amount
    of labeled data, which is not suitable for many situations such as agriculture,
    where labeling data is tedious and expensive. Unsupervised techniques such as
    graph-based represent a promising alternative to tackle such a problem. Indeed,
    the crop rows represent relationships between plants (relative position, co-occurrence…),
    that could be integrated as structured information with an unsupervised approach.
    However, little attention has been paid to graph-based techniques for crop row
    structures representation. In this paper we propose a new method based on hierarchical
    approach and unsupervised graph representation for crop row detection. The idea
    is to transform the field into a structured data where each plant can be linked
    to its neighbors according to their spatial relationships. Then, with a hierarchical
    clustering and a criterion of exploring the nodes of the graph we extract subgraphs
    that represent the aligned structures. We show that the graph matching technique
    can discard inconsistent structures such as weed aggregations. The results obtained
    show the validity of the proposed method, with higher performances.
  doi: 10.1016/j.eswa.2022.119478
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract BetaPowered by GenAIQuestions answered in this article
    Keywords 1. Introduction 2. Method 3. Experiment 4. Conclusion CRediT authorship
    contribution statement Declaration of Competing Interest Data availability References
    Show full outline Cited by (5) Figures (13) Show 7 more figures Tables (2) Table
    1 Table 2 Expert Systems with Applications Volume 216, 15 April 2023, 119478 Hierarchical
    graph representation for unsupervised crop row detection in images Author links
    open overlay panel Mamadou Dian Bah a, Adel Hafiane b, Raphael Canals a Show more
    Share Cite https://doi.org/10.1016/j.eswa.2022.119478 Get rights and content Highlights
    • Computer vision system and unsupervised approach for crop image analysis. •
    Parallel structures detection in UAV images. • Transformation of the crop field
    into a graph structure. • Unsupervised hierarchical analysis to group rows as
    distinct communities in a graph. • Use of SLIC and an autoencoder during segmentation
    of crop images. Abstract Crop row detection is an important aspect of smart farming.
    An accurate method of crop row detection ensures better navigation of the robot
    in the field as well as accurate weed control between rows, and crop monitoring.
    Recent deep learning approaches have emerged as the indispensable methods for
    most computer vision tasks. Therefore, crop row detection using neural networks
    is currently the most frequently studied approach. However, these methods require
    a large amount of labeled data, which is not suitable for many situations such
    as agriculture, where labeling data is tedious and expensive. Unsupervised techniques
    such as graph-based represent a promising alternative to tackle such a problem.
    Indeed, the crop rows represent relationships between plants (relative position,
    co-occurrence...), that could be integrated as structured information with an
    unsupervised approach. However, little attention has been paid to graph-based
    techniques for crop row structures representation. In this paper we propose a
    new method based on hierarchical approach and unsupervised graph representation
    for crop row detection. The idea is to transform the field into a structured data
    where each plant can be linked to its neighbors according to their spatial relationships.
    Then, with a hierarchical clustering and a criterion of exploring the nodes of
    the graph we extract subgraphs that represent the aligned structures. We show
    that the graph matching technique can discard inconsistent structures such as
    weed aggregations. The results obtained show the validity of the proposed method,
    with higher performances. Previous article in issue Next article in issue Questions
    answered in this article BetaPowered by GenAI This is generative AI content and
    the quality may vary. Learn more. What properties of the field are of interest
    in crop row detection? How are hierarchical graphs advantageous in crop row detection?
    What does the ground truth represent in crop row detection? What is the benefit
    of highlighting the crop row pattern? How do Bah, Hafiane, and Canals detect crop
    rows? Keywords Graph representationHierarchical graph representationCrop row detectionAutoencoderComputer
    vision 1. Introduction For many generations, industries including robotics and
    tractor manufacturing, …have been constantly innovating to make agricultural equipment
    more and more autonomous for sowing, harvesting, weeding, etc. These technologies
    are helping farmers to move from an agriculture where everything is applied uniformly
    to a much more targeted agriculture. This new farming technique is commonly referred
    to as precision agriculture. The main goal of precision agriculture is to implement
    the right management practice in order to allocate the appropriate quantity of
    inputs such as fertilizers, herbicides, seed, fuel, etc. to the right place and
    time (McBratney et al., 2005, Pierce and Nowak, 1999). However, all these processes
    require accurate guidance with respect to crop rows. Some studies showed the advantages
    of the automation of crop row detection for robot navigation (Burgos-Artizzu et
    al., 2011, Rovira-Más et al., 2005, Winterhalter et al., 2018), and for the detection
    of weeds between rows (Gée et al., 2008, Peña et al., 2013). Existing literature
    indicates that different imaging-based methods have been used for detecting crop
    rows (Ronchetti, Mayer, Facchi, Ortuani, & Sona, 2020). For instance the Hough
    transform (Hough, 1962) is one of the most used machine vision methods for identifying
    crop rows (Basso and de Freitas, 2020, Gée et al., 2008). It was specifically
    used to guide mobile robots in sugar beet and rapeseed fields in Åstrand and Baerveldt
    (2005). With extensive field tests the authors demonstrated that the system was
    accurate and fast enough to control a weeder and a mobile closed-loop robot with
    a standard deviation of the position of 2.7 and 2.3 cm, respectively. The Hough
    transform was designed to detect curves and straight lines in Leemans and Destain
    (2006). Bakker et al. (2008) applied the Hough transform to extract crop rows
    from images acquired by a robot using a specific angle interval. Jones, Gée, and
    Truchetet (2009) detected crop rows by modeling agronomic images taken from a
    virtual camera placed in a virtual field with and without perspective. Ji and
    Qi (2011) proposed RHT (Random Hough Transform) (Xu & Oja, 1993) to detect crop
    rows. The specificity of RHT is that the Hough transform is applied to randomly
    selected vegetation pixels. Furthermore compared to the Hough transform, about
    45% of processing time is saved. Bah, Hafiane, and Canals (2017) detect crop rows
    by using Hough transform and Simple Linear Iterative Clustering (SLIC) on the
    skeleton of the crop rows. Other authors have preferred to use linear regression
    to detect crop rows, assuming that each crop row is a cloud of points that can
    be fitted by a line. For instance, Søgaard and Olsen (2003) located the crop rows
    in a barley field using a weighted linear regression. Hague, Tillett, and Wheeler
    (2006) determined the position and orientation of crop rows by applying the non-linear
    version of the Kalman filter called extended Kalman filter (Julier & Uhlmann,
    2004). Montalvo et al. (2012) proposed to use linear regression to detect crop
    rows in a maize field with a high weed density. A combination of clustering and
    linear regression methods was used for the automatic detection of crop rows based
    on multiple regions of interest (Jiang, Wang, & Liu, 2015). The analysis of spots
    or blobs is a fundamental technique in computer vision, based on the analysis
    of image regions that present certain visual coherence. Fontaine and Crowe (2006)
    relied on the direction and center of gravity of the different blobs to propose
    a crop row detection method. Burgos-Artizzu et al. (2011) proposed different methods
    to detect crop rows using a combination of the center of gravity, the direction
    of blob regions and the direction of movement of a tractor with a camera mounted
    on it. Peña et al. (2013) developed an object-by-object image analysis (OBIA)
    procedure on a series of UAV images for the automatic discrimination of crop rows
    and weeds in a corn field. They found that the process is strongly influenced
    by the presence of weeds in close proximity or within crop rows. Template matching
    and green pixels accumulation are used (Li, Zhang, Du, & He, 2020). In Vidović,
    Cupec, and Hocenski (2016) crop row detection by global energy minimization was
    proposed. In Kise et al., 2005, Rovira-Más et al., 2008 the authors located the
    crop rows with an altimetric map of the field. But this method is generally used
    in cases where the plant heights are sufficiently large. A common drawback of
    the above methods is the need for information including the number of crop rows
    and the distance between crop rows. In addition, when using the Hough transform
    method, it is difficult to detect peaks in a robust way by avoiding spurious peaks,
    especially in complex scenes. The strip features cannot usually detect crop rows
    accurately in complex conditions such as high pressure images or missing plants.
    Recently, deep learning algorithms have proven to be effective for classification
    and semantic segmentation (Bah et al., 2018, Gai et al., 2020, Kerkech et al.,
    2018, Lottes et al., 2020), but very few have been designed to study the spatial
    relationship of pixels on the rows and columns of an image. In Pang et al. (2020),
    an instance segmentation algorithm (MaxArea Mask Scoring RCNN) is developed for
    detecting the crop-rows. A new deep localization network for intra-row rice detection
    at the single plant level in a paddy field is proposed in Huang et al. (2020).
    Method called CRowNet which uses a convolutional neural network (CNN) and the
    Hough transform to detect crop rows in images taken by an unmanned aerial vehicle
    (UAV) is proposed in Bah, Hafiane, and Canals (2020). Despite the great success
    of CNNs on regular Euclidean data such as images (2D grids) and texts (1D sequences),
    they suffer in solving problems in data with non-Euclidean characteristics (e.g.
    irregular structure). Given that the latter can be considered as graph instances,
    more recently work involving graph convolutional neural networks (GCNs) have begun
    to emerge (Hong et al., 2020, Ma et al., 2019). This increasing interest is attributed
    to two main factors: the increasing amount of non-Euclidean (the irregular) data
    in real-world applications and the limited performance of CNN when dealing with
    such data. Deep neural network architecture was proposed with the integration
    of graphs to solve issues related to the identification and refinement of a plantation-line
    position in remote sensing imagery (Gonçalves et al., 2021). In Ouyang and Li
    (2021) authors combine the advantages of deep semantic segmentation network (DSSN)
    and GCN to perform the segmentation of urban aerial imagery, identifying features
    like vegetation, pavement, buildings, water, vehicles, and others. Graph Convolutional
    Networks were applied to perform land cover classification in hyper-spectral images
    (Hong et al., 2020). However, a major limitation of GCN architectures is that
    they propagate information only through the edges of the graph and are unable
    to infer and aggregate information hierarchically (Ying et al., 2018). Lastly,
    there are some recent works that learn hierarchical graphs which is a very expressive
    representation of data, as it takes into account the relationship between instances
    of the graph, rather than treating them independently. In hierarchical graph,
    a node itself is a graph instance. In Mi and Chen (2020) a Hierarchical Graph
    network is proposed for visual relationship detection. In Li et al. (2019) authors
    study graph classification in a hierarchical graph, which predicts the class label
    of graph instances in a hierarchical graph. Although hierarchical methods have
    brought a great improvement in the representation of graphs, the use of supervised
    learning-based method is an limitation of this approach since it requires a large
    amount of labeled data. In this paper, we propose a hierarchical and unsupervised
    graph representation for crop row detection, by considering the field as a set
    of points where each point can be a plant or an aggregation of touching plants
    (vertex). Therefore, the field is transformed into a structure where plants are
    connected according to their spatial relationship (edges). The aim is to take
    advantage of the efficient representation of the data structure by graphs to extract
    geometric properties of the field. Thus, we exploit the ability of graphs to group
    nodes that share the same properties. Here we are interested in the properties
    of the alignment of plants in rows and the parallelism of these rows. To capture
    these two properties, we use a concept of hierarchical representation inspired
    by the field of natural language processing, by analogy, we assume that plants
    are words forming sentences (the crop rows) which together constitute a paragraph
    or document (the field). Following this concept and with unsupervised manner,
    plants are first detected and then grouped into a structure to form the graph.
    Then the nodes of the graph that constitute potential rows are grouped in the
    same communities according to their alignment. The communities are analyzed, then
    grouped according to a parallelism criterion to detect all the crop rows (Fig.
    1). The following summarizes the main contributions of this paper: Download :
    Download high-res image (718KB) Download : Download full-size image Fig. 1. Agricultural
    field presented as a graph. A graphical representation of a crop field, which
    considers the field as a set of points where each point can be a plant or an aggregation
    of plants that touch each other (vertex) and are connected by their spatial relationship
    (edges). • Unsupervised segmentation algorithm combining SLIC and autoencoder
    to efficiently extract vegetation and highlight crop row patterns • Transformation
    of the crop field into a graph structure • Unsupervised hierarchical analysis
    to group rows as distinct communities in a graph. This paper is organized as follows.
    Section 1 gives the general introduction. The details of materials and methods
    are presented in Section 2. Section 3 shows the results and discussions of the
    proposed algorithm. The conclusions are presented in Section 4. 2. Method This
    section describes the details of the materials and methods applied in this paper.
    This method is divided into 2 main steps (Fig. 2). In the first step the non-essential
    parts of the image, i.e., the background or the soil, are subtracted and then
    the points to be used as nodes of the graph are extracted. In the second step
    the graph is constructed, and the hierarchical method is applied to detect the
    crop rows. Download : Download high-res image (389KB) Download : Download full-size
    image Fig. 2. The main steps of the proposed method. 2.1. Points of interest detection
    with CNN and SLIC This phase is essential to build the graph. Its purpose is to
    extract points that are sufficiently representative of the positions of the plants
    in the image. These points can be the central pixel of each plant or simply the
    point representing an aggregation of plants very close to each other. Here, image
    is split into small regions. Based on low-level information, these regions are
    also called superpixels. Compared to the image representation by pixel, the superpixel
    is more consistent with human visual cognition and contains less redundancy. Download
    : Download high-res image (289KB) Download : Download full-size image Fig. 3.
    Architecture of the autoencoder. Both layers of the encoder have the same square
    kernel . The number of nodes of each layer is 16 and 4 for the first and second
    layer respectively. The original image shape is obtained with two layers of 2D
    transposed convolution operator. In this paper we propose to exploit a combination
    of CNN (autoencoder) (Fig. 3) and Simple Linear Iterative Clustering (SLIC) (Achanta
    et al., 2011) to efficiently extract superpixels. The autoencoder is trained to
    preprocess the image by removing noise and smoothing the image. Thus the color
    of vegetation pixels is accentuated, while those of non-vegetation pixels are
    attenuated. The interest of this preprocessing is the highlighting of the crop
    row pattern. The CNN model used is an autoencoder which aims to create an image
    with the same characteristics as the input image but with a particular attention
    to the vegetation (Fig. 4). Autoencoders are special neural networks that have
    the same number of neurons on their input and output layers. Their aims are to
    compress representations and preserve essential information for reconstructing
    the input. They are composed of two symmetric parts: encoder and decoder. The
    chosen autoencoder architecture has only two convolution layers within the encoder,
    because large networks require massive computational power and long training time
    to achieve the highest possible accuracy. The convolution kernel shape (3 × 3)
    is the same for both layers. Each convolution layer is followed by a max-pooling.
    This operation subsamples the input characteristic map by a factor of 2. Max-pooling
    enhances the translation invariance on small spatial shifts of the input image.
    Two layers are used in the decoder step. Decoders consist in transforming semantically
    the discriminating characteristics learned by the encoder at a lower resolution
    into a higher resolution. The image to reconstruct by the autoencoder (ground
    truth) is obtained as follows: • Input image is segmented with Otsu thresholding
    applied on ExG. • SLIC is applied on the segmented image to create superpixels
    • Superpixels are used to smooth the original image; each pixel is replaced by
    the average value of the superpixel to which it belongs. The advantage of replacing
    these 3 steps by an autoencoder is that for the preprocessing of new images, it
    is not necessary to choose a particular number of superpixels to be generated
    depending on the resolution and size of the image. The output of the autoencoder
    is segmented with the Excess Green (ExG) index (Eq. (1)) and the nonparametric
    and unsupervised automatic threshold selection method called Otsu thresholding
    (Otsu, 1979) since this approach has proven to be robust and simple. (1) where
    r, g and b are the normalized chromatic coordinates. In the last image obtained,
    all the vegetation pixels have the same value. It means after segmentation we
    have two kind of pixels the white ones which are vegetation and the black ones
    that represent the background. The advantage of this procedure is to create homogeneous
    superpixels which depend on the crop rows structure and not on the vegetation
    color variations. According to Fig. 5 the size of the superpixels must be chosen
    by making a trade-off. A size that is too small will result in more than one superpixel
    per plant and the edges will be small and the geometric representation of the
    crop rows will be more complex. Conversely, a superpixel that is too large disturbs
    the overall orientation of the crop rows. Fig. 6 shows an example of vegetation
    segmentation and the extraction of regions centers. We propose to use 16 as the
    optimal size of the superpixels to generate. Superpixels of a shape (16 × 16)
    are generated with the SLIC algorithm on segmented image. The centers of the superpixels
    are defined as the points of interest. Download : Download high-res image (691KB)
    Download : Download full-size image Fig. 4. Image without auto-encoder in left
    and image with autoencoder in right. The yellow color is the boundaries after
    vegetation segmentation. SLIC is a clustering-based method which creates a local
    grouping of pixels based on their spectral values defined by the values of the
    CIELAB color space and their spatial proximity (Dos Santos Ferreira, Matte Freitas,
    Gonçalves da Silva, Pistori, & Theophilo Folhes, 2017). A higher value of compactness
    makes superpixels more regularly shaped. A lower value makes superpixels adhere
    to boundaries better, making them irregularly shaped. Download : Download high-res
    image (1MB) Download : Download full-size image Fig. 5. Superpixels of different
    sizes generated by the SLIC method in red. (a), (b) and (c) represent respectively
    superpixels of size 8, 16 and 32. Download : Download high-res image (645KB) Download
    : Download full-size image Fig. 6. (a) Superpixel generated by SLIC in yellow,
    the size of superpixel is about 16 × 16. (b) Yellow circles represent the superpixels
    center (Point of Interest or vertices). Thus each centroid became a “node”. Central
    pixel of superpixel is now denoted . 2.2. Hierarchical graph representation This
    section describes the creation of the graph based on the results of the previous
    section. It also describes how the hierarchical representation of the graph is
    established. Graphs are popular modes of presenting complex relationship data.
    Graph is modeled using an adjacency matrix that can be weighted or unweighted.
    It is represented by , where ( ) is the set of nodes with elements, and is the
    set of edges of . 2.2.1. Graph construction The graph is constructed by applying
    triangulation (Rose, 1970). Triangulation is a well-known technique to cut a geometric
    shape (a plane, a polygon) into a collection of triangles (meshes). The Delaunay
    triangulation (Lee & Schachter, 1980) has been used to generate a graph where
    each vertex or node corresponds to the points of interest already extracted (superpixels
    centroid). Thus each centroid became a “node”. Central pixel of superpixel is
    now denoted (Fig. 6). Delaunay triangulation for a given set of centroid pixel
    is a triangulation DT( ) such that no point in is inside the circumcircle of any
    triangle in DT( ). It has the advantage of being unsupervised and find the minimum
    angle of all the angles of the triangles in the triangulation; they tend to avoid
    sliver triangles. Therefore, each edge of the graph form an angle and since each
    image has a global orientation obtained from the rows direction edges are weighted
    according to Eq. (2). This weighting allows us to identify the edges close or
    farther from the global orientation. The angle is obtained by applying Fourier
    transform (Josso, Burton, & Lalor, 2005) method. The principle of the algorithm
    is first to calculate the image Fourier transform modulus and then to characterize
    the distribution of this spectrum around the zero frequencies by Principal Component
    Analysis. (2) where represents the angle of edge ( ) formed between vertices and
    . Let and be two vectors that represent respectively the coordinates of vertices
    and in the image, is obtained by Eq. (3). (3) Once the graph is constructed, the
    next step is rows detection. The next sections, details this procedure, and Fig.
    7 gives an overview of the main steps. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 7. Flowchart of the proposed method.
    2.2.2. Community detection: Potential crop rows The method requires the use of
    subgraph or community analysis, which means grouping graph nodes into clusters.
    Two types of analysis are applied here: the first exploits the global orientation
    of crop rows obtained and the second exploits the parallel layout of crop rows.
    The last analysis allows us to identify the communities that are crop rows. The
    term community will refer to a subgraph in the remainder of this article. The
    created graph is processed to retain only the edges that follow an orientation
    around the global orientation computed beforehand by the Fourier transform. This
    enables us to focus on the edges that are likely to form the crop rows. The effect
    of this operation is the subdivision of the graph into communities (see Fig. 7).
    One of the key advantages of this approach are to determine the nodes that share
    the same geometrical properties. The edges with an orientation close to the global
    orientation are retained. These edges are identified by applying a threshold (Eq.
    (4)) which avoids considering edges between plants that are on parallel rows.
    Communities are created by assigning a label to each subgraph into distinct sets
    using the Louvain algorithm (Blondel, Guillaume, Lambiotte, & Lefebvre, 2008).
    (4) Louvain’s algorithm is a hierarchical community extraction algorithm applicable
    to large networks. It relies on a greedy procedure: from any partition of the
    vertices (generally the partition in singletons), the algorithm attempts to increase
    the value of the modularity while moving the vertices of their community towards
    any other neighbor. In much details, the algorithm computes the gain of modularity
    (Eq. (5)) obtained by adding vertex to community . Whereas modularity is an optimization
    function that allows to evaluate the presence of an edge between two vertices
    of an undirected network by comparing it to the probability of having such an
    edge in a random model following the same degree distribution as the original
    network. Formally, the modularity of a partition of an undirected graph is defined
    in Eq. (6). (5) (6) Where is the number of edges of G, represents the weight of
    the edge between and (set to 0 if such an edge does not exist), is the degree
    of vertex (i.e., the number of neighbors of node ), is the sum of all edge weights
    for vertices within the community (including edges which link to other communities).
    is the community to which node belongs and the is defined as 1 if , and 0 otherwise.
    Furthermore, depending on the field, not all nodes are useful (presence of weeds)
    since it is not excluded that these nodes have an impact on the final result.
    Moreover, it is possible that crop rows are broken down into sub-communities or
    regions. As crop rows generally follow a linear property, the Pearson correlation
    coefficient is applied to form communities ( ). Two communities and are merged
    if the Pearson correlation coefficient of the elements present in these two communities
    is close to 1. At the end of this process, potential crop rows are obtained. Next
    step, once the communities are analyzed and merged, is to detect and remove the
    communities formed by inter-row weeds. 2.2.3. Select relevant communities: Crop
    rows The selection of the relevant communities is based on the assumption that
    a community is a crop row when three criteria inside a graph-community are satisfied
    : (i) orientation, (ii) parallelism, (iii) distances between neighboring communities.
    At this level, each node of the initial graph belongs to a community which, for
    the moment, has no neighbors. Thus, two communities and are connected when at
    least one node belonging to has a connection with belonging to and/or vice versa
    (this step can be observed in the second part of Fig. 7). In the previous section
    the orientation condition was used to emphasize the orientation according to global
    orientation of crop rows. Now, the focus this time is on the edges with an orientation
    perpendicular to the global orientation, with the aim of detecting the parallel
    crop rows. Two communities are connected according to Eq. (7) and the edges are
    weighted according to the number of neighbors and distance between the two communities
    Eq. (8). As a community can have more than one neighbor the weights of the edges
    are replaced by their inverse, that involve edges between close communities are
    smaller than those between distant ones in terms of distance in the image. Hence,
    priority is given to the most distant communities, because we consider that the
    closer the communities, the higher the risk of having a weed community. The resultant
    graph is named , it is a graph representation of the existing relationships between
    the communities. (7) The value chosen for is , which avoids considering the edges
    between the same crop rows. (8) and represent the coordinates of node in the image.
    To identify the communities corresponding to the crop rows, a two-step process
    is used. In the first one the largest subgraph of is extracted. In the second
    step, the longest path in number of nodes and the shortest in distance is detected
    using Dijkstra algorithm. Given a graph and a source vertex in the graph, Dijkstra
    algorithm find the shortest paths from the source to all vertices in the given
    graph. Thus all nodes on that path represent the crop rows. For instance in Fig.
    8 the nodes marked in red are not included in the shortest path. As a reminder
    those nodes are a representation of a subgraph or community. Download : Download
    high-res image (367KB) Download : Download full-size image Fig. 8. Example of
    selection of relevant communities. Communities identified as inter-row weeds are
    in red. 3. Experiment In this section we evaluate our method both qualitatively
    and quantitatively. We also compare it to state-of-the-art methods. Experiments
    were conducted on 154 images (480 × 360 pixels) from public datasets number 000,
    001 and 002 (Sa et al., 2018). These images were acquired in a beet field by the
    Parrot RedEdge-M multispectral sensor 10 m from the ground. To account for crop
    row irregularities and to avoid the impact of weed pressure in the test data,
    we built the ground truth manually with the Matlab labeling tool (Image labeler).
    The labeling consisted of creating a mask around each row. The weed infestation
    rate (WIR) in these images varies from low to high. The WIR is the proportion
    of vegetation pixels considered as weeds in an image. According to the WIR classification,
    we have 45 images without weeds (WIR  5%), 21 images with a low rate of infestation
    ([5% - 15%[), 50 images with a moderate infestation rate ([15% - 35%[) and 38
    heavily infested (WIR  35%). These infestation rates were computed from the ground
    truth of the images available in the dataset. In total there were 1329 crop rows
    to detect ( ). To train the auto-encoder, the number of samples used was 816 images
    acquired in a bean field; 80% of the samples were used for training and 20% for
    validation. In Fig. 9 we presented an example of images obtained by using images
    smoothed with superpixels of size 10 and 64 and the corresponding results. The
    Adam optimizer was used, the learning rate was set to 0.001. Measuring detection
    performance, is not quite straight-forward because it is challenging to get the
    true position and direction for the center lines of crop rows due to natural variations
    in the crop growth stage. In Fig. 10 we can see that the ground truth is not a
    simple line on a row but rather a strip encompassing the plants of the crop row.
    This is because in crop row, plants are not perfectly aligned, their alignment
    does not fit exactly the strip or band model. Fig. 11(b) shows the connection
    of communities after the application of the proposed method on the image of Fig.
    11(a). Fig. 11(c) shows the original image and the labels of the selected communities.
    We notice that only relevant communities are retained and they do not have the
    same size since the crop rows are not all identical. Download : Download high-res
    image (765KB) Download : Download full-size image Fig. 9. (a) the original image.
    (b) Image segmented with ExG and Otsu thresholding. (c) and (d) represent respectively
    images smoothed with superpixels of size 10, 64. (e) and (g) are respectively
    the output of autoencoder trained with images smoothed with superpixels of size
    10 and 64. (f) and (h) are respectively the segmented version of (e) and (g).
    Download : Download high-res image (1MB) Download : Download full-size image Fig.
    10. (a) represents the input image and (b) the graph with the selected communities
    as crop rows. (c) the contour of the ground truth is marked in blue and the text
    represents the label of the selected communities. (d), (e) and (f) represent the
    step used to assess the method. In (d) we have the nodes which detect the crop
    row presented by a red dot. The red line is the crop row detection obtained with
    RANSAC on the nodes (red dots in (e). (f) Overlap after dilation of detected crop
    row and the skeleton of the ground truth. 3.1. Evaluation criteria The method
    is assessed at two levels. The first level consists of evaluating whether all
    nodes in the selected communities belong to crop rows (Fig. 10(d)). The second
    level analyzes the ability of a community to detect an entire crop row (Fig. 10(e)
    and Fig. 10(f)). The overlap coefficient ( ), or Szymkiewicz–Simpson coefficient
    (Vijaymeena & Kavitha, 2016) is computed for the assessment. This metric indicates
    whether the detected community is entirely included in the ground truth or not.
    The overlap coefficient is a similarity measure that is related to the Jaccard
    index. It measures the overlap between two sets as indicated in the equation (Eq.
    (9)). Given the ground truth of a crop row ( ), corresponds to the set of nodes
    belonging to the community that intersects with it. The is equal to 1 if all the
    nodes of the community are located inside . Since all rows are not all detected
    in same we propose to apply a threshold on it and compute multiple metrics in
    order to have a robust analysis. The metrics computed at each threshold are: the
    , the , the score and the Accuracy. reflects the ability to reveal the needed
    information (Eq. (11)), (Eq. (10)) indicates the correctness of the detected results,
    and the score indicates the balance between and (Eq. (12)). It is the harmonic
    mean of precision and recall. These indices were computed for each crop row. (Eq.
    (13)) is a metric that generally describes how the model performs across all classes.
    It is useful when all classes are of equal importance. It is calculated as the
    ratio between the number of correct detection to the total number of detections.
    We need to compute True Positive ( ), False Positive ( ), and False Negative (
    ) for metrics. For rows detection, corresponds to the number of rows correctly
    detected, while corresponds to the number of detection that are not crop rows
    and corresponds to the number of crop rows that were not detected by the method.
    (9) (10) (11) (12) (13) (14) 3.2. Results and discussion For a threshold of a
    confusion matrix is presented in Table 1. We notice that on the confusion matrix
    (Table 1) the method has overall missed ( ) 73 and detected 1289 crop rows with
    1257 good detection ( ) and 17 overdetection ( ). Miss detection ( ) occurs in
    most cases on the rows located on the edges of the image and over-detection (
    ) is a consequence of the presence of weeds between crop rows. In addition, the
    average score of the 1257 true detected crop rows is 0.983. Download : Download
    high-res image (904KB) Download : Download full-size image Fig. 11. Examples of
    crop rows detection after a hierarchical graph representation of the field. To
    effectively assess the method we varied the threshold from 0 to 1 and the result
    quantified through different metrics. Thus, for each value of , , , and are computed.
    For , the Recall, Precision, F1, Accuracy were respectively 0.945, 0.987, 0.965
    and 0.946. In Fig. 13 we remark that the performance is affected when is higher
    than 0.8. This analysis shows that the retained communities have nodes which are
    well located inside the crop row. We also evaluate the ability of given graph
    to detect an entire crop row. Since the communities are made up of points, algorithms
    such as Ransac, Hough transform and the linear regression were applied to the
    corresponding points (nodes) in each community to form a straight line. The mean
    OS obtained for RANSAC, the Hough transform and the linear regression are respectively
    0.971, 0.966 and 0.968. In addition, is computed to efficiently evaluate the overlap
    between the rows obtained with RANSAC, Hough transform and linear regression.
    , also called the Jaccard similarity coefficient Eq. (14) considers both the false
    positives and the missed values for each class. The is calculated as a ratio of
    the area of overlap to the area of the union between ground truth ( ) and the
    prediction. We thinned all the rows and each row is represented by its skeleton.
    Assuming that the thinning operation returns skeletons which represent the center
    line of each row, we dilate the skeletons to obtain the same crop row width as
    that measured in the field. Based on a ground measurement in the beet field, the
    size of a crop plant varies from 15 to 20 pixels. We therefore dilate the skeleton
    of detected rows with a square structuring element of 15 × 15 pixels on beet images.
    Fig. 10(f) presents the result of the thinning and the dilation. For the mean
    of IoU for RANSAC, Hough transform and linear regression are respectively 0.833,
    0.813 and 0.832. These results show that the community has good performance for
    the detection of entire crop rows. In Fig. 11 we show examples of crop row detection
    after hierarchical graph representation of the field. We also applied crop row
    detection method without the autoencoder and with different superpixel sizes;
    for a size of 16 × 16 the mean of IoU for RANSAC, Hough transform and linear regression
    are respectively 0.778, 0.747 and 0.781. We can notice that the use of the autoencoder
    has improved the result about 6%. In Table 2 we can see that the size of the superpixels
    has an impact on the results. A very large size of the superpixels makes them
    less meaningful. The result shows that the choice of superpixel size must meet
    a trade-off between the overall structure of the field and the representativeness
    of the plants or their aggregation in the crop rows. Large superpixels would give
    a non-representative graph of the field while very small ones would increase the
    number of vertices and even disturb the global representativeness of the field
    (Fig. 12). According to the ground truth the approximate size of a plant varies
    from 15 to 20 pixels; this may explain the results. The number of crop rows in
    a field can sometimes be considered as an indicator of good plant growth and good
    yield. Consequently, a curve of the according to the segmentation quality ( )
    required for the score is plotted. Fig. 13(b) shows the performance of the methods
    according to the obtained in each crop row. We notice that due to the irregularity
    of the crop rows shape, it is difficult to achieve a 100% overlap between the
    ground truth and the prediction. In addition, the accuracy decreases when is superior
    to 0.65. For a value of , 85.1%, 88.4% and 88.2% of crop crows are detected respectively
    by Hough Transform, RANSAC and linear regression. Download : Download high-res
    image (834KB) Download : Download full-size image Fig. 12. Graph generated with
    superpixels of different sizes generated by the SLIC method in red. (a), (b),
    (c) and (d) represent respectively superpixels of size 8, 16, 32 and 64. Compared
    to Gonçalves et al. (2021) which exploits the visual features of the plants here
    the only information exploited to analyze the edges is the global orientation.
    Table 1. Confusion matrix obtained by considering that . Confusion matrix obtained
    by considering that a crop row is detected if the OS score 0. That is to say that
    it is enough to be in contact with only one node of a community to be considered
    as a detected crop row. Empty Cell Predicted Empty Cell Positive Negative Actual
    Positive 1257 17 Negative 73 0 Download : Download high-res image (342KB) Download
    : Download full-size image Fig. 13. (a) , , and according to . (b) Mean of IoU
    of the three fitting method used according to the threshold . Table 2. Comparison
    with state of art methods. Graph-Ransac, Graph-Hough and Graph-Linear-Regression
    represent respectively the three methods applied on our graph results, Ransac,
    Hough transform and linear regression. 16, 32 and 64 represent the size of the
    superpixels where the vertices are extracted. Method Mean IoU Graph-Ransac (autoencoder,
    16) 0.833 Graph-Hough (autoencoder, 16) 0.813 Graph-Linear-Regression (autoencoder,
    16) 0.832 Graph-Ransac (no autoencoder, 16) 0.778 Graph-Hough (no autoencoder,
    16) 0.747 Graph-Linear-Regression (no autoencoder, 16) 0.781 Graph-Ransac (no
    autoencoder, 32) 0.810 Graph-Hough (no autoencoder, 32) 0.781 Graph-Linear-Regression
    (no autoencoder, 32) 0.810 Graph-Ransac (no autoencoder, 64) 0.403 Graph-Hough
    (no autoencoder, 64) 0.433 Graph-Linear-Regression (no autoencoder, 64) 0.404
    CRowNet (Bah et al., 2020) 0.832 CR-Hough-SLIC (Bah et al., 2017) 0.766 CR-Hough
    (Jones et al., 2009) 0.802 Furthermore, the methods most used to detect crop rows
    in aerial images use the Hough transform and geometric information of the field
    (inter-row distance, global orientation, etc.). We therefore compared our method
    to two methods based mainly on the Hough transform that we will call CR-Hough
    (Gée et al., 2008, Jones et al., 2009) and CR-Hough-SLIC (Bah et al., 2017). The
    first method, CR-Hough consists in detecting crop rows using known information
    such as the global orientation of crop rows and the inter-row distance. The second,
    CR-Hough-SLIC consists of applying the Hough transform on the vegetation skeleton
    and the superpixels generated by SLIC to detect crop rows without knowing the
    inter-row distance. However, for an image with crop rows, at least + 1 Hough transforms
    will be computed; the first one corresponds to the Hough transform computed on
    all crop row skeletons and the remaining correspond to the Hough transform of
    each row. These methods are evaluated according to Bah et al. (2020). On the beet
    images for CR-Hough, the theoretical value used is for the global orientation,
    the estimated inter-row distance is 50 pixels, and the row width is 20. The number
    of superpixels applied for CR-Hough-SLIC is 0.1% of the pixels present in the
    image. Given the success of the CNN in detecting crop rows we also used the CRowNet
    (Bah et al., 2020) method in our comparison. In Table 2 we noticed that the proposed
    method applied with RANSAC provides a better result. However, this result is very
    close to the one of CRowNet. Thus, we have shown that with a graph representation
    and hierarchical approach to analyze this graph, crop rows can be effectively
    detected in images without perspectives with relatively parallel crop rows with
    constant inter-row distance. 4. Conclusion The purpose of this research work was
    to exploit the principle of the ability of graph structure representation to detect
    crop rows in UAV images. The proposed method highlights the advantages of hierarchical
    graphs in a domain where the notion of relationship is important (i.e. plants
    are sown by row). The subgraph or community partition allowed us to study the
    linear properties of the crop fields and keep the relevant rows by exploiting
    the parallelism of the communities. We showed that the hierarchical representation
    graphs are as suitable as an unsupervised approach for crop rows detection. We
    noticed that the communities (sub-graphs) obtained were appropriate for the detection
    of whole crop rows with the use of the Hough transform, RANSAC or linear regression.
    Approximately 85% of the crop rows were detected with the Hough transform and
    88% with RANSAC and linear regression. This study considered only linear crop
    properties in drone images. Further research could explore non-linear structures
    with hierarchical graph for other types of crops or applications. We also plan
    to combine the results with CRowNet to obtain a more robust method. CRediT authorship
    contribution statement Mamadou Dian Bah: Conceptualization, Methodology, Software,
    Writing – review & editing, Writing – original draft preparation, Writing – original
    draft, Visualization, Investigation, Validation. Adel Hafiane: Conceptualization,
    Supervision, Writing – original draft preparation, Project administration, Writing
    – review & editing, Methodology. Raphael Canals: Conceptualization, Supervision,
    Writing – original draft preparation, Project administration, Writing – review
    & editing. Declaration of Competing Interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. Data availability Data
    will be made available on request. References Achanta et al., 2011 Achanta R.,
    Shaji A., Smith K., Lucchi A., Fua P., Süsstrunk S. SLIC superpixels compared
    to state-of-the-art superpixel methods IEEE Transactions on Pattern Analysis and
    Machine Intelligence, 34 (11) (2011), pp. 2274-2282, 10.1109/tpami.2012.120 Google
    Scholar Åstrand and Baerveldt, 2005 Åstrand B., Baerveldt A.-J. A vision based
    row-following system for agricultural field machinery Mechatronics, 15 (2) (2005),
    pp. 251-269, 10.1016/j.mechatronics.2004.05.005 View PDFView articleView in ScopusGoogle
    Scholar Bah et al., 2018 Bah, M. D., Dericquebourg, E., Hafiane, A., & Canals,
    R. (2018). Deep learning based classification system for identifying weeds using
    high-resolution UAV imagery. In Computing conference 2018. Google Scholar Bah
    et al., 2017 Bah M.D., Hafiane A., Canals R. Weeds detection in UAV imagery using
    SLIC and the hough transform 2017 seventh international conference on image processing
    theory, tools and applications, IEEE (2017), pp. 1-6, 10.1109/IPTA.2017.8310102
    View in ScopusGoogle Scholar Bah et al., 2020 Bah M.D., Hafiane A., Canals R.
    CRowNet: Deep network for crop row detection in UAV images IEEE Access, 8 (2020),
    pp. 5189-5200, 10.1109/ACCESS.2019.2960873 View in ScopusGoogle Scholar Bakker
    et al., 2008 Bakker T., Wouters H., van Asselt K., Bontsema J., Tang L., Müller
    J., et al. A vision based row detection system for sugar beet Computers and Electronics
    in Agriculture, 60 (1) (2008), pp. 87-95, 10.1016/j.compag.2007.07.006 View PDFView
    articleView in ScopusGoogle Scholar Basso and de Freitas, 2020 Basso M., de Freitas
    E.P. A UAV guidance system using crop row detection and line follower algorithms
    Journal of Intelligent and Robotic Systems, 97 (3) (2020), pp. 605-621 CrossRefView
    in ScopusGoogle Scholar Blondel et al., 2008 Blondel V.D., Guillaume J.-L., Lambiotte
    R., Lefebvre E. Fast unfolding of communities in large networks Journal of Statistical
    Mechanics: Theory and Experiment, 2008 (10) (2008), p. P10008, 10.1088/1742-5468/2008/10/p10008
    View in ScopusGoogle Scholar Burgos-Artizzu et al., 2011 Burgos-Artizzu X.P.,
    Ribeiro A., Guijarro M., Pajares G. Real-time image processing for crop / weed
    discrimination in maize fields Computers and Electronics in Agriculture, 75 (2)
    (2011), pp. 337-346, 10.1016/j.compag.2010.12.011 View PDFView articleView in
    ScopusGoogle Scholar Dos Santos Ferreira et al., 2017 Dos Santos Ferreira A.,
    Matte Freitas D., Gonçalves da Silva G., Pistori H., Theophilo Folhes M. Weed
    detection in soybean crops using ConvNets Computers and Electronics in Agriculture,
    143 (2017), pp. 314-324, 10.1016/j.compag.2017.10.027 View PDFView articleView
    in ScopusGoogle Scholar Fontaine and Crowe, 2006 Fontaine V., Crowe T.G. Development
    of line-detection algorithms for local positioning in densely seeded crops (2006)
    Google Scholar Gai et al., 2020 Gai J., Tang L., Steward B.L. Automated crop plant
    detection based on the fusion of color and depth images for robotic weed control
    Journal of Field Robotics, 37 (1) (2020), pp. 35-52 CrossRefView in ScopusGoogle
    Scholar Gée et al., 2008 Gée C., Bossu J., Jones G., Truchetet F. Crop/weed discrimination
    in perspective agronomic images Computers and Electronics in Agriculture, 60 (1)
    (2008), pp. 49-59, 10.1016/j.compag.2007.06.003 View PDFView articleView in ScopusGoogle
    Scholar Gonçalves et al., 2021 Gonçalves D.N., de Arruda M.d.S., Pistori H., Fernandes
    V.J.M., Ramos A.P.M., Furuya D.E.G., et al. A deep learning approach based on
    graphs to detect plantation lines (2021) arXiv preprint arXiv:2102.03213 Google
    Scholar Hague et al., 2006 Hague T., Tillett N., Wheeler H. Automated crop and
    weed monitoring in widely spaced cereals Precision Agriculture, 7 (1) (2006),
    pp. 21-32 CrossRefView in ScopusGoogle Scholar Hong et al., 2020 Hong D., Gao
    L., Yao J., Zhang B., Plaza A., Chanussot J. Graph convolutional networks for
    hyperspectral image classification IEEE Transactions on Geoscience and Remote
    Sensing (2020) Google Scholar Hough, 1962 Hough P.V.C. Method and means for recognizing
    complex patterns US Patent 3,069,654, 21 (1962), pp. 225-231, 10.1007/s10811-008-9353-1
    Google Scholar Huang et al., 2020 Huang S., Wu S., Sun C., Ma X., Jiang Y., Qi
    L. Deep localization model for intra-row crop detection in paddy field Computers
    and Electronics in Agriculture, 169 (2020), Article 105203 View PDFView articleView
    in ScopusGoogle Scholar Ji and Qi, 2011 Ji R., Qi L. Crop-row detection algorithm
    based on random hough transformation Mathematical and Computer Modelling, 54 (3–4)
    (2011), pp. 1016-1020, 10.1016/J.MCM.2010.11.030 View PDFView articleView in ScopusGoogle
    Scholar Jiang et al., 2015 Jiang G., Wang Z., Liu H. Automatic detection of crop
    rows based on multi-ROIs Expert Systems with Applications, 42 (5) (2015), pp.
    2429-2441, 10.1016/j.eswa.2014.10.033 View PDFView articleView in ScopusGoogle
    Scholar Jones et al., 2009 Jones G., Gée C., Truchetet F. Modelling agronomic
    images for weed detection and comparison of crop/weed discrimination algorithm
    performance Precision Agriculture, 10 (1) (2009), pp. 1-15, 10.1007/s11119-008-9086-9
    View in ScopusGoogle Scholar Josso et al., 2005 Josso B., Burton D.R., Lalor M.J.
    Texture orientation and anisotropy calculation by Fourier transform and principal
    component analysis Mechanical Systems and Signal Processing, 19 (5) (2005), pp.
    1152-1161, 10.1016/j.ymssp.2004.07.005 View PDFView articleView in ScopusGoogle
    Scholar Julier and Uhlmann, 2004 Julier S.J., Uhlmann J.K. Unscented filtering
    and nonlinear estimation Proceedings of the IEEE, 92 (3) (2004), pp. 401-422,
    10.1109/JPROC.2003.823141 View in ScopusGoogle Scholar Kerkech et al., 2018 Kerkech
    M., Hafiane A., Canals R. Deep leaning approach with colorimetric spaces and vegetation
    indices for vine diseases detection in UAV images Computers and Electronics in
    Agriculture, 155 (2018), pp. 237-243 View PDFView articleView in ScopusGoogle
    Scholar Kise et al., 2005 Kise M., Zhang Q., Rovira Más F. A stereovision-based
    crop row detection method for tractor-automated guidance Biosystems Engineering,
    90 (4) (2005), pp. 357-367, 10.1016/j.biosystemseng.2004.12.008 View PDFView articleView
    in ScopusGoogle Scholar Lee and Schachter, 1980 Lee D.-T., Schachter B.J. Two
    algorithms for constructing a delaunay triangulation International Journal of
    Computer & Information Sciences, 9 (3) (1980), pp. 219-242 View in ScopusGoogle
    Scholar Leemans and Destain, 2006 Leemans V., Destain M.F. Line cluster detection
    using a variant of the Hough transform for culture row localisation Image and
    Vision Computing, 24 (5) (2006), pp. 541-550, 10.1016/j.imavis.2006.02.004 View
    PDFView articleView in ScopusGoogle Scholar Li et al., 2019 Li J., Rong Y., Cheng
    H., Meng H., Huang W., Huang J. Semi-supervised graph classification: A hierarchical
    graph perspective The world wide web conference (2019), pp. 972-982 Google Scholar
    Li et al., 2020 Li S., Zhang Z., Du F., He Y. A new automatic real-time crop row
    recognition based on SoC-FPGA IEEE Access, 8 (2020), pp. 37440-37452 CrossRefView
    in ScopusGoogle Scholar Lottes et al., 2020 Lottes P., Behley J., Chebrolu N.,
    Milioto A., Stachniss C. Robust joint stem detection and crop-weed classification
    using image sequences for plant-specific treatment in precision farming Journal
    of Field Robotics, 37 (1) (2020), pp. 20-34 CrossRefView in ScopusGoogle Scholar
    Ma et al., 2019 Ma F., Gao F., Sun J., Zhou H., Hussain A. Attention graph convolution
    network for image segmentation in big SAR imagery data Remote Sensing, 11 (21)
    (2019), 10.3390/rs11212586 Google Scholar McBratney et al., 2005 McBratney A.,
    Whelan B., Ancev T., Bouma J. Future Directions of Precision Agriculture Precision
    Agriculture, 6 (1) (2005), pp. 7-23, 10.1007/s11119-005-0681-8 View in ScopusGoogle
    Scholar Mi and Chen, 2020 Mi, L., & Chen, Z. (2020). Hierarchical graph attention
    network for visual relationship detection. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 13886–13895). Google Scholar Montalvo
    et al., 2012 Montalvo M., Pajares G., Guerrero J.M., Romeo J., Guijarro M., Ribeiro
    A., et al. Automatic detection of crop rows in maize fields with high weeds pressure
    Expert Systems with Applications, 39 (15) (2012), pp. 11889-11897, 10.1016/j.eswa.2012.02.117
    View PDFView articleView in ScopusGoogle Scholar Otsu, 1979 Otsu N. A threshold
    selection method from gray-level histograms IEEE Transactions on Systems, Man,
    and Cybernetics, 9 (1) (1979), pp. 62-66, 10.1109/TSMC.1979.4310076 Google Scholar
    Ouyang and Li, 2021 Ouyang S., Li Y. Combining deep semantic segmentation network
    and graph convolutional neural network for semantic segmentation of remote sensing
    imagery Remote Sensing, 13 (1) (2021), 10.3390/rs13010119 Google Scholar Pang
    et al., 2020 Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N.,
    Thompson L., et al. Improved crop row detection with deep neural network for early-season
    maize stand count in UAV imagery Computers and Electronics in Agriculture, 178
    (2020), Article 105766 View PDFView articleView in ScopusGoogle Scholar Peña et
    al., 2013 Peña J.M., Torres-Sánchez J., Isabel De Castro A., Kelly M., López-Granados
    F. Weed mapping in early-season maize fields using object-based analysis of unmanned
    aerial vehicle (UAV) images PLoS ONE, 8 (10) (2013), 10.1371/journal.pone.0077151
    Google Scholar Pierce and Nowak, 1999 Pierce F.J., Nowak P. Aspects of precision
    agriculture (1999), pp. 1-85, 10.1016/S0065-2113(08)60513-1 View PDFView articleView
    in ScopusGoogle Scholar Ronchetti et al., 2020 Ronchetti G., Mayer A., Facchi
    A., Ortuani B., Sona G. Crop row detection through UAV surveys to optimize on-farm
    irrigation management Remote Sensing, 12 (12) (2020), p. 1967 CrossRefView in
    ScopusGoogle Scholar Rose, 1970 Rose D.J. Triangulated graphs and the elimination
    process Journal of Mathematical Analysis and Applications, 32 (3) (1970), pp.
    597-609, 10.1016/0022-247X(70)90282-9 View PDFView articleView in ScopusGoogle
    Scholar Rovira-Más et al., 2008 Rovira-Más F., Zhang Q., Reid J.F. Stereo vision
    three-dimensional terrain maps for precision agriculture Computers and Electronics
    in Agriculture, 60 (2) (2008), pp. 133-143 View PDFView articleView in ScopusGoogle
    Scholar Rovira-Más et al., 2005 Rovira-Más F., Zhang Q., Reid J., Will J. Hough-transform-based
    vision algorithm for crop row detection of an automated agricultural vehicle Proceedings
    of the Institution of Mechanical Engineers, Part D (Journal of Automobile Engineering),
    219 (8) (2005), pp. 999-1010 View in ScopusGoogle Scholar Sa et al., 2018 Sa I.,
    Popović M., Khanna R., Chen Z., Lottes P., Liebisch F., et al. Weedmap: a large-scale
    semantic weed mapping framework using aerial multispectral imaging and deep neural
    network for precision farming Remote Sensing, 10 (9) (2018), p. 1423 View in ScopusGoogle
    Scholar Søgaard and Olsen, 2003 Søgaard H.T., Olsen H.J. Determination of crop
    rows by image analysis without segmentation Computers and Electronics in Agriculture,
    38 (2) (2003), pp. 141-158 View PDFView articleView in ScopusGoogle Scholar Vidović
    et al., 2016 Vidović I., Cupec R., Hocenski Ž. Crop row detection by global energy
    minimization Pattern Recognition, 55 (2016), pp. 68-86 View PDFView articleView
    in ScopusGoogle Scholar Vijaymeena and Kavitha, 2016 Vijaymeena M., Kavitha K.
    A survey on similarity measures in text mining Machine Learning and Applications:
    An International Journal, 3 (2) (2016), pp. 19-28 Google Scholar Winterhalter
    et al., 2018 Winterhalter W., Fleckenstein F.V., Dornhege C., Burgard W. Crop
    row detection on tiny plants with the pattern hough transform IEEE Robotics and
    Automation Letters, 3 (4) (2018), pp. 3394-3401 CrossRefView in ScopusGoogle Scholar
    Xu and Oja, 1993 Xu L., Oja E. Randomized Hough transform (RHT): basic mechanisms,
    algorithms, and computational complexities CVGIP: Image Understanding, 57 (2)
    (1993), pp. 131-154 View PDFView articleGoogle Scholar Ying et al., 2018 Ying
    R., You J., Morris C., Ren X., Hamilton W.L., Leskovec J. Hierarchical graph representation
    learning with differentiable pooling Proceedings of the 32nd international conference
    on neural information processing systems, Curran Associates Inc., Red Hook, NY,
    USA (2018), pp. 4805-4815 View in ScopusGoogle Scholar Cited by (5) Image enhancement
    and microstructure characterization of energy dispersive X-ray spectroscopy images
    of blended cement pastes 2024, Expert Systems with Applications Show abstract
    A novel teacher–student hierarchical approach for learning primitive information
    2024, Expert Systems with Applications Show abstract SMR-RS: An Improved Mask
    R-CNN Specialized for Rolled Rice Stubble Row Segmentation 2023, Applied Sciences
    (Switzerland) Improved U-Net-Based Winter Wheat Crop Row Detection Method Using
    Texture Enhancement 2023, SSRN Image-based crop row detection utilizing the Hough
    transform and DBSCAN clustering analysis 2023, IET Image Processing View Abstract
    © 2022 Published by Elsevier Ltd. Recommended articles Multi-level feature re-weighted
    fusion for the semantic segmentation of crops and weeds Journal of King Saud University
    - Computer and Information Sciences, Volume 35, Issue 6, 2023, Article 101545
    Lamin L. Janneh, …, Yitong Yang View PDF Automated detection of Crop-Row lines
    and measurement of maize width for boom spraying Computers and Electronics in
    Agriculture, Volume 215, 2023, Article 108406 Xinyue Zhang, …, Shan Jiang View
    PDF The role of GNSS in the navigation strategies of cost-effective agricultural
    robots Computers and Electronics in Agriculture, Volume 112, 2015, pp. 172-183
    Francisco Rovira-Más, …, Verónica Sáiz-Rubio View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 4 Captures Readers: 10 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Expert Systems with Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Hierarchical graph representation for unsupervised crop row detection in
    images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sharma G.
  - Anand V.
  - Malhotra S.
  - Kukreti S.
  - Gupta S.
  citation_count: '0'
  description: The soybean, which is a fundamental component of worldwide agriculture,
    plays a crucial role in meeting the global need for protein and oil. Nevertheless,
    the spread of many diseases poses a significant risk to both the quantity and
    quality of soybean harvests. The timely and accurate categorization of diseases
    is crucial for the successful implementation of crop management strategies. This
    study aims to enhance the categorization of soybean diseases by addressing ten
    different disease classes using the ResNet50V2 deep learning model. In order to
    accomplish this objective, the Soybean Disease Leaf Image Classification Dataset
    has been employed for both the purposes of training as well as evaluating the
    model. The ResNet50V2 model's architectural depth and inclusion of skip connections
    augment its capacity to grasp complicated patterns present in leaf images, hence
    bolstering its efficacy in performing robust categorization. The experimental
    findings provide evidence for the effectiveness of our methodology since the model
    attains a notable training accuracy of 97% and validation accuracy rate of 96%
    when diagnosing illnesses in soybean plant leaves with a minimum loss of 0.05
    and 0.38. Moreover, this research endeavor makes a valuable contribution to the
    expanding domain of computer vision in the agricultural sector, facilitating the
    automation and enhancement of plant disease diagnosis processes.
  doi: 10.1109/SMARTGENCON60755.2023.10442288
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 3rd International Confer...
    DeepLeafNet: Multiclass Classification of Soybean Plant Leaves with ResNet50V2
    for Enhanced Crop Monitoring and Disease Detection Publisher: IEEE Cite This PDF
    Gunjan Sharma; Vatsala Anand; Sonal Malhotra; Sanjeev Kukreti; Sheifali Gupta
    All Authors 14 Full Text Views Abstract Document Sections I. Introduction II.
    Literature Review III. Proposed Methodology IV. Results and Discussion V. Conclusion
    Authors Figures References Keywords Metrics Abstract: The soybean, which is a
    fundamental component of worldwide agriculture, plays a crucial role in meeting
    the global need for protein and oil. Nevertheless, the spread of many diseases
    poses a significant risk to both the quantity and quality of soybean harvests.
    The timely and accurate categorization of diseases is crucial for the successful
    implementation of crop management strategies. This study aims to enhance the categorization
    of soybean diseases by addressing ten different disease classes using the ResNet50V2
    deep learning model. In order to accomplish this objective, the Soybean Disease
    Leaf Image Classification Dataset has been employed for both the purposes of training
    as well as evaluating the model. The ResNet50V2 model''s architectural depth and
    inclusion of skip connections augment its capacity to grasp complicated patterns
    present in leaf images, hence bolstering its efficacy in performing robust categorization.
    The experimental findings provide evidence for the effectiveness of our methodology
    since the model attains a notable training accuracy of 97% and validation accuracy
    rate of 96% when diagnosing illnesses in soybean plant leaves with a minimum loss
    of 0.05 and 0.38. Moreover, this research endeavor makes a valuable contribution
    to the expanding domain of computer vision in the agricultural sector, facilitating
    the automation and enhancement of plant disease diagnosis processes. Published
    in: 2023 3rd International Conference on Smart Generation Computing, Communication
    and Networking (SMART GENCON) Date of Conference: 29-31 December 2023 Date Added
    to IEEE Xplore: 28 February 2024 ISBN Information: DOI: 10.1109/SMARTGENCON60755.2023.10442288
    Publisher: IEEE Conference Location: Bangalore, India SECTION I. Introduction
    Agriculture serves as the primary prerequisite for meeting the global demand for
    food. The principal factors contributing to the discrepancy in agricultural commodity
    consumption and supply can be attributed to the loss of arable land and the expansion
    of people [1]. The soybean, commonly known as the “golden bean,” is a crucial
    agricultural commodity that plays a significant role in supplying necessary protein
    and oil for human and animal dietary needs. This particular factor assumes a pivotal
    position within the realm of agriculture, as it effectively tackles the global
    demand for sustenance [2]. Nevertheless, soybean fields are consistently confronted
    with the enduring challenge of plant diseases, which have the potential to detrimentally
    impact both crop yields and quality. These diseases pose a significant problem
    not only for agricultural practitioners but also have broader consequences for
    global food security and economic stability. The efficient management of many
    disorders necessitates precise and prompt categorization [3]. The integration
    of technology and agriculture has auspicious prospects. The use of computer vision
    and deep learning techniques in agricultural operations facilitates the automated
    identification and control of diseases, signifying a notable advancement in the
    realm of sustainable farming. The objective of this study is to enhance the classification
    of soybean diseases, specifically by differentiating between ten distinct disease
    groups. The primary tool utilized in our study is the fine-tuned ResNet50V2 model,
    a widely recognized deep-learning architecture renowned for its exceptional performance
    in the task of picture categorization [4]. In order to accomplish this objective,
    the Soybean Disease Leaf Image Classification Dataset has been employed, which
    is a comprehensive compilation of annotated soybean leaf pictures. This dataset
    serves as the fundamental resource for training and evaluating our model. The
    dataset comprises images depicting ten distinct categories of soybean illnesses,
    encompassing bacterial blight, brown spot, mosaic virus, yellow mosaic etc. The
    images exhibit superior resolution and are meticulously annotated, rendering them
    highly suitable for integration into machine learning (ML) [5] algorithms designed
    for the purpose of plant disease categorization. The depth and complicated design
    of this model, together with the use of skip connections, enable it to effectively
    identify minor abnormalities in leaf images that are indicative of disease. SECTION
    II. Literature Review Agriculture plays a pivotal role in the development and
    sustenance of India''s economy and society Within the field of agriculture, numerous
    systems have been suggested as potential solutions or mitigating measures to address
    prevailing issues. These systems leverage image processing techniques and various
    automated classification tools. The study conducted by [6] utilized a residual
    network with an attention module, incorporating a hybrid attention mechanism,
    to address the issue of residual error in a neural network. The objective was
    to recognize plant diseases using publicly available data sets from Plant Village,
    specifically focusing on apple and cherry crops as well as 10 other crop types
    including corn, grapes, and citrus. A comprehensive evaluation was conducted,
    testing the model on 60 different diseases. The results demonstrated the 92.08%
    accuracy. The authors of [7] developed a parallel pooling attention module employing
    ResNet50. They suggested the residual attention network framework to accurately
    diagnose four potato illnesses in the Plant Village dataset, achieving an accuracy
    of 93.86%. The CNN with multi-feature fusion was utilized by [8] to identify and
    classify 32 types of foliage in the Flavin collection and 189 types of foliage
    in the MEW 2014 database. The overall correct identification rates achieved were
    93.25% and 96.37% respectively. The process for classifying Anthracnose and Downey
    Mildew, which are diseases affecting watermelon leaves, was proposed by [9]. In
    this study, the median filter is employed for the purposes of noise reduction
    and segmentation. A neural network-based pattern recognition toolkit is employed
    for the purpose of classification. The accuracy of this study was determined to
    be 75.9% by analyzing the mean RGB color constituent. In a study conducted by
    [10], a technique utilizing CNN was presented for the classification and identification
    of soybean leaves. The utilization of CNN is employed in the process of backpropagation
    to train algorithms, hence enhancing accuracy. Consequently, the whole system
    yields improved outcomes characterized by heightened accuracy. To achieve the
    segmentation of the affected region, the process of texture segmentation is carried
    out by employing the technique of k-means clustering. The Google Net architecture,
    initially proposed by the authors referenced in [11], was trained on a dataset
    consisting of 550 picture samples of soybean leaflets. These samples were categorized
    into two classes: unhealthy and healthy. The unhealthy class was further divided
    into three subclasses, namely septoria brown spot, bacterial blight, and frog
    eye leaf spot. The training process employed a deep TL approach. The study employed
    a five-fold cross-validation technique to identify three classes of unhealthy
    images and one class of healthy images. The chosen already trained Google Net-CNN
    framework achieved an accuracy of 96.25%. In their study, the authors of reference
    [12] opted to substitute the conventional convolution technique with EfficientNet,
    a method that effectively decreases the number of parameters and processing expenses.
    The study utilized data obtained from the Indian Institute of Soybean Research
    and employed web scraping techniques to collect information on a diverse range
    of plant varieties and disease classifications. Two models were developed: one
    utilizing a CNN and the other using EfficientNetB0. The accuracy rates of illness
    characterization that these models attained were 84% for the CNN-based approach
    and 90% for the EfficientNetBO model. This research done by [13] addresses the
    challenge posed by a limited dataset pertaining to soybean leaf diseases through
    the creation of a synthetic dataset. In order to accurately identify soybean leaf
    diseases within intricate environments, the authors propose the utilization of
    a multi-feature fused Faster R-CNN approach. The model demonstrated a notable
    average mean accuracy of 83.34% when evaluated on an actual-life dataset. The
    present study introduces a refined ResNet50V2 [14] model that has been pre-trained
    for the purpose of classifying various diseases affecting soybean plant leaves
    into 10 distinct categories. Utilizing the Soybean Disease Leaf Image Classification
    Dataset, the model''s performance has been trained and tested. SECTION III. Proposed
    Methodology A. Dataset Employed The Soybean Disease Leaf Image Classification
    Dataset [15] has been employed to conduct this categorization task. The dataset
    comprises a collection of images depicting damaged soybean plant leaves. The dataset
    comprises images depicting ten distinct categories of soybean illnesses, encompassing
    bacterial blight (BLt), brown spot (BS), mosaic virus (MV), southern blight (SB),
    sudden death syndrome (SDS), yellow mosaic (YM), Crestamento (CR), Ferrugen (FR),
    Powdery Milddew (PMd) and Septoria (SP). The images exhibit a commendable level
    of resolution and are appropriately annotated, rendering them very suitable for
    integration into machine-learning frameworks aimed at plant disease categorization.
    Fig. 1 displays the sample images from the dataset. Fig. 1. Sample images of soybean
    plant leave diseases [15] Show All Fig. 2 displays the distribution of images
    class-wise. Fig. 2. Distribution of images class-wise in dataset Show All B. Image
    Pre-Processing All the images are pre-processed first before they are fed to the
    network. This helps in reducing the noise and any type of discrepancy in the images.
    The initial phase in the data processing pipeline entails the loading and preparation
    of the image data. The images undergo a resizing step to achieve a consistent
    size of 224×224 pixels. Then these images pixel values are rescaled between 0
    and 1. They are then organized into batches, with each batch. The images are treated
    as RGB color [16] images for subsequent processing. The assignment of labels is
    derived from the hierarchical arrangement of the directory structure. These labels
    are then transformed into a one-hot encoding format, which aids in the process
    of categorical classification. During the training process, the data is randomly
    shuffled to introduce randomization. Additionally, a random seed is established
    to ensure reproducibility. Data Augmentation is applied to increase the images
    [17]. The images are randomly flipped horizontally and vertically to provide a
    variety of images, this will help in effective feature training of the images.
    Fig. 3 gives the details of the number of images before and after augmentation
    is applied to the images. A segment comprising 20% of the data is allocated for
    the purpose of validation, and the datasets are thereafter partitioned into distinct
    subsets for training and validation. The datasets mentioned above are commonly
    employed for the purpose of training deep learning models in order to accomplish
    picture classification tasks. The training dataset is utilized for training the
    model, while the validation dataset is employed to evaluate the model''s performance
    during the training process. Fig. 3. Class-wise image analysis after augmentation
    Show All C. Proposed Methodology This work concentrates on the effective categorization
    of diseases related to soybean plant leaf disease into ten main classes. The approach
    presented for the classification of soybean plant leaf diseases is organized into
    a systematic sequence of clearly outlined procedures [18]. Fig. 4 showcases the
    proposed framework for this categorization study. Fig. 4. Proposed framework Show
    All The method begins with the collecting of data, namely a dataset consisting
    of images of soybean leaves. Following that, the gathered photos undergo a rigorous
    preprocessing phase. In this stage, the photos undergo a process of resizing to
    a standardized resolution of 224×224 pixels and are then rescaled to ensure uniformity.
    In order to enhance the model''s generalization and incorporate unpredictability
    [19], data augmentation techniques such as rotations, flips, and brightness modifications
    are employed. After undergoing preprocessing, the dataset is subjected to shuffling,
    resulting in a randomized order of the data points. Subsequently, the dataset
    is divided into two distinct subsets, namely the training set and the validation
    set. This partitioning is carried out in an 8:2 ratio, with the training set including
    80% of the data and the validation set containing the remaining 20%. The process
    of partitioning facilitates both the training of models and the evaluation of
    their performance. The subsequent step in the process is the feature extraction
    phase, in which a pre-existing CNN model, specifically ResNet50V2, is utilized
    to extract elevated-level features from the images that have been preprocessed
    and augmented. Following this, the model undergoes fine-tuning by integrating
    supplementary layers such as batch normalization [20], dropout layers to address
    overfitting concerns, and dense layers for the purpose of classification. The
    model is subsequently trained using the training dataset and evaluated by testing
    it on a separate validation set in order to measure its performance. Finally,
    the model utilizes the acquired features to categorize soybean leaf images into
    the pre-established ten disease categories. In brief, this proposed methodology
    involves a thorough data preparation stage, followed by feature extraction using
    a pre-trained model. The extracted features are then fine-tuned, and the model
    is trained and tested. Finally, the acquired feature knowledge is utilized for
    the purpose of accurately classifying soybean leaf diseases. D. Experimental Setup
    Within the framework of the study, the computational capabilities provided by
    Google Colab''s complimentary access to GPU resources is utilized [21]. The primary
    objective of this research centered on the field of image processing. In pursuit
    of this objective, several libraries sourced from Keras are effectively included
    in our experimental framework. The optimization of model performance in training
    circumstances involves the utilization of specified hyperparameters. The selection
    of a batch size of 64 was made with the intention of striking a delicate balance
    between computational efficiency and the model''s ability to converge [22]. During
    the optimization process, a learning rate of 0.0001 was initially utilized to
    regulate the magnitude of each step taken. The choice to utilize the RMSprop optimizer
    was driven by its adjustable learning rate properties, which mitigate the potential
    for overshooting during the optimization iteration. The ‘categorical_crossentropy’
    loss function is utilized due to its suitability for handling many classes. This
    loss function effectively measures the disparities between the predicted probabilities
    and the actual class probabilities. In order to optimize model convergence, the
    training process is run for 25 epochs, hence enabling the model to acquire significant
    insights from the available data. To mitigate the possible problem of overfitting
    during the training of the model, a dropout rate of 0.5 is incorporated. This
    regularization strategy is implemented to safeguard against the model''s tendency
    to learn irrelevant patterns or noise present in the training data. The hyperparameters
    and their corresponding information are concisely presented in Table I. The robustness
    of this experimental setup, which includes the utilization of GPU resources, a
    diverse dataset, and carefully selected hyperparameters, is essential for guaranteeing
    the effectiveness and generalizability of our model in the specific task of classifying
    soybean plant leaf diseases within real-world agricultural settings. Table I.
    Model hyperparameters utilized during the training of the model SECTION IV. Results
    and Discussion After training of the model, the performance is evaluated in terms
    of accuracy and loss. The model is trained for 25 epochs during which the accuracy
    and loss for training and validation are recorded. Table II presents a comprehensive
    overview of the training and validation performance of a machine learning model
    during several training epochs. Table II. Performance matrix during training of
    the model The evaluation of these metrics is essential in order to measure the
    learning progress and generalization capabilities of the model as it iteratively
    processes the information. During the initial stage, referred to as Epoch 1, the
    model commences with a TL value of 1.44 and a TA rate of 63.15%. Concurrently,
    the VL exhibits a comparatively lower value of 0.49, accompanied by a VA of 86.25%.
    The initial stage of training is indicative of the model''s performance at the
    outset. As the training program advances, a number of significant patterns become
    apparent. Significantly, the TL exhibits a consistent decline, starting at 1.44
    in Epoch 1 and gradually reducing to 0.05 by Epoch 25. This decrease in error
    suggests that the model is improving its ability to minimize the discrepancy between
    its predictions and the observed training data. The TA demonstrates a significant
    improvement across the epochs, rising from an initial value of 63.15% to a final
    value of 97.85%. This increase in accuracy indicates the enhanced capability of
    the model to accurately categorize the training data. On the other hand, the evaluation
    of the model''s generalization capacity beyond the training data involves closely
    monitoring the VL and VA on the validation dataset. The validation loss exhibits
    a slow decline, decreasing from 0.49 to 0.39. This trend indicates that the model
    is progressively improving its ability to minimize errors when presented with
    new, unseen data. The VA demonstrates a positive trajectory, commencing at 86.25%
    and progressively increasing to 96.00% by the 25th epoch. The observed increase
    in VA suggests that the model is becoming more proficient in accurately categorizing
    data that it has not encountered before, hence showcasing its capacity for strong
    generalization. Fig. 5. Graphical presentation of model accuracy Show All Fig.
    5 gives the graphical presentation for model accuracy. From the fig, it can be
    seen that the TA is increasing swiftly whereas the VA value drops in the start
    then it starts increasing. Fig. 6 displays the model loss presentation. From fig.
    6 it can be seen that both the losses are diminishing fluently. Fig. 6. Graphical
    presentation of model loss Show All In brief, the table gives an overview of the
    training and validation performance of the model during several training epochs.
    The observed decline in TL and rise in TA indicate enhanced learning performance
    on the training dataset. Conversely, the diminishing VL and escalating VA underscore
    the model''s rising capacity to generalize and make accurate predictions on novel,
    unidentified information. The observed progressive enhancement highlights the
    model''s efficacy in accurately categorizing soybean plant leaf diseases using
    the available dataset. A. Visualization of Results This section displays the classification
    results achieved after the training and testing of the model. Fig. 7 showcases
    the results achieved during testing of the model. Fig. 7. Visualization of classification
    results Show All From fig. 7, it can be noticed that the proposed ResNet50V2 model
    is performing well and categorizing the images correctly. SECTION V. Conclusion
    In summary, by employing transfer learning with the ResNet50V2 architecture to
    create a strong model, our study has effectively tackled the crucial problem of
    classifying illnesses of soybean plants'' leaves. The foundation for more precise
    and effective disease detection and management in soybean crops has been established
    by this study by compiling a large dataset of images of soybean leaf disease and
    categorizing it into ten different groups. With low loss, our model demonstrated
    remarkable performance, with a validation accuracy of 96% and a training accuracy
    of 97.85%. The model''s exceptional performance was largely attributed to the
    use of the RMSprop optimizer, a batch size of 64, and 25 training epochs. The
    application of a categorical cross-entropy loss function improved the model''s
    capacity to accurately classify the various diseases that affect soybean leaves.
    In addition to advancing agriculture by offering a useful tool for diagnosing
    and managing soybean diseases, this research shows how deep learning and transfer
    learning approaches may be applied to solve challenging real-world issues. In
    order to handle a larger spectrum of plant diseases, the model suggested in this
    study can be expanded upon and improved in subsequent research, resulting in more
    effective and sustainable agricultural methods. Authors Figures References Keywords
    Metrics More Like This A Two-stage Approach for Plant Disease Classification Based
    on Deep Neural Networks and Transfer Learning 2022 12th International Conference
    on Electrical and Computer Engineering (ICECE) Published: 2022 Crop Recommendation
    using Machine Learning and Plant Disease Identification using CNN and Transfer-Learning
    Approach 2022 IEEE Conference on Interdisciplinary Approaches in Technology and
    Management for Social Innovation (IATMSI) Published: 2022 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 3rd International Conference on Smart Generation Computing, Communication
    and Networking, SMART GENCON 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'DeepLeafNet: Multiclass Classification of Soybean Plant Leaves with ResNet50V2
    for Enhanced Crop Monitoring and Disease Detection'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mukherjee D.
  - Das A.
  - Ghosh N.
  - Nanda S.
  citation_count: '0'
  description: Agriculture serves as the backbone of India's economy, playing a pivotal
    role in shaping the nation's financial well-being. The presence of weeds greatly
    reduces the nutrient content of the soil. The proposed solution in this paper
    entails the utilization of a technology-driven approach for crop monitoring and
    weed management. The device consists of wireless sensor nodes spread throughout
    the field and a mobile weed detection and weedicide spraying unit. This innovative
    weed detection and weedicide spraying unit will be installed in an autonomous
    vehicle. The bot will consist of a camera module that will transfer the image
    data to a Deep learning model hosted on a local machine using WiFi. Using the
    results of the Deep learning model targeted weedicide spraying is achieved. The
    WSN network will be used to collect physical information of the field. This will
    be useful for smart irrigation. Using this system the random use of weedicides
    used can be checked. The farmer can remotely monitor the fields and efficiently
    irrigate them. Thus preventing the soil from degradation.
  doi: 10.1109/ELEXCOM58812.2023.10370051
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    Real Time Agricultural Monitoring with Deep Learning Using Wireless Sensor Framework
    Publisher: IEEE Cite This PDF Dibyarup Mukherjee; Avik Das; Nabamita Ghosh; Sarita
    Nanda All Authors 1 Cites in Paper 28 Full Text Views Abstract Document Sections
    I. Introduction II. System Modeling III. Working IV. Results and Discussion V.
    Conclusion Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: Agriculture serves as the backbone of India''s economy, playing a pivotal
    role in shaping the nation''s financial well-being. The presence of weeds greatly
    reduces the nutrient content of the soil. The proposed solution in this paper
    entails the utilization of a technology-driven approach for crop monitoring and
    weed management. The device consists of wireless sensor nodes spread throughout
    the field and a mobile weed detection and weedicide spraying unit. This innovative
    weed detection and weedicide spraying unit will be installed in an autonomous
    vehicle. The bot will consist of a camera module that will transfer the image
    data to a Deep learning model hosted on a local machine using WiFi. Using the
    results of the Deep learning model targeted weedicide spraying is achieved. The
    WSN network will be used to collect physical information of the field. This will
    be useful for smart irrigation. Using this system the random use of weedicides
    used can be checked. The farmer can remotely monitor the fields and efficiently
    irrigate them. Thus preventing the soil from degradation. Published in: 2023 International
    Conference on Electrical, Electronics, Communication and Computers (ELEXCOM) Date
    of Conference: 26-27 August 2023 Date Added to IEEE Xplore: 01 January 2024 ISBN
    Information: DOI: 10.1109/ELEXCOM58812.2023.10370051 Publisher: IEEE Conference
    Location: Roorkee, India SECTION I. Introduction India is a strong agricultural
    sector of the world and it makes up a significant portion of India''s gross domestic
    product (GDP). Annually India has incurred an agricultural loss of over $910 million
    solely due to the detrimental effects of weeds, severely impacting crop yields
    and economic productivity nationwide [1]. In traditional farming practices, the
    evaluation of weeds often requires the employment of expert individuals who meticulously
    inspect each plant line by line. This process is highly labor-intensive and time-consuming.
    Weeds and improper irrigation greatly diminish the nutrient quality of the soil.
    Consequently, this increases the price of food and restricts access to proper,
    nutritious food for the masses. Cheonsong Hu et al designed a robotic weeding
    system which uses precision spraying on weeds combining stereo cameras, inertial
    measuring units and multiple linearly actuating spray nozzles for herbicide spray
    on weeds. They used an NVIDIA Jetson AGX Xavier as an onboard computer [2]. Tien-cao-hoang
    et al proposed a wireless sensor network which lets users monitor environmental
    data for agriculture [3]. Koushik et al developed a static wireless sensor network
    to display the humidity and temperature. The nodes communicate with each other
    using the ESP NOW protocol [4]. Abdullah et al proposed an IoT based system for
    remote soil monitoring using static nodes. These nodes are capable of sensing
    the pH level, moisture, and soil temperature. These nodes used Bluetooth as a
    means of communication [5]. Xialong Wu et al developed a framework that performs
    naive Bayes filtering, 3D direct intra- and inter-camera visual tracking, and
    predictive control for precision weed removal [6]. S. Ganesh Sundaram et al developed
    machine learning based on CNN plus Xception algorithm and compared the accuracy
    of augmented and not augmented pictures. They used bounding boxes for detection
    of weed [7]. C. T. Selvi et al used a deep learning process for detection of weeds
    which are overlapping with the crops and detect them in real time using CNN [8].
    D. D. K. Rathinam et al designed a WSN network with bio sensor, temperature sensor
    and soil moisture sensor and used a mobile application for monitoring purposes
    [9]. M. P. Arakeri et al developed an automated robot for detection of onion weed
    and spraying weedicides. The image processing algorithm is used to detect the
    weeds [10]. McAllister et al worked on predictive weed growth modeling for predicting
    field conditions evolution and a bot was deployed in the field to monitor herbicide-resistant
    weeds [11]. Philipp et al used geometric patterns along with images consisting
    of 4 channels, RGB and near infra-red (NIR) to prevent the loss in crop-weed detection
    [12]. There exists systems which implements weed detection using expensive hardware.
    These systems were available for targeted type of crop and not universal. The
    existing systems doesn''t integrate the use of deep learning and wireless sensor
    network together. The major contribution of the devised system is to make an Autonomous
    Crop monitoring and management system. The proposed system is named as Autonomous
    Crop Monitoring System (ACMS). The system integrates real time weed detection
    with smart irrigation utilizing low cost hardware. In addition automated selective
    spraying of the weedicides at affected sites reduces the maintenance cost of the
    crop field. This leads to a reduction in soil pollution. Moreover the system monitors
    the temperature, humidity, and soil moisture content of the field under study
    and smart irrigation systems can be implemented. The designed prototype utilizes
    a 3-axis arm for capturing of images, which will be further processed for simultaneous
    weed detection. The system is compact and will detect the presence of weeds using
    the camera module. SECTION II. System Modeling A. Overview The proposed system
    incorporates an autonomous bot equipped with a sophisticated robotic arm that
    autonomously navigates through an agricultural field, following a predetermined
    path. As it traverses the path, the bot captures high-resolution images of the
    plants, which serve as input data for the software system. This software system
    utilizes advanced algorithms to analyze the images and activate the precise spraying
    mechanism. A network of n ESP32 devices has been strategically deployed to establish
    effective communication and data exchange. These devices operate in a simplex
    communication mode, allowing them to continuously monitor and gather essential
    data regarding soil characteristics. This data is then promptly reported to the
    farmer, enabling them to make informed decisions and enhance their crop management
    practices. By integrating cutting-edge technology and intelligent software, this
    system offers significant benefits in the agricultural domain. The autonomous
    bot with the help of robotic arm, ensures accurate and efficient plant monitoring,
    while the software''s advanced algorithms enable precise control of the spraying
    mechanism. The ESP32 devices, operating in a simplex communication mode, establish
    a reliable network that continuously monitors the soil characteristics, providing
    valuable insights to the farmer for enhanced crop management. B. Hardware Modeling
    The proposed system consists of a mobile agricultural robot equipped with a robotic
    arm that houses an ESP32 camera module. The robotic arm has 3 degrees of freedom
    and uses MG996 servos for actuation. The camera box serves as the end effector
    of the robotic arm, and images will be captured at the base of the plants. The
    robot navigates through uneven terrain using wheel encoders and a 3-axis gyroscope
    (MPU 6050) for self-guidance. As our main battery power supply we are using 3
    18650, 3.7V batteries providing a total of 11.1V. This voltage is directly supplied
    to the 12V input of the L298N. The 5V output of the L298N is used to a power to
    all microcontrollers and the attached sensor viz. wheel encoder, Gyroscope (MPU6050)
    and the 3 servo motors controlling the robotic arm. Additionally, it will also
    power the ESP32 camera module attached in the end effector of the robotic arm.
    The L298N motor driver is also responsible for powering the drive motors. Fig.
    2 shows the structural design of the bot which houses all the above mentioned
    components. Fig. 1. Real-time capturing of image by robot Show All Fig. 2. Structural
    design of the robot which is capable to traverse through the fields Show All Fig.
    3. WSN nodes over the observed plot Show All The WSN network, each sender node
    is equipped with a DHT11 digital humidity and temperature sensor, as well as a
    YL69 capacitive soil moisture sensor [15]. Communication between the nodes is
    established using the ESP NOW protocol. The sender nodes send data to the central
    node for further action. Overall, the system integrates a mobile agricultural
    robot with a robotic arm housing an ESP32 camera module. It utilizes servo actuators,
    power supplies, sensors, motor drivers, and an ESP32 network for efficient operation
    in agricultural environments. Fig. 3 depicts the distribution of the sensor nodes
    over the agricultural field. The individual sensor nodes consisting of all the
    required sensors are depicted in Fig. 5. C. Software Modeling A custom dataset
    was made with 1500 images containing weeds and another 1500 images with weed free
    areas. We obtained the dataset by clicking pictures. These images were then expanded
    to 5500 images of weeds and no weeds, using image augmentation. The total dataset
    for both the classes was split into training, validation, and testing datasets
    in the ratio of 7:2:1 respectively. Three architectures were used to train the
    ACMS and then they were compared to find the best suiting architecture. Feature
    extraction was performed on these models. All but the top layers of the pre-trained
    models were imported. This method is called transfer learning. A customized fully
    connected convolutional neural network layer was defined and the model was trained
    on the weeds dataset. The Resnet50 model [13] was used in PyTorch frameworks and
    the Xception model [14] was trained in the Tensorflow framework. The custom model
    was built using Sequential in the Keras library. Fig. 4. Circuit diagram of autonomous
    bot and node of wireless sensor network Show All Fig. 5. Wireless sensor node
    setup Show All SECTION III. Working The proposed system involves the development
    of a weed monitoring bot that uses ESP-32 Cam module and ESP-32 microcontrollers
    as its main hardware components. A simplified circuit diagram of the bot along
    with a single WSN node is depicted in Fig. 4. The bot will utilize a deep learning
    algorithm to detect weeds. Before the bot''s operation, it will be deployed manually
    to identify the plant positions in the field, and the data will be stored in a
    matrix form where each plant will become an element of the matrix. During the
    bot''s operation, it will stop near each plant according to the stored path and
    examine the plant. The robotic arm will take an image of the plant near the base,
    and this image will be sent to the Deep learning model hosted on the local machine
    to detect weed presence. Based on the model''s results, the spray nozzles in the
    robotic arm will be turned on to spray the required weedicide. After the above
    processes, the bot will move to the next plant, and the data will be stored in
    the matrix. This will continue for each plant until the last plant in the matrix.
    The pictorial representation of the bot functioning where each plant is a matrix
    element is depicted in the part A of the Fig. 6. Simultaneously, data like temperature,
    humidity and soil moisture from the WSN nodes will be collected based on which
    the farmer can take actions and irrigation is done. A flowchart of the designed
    system is mentioned in Fig. 8. The robotic arm housing the ESP32 cam module will
    be functioning based on preset angles encoded into it. As the bot moves along
    rows of crops, it stops in the presence of a plant. The ESP32 cam in the robotic
    arm enclosure captures images of the base of the plant. The captured image is
    then sent to the deep learning model housed in the local machine for further pre-processing
    before it is passed for classification. The deep learning model used in the proposed
    system is designed to perform binary image classification using different python
    libraries and frameworks. It classifies between weed and no weed. We have trained
    different models and compared their results based on different evaluation parameters.
    The first step in the process is creation of a dataset to train the model. The
    dataset was made by clicking 1500 pictures of weeds at plant base and no weed
    at the base. The raw images taken were then augmented for increasing the size
    of the dataset. The augmentation was done using python libraries and torchvision
    library of the pytorch framework. Each image is accessed and random flips, rotations,
    color jitter, and random resized crops are applied on that image. We use the torchvision
    for creating a pipeline of transforms. The transformations specified include random
    horizontal and vertical flips, random rotation, color jitter (adjusting brightness,
    contrast, saturation, and hue), and random resized crop (cropping and resizing
    the image to a specified size). This process will continue in a loop until the
    target number of images are reached. The proposed system uses the transfer learning
    method and convolutional neural network. The data is pre-processed and the images
    are resized to 224 × 224 pixels and then converted to tensor. It then normalizes
    the tensor values of the image. It subtracts the mean values [0.485, 0.456, 0.406]
    from each channel and divides by the standard deviation values [0.229, 0.224,
    0.225] for each channel. The model is then trained with the Resnet50 pre-trained
    model, the last fully connected layer of the model is retrieved and replaced with
    a linear layer whose output is two numbers of classes and the input is the number
    of the feature. In Xception model the input image size is set to (299, 299, 3).
    The input images were then normalised by dividing the value of each pixel by 255,
    in data pre-processing. Feature extraction was done on the top layer of Xception
    model. After the base layers of Xception model a global average pooling layer
    was put. A fully connected neural network layer consisting of 1024 neurons was
    then placed which is connected to the output neuron through a Rectified Linear
    Unit activation function finally the output was obtained after passing through
    a sigmoid activation layers of classes and the input is the number of the features.
    For data pre-processing in hybrid CNN model, the input image size is set to (256,
    256, 3). The initial 3 layers consist of 3 loosely connected layers consisting
    of 16, 32 and 16 neurons respectively. There is a Max pooling in between each
    of these layers. Then there is a flattening layer followed by a fully connected
    layer of 256 neurons with ReLU activation function. Finally the output is obtained
    after a sigmoid activation layer after the final neuron. All the models are trained
    for 20 epochs and the result and different evaluation parameters are obtained
    for comparison. Fig. 6. Technology stack of the working system Show All In Fig.
    6 part A, the bot is shown, moving in the fields and surveying the crops. The
    robot stop at pre-planned locations and ESP32 Cam take the images and send it
    to the local host. The local host runs the deep learning algorithm to classify
    real time images obtained from the bot on the field. Then the feedback is used
    by the microcontroller to turn on the pump to spray weedicides, if weed are detected.
    This is depicted in Fig. 6 region B. In Fig. 6 region C, shows the wireless sensor
    network distribution of the field being studied. The WSN system will consist of
    a set of n number of nodes stationed at n different sectors of the field. The
    number of nodes are scalable. The number of nodes and sectors depends on the size
    of the agricultural field size. Each node consisting of ESP32 as the microcontroller
    is connected with DHT11 and YL-69 soil moisture sensor for sensing parameters
    like temperature, moisture and soil moisture. The central node receives the data
    from all the other nodes via an ESP-now connection. Based on the data received
    the farmer is alerted about the field conditions. The soil moisture content information
    is used for turning the pump on for that sector to be irrigated. The information
    will be displayed on the web dashboard on the farmer''s smart device and also
    on a display system connected. to the receiver node. When the soil moisture sensor
    detects that the soil moisture is below 30% it will turn the pump on and that
    part will be irrigated until the soil moisture content reaches 75%. If the sector
    has high temperature above 42 degree Celsius then the pump will continue till
    the moisture content is 90%. For better power management the system will check
    the values after every 30 minutes during this time, the node will go to sleep
    hence saving the energy and extending battery life. This is explained in the flowchart
    in Fig. 8. The multiple technologies simultaneously working in the system is collectively
    summarized in Fig. 6. SECTION IV. Results and Discussion The evaluation parameters
    of 3 architectures were compared. The Xception model gave the highest accuracy
    of 99.9% followed by CNN with 99.01% and resnet50 with 96.80%. The table 1 shows
    the comparison of accuracy, precision and Recall of the 3 models. It was inferred
    from the table that the Xception model was best for the evaluation parameters.
    The Xception model took roughly 7 minutes per epoch when trained in an online
    environment with GPU Tesla T4. The resnet50 model took roughly 4 minutes per epoch.
    One of the main objectives of this paper is the real time detection of weeds and
    spraying of weedicides. In order to reduce wastage and overuse of weedicides,
    it is critical for the machine learning model to not spray weedicides in places
    where weeds are not present. The Xception model gave the best results, but the
    interface time is quite long (9s). Fig. 7. Dashboard showing temperature, humidity
    and soil moisture Show All Fig. 8. Flowchart of the designed system Show All Fig.
    9. Confusion matrix of different models Show All Table I. Comparison of performance
    of ACMS using different models Whereas for the hybrid CNN model, detected weeds
    in place of no weeds in only in 3 cases as shown in Fig. 9, this is better than
    the RESNET50 model. For the CNN model the inference time is only 166ms, which
    is much better than the Xception model. From confusion matrix of Fig. 9 we get
    the parameters of table 1. Table 2 shows the temperature, humidity, and soil moisture
    data of the field under study for different times of the day. The incoming data
    from all the nodes are tabulated here. In the morning as the soil moisture was
    high and the temperature was low, the water pump for irrigation was switched on.
    During the afternoon, in sectors 1 and 3, the soil moisture was below 30% and
    the temperature was above 42°C, the pump in these sectors will be switched on.
    According to the flowchart in Fig. 8, the pump will remain switched on till the
    soil moisture reaches 90%. Whereas, in sector 2, as soil moisture was 50%, the
    pump was in OFF state, in spite of the temperature being 43°C. During the evening,
    as soil moisture and temperature were outside the preset threshold and pump remained
    in the off state. Similarly at night, as the soil had adequate moisture content
    and the temperature being low, they remained off. The data of a particular date
    was taken and displayed on a web dashboard as shown in Fig. 7. The required set
    up for no weed and weed detection is shown in Table 3. When the bot moves and
    captures an image as seen in the first setup of Table 3, the image is correctly
    classified as weeds. For the second setup the captured image is correctly classified
    as no presence of weed. Table II. Node data received on 29th May, 2023 at Baranagar,
    West Bengal, India Table III. Results after testing the model in real time The
    deep learning model was giving wrong results because of the dataset. This was
    due to the dataset having very few training images. There was high bias present
    due to which overfitting occurred, to overcome this the dataset was augmented.
    While rotating the car, it rotates more than or less than the specified angle.
    Also the MPU 6050 sometimes loses calibration. This was mainly due to the SCL
    and SDA wires not being in proper contact throughout the operation of the bot.
    To overcome the above challenges some of the remedial strategies are suggested.
    In order to ensure the car only rotates the specified angle, we will be using
    a PID controller to correct the error in the rotation. Making a PCB with voltage
    regulators to appropriately supply power to each of the components. This will
    also help us to reduce the number of wires. SECTION V. Conclusion The proposed
    Autonomous Bot for precision farming is a promising solution to address the challenges
    faced in traditional farming. The use of inexpensive hardware, like ESP-32 Cam
    module and ESP 32 dev kit makes the system cost-effective and easy to implement.
    The deep learning algorithms for weed detection will reduce the need for manual
    labor, resulting in increased efficiency and accuracy. By using limited amounts
    of weedicides, the system will not only save resources but also protect the environment
    from harmful chemicals. The system''s ability to detect the location of the plant
    in the field and store data in matrix form will aid in autonomously guiding the
    bot throughout the field during operation. From the data acquired after training
    the 3 deep learning models. The Xception model delivered the best accuracy but
    the time to train the model and get an output was significantly longer. The CNN
    model took less time than RESNET 50 and accuracy was also better. Further testing
    and validation on different types of weeds may be required to ensure its effectiveness.
    Overall, the proposed Autonomous Bot for precision farming has the potential to
    revolutionize the agriculture sector in India and improve food security. The use
    of deep learning algorithms for weed detection will not only save time and labor
    but also boost productivity and yield in a sustainable and eco-friendly manner.
    SECTION VI. Future Scope The study can be extended to the use of solar powers
    to make it sustainable and also remove minimal human interaction. The proposed
    system can also be used to implement disease detection and spraying of weedicides.
    The system can be further improved to spray the required amount of weedicide according
    to the amount of weed present in the vicinity of the inspected plant. We plan
    to make a revised prototype that will be able to tackle the undulating terrain
    of the agricultural fields. Authors Figures References Citations Keywords Metrics
    More Like This Deep Learning with Wireless Sensor Network Platform for Multimedia
    Data Modeling 2023 Annual International Conference on Emerging Research Areas:
    International Conference on Intelligent Systems (AICERA/ICIS) Published: 2023
    Monitoring of soil parameters for effective irrigation using Wireless Sensor Networks
    2014 Sixth International Conference on Advanced Computing (ICoAC) Published: 2014
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE International Conference on Electrical, Electronics, Communication
    and Computers, ELEXCOM 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Real Time Agricultural Monitoring with Deep Learning Using Wireless Sensor
    Framework
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Maji S.
  - Vinay V.K.
  - Kumari S.
  - Banthia V.
  - Neerugatti V.
  citation_count: '0'
  description: The cotton crop certainty identification system is a computerized tool
    designed to aid farmers and agricultural experts in detecting and diagnosing diseases
    in cotton crops. This system uses image processing techniques to analyze images
    of cotton leaves and identify any signs of illness. The system also utilizes machine
    learning algorithms to classify the disease based on the symptoms observed, helping
    farmers to determine the appropriate treatment or management practices. A system
    is a valuable tool in increasing the efficiency and accuracy of disease detection
    in cotton crops, leading to higher crop yields and better overall crop health.Here,
    the data will be collected as a large dataset of images of cotton crops, which
    is then used to train a deep learning model. Once the model is trained, it can
    be used to accurately identify cotton crops in new images. This type of system
    has the potential to greatly improve the efficiency and accuracy of cotton crop
    monitoring and analysis.
  doi: 10.1109/ICCCNT56998.2023.10306483
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 14th International Confe...
    Cotton Crop Certainty Identification Using Deep Learning Techniques Publisher:
    IEEE Cite This PDF Supriti Maji; Vura KSMSC Vinay; Sonam Kumari; Vardhman Banthia;
    Vikram Neerugatti All Authors 53 Full Text Views Abstract Document Sections I.
    INTRODUCTION: II. LITERATURE SURVEY III. METHODOLOGY: IV. IMPLEMENTATION: V. RESULTS
    AND DISCUSSION Show Full Outline Authors Figures References Keywords Metrics Abstract:
    The cotton crop certainty identification system is a computerized tool designed
    to aid farmers and agricultural experts in detecting and diagnosing diseases in
    cotton crops. This system uses image processing techniques to analyze images of
    cotton leaves and identify any signs of illness. The system also utilizes machine
    learning algorithms to classify the disease based on the symptoms observed, helping
    farmers to determine the appropriate treatment or management practices. A system
    is a valuable tool in increasing the efficiency and accuracy of disease detection
    in cotton crops, leading to higher crop yields and better overall crop health.Here,
    the data will be collected as a large dataset of images of cotton crops, which
    is then used to train a deep learning model. Once the model is trained, it can
    be used to accurately identify cotton crops in new images. This type of system
    has the potential to greatly improve the efficiency and accuracy of cotton crop
    monitoring and analysis. Published in: 2023 14th International Conference on Computing
    Communication and Networking Technologies (ICCCNT) Date of Conference: 06-08 July
    2023 Date Added to IEEE Xplore: 23 November 2023 ISBN Information: ISSN Information:
    DOI: 10.1109/ICCCNT56998.2023.10306483 Publisher: IEEE Conference Location: Delhi,
    India SECTION I. INTRODUCTION: Cotton is one of the most important cash crops
    grown worldwide, and it plays a vital role in the global textile industry. However,
    cotton crops are susceptible to various diseases, significantly impacting crop
    yield and quality. Identifying the defect at an early stage is crucial in preventing
    it from spreading and causing irreparable damage to the crop. Traditionally, farmers
    have relied on manual observation to identify diseases in cotton crops, which
    is time-consuming and subjective. To address this challenge, researchers and experts
    have developed computerized tools and systems to aid in the identification and
    classification of cotton crop diseases. The cotton crop certainty identification
    system is one such tool that uses advanced technologies such as image processing
    and machine learning algorithms to analyze images of cotton leaves and identify
    any signs of disease. This system provides farmers with a more accurate and efficient
    method of detecting and diagnosing diseases in cotton crops, which can help them
    make informed decisions about appropriate treatment and management practices.
    The development of this system represents a significant step forward in the management
    of cotton crop diseases, which can lead to higher crop yields, better crop health,
    and ultimately, increased profitability for farmers. SECTION II. LITERATURE SURVEY
    [1] Naseer, N., Haider, M. S., Ashfaq, U. A., & Abbas, G. The review discussed
    the various diseases that affect cotton crops, including fungal, bacterial, and
    viral diseases. The authors provided an overview of the symptoms of each disease
    and the methods used to detect them. For example, they discussed using remote
    sensing techniques, such as hyperspectral imaging and multispectral imaging, to
    identify cotton diseases based on changes in plant reflectance. [2] Zhang, X.,
    Liu, S., Zhao, G., & Li, Y. This paper provided a literature review on cotton
    leaf disease identification based on image processing and machine learning techniques.
    The authors highlighted the importance of early and accurate disease detection
    to effectively manage and control cotton diseases, which can significantly reduce
    crop yield and quality. [3] Goyal, V., & Singh, A. The review discussed various
    machine learning algorithms used for cotton disease detection, including support
    vector machines (SVM), decision trees, random forests, and artificial neural networks
    (ANN). The authors reviewed the advantages and limitations of each algorithm and
    evaluated their effectiveness for disease classification. [4] Abbas, W., Larijani,
    H. R., & Tufail, M. A. The review discussed the use of different machine learning
    algorithms, including decision trees, support vector machines (SVM), artificial
    neural networks (ANN), and random forests, for cotton disease detection and classification.
    The authors reviewed the advantages and limitations of each algorithm and evaluated
    their effectiveness for disease recognition. [5] Gajendra, S., & Ramasamy, K.
    provided a comprehensive survey of deep learning-based approaches for cotton disease
    identification. They reviewed different deep learning models, such as convolutional
    neural networks (CNNs) and recurrent neural networks (RNNs), and discussed their
    advantages and limitations. [6] Zhou, J., Wang, F., & Zuo, W. provided a literature
    review on cotton leaf disease identification methods based on support vector machines
    (SVM). The authors emphasized the importance of accurate and timely detection
    of cotton leaf diseases for the effective management of cotton crops. [7] Bhatia,
    D., Kumar, M., & Kumar, A. The review discussed different machine-learning techniques
    used for cotton plant disease identification, such as decision trees, artificial
    neural networks, support vector machines, and random forests. The authors evaluated
    the effectiveness of these techniques in accurately identifying different types
    of cotton plant diseases. [8] Jiang, Y., Feng, S., Wang, S., & Huang, X. This
    paper conducted a literature review on the recognition of cotton leaf diseases
    based on texture feature extraction and an improved support vector machine (SVM)
    algorithm. The authors emphasized the importance of accurate and timely detection
    of cotton leaf diseases for the effective management of cotton crops. [9] Kanade,
    S. R., Wadnerkar, R. B., & Lokhande, S. D. The review covered different image
    processing techniques used for cotton plant disease detection, such as segmentation,
    feature extraction, and classification. The authors evaluated the effectiveness
    of these techniques in accurately identifying different types of cotton plant
    diseases. [10] Li, W., Wang, S., Li, Y., & Jiang, Y. This paper conducted a literature
    review on the identification of cotton leaf diseases using backpropagation (BP)
    neural networks. The authors highlighted the importance of early detection and
    accurate identification of cotton plant diseases for effective crop management
    and improved crop yield. [11] Mrs. Shruti U, Dr. Nagaveni V, Dr. Raghavendra B
    K This paper presents a comprehensive review of various machine learning techniques
    for plant disease detection. The authors explain the challenges in the field of
    agriculture and highlight the importance of plant disease detection. They also
    provide a brief overview of various image-processing techniques for plant disease
    detection. [12] Nagesh, N. H., & Patil, C. G. The authors provided an overview
    of cotton leaf diseases and their impact on crop production, highlighting the
    need for accurate and efficient disease detection methods. They then describe
    the proposed method, which involves the use of fuzzy logic for feature extraction
    and classification, and GLCM for texture analysis. Based on the literature survey,
    the gap identified is research has done less specifically to the cotton crops
    and the usage of machine learning and deep learning algorithms for detection and
    prevention of the cotton crop diseases was less. This paper addresses these gaps.
    SECTION III. METHODOLOGY: The process of cotton crop disease identification using
    Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) typically
    involves several steps. Here''s a step-by-step process: Data Collection: The first
    step is to collect a large dataset of images of cotton crops affected by various
    diseases. These images should be labeled with information about the specific disease
    present in each image. Data Preprocessing: The collected images must be preprocessed
    to make them ready for the training of the CNN and RNN models. This step includes
    resizing the images to a uniform size, normalizing pixel values, and splitting
    the dataset into training and validation sets. CNN Training: The next step is
    to train a CNN model on the preprocessed dataset. The CNN model is responsible
    for learning relevant features from the input images. The model is trained using
    backpropagation to update the weights of the neural network to minimize the classification
    error. RNN training: The output of the CNN model is fed into an RNN model that
    is responsible for modeling the temporal sequence of image frames. The RNN model
    learns to capture the sequential dependencies of the input data and predict the
    disease present in the input sequence. Fine-tuning: Once the CNN and RNN models
    are trained, they can be fine-tuned to improve their accuracy on the specific
    dataset of interest. This involves retraining the models on a smaller subset of
    the data, to fine-tune the models'' parameters to fit the specific dataset better.
    Testing: Finally, the trained CNN and RNN models can be used to predict the disease
    present in new images of cotton crops. The models take in the new image data and
    predict the disease present in the crop. Overall, the process of cotton crop disease
    identification using CNN and RNN involves collecting and preprocessing the data,
    training the CNN and RNN models on the dataset, fine-tuning the models to improve
    accuracy, and using the trained models to predict the disease present in new images
    of cotton crops. Figure 3.1 Flow diagram of the process of cotton crop detection
    using CNN and RNN techniques. Show All Figure 3.2 Leaf disease detection using
    deep learning. The 1st leaf shows leaf spots, the 2nd leaf shows V. Wilt leaf
    defect, and the 3rd leaf shows diseased leaf. (Cotton-Plant-Disease-Prediction,
    anillava) Courtesy: Data set. Show All Figure 3.3 This figure shows various types
    of cotton leaves and their state compared to healthy leaf. Courtesy: Wikipedia
    Show All Figure 3.4 Spotting of leaf disease (metric learning) Courtesy: BioMed
    central Show All SECTION IV. IMPLEMENTATION: The datasets that were collected
    were tested according to the code that which have been implemented. Also, the
    code was developed by using CNN and RNN which are known as deep learning techniques.
    After executing the code, the images were tested according to the datasets (i.e,
    Disease affected, healthy). If the detection system detects any disease in the
    cotton crop, then the system says that the plant was affected, if not the system
    says the plant was healthy. Different algorithms like SVM, CNN, and RNN have been
    used to identify cotton crop diseases based on leaf images. CNN and RNN algorithms
    outperformed SVM in terms of accuracy in several studies. CNN achieved over 97%
    accuracy in one study, while RNN achieved over 95% in another study. The effectiveness
    of SVM and RNN was compared in one study, and RNN outperformed SVM. Deep learning
    algorithms, especially CNN and RNN, appear to be more effective for cotton crop
    disease identification than traditional machine learning algorithms. However,
    the performance of each algorithm can vary depending on the dataset and experimental
    setup used in each study. Further research is necessary to determine the best
    approach for cotton disease identification. Figure 4.1 Performance analysis of
    the cotton crop certainty website. Note: The specific performance of each algorithm
    can vary depending on the dataset and experimental setup used in each project.
    Show All SECTION V. RESULTS AND DISCUSSION As we know CNN has multiple layers
    in it, and every layer in it will learn how to detect the image by collecting
    input from an outsource. Also, RNN is something that works with sequences like
    finance, video, sound, text, and many more. Combining the functionality of both
    CNN and RNN can give some outcomes. And in the case of image processing, it works
    with images and sequences of words. To get well-detailed layers, it is necessary
    to apply a filter or layer to each image. At it produces an outcome that is more
    progressive and more detailed after each layer. The pixels of the image are fed
    to the CNN, at which it performs convolution operations, which generates a route
    to convolve the map. Then ReLU function is used, but before that convolved map
    will be applied to the ReLU function, to generate a rectified feature map. The
    image is processed further with multiple convolutions. The result of this shows
    the detection of affected and normal leaves in the cotton plant. It helps to extract
    the textural pattern of the defective leaf. The veins of the leaf which is contrast
    with the green color of the leaf which makes it difficult to take no notice of
    it. Figure 5.1 This diagram shows the model accuracy of the dataset. Show All
    Figure 5.2 This diagram shows the model loss of the dataset. Show All We are working
    on 2 datasets one is test data and other is train data. We are using 20 epochs
    with an average loss: 0.4620 and average accuracy: 0.8191. Average val_loss: 0.3957
    with average val_accuracy: 0.84 In Datasets the train dataset contains approx.
    1951 images and test datasets contain approx. 36. SECTION VI. CONCLUSION: In conclusion,
    the use of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN)
    for cotton crop certainty identification has shown accurate results. The system
    involves the collection and preprocessing of a large dataset of images of cotton
    crops affected by various diseases, followed by training the CNN and RNN models
    on the dataset. The trained models are capable of accurately identifying the diseases
    present in new images of cotton crops, which can be a valuable tool for farmers
    and agriculture professionals. By identifying diseases in the early stages, farmers
    can take necessary actions to prevent the spread of the disease and minimize crop
    losses. However, it should be noted that the accuracy of the CNN and RNN models
    depends heavily on the quality and diversity of the training dataset. Therefore,
    it is essential to continue to collect and expand the dataset to improve the accuracy
    and robustness of the system. Furthermore, the system may also require regular
    updates and maintenance to adapt to new and emerging diseases. Overall, the use
    of CNN and RNN models for cotton crop disease identification has the potential
    to improve crop yields and reduce losses due to disease significantly, making
    it a valuable tool for the agricultural industry. SECTION VII. FUTURE WORKS Expansion
    to other crop diseases: While this system focused on cotton crop diseases, the
    same approach could be applied to other crop diseases as well. This would require
    the collection of more data and the development of new models but could lead to
    a more comprehensive system for plant disease identification. Real-time disease
    identification: The current system requires the input of an image and then outputs
    the classification result. However, it would be useful to have a system that could
    process live video feeds of crops and provide real-time disease identification.
    This would require optimization of the models and the integration of the system
    with the hardware that captures the video. Integration with precision agriculture
    systems: The disease identification system could be integrated with precision
    agriculture systems that provide recommendations for crop management. This would
    allow for targeted treatment of diseased areas of the crop and could potentially
    reduce the use of pesticides. Improved accuracy: While the accuracy of the current
    system is high, there is always room for improvement. More data could be collected
    to improve the training of the models, and alternative deep-learning techniques
    could be explored to improve the performance of the system. Deployment in the
    field: The current system is implemented as a software solution, but it would
    be useful to develop a portable hardware solution that could be used in the field.
    This would require optimization of the models for deployment on lower-power hardware
    and the development of a robust and reliable hardware solution that could withstand
    the harsh conditions of agricultural environments. Authors Figures References
    Keywords Metrics More Like This Impact Analysis of Stacked Machine Learning Algorithms
    Based Feature Selections for Deep Learning Algorithm Applied to Regression Analysis
    SoutheastCon 2022 Published: 2022 Crop Selection and Yield Prediction using Machine
    Learning Algorithms 2023 Second International Conference on Augmented Intelligence
    and Sustainable Systems (ICAISS) Published: 2023 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 14th International Conference on Computing Communication and Networking
    Technologies, ICCCNT 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Cotton Crop Certainty Identification Using Deep Learning Techniques
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kumar R.
  - Pal A.
  - Singh T.
  - Chauhan A.S.
  citation_count: '0'
  description: Leaf detection is a critical task in various fields, including agriculture,
    ecology, and botany, with applications ranging from plant species identification
    to disease diagnosis. This research paper explores the techniques used for leaf
    detection, the applications where it is employed, and the challenges faced in
    this domain. Traditional image processing techniques, machine learning approaches,
    deep learning-based methods, and hybrid approaches are examined and compared.
    The applications of leaf detection include agriculture and crop monitoring, plant
    species identification, disease diagnosis and management, ecological studies,
    and remote sensing. The challenges in leaf detection arise from the variability
    in leaf shapes and textures, occlusion and complex backgrounds, realtime processing
    requirements, and robustness to environmental factors. Future directions and emerging
    trends in multi-modal leaf detection, domain adaptation, integration of remote
    sensing technologies, and advanced image analysis techniques are discussed. The
    findings of this research have implications for further research and practical
    applications, highlighting the need for more robust and accurate leaf detection
    techniques and the utilization of advanced technologies. The research contributes
    to a deeper understanding of plant ecosystems, optimization of agricultural practices,
    and improved management of natural resources.
  doi: 10.1109/ICSEIET58677.2023.10303475
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    Leaf Detection: Techniques and Applications Publisher: IEEE Cite This PDF Rahul
    Kumar; Arpit Pal; Tejendra Singh; Alok Singh Chauhan All Authors 48 Full Text
    Views Abstract Document Sections I. Introduction II. Leaf Detection Techniques:
    [4] III. Applications of Leaf Detection: [3] IV. Challenges in Leaf Detection:
    [2] V. Future Directions and Emerging Trends:[1] Show Full Outline Authors References
    Keywords Metrics Abstract: Leaf detection is a critical task in various fields,
    including agriculture, ecology, and botany, with applications ranging from plant
    species identification to disease diagnosis. This research paper explores the
    techniques used for leaf detection, the applications where it is employed, and
    the challenges faced in this domain. Traditional image processing techniques,
    machine learning approaches, deep learning-based methods, and hybrid approaches
    are examined and compared. The applications of leaf detection include agriculture
    and crop monitoring, plant species identification, disease diagnosis and management,
    ecological studies, and remote sensing. The challenges in leaf detection arise
    from the variability in leaf shapes and textures, occlusion and complex backgrounds,
    realtime processing requirements, and robustness to environmental factors. Future
    directions and emerging trends in multi-modal leaf detection, domain adaptation,
    integration of remote sensing technologies, and advanced image analysis techniques
    are discussed. The findings of this research have implications for further research
    and practical applications, highlighting the need for more robust and accurate
    leaf detection techniques and the utilization of advanced technologies. The research
    contributes to a deeper understanding of plant ecosystems, optimization of agricultural
    practices, and improved management of natural resources. Published in: 2023 International
    Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET)
    Date of Conference: 14-15 September 2023 Date Added to IEEE Xplore: 07 November
    2023 ISBN Information: DOI: 10.1109/ICSEIET58677.2023.10303475 Publisher: IEEE
    Conference Location: Ghaziabad, India SECTION I. Introduction Leaf detection is
    a fundamental task with significant importance in various fields, including agriculture,
    ecology, botany, and remote sensing. The accurate detection of leaves in images
    or datasets is vital for a wide range of applications, such as plant species identification,
    disease diagnosis, and vegetation analysis. With the advancements in computer
    vision, machine learning, and image processing techniques, the development of
    effective leaf detection methods has become an area of active research. A. Importance
    of Leaf Detection Leaves are essential components of plants, playing a critical
    role in various physiological processes like photosynthesis, respiration, and
    transpiration. The ability to accurately detect and analyze leaves provides valuable
    insights into plant health, growth patterns, and overall ecosystem dynamics. Leaf
    detection techniques enable researchers and practitioners to study plant species
    diversity, monitor crop conditions, assess environmental impacts, and identify
    plant diseases or pests early on. Additionally, leaf detection contributes to
    the understanding of plant physiology and the optimization of agricultural practices
    for improved productivity and sustainability. B. Motivation for the Study The
    increasing demand for efficient and automated leaf detection methods arises from
    the need to streamline various processes across different domains. Traditional
    manual leaf identification and analysis methods are time-consuming, labour-intensive,
    and often prone to human error. Therefore, there is a strong motivation to develop
    robust and accurate techniques that can automate leaf detection, classification,
    and analysis tasks. Moreover, advancements in imaging technology, including high-resolution
    cameras, drones, and satellite imagery, have provided an abundance of leaf-related
    data. To make the most of this data and extract valuable insights, it is essential
    to have reliable leaf detection techniques that can handle large-scale datasets
    efficiently. The motivation behind this study is to explore the current state-of-the-art
    leaf detection techniques, understand their capabilities, limitations, and potential
    applications, and identify areas for improvement and further research. By conducting
    a comprehensive review of leaf detection techniques and their applications, this
    study aims to contribute to the existing body of knowledge in the field. The insights
    gained from this research can assist researchers, practitioners, and stakeholders
    in selecting suitable leaf detection methods for their specific needs and drive
    advancements in areas such as plant science, agriculture, environmental monitoring,
    and beyond. Ultimately, the study aims to promote the development of more accurate,
    efficient, and reliable leaf detection techniques that can facilitate advancements
    in various domains and contribute to a better understanding of our natural ecosystems.
    SECTION II. Leaf Detection Techniques: [4] A. Traditional Image Processing Technique
    Traditional image processing techniques have been widely employed for leaf detection
    tasks. These methods typically involve various image enhancement, segmentation,
    and feature extraction techniques. Common approaches include thresholding, edge
    detection, region-growing, template matching, and morphological operations. These
    techniques leverage image characteristics, such as color, texture, and shape,
    to identify and extract leaf regions. While traditional techniques have shown
    effectiveness in certain scenarios, they often struggle with complex backgrounds,
    variations in lighting conditions, and overlapping leaves. B. Machine Learning
    Approaches Machine learning techniques have gained popularity in leaf detection
    due to their ability to automatically learn and generalize from training data.
    These approaches involve the use of algorithms, such as Support Vector Machines
    (SVM), Random Forests, and k-Nearest Neighbors (k-NN), which learn patterns and
    relationships between features extracted from leaf images. Machine learning models
    are trained on labeled datasets, and once trained, they can classify new images
    as either containing leaves or not. Feature engineering plays a crucial role in
    these approaches, as relevant features need to be carefully selected or engineered
    to achieve good performance. C. Deep Learning-Based Methods Deep learning has
    revolutionized the field of computer vision, including leaf detection tasks. Convolutional
    Neural Networks (CNNs) have shown exceptional performance in various image recognition
    tasks, including leaf detection. Deep learning models can learn hierarchical representations
    of images through multiple layers, allowing them to automatically extract intricate
    features and patterns. Techniques like Transfer Learning, where pre-trained models
    on large-scale datasets are fine-tuned for leaf detection, have also been successful.
    Deep learning-based methods have demonstrated remarkable accuracy and robustness,
    even in challenging conditions, making them a popular choice for leaf detection
    tasks. D. Hybrid Approaches Hybrid approaches combine multiple techniques, such
    as traditional image processing methods and machine learning or deep learning
    algorithms. These approaches aim to leverage the strengths of different techniques
    to enhance the accuracy and efficiency of leaf detection. For example, traditional
    techniques can be used to preprocess images and extract initial features, which
    are then fed into a machine learning or deep learning model for classification.
    Hybrid approaches offer flexibility and the potential to achieve improved results
    by combining complementary techniques. E. Comparative Analysis of Techniques A
    comparative analysis of different leaf detection techniques is essential to understand
    their strengths, limitations, and suitability for specific applications. Factors
    such as accuracy, speed, robustness to variations in leaf characteristics, computational
    requirements, and training data requirements need to be considered. Comparative
    studies can involve benchmark datasets, performance metrics, and experimental
    evaluations to provide insights into the performance of different techniques.
    These analyses help researchers and practitioners make informed decisions when
    selecting the most suitable leaf detection technique for their specific requirements.
    By exploring traditional image processing techniques, machine learning approaches,
    deep learning-based methods, hybrid approaches, and conducting a comparative analysis,
    researchers can gain a comprehensive understanding of the available options and
    their performance characteristics in leaf detection tasks. This knowledge can
    guide the selection and development of appropriate techniques based on the specific
    requirements and constraints of different applications. SECTION III. Applications
    of Leaf Detection: [3] A. Agriculture and Crop Monitoring Leaf detection techniques
    find extensive applications in agriculture and crop monitoring. By accurately
    detecting and analyzing leaves, these techniques enable farmers and agricultural
    experts to assess plant health, monitor crop growth, and identify nutrient deficiencies
    or pest infestations. Leaf detection can aid in crop yield estimation, irrigation
    management, and the optimization of fertilization strategies. It also supports
    precision agriculture by providing valuable information for targeted interventions
    and improving overall crop productivity. B. Plant Species Identification Leaf
    detection plays a crucial role in plant species identification and classification.
    Each plant species has distinct leaf characteristics, including shape, size, vein
    patterns, and texture. By applying leaf detection techniques, researchers and
    botanists can automatically identify plant species based on these leaf features.
    This application is particularly valuable in biodiversity studies, conservation
    efforts, and ecological research. Automated plant species identification saves
    time and resources, enabling large-scale surveys and facilitating the documentation
    of plant diversity. C. Disease Diagnosis and Management Leaf detection techniques
    are instrumental in disease diagnosis and management in plants. Certain diseases
    manifest specific symptoms on leaves, such as discoloration, lesions, or abnormal
    growth patterns. By accurately detecting and analyzing these symptoms, plant pathologists
    and researchers can identify diseases early on, enabling timely intervention and
    disease management strategies. Leaf detection can assist in monitoring the progression
    of diseases, evaluating the effectiveness of treatments, and guiding breeding
    programs for disease-resistant plant varieties. D. Ecological Studies and Biodiversity
    Assessment In ecological studies, leaf detection techniques are employed to assess
    vegetation structure, monitor changes in plant communities, and estimate biodiversity.
    By detecting and analyzing leaves, researchers can quantify vegetation cover,
    identify dominant plant species, and study the spatial distribution patterns of
    plant populations. Leaf detection also contributes to the assessment of ecosystem
    health, understanding the impacts of environmental changes, and monitoring habitat
    restoration efforts. It provides valuable data for ecosystem modeling, conservation
    planning, and ecological restoration initiatives. E. Remote Sensing and Vegetation
    Mapping Leaf detection techniques are widely used in remote sensing applications
    for vegetation mapping and monitoring. Remote sensing platforms, such as satellites
    or drones equipped with high-resolution cameras, capture multispectral or hyperspectral
    data that can be used to detect and analyze leaves at a large scale. Leaf detection
    enables the mapping of vegetation types, estimating vegetation indices (e.g.,
    NDVI), and monitoring changes in vegetation cover over time. These applications
    are valuable for environmental monitoring, land management, and assessing the
    impact of land-use changes on ecosystems. By applying leaf detection techniques
    in agriculture, plant species identification, disease diagnosis, ecological studies,
    and remote sensing, researchers and practitioners can gain valuable insights into
    plant health, ecosystem dynamics, and environmental changes. Leaf detection enables
    automation, efficiency, and scalability in these applications, leading to more
    informed decision-making, sustainable practices, and improved management strategies
    in various domains. SECTION IV. Challenges in Leaf Detection: [2] A. Variability
    in Leaf Shapes and Textures One of the primary challenges in leaf detection is
    the wide variability in leaf shapes, sizes, and textures across different plant
    species. Leaves can have diverse shapes, ranging from simple to complex, and exhibit
    intricate texture patterns, such as veins, hairs, or surface irregularities. Accommodating
    this variability requires robust algorithms that can handle different leaf structures
    and adapt to variations in shape and texture. Developing techniques that can accurately
    detect and classify leaves across a wide range of species remains a significant
    challenge. B. Occlusion and Complex Backgrounds In natural settings, leaves are
    often partially occluded by other leaves, stems, or objects, making their detection
    and segmentation challenging. Occlusions can lead to incomplete or fragmented
    leaf detections, which can impact the accuracy of subsequent analysis or classification
    tasks. Additionally, complex backgrounds, such as cluttered vegetation, intricate
    foliage, or varying lighting conditions, can further complicate leaf detection.
    Developing algorithms that can effectively handle occlusions, separate overlapping
    leaves, and distinguish leaves from complex backgrounds is a persistent challenge
    in leaf detection. C. Real Time Processing and Scalability Leaf detection techniques
    must often operate in real-time or near real-time scenarios, especially in applications
    such as robotics, precision agriculture, or monitoring systems. Realtime processing
    requires efficient algorithms that can analyze and process leaf images quickly
    without sacrificing accuracy. Moreover, scalability is crucial when dealing with
    large-scale datasets or high-resolution imagery. Developing techniques that strike
    a balance between accuracy and processing speed, and can handle large volumes
    of data, is essential for practical applications of leaf detection. D. Robustness
    to Environmental Factors Leaf detection algorithms need to be robust and resilient
    to various environmental factors that can affect image quality and leaf appearance.
    Factors such as variations in lighting conditions, changes in seasonal foliage,
    presence of shadows, occlusion by rain or dewdrops, and variations in camera perspectives
    can significantly impact the performance of leaf detection methods. Ensuring the
    robustness of algorithms across different environmental conditions and mitigating
    the effects of such factors pose ongoing challenges in leaf detection research.
    Addressing these challenges in leaf detection requires the development of innovative
    algorithms, the integration of advanced techniques, and the utilization of diverse
    datasets. Researchers are exploring approaches such as data augmentation, transfer
    learning, advanced feature extraction methods, and fusion of multi-modal data
    to tackle these challenges. Additionally, the creation of comprehensive benchmark
    datasets and performance evaluation metrics specific to leaf detection can facilitate
    comparative analysis and drive progress in the field. Overcoming these challenges
    will enable more accurate, reliable, and versatile leaf detection techniques with
    broad applicability in various domains. SECTION V. Future Directions and Emerging
    Trends:[1] A. Multi-Modal Leaf Detection One future direction in leaf detection
    is the exploration of multi-modal approaches that combine information from different
    sources. By integrating data from multiple modalities such as visual images, thermal
    imaging, hyperspectral imaging, or 3D point clouds, researchers can extract more
    comprehensive and diverse information about leaves. Multimodal leaf detection
    techniques have the potential to improve accuracy, robustness, and the ability
    to capture additional leaf characteristics that may not be apparent in visual
    imagery alone. B. Domain Adaptation and Transfer Learning Domain adaptation and
    transfer learning techniques have gained attention in the field of leaf detection.
    The ability to leverage pre-trained models or knowledge from one domain and adapt
    it to another domain with limited labeled data is valuable. By transferring knowledge
    learned from abundant labeled data in related domains, such as general object
    recognition or plant species classification, to leaf detection tasks, researchers
    can overcome limitations caused by insufficient labeled data. Domain adaptation
    and transfer learning can enhance the performance of leaf detection algorithms,
    especially when dealing with limited or unbalanced datasets. C. Integration of
    Remote Sensing Technologies The integration of remote sensing technologies, such
    as satellite imagery, LiDAR (Light Detection and Ranging), or drones equipped
    with advanced sensors, presents exciting opportunities in leaf detection. These
    technologies provide rich and high-resolution data, enabling the detection and
    analysis of leaves at larger scales and capturing vegetation dynamics over time.
    Integrating remote sensing data with leaf detection techniques can facilitate
    accurate and comprehensive vegetation mapping, monitoring of large-scale ecosystems,
    and assessment of environmental changes with improved spatial and temporal resolution.
    D. Advanced Image Analysis Techniques Continued advancements in image analysis
    techniques hold great potential for enhancing leaf detection. Techniques such
    as semantic segmentation, instance segmentation, and object tracking can be leveraged
    to improve the accuracy and precision of leaf detection by providing pixel-level
    or objectlevel information. Additionally, advancements in deep learning architectures,
    such as attention mechanisms and graph neural networks, can enable more fine-grained
    and context-aware leaf detection. Exploring and incorporating these advanced image
    analysis techniques can lead to significant improvements in the performance and
    capabilities of leaf detection methods. By exploring multi-modal approaches, domain
    adaptation and transfer learning, integration of remote sensing technologies,
    and leveraging advanced image analysis techniques, researchers can push the boundaries
    of leaf detection. These future directions and emerging trends have the potential
    to enhance the accuracy, robustness, scalability, and applicability of leaf detection
    techniques across diverse domains such as agriculture, ecology, botany, and remote
    sensing. They open doors for innovative research and development, leading to improved
    understanding of plant ecosystems, optimized agricultural practices, and better
    management of natural resources. SECTION VI. Conclusion A. Summary of Key Findings
    In this research paper, we have explored the techniques and applications of leaf
    detection. We discussed various techniques, including traditional image processing,
    machine learning approaches, deep learning-based methods, and hybrid approaches.
    Each technique has its strengths and limitations, and their performance depends
    on factors such as leaf variability, occlusion, real-time processing requirements,
    and robustness to environmental factors. We also examined the applications of
    leaf detection, including agriculture and crop monitoring, plant species identification,
    disease diagnosis and management, ecological studies, and remote sensing. Leaf
    detection techniques have proven to be valuable in these domains, providing insights
    into plant health, biodiversity, and environmental changes. They enable automation,
    efficiency, and scalability in various tasks, contributing to improved decision-making,
    sustainable practices, and ecosystem management. B. Implications for Research
    and Applications The findings of this research have several implications for further
    research and practical applications. First, researchers can explore the development
    of more robust and accurate leaf detection techniques that can handle the challenges
    posed by leaf variability, occlusion, complex backgrounds, and environmental factors.
    This can involve the integration of advanced image analysis techniques, multi-modal
    approaches, and the utilization of remote sensing technologies to enhance the
    capabilities of leaf detection. Furthermore, the research highlights the importance
    of domain adaptation and transfer learning, which can address the limitations
    of limited labeled data in leaf detection. By leveraging knowledge from related
    domains and pre-trained models, researchers can enhance the performance and generalization
    capabilities of leaf detection algorithms. In practical applications, the findings
    of this research can guide the selection and implementation of suitable leaf detection
    techniques for specific needs. Agricultural experts, plant pathologists, ecologists,
    and remote sensing practitioners can benefit from the insights provided in this
    paper to optimize their practices, monitor plant health, assess biodiversity,
    and track environmental changes. C. Closing Remarks In conclusion, leaf detection
    is a vital task with numerous applications in agriculture, ecology, botany, and
    remote sensing. The techniques discussed in this research paper provide valuable
    tools for automating leaf detection, enabling efficient analysis, and supporting
    decision-making processes in various domains. While challenges such as leaf variability,
    occlusion, real-time processing, and robustness to environmental factors exist,
    future directions and emerging trends offer promising avenues for further advancements.
    By continually pushing the boundaries of leaf detection research, integrating
    diverse data sources, and leveraging advanced techniques, we can enhance the accuracy,
    efficiency, and applicability of leaf detection methods. This will contribute
    to a deeper understanding plant ecosystem, improved agricultural practices, effective
    disease management, and sustainable environmental stewardship. The future of leaf
    detection holds exciting possibilities, and continued research and development
    in this field will unlock new opportunities for innovation and progress. Authors
    References Keywords Metrics More Like This Transistor-based plant sensors for
    agriculture 4.0 measurements 2021 IEEE International Workshop on Metrology for
    Agriculture and Forestry (MetroAgriFor) Published: 2021 Shape and size estimation
    using stochastically deployed networked sensors 2008 IEEE International Conference
    on Systems, Man and Cybernetics Published: 2008 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 International Conference on Sustainable Emerging Innovations in Engineering
    and Technology, ICSEIET 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Leaf Detection: Techniques and Applications'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wu J.
  - Dar U.
  - Anisi M.H.
  - Abolghasemi V.
  - Wilkin C.N.
  - Wilkin A.I.
  citation_count: '0'
  description: Today, plant diseases have become a major threat to the development
    of agriculture and forestry, not only affecting the normal growth of plants but
    also causing food safety problems. Hence, it is necessary to identify and detect
    disease regions and types of plants as quickly as possible. We have developed
    a plant monitoring system consisting of sensors and cameras for early detection
    of plant diseases. First, we create a dataset based on the data collected from
    the strawberry plants and then use our dataset as well as some well-established
    public datasets to evaluate and compare the recent deep learning-based plant disease
    detection studies. Finally, we propose a solution to identify plant diseases using
    a ResNet model with a novel variable learning rate which changes during the testing
    phase. We have explored different learning rates and found out that the highest
    accuracy for classification of healthy and unhealthy strawberry plants is obtained
    with the learning rate of 0.01 at 99.77%. Experimental results confirm the effectiveness
    of the proposed system in achieving high disease detection accuracy.
  doi: 10.1109/CAFE58535.2023.10291622
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 IEEE Conference on AgriF...
    Plant Disease Detection: Electronic System Design Empowered with Artificial Intelligence
    Publisher: IEEE Cite This PDF Jiayi Wu; Usman Dar; Mohammad Hossein Anisi; Vahid
    Abolghasemi; Chris Newenham Wilkin; Andrey Ivanov Wilkin All Authors 51 Full Text
    Views Abstract Document Sections I. Introduction II. State-of-the-Art III. Proposed
    System V. Conclusion Authors Figures References Keywords Metrics Footnotes Abstract:
    Today, plant diseases have become a major threat to the development of agriculture
    and forestry, not only affecting the normal growth of plants but also causing
    food safety problems. Hence, it is necessary to identify and detect disease regions
    and types of plants as quickly as possible. We have developed a plant monitoring
    system consisting of sensors and cameras for early detection of plant diseases.
    First, we create a dataset based on the data collected from the strawberry plants
    and then use our dataset as well as some well-established public datasets to evaluate
    and compare the recent deep learning-based plant disease detection studies. Finally,
    we propose a solution to identify plant diseases using a ResNet model with a novel
    variable learning rate which changes during the testing phase. We have explored
    different learning rates and found out that the highest accuracy for classification
    of healthy and unhealthy strawberry plants is obtained with the learning rate
    of 0.01 at 99.77%. Experimental results confirm the effectiveness of the proposed
    system in achieving high disease detection accuracy. Published in: 2023 IEEE Conference
    on AgriFood Electronics (CAFE) Date of Conference: 25-27 September 2023 Date Added
    to IEEE Xplore: 30 October 2023 ISBN Information: DOI: 10.1109/CAFE58535.2023.10291622
    Publisher: IEEE Conference Location: Torino, Italy SECTION I. Introduction In
    recent years, plant diseases have become a major challenge to today''s forestry
    development. Plant diseases result in damage to a part of the tissue or organ
    of the plant, until it is destroyed, killed, or aesthetically ruined. Plant diseases
    affect plants from the outside to the inside, from top to bottom, from flowers
    and fruits to the root system in a comprehensive, all-round manner. This not only
    affect the normal growth of plants but can also cause a reduction in the yield
    and quality of agricultural products and, in serious cases, food safety problems
    [1]. Therefore, rapid identification and diagnosis of plant diseases can reduce
    the economic losses caused by plant diseases to the agricultural industry in the
    shortest possible time. Plant disease identification is a technique for processing,
    analysing, and understanding plant image datasets to identify potential kinds
    of disease objects. It is a key process for the timely and effective control of
    plant diseases [2]. Today, there are many different types of plant diseases in
    different stages of growth and in several growing areas, which makes it difficult
    for laymen to accurately identify the types of disease in a short period of time
    and in a large scale. Besides, manual identification has the disadvantages of
    slow identification speed and low accuracy which poses a major challenge in containing
    the outbreak of diseases in agriculture. With the continuous development of deep
    learning from one hand, and increasing the computation power from another hand,
    many researchers have started to study plant disease identification based on deep
    learning with either sensors or image data (or both). Using computer vision technology
    to identify plant disease areas and species can effectively reduce time costs
    and improve the efficiency of agricultural production [2]. Furthermore, with the
    advances in internet of things (IoT) technology, effective and continuous monitoring
    of various systems has become easier and more accessible. This has led to greater
    autonomy of systems in several industries as well as agriculture. The solutions
    which IoT offer are complemented by machine learning and computer vision-based
    techniques to improve the classification and detection performance. This paper
    provides a comprehensive overview of the state-of-the-art in plant disease detection
    with focus on recent developments in hardware design and deep learning approaches.
    Furthermore, it presents the current frequently used plant disease datasets, and
    summarises the current cutting-edge development techniques in the field of plant
    disease identification. It also showcases the details of an electronic IoT- based
    device for acquisition of strawberry plant data as well as the results of two
    deep learning models on the captured plant images to identify the disease. SECTION
    II. State-of-the-Art 2.1 Plant Diseases Datasets According to our investigation
    there exist around 14 available image datasets for plant disease diagnosis. The
    size and quality of the dataset will affect the accuracy of the deep learning
    model. Mostly, a large and high-quality dataset will improve the quality of the
    training process and the accuracy of monitoring of the deep learning model, allowing
    for more accurate identification of different types of plant diseases [3]. Due
    to space restriction in the paper, we have provided a summary of these datasets
    with the key details in Table 1. 2.2 Existing Plant Disease Detection Systems
    There exist limited works reported on developments of plant disease diagnosis
    with on-board hardware. Pankaj et al. proposed an IoT hardware sensor-based Cotton
    Disease prediction using convolutional neural network (CNN). Their IoT gadget
    has different sensors such as temperature, humidity, and PH to collect the data
    to be used for classification [4]. In another work by Mora et al., a plant disease
    detection using the Raspberry Pi 4 was implemented. Not many results are reported
    in this work, however, accuracy around 90% was obtained for prediction of plant
    disease on a private dataset [5]. A diagnostic system implemented on Raspberry
    Pi was proposed for scab and leaf disease detection. The authors used a CNN model
    and four classes of Scab, Black Rot, Cedar Rust, and Healthy were detected [6].
    A smart crop growth monitoring using edge artificial intelligence (AI) was developed
    in [7] where a complex system was implemented to monitor health plants and classify
    the pest and disease severity. They used cryptographic hardware modules, including
    RTEA32, RTEA64, XTEA32 and XTEA64, and used the binarized neural network and achieved
    76.57% accuracy for disease detection on dragon fruits. We can categorise the
    type of plan disease detection into three key types: direct object detection,
    multiclass classification, and binary segmentation. Direct objection detection
    is typically disease identification on a single plant type. A self-constructed
    SPIKE dataset from images of relevant complex wheat fields was used in an object
    detection method based on identifying diseased plants (or parts affected by diseases
    or pathogens) proposed by Hasan, M.M. et al. [8] The model used was an R-CNN architecture
    that generated four different models, four different datasets of training and
    test images based on four different datasets to capture plant diseases at different
    growth stages with an accuracy of 93.4%. Toda Y et al. used the YOLOV3 - DenseNet
    algorithm for direct object detection, focusing on disease object detection concerning
    growing apple leaves, with an accuracy of 95.75%. And using human intervention
    to validate the authenticity of the model and the training dataset, a CNN trained
    using publicly available plant disease image datasets, various neuron and layer
    visualisation methods were applied [9]. Zhang, S. et al. used the GPDCNN algorithm
    for multiclass classification detection of cucumber images, i.e., using different
    stages of the plant for possible disease detection. An accuracy of 94.65% was
    achieved [10]. Hari et al. in 2019 used the PDDNN algorithm for the detection
    of various plant disease images, using TensorFlow as the framework, with an accuracy
    of 86% [11]. As a comparison, Picon et al. also published a paper in 2019 using
    the RESNET-MC1 algorithm for the detection of various plant disease images using
    TensorFlow and Keras as the framework of choice, with an accuracy of 98% [12].
    Howlader et al. use the AlexNet algorithm to detect plant diseases on guava leaves
    with an accuracy of 98.74% [13]. Nagasubramanian et al. used the 3D-CNN algorithm
    to detect plant diseases in soybean using a binary classification method, i.e.,
    only diseased or healthy, without distinguishing between specific growth regions
    and growth stages, with an accuracy of 95.73% [14]. Arunangshu Pal proposed an
    Agricultural Inspection (AgriDet) framework, The framework combines the traditional
    Inception-Visual Geometry Group network (INC-VGGN) and the Kohonen-based deep
    learning network to detect plant diseases and classify the severity of diseased
    plants where the performance of the statistical analysis is validated to demonstrate
    the effectiveness of the results in terms of accuracy, specificity, and sensitivity
    [15]. In the- article by Amal Mathew et al., the support vector machine (SVM)
    classifier was replaced with a voting classifier to classify the data into multiple
    classes. The accuracy of voting and SVM classifiers are compared. The results
    show that the accuracy of the proposed method is improved by 10% [16]. Punam Bedi
    et al. proposed a hybrid system based on convolutional auto-encoder (CAE) and
    CNN that can achieve automatic detection of plant diseases. In the experiment,
    CAE is used to compress the parameters required for training, and the parameters
    required for the hybrid model are reduced. The proposed hybrid model used only
    9914 training parameters. The experiment uses a public dataset called PlantVillage
    to obtain leaf images of peach plants with the training and testing accuracies
    reported at 99.35% and 98.38%, respectively [17]. Abdalla et al. used the VGG16
    Encoder algorithm to detect binary segmentation of 400 oilseed images in two different
    environments with an accuracy of 96% [18]. Table 1 Summary of available public
    plant disease datasets Lin et al. used the U-Net segmentation algorithm to segment
    cucumber leaves with an accuracy of 96.08% [19]. Wiesner-Hanks et al. implemented
    a binary segmentation task to identify maize diseases using the ResNet - Crowdsourced
    algorithm for binary segmentation, which divides the image into homogeneous regions
    according to defined criteria and generates a binary image of the plant disease
    with the highest accuracy rate, i.e. 99.79% [20]. A collective summary of the
    existing methods with relevant details are provided in Table 2. Table 2 Algorithm
    and detection method of the cited article 2.3 Challenges, Perspectives, and Our
    Proposed Solution Looking at the plant disease target detection algorithms in
    recent years, the overall accuracy is high, basically above 90%, and accurate
    detection of cucumber, wheat and various plants can be achieved regardless of
    which detection method is used. In some studies, due to the different data sets
    used, the proposed algorithms ignore disease areas without obvious boundaries
    when identifying them, i.e., they are unable to accurately detect the extent of
    the disease. However, most of the current papers only address plant disease detection
    in a single environment or photographs taken of individual leaves in the laboratory,
    and there is less disease target detection for photographs taken in complex environments
    or in natural scenarios. Difficulties such as lighting, shading, superimposition
    and background bias exist in practical applications, therefore disease identification
    in complex natural conditions is an area of ongoing research. Another key limitation
    of existing system is using pre-developed embedded systems are which negatively
    affects adaptability and flexibility of the system. In this paper, we develop
    a fully scalable system from scratch (including sensors, cameras, connectivity,
    etc.) which can be generalised for several agricultural applications. This system
    mitigates the aforementioned challenge of collecting data/images from an operational
    farm. Using this system, we have collected and processed the “Strawberry Dataset”
    which their details and the results will be discussed next. SECTION III. Proposed
    System In this section, we showcase the details and results of our plant disease
    detection system. As shown in Figure 1(a) and (b), we develop a network of sensors
    and cameras that are wirelessly connected to a base station, continuously monitor
    the conditions of plants, and seamlessly transmit the images and sensors'' data.
    In the following, first, the hardware specifications and design for data/image
    capturing and communication is described. Then, the results of applying deep learning
    models on both collected dataset and also existing datasets are provided. 3.1
    Hardware The imaging system is composed of an SVC3 camera that is able to capture
    images at 2560 x 1920 resolution. The camera features 20x optical zoom as well
    as 255 degrees pan and 120 degrees tilt that enables the capture of high-quality
    close-up images of the plant matter over a large area. A Raspberry Pi based camera
    controller which is deployed on the same Wi-Fi Network as the cameras, requests
    images from each camera at fixed intervals during the day before uploading them
    via a Wi-Fi access point. In contrast to the imaging system, the sensor network
    has been custom designed to meet the needs of this application. A 3D rendering
    of the edge node''s PCB can be seen in Figure 1(c). The ATMega644p microcontroller
    is responsible for interfacing with 7 sensor modules; temperature, pressure, humidity,
    ambient light, U.V light, soil moisture and leaf wetness. The microcontroller
    samples the sensors roughly once every 30 minutes and uses a Semtech SX1262 LoRa
    Figure 1 The proposed IoT-based plant disease detection system implemented at
    Wilkin & Sons in Tiptree. Show All Figure 2 Structure of the proposed model based
    on ResNet. Show All Figure 3 Accuracy ((a) and (b)) and loss function ((c) and
    (d)) performance using the proposed model with different datasets. Show All Transceiver
    to transmit the data to a Dragino LG01-N LoRa gateway which pushes this data to
    a privately hosted server that is responsible for parsing the data and storing
    it in a database hosted on Amazon Web Services. 3.2 Software We used the 9-layer
    structure of the ResNet as a baseline. In this model, each layer feeds into the
    next layer and directly into the layers about 2–3 hops away. Conventional pre-processing
    such as image re-sizing was applied to input images where required. The network
    in this project uses a combination of two convolutional layers and two residual
    blocks, regularising each layer first, then using ReLU as the activation function
    and Max Pooling to reduce the size of the data and increase the speed of computation.
    Furthermore, the Adam Optimiser and the Cross-Entropy loss function were employed
    in this model. The changes made to the dimensions of each layer are indicated
    in the model structure diagram in Figure 2. In this model, the learning rate can
    change with the training rounds, and only the maximum learning rate needs to be
    set when setting the parameters. The model will constantly change the learning
    rate during training and obtain different accuracy rates in each training process.
    After many tries, when training the New Plant Diseases Dataset, we set the max
    learning rate to 0.04 to obtain highest accuracy. When training the strawberry
    dataset, set the learning rate to 0.01 to achieve the highest accuracy. In the
    last layer, the data is flattened, and linear regression is used to classify the
    different types of plant diseases. A total of 6,589,734 parameters were calculated
    to be trained. IV. Performance Evaluation 4.1 Selection of Dataset To provide
    a comprehensive evaluation and the generalisability power of the proposed system,
    we use a widely used public dataset, i.e., “New Plant Diseases Dataset” as well
    as data collected by ourselves at Wilkin & Sons in Tiptree [21] (see Figure 1).
    “New Plant Diseases Dataset” is created using offline augmentation from the original
    PlantVillage Dataset. This dataset consists of about 87K RGB images of healthy
    and diseased crop leaves which is categorized into 38 different classes of 14
    unique plants. The total dataset is divided into 80/20 ratio of training and validation
    set preserving the directory structure. And a new directory containing 33 test
    images is created for prediction purposes. The other dataset, called “strawberry
    dataset”, contains healthy strawberries and 2 types of diseases of strawberry,
    including Strawberry Leaf scorch and Strawberry Mildew. Each type contains about
    2000 pictures of strawberries. 4.2 Experimental Results To measure the accuracy,
    we have used cross-validation and considered 80% of data for training and 20%
    for testing. We used batch size 32 and run the model for 10 number of epochs.
    Figure 3(a) and (b) show the changing trend of accuracy with rounds for the two
    datasets, respectively. It can be observed that the accuracy fluctuates but the
    overall trend is increasing and finally reaches 97.36% and 99.77%. Figure 3(c)
    and (d) depict the change trend of loss with rounds, and the overall loss is low
    and stable. Instead of using a fixed learning rate in this project, we use a learning
    rate scheduler which will change the learning rate after each batch of training.
    There are several strategies for changing the learning rate during training, i.e.,
    starting with a low learning rate, gradually increasing it to a high learning
    rate in batches over about 30% of the cycles, and then gradually reducing it to
    a very low value over the remaining cycles, so only the maximum learning rate
    needs to be set when setting the parameters. The results in these figures show
    that in the 10 running epochs, the accuracy of the validation set for New Plant
    Diseases Dataset finally reaches 97.69% and 99.77% for Strawberry dataset. SECTION
    V. Conclusion In this paper, a comprehensive collection and comparison among existing
    plant disease datasets were provided. We designed and implemented a full embedded
    electronic systems including sensors'' data capturing as well as RGB camera image
    acquisition from strawberry plants. A new modified ResNet model was proposed and
    applied to both collected strawberry plants'' data as well as a public dataset.
    The obtained results show high detection accuracy and hence the effectiveness
    of the proposed system. We further aim to expand the number of data collection
    nodes throughout the farm, enriching the dataset and developing a fusion model
    to analyse both sensors'' data and image data simultaneously to provide early
    and accurate prediction of potential diseases. ACKNOWLEDGEMENT This research was
    funded by a Knowledge Transfer Partnership (KTP) from Innovate UK (Partnership
    No: 12298), between Wilkin & Sons Ltd and the University of Essex. Authors Figures
    References Keywords Metrics Footnotes More Like This Radar Signal Abnormal Point
    Classification based on Camera-Radar Sensor Fusion 2023 International Conference
    on Artificial Intelligence in Information and Communication (ICAIIC) Published:
    2023 Three-dimensional imaging sensor system using an ultrasonic array sensor
    and a camera SENSORS, 2010 IEEE Published: 2010 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 IEEE Conference on AgriFood Electronics, CAFE 2023 - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Plant Disease Detection: Electronic System Design Empowered with Artificial
    Intelligence'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liao J.
  - Tao W.
  - Zang Y.
  - Zeng H.
  - Wang P.
  - Luo X.
  citation_count: '1'
  description: Diseases and insect pest are the most restricting factors affecting
    the crop health, the improvement of the crop yield and quality. It is of great
    significant to strengthen the development of crop disease and insect pest monitoring.
    Therefore, to undertake the precise prevent and control on the crop disease and
    insect pest is key for ensuring the food safety, and improve the yield and quality
    of crop. The traditional disease and insect pest monitoring mainly relies on the
    manual field investigation, with low efficiency and quality, which can no longer
    meet the needs of efficient, intelligent and professional modern agriculture.
    With the development of information technology, the monitoring of crop diseases
    and insect pest has gradually developed from the traditional manual monitoring
    to remote sensing monitoring. Crop monitoring platform, monitoring sensor technology,
    data analysis and processing technology are key technologies for the development
    of remote sensing monitoring of crop diseases and insect pest. The development
    of the above technologies determined the development of remote sensing monitoring
    technology of crop disease and insect pest. The research progress of monitoring
    platform, monitoring sensor technology, data analysis and processing technology
    for crop disease and insect pests were summarized. In terms of monitoring platform,
    the research status of ground machinery platform, aircraft platform, and satellite
    platform was summarized. In the monitoring sensor technology, the research progress
    of radar sensor, image sensor, thermal imaging sensor and spectral sensor for
    crop diseases and pests monitoring was summarized. In data analysis and processing
    technology, the research achievements of classical statistical algorithms, computer
    image processing algorithms, machine learning algorithms and deep learning algorithms
    in crop diseases and insect pests monitoring were expounded. Furthermore, recommendations
    were proposed for further promoting the development of crop diseases and insect
    pests monitoring, including building multi-scale integrated application monitoring
    platform, promoting the development of multi-scale data fusion sensor, and continuous
    optimizing multidisciplinary theory and algorithm structure research.
  doi: 10.6041/j.issn.1000-1298.2023.11.001
  full_citation: '>'
  full_text: '>

    "首页 | 学会首页 | 学报简介 | 投稿须知 | 编委会 | 期刊浏览 | EI收录结果 | 联系我们 | OSID建码 | English | 加入收藏
    用户登录          版权信息 主管：中国科学技术协会 主办：中国农业机械学会 中国农业机械化科学研究院集团有限公司 编辑出版：《农业机械学报》编辑部
    主编：任露泉 国际刊号：ISSN 1000-1298 国内刊号：CN 11-1964/S CODEN：NUYCA3 收录机构：EI /SCOPUS / CA
    / CSA/JSTChina 刊期：月刊，每月末25日出版 邮发代号：2-363 国外代号：M289 下载中心更多>>  . 2013-2023年总目录  .
    编改注意事项  . 本刊近年期刊评价指标(2023-11)  . 参考文献著录标准  . 论文写作模板  . 编辑流程  . EI文摘写作要求 友情链接  .
    中国科协  . 中国农业机械学会  . 中国农业机械化科学研究院 2023年增刊、2024年第2期已经被EI收录 信息公告 更多>>  . 《农业机械学报》高质量发展座谈会在昆明召开  .
    喜报：《农业机械学报》入选中国最具国际影响力学术期刊  . 中国农业机械学会2023首届博士生论坛优秀口头报告和优秀墙报入选名单  . 祝贺《农业机械学报》荣获“机械工业科学技术奖”二等奖  .
    3.166！《农业机械学报》影响因子居首位！  . 《农业机械学报》第一届青年编委会名单  . 2021年度优秀审稿人和2020年度优秀论文  . 2020年度优秀审稿人和2019年度优秀论文  .
    2013-2017年度优秀审稿专家  . 2013-2023年出版统计年报  . 2022年度优秀审稿人和2021年度优秀论文  . 本刊刊出综述论文（2013-2023）  .
    2018~2019年度优秀审稿专家  . 2018年度《农业机械学报》优秀论文 微信公众号 期刊检索 高级检索 刊次检索                     2024年                     2023年                     2022年                     2021年                     2020年                     2019年                     2018年                     2017年                     2016年                     2015年                     2014年                     2013年                     2012年                     2011年                     2010年                     2009年                     2008年                     2007年                     2006年                     2005年                     2004年                     2003年                     2002年                     2001年                     2000年                     1999年                     1998年                     1997年                                                 第2期
    第3期 第4期 第5期 第6期 第7期 第8期 第9期 第10期 第1期 第11期 第12期 关键词检索 按                                           文章编号            中文标题            英文标题            作者英文名            作者中文名            单位中文名            单位英文名            中文关键词            英文关键词            中文摘要            英文摘要            基金项目                                    关键词：
    从                            1997年                          1998年                          1999年                          2000年                          2001年                          2002年                          2003年                          2004年                          2005年                          2006年                          2007年                          2008年                          2009年                          2010年                          2011年                          2012年                          2013年                          2014年                          2015年                          2016年                          2017年                          2018年                          2019年                          2020年                          2021年                          2022年                          2023年                          2024年                            到                            2024年                          2023年                          2022年                          2021年                          2020年                          2019年                          2018年                          2017年                          2016年                          2015年                          2014年                          2013年                          2012年                          2011年                          2010年                          2009年                          2008年                          2007年                          2006年                          2005年                          2004年                          2003年                          2002年                          2001年                          2000年                          1999年                          1998年                          1997年                          总目录
    2024年总目录 2023年总目录 2022年总目录 2021年总目录 2020年总目录 2019年总目录 2018年总目录 2017年总目录 2016年总目录
    2015年总目录 2014年总目录 2013年总目录 2012年总目录 2011年总目录 2010年总目录 2009年总目录 2008年总目录 2007年总目录
    期刊订阅  农业机械学报淘宝  农业机械学报微店     当期目录（2024年第3期） 综述   ·  移动机器人视觉里程计技术研究综述 陈明方，黄良恩，王森，张永霞…163
    农业装备与机械化工程   ·  基于变前视距离的四轮同步转向农机改进纯跟踪控制 沈跃，赵莎，张亚飞，何思伟，…159   ·  茶园仿生往复式开沟松土机设计与试验
    秦宽，郎旭涛，沈周高，吴正敏…177   ·  玉米免耕播种自动调偏系统设计与试验 张振国，郭全峰，蒋贵菊，王蕴…140   ·  气吸双行错置式玉米密植精量排种器设计与试验
    王韦韦，宋岚洲，石文兵，魏德…174   ·  气力辅助充种式花生精量排种器设计与试验 郭鹏，郑效帅，王东伟，侯加林…107   ·  基于IWHO-EKF的高速免耕播种机播种深度监测系统研究
    王淞，衣淑娟，赵斌，李衣菲，…106   ·  舀种勺舌式胡麻精量穴播器设计与试验 李辉，赵武云，石林榕，戴飞，…96   ·  番茄钵苗移栽探出式取钵机构设计与试验
    辛亮，王明成，孙国玉，张浩，…105   ·  植物工厂岩棉块种苗移植机移植部件设计与试验 童俊华，刘珂，刘霓红，孙良，…87   ·  辣椒苗夹茎式双排自动取投苗装置设计与试验
    邱硕，于博，计东，田素博，赵…116   ·  蔬菜泡沫育苗盘多适应性自动叠盘装置设计与试验 李旭，伍硕祥，匡敏球，刘青，…97   ·  机收棉田残膜混合物粉碎揉丝装置设计与试验
    谢建华，孟庆河，张佳，刘旺，…90   ·  基于顶芽智能识别的棉花化学打顶系统研究 韩鑫，韩金鸽，陈允琳，兰玉彬…93   ·  簇生番茄果梗超声切割过程仿真与试验
    张军，辛迪，蓝伟科，党柯华，…97   ·  基于熵产理论的多级液力透平能量耗散机理分析 王晓晖，蒋虎忠，苗森春，白小…81 农业信息化工程   ·  基于遥感多参数和CNN-Transformer的冬小麦单产估测
    王鹏新，杜江莉，张悦，刘峻明…86   ·  基于多注意力机制与编译图神经网络的高光谱图像分类 孙杰，杨静，丁书杰，李少波，…63   ·  基于Sentinel-2与时序Sentinel-1
    SAR特征的赣南柑橘种植区识别方法 唐琪，李恒凯，周艳兵，王秀丽74   ·  基于PCIe级联网口的农业监测视频高速传输系统研究 段瑞枫，陈艳，洪凯，张就，张…135   ·  基于Swin
    Transformer与GRU的低温贮藏番茄成熟度识别与时序预测研究 杨信廷，刘彤，韩佳伟，郭向阳…67   ·  基于改进LSTM的蘑菇生长状态时空预测算法
    杨淑珍，黄杰，苑进76   ·  基于改进YOLO v7轻量化模型的自然果园环境下苹果识别方法 张震，周俊，江自真，韩宏琪113   ·  基于改进YOLO
    v5的复杂环境下花椒簇识别与定位方法 黄华，张昊，胡晓林，聂兴毅82   ·  基于双节点-双边图神经网络的茶叶病害分类方法 张艳，车迅，汪芃，汪玉凤，胡…65   ·  基于形色筛选的苹果园羽化害虫粘连图像分割方法
    刘双喜，王云飞，张宏建，孙林…78   ·  基于SimAM-ConvNeXt-FL的茶叶病害小样本分类方法研究 田甜，程志友，鞠薇，张帅62   ·  基于YOLO
    v8n-seg-FCA-BiFPN的奶牛身体分割方法 张姝瑾，许兴时，邓洪兴，温毓…75   ·  水产养殖中水质与鱼类行为双向映射模型研究 魏天娇，胡祝华，范习禹60
    农业水土工程   ·  四川省能源和粮食生产用水竞争及与经济关系研究 康银红，贺帅，王嘉驰，倪铁峰…44   ·  河南省氮素农业面源污染风险评价与关键管控区识别
    高林林，吴用，杨书涵，刘雪珂…40   ·  平原河网地区稻麦轮作农田排水与氮素流失特征研究 邹家荣，贾忠华，朱卫彬，刘文…51   ·  极性交换电场辅助植物修复稻田土壤镉污染研究
    栾雅珺，徐俊增，李亚威，胡哲…35   ·  减施氮肥和接种根瘤菌对大豆生理生长与氮素利用效率及产量的影响 向友珍，张威，唐子竣，付骏宇…51 农业生物环境与能源工程   ·  基于模型预测控制的菇房空调节能控制方法
    张馨，孔祥书，郑文刚，王明飞…44 农产品加工工程   ·  基于区块链的农产品供应链溯源数据多条件查询优化方法研究 高官岳，孙传恒，罗娜，徐大明…44   ·  基于电学参数的贺兰山东麓赤霞珠葡萄酒子产区判别
    马海军，朱娟娟，周乃帅，安雅…40   ·  基于SIRI和CNN的苹果隐性损伤检测方法 王玉伟，杨玲玲，朱浩杰，饶元…55   ·  多出口扇形喷嘴干冰喷射速冻蓝莓特性研究
    宁静红，宋志朋，杨鑫，任子亮…33 车辆与动力工程   ·  基于多岛遗传算法的电动拖拉机分布式驱动系统优化设计与试验 李贤哲，张明柱，刘孟楠，徐立…71   ·  温室电动拖拉机旋耕稳定性时序分析与前馈PID控制方法研究
    杨杭旭，周俊，齐泽中，孙晨阳…65 机械设计制造及其自动化   ·  基于共形几何代数的并联机器人逆运动学分析方法 柴馨雪，李翔毅，汤陈昕，李秦…51   ·  基于模型的四足机器人步态转换控制研究
    陈久朋，李春磊，伞红军，康伟…39   ·  考虑润滑间隙效应的空间并联机构动力学优化 陈修龙，居硕，贾永皓44   ·  基于Prandtl-Ishlinskii模型的气动肌肉迟滞特性动态建模与控制方法
    段慧茹，谢胜龙，万延见，陈迪剑43 主管单位：中国科学技术协会 主办单位：中国农业机械学会;中国农业机械化科学研究院集团有限公司  主编：任露泉 地址：北京德胜门外北沙滩1号6信箱  邮政编码：100083
    电话：64882610  技术支持：北京勤云科技发展有限公司  京ICP备11001094号-1 京公网安备 11010502033880号"'
  inline_citation: '>'
  journal: Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural
    Machinery
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Research Progress and Prospect of Key Technologies in Crop Disease and Insect
    Pest Monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sahasranamam V.
  - Ramesh T.
  - Rajeswari R.
  citation_count: '0'
  description: Paddy leaf diseases substantially threaten global rice production,
    resulting in significant crop yield losses and economic implications. Therefore,
    timely detection and accurate identification of these diseases are imperative
    for effective management strategies. Recent advancements in agricultural technologies
    have led to Unmanned Aerial Vehicles (UAVs) emerging as a promising tool for crop
    monitoring due to their capability to provide higher-resolution and real-time
    data. This research paper aims to afford a comprehensive outline of the use of
    UAVs in monitoring and identifying paddy leaf diseases, shedding light on their
    potential applications, challenges, and prospects. This paper showcases the pivotal
    role of UAVs in transforming disease monitoring practices in paddy fields. Equipped
    with sophisticated cameras and sensors, UAVs can capture aerial images and multispectral
    data of vast agricultural areas efficiently. These data can then be processed
    and analyzed using state-of-the-art image processing and machine-learning algorithms
    to identify disease symptoms, classify disease types, and assess the severity
    of infections. The paper delves into the various UAV-based disease identification
    techniques, elaborating on image processing methods employed for feature extraction,
    such as image segmentation, texture analysis, and color space transformations.
    Finally, the research paper addresses the prospects of UAVs in paddy leaf disease
    monitoring. The paper also suggests directions for further research on real-time
    disease mapping and automated spraying systems based on UAV data, aiming to optimize
    disease management efforts. Applications based on UAVs should dynamically implement
    pest and nutrient management approaches.
  doi: 10.1109/ICIDeA59866.2023.10295173
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 IEEE 2nd International C...
    Monitoring and Identifying Paddy Leaf Diseases Using Unmanned Aerial Vehicles
    (UAVs) with Machine Learning- A Survey Publisher: IEEE Cite This PDF V. Sahasranamam;
    T. Ramesh; R. Rajeswari All Authors 47 Full Text Views Abstract Document Sections
    I. Introduction II. Literature Review III. Methodology IV. Standard Process of
    Plant Disease Detection V. Protocols Strength and Limitations Show Full Outline
    Authors References Keywords Metrics Abstract: Paddy leaf diseases substantially
    threaten global rice production, resulting in significant crop yield losses and
    economic implications. Therefore, timely detection and accurate identification
    of these diseases are imperative for effective management strategies. Recent advancements
    in agricultural technologies have led to Unmanned Aerial Vehicles (UAVs) emerging
    as a promising tool for crop monitoring due to their capability to provide higher-resolution
    and real-time data. This research paper aims to afford a comprehensive outline
    of the use of UAVs in monitoring and identifying paddy leaf diseases, shedding
    light on their potential applications, challenges, and prospects. This paper showcases
    the pivotal role of UAVs in transforming disease monitoring practices in paddy
    fields. Equipped with sophisticated cameras and sensors, UAVs can capture aerial
    images and multispectral data of vast agricultural areas efficiently. These data
    can then be processed and analyzed using state-of-the-art image processing and
    machine-learning algorithms to identify disease symptoms, classify disease types,
    and assess the severity of infections. The paper delves into the various UAV-based
    disease identification techniques, elaborating on image processing methods employed
    for feature extraction, such as image segmentation, texture analysis, and color
    space transformations. Finally, the research paper addresses the prospects of
    UAVs in paddy leaf disease monitoring. The paper also suggests directions for
    further research on real-time disease mapping and automated spraying systems based
    on UAV data, aiming to optimize disease management efforts. Applications based
    on UAVs should dynamically implement pest and nutrient management approaches.
    Published in: 2023 IEEE 2nd International Conference on Industrial Electronics:
    Developments & Applications (ICIDeA) Date of Conference: 29-30 September 2023
    Date Added to IEEE Xplore: 31 October 2023 ISBN Information: DOI: 10.1109/ICIDeA59866.2023.10295173
    Publisher: IEEE Conference Location: Imphal, India SECTION I. Introduction Rice
    (Oryza sativa) stands as one of the most crucial cereal crops, serving as a staple
    food in Asia, Africa, and Latin America. With the global population projected
    to reach 9 billion by 2050, ensuring food security has become an urgent global
    challenge. However, the sustainable production of rice faces numerous obstacles,
    including the relentless threat posed by paddy leaf diseases. Paddy plants can
    reach heights of 90 to 150 cm [1]. The environment (frost, temperature, humidity,
    air, and drought) and pathogens (bacteria, nematodes, viruses, and fungi) are
    the main causes of the increase and spread of disease through these above-mentioned
    elements [2]. In recent years, rice cultivation and research have faced unprecedented
    difficulties. In some major rice-producing nations, production and yield have
    been declining for many years despite rising demand from people. For instance,
    in just 2020, over 100 million people-mostly from Asia, and Africa became poor
    [3]. UAVs are one of the latest technologies used today in precision agriculture.
    Precision agriculture is a method for increasing yields and managing crops through
    technology tools [4]. People with experience detecting and monitoring plant diseases
    formerly kept a watch on plant diseases visually in agricultural fields. This
    type of observation may result in inaccuracies, bias, and errors [5]. Paddy leaf
    diseases encompass a range of devastating infections caused by various pathogens,
    including fungi, bacteria, and viruses. The most prevalent diseases affecting
    rice crops worldwide include blast (Pyricularia oryzae), sheath blight (Rhizoctonia
    solani), and bacterial leaf blight (Xanthomonas oryzae pv. oryzae) as described
    in Table 1. These diseases often lead to severe economic losses, reduced crop
    yield, and lower grain quality. Table 1 A Summary of Rice Leaf Diseases. Timely
    detection and accurate identification of paddy leaf diseases are paramount for
    successful disease management. Early intervention can significantly mitigate the
    extent of damage, minimize yield losses, and enhance the overall sustainability
    of rice cultivation. However, traditional methods for disease monitoring have
    proven to be inefficient and limited in their capacity to meet these critical
    requirements [29] Historically, farmers have relied on manual scouting and visual
    inspection to identify diseased plants within paddy fields. While these methods
    have served as the foundation of disease monitoring, they suffer from inherent
    limitations. Manual scouting is labor-intensive, time-consuming, and impractical
    for covering extensive agricultural areas leads to delayed action and crop loss.
    To address these limitations and revolutionize disease monitoring practices, cutting-edge
    agricultural technologies have emerged, and among them, Unmanned Aerial Vehicles
    (UAVs) have shown immense potential. UAVs, commonly known as drones and various
    types of drones, have gained prominence in various industries due to their versatility
    and capacity to gather high-resolution data in real-time. In recent years, researchers
    and agricultural practitioners have recognized the value of UAVs in revolutionizing
    paddy leaf disease monitoring [30]. The application of UAVs in agriculture is
    not limited to disease monitoring alone; these unmanned aerial platforms have
    proven valuable in diverse agricultural tasks, including crop health assessment,
    irrigation management, precision farming, and yield prediction [31]. This research
    paper aims to provide an encompassing overview of the role of UAVs in monitoring
    and identifying paddy leaf diseases. It will explore the various techniques used
    for disease detection and classification, the advantages and challenges of using
    UAVs in disease monitoring, and the potential for future advancements in this
    field. By shedding light on the innovative application of UAV technology, this
    paper aims to contribute to the enhancement of disease management strategies and
    the promotion of sustainable rice production in the face of mounting global food
    security challenges [32]. SECTION II. Literature Review The literature review
    for this research article explores the body of information that already exists
    about the use of unmanned aerial vehicles (UAVs) for paddy leaf disease monitoring
    and identification. It involves an extensive exploration of relevant academic
    papers, articles, conference proceedings, and research reports that focus on UAV
    applications in agriculture, specifically in rice crop disease management. In
    the research article [6], the researchers aim to find the Paddy area where the
    damaged leaves exist. An image processing program is installed in Raspberry Pi
    4 and attached to the drone to find the healthiness of the paddy crop, Hue saturation
    value is used to find the color variance of the leaves. In the research article
    [7], the researchers compared and discussed the benefits of drones in detecting
    plant disease and monitoring plants. Compare the traditional methods of disease
    identification in plants with the new drone technology. RGB, thermal images, Spectral
    camera images, V-NIR Visible and Near-Infrared Images, and multispectral images
    for effective image input used for creating datasets for processing. CNN, F-CNN
    YOLO-3 algorithm for disease detection and classification are mentioned. In [8],
    the researchers used a Machine Learning algorithm and developed a neural network
    model using a dataset from the internet. The images were split into train and
    test data and trained. The trained model detects disease draws a bounding box
    and displays the corresponding disease. The test model was trained using inception_v3
    and produced an accuracy of 92%. The test model trained using Google’s AutoML
    has produced an accuracy of 94%. In the article [9], the authors gathered data
    sets using web mining, processed images, and trained models. The model is converted
    into a tflite model for mobile applications. For object detection on smartphones,
    YOLOv3 and YOLOv4 tiny models are utilized. The YOLOv4 tiny yielded better results
    with an accuracy of 98.13%. For disease identification from paddy leaves, a smartphone
    app called E-Crop Doctor was created. It also recommends the best pesticides for
    farmers. The application also contains a chatbot called “docCrop” that offers
    farmers around-the-clock assistance. In the study [10], Researchers developed
    an image processing system for identifying and classifying the paddy disease.
    The AdaBoost classifier is used to pinpoint the paddy plant’s diseased area. The
    percentage of detection accuracy is found to be 83.33%. The Support Vector Machine
    (SVM), Scale Invariant Feature Transform (SIFT) feature and classifiers k-Nearest
    Neighbor (k-NN) are used to classify various categories of diseases like brown
    spot, leaf blast, and bacterial blight. The main aim of the work is to detect
    the disease at the earliest stage and to take timely action to minimize crop loss.
    Accuracy is 93.33 % using k-NN and 91% using SVM. In [11], Leaf Area Index (LAI)
    estimation is done using digital Leaf images of paddy taken by UAVs. At different
    phases of the paddy crop, images with RGB color texture are taken. Linear model,
    a multi-variable regression model is used to estimate color index textures. This
    work provides an effective approach for estimating rice LAI from digital images
    captured by UAVs. In [12], the model detects Hespa, rice pests, and stem borers
    using UAVs. Data from UAV and web sources are used. Images are preprocessed and
    used for training the model and also used for analysis with another model. Images
    are analyzed with a modified new Yolo-Convolutional Neural Network (YO-CNN)-based
    better on the results detecting the disease. The research [13], discussed in detail
    how to integrate and install a UAV real-time system for monitoring paddy crops.
    Using neural networks and multivariable regressions, the system was capable of
    calculating nitrogen content and biomass changes throughout the growth cycle of
    the crop. In [14], multi-spectral imagery and Geographical Information systems
    are processed in AGI software for analyzing the Normalized Difference Vegetative
    Index to monitor the growth of the paddy crop. The NDVI map was created using
    raster analysis performed on multispectral photos using the ArcGIS software. With
    a higher NDVI value indicating a better crop and a lower NDVI value suggesting
    an unhealthy crop, the NDVI map helped identify agricultural damaged areas. In
    [15], algorithms with an ensemble classification technique were presented to assess
    bacterial disease infections in rice using images using drones. Images taken by
    UAVs were processed using Convolution Neural Networks (CNN). The paddy condition
    and infection inside the image were determined. The proposed algorithms have an
    accuracy of 89.84 percent for classifying rice illnesses. In [16], the author
    developed a smartphone app Padi2U for rice management. Multi-spectral images are
    used to get field data on rice. Padi2U offers advice for managing weeds, controlling
    pests and diseases, and checking rice conforming to the Department of Agriculture’s
    (DOA) guidelines. Vegetation index map output from the multi-spectral photos allows
    farmers to view paddy field data to determine the crop condition position online.
    The images gathered from launching height and directions are used as training
    images for Rice Bakanae Disease (RBD) recognition and also categorization algorithms
    in the paper [17]. The YOLOv3 method was set up to find RBD area bunches. The
    bounding box dimensions were used to identify and extract the RBD bunch regions.
    The ResNet 50-layer algorithm was used to classify RBD-infected culm numbers,
    and an accuracy of 80.36% was achieved. In the research paper [18], Multi-Spectral
    images from UAVs are used to find the Nitrogen content and biomass in the rice
    field. GrabCut segmentation is used for the Vegetative Index (VI). Machine learning
    models were used in the growth cycle of the plant’s vegetative, reproductive,
    and ripening and are compared. The article [19], describes the need for agronomic
    traits that are needed for yield and also for Nitrogen requirements in rice. Models
    were created in the growth stage. Vegetation indices are obtained from spectral
    cameras from UAVs. The findings demonstrated that high accuracy can be attained
    for all attributes except the Harvest Index (HI) at the booting stage as well
    as for nitrogen uptake and biomass at the tillering stage. In [20], the drone
    multirotor with RGB imaging photos used as the research’s primary input data were
    used to depict rice fields. The image was aligned and mosaiced by Structure from
    Motion (SfM) techniques by the AgisoftPhotoScan program. The empirical regression
    formula converts digital image numbers into reflectance values. The plant health
    readings range from 0 to 0.33 indicating stress, 0.33 to 0.66 moderate health,
    and 0.66 to 1 indicating good and healthy. A summary of recent research on disease
    detection in rice is presented in Table 2. Table 2 A Summary of Recent Research
    on Rice Leaf Disease Detection [6–20]. SECTION III. Methodology The methodology
    employed in this research paper encompasses several key steps to comprehensively
    explore monitoring and identifying paddy leaf diseases by Unmanned Aerial Vehicles
    (UAVs). Firstly, a thorough literature review is conducted to gain a deep kind
    of the current advanced methodologies related to UAV-based disease monitoring
    in rice crops. This review involves gathering and analyzing relevant research
    papers, articles, conference proceedings, and other scholarly resources from reputable
    journals and databases. Through this process, the paper establishes a strong foundation
    of knowledge and identifies any existing gaps or areas requiring further investigation.
    Data collection forms the next crucial step in the methodology. Various sources
    are utilized to gather relevant data for the research. Academic databases, research
    institution reports, publicly available datasets, and case studies involving UAV
    applications in paddy fields are all examined. The collected data includes information
    on disease incidence, symptoms, and severity, as well as UAV-derived aerial imagery
    and multispectral data representative of real-world scenarios [33]. The identification
    and study of disease detection techniques form a central aspect of the research
    methodology. Different techniques utilized in UAV-based disease monitoring are
    carefully examined. This includes an exploration of image processing algorithms
    for feature extraction, such as image segmentation, texture analysis, and color
    space transformations. Furthermore, the paper investigates machine learning and
    deep learning models, such as Support Vector Machines (SVMs), Random Forests,
    Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs), to
    determine their efficacy in classifying diseased and healthy rice plants. CNN
    is the best model for identification of leaf diseases [34]. To evaluate the performance
    of the identified disease detection techniques, the research designs and sets
    up appropriate experiments. Simulated or real UAV-derived data, alongside ground
    truth data on disease presence and severity, are used to confirm the correctness
    and efficiency of the particular algorithms. The experimental design takes into
    account different disease types, varying levels of disease severity, environmental
    conditions, and the impact of various UAV flight parameters on data quality [35].
    Data analysis is a critical phase where the collected data and experimental results
    are subjected to a thorough examination. Statistical methods are employed to assess
    the accuracy, sensitivity, specificity, and overall performance of the disease
    detection techniques. The analysis also involves comparing the efficiency of different
    algorithms and identifying any limitations or challenges associated with each
    method. Furthermore, the research addresses ethical considerations related with
    the use of UAVs in smart farming. This includes concerns related to privacy, data
    security, and environmental impact. Responsible UAV usage practices are proposed
    to ensure the ethical implementation of this technology [36]. Through comprehensive
    discussion, the research paper presents the findings, comparing the performance
    of different disease detection techniques, and analyzing their strengths and weaknesses.
    It explores potential future prospects, including the integration of UAVs with
    other remote sensing technologies like hyperspectral imaging and LIDAR, and their
    implications for real-time disease mapping and automated spraying systems based
    on UAV data [37]. In conclusion, the research methodology employed in this paper
    facilitates a thorough examination of the role of UAVs in monitoring and identifying
    paddy leaf diseases. By following this methodology, the paper contributes valuable
    insights to the field of agricultural technology and disease management, fostering
    advancements that can enhance global rice production and address food security
    challenges. Proper citation and referencing of all sources ensure the research’s
    credibility and acknowledge the contributions of previous studies in the field.
    SECTION IV. Standard Process of Plant Disease Detection A. Acquisition of Images
    Acquiring leaf disease images from various sources like the web, free open datasets
    of plant diseases, UAV real-time images, collected from different seasons, and
    the environment of the plant are collected and stored. The main aim is to create
    a data set for further processing [38]. B. Processing of Images Here unwanted
    image background is eliminated. The size of the image is reduced. The color, sharpness,
    and brightness of the image are enhanced. There are noises in images such as gaussian
    noise, salt and pepper noise will diminish image quality. Gaussian filter, mean
    filter are used in image processing to reduce or remove noises from images to
    enhance the quality and clarity of images [39] [40]. C. Segmentation of Images
    Image segmentation means dividing/ segmenting a digital image into multiple image
    partitions. Here the diseased leaf of the paddy plant is divided into segments
    to detect and classify the paddy disease. It investigates the paddy image to uncover
    relevant information for feature extraction [41]. D. Finding Features It is the
    process of extracting the interesting part in plant disease detection and classification.
    Here the shape, color, and texture of the disease are used to detect the dis-ease
    and classify it. The disease differs and symptoms of the paddy crop also differ.
    The paddy crop leaf disease systems are capable of quickly identifying illnesses
    based on the form, color, and texture of the diseased plant and classifying them
    [42]. E. Classification To classify and predict the correct disease in paddy plants
    various machine learning algorithms are used. Deep learning with more convolution
    layers is used to classify and predict the disease. Convolutional Neural Networks
    (CNN) are widely used for image detection and classification. Leaf disease characteristics
    are extracted from an image called pooling. Transfer learning and pre-training
    on ImageNet were employed with CNN models [40]. SECTION V. Protocols Strength
    and Limitations For leaf image disease detection, both supervised and unsupervised
    learning approaches can be utilized, depending on the availability of labelled
    data and the complexity of the task. Also, deep learning algorithms are highly
    effective due to their ability to automatically learn complex patterns from images.
    One of the most commonly used and suitable deep-learning designs for this task
    is CNNs [43]. A. Supervised Learning Supervised Learning: Supervised learning
    algorithms require labelled training data, where each leaf image is associated
    with a corresponding label indicating the presence or absence of a disease. If
    you have a substantial amount of accurately labelled data, supervised learning
    algorithms such as CNNs, SVMs, Random Forests, and Gradient Boosting can be suitable
    choices. CNNs are particularly effective for image-based tasks like disease detection,
    as they can automatically learn relevant features from the images and identify
    patterns indicative of diseases [44]. B. Unsupervised Learning Unsupervised learning
    algorithms don’t require labelled data but focus on finding patterns or structures
    within the data itself. They can be useful when labelled data is scarce or hard
    to obtain. Techniques like clustering and dimensionality reduction can aid in
    discovering groupings of similar leaf images or reducing the data’s complexity
    [45]. C. Hybrid Approaches In some cases, a hybrid approach could be beneficial.
    We could use unsupervised learning to pre-process or segment the data into meaningful
    clusters or groups, and then apply supervised learning within each cluster to
    fine-tune disease detection algorithms [46]. D. Deep Learning Algorithms CNNs
    are well-suited for image-related tasks like disease detection in paddy plants.
    They can automatically extract hierarchical features from images, capturing both
    low-level details and high-level patterns. It is made up of convolutional layers
    that combine the input image with learnable filters to extract features. Pooling
    layers are used to down sample the extracted features and reduce the spatial dimensions.
    These features are then passed through fully connected layers for classification.
    Transfer learning can also be applied by using pre-trained CNN models like ResNet,
    Inception or VGG which were developed using extensive datasets like ImageNet.
    By fine-tuning these models on paddy disease dataset, you can achieve good results
    even with limited labelled data. SECTION VI. Discussion While UAVs have shown
    great promise in disease detection, some studies have also highlighted challenges
    and limitations. Environmental factors, such as cloud cover and lighting conditions,
    can affect the quality of UAV imagery and subsequently impact disease identification
    accuracy. Additionally, regulatory restrictions and safety protocols related to
    UAV flights have been noted as potential barriers to widespread adoption. Overall,
    the literature review establishes that UAVs have emerged as a transformative technology
    in agriculture, particularly in the context of paddy leaf disease monitoring.
    The integration of advanced image processing techniques and machine learning algorithms
    has enabled accurate disease identification, while real-time data acquisition
    has facilitated timely intervention and effective disease management. Further
    research is needed to optimize UAV disease detection and classification process.
    Moreover to enable small farmers to utilize UAV resources, UAV manufacturers,
    agricultural scientists, and researchers should provide an inexpensive drone.
    The discussion section of this research paper focuses on the analysis and interpretation
    of the findings obtained from the literature review, data analysis, and experimental
    results. It provides a critical examination of the effectiveness and limitations
    of using UAVs for monitoring and identifying paddy leaf diseases. The discussion
    aims to draw meaningful insights, propose potential solutions to challenges, and
    highlight the significance of UAV-based disease monitoring in rice crops. Firstly,
    the research paper discusses the performance of different disease detection techniques
    identified in the literature review. It compares the accuracy and efficiency of
    image processing algorithms, such as image segmentation, texture analysis, and
    color space transformations, in extracting relevant features indicative of disease
    presence. Convolutional neural networks, Random Forest, Support vector machines
    (SVMs), and recurrent neural networks (RNNs) are among the machine learning and
    deep learning models that have been evaluated for their ability to distinguish
    between damaged and healthy rice plants. The discussion sheds light on the strengths
    and weaknesses of each technique and offers insights into their practical implications
    for disease management. Furthermore, the research paper examines the practical
    challenges faced in UAV-based disease monitoring. Environmental factors, such
    as weather conditions and lighting, can affect the quality of UAV-captured imagery
    and influence disease detection accuracy. The discussion explores potential mitigation
    strategies, such as optimizing flight schedules and using image enhancement techniques,
    to improve data quality under challenging environmental conditions. Additionally,
    the paper addresses concerns related to data processing complexities, highlighting
    the need for efficient algorithms and computing resources to handle large volumes
    of UAV-derived data. The discussion also delves into ethical considerations associated
    with the use of UAVs in smart agriculture. Privacy concerns related to aerial
    surveillance of farmlands and data security are addressed, emphasizing the importance
    of implementing responsible UAV usage practices. The research paper proposes guidelines
    and best practices to protect the privacy rights of farmers and landowners while
    maximizing the benefits of UAV technology in disease monitoring. SECTION VII.
    Conclusion The conclusion of this research paper provides a summary of the key
    findings and insights obtained from the study on monitoring and identifying paddy
    leaf diseases using Unmanned Aerial Vehicles (UAVs). It highlights the significance
    of UAV technology in revolutionizing disease monitoring practices in rice crops
    and its potential impact on global food security. The research paper concludes
    that UAVs offer a transformative approach to disease monitoring in paddy fields.
    By leveraging high-resolution aerial imagery and multispectral data, UAVs enable
    rapid and accurate detection of paddy leaf diseases, including blast, sheath blight,
    and bacterial leaf blight. The integration of image processing algorithms and
    machine learning models enhances disease identification accuracy and provides
    timely information for effective disease management strategies. The research paper
    also emphasizes the practical advantages of using UAVs in disease monitoring.
    UAV surveys cover large agricultural areas rapidly, overcoming the limitations
    of traditional manual scouting methods. This time and cost efficiency make UAVs
    attractive for farmers and agricultural practitioners. Ethical considerations
    related to UAV usage in agriculture are also highlighted in the conclusion. The
    paper proposes responsible UAV usage practices to protect the privacy rights of
    farmers and landowners while maximizing the benefits of UAV-based disease monitoring.
    In conclusion, the research paper demonstrates that UAVs Ultimately, the conclusion
    reinforces the importance of UAVs in contributing to global food security by facilitating
    more efficient and effective disease management practices in rice crops. Pest
    and nutrient management techniques should be implemented using drone applications.
    ACKNOWLEDGEMENT The author acknowledge the support and funding project “Drone
    based Paddy Crop Management System for Pest Control” by DST-NMICPS, TiHAN IIT
    Hyderabad. Authors References Keywords Metrics More Like This Design of Real-Time
    System Based on Machine Learning for Snoring and OSA Detection ICASSP 2022 - 2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
    Published: 2022 An integrated approach of Genetic Algorithm and Machine Learning
    for generation of Worst-Case Data for Real-Time Systems 2022 IEEE/ACM 26th International
    Symposium on Distributed Simulation and Real Time Applications (DS-RT) Published:
    2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 'Proceedings of 2023 IEEE 2nd International Conference on Industrial Electronics:
    Developments and Applications, ICIDeA 2023'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Monitoring and Identifying Paddy Leaf Diseases Using Unmanned Aerial Vehicles
    (UAVs) with Machine Learning- A Survey
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Woodson M.
  - Zhang J.
  citation_count: '0'
  description: Advances in computer vision have resulted in promising research and
    applications in the domain of precision agriculture. In particular, Deep Learning
    has rapidly improved state of the art in object detection and segmentation, both
    of which prove vital in crop monitoring and yield forecasting. In most object
    detection problems, transfer learning serves as the established paradigm for applying
    Deep Learning systems on downstream tasks. This is particularly important in agricultural
    vision applications, where available data is relatively scarce compared to the
    large demand required of deep learning systems. Recent advances in Self-Supervised
    Learning have generated pretraining methods that approach the transfer performance
    of supervised pretraining on a variety of downstream tasks. To demonstrate the
    impact of Self-Supervised learning in agriculture, this paper evaluates the transfer
    performance of one self-supervised method, BYOL, on grape cluster detection. By
    comparing BYOL with supervised pretraining on a Faster R-CNN architecture, this
    work demonstrates that Self-Supervised Learning is competitive with other supervised
    pretraining methods in agricultural applications, showing its promise in advancing
    precision agriculture to more accurate and robust solutions.
  doi: 10.1007/978-3-031-37717-4_68
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Intelligent Computing Conference
    paper Evaluating Self-supervised Transfer Performance in Grape Detection Conference
    paper First Online: 01 September 2023 pp 1043–1057 Cite this conference paper
    Access provided by University of Nebraska-Lincoln Download book PDF Download book
    EPUB Intelligent Computing (SAI 2023) Michael Woodson & Jane Zhang  Part of the
    book series: Lecture Notes in Networks and Systems ((LNNS,volume 711)) Included
    in the following conference series: Science and Information Conference 387 Accesses
    Abstract Advances in computer vision have resulted in promising research and applications
    in the domain of precision agriculture. In particular, Deep Learning has rapidly
    improved state of the art in object detection and segmentation, both of which
    prove vital in crop monitoring and yield forecasting. In most object detection
    problems, transfer learning serves as the established paradigm for applying Deep
    Learning systems on downstream tasks. This is particularly important in agricultural
    vision applications, where available data is relatively scarce compared to the
    large demand required of deep learning systems. Recent advances in Self-Supervised
    Learning have generated pretraining methods that approach the transfer performance
    of supervised pretraining on a variety of downstream tasks. To demonstrate the
    impact of Self-Supervised learning in agriculture, this paper evaluates the transfer
    performance of one self-supervised method, BYOL, on grape cluster detection. By
    comparing BYOL with supervised pretraining on a Faster R-CNN architecture, this
    work demonstrates that Self-Supervised Learning is competitive with other supervised
    pretraining methods in agricultural applications, showing its promise in advancing
    precision agriculture to more accurate and robust solutions. Keywords Object Detection
    Self-Supervised Learning Agriculture Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others Semantic
    Segmentation of Vineyard Images Using Convolutional Neural Networks Chapter ©
    2020 Residual Cascade CNN for Detection of Spatially Relevant Objects in Agriculture:
    The Grape-Stem Paradigm Chapter © 2023 Acre-Scale Grape Bunch Detection and Predict
    Grape Harvest Using YOLO Deep Learning Network Article 03 February 2024 1 Introduction
    People across the world rely on the availability of fresh foods to feed themselves
    and their families. Whether it be farming processes that ensure the health and
    abundance of natural fruits and vegetables, or practices that provide dairy and
    meats, agriculture has remained an integral component of our lives. The future
    of agriculture faces many challenges in addressing the needs of a rapidly growing
    world population: climate change, increasing demand for energy, resource shortages,
    and labor shortages [26]. These challenges have given rise to Precision Agriculture,
    which utilizes technology to improve the efficiency of agricultural processes.
    Yield forecasting is one important process in agriculture. Yield forecasting is
    a technique where farmers obtain early crop counts to estimate the expected yield
    at harvest. This can help farmers accurately prepare for harvest, providing data
    to inform decisions in packaging, marketing, and labor requirements [37]. However,
    yield forecasting remains manual: workers sample a subset of the vineyard, obtaining
    fruit counts, and extrapolate these numbers to cover the entirety of the vineyard,
    taking historical yields, weather, and field measurements into account [37]. Consequently,
    this manual process introduces a variety of biases and errors into the eventual
    yield forecast. To improve the accuracy and speed of forecasting, research has
    focused on applying computer vision to crop detection. By developing a system
    that can automatically detect and count crops, one could deploy an image acquisition
    system throughout a field to obtain accurate counts early in the harvest cycle.
    Before deep learning gained popularity, classical vision approaches were applied
    to various crop counting tasks. However, classical techniques were challenged
    by natural environmental conditions associated with agricultural applications
    such as dynamic lighting, weather patterns, and heavy occlusion. Deep learning,
    combined with the practice of transfer learning, has helped address such challenges
    to further advance state-of-the-art in agricultural tasks such as crop detection
    and phenotyping. While transfer learning is effective in agriculture, many systems
    are constrained by the current paradigm of supervised pretraining. Supervised
    pretraining requires that massively large image databases, such as ImageNet [11],
    are specially curated and largely annotated. Labeling such a dataset comes with
    great cost, as these datasets contain millions of images. This high cost, as a
    result, restricts the domain of learnable features, since labeled data comprises
    a small subset of all possible images that networks could learn from. Furthermore,
    obtaining labeled data becomes much more difficult across different imaging modalities.
    When considering the potential for agricultural applications to leverage multi-modal
    data, it’s important to explore training methods that obviate the need for ground-truth
    labels, broadening the scope of learnable features that transfer well to downstream
    tasks such as crop detection. Such considerations have given rise to a class of
    pretraining called Self-Supervised Learning, where features can be learned from
    data without class labels. Self-Supervised Learning (SSL) has shown success in
    Natural Language Processing (NLP), where networks can learn word embeddings by
    completing tasks using unlabeled data, such as the masked word prediction task
    used in BERT [12]. These word embeddings carry great semantic value in the feature
    space, transferring well to other language tasks. The success of SSL in NLP has
    motivated vision-based training approaches that can learn from image data itself,
    rather than relying on annotations which describe the images. Recent self-supervised
    techniques have approached, or even surpassed, the success of supervised pretraining
    for image classification tasks, while displaying an ability to accurately transfer
    to a variety of downstream tasks such as dense prediction, few-shot recognition,
    and object detection [16]. While SSL has been shown to perform well in various
    imaging benchmarks, the effectiveness of self-supervised transfer performance,
    to the best of our knowledge, has not yet been analyzed in an agricultural field
    setting. Given the early success of Self-Supervised Learning on various benchmark
    datasets, it’s important to analyze its impact on real-world applications such
    as crop detection. As a result, this work’s contributions are as follows: We evaluated
    the transfer performance of BYOL (Bootstrap Your Own Latent) [19], a state-of-the-art
    SSL pretraining method, on grape cluster detection. We compared BYOL transfer
    performance to supervised pretraining methods, particularly pretraining on ImageNet
    image classification and COCO [29] object detection. By demonstrating whether
    SSL can compete with supervised pretraining in an important agricultural task,
    we hope to illustrate its effectiveness as a powerful feature generator, while
    illuminating its potential to learn features that are unconstrained by annotated
    datasets. The paper is outlined as follows: Sect. 2 conducts a review of various
    literature in fruit detection and yield forecasting. This is followed by Sect.
    3, which provides a background of Self-Supervised Learning. Section 4 then outlines
    the experimental methods used to compare BYOL with supervised pretraining, followed
    by Sect. 5, which demonstrates the results of the experiment. Finally, this paper
    offers concluding remarks in Sect. 6. 2 Related Work There exists a variety of
    research in the area of crop detection and phenotyping. One category of research
    focuses on hand-crafted features to characterize and classify crops. Color-based
    segmentation can be used to identify fruit pixels [31, 38, 40], though such approaches
    struggle to generalize across various environments, lighting conditions, and ripening
    stages. Texture and SIFT features can help improve crop detection when crop color
    blends in with the surrounding environment, [2, 30, 37, 39], but such approaches
    often utilize artificial illumination, which restricts widespread use. More recent
    approaches utilize deep learning for both crop detection and segmentation tasks.
    Fruit segmentation maps can be generated by applying networks in a sliding window
    fashion [7, 33]. Segmentation maps cannot count fruit by itself, so some work
    introduced a second stage which learns to count fruit for each segmentation mask
    [8, 20]. Alternatively to segmentation, counting applications can employ object-detection
    frameworks to identify separate crop instances. Research has used both single-shot
    detectors [5, 47] and region-proposal networks for fruit detection [3, 18, 45],
    with the Faster R-CNN [41] seeing greater success at localizing crop instances.
    For some crop, localizing crop instances for yield forecasting is ambiguous; one
    could approach grape detection, for example, by identifying grape clusters or
    individual grape berries, which presents a trade-off between yield accuracy and
    annotation costs. At the cost of greater annotation demands, [48] counts individual
    grape berries by treating detection as a semantic segmentation task, assigning
    pixels to background, berry, and berry edge classes, then using connected components
    to count the individual berries. Alternatively, work by [10] adapted crowd-counting
    networks to grape berry counting tasks, using dot annotations to train a network.
    Although deep learning improved the accuracy and robustness of fruit detection
    systems, deep learning still struggles to cope with occlusion caused by overlapping
    crop and dense canopies. RGB-D cameras, LIDAR, and photogrammetry techniques can
    alleviate occlusion effects [13, 22, 27, 42, 46], but deep learning systems tend
    to operate on RGB image data for object detection, as networks are often pretrained
    on ImageNet-like data before transfer learning. Likewise, while agriculture vision
    often features multispectral data such as IR [17, 23, 43] to counteract difficult
    conditions such as direct sunlight exposure, supervised transfer learning is limited
    in providing pre-learned representations across various input modalities. Meanwhile,
    Self-Supervised Learning has shown success in discovering semantically strong
    feature embeddings without labels. Networks pretrained with SSL can outperform
    ImageNet supervised transfer performance on a variety of different datasets, including
    Cars [25], Flowers [35], and Food101 [4]; SSL especially performs well in a semi-supervised
    setting [6, 9, 19, 49]. This is important for agriculture, where multispectral
    annotations are sparse. Furthermore, SSL can be applied on 3-D data, learning
    representations that are invariant to 3-D input formats [50] or finding representations
    that associate 3-D objects with 2-D renderings [1]. Such models can transfer to
    tasks such as 3-D object detection or 3-D segmentation, potentially allowing agriculture
    systems to acquire 3-D scene understanding. 3 Background 3.1 Self-supervised Learning
    Self-Supervised Learning (SSL) is a subset of unsupervised learning that aims
    to identify useful structures in data without requiring ground-truth annotations
    to guide the learning process. Self-supervised methods learn by forming annotations
    from the data itself, then optimizing through a learning task that utilizes these
    annotations. SSL approaches can be further divided into pretext methods and contrastive
    methods, which are further explained in the sections below. SSL has recently gained
    momentum in vision, where SSL can effectively pretrain networks on large datasets
    without labels, uncovering useful feature representations in the process. This
    approach allows models to develop a strong semantic understanding of a wide variety
    of data subjects and modalities, even extending well to long-tailed distributions
    [50]. 3.2 Pretext Tasks Pretext tasks attempt to learn from data by solving tasks
    associated with the input image. A parallel can be found in NLP: BERT [12], for
    example, would accept input sentences with random words masked out, and the pretext
    task involved training a classifier to predict the missing words. By solving this
    pretext task, the encoder network generated features that effectively modeled
    language semantics. Self-supervised pretext tasks in vision often involve modifying
    an input image such that annotations can be formed automatically. For example,
    [14] extracted a random pair of patches from an image, fed the patches into a
    siamese network, then predicted the patch configuration using a softmax classifier.
    By predicting the relative position of one patch to another (from a set known
    patch configurations), [14] was able to learn features that transferred well to
    object detection. Various other pretext tasks were designed to improve transfer
    performance on common image benchmarks. For example, [36] attempted to solve jigsaw
    puzzles by splitting input images into patches and permuting the generated patches.
    This paper expanded upon [14] by feeding each patch individually into a siamese-ennead
    network, generating 9 feature vectors that were combined and classified as 1 of
    the possible 64 permutations. The work by [15] expanded upon the previous works
    by combining multiple pretext tasks when training a ResNet-101-v2 [21]. This was
    achieved by using separate network heads, one for each task, and updating weights
    from a shared network trunk. By combining gradients from a diverse set of pretext
    tasks such as image colorization, patch relative-position prediction, exemplar
    learning, and motion segmentation, [15] found that transfer performance improved
    compared to using a single pretext task, demonstrating that features from separate
    tasks are complimentary. However, pretext methods contain several problems. First,
    transfer performance largely depends on choosing a pretext task that bears association
    to the downstream application. Furthermore, combining pretext tasks can complicate
    the network architecture and training procedure [15]. Lastly, pretext learning
    generates features that degrade in quality in the final layers of the network.
    This occurs because network layers are optimized to solve a particular pretext
    task rather than discover an optimal feature representation. Therefore, contrastive
    methods have recently gained the most attention. 3.3 Contrastive Learning Rather
    than solving pretext tasks, contrastive methods attempt to discover feature embeddings
    that are invariant to image transformations. For example, if an image were input
    into a network, the final-layer features should be highly similar to the features
    produced by an augmentation of that same image. This idea forms the basis of contrastive
    methods, which aims to learn representations that are invariant to low-level transformations,
    thus capturing image semantics. Contrastive methods operate as follows: By inputting
    an image I and an augmented version of that image \\(I^\\textrm{t}\\), a network
    is optimized to produce a pair of feature vectors \\(\\mathbf {v_I} = f(I,\\theta
    )\\) and \\(\\mathbf {v_{I^t}} = f(I^t,\\theta )\\) such that \\(\\mathbf {v_I}
    \\approx \\mathbf {v_{I^t}}\\). Network parameters are optimized by minimizing
    the cost function (1) [34], which has the effect of maximizing agreement between
    the two feature embeddings: $$\\begin{aligned} \\hat{\\theta } = argmin_\\theta
    -{\\log [{\\langle \\mathbf {v_I},\\mathbf {v_{I^t}} \\rangle }]} \\;, \\end{aligned}$$
    (1) where \\(\\langle \\cdot ,\\cdot \\rangle \\) is often the cosine similarity.
    This process, however, leaves a network prone to trivial solutions, where all
    joint embeddings are identical for every input pair. Therefore, contrastive methods
    introduce negative samples into the learning process, jointly maximizing the similarity
    between positive examples and minimizing the similarity measure between negative
    samples. Since images are unlabeled, a negative sample is any image \\(I''\\)
    that is different than the original image I. Therefore, contrastive learning,
    in general, processes an image I, a transformed version of that image \\(I^\\textrm{t}\\)
    (which acts as a positive sample), and a set of negative samples \\(I'' \\in D_\\textrm{N}
    = \\{I_1...I_\\textrm{N}\\}\\). By introducing a set of negative samples into
    the training objective, many contrastive methods formulate a variant of the following
    loss [9]: $$\\begin{aligned} L_\\theta = -{\\log \\frac{\\exp (\\langle \\mathbf
    {v_{I}},\\mathbf {v_{I^t} \\rangle }/\\tau )}{\\exp (\\langle \\mathbf {v_{I}},\\mathbf
    {v_{I^t}} \\rangle /\\tau ) + \\sum _{I'' \\in D_\\textrm{N}} \\exp (\\langle
    \\mathbf {v_{I''}},\\mathbf {v_{I^t}} \\rangle / \\tau )}} \\;, \\end{aligned}$$
    (2) where \\(D_\\textrm{N}\\) is a set of randomly-selected negative samples from
    the training set, \\(\\langle \\mathbf {v_{I}},\\mathbf {v_{I^t}} \\rangle \\)
    is the cosine similarity \\(\\mathbf {v_{I}} \\cdot \\mathbf {v_{I^t}} / \\Vert
    \\mathbf {v_{I}}\\Vert \\Vert \\mathbf {v_{I^t}}\\Vert \\), and \\(\\tau \\) is
    a temperature value that can be set as a hyperparameter. Using the above framework,
    contrastive learning methods approach ImageNet supervised pretraining on a variety
    of downstream tasks [34]. Furthermore, unlike pretext features, contrastive features
    do not degrade in the deepest layers of a network. A downside to contrastive learning,
    however, is that effective learning requires a large quantity of negative samples.
    This is required since many negative samples make poor training examples, so methods
    must increase the batch size as a way to provide a sufficient pool of useful negatives.
    For example, [9] used up to 16384 negative samples per positive image pair. While
    contrastive methods have shown great promise in generating useful features in
    a transfer setting, the expensive training requirements has motivated recent methods
    to avoid negative samples. BYOL [19], the SSL method evaluated in this work, established
    a training procedure that required only positive image pairs. This was achieved
    by employing an online network and a target network. Like other contrastive methods,
    each network received augmented versions of the same image. However, while the
    online and target networks were identical in structure, the online network and
    the target network had different sets of weights. Specifically, the target weights
    \\(\\xi \\) were an exponential moving average of the online network weights,
    \\(\\theta \\). Therefore, while the online network received weight updates through
    its gradient, the target network avoided gradient updates via a stop-gradient,
    and instead updated its weights as follows [19]: $$\\begin{aligned} \\xi = \\tau
    \\xi - (1-\\tau )\\theta \\;, \\end{aligned}$$ (3) where \\(\\tau \\) is a target
    decay weight \\(\\tau \\in [0,1]\\). Since networks will generate different feature
    vectors as a result of differing weights, the online network also contains an
    additional prediction module \\(q_\\theta \\), which attempts to map the online
    feature to the target network feature. The BYOL architecture uses a normalized
    Euclidean distance between the output vectors as the loss metric, represented
    as $$\\begin{aligned} L_{\\theta , \\xi }(q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}), \\mathbf{{\\overline{z''}_\\xi }}) = \\Vert q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}) - \\mathbf{{\\overline{z''}_\\xi }}\\Vert ^2 = 2 - 2q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}) \\cdot \\mathbf{{\\overline{z''}_\\xi }} \\;, \\end{aligned}$$ (4) where \\(\\mathbf
    {z_\\theta }\\) and \\(\\mathbf {z_\\xi }\\) are joint embeddings produced by
    network pair, and \\(\\mathbf {\\overline{z}_\\theta } = \\mathbf {z_\\theta }/\\Vert
    \\mathbf {z_\\theta }\\Vert \\) and \\(\\mathbf {\\overline{z''}_\\xi } = \\mathbf
    {z_\\xi } / \\Vert \\mathbf {z_\\xi }\\Vert \\) are normalized to unit length
    [19]. 4 Methods and Materials 4.1 Data The Embrapa Wine Grape Instance Segmentation
    Dataset (WGISD)[44], provided by Santos et al. [45], was used for both training
    and evaluation. This dataset contains 300 images of grapes captured at the Guaspari
    Winery in Espírito Santo do Pinhal, São Paulo, Brazil. The dataset consists of
    five different grape varieties: Chardonnay, Cabernet Franc, Cabernet Sauvignon,
    Sauvignon Blanc, and Syrah. Examples of the grape varieties can be seen in Fig.
    1. The different grape varieties offer a diverse set of visual characteristics,
    differing in color, size, and compactness. To prevent network bias toward any
    grape variety, the dataset contains near-equal examples of each variety. The dataset
    comes with bounding box and mask annotations. While box annotations are provided
    with every image in the dataset, mask annotations were applied to a random subset
    of 110 images. This amounts to a total of 4431 boxes and 2020 binary masks in
    the dataset. To maximize the amount of data available for training and testing,
    we focus on grape cluster detection using bounding boxes, ignoring the mask segmentation
    data. The ground-truth bounding boxes were manually annotated by Santos et al.
    [45]. In order to evaluate the performance of the network, the dataset was partitioned
    into training and test sets. The training partition contains 80% of the dataset,
    while 20% was reserved for testing. Additionally, 20% of the training data was
    reserved for validation. The data was partitioned such that both the training
    and test sets were balanced with respect to grape varieties. The exact details
    of how data was partitioned can be seen in Table 1; we followed the recommendations
    provided by [45]. The grape varieties are provided for illustrative purposes,
    as evaluation was performed over the entire test set without analyzing performance
    on specific varieties. Fig. 1. Different Grape Varieties: a Chardonnay b Cabernet
    Sauvignon c Sauvignon Blanc d Cabernet Franc e Syrah Full size image Table 1.
    Dataset Partitions by Variety Full size table 4.2 Training Procedure To study
    the current capabilities of SSL transfer performance on grape detection, we selected
    two supervised pretraining methods as a reference of comparison. First, we used
    weights initialized from ImageNet supervised pretraining. Additionally, since
    we evaluated SSL in an object-detection task, we also used weights initialized
    from supervised pretraining on COCO object detection. As part of this work, we
    aimed to follow the procedure and build upon the results of [45], as we used the
    same dataset. As a result, like [45], we treat grape detection as an object detection
    task, where grape clusters must be localized with bounding boxes. When considering
    grape detection for the purposes of yield forecasting, it is also sensible to
    count individual grape berries in an image. However, we reserve alternative approaches
    for future research, as we first aim to compare BYOL to a reference experiment
    utilizing supervised pretraining. We also used the same set of augmentations for
    training as [45]: random pixel dropout, additive gaussian noise, gaussian blur,
    contrast enhancement, and horizontal flipping. A random subset of augmentations
    were applied to each image batch during training. A Faster R-CNN with a ResNet-50
    backbone and an FPN [28] was used for this experiment. We used the AdamW optimizer
    [32] and performed a hyperparameter search over the following hyperparameters:
    the learning rate, the decoupled weight decay factor in AdamW, and the NMS threshold
    used in the Faster R-CNN. The hyperparameter search was conducted using Population
    Based Training [24]. All other hyperparameters are set to the defaults established
    in the torchvision library. While we designed an experiment to compare the transfer
    performance of 3 pretraining methods (Supervised ImageNet, Supervised COCO, and
    BYOL), we also considered the effect of freezing different ResNet layers. More
    specifically, we looked at three finetuning scenarios: First, we froze all 5 ResNet
    layers, only updating FPN and Faster R-CNN weights. Next, we finetuned the last
    ResNet layer, followed by finetuning the entire backbone. These scenarios were
    coined “Freeze 5\", “Freeze 4\", and “Freeze 0\", respectively, corresponding
    to the number of ResNet layers that were frozen during training. It is worth mentioning
    that COCO pretraining was the only method that initialized FPN weights; otherwise,
    both the FPN and Faster R-CNN weights were trained from scratch while finetuning.
    Lastly, while evaluation was reported on the WGISD test partition using COCO AP
    metrics, we also analyzed performance on a separate grape dataset: the CR2 dataset
    [10]. Since the CR2 dataset lacks bounding-box annotations (it includes dot annotations
    instead), performance was observed by visual inspection only. However, introducing
    this dataset helped illustrate the ability of each pretraining method and each
    finetuning scenario at generalizing to a new variety (Teroldego) in a different
    vineyard. Detection on the CR2 dataset is presented in Sect. 5. 5 Results Numerical
    results for all pretraining methods and finetuning scenarios can be seen in Table
    2. COCO AP ranged from 0.404 to 0.551 and \\(AP_{50}\\) ranged from 0.808 to 0.886,
    with BYOL achieving 0.505 AP and 0.877 \\(AP_{50}\\). A network trained from scratch
    was added for reference. In general, BYOL was competitive with supervised pretraining
    methods, even outperforming ImageNet supervised pretraining for all finetuning
    scenarios. COCO supervised pretraining consistently performed the best. This is
    possibly due to the fact that COCO pretraining involves object detection, which
    matches the downstream task at hand. Furthermore, COCO pretraining initialized
    FPN weights, while BYOL and ImageNet supervised pretraining do not. However, when
    considering the potential to effortlessly add training data for BYOL (and SSL
    methods in general) compared to supervised counterparts, the competitive results
    are promising. These results are an improvement over [45], which reported a 0.71
    \\(AP_{50}\\). This improvement is likely because [45] trained with the subset
    of WGISD containing mask annotations. Table 2. Transfer Performance of All Pretraining
    Methods. Results are Provided using COCO AP, which Computes Average Precision
    over a Range of IoU Thresholds. Additionally, \\(AP_{50}\\) is Provided, which
    Represents AP at a Single IoU Threshold of 0.5. A Network Trained from Scratch
    is Reported for Reference. Supervised COCO Pretraining is most Accurate, while
    BYOL is the Second Most Accurate. Finetuning the Entire Network Improves Performance
    in every Case Full size table One particularly interesting case is the relative
    performance between “Freeze 5\" and “Freeze 4\" scenarios. SSL methods employing
    pretext tasks observe a phenomenon where deep layers end up optimizing for the
    task being solved, resulting in poor transfer performance. By introducing “Freeze
    4\", we could test whether downstream applications benefited from updating final-layer
    features. However, in general, finetuning the final ResNet layer had little effect
    on the performance for grape detection, indicating that SSL features were well-suited
    for downstream tasks. Image results for each pretraining method can be seen in
    Fig. 2. Solid bounding boxes indicate true positives, while dashed boxes are false
    positives. Only bounding boxes with confidence score > 0.7 are displayed, with
    an IoU threshold of 0.5. Consistent with the numerical results, image results
    demonstrate that each pretraining method is competitive with the others. At times,
    detection accuracy suffers when cluster boundaries are ambiguous. Without in-field
    or 3-D data, cluster boundaries are difficult to discern, even for human annotators.
    Figure 3 demonstrates the effect of freezing different backbone layers with a
    BYOL-trained network. These results suggest that one should finetune an entire
    network for best performance on grape detection, as freezing layers produced images
    with overlapping bounding-box placement. Lastly, Fig. 4 displays BYOL transfer
    performance on an unseen dataset, CR2. Despite the distinct visual characteristics
    compared to WGISD, performance actually generalized well, even detecting overlapping
    clusters. Similar to detection on WGISD, CR2 performance appeared to improve when
    the entire backbone was finetuned during training. Fig. 2. Image Results for Networks
    Initialized via Different Pretraining Methods. True Positives (Solid Bounding
    Box) and False Positives (Dashed Bounding Box) are marked. Detections have Confidence
    Score > than 0.7 and an IoU Threshold of 0.5. a Network Pretrained with ImageNet
    Supervised Classification. b Network Pretrained with BYOL SSL Method. c Network
    Pretrained with COCO Supervised Object Full size image Fig. 3. Cluster Detection
    Performance for a BYOL-Trained Network when Freezing Different Layers. True Positives
    (Solid Bounding Box) and False Positives (Dashed Bounding Box) are Marked. a Freeze
    5 b Freeze 4 c Freeze 0 Full size image Fig. 4. BYOL Transfer Performance on the
    CR2 Dataset. Detections have Confidence Score > 0.7. The CR2 Dataset was not seen
    during Training and does not Contain Ground-Truth Bounding-Box Annotations, so
    Evaluation is by Inspection only Full size image 5.1 Future Work Research should
    explore training methods where Self-Supervised Learning is applied to datasets
    outside of ImageNet. Since SSL is not constrained to annotated data, datasets
    can increase in size, potentially allowing models to increase in size to carry
    greater learning capacity. Such systems can be applied to agriculture to continue
    to improve upon state-of-the-art. Furthermore, SSL should be applied across data
    modalities, with applications in agriculture leveraging new features for more
    dynamic vision systems. 6 Conclusion This work studied the performance capabilities
    of BYOL, a Self-Supervised Learning method, when transferred to an agricultural
    application such as grape cluster detection. BYOL transfer performance was compared
    to supervised pretraining methods using a Faster R-CNN with a ResNet-50 FPN backbone.
    Specifically, BYOL was compared with supervised pretraining on ImageNet and COCO.
    BYOL transfer performance was competitive with other pretraining methods, illustrating
    its future potential in learning features across data modalities, allowing deep
    learning to make progress in agricultural applications ranging from yield forecasting
    to crop phenotyping. References Afham, M., Dissanayake, I., Dissanayake, D., Dharmasiri,
    A., Thilakarathna, K., Rodrigo, R.: CrossPoint: self-supervised cross-modal contrastive
    learning for 3D point cloud understanding. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9902–9912 (2022) Google Scholar   Aquino,
    A., Millan, B., Diago, M.-P., Tardaguila, J.: Automated early yield prediction
    in vineyards from on-the-go image acquisition. Comput. Electron. Agric. 144, 26–36
    (2018) Article   Google Scholar   Bargoti, S., Underwood, J.: Deep fruit detection
    in orchards. In: 2017 IEEE International Conference on Robotics and Automation
    (ICRA), pp. 3626–3633. IEEE (2017) Google Scholar   Bossard, L., Guillaumin, M.,
    Van Gool, L.: Food-101 – mining discriminative components with random forests.
    In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,
    vol. 8694, pp. 446–461. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10599-4_29
    Chapter   Google Scholar   Bresilla, K., Perulli, G.D., Boini, A., Morandi, B.,
    Grappadelli, L.C., Manfrini, L.: Single-shot convolution neural networks for real-time
    fruit detection within the tree. Front. Plant Sci. 10, 611 (2019) Google Scholar   Caron,
    M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised
    learning of visual features by contrasting cluster assignments. Adv. Neural Inf.
    Process. Syst. 33, 9912–9924 (2020) Google Scholar   Cecotti, H., Rivera, A.,
    Farhadloo, M., Pedroza, M.A.: Grape detection with convolutional neural networks.
    Expert Syst. Appl. 159, 113588 (2020) Google Scholar   Chen, S.W., et al.: Counting
    apples and oranges with deep learning: a data-driven approach. IEEE Robot. Autom.
    Lett. 2(2), 781–788 (2017) Google Scholar   Chen, T., Kornblith, S., Norouzi,
    M., Hinton, G.: A simple framework for contrastive learning of visual representations.
    In: International Conference on Machine Learning, pp. 1597–1607. PMLR (2020) Google
    Scholar   Coviello, L., Cristoforetti, M., Jurman, G., Furlanello, C.: GBCNet:
    in-field grape berries counting for yield estimation by dilated CNNs. Appl. Sci.
    10(14), 4870 (2020) Article   Google Scholar   Deng, J., Dong, W., Socher, R.,
    Li, L.-J., Li, K., Fei-Fei, L.: ImageNet: a large-scale hierarchical image database.
    In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255.
    IEEE (2009) Google Scholar   Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.:
    BERT: pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805 (2018) Dey, D., Mummert, L., Sukthankar, R.: Classification
    of plant structures from uncalibrated image sequences. In: 2012 IEEE Workshop
    on the Applications of Computer Vision (WACV), pp. 329–336. IEEE (2012) Google
    Scholar   Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation
    learning by context prediction. In: Proceedings of the IEEE International Conference
    on Computer Vision, pp. 1422–1430 (2015) Google Scholar   Doersch, C., Zisserman,
    A.: Multi-task self-supervised visual learning. In: Proceedings of the IEEE International
    Conference on Computer Vision, pp. 2051–2060 (2017) Google Scholar   Ericsson,
    L., Gouk, H., Hospedales, T.M.: How well do self-supervised models transfer? In:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 5414–5423 (2021) Google Scholar   Feng, J., Zeng, L., He, L.: Apple fruit
    recognition algorithm based on multi-spectral dynamic image analysis. Sensors
    19(4), 949 (2019) Article   Google Scholar   Ge, Y., Xiong, Y., From, P.J.: Instance
    segmentation and localization of strawberries in farm conditions for automatic
    fruit harvesting. IFAC-PapersOnLine 52(30), 294–299 (2019) Google Scholar   Grill,
    J.-B., et al.: Bootstrap your own latent-a new approach to self-supervised learning.
    Adv. Neural Inf. Process. Syst. 33, 21271–21284 (2020) Google Scholar   Häni,
    N., Roy, P., Isler, V.: A comparative study of fruit detection and counting methods
    for yield mapping in apple orchards. J. Field Robot. 37(2), 263–282 (2020) Article   Google
    Scholar   He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual
    networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
    vol. 9908, pp. 630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0_38
    Chapter   Google Scholar   Herrero-Huerta, M., González-Aguilera, D., Rodriguez-Gonzalvez,
    P., Hernández-López, D.: Vineyard yield estimation by automatic 3D bunch modelling
    in field conditions. Comput. Electron. Agric. 110, 17–26 (2015) Article   Google
    Scholar   Hung, C., Nieto, J., Taylor, Z., Underwood, J., Sukkarieh, S.: Orchard
    fruit segmentation using multi-spectral feature learning. In: 2013 IEEE/RSJ International
    Conference on Intelligent Robots and Systems, pp. 5314–5320. IEEE (2013) Google
    Scholar   Jaderberg, M., et al.: Population based training of neural networks.
    arXiv preprint arXiv:1711.09846 (2017) Krause, J., Stark, M., Deng, J., Fei-Fei,
    L.: 3D object representations for fine-grained categorization. In: 4th International
    IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia
    (2013) Google Scholar   VAN WOENSEL Lieve. Precision-agriculture and the future
    of farming in Europe. https://policycommons.net/artifacts/1996735/precision/2748500/
    (2016). Accessed 15 April 2022 Lin, G., Tang, Y., Zou, X., Xiong, J., Fang, Y.:
    Color-, depth-, and shape-based 3d fruit detection. Precis. Agric. 21(1), 1–17
    (2020) Article   Google Scholar   Lin, T.-Y., Dollár, P., Girshick, R., He, K.,
    Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2117–2125 (2017) Google Scholar   Lin, T.-Y., et al.: Microsoft COCO: common
    objects in context. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.)
    ECCV 2014. LNCS, vol. 8693, pp. 740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1_48
    Chapter   Google Scholar   Liu, S., Cossell, S., Tang, J., Dunn, G., Whitty, M.:
    A computer vision system for early stage grape yield estimation based on shoot
    detection. Comput. Electron. Agric. 137, 88–101 (2017) Article   Google Scholar   Liu,
    S., Whitty, M., Cossell, S.: Automatic grape bunch detection in vineyards for
    precise yield estimation. In: 2015 14th IAPR International Conference on Machine
    Vision Applications (MVA), pp. 238–241. IEEE (2015) Google Scholar   Loshchilov,
    I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101
    (2017) Marani, R., Milella, A., Petitti, A., Reina, G.: Deep neural networks for
    grape bunch segmentation in natural images from a consumer-grade camera. Precis.
    Agric. 22(2), 387–413 (2021) Article   Google Scholar   Misra, I., van der Maaten,
    L.: Self-supervised learning of pretext-invariant representations. In: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6707–6717
    (2020) Google Scholar   Nilsback, M.-E., Zisserman, A.: Automated flower classification
    over a large number of classes. In: 2008 Sixth Indian Conference on Computer Vision,
    Graphics & Image Processing, pp. 722–729. IEEE (2008) Google Scholar   Noroozi,
    M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw
    puzzles. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
    vol. 9910, pp. 69–84. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46466-4_5
    Chapter   Google Scholar   Nuske, S., Wilshusen, K., Achar, S., Yoder, L., Narasimhan,
    S., Singh, S.: Automated visual yield estimation in vineyards. J. Field Robot.
    31(5), 837–860 (2014) Article   Google Scholar   Palacios, F., Diago, M.P., Tardaguila,
    J.: A non-invasive method based on computer vision for grapevine cluster compactness
    assessment using a mobile sensing platform under field conditions. Sensors 19(17),
    3799 (2019) Google Scholar   Pothen, Z.S., Nuske, S.: Texture-based fruit detection
    via images using the smooth patterns on the fruit. In: 2016 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 5171–5176. IEEE (2016) Google
    Scholar   Reis, M.J.C.S., et al.: Automatic detection of bunches of grapes in
    natural environment from color images. J. Appl. Logic 10(4), 285–290 (2012) Google
    Scholar   Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time
    object detection with region proposal networks. In: Advances in Neural Information
    Processing Systems. vol. 28 (2015) Google Scholar   Roy, P., Isler, V.: Surveying
    apple orchards with a monocular vision system. In: 2016 IEEE International Conference
    on Automation Science and Engineering (CASE), pp. 916–921. IEEE (2016) Google
    Scholar   Sa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., McCool, C.: DeepFruits:
    a fruit detection system using deep neural networks. sensors 16(8), 1222 (2016)
    Google Scholar   Santos, T.T., Buiani, M.: Embrapa wine grape instance segmentation
    dataset - embrapa wgisd. https://github.com/charlespwd/project-title (2019) Santos,
    T.T., de Souza, L.L., dos Santos, A.A., Avila, S.: Grape detection, segmentation,
    and tracking using deep neural networks and three-dimensional association. Comput.
    Electron. Agric. 170, 105247 (2020) Google Scholar   Santos, T.T., Bassoi, L.H.,
    Oldoni, H., Martins, R.L.: Automatic grape bunch detection in vineyards based
    on affordable 3D phenotyping using a consumer webcam. In: CONGRESSO BRASILEIRO
    DE AGROINFORMÁTICA, 11, 2017, Campinas. Ciência de (2017) Google Scholar   Wang,
    Z., Walsh, K., Koirala, A.: Mango fruit load estimation using a video based mangoYOLO-kalman
    filter-hungarian algorithm method. Sensors 19(12), 2742 (2019) Article   Google
    Scholar   Zabawa, L., et al.: Detection of single grapevine berries in images
    using fully convolutional neural networks. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, pp. 0–0 (2019) Google Scholar   Zbontar,
    J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: self-supervised learning
    via redundancy reduction. In: International Conference on Machine Learning, pp.
    12310–12320. PMLR (2021) Google Scholar   Zhang, Z., Girdhar, R., Joulin, A.,
    Misra, I.: Self-supervised pretraining of 3D features on any point-cloud. In:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10252–10263
    (2021) Google Scholar   Download references Author information Authors and Affiliations
    California Polytechnic State University, San Luis Obispo, CA, 93407, USA Michael
    Woodson & Jane Zhang Corresponding author Correspondence to Jane Zhang . Editor
    information Editors and Affiliations Faculty of Science and Engineering, Saga
    University, Saga, Japan Kohei Arai Rights and permissions Reprints and permissions
    Copyright information © 2023 The Author(s), under exclusive license to Springer
    Nature Switzerland AG About this paper Cite this paper Woodson, M., Zhang, J.
    (2023). Evaluating Self-supervised Transfer Performance in Grape Detection. In:
    Arai, K. (eds) Intelligent Computing. SAI 2023. Lecture Notes in Networks and
    Systems, vol 711. Springer, Cham. https://doi.org/10.1007/978-3-031-37717-4_68
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-37717-4_68
    Published 01 September 2023 Publisher Name Springer, Cham Print ISBN 978-3-031-37716-7
    Online ISBN 978-3-031-37717-4 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Sections Figures References Abstract Introduction Related Work Background
    Methods and Materials Results Conclusion References Author information Editor
    information Rights and permissions Copyright information About this paper Publish
    with us Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Evaluating Self-supervised Transfer Performance in Grape Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rajan P.S.
  - 'Sathya '
  - Padma Suresh L.
  - George P.
  - Varghese A.
  citation_count: '0'
  description: Agriculture is the main occupation in our country. Many people depend
    on agriculture. There are many factors that reduce production. To keep track of
    the essential nutrients we can monitor soil parameters. The matters that arise
    with the amount of nutrients present in the soil have to be identified. Food safety
    can be achieved with computer vision. We can find the presence of bacteria and
    other disease occurring in plants by monitoring the leaves. A framework for crop
    monitoring using sensors, Arduino Uno and Wifi can be made together with leaf
    disease detection. Analysis can be done using K means classifier and SVM. Soil
    moisture sensor detects the presence of water content in soil. Camera or Drone
    can be used to take pictures of crops and can be stored in cloud servers. The
    improvement of machine learning and Internet Of Things (IoT) gives solution for
    problems which farmers are facing in their daily life. This work reviews published
    articles in smart farming and suggest methods to extract features to detect leaf
    diseases along with smart farming.
  doi: 10.1109/ICCPCT58313.2023.10245859
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2023 International Conference... A Review of IoT
    Based Smart Farming Using CNN for Improving Agriculture Management Publisher:
    IEEE Cite This PDF Princy Sera Rajan; Sathya; L. Padma Suresh; Preetha George;
    Anu Varghese All Authors 109 Full Text Views Abstract Document Sections I. Introduction
    II. Methadologies III. Use of Sensors in Smart Agriculture IV. Flow Process for
    Leaf Disease Detection V. Procedure for Preparing Training Model Show Full Outline
    Authors Figures References Keywords Metrics Abstract: Agriculture is the main
    occupation in our country. Many people depend on agriculture. There are many factors
    that reduce production. To keep track of the essential nutrients we can monitor
    soil parameters. The matters that arise with the amount of nutrients present in
    the soil have to be identified. Food safety can be achieved with computer vision.
    We can find the presence of bacteria and other disease occurring in plants by
    monitoring the leaves. A framework for crop monitoring using sensors, Arduino
    Uno and Wifi can be made together with leaf disease detection. Analysis can be
    done using K means classifier and SVM. Soil moisture sensor detects the presence
    of water content in soil. Camera or Drone can be used to take pictures of crops
    and can be stored in cloud servers. The improvement of machine learning and Internet
    Of Things (IoT) gives solution for problems which farmers are facing in their
    daily life. This work reviews published articles in smart farming and suggest
    methods to extract features to detect leaf diseases along with smart farming.
    Published in: 2023 International Conference on Circuit Power and Computing Technologies
    (ICCPCT) Date of Conference: 10-11 August 2023 Date Added to IEEE Xplore: 22 September
    2023 ISBN Information: DOI: 10.1109/ICCPCT58313.2023.10245859 Publisher: IEEE
    Conference Location: Kollam, India SECTION I. Introduction The main source of
    income for people of India is agriculture. A system that uses different sensors
    to get real time data with soil moisture sensor, temperature and humidity sensor
    can be used to increase productivity. Farmers won''t be aware of the fact that
    they can consult experts when diseases occur in plants. A solution for plant health
    monitoring by using image processing technique is also suggested. Most common
    problems that arise in the field of agriculture that farmers face are diseases
    that occur in leaves, the harmful pest, the change in weather conditions that
    leads to floods and other problems that occur due to lack of nutrients. Nowadays
    commonly used practice is the observation with naked eye. The main approach adopted
    in practice for detection and identification of plant diseases is naked eye observation
    of expertsThe main highlight of smart farming is the use of systems from remote
    areas. Without coming to farmland farmer can monitor the diseases occurring in
    plants. Data processing and feature extraction can be done using image processing
    tool. By the use of Iot weather conditions can be monitored. The farm necessities
    like moisture of the soil can be adjusted. Leaf disease detection will be helpful
    to farmers across the globe to identify existence of a disease This paper reviews
    the methodologies in smart agriculture using IoT and machine learning algorithms.
    IoT technology helps to find the parameters like temperature, humidity, moisture,
    nutrients and also controls motor using microcontroller. Agriculture land monitoring
    can be done using Arduino UNO. Testing of soil parameters is the key parameter
    in agriculture. The soil fertility is an important fact in increasing the production
    [2]. Machine learning helps to identify the soil fertility and other diseases
    that occur in plants. Nowadays smart phone is used to collect pictures of farm
    land, plants etc and pictures can be uploaded in cloud servers. The main parts
    of the system are hardware with sensors, mobile app and cloud servers. The hardware
    with sensors can be implemented in farm field to get data from crops. Moisture
    sensors, humidity sensors and temperature sensors are used to monitor the field.
    The collected data from farm field is given to microcontroller. Then the mobile
    application can be used from any remote location[3]. In a latest work [6] the
    authors developed a smart agriculture system to reduce the labour and time wastage
    in collecting data from farm fields. Here sensors are connected to microcontroller
    so that data obtained is given to Adafruit IO. So the Soil moisture sensor collects
    the moisture data from the field. If the water content is low below a predefined
    level message is given to farmer to make the motor switch ON/OFF for pumping water
    so that water wastage can also be reduced. Also the system is incorporated with
    temperature sensor, humidity sensor and barometric pressure sensor to check the
    temperature, humidity and atmospheric pressure accordingly. Deep learning is a
    subset of Machine learning. Diseases in crops can be identified in early stage
    if we can identify the major diseases occurring in leaves of plants. It will help
    the farmers a lot. Agriculture is the main income of many people in India. Heavy
    losses occur due to damage of crops. Sensors can be used for measuring soil parameters,
    humidity, temperature etc. Images can be collected using drones. Prediction of
    disease can be done using machine learning algorithms. In a recent work [8] a
    mobile app is developed and farmer can get the information from the mobile app
    itself. A novel approach to find brown spot disease using CNN is used here. To
    increase the efficiency of the method images of diseased leaves in different background
    is collected using drones and IoT. Databases are formed to store images of fresh
    leaves and diseased leaves for training and testing. Preprocessing of collected
    images from servers have to be done with image processing. Real time detection
    of brown spot disease is done in this paper[8] by employing a deep convolution
    network. In [10] the author displays the need of 5G network for smart farming.
    Smart agriculture uses the technology of Artificial Intelligence and IoT. The
    main problems which farmers are facing are the climate changes, soil parameters
    and diseases occurring in plants. The main challenges in agricultural production
    include soil and salinity DSS: a Web based Decision Support System helps to provide
    water to crops when needed, thereby improving efficiency.[1] A smart agriculture
    system developed to solve 3 main issues food security, Adaptation, mitigation.
    The amount of data used in agriculture monitoring is very big. It results in decrease
    of ability in 4G networks. With the use of 5G the processing of data becomes easier[10]
    Smart agriculture helps in improving the amount of real time data, remote monitoring
    of farm field, irrigation control, evaluation of diseases in leaves and increasing
    production. This paper covers many scientific methods of smart farming. We have
    reviewed articles from many scientific publishers like Elsevier, Springer, IEEE
    etc. The authors have reviewed many papers published including books and other
    published journals. Most of the papers are from last 3 years. The solutions for
    problems which farmers are facing can be solved by the emerging technologies Artificial
    Intelligence and internet of things. This work reviews the methodologies related
    to smart farming from 2020 to 2023. SECTION II. Methadologies Automation is done
    in different fields by the use of Internet Of Things. The same can be done in
    agriculture also. In [ 6 ] authors designed an IoT based framework that obtains
    data from soil. It helps to identify the intrusion of animals. Water wastage can
    also be avoided. The advancement in machine learning helps small farmers also
    to rely on the latest technologies in automation. Thus production and profit can
    be monitored. Based on the data collected, a mail is sent to the user to start
    pump or not. The author used Arduino Uno for collecting data obtained from different
    sensors like soil moisture, temperature and humidity sensor. Then GSM module is
    used to send data to Thingspeak. Then email is sent to the given contact to switch
    ON the motor for irrigation In [4] the author made an IoT based smart agriculture
    monitoring framework by using Node MCU ESP32.Here the Node MCU ESP32 receives
    the datas measured like temperature, humidity, soil moisture level etc. After
    that they designed an intrusion detection system using another Sensor. The sensor
    will identify the intrusion of animals, thieves and other humans. A mobile application
    was developed to identify the moisture content. A smart agriculture monitoring
    system can reduce the labour involved in agriculture. We can also get more accurate
    information from farm fields[11]. Comparison of different Remote Monitoring Systems(RMS)
    is shown in Table 1 Table 1. Different arrangements for RMS In the paper [12]
    feature extraction is done by performing various transforms in the image which
    is understood as number arrays or matrix by a computer. In this work k-means clustering
    and support vector machine is used to identify different types of diseases. Identification
    of leaf disease is done by K-Means and SVM classifier. In the system diseased
    area of leaf is found using K means algorithm and then it is further processed.
    Feature extraction is the important step in disease detection. Classification
    of disease is done by SVM classifier. Mannual interpretation requires a lot of
    effort and knowledge in plant disease It needs more time also. A smart farming
    system for finding the infections occurring in plants can be developed with Internet
    Of Things (IoT) [1]. The system gave an accuracy of 83.26%.The technique used
    was expensive. Fig 1: Internet of things (IoT) platform Show All SECTION III.
    Use of Sensors in Smart Agriculture To monitor the parameters in smart system
    sensors are employed. To measure the parameters like nutrients, phosphate contents,
    soil moisture etc sensors have to be used. Smart agriculture system collects the
    data received from different sensors and makes a suitable decision whether to
    start irrigation or not. These smart sensors measure the variations in soil moisture,
    temperature, soil PH value, humidity [4].We can use sensors either as fixed or
    remote. Remote sensors can be employed with the help of drone. Fixed sensors are
    placed with equal spacing in the farm field It is the most commonly used method.
    The use of remote sensors have many issues regarding flight time of drones and
    life of battery. But it has more accurate results. Sensors can be grouped together
    to form nodes. Fig 2. Block diagram of smart agriculture Show All Use of sensors
    helps to receive all the data and transmit it to various destinations by wireless
    communication methods. A sensor node contains sensors, battery, transmitter and
    receiver. Many sensor nodes connected together is called Wireless Sensor Network
    (WSN).The Table 2 highlights the different sensors that can be used in smart farming.
    Table 2. List of the sensor modules used in various survey SECTION IV. Flow Process
    for Leaf Disease Detection Machine learning algorithm use image processing as
    a major step. The collected images can be stored in cloud servers. In a recent
    work [8] data is collected from rice paddy using drone. Feature selection is done
    by an image processing tool training set and testing dataset are prepared using
    the same tool.CNN model is made and an app was developed for farmers. The flow
    process for leaf disease detection is shown in figure. First the image is to be
    captured. Image can be captured using camera or drones. Captured image have to
    be processed. Then image segmentation is done. Part of the leaves which are affected
    by disease are separated by image segmentation. Feature extraction is done. The
    datas collected are split into training set and testing set. Training can be done
    using convolution neural network. Classification can be done using techniques
    like SVM. Then performance can be monitored. Use of CNN model along with the various
    sensors improves accuracy of performance of the system. Fig 3. Flow process for
    leaf disease detection Show All A. Data Collection Dataset can be prepared or
    predefined dataset can be used.For preparing dataset several images of diseased
    leaves and fresh leaves have to be taken by using camera. Then these images are
    to be labelled. The images are then sent for identification of disease. The images
    collected are to be trained for supervised learning. Then only testing can be
    done. B. Image Preprocessing Image preprocessing is essential for getting high
    accuracy of results. Dewdrops, dust and presence of insect secretions have to
    be removed. To get rid of these problems RGB photo have to be converted into gray
    scale image. Size of the picture also have to be reduced. C. Image Segmentation
    Image segmentation is the main process in classification of disease. The two types
    in segmentation are based on similarities and discontinuities. Edge detection
    method is used. In discontinuities images are segmented based on sudden changes
    in intensities of values. D. Feature Extraction Feature extraction extracts features
    of the objects present in images. The features are classified into three types
    based on shape, colour and texture. Due to diseases present in leaves shape of
    leaves are changed. Colour is another important parameter to determine disease.
    Texture determines how the affected areas of images are spread in images. SECTION
    V. Procedure for Preparing Training Model The collected images have to be preprocessed
    to load in training model. Label should be provided to recognize diseased and
    normal leaves. Testing data and training data have to be separated. Tensor flow
    helps to create dataflow graph Training is done by using CNN. Validation loss
    and validation accuracy have to be calculated. The model in [8] is successful
    in early detection of disease occurring in rice paddy. The above work does not
    use any predefined set. Fig. 4 Application of IoT in various fields Show All SECTION
    VI. Need of 5G Network in Smart Farming Development in communication technology
    helps smart farning techniques a lot. For the last ten years 4G technology is
    used to share data in agriculture field. But with the increased usage of network
    the efficiency of 4G network is decreased. Fifth generation network provides a
    high speed network to transform data.5G is many times faster than 4G as the download
    speed is increased hundred times in 5G.To download a two hour movie, it takes
    only four seconds on 5G Fig 5 Application of 5G network Show All Advantages Of
    5G network High speed data transfer, efficiency is increased, more coverage, communication
    performance is increased. SECTION VII. Discussion A review based on papers published
    in the area of smart farming is done. Figure 6 shows the bar diagram analysis
    of percentage of papers published in different areas of smart farming from the
    papers reviewed from 2020 to 2023. Although many papers are available in the field
    of smart farming, nothing is satisfactory. It is understood that monitory losses
    occur for farmers in the field of production due to leaf diseases and environmental
    factors. This work can be converted into experimental level and can identify the
    effectiveness of the methodology. Figure 6 Bar diagram analysis Show All SECTION
    VIII. Conclusion This article suggests the requirement of 5G network for high
    speed transfer. Many countries are facing challenges regarding smart farming.
    This paper displays the importance of smart agriculture in farming. The use of
    IoT and CNN have influenced smart agriculture a lot. IoT can be used in irrigation,
    pest control, moisture, humidity etc. and can make proper decision on time so
    that labor of farmers can be reduced. Use of CNN model helps to detect plant disease
    accurately. Early detection of disease helps to prevent spread of diseases and
    helps to improve overall productivity. The smart farming technology have to be
    supported by government to improve production. Authors Figures References Keywords
    Metrics More Like This Internet of Things and Wireless Sensor Networks for Smart
    Agriculture Applications: A Survey IEEE Access Published: 2023 Role of Internet-of-Things
    (IoT) and Sensor Devices in Smart Agriculture: A Survey 2022 6th International
    Conference on Intelligent Computing and Control Systems (ICICCS) Published: 2022
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the International Conference on Circuit Power and Computing
    Technologies, ICCPCT 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Review of IoT Based Smart Farming Using CNN for Improving Agriculture Management
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
