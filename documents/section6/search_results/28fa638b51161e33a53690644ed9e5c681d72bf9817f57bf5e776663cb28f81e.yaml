- analysis: '>'
  authors:
  - Santhrupth B.C.
  - Devaraj Verma C.
  citation_count: '1'
  description: Sugarcane is an important crop, but its production is hampered by problems
    such as water shortages and disease. This article presents a machine learning-based
    approach to accurately detect and classify sugarcane leaf diseases using convolutional
    neural networks (CNNs), random forests, and support vector machine models. This
    study focuses on the global importance of sugarcane, the prevalence of sugarcane
    in medical applications, and the main cultivation regions. The implementation
    includes the use of Python programming and deep learning algorithms, specifically
    his SVM, random forest, and CNN to classify sugarcane leaf diseases based on color,
    texture, and shape features. This process includes data collection, local binary
    patterns, texture analysis using Gabor filters and his GLCM, and disease classification.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: International Journal of Intelligent Systems and Applications in Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Intelligent Disease Detection in Sugarcane Plants: A Comparative Analysis
    of Machine Learning Models for Classification and Diagnosis'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wolter-Salas S.
  - Canessa P.
  - Campos-Vargas R.
  - Opazo M.C.
  - V. Sepulveda R.
  - Aguayo D.
  citation_count: '0'
  description: Lettuce (Lactuca sativa L.) is highly susceptible to drought and water
    deficits, resulting in lower crop yields, unharvested areas, reduced crop health
    and quality. To address this, we developed a High-Throughput Phenotyping platform
    using Deep Learning and infrared images to detect stress stages in lettuce seedlings,
    which could help to apply real-time agronomical decisions from data using variable
    rate irrigation systems. Accordingly, a comprehensive database comprising infrared
    images of lettuce grown under drought-induced stress conditions was built. In
    order to capture the required data, we deployed a Raspberry Pi robot to autonomously
    collect infrared images of lettuce seedlings during an 8-day drought stress experiment.
    This resulted in the generation of a database containing 2119 images through augmentation.
    Leveraging this data, a YOLOv8 model was trained (WS-YOLO), employing instance
    segmentation for accurate stress level detection. The results demonstrated the
    efficacy of our approach, with WS-YOLO achieving a mean Average Precision (mAP)
    of 93.62% and an F1 score of 89.31%. Particularly, high efficiency in early stress
    detection was achieved, being a critical factor for improving food security through
    timely interventions. Therefore, our proposed High-Throughput Phenotyping platform
    holds the potential for high-yield lettuce breeding, enabling early stress detection
    and supporting informed decision-making to mitigate losses. This interdisciplinary
    approach highlights the potential of AI-driven solutions in addressing pressing
    challenges in food production and sustainability. This work contributes to the
    field of precision agricultural technology, providing opportunities for further
    research and implementation of cutting-edge Deep Learning techniques for stress
    detection in crops.
  doi: 10.1007/978-3-031-48858-0_27
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Advanced
    Research in Technologies, Information, Innovation and Sustainability ARTIIS 2023:
    Advanced Research in Technologies, Information, Innovation and Sustainability
    pp 339–351Cite as Home Advanced Research in Technologies, Information, Innovation
    and Sustainability Conference paper WS-YOLO: An Agronomical and Computer Vision-Based
    Framework to Detect Drought Stress in Lettuce Seedlings Using IR Imaging and YOLOv8
    Sebastian Wolter-Salas , Paulo Canessa , Reinaldo Campos-Vargas , Maria Cecilia
    Opazo , Romina V. Sepulveda & Daniel Aguayo  Conference paper First Online: 20
    December 2023 152 Accesses Part of the book series: Communications in Computer
    and Information Science ((CCIS,volume 1935)) Abstract Lettuce (Lactuca sativa
    L.) is highly susceptible to drought and water deficits, resulting in lower crop
    yields, unharvested areas, reduced crop health and quality. To address this, we
    developed a High-Throughput Phenotyping platform using Deep Learning and infrared
    images to detect stress stages in lettuce seedlings, which could help to apply
    real-time agronomical decisions from data using variable rate irrigation systems.
    Accordingly, a comprehensive database comprising infrared images of lettuce grown
    under drought-induced stress conditions was built. In order to capture the required
    data, we deployed a Raspberry Pi robot to autonomously collect infrared images
    of lettuce seedlings during an 8-day drought stress experiment. This resulted
    in the generation of a database containing 2119 images through augmentation. Leveraging
    this data, a YOLOv8 model was trained (WS-YOLO), employing instance segmentation
    for accurate stress level detection. The results demonstrated the efficacy of
    our approach, with WS-YOLO achieving a mean Average Precision (mAP) of 93.62%
    and an F1 score of 89.31%. Particularly, high efficiency in early stress detection
    was achieved, being a critical factor for improving food security through timely
    interventions. Therefore, our proposed High-Throughput Phenotyping platform holds
    the potential for high-yield lettuce breeding, enabling early stress detection
    and supporting informed decision-making to mitigate losses. This interdisciplinary
    approach highlights the potential of AI-driven solutions in addressing pressing
    challenges in food production and sustainability. This work contributes to the
    field of precision agricultural technology, providing opportunities for further
    research and implementation of cutting-edge Deep Learning techniques for stress
    detection in crops. Keywords Digital Agriculture Computer Vision High-Throughput
    Phenotyping Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF 1 Introduction Nowadays, the agricultural sector has been severely affected
    by water shortages affecting horticultural production. The effects of water stress
    on horticultural crops can induce physiological stress in plants, leading to stunted
    growth, diminished produce quality, and increased susceptibility to pests and
    diseases (Molina-Montenegro et al., 2011; Knepper y Mou, 2015; Kumar et al. 2021).
    The resulting water scarcity has significantly affected lettuce (Lactuca sativa
    L.) an extensively cultivated leafy vegetable, requiring an adequate water supply
    for optimal growth and quality. Lettuce stands as one of the most extensively
    cultivated leafy vegetables globally, encompassing a cultivation area of 1.3 million
    hectares and yielding approximately 29 million tons (Kim et al. 2016; Chen et
    al. 2019). Various cultivation methods are employed for lettuce, including hydroponic
    systems, greenhouses, and plant factories, while open-field cultivation remains
    prevalent (Donoso 2021). However, decreased water availability necessitates appropriate
    water management practices, influencing irrigation strategies and crop performance.
    Due to the water-intensive nature of lettuce cultivation, it is especially susceptible
    to water stress (Kumar et al. 2021). Optimal irrigation management during the
    seedling stage is closely linked to future productivity and the provision of healthy,
    uniform seedlings, thereby impacting the overall yield of horticultural crops
    (Shin et al. 2021). In seedling farms, the determination of irrigation management
    predominantly relies upon cultivation techniques and the visual discernment of
    the crop manager (Chen et al. 2014; Yang et al. 2020). Nonetheless, deficient
    and subjective cultivation methods yield undesirable consequences, including escalated
    labour and temporal requirements. In the context of water scarcity, the incorporation
    of Artificial Intelligence (AI) methodologies, specifically Deep Learning (DL),
    holds the potential for improving the well-being of vegetable crops, such as lettuce
    (Das Choudhury et al. 2019). DL algorithms excel at analyzing large volumes of
    data and extracting meaningful patterns, which can aid in optimizing irrigation
    management, predicting crop water needs, and improving resource-use efficiency
    (Cheng et al. 2020; Xiao et al. 2022; Gill et al. 2022). By leveraging the power
    of DL, it is possible to create predictive models that incorporate various environmental
    and crop-specific parameters to optimize irrigation scheduling, thereby reducing
    water usage and ensuring optimal plant growth (Kamarudin et al. 2021). In addition,
    DL algorithms can aid in the early detection and identification of plant stress
    symptoms caused by a lack of water, allowing for prompt intervention and mitigating
    yield losses (Kamarudin and Ismail 2022). For example, by analyzing Infrared (IR)
    imagery, these algorithms can detect minute changes in the observed phenotype,
    allowing for the early detection of crop stress phenotypes (Paulo et al. 2023).
    This early detection enables producers to implement targeted irrigation strategies,
    optimize resource allocation, and mitigate the adverse effects of water stress
    on crop health and yield (Islam and Yamane 2021; Chen et al. 2014). Thus, the
    precise and effective identification of stress-induced characteristics is imperative
    for the progression of our understanding of plant reactions to environmental stressors
    and the development of practical mitigation approaches. DL models, such as YOLO,
    have become prominent in the field of automated and High-Throughput Phenotyping
    (HTP) (Buzzy et al. 2020; Zhang and Li 2022; Cardellicchio et al. 2023; Xu et
    al. 2023). YOLO is a sophisticated object detection and instance segmentation
    model that utilizes deep neural networks to identify and precisely locate objects
    within images (Song et al. 2021). The cutting-edge architecture has exhibited
    exceptional efficacy in diverse Computer Vision (CV) assignments (Chen et al.
    2021). Using YOLO in plant science has significantly transformed phenotyping,
    providing a potent approach for automating stress-related phenotype identification
    and measurement (Chen et al. 2021, Xu et al. 2023). Through the process of training
    YOLO on extensive collections of plant images, the model can acquire the ability
    to identify and precisely locate stress symptoms (Mota-Delfin et al. 2022). Its
    real-time processing capability enables rapid analysis of large-scale datasets,
    facilitating HTP (James et al. 2022). This speed is critical for real-time monitoring
    of plant responses to stress and allows for timely interventions to mitigate damage
    and optimize crop management. Detailed knowledge of stress patterns in a plant
    population can guide targeted breeding tasks and precise agricultural interventions.
    1.1 Related Work  In recent years, there have been notable advancements in the
    field of crop phenotyping through the utilization of CV and DL approaches (Wang
    and Su 2022; Jiand and Li 2020; Chandra et al. 2020; Li et al. 2020). Particularly,
    instance segmentation refines the classic object detection task by identifying
    individual object instances and segmenting them pixel-wise. Recent studies have
    investigated the utilization of YOLO models in order to perform instance segmentation
    and object detection tasks specifically for plant phenotyping. In recent studies
    conducted by Khalid et al. (2023) and Qiu et al. (2022), a comparable methodology
    was employed, wherein numerous YOLO models were utilized for the timely identification
    of pests and illnesses in the field of agriculture. The primary objective of the
    research conducted by Khalid et al. (2023) is to discern and classify thistle
    caterpillars, red beetles, and citrus psylla pests. This was achieved through
    the utilization of a dataset of 9875 images, which were acquired under different
    lighting conditions. The YOLOv8 model demonstrates superior performance in the
    detection of tiny pests, surpassing prior studies with a mean Average Precision
    (mAP) of 84.7% and an average loss of 0.7939. Similarly, Rong et al. (2023) present
    a visual methodology for efficient point cloud processing of tomato organs, essential
    for automated crop management. The method involves segmenting tomato organs using
    instance segmentation and a strategy that utilizes point cloud constraints to
    match the organs. YOLOv5-4D detects the region of interest on tomatoes, achieving
    a mAP of 0.953, being slightly more accurate than the native YOLOv8 model. The
    proposed point cloud constraint-based search method effectively matches tomato
    organs in 3D space, yielding an 86.7% success rate in multiple real scenarios.
    However, the utilization of YOLOv5 in the agricultural sector, particularly in
    crop phenotype research, has gained significant traction and reached a level of
    maturity (Kong et al. 2023). Liu et al. (2023) proposed Small-YOLOv5 for automatically
    identifying the growth period of rice, which is crucial for producing high-yield
    and high-quality rice. The Small-YOLOv5 approach utilizes MobileNetV3 as the backbone
    feature extraction network, resulting in smaller model size and fewer parameters,
    thus improving the detection speed. Experimental results demonstrate that Small-YOLOv5
    outperforms other popular lightweight models, achieving a 98.7% mAP value at a
    threshold of 0.5 and a 94.7% mAP at a threshold range of 0.5 to 0.95. Moreover,
    Small-YOLOv5 significantly reduces the model parameters and volume. This is still
    project-dependent, as the work of Blekos et al. (2023) achieves the second-highest
    bounding box accuracy using YOLOv8 for grape maturity estimations utilizing their
    custom dataset of 2500 images. This result outperforms all YOLO versions, with
    an 11% accuracy margin compared to YOLOv3. Previous studies in this domain have
    not explored the application of YOLO for this purpose in lettuce. The closest
    related research is the work of Wang et al. (2023), which shares similarities
    but focuses on microscopic imaging of stomatal opening and closure. This excellent
    investigation has limitations, mainly in data acquisition and in the stage of
    abiotic stress. However, in our study, an IR camera is utilized to capture a complete
    frontal view of the entire plant. This approach eliminates the need for costly
    microscopic cameras and allows the use of ground drones for phenotyping purposes,
    enhancing water usage through the early detection of water stress. Herein we develop
    an HTP platform for the early detection of drought stress in lettuce using the
    DL model YOLOv8 by IR imaging. Accordingly, we developed an autonomous platform
    to generate a database that comprehends the different levels of stress that can
    affect lettuce over an extended period of water deficit. This database was used
    to train a YOLOv8 model that successfully from IR images lettuces exposed to different
    water stress levels. This development can be used to build novel strategies for
    efficient water use based on automatic stress detection. 2 Methodology 2.1 Plant
    Material and Experimental Conditions In order to define the state of water stress,
    it is necessary to grow lettuce seedlings. Lettuce seedlings (Lactuca sativa L.
    var. Capitata (L.) Janchen) were grown in a greenhouse under a controlled environment
    and irrigation. The lettuce seedlings were grown up to 10 days after the appearance
    of their true leaves continuing to the experimental phase. In this phase water
    was not administered to the experimental group for 8 days, maintaining a control
    group with normal water administration. A total of 72 individuals corresponded
    to the control group, meanwhile, 60 individuals corresponded to the experimental
    group. 2.2 Database Collection The water deficit stress level was defined using
    the morphological state of the lettuce seedlings. Based on this, an autonomous
    robot (named as High-Performance Autonomous Phenotyper, HPAP) with a camera capable
    of detecting and capturing digital images of lettuce seedlings in motion was built
    using Arduino. The images were using the Camera Module 2 Pi NoIR with a Sony IMX219
    8-megapixel sensor and calibrated with OpenCV library using Python. In addition,
    3 ultrasound sensor modules connected to the Raspberry Pi 3B were connected via
    a prototyping board and used to census the distance of its surroundings and make
    decisions on its trajectory automatically and correctively. HPAP is programmed
    to stay between 12 to 14 cm (cm) away from the surface where the lettuce seedlings
    are located, so if it is outside the threshold, it can correct its trajectory
    to maintain the proximity margin (Fig. 1). 2.3 Stress Detection The stress detection
    was assessed using YOLOv8 instance segmentation using a total of 2119 images (named
    Water Stress - YOLO, or WS-YOLO). The image resolution used was 640 × 480 pixels.
    The image annotation was automated using Supervision and fine-tuned Segment Anything
    Model (SAM) model using 4 states: ‘healthy lettuce’, ‘lettuce with mild stress’,
    ‘lettuce with moderate stress’ and ‘lettuce with severe stress’ based on the morphological
    characteristics and literature. The control group was annotated as ‘healthy’ as
    long as it did not show symptoms of water stress. Lettuces were annotated as ‘moderately
    stressed’ when they exhibited the first symptoms of water stress, such as slight
    wilting of the leaves and reduced turgor. In the case of ‘severe stress’ (or plant
    death) lettuces were annotated when they had traits such as wilting, reduced leaf
    area, leaf decay and loss of biomass. Finally, ‘mild’ (or early) stressed plants
    belonging to the experimental group (without irrigation) were annotated when they
    exhibited no stress symptoms since day 1. This was corroborated in the literature
    (see Discussion). Furthermore, the dataset was augmented by applying a horizontal
    flip, crop with a minimum zoom of 13% and a maximum zoom of 50%, rotation of 20°,
    saturation of 20%, exposure of 15%, and up to 5% pixel noise. The dataset was
    divided into training, validation, and test sets (70:20:10). The hyperparameters
    used in WS-YOLO model were defined to achieve a trade-off between the precision
    of the model and its computational efficiency. A batch size of 6 was used, resulting
    in good training times while preserving model stability. Additionally, an initial
    learning rate of 1*10^5 was implemented using ‘AdamW’ optimizer during 25 epochs.
    The incorporation of momentum of 0.85 and weight decay of 1*10^4 was implemented
    to expedite convergence and forestall overfitting. The PyTorch DL framework was
    utilized to implement the WS-YOLO model, which underwent training on a Windows
    System equipped with a 14-core Intel i5 CPU and an NVIDIA RTX 3070 Ti graphics
    card. The pre-trained model used was YOLOv8x-seg. 2.4 Model Evaluation The efficacy
    of the performed experiments on the WS-YOLO model was evaluated using Precision,
    Recall, F1-score, and Mean Average Precision (mAP) of 50% and between 50 and 95%
    of the Intersection over Union (IoU) as the evaluation metrics. The methodology
    for calculating is presented in the Eqs. (1–4). The abbreviations of these equations
    are: True positive (TP), False Positive (FP), False Negative (FN) and Average
    Precision (AP). $$\\mathbf{P}\\mathbf{r}\\mathbf{e}\\mathbf{c}\\mathbf{i}\\mathbf{s}\\mathbf{i}\\mathbf{o}\\mathbf{n}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FP}}$$ (1) $$\\mathbf{R}\\mathbf{e}\\mathbf{c}\\mathbf{a}\\mathbf{l}\\mathbf{l}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FN}}$$ (2) $$\\mathbf{F}1= \\frac{2
    *\\mathrm{ Precision }*\\mathrm{ Recall}}{\\mathrm{Precision }+\\mathrm{ Recall}}$$
    (3) $$\\mathbf{m}\\mathbf{A}\\mathbf{P}= \\frac{1}{{\\text{n}}}\\sum_{\\mathrm{k
    }= 1}^{\\mathrm{k }=\\mathrm{ n}}{{\\text{AP}}}_{\\mathrm{ k}}$$ (4) 3 Results
    and Discussion The trained WS-YOLO model successfully detects the experimentally-defined
    stress levels of lettuce seedlings. The model was evaluated using Precision, Recall,
    F1-score, mAP (50%), and mAP (50–95%) of the lettuce image analysis for evaluating
    the performance and effectiveness. Table 1 displays the experimental outcomes.
    Table 1. WS-YOLO performance. Full size table The calculated precision shows that
    90.63% of the instances predicted by the model are correct. This high precision
    value indicates a low false positive rate for this model. Furthermore, the recall
    value indicates that the model correctly identified 88.08% of real instances.
    This observation suggests a significant level of recall, indicating that the model
    exhibits a low rate of false negatives (Fig. 2). The F1 score is the harmonic
    mean of precision and recall, providing a balanced measure of both metrics. With
    an F1 score of 89.31%, this model has a good balance between precision and recall,
    obtaining the best score with 0.593 confidence (Fig. 3). The mAP at an IoU threshold
    of 50% measures the average precision across different segmented categories. A
    score of 93.62% indicates that the model achieves a high precision-recall tradeoff
    on average across the segmented categories. This result indicates good overall
    performance. The mAP assessed across a range of IoU thresholds from 50% to 95%
    provides a more stringent evaluation. The model demonstrates satisfactory performance
    in terms of precision and recall throughout a broader spectrum of IoU thresholds,
    albeit slightly lower than the mAP at 50%, with a value of 74.07%. With respect
    to other plant phenotyping research, the values presented in this study are within
    good parameters. In example, the work conducted by Lin et al. (2023) introduces
    YOLO-Tobacco, an improved YOLOX-Tiny network designed for detecting fungal-induced
    diseases. This network achieved a detection accuracy of 80.45% AP, a recall of
    69.27%, a precision of 86.25%, and an F1-score of 0.7683. Although these results
    are slightly lower compared to our findings (Table 1), this discrepancy may be
    attributed to the limited number of images used for model training. Conversely,
    Wang and He (2021) utilized the YOLOv5s model to successfully detect apple fruitlets
    before fruit thinning. Their study yielded favourable outcomes by employing a
    dataset containing 3165 images, resulting in a recall of 87.6%, precision of 95.8%,
    and an F1-score of 91.5%. In contrast to the previous example, these results surpass
    those achieved by the WS-YOLO model. Similarly, Isaac et al. (2023) employed the
    YOLOv5 model to detect cotton bolls. Their research achieved a precision of 0.84,
    a recall of 0.99, and an F1-score of 0.904 by utilizing a dataset comprising 9000
    images. Therefore, obtaining a larger image dataset is essential for attaining
    improved results. The WS-YOLO model visualization (Fig. 4) was carried out to
    view the segmented and detected lettuces with their stress levels across the experiment
    duration. According to our experimental design, the model successfully segmented
    healthy lettuces on day 8. This consistency is maintained throughout the experiment.
    In turn, Fig. 3 shows evidence of the detection of mild stress in lettuce during
    day 2. Mild water stress was detected on the second day using IR images, which
    provide different information compared to conventional images. Numerous phenotypic
    alterations, such as changes in biomass, are often observable following stress
    treatment, although certain changes, like water content dynamics, are less obvious
    or subtle to discern without specialized instruments. Previous studies have mentioned
    that IR images would be particularly effective for analyzing water stress (Berger
    et al. 2010; Munns et al. 2010; Chen et al. 2014). IR cameras possess high spectral
    sensitivity, capturing wavelengths between 400 and 1000 nm. This range allows
    for capturing information about leaf width, which is influenced by their water
    content (Fahlgren et al. 2015). Specifically, wavelengths between 700 and 1000
    nm exhibit higher reflectance in plant tissues compared to visible light, whose
    reflection is affected by leaf thickness (Ma et al. 2022). Osco et al. (2019)
    effectively identified physiological alterations resulting from water stress by
    employing hyperspectral imaging. Furthermore, they employed artificial neural
    network algorithms to classify the obtained images on the initial day of the experiment.
    Additionally, Basahi et al. (2014) determined a decrease in the relative water
    content in lettuce after 2 days of water stress. Moreover, the study conducted
    by Knepper and Mou (2015) supports this observation, reporting similar findings
    in three distinct lettuce varieties. Notably, only one of these strains exhibited
    a significant reduction in the relative water content of its leaves upon the initial
    day of drought stress induction. Based on this, it can be inferred that lettuce
    begins to experience water stress within a few days of initiating the experimental
    phase. Accordingly, on the fourth day, the model detected the first signs of wilting
    in the leaves of lettuce subjected to water stress (Fig. 4). Leaf wilting, as
    a morphological trait of stress, is caused by the loss of turgidity in the leaves,
    which eventually yields due to a lack of cellular elasticity (Seleiman et al.
    2021). This finding aligns with the study conducted by Shin et al. 2020, where
    similar leaf morphology was detected on the sixth day. However, it is important
    to consider that environmental differences and variations in lettuce strains may
    influence these observations. As discussed earlier, stress begins to manifest
    early depending on the resistance of the specific strain. Additionally, it is
    worth noting that not all lettuce plants in Fig. 4 exhibit moderate stress. Some
    lettuce plants do not show notable morphological characteristics of this stress,
    which may be attributed to genetic differences. These differences can result in
    slightly more resistant lettuce plants compared to others in response to drought
    stress (Lafta et al. 2021; Park et al. 2021). By the eighth day of water stress,
    a complete leaf drooping is observed, with reduced leaf area and length, indicating
    severe stress or plant death. This condition is attributed to the low soil moisture
    content and insufficient physiological responses to cope with advanced water stress.
    The novelty of our study resides in the integration of CV techniques to tackle
    the difficult task of detecting early stress in crops with a vision of inexpensive
    agronomic management. Although previous work explored the use of CV methods to
    determine plant phenotypes, the use of the YOLOv8 (WS-YOLO) model to assess stress
    levels in lettuce by IR cameras, to our knowledge, is unprecedented. This innovative
    approach leverages YOLOv8''s segmentation capabilities, allowing for precise identification
    and characterization of stress-induced phenotypic changes through IR imaging with
    good accuracy. In addition, the creation of a comprehensive database of IR images
    obtained through automated data collection of the Raspberry Pi robot expands the
    range of alternatives for data collection for similar research studies. As a result,
    our research innovates by combining the most advanced DL techniques with automated
    IR data collection, leading to alternatives for precision irrigation, food safety
    and sustainability. While there have been notable advancements in crop phenotyping
    and instance segmentation using YOLO-based models, there remain challenges in
    handling diverse environmental conditions, scale variations, and occlusions in
    crop images. Our study has revealed promising findings regarding the use of YOLOv8
    for stress detection in lettuce. However, there are several areas that warrant
    further investigation in future research. Firstly, it would be beneficial to expand
    the application of this approach to different crop species to assess its generalizability
    and effectiveness. Water stress caused by drought is already causing issues in
    the cultivation of various leafy vegetables (Khalid et al. 2022). Additionally,
    an intriguing avenue for future research involves incorporating temporal dynamics
    into the model. This would involve training the WS-YOLO model on sequential imagery,
    enabling real-time stress monitoring throughout the entire lifecycle of a crop.
    Additionally, considering the ever-evolving landscape of deep learning architectures,
    it would be valuable for future studies to evaluate the performance of emerging
    and older models and architectures in comparison to WS-YOLO. This could potentially
    lead to even higher levels of accuracy in stress detection. Finally, it is necessary
    to assess the phenotypic state using omics techniques. This has the potential
    to enhance the classification and robustness of this study. Fig. 1. Experimental
    design for WS-YOLO development. Full size image Fig. 2. Normalized Confusion Matrix
    of WS-YOLO. Full size image Fig. 3. F1 score over Confidence in different categories
    of stress detection of WS-YOLO. Full size image Fig. 4. WS-YOLO detection on Control
    and Water Stress group through the duration of the experiment. Full size image
    4 Conclusion and Future Perspectives This research contributes to the field of
    agricultural technology and stress detection in lettuce. By introducing a novel
    HTP platform that leverages DL, Robotics, and CV, the study addresses the critical
    challenge of early stress detection through IR imaging in lettuce, crucial for
    ensuring food security and mitigating yield losses. The application of WS-YOLO
    model with instance segmentation demonstrates promising results, achieving a mAP
    of 93.62% and an F1 score of 89.31%. These findings showcase the efficacy and
    potential of AI-driven solutions in tackling pressing challenges in food production
    and sustainability. Moreover, the creation of a comprehensive database of IR images
    through autonomous data collection further enriches the scientific knowledge base
    and opens opportunities for further research in cutting-edge DL techniques for
    stress detection in crops. Nonetheless, effectively demonstrating water stress
    in lettuce through experimental analysis is crucial. This approach would provide
    greater robustness in phenotype detection and enable the characterization of the
    physiology of this lettuce strain. References Basahi, J.: Effects of Enhanced
    UV-B Radiation and Drought Stress on Photosynthetic Performance of Lettuce (Lactuca
    sativa L. Romaine) Plants. Ann. Res. Rev. Biol. 4, 1739–1756 (2014) Google Scholar   Berger,
    B., Parent, B., Tester, M.: High-throughput shoot imaging to study drought responses.
    J. Exp. Bot. 61, 3519–3528 (2010) Article   Google Scholar   Blekos, A., et al.:
    A grape dataset for instance segmentation and maturity estimation. Agronomy 13,
    1995 (2023) Article   Google Scholar   Buzzy, M., Thesma, V., Davoodi, M., Mohammadpour
    Velni, J.: Real-time plant leaf counting using deep object detection networks.
    Sensors 20, 6896 (2020) Google Scholar   Cardellicchio, A., et al.: Detection
    of tomato plant phenotyping traits using YOLOv5-based single stage detectors.
    Comput. Electron. Agric.. Electron. Agric. 207, 107757 (2023) Article   Google
    Scholar   Chandra, A.L., Desai, S.V., Guo, W., Balasubramanian, V.N.: Computer
    Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey. arXiv
    1–26 (2020). https://doi.org/10.48550/arXiv.2006.11391 Chen, D., et al.: Dissecting
    the phenotypic components of crop plant growth and drought responses based on
    high-throughput image analysis. Plant Cell 26, 4636–4655 (2014) Article   Google
    Scholar   Chen, W., Zhang, J., Guo, B., Wei, Q., Zhu, Z.: An apple detection method
    based on Des-YOLO v4 algorithm for harvesting robots in complex environment. Math.
    Probl. Eng.Probl. Eng. 2021, 1–12 (2021) Google Scholar   Chen, Z., et al.: Assessing
    the performance of different irrigation systems on lettuce (Lactuca sativa L.)
    in the greenhouse. PLOS ONE 14, e0209329 (2019) Google Scholar   Cheng, Q., Zhang,
    S., Bo, S., Chen, D., Zhang, H.: Augmented reality dynamic image recognition technology
    based on deep learning algorithm. IEEE Access 8, 137370–137384 (2020) Article   Google
    Scholar   Das Choudhury, S., Samal, A., Awada, T.: Leveraging image analysis for
    high-throughput plant phenotyping. Front. Plant Sci. 10 (2019) Google Scholar   Donoso,
    G.: Management of water resources in agriculture in chile and its challenges.
    Int. J. Agric. Natural Resources 48, 171–185 (2021) Article   Google Scholar   Fahlgren,
    N., Gehan, M.A., Baxter, I.: Lights, camera, action: high-throughput plant phenotyping
    is ready for a close-up. Curr. Opin. Plant Biol.. Opin. Plant Biol. 24, 93–99
    (2015) Article   Google Scholar   Gill, T., Gill, S.K., Saini, D.K., Chopra, Y.,
    de Koff, J.P., Sandhu, K.S.: A comprehensive review of high throughput phenotyping
    and machine learning for plant stress phenotyping. Phenomics 2, 156–183 (2022)
    Article   Google Scholar   Islam, M.P., Yamane, T.: HortNet417v1—a deep-learning
    architecture for the automatic detection of pot-cultivated peach plant water stress.
    Sensors 21, 7924 (2021) Article   Google Scholar   James, K.M.F., Sargent, D.J.,
    Whitehouse, A., Cielniak, G.: High-throughput phenotyping for breeding targets—Current
    status and future directions of strawberry trait automation. Plants, People, Planet
    4, 432–443 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.: Lightweight
    deep CNN models for identifying drought stressed plant. IOP Conf. Ser. Earth Environ.
    Sci. 1091, 012043 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.,
    Saidi, N.B.: Deep learning sensor fusion in plant water stress assessment: a comprehensive
    review. Appl. Sci. 11, 1403 (2021) Article   Google Scholar   Khalid, M.F., et
    al.: Alleviation of drought and salt stress in vegetables: crop responses and
    mitigation strategies. Plant Growth Regul.Regul. 99, 177–194 (2022) Article   Google
    Scholar   Khalid, S., Oqaibi, H.M., Aqib, M., Hafeez, Y.: Small pests detection
    in field crops using deep learning object detection. Sustainability 15, 6815 (2023)
    Article   Google Scholar   Kim, M. J., Moon, Y., Tou, J. C., Mou, B., Waterland,
    N.L.: Nutritional value, bioactive compounds and health benefits of lettuce (Lactuca
    sativa L.). J. Food Composition Anal. 49, 19–34 (2016) Google Scholar   Knepper,
    C., Mou, B.: Semi-high throughput screening for potential drought-tolerance in
    lettuce (lactuca sativa) germplasm collections. J. Vis. Exp. 98, 1–6 (2015) Google
    Scholar   Kong, S., Li, J., Zhai, Y., Gao, Z., Zhou, Y., Xu, Y.: Real-time detection
    of crops with dense planting using deep learning at seedling stage. Agronomy 13,
    1503 (2023) Article   Google Scholar   Kumar, P., Eriksen, R. L., Simko, I., Mou,
    B.: Molecular mapping of water-stress responsive genomic loci in lettuce (Lactuca
    spp.) using kinetics Chlorophyll fluorescence, hyperspectral imaging and machine
    learning. Front. Genetics 12 (2021) Google Scholar   Lafta, A., Sandoya, G., Mou,
    B.: Genetic variation and genotype by environment interaction for heat tolerance
    in crisphead lettuce. HortScience 56, 126–135 (2021) Article   Google Scholar   Li,
    Z., Guo, R., Li, M., Chen, Y., Li, G.: A review of computer vision technologies
    for plant phenotyping. Comput. Electron. Agric.. Electron. Agric. 176, 105672
    (2020) Article   Google Scholar   Lin, J., et al.: Improved YOLOX-Tiny network
    for detection of tobacco brown spot disease. Front. Plant Sci. 14 (2023) Google
    Scholar   Liu, K., Wang, J., Zhang, K., Chen, M., Zhao, H., Liao, J.: A lightweight
    recognition method for rice growth period based on improved YOLOv5s. Sensors 23,
    6738 (2023) Article   Google Scholar   Ma, Z., et al.: A review on sensing technologies
    for high-throughput plant phenotyping. IEEE Open J. Instr. Measure. 1, 1–21 (2022)
    Article   Google Scholar   Mota-Delfin, C., López-Canteñs, G. de J., López-Cruz,
    I.L., Romantchik-Kriuchkova, E., Olguín-Rojas, J.C.: Detection and counting of
    corn plants in the presence of weeds with convolutional neural networks. Remote
    Sensing 14, 4892 (2022) Google Scholar   Munns, R., James, R.A., Sirault, X.R.R.,
    Furbank, R.T., Jones, H.G.: New phenotyping methods for screening wheat and barley
    for beneficial responses to water deficit. J. Exp. Bot. 61, 3499–3507 (2010) Article   Google
    Scholar   Osco, L.P., et al.: Modeling hyperspectral response of water-stress
    induced lettuce plants using artificial neural networks. Remote Sensing 11, 2797
    (2019) Article   Google Scholar   Park, S., Kumar, P., Shi, A., Mou, B.: Population
    genetics and genome‐wide association studies provide insights into the influence
    of selective breeding on genetic variation in lettuce. The Plant Genome 14 (2021)
    Google Scholar   de Paulo, R.L., Garcia, A.P., Umezu, C.K., de Camargo, A.P.,
    Soares, F.T., Albiero, D.: Water stress index detection using a low-cost infrared
    sensor and excess green image processing. Sensors 23, 1318 (2023) Article   Google
    Scholar   Qiu, R.-Z., et al.: An automatic identification system for citrus greening
    disease (Huanglongbing) using a YOLO convolutional neural network. Frontiers in
    Plant Science 13 (2022) Google Scholar   Rong, J., Yang, Y., Zheng, X., Wang,
    S., Yuan, T., Wang, P.: Three-Dimensional Plant Pivotal Organs Photogrammetry
    on Cherry Tomatoes Using an Instance Segmentation Method and a Spatial Constraint
    Search Strategy. (2023). https://doi.org/10.2139/ssrn.4482155 Article   Google
    Scholar   Seleiman, M.F., et al.: Drought stress impacts on plants and different
    approaches to alleviate its adverse effects. Plants 10, 259 (2021) Article   Google
    Scholar   Song, P., Wang, J., Guo, X., Yang, W., Zhao, C.: High-throughput phenotyping:
    breaking through the bottleneck in future crop breeding. Crop J. 9, 633–645 (2021)
    Article   Google Scholar   Wang, D., He, D.: Channel pruned YOLO V5s-based deep
    learning approach for rapid and accurate apple fruitlet detection before fruit
    thinning. Biosys. Eng.. Eng. 210, 271–281 (2021) Article   Google Scholar   Wang,
    J., Renninger, H., Ma, Q., Jin, S.: StoManager1: An Enhanced, Automated, and High-throughput
    Tool to Measure Leaf Stomata and Guard Cell Metrics Using Empirical and Theoretical
    Algorithms. arXiv 1–15 (2023). https://doi.org/10.48550/arXiv.2304.10450 Wang,
    Y., et al.: Insights into the stabilization of landfill by assessing the diversity
    and dynamic succession of bacterial community and its associated bio-metabolic
    process. Sci. Total. Environ. 768, 145466 (2021) Article   Google Scholar   Wang,
    Y.-H., Su, W.-H.: Convolutional neural networks in computer vision for grain crop
    phenotyping: a review. Agronomy 12, 2659 (2022) Article   Google Scholar   Xiao,
    Q., Bai, X., Zhang, C., He, Y.: Advanced high-throughput plant phenotyping techniques
    for genome-wide association studies: a review. J. Adv. Res. 35, 215–230 (2022)
    Article   Google Scholar   Xu, J., Yao, J., Zhai, H., Li, Q., Xu, Q., Xiang, Y.,
    Liu, Y., Liu, T., Ma, H., Mao, Y., Wu, F., Wang, Q., Feng, X., Mu, J. & Lu, Y.
    TrichomeYOLO: A Neural Network for Automatic Maize Trichome Counting. Plant Phenomics
    5, (2023) Google Scholar   Yang, W., et al.: Crop phenomics and high-throughput
    phenotyping: past decades, current challenges, and future perspectives. Mol. Plant
    13, 187–214 (2020) Article   Google Scholar   Zhang, P., Li, D.: YOLO-VOLO-LS:
    a novel method for variety identification of early lettuce seedlings. Front. Plant
    Sci. 13 (2022) Google Scholar   Download references Acknowledgement This research
    was funded by ANID BECAS/DOCTORADO NACIONAL (2023) 21231516 (S.W.S.), ANID/FONDECYT
    1200260 (R.C.V.), FONDEF ID19I10160 (D.A.), Proyecto interno UDLA DI-08/22 (C.O.),
    ANID/Millennium Science Initiative Program ICN17_022 and ANID/FONDECYT 1190611
    (P.C.). Author information Authors and Affiliations Center for Bioinformatics
    and Integrative Biology, Facultad de Ciencias de la Vida, Universidad Andrés Bello,
    Av. República 330, 8370186, Santiago, Chile Sebastian Wolter-Salas & Romina V.
    Sepulveda Centro de Biotecnología Vegetal, Facultad de Ciencias de la Vida, Universidad
    Andrés Bello, Av. República 330, 8370186, Santiago, Chile Paulo Canessa ANID–Millennium
    Science Initiative–Millennium Institute for Integrative Biology (iBIO), Av. Libertador
    Bernardo O’Higgins 340, 7500565, Santiago, Chile Paulo Canessa Centro de Estudios
    Postcosecha, Facultad de Ciencias Agronómicas, Universidad de Chile, Av. Santa
    Rosa 11315, 8831314, Santiago, Chile Reinaldo Campos-Vargas Instituto de Ciencias
    Naturales, Facultad de Medicina Veterinaria y Agronomía, Universidad de Las Américas,
    Av. Manuel Montt 948, 7500000, Santiago, Chile Maria Cecilia Opazo Instituto de
    Tecnología para la Innovación en Salud y Bienestar, Facultad de Ingeniería, Universidad
    Andrés Bello, Quillota 980, 2531015, Viña del Mar, Chile Daniel Aguayo Corresponding
    author Correspondence to Romina V. Sepulveda . Editor information Editors and
    Affiliations Universidad Estatal Peninsula de Santa Elena Campus Matriz, La Libertad,
    Ecuador Teresa Guarda Algoritmi Research Centre, University of Minho, Guimarães,
    Portugal Filipe Portela Universidad a Distancia de Madrid, Madrid, Spain Jose
    Maria Diaz-Nafria Rights and permissions Reprints and permissions Copyright information
    © 2024 The Author(s), under exclusive license to Springer Nature Switzerland AG
    About this paper Cite this paper Wolter-Salas, S., Canessa, P., Campos-Vargas,
    R., Opazo, M.C., V. Sepulveda, R., Aguayo, D. (2024). WS-YOLO: An Agronomical
    and Computer Vision-Based Framework to Detect Drought Stress in Lettuce Seedlings
    Using IR Imaging and YOLOv8. In: Guarda, T., Portela, F., Diaz-Nafria, J.M. (eds)
    Advanced Research in Technologies, Information, Innovation and Sustainability.
    ARTIIS 2023. Communications in Computer and Information Science, vol 1935. Springer,
    Cham. https://doi.org/10.1007/978-3-031-48858-0_27 Download citation .RIS.ENW.BIB
    DOI https://doi.org/10.1007/978-3-031-48858-0_27 Published 20 December 2023 Publisher
    Name Springer, Cham Print ISBN 978-3-031-48857-3 Online ISBN 978-3-031-48858-0
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Download book PDF Download book EPUB Sections Figures
    References Abstract Introduction Methodology Results and Discussion Conclusion
    and Future Perspectives References Acknowledgement Author information Editor information
    Rights and permissions Copyright information About this paper Publish with us
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Communications in Computer and Information Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'WS-YOLO: An Agronomical and Computer Vision-Based Framework to Detect Drought
    Stress in Lettuce Seedlings Using IR Imaging and YOLOv8'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Abbasi R.
  - Martinez P.
  - Ahmad R.
  citation_count: '3'
  description: Crops grown on aquaponics farms are susceptible to various diseases
    or biotic stresses during their growth cycle, just like traditional agriculture.
    The early detection of diseases is crucial to witnessing the efficiency and progress
    of the aquaponics system. Aquaponics combines recirculating aquaculture and soilless
    hydroponics methods and promises to ensure food security, reduce water scarcity,
    and eliminate carbon footprint. For the large-scale implementation of this farming
    technique, a unified system is needed that can detect crop diseases and support
    researchers and farmers in identifying potential causes and treatments at early
    stages. This study proposes an automatic crop diagnostic system for detecting
    biotic stresses and managing diseases in four leafy green crops, lettuce, basil,
    spinach, and parsley, grown in an aquaponics facility. First, a dataset comprising
    2640 images is constructed. Then, a disease detection system is developed that
    works in three phases. The first phase is a crop classification system that identifies
    the type of crop. The second phase is a disease identification system that determines
    the crop's health status. The final phase is a disease detection system that localizes
    and detects the diseased and healthy spots in leaves and categorizes the disease.
    The proposed approach has shown promising results with accuracy in each of the
    three phases, reaching 95.83%, 94.13%, and 82.13%, respectively. The final disease
    detection system is then integrated with an ontology model through a cloud-based
    application. This ontology model contains domain knowledge related to crop pathology,
    particularly causes and treatments of different diseases of the studied leafy
    green crops, which can be automatically extracted upon disease detection allowing
    agricultural practitioners to take precautionary measures. The proposed application
    finds its significance as a decision support system that can automate aquaponics
    facility health monitoring and assist agricultural practitioners in decision-making
    processes regarding crop and disease management.
  doi: 10.1016/j.aiia.2023.09.001
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. Research
    methodology 4. Experimental results and discussion 5. Conclusions and future prospects
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgments
    References Show full outline Cited by (3) Figures (9) Show 3 more figures Tables
    (8) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show all tables Artificial
    Intelligence in Agriculture Volume 10, December 2023, Pages 1-12 Crop diagnostic
    system: A robust disease detection and management system for leafy green crops
    grown in an aquaponics facility Author links open overlay panel R. Abbasi a, P.
    Martinez b, R. Ahmad a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.aiia.2023.09.001
    Get rights and content Under a Creative Commons license open access Highlights
    • Developed a crop disease diagnostic and management system for aquaponics facility.
    • Used a transfer learning method to initialize weights of ResNet-50 and YOLOv5s.
    • Achieved promising classification accuracy and mAP@0.5. • Integrated final disease
    detection system with ontology model to manage diseases. Abstract Crops grown
    on aquaponics farms are susceptible to various diseases or biotic stresses during
    their growth cycle, just like traditional agriculture. The early detection of
    diseases is crucial to witnessing the efficiency and progress of the aquaponics
    system. Aquaponics combines recirculating aquaculture and soilless hydroponics
    methods and promises to ensure food security, reduce water scarcity, and eliminate
    carbon footprint. For the large-scale implementation of this farming technique,
    a unified system is needed that can detect crop diseases and support researchers
    and farmers in identifying potential causes and treatments at early stages. This
    study proposes an automatic crop diagnostic system for detecting biotic stresses
    and managing diseases in four leafy green crops, lettuce, basil, spinach, and
    parsley, grown in an aquaponics facility. First, a dataset comprising 2640 images
    is constructed. Then, a disease detection system is developed that works in three
    phases. The first phase is a crop classification system that identifies the type
    of crop. The second phase is a disease identification system that determines the
    crop''s health status. The final phase is a disease detection system that localizes
    and detects the diseased and healthy spots in leaves and categorizes the disease.
    The proposed approach has shown promising results with accuracy in each of the
    three phases, reaching 95.83%, 94.13%, and 82.13%, respectively. The final disease
    detection system is then integrated with an ontology model through a cloud-based
    application. This ontology model contains domain knowledge related to crop pathology,
    particularly causes and treatments of different diseases of the studied leafy
    green crops, which can be automatically extracted upon disease detection allowing
    agricultural practitioners to take precautionary measures. The proposed application
    finds its significance as a decision support system that can automate aquaponics
    facility health monitoring and assist agricultural practitioners in decision-making
    processes regarding crop and disease management. Previous article in issue Next
    article in issue Keywords Computer visionDeep learningDisease detectionLeafy cropsAquaponicsDigital
    farming 1. Introduction An aquaponic system is the combination of two well-known
    technologies, namely recirculating aquaculture system (RAS) and a hydroponics
    system (soilless growing of plants) that work together in an integrated environment
    (Abbasi et al., 2021a). The rationale of this soilless growing system involves
    sharing the mutual benefit of the available resources, such as water and nutrients,
    between aquaculture and plant production. Fish eats food and excretes waste consisting
    of ammonia (NH3+) and other constituents, which are then converted by certain
    microbes to nitrates (NO3−). This enriched effluent is then pumped into the hydroponic
    component of the system, where the nutrients are readily available for uptake.
    Under this general idea, it can be implied that aquaponic is a green and sustainable
    food production system (Yanes et al., 2020). Despite all the advantages offered
    by this emerging and growing technology, a few challenges need special attention,
    particularly considering its large-scale implementation. Being a greenhouse and
    a symbiotic environment, the parameters and factors (light, temperature, pH, moisture,
    etc.) that need to be controlled are diverse (Abbasi et al., 2021b). For the system
    to be functional and efficient, a delicate equilibrium among these parameters
    must be established (Gillani et al., 2022). Optimal conditions must be met for
    the growth and development of all three varieties of organisms that are present
    in the system (fish, bacteria, and plants). Another significant challenge is related
    to crop diseases resulting from either nutrient deficiency or inadequate management
    of the system, impacting crop quality and causing crop wastage (Dhal et al., 2022;
    Stouvenakers et al., 2019). As Khirade and Patil pointed out, identifying crop
    diseases and applying disease management practices are key to preventing losses
    in the yield and quantity of agricultural products (Khirade and Patil, 2015).
    For this reason, early detection of disease outbreaks is crucial for the progress
    of aquaponics farms. Traditionally, crop diagnostic is performed by agricultural
    specialists who visually examine the plant leaves. This practice, however, is
    subjective, destructive, time-consuming, and labor-intensive (Dutot et al., 2013).
    Moreover, it also requires the experts to be proficient with extensive knowledge
    of various diseases, their symptoms, and treatments (Khan et al., 2022). Other
    methods include chemical analyses, leaf color chart (LCC) matching, soil plant
    analysis development (SPAD), hyperspectral imaging, and spectral remote sensing,
    which again are either time-consuming or costly or destructive techniques (Weaver
    et al., 2020). To address these problems, different automatic crop disease detection
    systems based on artificial intelligence (AI) techniques such as machine learning
    and deep learning are developed as they offer contactless, rapid, environmental-friendly,
    and accurate methods for performing a non-invasive evaluation of crops'' health
    and quality (Bedi and Gole, 2021; Singh et al., 2020). Deep learning techniques
    offer two significant advantages over machine learning techniques. First, the
    feature extraction process is automatic, and second, the time to process large
    datasets of high dimensions is significantly reduced (Bedi and Gole, 2021). In
    addition to disease detection, it is also paramount that farm practitioners and
    researchers have access to relevant information about crop management strategies
    that allow them to pick up methods and treatments appropriately to prevent diseases,
    thereby gaining both economic and environmental benefits (Barosa et al., 2019).
    In most cases, such information is dispersed throughout multiple heterogeneous
    data sources — posing a need for a unified model that contains knowledge about
    the causes and treatments of different crop diseases. Semantic technologies such
    as ontologies have proven effective for data integration in multiple domains (Rodríguez-García
    et al., 2021). An ontology is a formal and explicit specification of a shared
    conceptualization (Studer et al., 1998). The logical formalisms behind ontological
    models allow autonomous agents to interpret the information that is being processed
    (Horrocks et al., 2005). Ontology can be used to construct a knowledge base containing
    relevant information about causes and suggested treatments of crop diseases, which
    can be extracted upon disease detection (Rodríguez-García et al., 2021). With
    this information, farm practitioners are able to get clear guidelines to effectively
    perform crop monitoring and disease management. In this study, an automatic system
    based on deep learning techniques is presented for the detection and classification
    of diseases in four leafy green crops, lettuce, basil, parsley, and spinach, grown
    in an aquaponics facility. Taking advantage of semantic technologies, an ontology
    model, ‘AquaONT’ is developed by authors in previous work (Abbasi et al., 2021b)
    that contains knowledge about causes and treatments of different diseases. This
    ontology model is integrated with a disease detection system through an interface
    established on a cloud-based application. The remainder of the paper is structured
    as follows: Section 2 summarizes the most recent literature related to crop disease
    detection systems, Section 3 presents the methodology used to design the proposed
    system, Section 4 discusses the experimental results and findings, and finally,
    Section 6 concludes the paper and presents the future prospects. 2. Related work
    The rapid developments in AI have made a major breakthrough in deep learning (DL)
    and computer vision (CV) technologies by solving complex problems like image classification,
    object detection, speech recognition, voice recognition, natural language processing,
    and medical imaging, among others (Abbasi et al., 2022a; Subeesh and Mehta, 2021).
    In particular, convolutional neural networks (CNNs) have proved their efficiency
    in various sectors such as automotive, healthcare, or retail, and are also being
    integrated in agriculture for automatic crop disease detection — presenting a
    reasonable alternative to traditional practices (Pathan et al., 2020). In recent
    years, several models and applications have been developed for crop disease identification
    and diagnosis. This section investigates some latest works present in the literature.
    Anami et al. designed a deep convolutional neural network (DCNN) based framework
    for automatic recognition and classification of various biotic and abiotic paddy
    crop stresses using the pre-trained visual geometry group model, VGG-16 (Anami
    et al., 2020). The field images are used in the proposed approach captured during
    the booting growth stage. Bedi and Gole proposed a hybrid model based on a convolutional
    autoencoder (CAE) network and CNN for automatic bacterial spot disease detection
    present in peach plants using their leaf images from a publicly available dataset
    named ‘PlantVillage’ (Bedi and Gole, 2021). Paymode and Malode developed a CNN-based
    method using pre-trained VGG-16 for detecting healthy, unhealthy, and diseased
    leaves in tomato and grape plants (Paymode and Malode, 2022). Fuentes et al. combined
    ResNet with Faster R-CNN, R-FCN, and SSD. They proposed a method to detect the
    diseases and insect pests of tomato plants, achieving the effective identification
    of nine different types of diseases and insect pests (Fuentes et al., 2017). Chen
    et al. proposed a method to detect rice plant diseases using the DenseNet model
    of deep transfer learning (Chen et al., 2020). To identify the cucumber disease
    spots in greenhouses, Ma et al. developed a CNN-based system, combining a compound
    color feature with a region-growing algorithm (Ma et al., 2018). A disease recognition
    algorithm based on VGGNet and InceptionV3 with reduced model size and improved
    recognition accuracy is proposed by Rahman et al. for rice plants (Rahman et al.,
    2020). Oppenheim et al. proposed a disease classification algorithm based on an
    improved VGG network for accurate and quick identification and classification
    of spots on potato crops (Oppenheim et al., 2019). A method based on an improved
    CNN is proposed by Fan et al. to identify nine kinds of common corn diseases from
    images with a complex background (Fan et al., 2021). Khan et al. proposed an apple
    disease detection system that works in two stages (Khan et al., 2022). Based on
    the Xception model, the first stage classifies whether the leaf is healthy or
    diseased, and the second stage, based on Faster-RCNN, performs disease detection.
    Qi et al. developed a disease recognition system based on an improved YOLOv5 (squeeze-and-excitation
    (SE) module is added) model to identify the tomato virus diseases in the greenhouse
    (Qi et al., 2022). Nandhini et al. proposed a deep learning model that combines
    RNN and CNN for disease classification and early prediction in the Plantain tree
    (Nandhini et al., 2022). Abbas et al. proposed a deep learning-based method for
    tomato disease detection that utilizes the Conditional Generative Adversarial
    Network (C-GAN) to generate synthetic images of tomato plant leaves (Abbas et
    al., 2021). A DenseNet121 model was then trained on synthetic and real images
    using transfer learning to classify the tomato leaves images into ten categories
    of diseases. An efficient detection model (EFDet) consisting of an efficient backbone
    network, a feature fusion module, and a predictor is proposed for the detection
    of cucumber leaf diseases in complex backgrounds by Liu et al. (Liu et al., 2021).
    Likewise, a YOLOv5-based disease detection model to detect bacterial spot disease
    in bell pepper plant from the symptoms seen on the leaves (Mathew and Mahesh,
    2022). A framework is proposed for an aquaponics system based on image processing
    and decision tree methodology that performs disease detection of four leaf species,
    eggplant, chilli, citrus, and mandarin, and automatically generates a report which
    is sent to the owner through the mobile application if the disease is detected
    (Barosa et al., 2019). Likewise, a CNN-based approach for detecting plant disease
    in smart hydroponics provides a tool to the farmers capable of doing the task
    of an agricultural extension worker with even better accuracy (Musa et al., 2021).
    An application based on image processing and SVM is developed to classify apple
    diseases (Lisha Kamala and Anna Alex, 2021). Yudha et al. proposed a model based
    on Faster R-CNN with Inception V2 algorithm to recognize the diseases in hydroponic
    lettuce (Yudha Pratama et al., 2020). The literature survey has revealed that
    researchers have extensively used deep learning techniques for plant or crop disease
    detection and classification. The analysis shows that most disease detection systems
    are developed for open-air farms. Only a few systems are developed for modern
    farming systems, such as aquaponics or hydroponics. Most models are developed
    considering multiple diseases of only one crop. Moreover, to the best of the authors''
    knowledge, no comprehensive and unified disease detection system is proposed for
    identifying diseases of multiple leafy green crops grown in aquaponics facilities.
    Disease detection in leafy green presents various challenges. For instance, there
    exists a strong resemblance among the foliage of different leafy green crops that
    might impact the performance of the detection system. Secondly, due to differences
    in light illumination during imaging, the visual symptoms of different diseases
    may appear similar. Another challenge is the availability of a dataset of leafy
    green crops that can be used for disease detection. Deep learning models require
    a huge amount of data for training, and to the best of the author''s knowledge,
    there is no sufficient sized large-scale open-source dataset available that can
    be utilized for this research. There are a few datasets, such as PlantVillage,
    PlantDoc, and CropDeep (Noyan, 2022; Singh et al., 2019; Zheng et al., 2019).
    PlantDoc and PlantVillage are open-source datasets with no categories of leafy
    green crops. CropDeep dataset contains images of some of the leafy green, but
    it is not open source. Lastly, none of the aforementioned models provides information
    related to the causes and treatments of detected diseases. Apart from AI techniques,
    ontology-based systems are also developed over the years for plant disease diagnosis
    and treatment recommendations. Jearanaiwongkul et al. developed an ontology-based
    expert system called ‘RiceMan’ for disease identification and control recommendation
    in rice crops (Jearanaiwongkul et al., 2021). Likewise, Rodríguez-García et al.
    proposed a decision support system based on an ontology model for crop pests and
    diseases recognition (Rodríguez-García et al., 2021). It also provides information
    on agriculture practices and permitted pest control measures. In these systems,
    users are required to select crop and observed symptoms from the list for further
    processing, which is a time-consuming process. Whereas, in deep learning models,
    this information can be obtained by using crop images. Deep learning techniques
    can be combined with ontology models to develop efficient decision support systems
    for disease management in crops. The idea of combining the two techniques is relatively
    new in the agriculture sector, and hence, limited work is done in this regard
    that primarily focuses on enabling smart services (monitoring and control) in
    IoT-based farming systems or detection of cyber-attacks (Abbasi et al., 2021b).
    Considering the research gaps and potential opportunities, this study aims to
    create a dataset consisting of high-quality RGB images (healthy and diseased)
    of four leafy green crops: little gem romaine lettuce, spinach, parsley, and basil.
    This study also aims to develop a crop diagnostic system based on deep learning
    models and ontology models for detecting diseases and identifying causes and potential
    treatments in stated crops, respectively. 3. Research methodology The block diagram
    illustrating the three sequential modules of the research methodology is shown
    in Fig. 1. First module involves the preparation of the dataset and training of
    classification and object detection models. The disease detection model works
    in three phases. The first and second phase uses lightweight classification models
    to classify the type of crop and identify whether the classified crop has a disease
    or not, respectively. Phase 3 is the detection stage that uses an object detection
    model to detect and localize the diseased and non-diseased spots in the crops.
    The third phase also tells the class of the diseased spots. The purpose behind
    adding two classification phases before the detection phase is three-fold. First,
    to improve the detection performance by reducing the number of wrong detections
    which could arise as the model has to identify and localize different disease
    spots of varying sizes. Second, to determine the characteristics of the crop identified
    in the first phase in relation to aquaponics'' system design by linking it with
    the knowledge model. Lastly, to reduce the overall processing time by filtering
    out invalid inputs in the second phase. The second module aims to extract the
    instances of relevant classes such as potential causes and treatments of detected
    diseases from the ontology model ‘AquaONT’ developed by authors in previous work
    (Abbasi et al., 2021b). In the third module, a cloud-based application is developed
    using Streamlit1, where a pre-trained disease detection model and ontology model
    are deployed to obtain a complete crop diagnostic system. Upon identification
    of the crop in phase 1, its characteristics in relation to optimal environmental
    (pH, temperature, illumination, etc.), growth (width, height, area, etc.), and
    grow bed design (plant site spacing) parameters for an aquaponics facility are
    extracted from ontology model using OWLready22 (ontology-oriented programming
    package in Python). The authors have conducted a study that identified design
    parameters as vital knowledge in ensuring high crop yields and product quality
    in an aquaponics facility (Abbasi et al., 2021a). Likewise, once the disease and
    its type are detected in phase 3, the potential causes and recommended treatments
    are extracted from the ontology model. Each element of each module is presented
    in detail in the following subsections. Download : Download high-res image (404KB)
    Download : Download full-size image Fig. 1. Proposed methodology for disease detection
    and control recommendation system. 3.1. Dataset preparation The dataset preparation
    involves three steps, i) data acquisition, ii) data annotation, and iii) data
    augmentation, which are detailed below. 3.1.1. Data acquisition This study considers
    four leafy green crops, lettuce, basil, parsley, and spinach. The dataset consists
    of healthy and diseased images of these crops, which are acquired from different
    sources such as NFT based aquaponics facility built in AllFactory 4.0 Lab (University
    of Alberta, Canada), Google search engine, and Ecosia3 (a search engine based
    in Berlin, Germany). The diseases considered for the four crops while developing
    the dataset are listed below. • Lettuce: Bacterial leaf spot and Downy mildew
    • Basil: Downy mildew • Parsley: Septoria leaf spot • Spinach: Downy mildew and
    Stemphylium leaf spot To enhance the flexibility of the model to correctly classify
    and detect disease, it is ensured that images have non-homogeneous backgrounds,
    different illumination conditions, and disease maturity stages (Jha et al., 2019).
    A total of 2000 images are gathered from all the resources. Among these images,
    800 images showed healthy crops (200 images per crop), and 1200 images showed
    the diseases mentioned above (240 images per disease). Fig. 2 shows some of the
    sample images from the dataset. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 2. Samples from leafy green image dataset. (For
    interpretation of the references to color in this figure legend, the reader is
    referred to the web version of this article.) 3.1.2. Data annotation Data annotation
    is one of the vital steps for the successful development of object detection models.
    The process is manual and involves labeling the desired objects in an image with
    a label or tag that refers to a particular class. The labeled data is used during
    the training of the model. There are various open-source annotation tools, but
    in this study, LabelImg4 is used. LabelImg is a python based graphical annotation
    tool that supports a variety of deep learning algorithms (Qi et al., 2022). In
    this study, the annotations are generated in COCO JSON and YOLO Darknet TXT formats
    because in the disease detection phase, two object detection models are tested
    to design the final system. 3.1.3. Data augmentation Next, the data augmentation
    process is performed to supplement and enrich the dataset. This helps increase
    the model''s generalizability and overcome the problem of overfitting. Moreover,
    it also allows the model to learn as many relevant features as possible. This
    study uses Albumentations, a Python library, for fast and flexible image augmentations
    (Buslaev et al., 2020). The different augmentation techniques applied are flip,
    rotation, noise, blur, and brightness. Fig. 3 shows examples of different augmentation
    operations. After applying the data augmentation, the final dataset comprises
    of 2640 images with their annotations. The final distribution of the dataset is
    presented in Table 1. Download : Download high-res image (409KB) Download : Download
    full-size image Fig. 3. Example of different augmentation operations applied on
    original image. Table 1. Distribution of crop information in the used dataset
    among the studied crops. Crop Healthy Diseased Total Disease 1 Disease 2 Lettuce
    240 280 280 800 Basil 240 280 – 520 Spinach 240 280 280 800 Parsley 240 280 –
    520 3.2. Disease detection model development Object detection is a complex task,
    and disease detection of leafy green crops comes with its own set of challenges.
    To overcome these challenges, the detection process in this study is divided into
    three primary phases. Fig. 4 shows the detailed pipeline of the disease detection
    model. Download : Download high-res image (864KB) Download : Download full-size
    image Fig. 4. Detailed pipeline for the crop diagnostic process. The first phase
    of the proposed system uses a lightweight CNN architecture to classify input images
    into one of the four types of crops: lettuce, basil, parsley, and spinach. ResNet-50
    is used as the base model for the CNN architecture in this study and its last
    layer is replaced with one global average pooling layer, one dense layer (fully
    connected layer) of size 1024 and activation function ReLu, and one output layer
    that uses Softmax for classification task and making final predictions. ResNet050
    is used as it has a simple design, high accuracy, and is suitable for small datasets
    (He et al., 2015). The crop type identified in this stage saves to a folder and
    also acts as an input to the next phase. Phase 2 of the system also uses ResNet-50
    and classifies the input from phase 1 into one of the following eight classes.
    i) Lettuce-Healthy ii) Lettuce-Diseased iii) Basil-Heathy iv) Basil-Diseased v)
    Spinach-Healthy vi) Spinach-Diseased vii) Parsley -Healthy viii) Parsley-Diseased
    The architectural design of ReNet-50 used in phase 2 is kept similar as in phase
    1 except for the output layer which now has eight classes. If the input image
    classified into one of the ‘Diseased’ crop categories, it goes to phase 3. On
    the other hand, if any of the ‘Healthy’ crop categories are identified, the process
    ends, and the classified image does not go to the next phase for further processing.
    The third phase of the proposed system is disease detection, which involves classifying
    and localizing the diseased spots in an image and classifying them into one of
    the disease classes mentioned below. i. Lettuce-Bacterial leaf spot ii. Lettuce-Downy
    mildew iii. Basil-Downy mildew iv. Parsley-Septoria leaf spot v. Spinach-Downy
    mildew vi. Spinach-Stemphylium leaf spot Phase 3 activates only when the input
    from the previous phase is one of the ‘Diseased’ categories. To develop a disease
    detection model, an object detection algorithm is used. In the past recent years,
    advances in deep learning and computer vision have greatly accelerated the momentum
    of object detection (Khan et al., 2022). Numerous object detection algorithms
    (object detectors) are developed and used in the disease detection of crops. These
    detectors are broadly classified into two categories: i) two-stage detectors based
    on region proposal and ii) one-stage detectors based on regression or classification
    (Nguyen et al., 2020). The popular two-stage detectors are Fast-RCNN, Faster-RCNN,
    and Mask-RCNN, and one-stage detectors involve YOLO (You Only Look Once) family
    (Liu et al., 2021). Khan et al. conducted a research where they ran three different
    models, Faster-RCNN, YOLOv4, and EfficientDet, to solve a similar kind of problem
    for apple crops (Khan et al., 2022). It has been observed that Faster RCNN with
    mAP (mean average precision) of 42.1% outperformed YOLOv4 (mAP of 41.4%) and EfficientDet
    (mAP of 38%). As per these results, Faster-RCNN seems the right choice for this
    study. But YOLOv5 model developed by Ultralytics (Glenn, 2023) has substantially
    improved the detection speed while maintaining the detection accuracy. Therefore,
    both approaches are tested in this study. 3.3. Disease detection model training
    NVIDIA GeForce RTX 3090 is used to train all the models in three phases of the
    disease detection system. The classification model developed in stage 1 is implemented
    in PyTorch (an open source machine learning framework based on the torch library
    developed by Meta AI5). Using the transfer learning (TL) approach, ResNet-50 pre-trained
    on ImageNet is used (Russakovsky et al., 2015). The pre-trained model saves a
    lot of time as it is already trained on some dataset and hence contains the weights
    and biases of previous training that represent the features of the dataset it
    was trained on, which are often transferable to different datasets (Abbas et al.,
    2021). Hence, model parameters are initialized using the TL approach and then
    retrained on a custom dataset prepared in section 3.1.1 with a learning rate of
    0.0001, a batch size of 64, an input size of 224×224×3, and 100 epochs. The model
    was tuned using the Adam optimizer. For the classification model in phase 2, a
    batch size of 64 is used, and values of the remaining hyperparameters are kept
    the same. For training the object detection models, the dataset is split into
    75% for training, 20% for validation, and 5% for testing. The first model is implemented
    in Detectron2 that uses pre-trained architecture (trained on COCO dataset) ‘Faster-RCNN
    with ResNet-101 + FPN’. The model uses COCO JSON annotation format and is trained
    for 3000 iterations with the initial learning rate of 0.01 for the first 500 iterations
    and then 0.001 for the next 2500 iterations. The second model, YOLOv5s, is implemented
    in PyTorch. Again, a pre-trained version of the algorithm is used to enhance the
    training process and reduce time. For YOLOv5s, the annotation format is YOLO Darknet
    TXT but with the addition of a YAML file containing model configuration and class
    values. The model is trained for 3000 iterations. The hyperparameters and their
    values for the two models are shown in Table 2. Table 2. Values of hypermeters
    used for two objection detection methods. Hyperparameters Methods Faster-RCNN
    YOLOv5s Input size 600 × 600 416 × 416 Batch size 16 16 Learning rate 0.001 lr0
    = 0.01, lrf = 0.001 Momentum 0.89 0.937 Gamma value 0.1 fl_gamma = 0.0 Weight
    decay 0.0001 0.0005 Training time 1.5 h 50 min 3.4. Ontology model The complete
    development and details of all the concepts and instances of ontology model ‘AquaONT’
    developed by authors are available at (Abbasi et al., 2021b). AquaONT is a unified
    ontology model that represents and stores the essential knowledge of an aquaponics
    4.0 system. It consists of six concepts: Consumer Product, Ambient Environment,
    Contextual Data, Production System, Product Quality, and Production Facility.
    In this study, two classes, ‘Consumer Product’ and ‘Product Quality’ are used
    for knowledge extraction. The ‘Consumer Product’ class provides an abstract view
    of the type, growth status, and growth parameters of ready-to-harvest crops in
    an aquaponics system. Whereas the ‘Product Quality’ class provides knowledge on
    crop attributes related to pathology (crop diseases, causes, and the ways and
    means by which these can be managed or controlled) and morphology (canopy dimensions
    such as area, length, width, etc.). Four crops: lettuce, basil, parsley, and spinach,
    are considered in this study. Their growth conditions and morphological and pathological
    attributes stored as instances of the respective classes are extracted once the
    crop and disease are classified. Fig. 5 shows the hierarchical architecture of
    the ‘Consumer Product’ and ‘Product Quality’ classes with their instances for
    the ‘Basil’ crop in Protégé6 (an open-source ontology editor and framework developed
    at Stanford University) environment. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 5. Hierarchical structure of ‘Consumer
    Product’ and ‘Product Quality’ classes and respective instances in relation to
    Basil Crop. 3.5. Cloud-based application The trained model of the crop disease
    detection system is then saved and deployed on a cloud-based application built
    on Streamlit. The ontology model ‘AquaONT’ is also deployed on application, and
    relevant classes are integrated with the final disease detection model through
    Owlready2 library. The layout of the application is shown in Fig. 6. It consists
    of two user inputs ‘Select Model’ and ‘Upload Image’. ‘Select Model’ provides
    an option to select the model as per requirement, which in this study are ‘Crop
    Classification’ referring to phase 1, ‘Disease or No Disease’ referring to phase
    2, and ‘Disease Type, causes and Treatments’ referring to phase 3 of the proposed
    disease detection system. After model selection, an image is uploaded which is
    used by all the models. Once the disease is detected and classified, the causes
    and treatments of the disease are extracted from the ontology model automatically
    and displayed on the application panel. This kind of information is useful as
    it will allow agricultural practitioners to determine the causes of diseases and
    take precautionary steps in the early stages to avoid crop wastage and economic
    loss. Download : Download high-res image (2MB) Download : Download full-size image
    Fig. 6. Layout of cloud-based application for disease detection. 4. Experimental
    results and discussion This section presents the results of experiments performed
    in the current research work. First, the performance evaluation of deep learning
    models in three phases of the disease detection system is discussed. Next, the
    trained and validated system is tested on new data. In the end, the significance
    of the complete system is presented. The performance of the classification model
    in phase 1 is evaluated using a validation dataset. For this phase, there are
    four classes to be classified, namely lettuce, basil, spinach, and parsley. The
    distribution of labeled images in the validation set for this model is shown in
    Table 3. Table 3. Dataset distribution of validation set for phase 1. Class (Health
    + Diseased) Number of images Lettuce 160 Basil 104 Spinach 160 Parsley 104 The
    performance of the model is presented in the form of a confusion matrix (CM) shown
    in Fig. 7. The overall accuracy, precision, recall, and F-measure are computed
    by using the respective formulae, following common metrics for the performance
    of deep learning models in the literature (Khan et al., 2022). The computed metrics
    are summarized in Table 4. Download : Download high-res image (114KB) Download
    : Download full-size image Fig. 7. Confusion matrix of classification results
    in phase 1. Table 4. Results of classification model in phase 1. Crop Accuracy
    Precision Recall F1-Score Lettuce 0.97 0.95 0.96 0.96 Basil 0.98 0.96 0.96 0.96
    Spinach 0.96 0.94 0.94 0.94 Parsley 0.99 1 0.98 0.99 Average – 96.25% 96% 96.25%
    Overall accuracy 95.83% The classification model in phase 1 has achieved an overall
    accuracy of 95.83%, average precision of 96.25%, average recall of 96%, and average
    F1-score of 96.25%. As noted in Table 4, the performance metrics of the ‘spinach’
    class are lower than the other classes. Most model confusion comes in between
    spinach, basil, and lettuce leaves, particularly during the initial stages of
    their growth cycle. Next, the performance of the classification model in phase
    2 is evaluated in a similar fashion. For phase 2, there are six classes that model
    classifies, which are mentioned in section 3.2. Table 5 shows the distribution
    of the validation set used for the model in phase 2. Table 5. Distribution of
    validation dataset for phase 2. Class Number of images Lettuce-Healthy (LH) 48
    Lettuce-Diseased (LD) 112 Basil-Healthy (BH) 48 Basil-Diseased (BD) 56 Spinach-Healthy
    (SH) 48 Spinach-Diseased (SD) 112 Parsley-Healthy (PH) 48 Parsley-Diseased (PD)
    56 The CM for this model is shown in Fig. 8 and performance metrics are summarized
    in Table 6. Download : Download high-res image (222KB) Download : Download full-size
    image Fig. 8. Confusion matrix of classification results in phase 2. Table 6.
    Performance metrics of classification model in phase 2. Class Accuracy Precision
    Recall F1-score LH 0.979 0.86 0.92 0.89 LD 0.981 0.96 0.95 0.95 BH 0.989 0.90
    0.98 0.94 BD 0.983 0.91 0.93 0.92 SH 0.981 0.91 0.88 0.89 SD 0.983 0.96 0.96 0.96
    PH 0.994 0.98 0.96 0.97 PD 0.992 0.98 0.96 0.97 Average – 94% 94% 93.6% Overall
    accuracy 94.13% The classification model in phase 2 has achieved an overall accuracy
    of 94.13%, average precision of 94%, average recall of 94%, and average F1-score
    of 93.6%. It can be observed from the CM in Fig. 8 that the model is also prone
    to confusion in distinguishing between some of the classes. For instance, six
    examples of LD (Lettuce-Diseased) are classified among LH (1), BD (1), SH (2),
    and SD (2). This might be due to a lack of clarity in identifying leaf patterns
    and diseased spots. Finally, the performance of selected models for the detection
    phase (phase 3) is evaluated using a validation dataset. For this phase, there
    are six different diseases that models have to detect in crop leaves. These six
    diseases and their distribution in the validation dataset are given in Table 7.
    Table 7. Distribution of validation dataset in phase 3. Class Number of images
    Lettuce-Bacterial Leaf Spot (LBS) 56 Lettuce-Downy Mildew (LDM) 56 Basil-Downy
    Mildew (BDM) 56 Parsley-Septoria Leaf Spot (PSS) 56 Spinach-Downy Mildew (SDM)
    56 Spinach- Stemphylium Leaf Spot (SSS) 56 In this phase, the metric that is used
    to evaluate and compare the performance of two models, i-e, Faster-RCNN, and YOLOv5s,
    is mean average precision (mAP). The mAP is the primary evaluation indicator used
    for the evaluation of object detection models (Khan et al., 2022). In particular,
    mAP@0.5 (mean value of mAP at IOU threshold = 0.5) is evaluated. The comparison
    of the two models against all the classes is presented in Table 8. It can be seen
    that YOLOv5s with mAP@0.5 of 82.13% have outperformed Faster R-CNN. The two models
    have achieved the best mAP score for Lettuce-Bacterial Leaf Spot (LBS), Parsley-Septoria
    Leaf Spot (PSS), and Spinach-Stemphylium Leaf Spot (SSS), whereas a low mAP score
    is observed for Lettuce-Downy Mildew (LDM), Basil-Downy Mildew (BDM), and Spinach-Downy
    Mildew (SDM). Downy Mildew initially causes light green to yellow angular spots
    on the upper surfaces of leaves and hence looks similar independently of the crop
    type. This causes confusion for the detector in distinguishing the crop-specific
    Downy Mildew. But with more data, this issue can easily be resolved. Later in
    the growth cycle, the plant tissue affected with Downy Mildew turns tan in spinach,
    purplish brown in basil, and light brown in lettuce, which are correctly identified
    by the detector. Table 8. Class-wise comparison of two detection models. Class
    mAP Faster-RCNN YOLOV5s Lettuce-Bacterial Leaf Spot (LBS) 77.32 83.86 Lettuce-Downy
    Mildew (LDM) 73.89 78.63 Basil-Downy Mildew (BDM) 75.47 80.11 Parsley-Septoria
    Leaf Spot (PSS) 78.63 84.55 Spinach-Downy Mildew (SDM) 74.19 79.87 Spinach-Stemphylium
    Leaf Spot (SSS) 79.52 85.74 mAP@0.5 76.34 82.13 The performance evaluations of
    models in three phases have shown that detection models are not as straightforward
    as classification models. This is because an image consists of many objects which
    belong to either the same class or different classes. Hence, three things must
    be verified during evaluation, including object class, bounding box (object location),
    and confidence. In the end, the two detection models are compared in terms of
    inference time which is an important metric that determines the detection speed.
    It is observed that one-stage detector i-e., YOLOv5s with a detection speed of
    52.8 FPS (frames per second) is faster than Faster-RCNN with a detection speed
    of 43.2 FPS. Moreover, it is also observed that YOLOv5s accurately detect objects
    of varying sizes with little to no overlapping boxes. All the comparisons between
    the two detection models show that YOLOv5s have a clear advantage in terms of
    accuracy and run speed. Therefore, in this study, YOLOv5s is used for developing
    the disease detection system. After training and validation, the final crop disease
    detection system with YOLOv5s is tested using the test set containing new images.
    The system has shown promising results by effectively classifying and detecting
    the diseases in specified crops, which shows the system''s robustness in terms
    of dealing with a variety of objects having different shapes, patterns, textures,
    and colors. Fig. 9 shows examples where the system has accurately classified the
    crop and detected the diseased and healthy spots in crop leaves. Images in the
    first row of Fig. 9 are the results from three phases of the disease detection
    system for the Lettuce crop, which is suffering from Bacterial Leaf Spot disease.
    Similarly, row 2 and row 3 are the results from three phases of the system showing
    Spinach and Parsley, respectively, and the diseases they are suffering from, such
    as Downy Mildew and Septoria Leaf Spot disease respectively. Download : Download
    high-res image (1MB) Download : Download full-size image Fig. 9. Results from
    proposed disease detection system. The final crop disease detection system is
    then deployed on a cloud-based application developed in section 3.5. Fig. 6 shows
    the layout of the application. The ontology model discussed in section 3.4 is
    also integrated with the final system to build a complete real-time crop diagnostic
    system. The images are acquired wirelessly from the aquaponics facility through
    an interface developed on the Google Cloud Platform by the authors in previous
    work (Abbasi et al., 2022b). The images are stored in a folder to be used by the
    crop diagnostic system. Once the crop type and its disease are identified, the
    causes and treatments are automatically extracted from the ontology model and
    displayed on the application panel. For instance, Fig. 6 shows an example of working
    crop diagnostic system for parsley crops. The disease detected by the system after
    image uploading is Septoria Leaf Spot. The crop diagnostic system extracts the
    knowledge about potential causes and general treatments of this disease from AquaONT.
    The primary causes of Septoria Leaf Spot in Parsley could be high humidity level,
    infected seeds, leaf wetness, etc. This disease could also be caused due to irregular
    variations in air temperature. The potential preventive measures and treatments
    suggested by the system for this disease include: maintaining optimal humidity
    and temperature levels in accordance with Parsley crop and indoor aquaponics environment
    throughout the growth cycle, treating seeds before germination with hot water
    or Clorox bleach, using conventional fungicides if the disease is spread out in
    multiple plants. Downy Mildew disease is one of the most common diseases observed
    in different crops (McGrath, 2021). In the greenhouse or indoor farming environment,
    the potential causes of this disease are the same irrespective of crop type, which
    includes: high humidity, cool temperatures, infected seeds, and leaf wetness (Margaret
    Tuttle McGrath, 2021). Therefore, the methods to treat Downy Mildew in lettuce,
    basil, and spinach are also similar. This means that the classification of Downy
    Mildew disease with respect to crop type does not impact the results related to
    disease treatments. Despite this independence, it is still significant to perform
    the classification of Downy Mildew for each crop individually as its symptoms
    for three crops, lettuce, basil, and parsley, change later in the growth cycle.
    This might cause confusion for the detector to distinguish Downy Mildew from other
    diseases. For instance, the lettuce tissue affected with Downy Mildew eventually
    turns brown in later stages and these symptoms are similar to the Bacterial Leaf
    Spot symptom in lettuce, and both diseases have different treatment methods. The
    significance of the proposed system is that it can act as a vital tool for agriculturalists
    who wants to develop and digitize aquaponics farm. This system will allow them
    to diagnose diseases at early stages and also assist them in decision-making regarding
    crop characteristics and treatments of diseases. Moreover, this study will also
    promote the introduction of new implementations, such as research on the complex
    relationship between dynamic parameters (environmental and water) and diseases
    in aquaponics farms and self-adapting farms in case of disease detection. These
    smart technologies in the aquaponics system will reduce crop wastage and ensure
    both economic and environmental benefits. 5. Conclusions and future prospects
    This study proposes a crop diagnostic system for leafy green crops grown in an
    aquaponics environment. Four leafy green crops, lettuce, basil, spinach, and parsley,
    are considered. The first dataset is developed that contains 2640 healthy and
    diseased images of these four crops collected from various sources. Next, a system
    is proposed that can efficiently and effectively identify crops and diseases.
    The detection system works in three phases. The first phase classifies the crop
    type, the second phase classifies whether the crop is healthy or diseased, and
    then in the third phase, the disease is detected if the crop is classified as
    diseased in the previous phase. All the models used in this study are initialized
    using transfer learning and then trained on a dataset prepared for leafy green
    crops. The performance of the models is evaluated, and promising results are achieved.
    For instance, in the detection phase, YOLOv5s with mAP@0.5 of 82.13% and detection
    speed of 52.8 FPS has outperformed Faster-RCNN. Based on the performance, YOLOv5s
    is selected as a final model for this study. The ontology model that contains
    knowledge related to causes and treatments of diseases is then integrated with
    the final crop disease detection system. Finally, a cloud-based application is
    designed where the final crop diagnostic system consisting of a disease detection
    system and ontology model is deployed. The proposed system proves to be accurate
    and flexible enough to be used in real scenarios and hence is not limited to being
    disturbed by potential changing conditions and environments. It can be a helpful
    tool for agricultural practitioners who want to explore modern farming practices
    and want to integrate smart techniques into their farms. This system will not
    only help them in disease diagnosis and quantification but will also assist them
    in decision-making regarding potential treatments against identified diseases
    at early stages. For future work, the system will be extended to include other
    leafy green crops. Moreover, the dataset will also be extended, and more real-field
    images will be incorporated. Moreover, a mobile application will be constructed,
    reducing the latency, and providing data privacy, which normally occurs in cloud-based
    systems. CRediT authorship contribution statement R. Abbasi: Conceptualization,
    Methodology, Software, Validation, Formal analysis, Visualization, Investigation,
    Data curation, Writing – original draft, Writing – review & editing. P. Martinez:
    Conceptualization, Methodology, Visualization, Writing – review & editing, Supervision.
    R. Ahmad: Supervision, Funding acquisition, Project administration, Writing –
    review & editing. Declaration of Competing Interest The authors declare that they
    have no known competing financial interests or personal relationships that could
    have appeared to influence the work reported in this paper. Acknowledgments The
    authors acknowledge the financial support of this work from the Natural Sciences
    and Engineering Research Council of Canada (NSERC) (Grant File No. ALLRP 545537-19
    and RGPIN-2017-04516). References Abbas et al., 2021 A. Abbas, S. Jain, M. Gour,
    S. Vankudothu Tomato plant disease detection using transfer learning with C-GAN
    synthetic images Comput. Electron. Agric., 187 (2021), Article 106279, 10.1016/J.COMPAG.2021.106279
    View PDFView articleView in ScopusGoogle Scholar Abbasi et al., 2021a R. Abbasi,
    P. Martinez, R. Ahmad An ontology model to support the automated design of aquaponic
    grow beds Proced. CIRP, 100 (2021), pp. 55-60, 10.1016/j.procir.2021.05.009 View
    PDFView articleView in ScopusGoogle Scholar Abbasi et al., 2021b R. Abbasi, P.
    Martinez, R. Ahmad An ontology model to represent aquaponics 4.0 system’s knowledge
    Inf. Process. Agric. (2021), 10.1016/J.INPA.2021.12.001 Google Scholar Abbasi
    et al., 2022a R. Abbasi, P. Martinez, R. Ahmad The digitization of agricultural
    industry – a systematic literature review on agriculture 4.0 Smart Agric. Technol.,
    2 (2022), Article 100042, 10.1016/J.ATECH.2022.100042 View PDFView articleView
    in ScopusGoogle Scholar Abbasi et al., 2022b R. Abbasi, P. Martinez, A.R Data
    acquisition and monitoring dashboard for IoT enabled aquaponics facility The 10th
    International Conference on Control, Mechatronics and Automation (ICCMA 2022)
    (Accepted), IEEE (2022) Google Scholar Anami et al., 2020 B.S. Anami, N.N. Malvade,
    S. Palaiah Deep learning approach for recognition and classification of yield
    affecting paddy crop stresses using field images Artif. Intell. Agric., 4 (2020),
    pp. 12-20, 10.1016/J.AIIA.2020.03.001 View PDFView articleView in ScopusGoogle
    Scholar Barosa et al., 2019 R. Barosa, S.I.S. Hassen, L. Nagowah Smart aquaponics
    with disease detection 2nd Int. Conf. Next Gener. Comput. Appl. 2019, NextComp
    2019 - Proc (2019), 10.1109/NEXTCOMP.2019.8883437 Google Scholar Bedi and Gole,
    2021 P. Bedi, P. Gole Plant disease detection using hybrid model based on convolutional
    autoencoder and convolutional neural network Artif. Intell. Agric., 5 (2021),
    pp. 90-101, 10.1016/J.AIIA.2021.05.002 View PDFView articleView in ScopusGoogle
    Scholar Buslaev et al., 2020 A. Buslaev, V.I. Iglovikov, E. Khvedchenya, A. Parinov,
    M. Druzhinin, A.A. Kalinin Albumentations: Fast and flexible image augmentations
    Inf., 11 (2020), 10.3390/INFO11020125 Google Scholar Chen et al., 2020 J. Chen,
    D. Zhang, Y.A. Nanehkaran, D. Li Detection of rice plant diseases based on deep
    transfer learning J. Sci. Food Agric., 100 (2020), pp. 3246-3256, 10.1002/JSFA.10365
    View in ScopusGoogle Scholar Dhal et al., 2022 S.B. Dhal, M. Bagavathiannan, U.
    Braga-Neto, S. Kalafatis Nutrient optimization for plant growth in Aquaponic irrigation
    using machine learning for small training datasets Artif. Intell. Agric., 6 (2022),
    pp. 68-76, 10.1016/J.AIIA.2022.05.001 View PDFView articleView in ScopusGoogle
    Scholar Dutot et al., 2013 M. Dutot, L.M. Nelson, R.C. Tyson Predicting the spread
    of postharvest disease in stored fruit, with application to apples Postharvest
    Biol. Technol., 85 (2013), pp. 45-56, 10.1016/J.POSTHARVBIO.2013.04.003 View PDFView
    articleView in ScopusGoogle Scholar Fan et al., 2021 X. Fan, J. Zhou, Y. Xu, X.
    Peng Corn disease recognition under complicated background based on improved convolutional
    neural network. Nongye Jixie Xuebao/transactions Chinese Soc Agric. Mach., 52
    (2021), pp. 210-217, 10.6041/J.ISSN.1000-1298.2021.03.023 View in ScopusGoogle
    Scholar Fuentes et al., 2017 A. Fuentes, S. Yoon, S.C. Kim, D.S. Park A robust
    deep-learning-based detector for real-time tomato plant diseases and pests recognition
    Sensors, 17 (2017), p. 2022, 10.3390/S17092022 View in ScopusGoogle Scholar Gillani
    et al., 2022 S.A. Gillani, R. Abbasi, P. Martinez, R. Ahmad Review on energy efficient
    artificial illumination in aquaponics Clean. Circ. Bioecon., 2 (2022), Article
    100015, 10.1016/J.CLCB.2022.100015 View PDFView articleView in ScopusGoogle Scholar
    Glenn, 2023 Glenn Ultralytics/yolov5 [WWW Document]. URL https://github.com/ultralytics/yolov5
    (2023) Google Scholar He et al., 2015 K. He, X. Zhang, S. Ren, J. Sun Deep residual
    learning for image recognition Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
    Recognit. 2016-December (2015), pp. 770-778, 10.48550/arxiv.1512.03385 View in
    ScopusGoogle Scholar Horrocks et al., 2005 I. Horrocks, P.F. Patel-Schneider,
    S. Bechhofer, D. Tsarkov OWL rules: a proposal and prototype implementation Web
    Semant. (2005), 10.1016/j.websem.2005.05.003 Google Scholar Jearanaiwongkul et
    al., 2021 W. Jearanaiwongkul, C. Anutariya, T. Racharak, F. Andres An ontology-based
    expert system for Rice disease identification and control recommendation Appl.
    Sci., 11 (2021), p. 10450, 10.3390/APP112110450 View in ScopusGoogle Scholar Jha
    et al., 2019 K. Jha, A. Doshi, P. Patel, M. Shah A comprehensive review on automation
    in agriculture using artificial intelligence Artif. Intell. Agric., 2 (2019),
    pp. 1-12, 10.1016/J.AIIA.2019.05.004 View PDFView articleView in ScopusGoogle
    Scholar Khan et al., 2022 A.I. Khan, S.M.K. Quadri, S. Banday, J. Latief Shah
    Deep diagnosis: a real-time apple leaf disease detection system based on deep
    learning Comput. Electron. Agric., 198 (2022), Article 107093, 10.1016/J.COMPAG.2022.107093
    View PDFView articleView in ScopusGoogle Scholar Khirade and Patil, 2015 S.D.
    Khirade, A.B. Patil Plant disease detection using image processing Proc. - 1st
    Int. Conf. Comput. Commun. Control Autom. ICCUBEA 2015 (2015), pp. 768-771, 10.1109/ICCUBEA.2015.153
    View in ScopusGoogle Scholar Lisha Kamala and Anna Alex, 2021 K. Lisha Kamala,
    S. Anna Alex Apple fruit disease detection for hydroponic plants using leading
    edge technology machine learning and image processing Proc. - 2nd Int. Conf. Smart
    Electron. Commun. ICOSEC 2021 (2021), pp. 820-825, 10.1109/ICOSEC51865.2021.9591903
    View in ScopusGoogle Scholar Liu et al., 2021 C. Liu, H. Zhu, W. Guo, X. Han,
    C. Chen, H. Wu EFDet: an efficient detection method for cucumber disease under
    natural complex environments Comput. Electron. Agric., 189 (2021), Article 106378,
    10.1016/J.COMPAG.2021.106378 View PDFView articleView in ScopusGoogle Scholar
    Ma et al., 2018 J. Ma, K. Du, F. Zheng, L. Zhang, Z. Sun Disease recognition system
    for greenhouse cucumbers based on deep convolutional neural network. Nongye Gongcheng
    Xuebao/transactions Chinese Soc Agric. Eng., 34 (2018), pp. 186-192, 10.11975/J.ISSN.1002-6819.2018.12.022
    View in ScopusGoogle Scholar Mathew and Mahesh, 2022 M.P. Mathew, T.Y. Mahesh
    Leaf-based disease detection in bell pepper plant using YOLO v5 Signal, Image
    Video Process, 16 (2022), pp. 841-847, 10.1007/S11760-021-02024-Y/FIGURES/12 View
    in ScopusGoogle Scholar McGrath, 2021 Margaret Tuttle McGrath Pest management
    [WWW Document] Cornell Univ (2021) URL https://www.vegetables.cornell.edu/pest-management/
    accessed 8.3.22 Google Scholar Musa et al., 2021 A. Musa, M. Hamada, F.M. Aliyu,
    M. Hassan An intelligent plant Dissease detection system for smart hydroponic
    using convolutional neural network Proc. - 2021 IEEE 14th Int. Symp. Embed. Multicore/Many-Core
    Syst. MCSoC 2021 (2021), pp. 345-351, 10.1109/MCSOC51149.2021.00058 View in ScopusGoogle
    Scholar Nandhini et al., 2022 M. Nandhini, K.U. Kala, M. Thangadarshini, S. Madhusudhana
    Verma Deep learning model of sequential image classifier for crop disease detection
    in plantain tree cultivation Comput. Electron. Agric., 197 (2022), Article 106915,
    10.1016/J.COMPAG.2022.106915 View PDFView articleView in ScopusGoogle Scholar
    Nguyen et al., 2020 N.D. Nguyen, T. Do, T.D. Ngo, D.D. Le An evaluation of deep
    learning methods for small object detection J. Electr. Comput. Eng., 2020 (2020),
    10.1155/2020/3189691 Google Scholar Noyan, 2022 M.A. Noyan Uncovering Bias in
    the Plant Village Dataset. (2022), 10.48550/arxiv.2206.04374 Google Scholar Oppenheim
    et al., 2019 D. Oppenheim, G. Shani, O. Erlich, L. Tsror Using deep learning for
    image-based potato tuber disease detection Phytopathology, 109 (2019), pp. 1083-1087,
    10.1094/PHYTO-08-18-0288-R View in ScopusGoogle Scholar Pathan et al., 2020 M.
    Pathan, N. Patel, H. Yagnik, M. Shah Artificial cognition for applications in
    smart agriculture: a comprehensive review Artif. Intell. Agric., 4 (2020), pp.
    81-95, 10.1016/J.AIIA.2020.06.001 View PDFView articleView in ScopusGoogle Scholar
    Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for multi-crop
    leaf disease image classification using convolutional neural network VGG Artif.
    Intell. Agric., 6 (2022), pp. 23-33, 10.1016/J.AIIA.2021.12.002 View PDFView articleView
    in ScopusGoogle Scholar Qi et al., 2022 J. Qi, X. Liu, K. Liu, F. Xu, H. Guo,
    X. Tian, M. Li, Z. Bao, Y. Li An improved YOLOv5 model based on visual attention
    mechanism: application to recognition of tomato virus disease Comput. Electron.
    Agric., 194 (2022), Article 106780, 10.1016/J.COMPAG.2022.106780 View PDFView
    articleView in ScopusGoogle Scholar Rahman et al., 2020 C.R. Rahman, P.S. Arko,
    M.E. Ali, M.A. Iqbal Khan, S.H. Apon, F. Nowrin, A. Wasif Identification and recognition
    of rice diseases and pests using convolutional neural networks Biosyst. Eng.,
    194 (2020), pp. 112-120, 10.1016/J.BIOSYSTEMSENG.2020.03.020 View PDFView articleView
    in ScopusGoogle Scholar Rodríguez-García et al., 2021 M.Á. Rodríguez-García, F.
    García-Sánchez, R. Valencia-García Knowledge-based system for crop pests and diseases
    recognition Electron, 10 (2021), p. 905, 10.3390/ELECTRONICS10080905 View in ScopusGoogle
    Scholar Russakovsky et al., 2015 O. Russakovsky, J. Deng, H. Su, J. Krause, S.
    Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg, L.
    Fei-Fei ImageNet large scale visual recognition challenge Int. J. Comput. Vis.,
    115 (2015), pp. 211-252, 10.1007/S11263-015-0816-Y Google Scholar Singh et al.,
    2019 D. Singh, N. Jain, P. Jain, P. Kayal, S. Kumawat, N. Batra PlantDoc: a dataset
    for visual plant disease detection ACM Int. Conf. Proceeding Ser, 249–253 (2019),
    10.1145/3371158.3371196 Google Scholar Singh et al., 2020 V. Singh, N. Sharma,
    S. Singh A review of imaging techniques for plant disease detection Artif. Intell.
    Agric., 4 (2020), pp. 229-242, 10.1016/J.AIIA.2020.10.002 View PDFView articleView
    in ScopusGoogle Scholar Stouvenakers et al., 2019 Gilles Stouvenakers, Peter Dapprich,
    Sebastien Massart, M.H. Jijakli, G. Stouvenakers, S. Massart, M.H. Jijakli, P.
    Dapprich Plant pathogens and control strategies in aquaponics Aquapon. Food Prod.
    Syst., 353–378 (2019), 10.1007/978-3-030-15943-6_14 Google Scholar Studer et al.,
    1998 R. Studer, V.R. Benjamins, D. Fensel Knowledge engineering: principles and
    methods Data Knowl. Eng. (1998), 10.1016/S0169-023X(97)00056-6 Google Scholar
    Subeesh and Mehta, 2021 A. Subeesh, C.R. Mehta Automation and digitization of
    agriculture using artificial intelligence and internet of things Artif. Intell.
    Agric., 5 (2021), pp. 278-291, 10.1016/J.AIIA.2021.11.004 View PDFView articleView
    in ScopusGoogle Scholar Weaver et al., 2020 W.N. Weaver, J. Ng, R.G. Laport LeafMachine:
    using machine learning to automate leaf trait extraction from digitized herbarium
    specimens Appl. Plant Sci., 8 (2020), 10.1002/APS3.11367 Google Scholar Yanes
    et al., 2020 A.R. Yanes, P. Martinez, R. Ahmad Towards automated aquaponics: a
    review on monitoring, IoT, and smart systems J. Clean. Prod. (2020), 10.1016/j.jclepro.2020.121571
    Google Scholar Yudha Pratama et al., 2020 I. Yudha Pratama, A. Wahab, M. Alaydrus
    Deep learning for assessing unhealthy lettuce hydroponic using convolutional neural
    network based on faster R-CNN with Inception V2 2020 5th Int. Conf. Informatics
    Comput, 2020, ICIC (2020), 10.1109/ICIC50835.2020.9288554 Google Scholar Zheng
    et al., 2019 Y.Y. Zheng, J.L. Kong, X.B. Jin, X.Y. Wang, T.L. Su, M. Zuo CropDeep:
    the crop vision dataset for deep-learning-based classification and detection in
    precision agriculture Sensors, 19 (2019), p. 1058, 10.3390/S19051058 View in ScopusGoogle
    Scholar Cited by (3) Mitigating Global Challenges: Harnessing Green Synthesized
    Nanomaterials for Sustainable Crop Production Systems 2024, Global Challenges
    Aquaponics: A Sustainable Path to Food Sovereignty and Enhanced Water Use Efficiency
    2023, Water (Switzerland) Hydroponic lettuce defective leaves identification based
    on improved YOLOv5s 2023, Frontiers in Plant Science 1 https://streamlit.io/.
    2 https://pypi.org/project/Owlready2/. 3 https://www.ecosia.org/. 4 https://github.com/tzutalin/labelImg.
    5 https://pytorch.org/hub/pytorch_vision_resnet/. 6 https://protege.stanford.edu/products.php#desktop-protege.
    © 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications
    Co., Ltd. Recommended articles GxENet: Novel fully connected neural network based
    approaches to incorporate GxE for predicting wheat yield Artificial Intelligence
    in Agriculture, Volume 8, 2023, pp. 60-76 Sheikh Jubair, …, Mike Domaratzki View
    PDF Development and evaluation of temperature-based deep learning models to estimate
    reference evapotranspiration Artificial Intelligence in Agriculture, Volume 9,
    2023, pp. 61-75 Amninder Singh, Amir Haghverdi View PDF Cumulative unsupervised
    multi-domain adaptation for Holstein cattle re-identification Artificial Intelligence
    in Agriculture, Volume 10, 2023, pp. 46-60 Fabian Dubourvieux, …, Romaric Audigier
    View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 3 Captures
    Readers: 47 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Artificial Intelligence in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Crop diagnostic system: A robust disease detection and management system
    for leafy green crops grown in an aquaponics facility'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Devi N.
  - Sarma K.K.
  - Laskar S.
  citation_count: '10'
  description: Crop health monitoring and weed removal are two crucial elements dictating
    efficient, productive and resilient cultivation. Due to frequent attacks by pest
    and pathogens, the crops become diseased resulting in degradation of the quality
    and quantity of the production. The process of continuous monitoring of crop health
    is challenging and requires the involvement of information and communication technologies
    (ICT). The outcome is precision agriculture where the Internet of Things (IoT)
    and Artificial Intelligence (AI) techniques are vital ingredients. The design
    of an integrated approach of precision agriculture based on IoT and AI is discussed
    here which is tailored for real time crop health monitoring and performs various
    other operations like weed detection, ambient air sensing, watering the vegetation
    automatically at regular intervals of time, spraying of pesticides etc. The proposed
    system is a combination of an IoT formed using sensors and devices, image processing
    and machine learning (ML)/ deep learning (DL) techniques confined to the cultivation
    of fifteen varieties of beans found in India. The work involves two intelligent
    learning models configured to capture spatio-temporal attributes of image samples
    and sensor inputs and for real time discrimination between healthy and diseased
    bean leaves, detection of weeds growing around the cultivation land and also for
    process control. The first approach employs a DL structure named EfficientNetB7
    along with a Bidirectional Long Short Term Memory (BiLSTM) while the second method
    adopts a VGG16 with an integrated attention mechanism. Also experiments have been
    carried out using benchmark ML classifiers like Support Vector Machine (SVM),
    Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer Perceptron (MLP) and
    Time Delay Neural Network (TDNN) combined with feature extraction techniques.
    Segmentation methods have been used to separate out the diseased sections of the
    leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
  doi: 10.1016/j.ecoinf.2023.102044
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methodology
    3. Design details of the proposed models 4. Experimental results 5. Conclusion
    Ethics approval Consent to participate Consent for publication Availability of
    data Grants and funding Author''s contribution Declaration of Competing Interest
    Acknowledgements Data availability References Show full outline Cited by (11)
    Figures (15) Show 9 more figures Tables (12) Table 1 Table 2 Table 3 Table 4 Table
    5 Table 6 Show all tables Ecological Informatics Volume 75, July 2023, 102044
    Design of an intelligent bean cultivation approach using computer vision, IoT
    and spatio-temporal deep learning structures Author links open overlay panel Nilakshi
    Devi a, Kandarpa Kumar Sarma b, Shakuntala Laskar a Show more Share Cite https://doi.org/10.1016/j.ecoinf.2023.102044
    Get rights and content Highlights • The design of EfficientNetB7 with BiLSTM and
    VGG16 with attention has been proposed. • The models capture the spatio-temporal
    attributes of the bean leaf samples. • Segmentation is performed to separate the
    diseased area of the leaves. • DL-IoT set up to monitor the environmental parameters
    and execute process control. Abstract Crop health monitoring and weed removal
    are two crucial elements dictating efficient, productive and resilient cultivation.
    Due to frequent attacks by pest and pathogens, the crops become diseased resulting
    in degradation of the quality and quantity of the production. The process of continuous
    monitoring of crop health is challenging and requires the involvement of information
    and communication technologies (ICT). The outcome is precision agriculture where
    the Internet of Things (IoT) and Artificial Intelligence (AI) techniques are vital
    ingredients. The design of an integrated approach of precision agriculture based
    on IoT and AI is discussed here which is tailored for real time crop health monitoring
    and performs various other operations like weed detection, ambient air sensing,
    watering the vegetation automatically at regular intervals of time, spraying of
    pesticides etc. The proposed system is a combination of an IoT formed using sensors
    and devices, image processing and machine learning (ML)/ deep learning (DL) techniques
    confined to the cultivation of fifteen varieties of beans found in India. The
    work involves two intelligent learning models configured to capture spatio-temporal
    attributes of image samples and sensor inputs and for real time discrimination
    between healthy and diseased bean leaves, detection of weeds growing around the
    cultivation land and also for process control. The first approach employs a DL
    structure named EfficientNetB7 along with a Bidirectional Long Short Term Memory
    (BiLSTM) while the second method adopts a VGG16 with an integrated attention mechanism.
    Also experiments have been carried out using benchmark ML classifiers like Support
    Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer
    Perceptron (MLP) and Time Delay Neural Network (TDNN) combined with feature extraction
    techniques. Segmentation methods have been used to separate out the diseased sections
    of the leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
    Previous article in issue Next article in issue Keywords Artificial intelligenceConvolution
    neural networkLearning based systemSmart farming 1. Introduction Precision agriculture
    (PA) has become essential to increase productivity of traditional cultivation
    (Naik et al., 2022). Due to various factors like environment, pest and pathogens,
    soil etc. crop cultivation is severely affected which decrease their quality as
    well as the production quantity (Naik et al., 2022). It leads to severe financial
    loss and food crisis. In a recent report, it has been estimated that a sizeable
    percentage of the crop yields globally have been decreased due to pests and pathogens
    (CABI, 2022). Thus, it is has become essential that the farmers should be well
    equipped with the state-of-the-art (SOTA) technologies for proper monitoring of
    the health of the cultivated plants on regular basis. Artificial Intelligence
    (AI) in combination with Internet of Things (IoT) have become popular in agriculture
    due to several factors like continuous process monitoring, accurate control, quick
    decision making, automation, reliability etc(Jha et al., 2019). The use of AI
    tools is also beneficial since these are adaptive systems that can understand
    its surroundings, capture the relevant details and retain the knowledge to find
    solutions for the real world problems. AI is already triggering major transformations
    in the agriculture sector helping the farmers to cope up with several challenges
    like monitoring plant diseases, weed detection, identification and control of
    pest and pathogens, crop harvesting etc. with minimum loss of time (Jha et al.,
    2019). Further, AI approaches constituted by machine learning (ML) and deep learning
    (DL) tools provide enough capability to handle real life challenges and demonstrate
    the ability to generate human like decision making (Hapsari et al., 2022) in case
    of agriculture. Over the years researchers have attempted several approaches for
    accurate plant disease identification and detection. Use of ML methods as part
    of precision agriculture including plant disease detection (Pantazi et al., 2019)
    has been widely reported. ML techniques like Support Vector Machine (SVM), Random
    Forest (RF), K-Nearest Neighbor (KNN), Artificial Neural Network (ANN), Naive
    Bayes etc. have been extensively used (Rudagi et al., 2022) in agriculture. A
    technique for classification of rice disease has been developed using SVM like
    ML algorithms (Maione et al., 2018). Pantazi et al. have developed a system for
    detection of yellow rust disease in wheat leaves using hyper spectral imaging
    data and SVM technique (Pantazi et al., 2019). In another study, the authors (Binch
    and Fox, 2017) have reported a method for crop and weed detection using SVM technique.
    Three different wheat disease detection methods using Naïve Baye''s classifier
    have been reported by (Johannes et al., 2017). In another approach, an ANN model
    has been used (Johann et al., 2016) for crop monitoring and estimation of soil
    parameters. An ANN based system for weed detection using multispectral images
    captured by unmanned aircraft system (UAS) has been reported in (Kashefi et al.,
    2017). These ML approaches could provide satisfactory results but it has been
    observed that the efficiency of these models decline significantly as the size
    (Sibiya et al., 2019) and diversity (Sibiya et al., 2019) of the data increases.
    Moreover, the manual process of feature extraction in a ML model is much time-consuming
    and erroneous at times which also act as a limitation of this class of techniques
    (Goodfellow et al., 2016). Thus, to overcome these crucial limitations, the attention
    shifted towards the DL models as alternatives to ML methods (Singh et al., 2020)
    Further it is also a part of the global trend of replacing ML approaches with
    DL techniques. DL being a specialized subset of the ML techniques enables a computerized
    set-up to thrive upon larger volumes of data, use a complex stage of objective
    driven in built feature extraction process and catalyze a tunable classifier block
    to demonstrate the ability of superior decision making while handling real life
    situations (Goodfellow et al., 2016). Further, due to the availability of Graphical
    Processing Units (GPUs) and other high performance architectures that can process
    huge volumes of raw data without the traditional feature extraction stage dependent
    on human intervention, Sutaji and Yildiz (2022) DL supports self- sustaining and
    continuous learning. Popularly, DL methods consist of two approaches: one is to
    develop the whole model from scratch and the other option is the adoption of transfer
    learning (TL) (Goodfellow et al., 2016). The former method is time consuming and
    requires a huge amount of data to train the network which is not always accessible
    (Shah et al., 2022). Thus, the latter method started to become attractive. Lately,
    transfer learning has turned out to be a process where the model is trained on
    a huge dataset and the knowledge acquired during the training is stored for solving
    similar task (Geron, 2019) and passed on to networks configured for identical
    chores. It popularized the pre-trained models (PTMs) that utilize the concept
    of transfer learning and accelerated their applications in a range of real world
    scenarios. The PTMs are emerging as efficient and expandable frameworks in solving
    many image classifications and computer vision problems (Shah et al., 2022) including
    those relevant to precision agriculture and ecological informatics. The PTMs reduce
    the time and effort considerably and offer a higher learning rate during training
    since the models are already accustomed for similar assignments (Geron, 2019).
    With the increasing popularity of the DL architectures, Convolution Neural Network
    (CNN) – a common DL architecture, has been explored extensively for precision
    agriculture with special focus on efficient plant disease detection and classification
    (Akram et al., 2017) (Ferentinos, 2018). As DL architectures evolved with time,
    models like VGG16, ResNet, DenseNet etc. have been successfully used for plant
    disease detection with higher classification accuracies (CA) (Chen et al., 2020).
    The Residual Neural Network (ResNet) has been fruitful in addressing the notorious
    vanishing gradient problem of the DL networks (Tool et al., 2019) and has been
    applied extensively in pattern recognition problems (Tool et al., 2019). Other
    CNN structures like DenseNet, SqueezeNet, Xception etc. have also been commonly
    used for plant disease detection (Yadav et al., 2021). CNN architectures like
    AlexNet, GoogleNet etc. have been explored for disease classification of different
    vegetables like tomato, potato, lady''s finger, beans, spinach etc. (Yadav et
    al., 2021) (Pandey, 2022). As indicated above, there are many instances of different
    DL structures being adopted for accurate plant disease detection (Picon, 2019;
    Singh et al., 2020; Shah et al., 2022; Thi et al., 2022, Manjula et al., 2022).
    Another study reports a deep transfer learning method for Casava plant disease
    detection (Shah et al., 2022). Too et al. proposed a plant disease detection method
    using ResNet50, ResNet101 and InceptionV1 (Tool et al., 2019). Further, computer
    vision and ML/DL techniques have also been successfully explored in many plant
    disease detection applications (Traore et al., 2019), ResNet50 based classification
    of five crop diseases (Das et al., 2022), DenseNet and Inception nets for rice
    crop disease detection (Hapsari et al., 2022), papaya leaf disease detection using
    ResNet (Veeraballi, 2020), grape leaf disease detection using InceptionV1 and
    ResNetV2 (Xie et al., 2020), attention dense learning (ADL) mechanism for classification
    of leaf health conditions of non-identical plants (Akshay Pandey and Kamal Jain,
    2021) etc. In another study, an IoT based system has been reported for remote
    checking of agriculture parameters (Nicola and Pisana, 2021). Asmita et al. reported
    the use of ML and DL techniques integrated with IoT for crop monitoring (Asmita
    Hobisiyashi, 2022). Joshi et al. proposed a CNN based system for detection of
    disease in mungo bean leaves (Rakesh et al., 2021). Another study involves a CNN
    aided work for disease detection in peach crops (Yadav et al., 2021). Gokulnath
    and Usha (2021) have proposed a method for plant disease identification using
    LF-CNN. Tiwari et al. proposed a work on multi class plant disease detection and
    classification using leaf images with a dense CNN (Tiwari et al., 2021). A DL
    approach for tomato leaf disease detection has been reported which employs EfficientNetB1
    with different classifiers like RF and SVM (Chug et al., 2022). Sutaji and Yildiz
    (2022) have proposed MobileNetV2 and Xception models for prediction of plant diseases.
    Naik et al. have designed a squeeze and excitation based CNN model for chili leaf
    disease detection (Naik et al., 2022). Zan et al. have developed a CNN model named
    MatDet for tomato maturity detection and classification of tomato leaf diseases
    (Zan et al., 2022). Limei et al. have developed an R-CNN model for estimation
    of strawberry leaf scorch severity (Xia et al., 2022). Ali et al. have developed
    a DL based prediction model for plant disease identification (Hobisiyashi and
    Yadav, 2022). Olivia et al. have proposed a BLeafNet model for plant disease detection
    using leaf RGB images (Olivia et al., 2022). Mesut Togacar (2022) has proposed
    a method to detect weeds growing along with seedlings using DarkNet models and
    meta-heuristic algorithms. In another study, Bhagat et al. have proposed a method
    for plant leaf segmentation using Eff-UNet model (Bhagat et al., 2022). Olfa et
    al. (2023) proposed a DL based segmentation method for plant disease detection.
    Kaya et al. have proposed a multi head CNN model for identification of plant diseases
    using RGB images (Kaya et al., 2023). In another study, a DenseNet model is proposed
    for classification of maize disease (Wang et al., 2023a, Wang et al., 2023b).
    Vimal et al. have proposed a fine tuned CNN model for classification of beans
    plant leaf diseases (Vimal et al., 2023). The performances of these models have
    been evaluated using metrics like CA, precision, recall and F1 score. Several
    works also have used other metrics like sensitivity, specificity, type I and type
    II error, micro and macro F1 score to evaluate the performance of the models.
    The above works have discussed several dimensions related to precision agriculture,
    yet there are ample of opportunities to explore new approaches to develop effective
    solutions and lend a helping hand to the farming community. Further, there are
    sufficient scopes to formulate mechanisms for designing intelligent decision support
    and process control systems for more accurate detection and prediction of plant
    leaf health, weed detection, water sprinkling, conservation of resources, optimized
    use of chemicals for control of unwanted vegetation growth etc. Especially the
    improvements in performance that can be obtained by adopting innovative DL networks
    combined with computer vision methods and IoT set-ups in an integrated approach
    need to be explored. DL structures that can handle spatial and temporal variations
    in samples are expected to contribute towards better performance especially with
    real-time inputs. The CNN based structures are reliable in capturing spatial content
    of samples especially those from visual inputs but are inefficient in handling
    details with time varying characteristics (Goodfellow et al., 2016). For temporal
    attributes, recurrent structures like the long short term memory (LSTM) cells
    are considered to be efficient (Goodfellow et al., 2016). Especially, LSTMs configured
    in a two way processing mode called bi-directional LSTM (BiLSTM) are regarded
    to be competent in capturing sequences and temporal variations (Goodfellow et
    al., 2016). Moreover, integrated and AI aided frameworks with the ability to extract
    spatial and temporal variations while employed for plant leaf health monitoring,
    weed detection and water sprinkling as per continuously captured on-field sensor
    data have relevance not only for the farming community but also for ecological
    conservation. Here, we discuss the design of an approach that extracts both spatial
    and temporal attributes of samples using a set-up that combines multiple DL methods
    and an IoT arrangement for application in precision agriculture and especially
    configured for the cultivation of several varieties of beans found in different
    parts of India. The system is designed to determine the status of the bean cultivation
    by looking into the health of the leaves, execute varied parameter monitoring
    and process control and segregate weed growth from the crop plantation. The main
    constituent of the system is a DL block formed by a CNN based EfficientNetB7 with
    a BiLSTM structure and a VGG16 with an attention layer. Here, the EfficientNetB7
    and VGG16 capture the spatial contents while the BiLSTM and attention blocks deal
    with the temporal attributes of the input respectively. These two structures have
    been trained with images of fifteen varieties of bean leaves which are obtained
    from the “Beans” database. Further, these samples are augmented with the bootstrapping
    method and tested with actual samples collected during field visits to the bean
    cultivation farms. The performances of the networks are compared with that obtained
    from SVM, RF, KNN, Multi-Layer Perceptron (MLP) and Time Delay Neural Network
    (TDNN). Moreover, physiological attributes of the leaves are captured using features
    like Gray Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP) and Local
    Binary Gray Level Co-occurrence Matrix (LBGLCM) while extracted sections (region
    of interest (ROI)) of the leaves in healthy and diseased forms are obtained using
    a few segmentation techniques including Fuzzy C-Means clustering (FCM) for extracting
    labels. These ROIs are used as targets during the training of the classifiers.
    The above ML/ DL methods are trained and the best approach is determined from
    a series of validation cycles. Further, on-field testing is carried out with samples
    collected using near infrared (NIR) camera. Subsequently, data from soil moisture
    sensor, temperature sensor and humidity sensor are connected to a processing node
    with WiFi access which forms an IoT pack for continuous monitoring. The system
    accepts camera and sensor inputs to provide discrimination decision regarding
    plant health, weed growth and also triggers process controls activities like watering
    the plants. The proposed DL models have consistently demonstrated accuracies around
    96%‐98% along with several statistical parameters indicating satisfactory levels
    of reliabilities. The performances of the proposed approaches are also compared
    with previously reported works in the area of precision agriculture and ecological
    informatics. Also, the computation complexity of the proposed models has been
    analyzed to validate the ability of the approaches in solving complex problems
    with better response times. The proposed systems can be integrated to UAS such
    as drones to monitor large patches of agricultural land. Further, an impact analysis
    of the work has been carried out by certain on-field experiments and the outcomes
    are discussed with linkages to ecological and resource conservation. The rest
    of the paper is categorized as follows: In Section 2, certain aspects related
    to the materials used and methodology adopted as part of the work have been discussed.
    Design details are covered in Section 3. Experimental results and discussion are
    presented in Section 4. Section 5 concludes the manuscript. 2. Materials and methodology
    Here we provide the details of the materials used and methodology adopted as part
    of the work including a short discussion on related considerations. 2.1. Related
    considerations Fifteen varieties of beans (kidney beans, black beans, cranberry
    beans, chickpeas, lima beans, soyabean, red beans, mung beans etc.) are grown
    in India which requires tropical and subtropical higher temperatures (20 to 30
    degree centigrade), humidity for plant growth with fruiting during winter and
    takes around three months from germination. It supplies green pods through winter
    and spring in loam soil but grows well in alkaline and saline soils tolerating
    pH value up to 8.5. Normalized Difference Vegetation Index (NDVI) is used to quantify
    the health of the bean leaf. The NDVI is a measure to find the amount of green
    vegetation a land contains (Vido et al., 2020). It works on the principle that
    healthy vegetation reflects more of visible light compared to unhealthy vegetation.
    NDVI can be calculated using Eq. (1) (1) For continuous monitoring of plants,
    several sensors are required that are connected and operated in a uniform platform
    constituting an IoT. IoT is an arrangement of certain devices and sensors that
    collect, processes and transmits the data to another arrangement through the Internet
    or using any other network (Almadhor and Rauf, 2021). With an IoT arrangement,
    ML/DL components can be integrated for constituting an efficient decision making
    system. A simple arrangement of an IoT network connected with a DL based decision
    support system (DL-DSS) and process control is shown in Fig. 1. Download : Download
    high-res image (324KB) Download : Download full-size image Fig. 1. IoT based platform
    connected with DL-DSS and process control. Image segmentation is an important
    aspect of the work used to extract the ROIs from the samples. This work adopts
    K-Means clustering (KMC), Fuzzy C-means clustering (FCM) and Region Growing methods
    (Goodfellow et al., 2016) along with feature extraction to capture physiological
    attributes of the bean leaf samples. Feature extraction methods like GLCM, LBP
    and LBGLCM have been explored for studying the leaf samples (Ramesh et al., 2018).
    GLCM studies the spatial relationship among the pixels and is a second order statistical
    texture analysis method (Ramesh et al., 2018). A schematic of the feature extraction
    process using GLCM has been depicted in Fig. 2. Download : Download high-res image
    (261KB) Download : Download full-size image Fig. 2. Process of feature extraction
    necessary for the classifiers. Using GLCM, we have analyzed a set of features
    like energy, homogeneity, contrast and correlation which are useful in obtaining
    the details of the texture of the input images (Ramesh et al., 2018). The LBP
    features describe the statistical and structural model of an image. Further, the
    LBP and GLCM features are combined to generate the LBGLCM feature which is helpful
    in studying the composition of an image. Energy, Homogeneity, Contrast, Entropy
    and Correlation are some of the attributes that are captured by the combined LBP
    and GLCM feature set (Ramesh et al., 2018). Some of the benchmark classifiers
    used is CNN, SVM, MLP, TDNN, RF, etc. Further, we have used certain PTMs like
    Resnet152, InceptionV3, MobileNetV2 and proposed the EfficientNetB7 with BiLSTM
    and VGG16 with an attention mechanism that also have the abilities to carry out
    efficient disease detection, weed identification and process control. 2.2. Proposed
    methodology The block diagram of the complete work is summarized in Fig. 1. As
    already indicted above, the main constituent of the system is a DL block formed
    by two separate frameworks. The first one is formed using a CNN based EfficientNetB7
    with a BiLSTM network and the second one consists of a VGG16 with an attention
    layer. These two structures have been trained with fifteen varieties of bean leaves.
    A sizeable portion of the samples are obtained from the “Beans” database (Makerere
    AI Lab, January 2020) which is augmented with bootstrapping method. The performances
    of these networks are compared with that obtained from SVM, RF, KNN, MLP and TDNN
    classifiers. The performances of each of these classifiers are compared for ascertaining
    the most effective method for performing the stated objectives. Further, physiological
    attributes of the bean leaves are captured using features like GLCM, LBP and LBGLCM
    while extracted sections of the leaves in healthy and diseased forms are obtained
    using image segmentation. Fuzzy clustering is used for obtaining apriori labels
    for the classifiers. After these discrimination methods are trained, the best
    approach is determined from a series of validation cycles and then on-field testing
    is carried out with samples collected using NIR camera. The system accepts camera
    and sensor inputs to provide discriminator decision regarding plant health, weed
    growth and also triggers process controls activities like watering the plants
    at regular intervals of time. The proposed DL models have consistently demonstrated
    accuracies around 96%–98% along with several statistical parameters indicating
    satisfactory levels of accuracy. As already reported, a DL-DSS has been designed
    for real time monitoring of the health of the crops, weed detection and process
    control using AI and IoT as shown in Fig. 3. Download : Download high-res image
    (227KB) Download : Download full-size image Fig. 3. Expanded depiction of the
    proposed approach of precision agriculture. Leaf images of different bean species
    have been collected during visits to cultivation farms. A few of them are shown
    in Fig. 4. About 600 leaf images of different bean species have been captured
    by the NIR camera during visits to cultivation farms. Data bootstrapping techniques
    have been used to increase the number of training samples. This process enhances
    the learning efficiency of the DL network and other classifiers. Further, KMC,
    FMC and Region Growing methods have been used to extract and analyze the diseased
    region of the non-healthy bean leaves. Feature extraction has been performed using
    GLCM and LBP which captures the texture of an image through its pixel values.
    Features like energy, homogeneity, contrast, correlation etc. are obtained with
    the help of the GLCM matrix. These features are then used to study the physiological
    attributes of the leaf images. Classification is performed using ML classifiers
    like SVM, RF, KNN, MLP and TDNN along with the two proposed deep learning models
    EfficientNetB7-BiLSTM and VGG19 with attention for real time monitoring of the
    crop plants. Only the ML classifiers use the features extracted manually. In case
    of the DL models, the manually extracted features supplement the inbuilt feature
    extraction mechanism. Further, as the system is trained to discriminate between
    bean plants and weeds, it contributes towards demarcation of actual and wasteful
    growth. The IoT based framework continuously gathers data regarding the health
    of the plants and other environmental conditions. The collected information is
    sent to the cloud for storage in a database which can be easily accessed by the
    farmers through mobile app. Download : Download high-res image (356KB) Download
    : Download full-size image Fig. 4. Data augmentation performed to increase the
    number of training samples. The NDVI values are calculated for both healthy and
    non-healthy bean leaf samples. The NDVI value of a healthy leaf is found to be
    0.91 whereas of non-healthy i.e. diseased leaf, the value is 0.41. A schematic
    of this processing is shown in Fig. 5. Download : Download high-res image (309KB)
    Download : Download full-size image Fig. 5. Steps of leaf image processing and
    classification using ML classifiers. In the subsequent sections, the details of
    each of the proposed methods are presented. 3. Design details of the proposed
    models The main constituent of the system is a DL block formed by two separate
    frameworks. Each of these two frameworks is discussed in details. 3.1. Proposed
    DL Model I (EfficientNetB7 with a BiLSTM) This model employs an EfficientNetB7
    with a BiLSTM for performing operations like deep feature extraction, multi-class
    image classification using segments of bean leaf samples extracted from a frame,
    detect weed growth and also trigger process controls activities like watering
    the plants at regular intervals of time. This model has three modules: pre-processing,
    deep feature extraction and multi-class classification as shown in Fig. 6. Download
    : Download high-res image (263KB) Download : Download full-size image Fig. 6.
    Architecture of the proposed model I. 3.1.1. EfficientNet The EfficientNet is
    trained on the ImageNet database that contains about 15 million labeled images
    of different categories (Thi et al., 2022). This network uniformly scales the
    three most important parameters, the depth, width and resolution through a compound
    coefficient. The EfficientNet has been effectively used in image processing task
    as a SOTA technique (Thi et al., 2022). The number of layers in any network is
    determined by the requirements of a given complex problem. The EfficientNet has
    different baseline networks starting from B0 to B7. With a layer of 237 B0 baseline
    network has the lowest number of layers whereas the B7 network has the highest
    number of layers 837 (Thi et al., 2022). As already mentioned, the EfficientNet
    B7 captures the spatio attributes of the samples. 3.1.2. BiLSTM A BiLSTM model
    consists of two LSTMs: one taking the input in the forward path and the other
    in the backward flow (Yang et al., 2019) while facilitating processing in both
    directions. LSTM is a kind of Recurrent Neural Network (RNN) where the output
    from the last step is fed as input to the current step. LSTMs are much preferred
    over RNN due to their ability to grasp the vanishing gradient problem and process
    samples with temporal attributes (Goodfellowet al. 2016; Yang et al., 2019). The
    LSTM has a structure containing four learning layers and different memory blocks
    called cells. The structure consists of three gates namely: forget gate, input
    gate and output gate. Each gate performs its unique function. The inputs are fed
    to the network through the input gate which plays the role of adding useful information
    using appropriate function and preserving the relevant information. The forget
    gate discards all the unwanted information and finally, the output gate retains
    all the relevant information from the current cell state, which is then send to
    the next cell after point wise multiplication. This way the input flows in one
    direction only in a LSTM network. This process is not always suitable for certain
    classification tasks which require both forward and backward contextual relationship
    for efficiency (Goodfellow et al., 2016; Yang et al., 2019). Thus, to overcome
    this limitation, a single layer stack of BiLSTM is used in our work which consists
    of two distinct hidden layers to model the input in both the forward and backward
    direction. The BiLSTM layer deals with the temporal attributes of the samples,
    retains and circulates the contextual content throughout the network. The output
    of the BiLSTM, then passes through the fully connected (FC) layers and finally
    moves to the softmax classifier at the output layer. 3.1.3. Working of the proposed
    Model I The input images are first normalized and resized to a fixed dimension
    of 224 × 224 × 3 to reduce computation complexity. These input images are then
    fed to the EfficientNetB7 where each of the input images is processed through
    a stack of 813 layers followed by a BiLSTM layer. The EfficientNetB7 then generates
    an output of feature vectors of 7 × 7 × 2560 dimensions for each image. These
    feature vectors are fed as input to the BiLSTM network to execute the training
    and extract the spatio-temporal attributes of the samples. Thereafter, the flattening
    layer is added to convert the vectors to a 1-dimensional vector. A dropout layer
    is added immediately after the flattening layer with a dropout rate of 30% to
    prevent the model from over fitting. While we have experimented with various dropout
    rates such as 10%, 20%, 30%, 40%, efficient results have been obtained with a
    dropout of 30% (value = 0.3). The speed-up of the computation of the EfficientNetB7
    is attributed to the optimized structure and learning achieved using the dynamic
    architectural trimming which is facilitated by this dropout mechanism. The next
    layer is the dense layer consisting of 4096 neurons with Rectified Linear Activation
    (ReLU). Finally, the network has the softmax layer with three neurons representing
    three different classes (angular leaf spot, bean rust and healthy). The training
    is supervised and is carried out using the apriori labels obtained from the segmented
    ROI''s of the bean leaves. The configuration of the proposed model is provided
    in the Table 1. Table 1. Details of network configuration and parameters of the
    Proposed Model I. Layer Type Size Layer 1 Input Layer 500 × 500 × 3 Layer 2 Resize
    Layer 224 × 224 × 3 Layer 3 EfficientNetB7 Layer 7 × 7 × 2560 Layer 4 Bidirectional
    LSTM 256 Layer 5 Flatten 5632 Layer 6 Dropout layer (value = 0.3) 4096 Layer 7
    Fully Connected (Dense) 4096 Layer 8 Softmax Layer 3 Total Parameters: 215765402
    Trainable Parameters: 64097687 Non- Trainable Parameters: 151667715 3.2. Proposed
    Model II (VGG16 with an attention layer) The second proposed model is based on
    a pre-trained CNN named VGG16 integrated with an attention mechanism. The attention
    mechanism has been integrated at each stage of the network for learning of the
    class features with greater focus so as to enhance the feature awareness and ensure
    spatio-temporal extraction of the relevant details. The schematic is shown in
    Fig. 7. Download : Download high-res image (305KB) Download : Download full-size
    image Fig. 7. Architecture of the Proposed Model II. 3.2.1. VGG16 The VGG16 with
    16 layers is trained on the ImageNet dataset (over 15 million images of 1000 different
    classes) (Ashish et al., 2020). The architecture of VGG16 consists of small convolution
    filters with each hidden layers using ReLU activation function. Finally, the network
    works with three fully connected layers where the first two layers have 4096 channels
    and the third layer has 1000 channels for each class. In our work, the VGG16 has
    been integrated with multiple attention layers each reinforcing the class details
    allowing effective training to take place. The attention layer is required to
    ensure precise focus on specific parts of a sequence when processing a large assortment
    of data. In DL, the use of attention mechanism helps the network in remembering
    long sequences of data for a longer period of time (Goodfellow et al., 2016).
    The illustration of the attention mechanism is shown in Fig. 8. Download : Download
    high-res image (321KB) Download : Download full-size image Fig. 8. Illustration
    of the attention mechanism. In Fig. 5 [X] is the input, [Y] is the output, Yt
    is the target (apriori ROI segmented out of the leaf samples) and μ1, μ2 …μn are
    the alignment scores. There are two rows of LSTM cells which are grouped with
    processing sequence going from the forward to the backward direction and vice-versa.
    The output of these two channels of LSTM cells are combined and weighted with
    alignment scores which are then compared with the target vector Yt. The difference
    or error vector drives the learning process of the attention mechanism and a gradient
    descent process is updated till the goals are met. 3.2.2. Implementation of the
    proposed Model II Similar to the proposed Model I, here also the input RGB images
    are normalized to reduce the computational complexity of the system and then resized
    to a fixed dimension of 224 × 224 × 3. Next, these pre-processed images are passed
    through the usual layers of the VGG16 network having 13 convolution layers and
    three fully connected layers. At the indicated stages, the attention mechanisms
    are implemented for detailed learning of the class features and embedding these
    into the training process of the network. This way identical numbers of attention
    layers for the three classes are implemented. Finally, the classifier section
    is put in place which is constituted by the softmax activation function with three
    neurons. The configuration of the proposed model is shown in Table 2. The softmax
    activation function is defined as follows: (2) Table 2. Details of network configuration
    and parameters of the Proposed Model II. Convolution Block Type Size Input Resize
    Image 224 × 224 × 3 Block 1 Convolution Convolution Pooling conv3–64 conv3–64
    Block 2 Convolution Convolution Pooling conv3–128 conv3–128 Block 3 Convolution
    Convolution Convolution Pooling conv3‐256 conv3–256 conv3––256 Block 4 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Block 5 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Fully Connected
    Layers Layer 1 Layer2 Layer3 4096 4096 1000 Output Layer Softmax 3 Total Parameters:
    138,357,544 Trainable Parameters: 138, 357, 544 Non-Trainable Parameters: 0 The
    learning schedule is extended to a few more classifiers which are used for performance
    benchmarking. 3.3. Experimental environment and computation The proposed models
    have been implemented in Python using Keras application programming interface
    (API) that runs on top of Google''s Tensor Flow open source library (Geron, 2019).
    The Google Colab integrated development environment (IDE) is used for writing
    and implementing the Python codes for the proposed deep learning models. A computer
    system with Intel i7 processor and 16 GB RAM has been used as the host setup.
    3.4. Design of an IoT based framework for on-field testing The proposed framework
    comprises of temperature, humidity and soil moisture sensors for gathering the
    required information contributing to the productivity of the farming land. These
    sensors are then directly connected with the Arduino UNO board. An application
    has been designed using the Blync app that sends the collected information to
    the cloud stage for further investigation. Further, we have used ThingSpeak platform
    for data presentation and analysis for visualization of real data captured using
    the IoT platform. Schematic of the proposed framework is depicted in Fig. 9. Download
    : Download high-res image (117KB) Download : Download full-size image Fig. 9.
    IoT framework. The sensors are connected to the Arduino board and the output is
    directly forwarded to the server, which is then saved automatically in the database
    of the app. The DHT11 sensor is used to obtain the temperature and humidity values.
    Moreover, since our proposed system is designed to work in hot and humid conditions,
    this sensor is a proper choice to obtain the temperature details. The soil moisture
    sensor helps in moisture sensing by measuring the water content in the soil. The
    sensor has advantages like anti-rusting property and has a long power life. The
    block diagram of the proposed IoT framework for an intelligent farming system
    is shown in Fig. 10. Download : Download high-res image (315KB) Download : Download
    full-size image Fig. 10. Block diagram showing the connections of the different
    sensors forming the IoT set-up. The Arduino UNO collects the real-time data from
    different sensors and integrates them enabling automated decision making and control
    of the motors driving the processes whenever required. LEDs are used to indicate
    the working status of the sensors. The red LED placed near the sensor indicates
    any technical fault in the system whilethe green LED indicates the working condition
    of the system. The data gathered from the sensors is sent to the ThingSpeak platform
    and the mobile app. The flowchart of the proposed system is shown in Fig. 11.
    For testing our proposed system, we visited a few bean cultivation farms and gathered
    leaf samples of fifteen different bean species found in India like kidney beans,
    black beans, cranberry beans, chickpeas, lima beans, soyabean, red beans, mung
    beans etc. It was observed that our proposed systems can efficiently discriminate
    between healthy and non-healthy bean leaves as well as accurately detect common
    bean diseases like bean rust and angular leaf spot in the leaves. Further, the
    system is also able to predict environmental parameters and identify land areas
    having weeds and cultivable land. Download : Download high-res image (436KB) Download
    : Download full-size image Fig. 11. Flowchart highlighting the working of the
    real-time monitoring and process control system. 3.5. DSS for weed control The
    proposed system is trained to detect weed and bean crop. Initially we took samples
    of the weed and the bean cultivation land and labeled them. The labeled samples
    are used to train the classifier (VGG-16) to identify the weed area and crop section.
    This ability of the classifier will help the farmer to mark the region of the
    plot of land where weed removal measures can be taken. It has been recorded that
    at least 37 types of weeds grow around fifteen types of bean cultivations in India.
    Weeds suck the nutrients of the soil making it infertile. Hence, proper weed detection
    is a crucial attribute of the proposed system. It contributes towards lowering
    of accidental mechanical cutting of bean plants assuming to be weed growth. Further,
    with proper identification of wild growth, control spreading of weed removing
    chemicals can be carried out which protects the fertility of the land and also
    saves the bean leaves from damage. 3.6. Fertilizer and disinfectant spreading
    The IoT-DSS has been programmed to demonstrate the spreading of fertilizer and
    disinfectant at multiple locations where and when required. This process is expected
    to assist the farmers as a measure to pest control for healthy cultivation. 4.
    Experimental results A series of experiments have been performed to validate the
    work and check the reliability of our proposed system. The experimental parameters
    are summarized in Table 3. Initial steps like noise removal, normalization, resizing
    have been performed as a part of the pre-processing procedure. After pre-processing,
    feature extraction using GLCM, LBP and LBGLCM has been carried out. Features like
    homogeneity, energy, contrast, correlation and entropy are considered to analyze
    the texture of the processed images. The GLCM analysis the statistical measures
    from the images. Different classifiers like SVM, TDNN, RF, MLP and KNN have been
    used for image classification. Further, we created a graphical user interface
    (GUI) for carrying the required experiments. After image pre-processing, as shown
    in Fig. 12, segmentation has been performed using FCM and KMC techniques. Two
    important parameters namely Intersection over Union (IOU) (Eq. (3)) and Pixel
    Accuracy (PA) (Eq. (4)) are utilized to establish the reliability of our system
    spatial domain. IOU is an important measurement that indicates how accurately
    the system performs while pixel accuracy is calculated to observe the number of
    correctly classified pixels. Further, the segmentation accuracy (Acc) is also
    calculated through pixel to pixel match. Table 4 shows the results (IoU, Acc and
    PA) obtained from FMC, KMC and Region Growing segmentation methods when extracting
    ROIs using SVM, RF, KNN, MLP and TDNN. It can be clearly observed that FMC-TDNN
    is the most reliable combination in terms of Acc, IOU and PA. (3) (4) Table 3.
    Experimental Parameters. Description Type Pre-processing Noise removal, normalization,
    resizing Segmentation FCM, KMC, Region Growing Classifiers SVM,RF,MLP,TDNN and
    KNN Parameters Bean leaf images, NDVI, temperature, humidity, soil moistureand
    air quality Data Used Training set of 10,000 mixed bean leaf images Download :
    Download high-res image (294KB) Download : Download full-size image Fig. 12. Operations
    of pre-processing on the (a) diseased leaf, (b) segmentation, (c) edge detection
    and (d), (e), (f) selection of region of interest (ROI). Table 4. Summary of IOU,
    Acc and PA obtained using FMC, KMC and Region Growing clustering in percentage
    (%). Method SVM RF KNN MLP TDNN Acc IOU PA Acc IOU PA Acc IOU PA Acc IOU PA Acc
    IOU PA FMC 84 82 83 84 84 85 83 82 83 87 82 83 92 89 90 KMC 84 81 82 83 83 84
    83 81 83 86 81 83 93 88 89 Region Growing 83 81 82 83 82 82 83 82 82 84 82 81
    89 86 87 Table 5 shows the segmentation accuracy achieved using three different
    feature sets. It can be observed that the GLCM-SVM and GLCM-TDNN gives the best
    results but as TDNN is trainable and robust under varied conditions, the GLCM-TDNN
    combination is used for benchmarking. Table 5. Summary of the segmentation accuracy
    obtained from different feature sets. Method SVM (%) RF(%) KNN(%) MLP(%) TDNN(%)
    GLCM 84 84 83 87 89 LBP 83 84 82 88 88 LBGLCM 83 83 81 89 90 From the experiments,
    a few statistical parameters are obtained and the results are summarized in Table
    6. The results have been derived using GLCM features and FCM segmentation techniques.
    It has been observed that while the learning based methods provide reliability
    still the performance cannot be extended beyond certain limits. Further, with
    diversity in the content, the reliability suffers, which clearly indicates the
    limitations of ML based approaches. Hence, a set of experiments have been carried
    out using DLbased methods. Table 6. Summary of Performance Metrics of Different
    Classifiers. Model CA Precision Recall Specificity Type I Error Type II Error
    F1 Score Micro Avg Macro Avg TDNN 88% 0.88 0.87 0.89 0.17 0.20 0.86 0.88 0.88
    MLP 86% 0.86 0.85 0.88 0.19 0.22 0.86 0.86 0.86 KNN 76% 0.76 0.75 0.80 0.24 0.26
    0.76 0.76 0.75 RF 78% 0.78 0.78 0.84 0.22 0.21 0.78 0.78 0.78 SVM 79% 0.79 0.79
    0.82 0.21 0.22 0.78 0.79 0.79 Table.7 summarizes the results obtained using existing
    PTMs. It can be observed that MobileNetV2 has higher classification accuracy(CA)
    in comparison with other PTMs InceptionV3, ResNet152 etc. Table 7. Performance
    Metrics using some existing PTMs and proposed models. Models CA Precision Recall
    Specificity Type I Error Type II Error F1 Score Micro Avg Macro Avg ResNet152
    92% 0.92 0.91 0.94 0.02 0.22 0.92 0.91 0.92 InceptionV3 90% 0.90 0.90 0.92 0.05
    0.24 0.90 0.89 0.90 MobileNetV2 93% 0.93 0.93 0.93 0.06 0.36 0.93 0.93 0.92 Proposed
    Model I 96% 0.96 0.95 0.95 0.04 0.08 0.96 0.95 0.95 Proposed Model II 98% 0.98
    0.97 0.96 0.04 0.09 0.98 0.97 0.98 Fig. 13 shows the accuracy achieved during
    network training and model loss suffered by the Proposed Model I (EfficientNetB7
    with BiLSTM). The performance has been achieved in 100 epochs which indicates
    the framework is computationally efficient in completing the training in less
    amount of time. It has been observed that initially the training starts with some
    fluctuations but reaches satisfactory performance levels without taking high numbers
    of epochs. During the first 20 epochs, the accuracy of the model with both train
    and test data reaches the mid 80''s and 90''s in percentage range. This clearly
    indicates the proposed model EfficientNetB7 with BiLSTM has capacity of fast processing.
    The epochs are further extended to see where the model''s performance saturates.
    It was observed that beyond 20 epochs, the performance shows no major improvement.
    This plot is obtained from average values of hundred trials using training and
    testing data. Download : Download high-res image (256KB) Download : Download full-size
    image Fig. 13. (a) Training Accuracy and (b) Model Loss graphs of Proposed Model
    I (EfficientNetB7 with BiLSTM). Another set of graphs has been obtained for the
    Proposed Model II (VGG16 with attention layer) when subjecting it to a series
    of training cycles. The average performance derived in terms of accuracy and model
    loss for 100 epochs is shown in Fig. 14. It can be observed clearly that the accuracy
    values attain stable state beyond 5 epochs. The accuracy performance reach mid
    80''s and crosses into the 90''s percentage range beyond 10 epochs. Same is the
    case with the model loss graph. The model loss saturates beyond 20 epochs. Download
    : Download high-res image (211KB) Download : Download full-size image Fig. 14.
    (a) Training Accuracy and (b) Model Loss graphs of Proposed Model II (VGG16 with
    attention layer). Table.8shows the computation load associated with our proposed
    models and the PTMs that are used as benchmark methods. The performance associated
    with 100 epoch cycles has been presented in Table 8. It has been found that EfficientNetB7
    with BiLSTM takes about 1100 s to complete with 11 s per epoch, which is over
    35% better compared to the VGG16 with attention layer based approach. Compared
    to Resnet152, the proposed model I is computationally two times less demanding
    and compared to MobileNetV2 and InceptionV3 it is about 2.18 and 2.54 times more
    efficient respectively. This clearly indicates the advantage of the EfficientNetB7
    based method. The specific model parameters of both the approaches are shown in
    Table 9. With learning rate of 0.001, Adam optimizer, 30% dropout rate and batch
    size of 32, categorical cross entropy used as the loss function to run training
    cycles of 100 epochs, the proposed approaches appear reliable, robust and efficient.
    Table.10 shows the comparison of our proposed models with existing SOTA models
    using leaf samples of different vegetables. Here also we can clearly observe that
    our proposed approaches performed fairly well in comparison with other methods.
    Table 8. Computation Time of the Proposed Models and other PTM. Model Epochs Computation
    Time (sec) Computation time per step (msec) Proposed Model I (EfficientNetB7 with
    BiLSTM) 100 11 s per epoch 72 ms Proposed Model II (VGG16 with attention) 100
    17 s per epoch 88 ms ResNet152 100 22 s per epoch 122 ms MobileNetV2 100 24 s
    per epoch 142 ms InceptionV3 100 28 s per epoch 288 ms Table 9. Model Parameters:
    Train-Test Split ratio: 80:20 Learning rate: 0.001 Choice of Optimizer: Adam Loss
    Function: Categorical Cross Entropy Dropout rate: 30%, 40%, 50% Epochs: 100 Batch
    size: 32 Table 10. Comparison of our proposed models with SOTA models applied
    in precision agriculture of several crops with leaf taken as input. Model Leaf
    Classification accuracy (CA) f1-score EfficientNetB7 with KNN, RF (Chug et al.,
    2022) Tomato 88% 0.87 Inception,ResnetV2(Singh et al., 2020) Multiple crops 70%
    0.70 InceptionV1,ResNet50 (Xie et al., 2020) Grape 78% 0.79 ResNet50 (Veeraballi,
    2020) Papaya 85% – Naïve''s Bayes (Sahoo et al., 2020) Maize 77% – Proposed Model
    I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II (VGG16 with attention)
    Bean 98% 0.98 Table.11 shows the comparison of our proposed models with existing
    SOTA techniques using only bean leaves. It can be clearly observed that our proposed
    approaches demonstrate 1–7% accuracy improvements compared with the other existing
    methods. Same is the case with F1-score. The confusion matrix obtained from the
    original Beans dataset using proposed model I and II are shown in Fig. 15. Table
    11. Comparison of our proposed models with SOTA models with Beans dataset. Model
    Leaf CA f1-score VirLeafNet1(Rakesh et al., 2021) Bean 91.23% – AlexNet, GoogleNet
    (Chen et al., 2020) Bean 94% 0.93 CNN(Himadriet al., 2022) Bean 93% – GoogleNet
    (Amit et al., 2021) Bean 95% 0.95 MobileNetV2(Recep and Lahcen, 2022) Bean 92%
    – Proposed Model I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II
    (VGG16 with attention) Bean 98% 0.98 Download : Download high-res image (84KB)
    Download : Download full-size image Fig. 15. Confusion Matrix of (a) proposed
    model I and (b) proposed II. Proposed model I is able to classify diseased and
    healthy leaves efficiently with an accuracy of 96% with an F1-score of 0.96 and
    proposed model II could successfully discriminate between diseased and healthy
    leaves with an accuracy of 98% with an F1-score of 0.98 respectively. Further,
    the two proposed approaches perform better than a number of PTMs like ResNet152,
    MobileNetV2 and InceptionV3 as shown in Table.7. 4.1. Performance comparison with
    existing SOTA models The performance of the proposed models has been compared
    with the existing SOTA models to understand how well our system performed in accurate
    detection of plant diseases. Comparison of performance of our proposed approaches
    has been shown in Table.10. Table.11 summarizes the comparison results of the
    existing models in literature employed for image based plant disease detection
    with our proposed models using only Bean leaves. The performance of the system
    designed using the EfficientNetB7 with BiLSTM and VGG16 with an integrated attention
    mechanism have been compared with that obtained using SVM, RF, KNN, MLP and TDNN.
    Subsequently, the trained network has been tested on real field samples collected
    using an IoT based approach which also monitors temperature, humidity and soil
    moisture. Further, experiments have also been performed using some existing CNNs
    like ResNet152, MobileNetV2 and InceptionV3. Results show that our proposed DL
    models could classify healthy and diseased bean leaves with accuracy of 96% and
    98%respectively using less computation time. Further, our proposed models outperform
    some existing models in literature like VirLeafNet (Rakesh et al., 2021), DADCNN-5
    (Akshay and Kamal, 2022) and R-CNN (Zan et al., 2022) that have been employed
    for classification and identification of plant leaf diseases and are related to
    ecological informatics. Furthermore, the average processing time of a single bean
    leaf image is 0.011 s demonstrated by our proposed approach which is less than
    the processing time utilized by some existing models for plant disease detection
    (Rakesh et al., 2021) and (Ramesh et al., 2018). 4.2. Impact analysis and discussion
    The impact analysis of the proposed approach is presented in terms of reliability
    of identification of bean leaf health, weed detection and water sprinkling compared
    to the tasks executed by a few human volunteers (of three different experience
    categories). An area of 20 ft × 20 ft with about 400 bean plants nurtured in certain
    number of rows is considered for the study. Each row has a sensor pack consisting
    of air quality sensor (MQ135), temperature and humidity sensor (DHT11) and soil
    moisture sensor connected to a few Arduino UNO boards which are linked up to a
    host computer using Wi-Fi access. Further, a NIR camera with a Wi-Fi module is
    placed over a slider arrangement laid at a height of 5 ft that can be mechanically
    moved over each row. This arrangement (sensor pack and the NIR camera) is used
    as the data capture block to feed samples to the two proposed approaches and subjected
    to performance evaluation for identification of bean leaf health, weed detection,
    ascertaining air quality and soil moisture condition and initiation of water sprinkling.
    The above performances are compared with that obtained using human volunteers
    (of three different experience categories). The first category is a volunteer
    with about one year bean cultivation experience with knowledge of the requirements
    but with temporary involvement with the effort. The second and third categories
    of volunteers have adequate knowhow about bean cultivation, have been involved
    continuously with the process for over two years and are regularly associated.
    The data have been compiled over a period of seven days and is summarized in Table.12.
    Table 12. Summary results of performance evaluation of the proposed system in
    terms of accuracy compared to human observers. Sl no. Method Accuracy in % Bean
    leaf health identification Weed detection Air quality assessment Soil moisture
    assessment Water sprinkling 1 Proposed approach 1 95 95 94 94 94 2 Proposed approach
    2 96 96 95 95 96 3 MobileNetV2 (Recep and Lahcen, 2022) 92 93 92 92 92 4 GoogleNet
    (Amit et al., 2021) 94 93 93 93 93 5 Human with 1 year experience 58 52 50 55
    61 6 Human with 2 years'' experience 63 65 62 64 63 7 Human with 5 years'' experience
    80 79 71 77 79 During this period, the observation accuracies notched by the proposed
    approaches and three persons of different bean cultivation experiences in case
    of bean leaf health identification, weed detection, air quality assessment, soil
    moisture estimation and water sprinkling have been recorded. A factor that appears
    to be obvious is the fact that continuous monitoring of agricultural produce is
    strength of automation frameworks and is not convenient for human beings. While
    bean leaf health identification and weed detection at microscopic level can be
    meticulously carried out by the proposed sensor pack integrated to the AI aided
    decision support system, the same turns out to be highly repetitive and tedious
    tasks for a human where errors are likely to take place. This is established by
    the fact that human errors are between 37% and 15% when compared with the proposed
    approach 1 while it is between 38% and 16% for the proposed approach 2. The most
    experience human volunteer notches up average accuracies of around 80% which is
    at least 15% less than the AI based methods. With lower errors in monitoring and
    timely human intervention, the possibility of rise in productivity and decrease
    in financial involvement are the logical spinoffs despite the fact that an automated
    system is likely to suffer breakdown at times and are constrained by availability
    of uninterrupted electricity supply, hermetically tight packaging and flexible
    deployment to prevent damage of the components due to environment factors etc.
    While bean leaf health identification is crucial for better productivity of a
    plot of land, more important is early detection of the disease affecting the plants
    and timely intervention. An early detection is possible by the application of
    the proposed approaches and timely measures to reduce the loss can be initiated.
    The monitoring and detection process can be executed in a continuous manner with
    very little human involvement except while initiating the measures to use medicines
    to reduce the leaf diseases. Similarly, weed growth can be a major challenge for
    the cultivators during the initial and subsequent periods as it perennially threatens
    to encroach on the resources earmarked for the bean farming Togacar, 2022). Weeds
    are unwanted growth that invades the cultivation land and damage the food crops.
    Moreover, accurate detection of weed and demarcation of such areas can lower contamination
    due to unregulated spraying of chemicals for weed control and prevent damage to
    the fertile land. Further, precise demarcation of weed filled areas can also help
    the farmers to use mechanical means of weed cutting which can prevent damage to
    the bean cultivation and the fertile land due to use of chemicals for removal
    of the unwanted vegetation growth. This is pertinent due to the fact that over
    37 different types of weeds like pig weed, crab grass, goose grass etc. are observed
    around bean cultivation areas in India. Weeds always threaten to deprive the bean
    crops from adequate amount of sunlight, nutrients, water (Togacar, 2022). Moreover,
    many weeds are also host of plant disease organisms (Gokulnath and Usha, 2021).
    Hence an automated approach for weed detection is an essential tool for the farmers.
    With an automated approach of bean leaf health identification and weed detection,
    the volume of irrelevant manpower required can be reduced which increases the
    cost effectiveness of the overall effort. In addition to the above, the execution
    time is short and the decision making of the proposed approaches is fast and reliable
    which is not possible in all cases to be matched by personnel employed for the
    purpose. The sensor pack provides precise thresholds at which water sprinkling
    could be initiated. It helps in preventing water wastage as well. The accurate
    timings and amount of water sprinkling might be not always maintained with human
    involvement. In view of the above, the role of an AI assisted agriculture system
    configured for bean cultivation working in complementary supplementary roles to
    the human cultivator is widespread and its impact on the cultivation process and
    the surrounding ecology shall be far-reaching. 5. Conclusion The proposed system
    is a combination of IoT devices, image processing techniques and ML/DL based systems
    applied as part of a precision agriculture setup related to several varieties
    of bean species found in India. In this work, we have proposed two DL models for
    real time detection of healthy and non-healthy (diseased) bean leaves, weeds growing
    around the cultivation land and monitoring of related environment parameters and
    controlled sprinkling of water. The first approach uses EfficientNetB7 along with
    a BiLSTM layer and the second approach employs VGG16 with an integrated attention
    mechanism. Further, experiments have been carried using SVM, RF, KNN, MLP and
    TDNN using features GLCM, LBP and LBGLCM to capture physiological attributes of
    the bean leaf samples which in combination with different segmentation methods
    separates the diseased areas of the leaves. These are then used as apriori labels
    for the classifiers to reinforce the previously known details of the bean varieties.
    Subsequently, the trained network is tested using samples collected during visits
    to bean cultivation farms. Moreover, experiments have also been performed using
    some existing CNNs like ResNet152, MobileNetV2 and InceptionV3. The proposed methods
    have been compared with existing SOTA techniques and it has been observed that
    our proposed DL models could classify healthy and diseased bean leaves with accuracy
    1–7% better than the existing methods. Our proposed models I and II consistently
    demonstrated classification accuracies of 96% and 98% respectively. The computational
    framework of the proposed models has also been analyzed and we observed that the
    computation time of models I and II have been 11 s and 17sper epoch respectively.
    Also, the processing time of a single bean leaf image is 0.011 s. Compared to
    other PTMs, our proposed models are found to be computationally less demanding.
    This clearly highlights the computational efficiency of our proposed models. These
    systems can be integrated to UAVs for extensive crop monitoring in large patches
    of agricultural land in less amount of time. The key novelty of the work is an
    AI aided accurate and efficient decision support mechanism that reliably identifies
    bean leaf disease, recognizes weed growth with proper demarcation of the area
    under bean cultivation, continuous monitoring of the ambient conditions and air
    which triggers regulated water sprinkling. With continuous and automated monitoring
    of the health state of the bean leaves, the farmer obtains considerable support
    to enhance productivity. The weed monitoring helps to prevent encroachment of
    nutrients by wild growth and ensures better output from the bean cultivation.
    With weed area demarcation, use of mechanical means to remove the wild grass prevents
    chemical contamination of the fertile land. Controlled sprinkling of water prevents
    waste of a precious commodity like water. Further, the continuous monitoring of
    the air enables the farmers to visualize the best ambiance for seed germination
    and growth. In view of the above, the proposed system is expected to have decisive
    impact not only in assisting the farmer to enhance productivity but also to contribute
    towards ecological preservation. Ethics approval Not Applicable. Consent to participate
    All the authors approved to participate in this research. Consent for publication
    All the authors approved the publication of this research. Availability of data
    The authors do not have the permission to share the data. Grants and funding The
    authors did not receive any funds or grants from any organization. Author''s contribution
    Nilakshi Devi- experimental work, manuscript preparation, result generation and
    analysis; Kandarpa Kumar Sarma- conceptualization, editing of manuscript, supervision
    of experimental work and analysis; ShakuntalaLashkar- Supervision and analysis.
    Declaration of Competing Interest The authors declared that they have no competing
    interest. Acknowledgements The authors would like to thank Nayan Bean Farm, Sonapur,
    Guwahati, Assam for providing the bean leaf samples. Data availability The authors
    do not have permission to share data. References Akram et al., 2017 T. Akram,
    Sayeed Rameez, Mohammad Kmaran Towards real time crops surveillance for disease
    classification: exploiting parallelism in computer vision Comput. Electr. Eng.,
    59 (2017), pp. 15-26 View PDFView articleView in ScopusGoogle Scholar Akshay Pandey
    and Kamal Jain, 2021 Akshay Pandey, Kamal Jain A robust deep attention dense convolutional
    neural network for plant leaf disease identification and classification from smart
    phone captured real world images Ecological Informatics.vol.70 (2021) Google Scholar
    Almadhor and Rauf, 2021 A. Almadhor, H.T. Rauf AI-driven framework for recognition
    of guava plant diseases through machine learning from DSLR camera sensor based
    high resolution imagery Sensors., 21 (2021), p. 3830 CrossRefView in ScopusGoogle
    Scholar Amit et al., 2021 Prakash Amit, Sahu Priyanka, Singh Dinesh Deep learning
    models for beans crop diseases: classification and visualization techniques Int.
    J. Modern Agric., 10 (2021) Google Scholar Ashish, 2020 Kumar Ashish, et al. Res-VGG:
    A novel model for plant disease detection by fusing VGG16 and ResNet models International
    Conference on Machine Learning, Image Processing, Network Security and Data Science
    (2020), pp. 383-400 Google Scholar Asmita Hobisiyashi, 2022 Asmita Hobisiyashi
    ShivamYadav Cloud Based IoT controlled System Model for Plant Disease Monitoring.Predictive
    Analytics in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Bhagat
    et al., 2022 Sandesh Bhagat, et al. Eff-UNet++: a novel architecture for plant
    leaf segmentation and counting Ecol. Inform., 68 (2022) Google Scholar Binch and
    Fox, 2017 A. Binch, C.W. Fox Controlled comparison of machine vision algorithms
    for Rumex and Urtica detection in grassland Comput. Electron. Agric., 140 (2017),
    pp. 123-138 View PDFView articleView in ScopusGoogle Scholar CABI, 2022 CABI https://www.cabi.org/
    (2022) (Last checked on June 2022) Google Scholar Chen et al., 2020 Junde Chen,
    Defu Zhang, Jinxiu Chen Using deep transfer learning for image based plant disease
    identification Comput. Electron. Agric., 173 (2020) Google Scholar Chug et al.,
    2022 Anuradha Chug, Anshul Bhatia, Amit Prakash, Dinesh Singh A novel framework
    for image-based plant disease detection using hybrid deep learning approach J.
    Soft Comput. (2022), pp. 234-242 Springer Google Scholar Das et al., 2022 Amit
    Das, Himadri Saha, Amlan Chakrabarti Deep learning based automated disease detection
    and pests classification in Indian mung bean MultiMedia Tools and Applications,
    Springer (2022) Google Scholar Ferentinos, 2018 K.P. Ferentinos Deep learning
    models for plant disease detection and diagnosis Comput. Electron. Agric., 114
    (2018), pp. 311-318 View PDFView articleView in ScopusGoogle Scholar Geron, 2019
    Aurelin Geron Hands-on Machine Learning with Scikit-Learn, Keras and Tensor Flow
    O’ReillyPublisher (2019), pp. 234-345 Google Scholar Gokulnath and Usha, 2021
    B.V. Gokulnath, Devi Usha Identifying and classifying plant disease using resilient
    LF-CNN Ecol. Inform., 63, Elsevier (2021) Google Scholar Goodfellow et al., 2016
    Ian Goodfellow, Yoshua Bengio, Aaron Courville Deep Learning MIT Press (2016)
    Google Scholar Hapsari et al., 2022 Rinci Kembang Hapsari, Gan Hong Seng, Miswanto
    Miswanto Modified Gray Level Haralick Texture Features For Early Detection of
    Diabetes Mellitus and High Cholestrol in Iris Image vol. 2 (2022), p. 11 Google
    Scholar Hobisiyashi and Yadav, 2022 Asmita Hobisiyashi, Shivam Yadav Cloud based
    IoT controlled system model for plant disease monitoring Predictive Analytics
    in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Jha et al., 2019
    K. Jha, A. Doshi, P. Patel, M. Shah A comprehensive review on automation in agriculture
    using artificial intelligence Artif. Intelligence Agric., 2 (2019), pp. 1-12 June
    View PDFView articleView in ScopusGoogle Scholar Johann, 2016 Andre L. Johann,
    et al. Soil moisture modeling based on stochastic behavior of forces on a no-till
    chisel opener Comput. Electron. Agric., 121 (2016), pp. 420-428 View PDFView articleView
    in ScopusGoogle Scholar Johannes et al., 2017 A. Johannes, et al. Automatic plant
    disease diagnosis using mobile capture devices applied on a wheat use case Comput.
    Electron. Agric. (2017), pp. 200-209 View PDFView articleView in ScopusGoogle
    Scholar Kashefi et al., 2017 Javed Kashefi, et al. Novelty Detection Classifiers
    in Weed Mapping: Silybummarianum Detection on UAV Multispectral Images vol. 17
    (2017) Google Scholar Kaya et al., 2023 Yasin Kaya, et al. A novel multi-head
    CNN design to identify plant diseases using the fusion of RGB images Ecol. Inform.,
    75 (2023) Google Scholar Maione et al., 2018 Camila Maione, et al. Recent applications
    of multivariate data analysis methods in the authentication of rice and the most
    analyzed parameters: a review Taylor &Francis (2018), pp. 1868-1879 Online Google
    Scholar Manjula et al., 2022 Manjula, et al. Plant disease detection using deep
    learning Lecture Notes in Electrical Engineering, Springer (2022), pp. 1389-1396
    CrossRefView in ScopusGoogle Scholar Naik et al., 2022 Naik, et al. Detection
    and classification of chilli leaf disease using a squeeze-and-excitation-based
    CNN model Ecol. Inform., 69 (2022) Google Scholar Nicola and Pisana, 2021 Papini
    Nicola, Placidi Pisana Monitoring Soil and Ambient Parameters in IoT Precision
    Agriculture Sceneriao: An Original Modelling Approach Dedicated to Low Cost Water
    Content Sensors vol. 21 (2021), p. 5110 Google Scholar Olivia et al., 2022 Diego
    Olivia, et al. BLeafNet: a Bonferroni mean operator based fusion of CNN models
    for plant identification using leaf image classification Ecol. Inform., 69 (2022)
    Google Scholar Pandey, 2022 A. Pandey, et al. An intelligent system for crop identification
    and classification from UAV images using conjugated dense convolutional neural
    network Comput. Electron. Agric. (2022), p. 567 Google Scholar Pantazi et al.,
    2019 X.E. Pantazi, et al. Automated Leaf Disease Detection in Different Crop Species
    through Image Features Analysis and One Class Classifiers Computers and Electronics
    in Agriculture (2019) Google Scholar Picon, 2019 A. Picon, et al. Deep convolutional
    neural networks for mobile capture device-based crop disease classification in
    the wild Comput. Electron. Agric..pp.235 (2019) Google Scholar Rakesh et al.,
    2021 Chandra Joshi Rakesh, Kaushik Manoj, Kishore Dutta Malay, Srivastava Ashish,
    Choudhury Nandlal VirLeafNet: automatic analysis and viral disease diagnosis using
    deep learning in VignaMungo plant Ecol. Inform., 61 (2021) Elsevier Google Scholar
    Ramesh et al., 2018 Shima Ramesh, et al. Plant disease detection using machine
    learning International Conference on Design Innovation for 3Cs Compute Communicate
    Control (ICDI3C), IEEE (2018) Google Scholar Recep and Lahcen, 2022 Eryigit Recep,
    Elfatimi Lahcen Bean Leaf Disease Classification Using Mobile Net Models IEEE
    Access, 10 (2022), pp. 9471-9482 Google Scholar Rudagi et al., 2022 Jayashri Rudagi,
    et al. Plant leaf disease detection using computer vision and machine learning
    algorithms Glob. Trans. Proc., 3 (2022), pp. 305-310 Google Scholar Sahoo et al.,
    2020 Sahoo, et al. Maize leaf disease detection and classification using machine
    learning algorithms Progress in Computing, Analytics and Networking (2020), pp.
    659-669 Google Scholar Shah et al., 2022 Deshna Shah, et al. Image based plant
    disease detection Data Intelligence and Cognitive Informatics, Springer (2022),
    pp. 651-666 CrossRefGoogle Scholar Sibiya et al., 2019 Sibiya, et al. A computational
    procedure for the recognition and classification of maize leaf disease out of
    healthy leaves using convolution neural networks J. Agr. Eng. (2019), pp. 119-131
    CrossRefView in ScopusGoogle Scholar Singh et al., 2020 Singh, et al. PlantDoc:
    A dataset for visual plant disease detection Proceedings of the 7th ACM IKDD CoDS
    and 25th COMAD (2020), pp. 249-253 Google Scholar Sutaji and Yildiz, 2022 Deni
    Sutaji, Oktay Yildiz LEMOXINET: Lite ensemble MobileNetV2 and Xception models
    to predict plant disease Ecol. Inform., 70 (2022) Google Scholar Thi et al., 2022
    Hanh Bui Thi, et al. Enhancing the performance of transferred efficient net models
    in leaf image-based plant disease classification J. Plant Dis. Protect. (2022),
    pp. 623-634 Google Scholar Tiwari et al., 2021 Vaibhav Tiwari, Rakesh Chandra
    Joshi, Malay Kishore Dutta Dense convolutional neural network basedmulti class
    plant disease detection and classification using leaf images Ecol. Inform., 63
    (2021) Google Scholar Togacar, 2022 Mesut Togacar Using DarkNet models and metaheuristic
    optimization methods together to detect weeds growing along with seedlings Ecol.
    Inform., 68 (2022) Google Scholar Tool et al., 2019 E.C. Tool, et al. A comparative
    analysis of fine tuning deep learning models for plant disease identification
    Comput. Electron. Agric. (2019), pp. 272-279 Google Scholar Traore et al., 2019
    D. Traore, et al. Deep neural networks with transfer learning in millet crop images
    Comput. Ind., 108 (2019), pp. 115-120 Google Scholar Veeraballi, 2020 Veeraballi,
    et al. Deep learning based approach for classification and detection of papaya
    leaf diseases Adv. Intelligent Syst. Comput..pp.567 (2020) Google Scholar Vimal
    et al., 2023 Vimal, et al. Classification of beans leaf diseases using fine tuned
    CNN model Procedia Comput. Sci., 218 (2023) Google Scholar Wang et al., 2023a
    Wang, et al. Sweet potato leaf detection in a natural scene based on faster R-CNN
    with a visual attention mechanism and DIoU-NMS Ecol. Inform., 73 (2023) Google
    Scholar Wang et al., 2023b Wang, et al. A novel deep learning method for maize
    disease identification based on small sample-size and complex background datasets
    Ecol. Inform., 75 (2023) Google Scholar Xia, 2022 Limei Xia Automatic strawberry
    leaf scorch severity estimation via faster R-CNN and few-shot learning Ecol. Inform.,
    70 (2022) Google Scholar Xie et al., 2020 Xie, et al. A deep learning real time
    detector for grape leaf disease using improved convolutional neural networks Front.
    Plant Sci. (2020), pp. 1-14 Google Scholar Yadav et al., 2021 S. Yadav, et al.
    Identification of disease using deep learning and evaluation of bacteriosis in
    peach leaf Ecol. Inform., 61 (2021) Google Scholar Yang et al., 2019 Yang Yang,
    et al. Disease prediction model based on BiLSTM and attention mechanism IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM), IEEE (2019) Google Scholar
    Zan, 2022 Wang Zan, et al. An improved faster R-CNN model for multi-object tomato
    maturity detection in complex scenarios Ecol. Inform., 72 (2022) Google Scholar
    Olfa, 2023 Olfa, et al. Deep learning-based segmentation for disease identification
    Ecol. Inform. (2023), p. 34 Available online on January 2023. Google Scholar Cited
    by (11) Image patch-based deep learning approach for crop and weed recognition
    2023, Ecological Informatics Show abstract Strawberry R-CNN: Recognition and counting
    model of strawberry based on improved faster R-CNN 2023, Ecological Informatics
    Show abstract Identification of suitable location to cultivate grape based on
    disease infestation using multi-criteria decision-making (MCDM) and remote sensing
    2023, Ecological Informatics Show abstract Sustainable integrated farming in agriculture
    2023, Water-Soil-Plant-Animal Nexus in the Era of Climate Change Machine Learning
    for Weather-Driven Energy Consumption Forecasting and Optimization in Moroccan
    Agricultural Greenhouses 2023, SSRN Monitoring and Sensing of Real-Time Data with
    Deep Learning Through Micro- and Macro-analysis in Hardware Support Packages 2023,
    SN Computer Science View all citing articles on Scopus View Abstract © 2023 Elsevier
    B.V. All rights reserved. Recommended articles Development of artificial intelligence
    based systems for prediction of hydration characteristics of wheat Computers and
    Electronics in Agriculture, Volume 128, 2016, pp. 34-45 S.M. Shafaei, …, S. Kamgar
    View PDF Weekly carbon dioxide exchange trend predictions in deciduous broadleaf
    forests from site-specific influencing variables Ecological Informatics, Volume
    75, 2023, Article 101996 David A. Wood View PDF Expansion risk of the toxic dinoflagellate
    Gymnodinium catenatum blooms in Chinese waters under climate change Ecological
    Informatics, Volume 75, 2023, Article 102042 Changyou Wang, …, Zhuhua Luo View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 9 Captures
    Readers: 66 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Ecological Informatics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Design of an intelligent bean cultivation approach using computer vision,
    IoT and spatio-temporal deep learning structures
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Phade G.
  - Kishore A.T.
  - Omkar S.
  - Kumar M.S.
  citation_count: '0'
  description: Farmers of the twentieth century are adopting the use of leading technical
    facilities in the field of agriculture to sustain the competitiveness of the global
    market economy. With the help of modern technology, they are trying to reduce
    production costs and improve crop yield with better product quality. A new era
    in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36'N, 77 12'E) in
    the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7-10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
  doi: 10.1002/9781394168002.ch12
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Drone Technology: Future Trends and Practical Applications Chapter
    12 IoT-Enabled Unmanned Aerial Vehicle An Emerging Trend in Precision Farming
    Gayatri Phade,  A. T. Kishore,  S. Omkar,  M. Suresh Kumar Book Editor(s):Sachi
    Nandan Mohanty,  J.V.R. Ravindra,  G. Surya Narayana,  Chinmaya Ranjan Pattnaik,  Y.
    Mohamed Sirajudeen First published: 12 May 2023 https://doi.org/10.1002/9781394168002.ch12
    PDF TOOLS SHARE Summary Farmers of the twentieth century are adopting the use
    of leading technical facilities in the field of agriculture to sustain the competitiveness
    of the global market economy. With the help of modern technology, they are trying
    to reduce production costs and improve crop yield with better product quality.
    A new era in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36''N, 77 12''E)
    in the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7–10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
    References Drone Technology: Future Trends and Practical Applications References
    Related Information Recommended Unmanned Aerial Vehicles for Agriculture: an Overview
    of IoT‐Based Scenarios Bacco Manlio,  Barsocchi Paolo,  Gotta Alberto,  Ruggeri
    Massimiliano Autonomous Airborne Wireless Networks, [1] Role of AI and Big Data
    Analytics in UAV‐Enabled IoT Applications for Smart Cities Madhuri S. Wakode Unmanned
    Aerial Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Unmanned Aerial Vehicle (UAV) Mengxiang Li,  Liang Tang International Encyclopedia
    of Geography: People, the Earth, Environment and Technology, [1] Review on unmanned
    aerial vehicles, remote sensors, imagery processing, and their applications in
    agriculture Daniel Olson,  James Anderson Agronomy Journal Unmanned Aerial Vehicle
    (UAV): A Comprehensive Survey Rohit Chaurasia,  Vandana Mohindru Unmanned Aerial
    Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About
    Cookies Manage Cookies Accessibility Wiley Research DE&I Statement and Publishing
    Policies Developing World Access HELP & SUPPORT Contact Us Training and Support
    DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers & Corporate
    Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024
    John Wiley & Sons, Inc or related companies. All rights reserved, including rights
    for text and data mining and training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: 'Drone Technology: Future Trends and Practical Applications'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'IoT-enabled unmanned aerial vehicle: An emerging trend in precision farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Abbasi R.
  - Martinez P.
  - Ahmad R.
  citation_count: '0'
  description: Chlorosis, or leaf yellowing, in crops is one of the quality issues
    that primarily occurs due to interference in the production of chlorophyll contents.
    The primary contributors to inadequate chlorophyll levels are abiotic stresses,
    such as inadequate environmental conditions (temperature, illumination, humidity,
    etc.), improper nutrient supply, and poor water quality. Various techniques have
    been developed over the years to identify leaf chlorosis and assess the quality
    of crops, including visual inspection, chemical analyses, and hyperspectral imaging.
    However, these techniques are expensive, time-consuming, or require special skills
    and precise equipment. Recently, computer vision techniques have been implemented
    in the agriculture field to determine the quality of crops. Computer vision models
    are accurate, fast, and non-destructive, but they require a lot of data to achieve
    high performance. In this study, an image processing-based solution is proposed
    to solve these problems and provide an easier, cheaper, and faster approach for
    identifying the chlorosis in lettuce crops grown in an aquaponics facility based
    on their sensory property, foliage color. The ‘HSV space segmentation’ technique
    is used to segment the lettuce crop images and extract red (R), green (G), and
    blue (B) channel values. The mean values of the RGB channels are computed, and
    a color distance model is used to determine the distance between the computed
    values and threshold values. A binary indicator is defined, which serves as the
    crop quality indicator associated with foliage color. The model’s performance
    is evaluated, achieving an accuracy of 95%. The final model is integrated with
    the ontology model through a cloud-based application that contains knowledge related
    to abiotic stresses and causes responsible for lettuce foliage chlorosis. This
    knowledge can be automatically extracted and used to take precautionary measures
    in a timely manner. The proposed application finds its significance as a decision
    support system that can automate crop quality monitoring in an aquaponics farm
    and assist agricultural practitioners in decision-making processes regarding crop
    stress management.
  doi: 10.3390/agriculture13030615
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agriculture All Article Types Advanced   Journals
    Agriculture Volume 13 Issue 3 10.3390/agriculture13030615 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editor Maciej
    Zaborowicz Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 2083 Citations 1 Table of Contents Abstract Introduction Related
    Work Research Methodology Results and Discussion Conclusions Author Contributions
    Funding Institutional Review Board Statement Data Availability Statement Acknowledgments
    Conflicts of Interest Appendix A References Altmetric share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic
    Systems by Rabiya Abbasi 1, Pablo Martinez 2 and Rafiq Ahmad 1,* 1 Aquaponics
    4.0 Learning Factory (AllFactory), Department of Mechanical Engineering, University
    of Alberta, 9211 116 St., Edmonton, AB T6G 2G8, Canada 2 Department of Architecture
    and Built Environment, Northumbria University, Newcastle upon Tyne NE7 7YT, UK
    * Author to whom correspondence should be addressed. Agriculture 2023, 13(3),
    615; https://doi.org/10.3390/agriculture13030615 Submission received: 20 February
    2023 / Revised: 28 February 2023 / Accepted: 2 March 2023 / Published: 3 March
    2023 (This article belongs to the Section Digital Agriculture) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Chlorosis, or leaf yellowing, in crops is one
    of the quality issues that primarily occurs due to interference in the production
    of chlorophyll contents. The primary contributors to inadequate chlorophyll levels
    are abiotic stresses, such as inadequate environmental conditions (temperature,
    illumination, humidity, etc.), improper nutrient supply, and poor water quality.
    Various techniques have been developed over the years to identify leaf chlorosis
    and assess the quality of crops, including visual inspection, chemical analyses,
    and hyperspectral imaging. However, these techniques are expensive, time-consuming,
    or require special skills and precise equipment. Recently, computer vision techniques
    have been implemented in the agriculture field to determine the quality of crops.
    Computer vision models are accurate, fast, and non-destructive, but they require
    a lot of data to achieve high performance. In this study, an image processing-based
    solution is proposed to solve these problems and provide an easier, cheaper, and
    faster approach for identifying the chlorosis in lettuce crops grown in an aquaponics
    facility based on their sensory property, foliage color. The ‘HSV space segmentation’
    technique is used to segment the lettuce crop images and extract red (R), green
    (G), and blue (B) channel values. The mean values of the RGB channels are computed,
    and a color distance model is used to determine the distance between the computed
    values and threshold values. A binary indicator is defined, which serves as the
    crop quality indicator associated with foliage color. The model’s performance
    is evaluated, achieving an accuracy of 95%. The final model is integrated with
    the ontology model through a cloud-based application that contains knowledge related
    to abiotic stresses and causes responsible for lettuce foliage chlorosis. This
    knowledge can be automatically extracted and used to take precautionary measures
    in a timely manner. The proposed application finds its significance as a decision
    support system that can automate crop quality monitoring in an aquaponics farm
    and assist agricultural practitioners in decision-making processes regarding crop
    stress management. Keywords: image processing; crop health; abiotic stresses;
    aquaponics; digital farming 1. Introduction Aquaponics is a controlled environment
    agriculture practice that combines aquaculture (farming of fish), hydroponics
    (soilless growing of plants), and nitrifying bacteria in a symbiotic environment.
    This agricultural technique promises to be a suitable alternative to global environmental
    and food problems [1,2]. Little gem romaine lettuce is one of the most common
    crops grown in the aquaponics system because it has a high growth rate, short
    growth cycle, high planting density, and low energy demand [3]. Just like traditional
    agriculture, lettuce crops grown in aquaponics may face abiotic stresses, such
    as inadequate environmental conditions (humidity, temperature, illumination, etc.),
    irregular supply of nutrient-enriched water due to the inaccurate design of the
    system, poor water quality (improper pH), and insufficient concentrations of required
    minerals, such as N-NO3, P, K, Ca, and Mg in the effluent [4,5]. These stresses
    adversely impact the growth and quality of lettuce in a plant factory. In addition
    to yields, the quality of crops is essential for market acceptance as they affect
    consumers’ purchase behavior [6]. Hence, it is vital to maintain the quality of
    crops and rectify the factors impacting them. The quality of crops is assessed
    using morphological traits (crop height, width, area, and volume), biomass production,
    nutritional value, and sensory attributes (color, texture, smell, and taste) [7].
    Visual indices, such as size, appearance, and green color, are the obvious quality
    indicators of lettuce that greatly impact consumers’ buying attitudes [6]. In
    this essence, these indices can be used to determine the quality of lettuce crops
    in a plant factory. Particularly, foliage color, which determines the chlorophyll
    content, is one of the key quality indicators [8]. The green color of the foliage
    represents that the crop is healthy, and the yellow color signifies that the crop
    is suffering from chlorosis. Leaf chlorosis is generally caused by different types
    of stresses, such as irregular illumination or temperature conditions, etc., which
    cause interference in the production of chlorophyll contents [4]. The irregular
    chlorophyll content levels represent the deficiency of secondary metabolites in
    lettuce, such as phenolic compounds, vitamins A and C, and carotenoid, which enhances
    the anti-oxidation ability of the human body and the suppression of inflammatory
    disease and cancer [9]. In order to achieve high-quality crops, it is necessary
    to identify leaf chlorosis and abiotic stresses by monitoring the crop throughout
    the growth cycle. The conventional method to identify leaf chlorosis and plant
    quality is based on visual observation, requiring certain expertise from agriculture
    practitioners [10]. Visual detection, however, is a time-consuming and laborious
    task, and there is a probability of misdiagnosis, especially in the early growth
    stages [10]. Other methods include chemical analyses and leaf color chart (LCC)
    matching, which, again, are costly, time-consuming, and destructive techniques.
    Chemical methods involve the collection of plant tissue for laboratory analyses
    of plant leaves. The Kjeldahl digestion assay is one of the most widely used chemical
    methods [11]. Although this method is accurate, sample preprocessing and delays
    in laboratory analyses hinder its widespread usage. The standard LCC tool is also
    available and used as a reference to estimate leaf color and plant quality [12].
    This technique is widely used in many countries but is a manual inspection process
    and, hence, time-consuming. In order to overcome these challenges, agriculture
    methods have been automated for years, and, hence, several non-destructive methods
    have been proposed to detect leaf chlorosis and plant quality. One of the methods
    is the spectral reflection method, which uses the property of chlorophyll with
    different reflection intensities at different wavebands to assess the quality
    of the plant. Several portable meters, such as SPAD (soil plant analysis development),
    are developed based on this method [13]. The spectral instruments are fast and
    fairly accurate but very expensive. Hyperspectral imaging and spectral remote
    sensing also use the spectral reflection principle [14]. Again, hyperspectral
    instruments are costly and require specific environmental conditions for proper
    sampling. With the development of technology, some researchers applied computer
    vision techniques to detect the quality of plants and leaf yellowing based on
    their nutritional status. Computer vision is a low-cost and non-destructive approach,
    but it requires a large amount of data for training and achieving the desired
    performance of the model [15]. Considering the aforementioned challenges, this
    paper proposes a methodology based on an image processing technique to identify
    chlorosis in lettuce crops grown in an aquaponics facility based on their foliage
    color. To be more certain, the estimation of chlorophyll content or nutrient deficiency
    is out of the scope of this study. The focus of the study is to determine the
    plant quality by extracting the foliage and its red (R), green (G), and blue (B)
    channel values using HSV space segmentation, where HSV stands for hue, saturation,
    and value [16]. The foliage color detection model is then developed using mean
    values of the R, G, and B channels and a color distance model. The color distance
    model calculates the foliage color difference from the threshold values. Numerous
    color distance models are available for this purpose, such as the Euclidean and
    color approximation distances (CIE76, CIE94, CIEDE2000, etc.). In this study,
    the Euclidean distance (ED) model is used, as it is the simplest method of finding
    the distance between two colors within an RGB color space [4]. Moreover, it works
    well when a single color is to be compared to a single color, and the need is
    to simply know whether a distance is greater or smaller, which is the case with
    the proposed model in this study. The model is built in a Jupyter notebook and
    saved in a local directory. An ontology model, ‘AquaONT’, developed by authors
    in previous work, is integrated with the proposed model through a cloud-based
    application built on Streamlit, which is an open-source app framework for machine
    learning and data science [17]. The ontology model provides information on causes
    and abiotic stresses responsible for leaf chlorosis in lettuce crops. The remainder
    of the paper is structured as follows: Section 2 will present the related work;
    Section 3 will explain the methodology used to develop the system; Section 4 will
    present the results and discussion along with model significance; and finally,
    Section 5 will discuss the conclusions and future work. 2. Related Work This section
    presents the recent and relevant image processing-based models that have used
    different color spaces and techniques to identify leaf chlorosis and assess the
    quality of crops. Yang et al. proposed a model based on a support vector machine
    (SVM) and advanced imaging processing techniques, such as image binarization,
    mask, and filling approaches for the extraction of selective color features, such
    as a* (CIELAB color space), G (green from RGB color space), and H (hue from HSV
    color space) to detect the yellow and rotten lettuce leaves in a hydroponics system
    [18]. The model has achieved an accuracy of 98.33%. Maity et al. proposed a model
    based on Otsu’s method and k-means clustering technique to detect faulty regions
    in leaves [19]. Wang et al. developed the HSV and decision tree-based method for
    the greenness identification of maize seedling images captured in the outdoor
    field [20]. Benjamin et al. proposed a methodology based on the color analysis
    technique to determine the quality of tomato leaves using Otsu’s method, SVM,
    k-NN (k-nearest neighbor), and multi-layer perceptron (MLP) [21]. Their model
    obtained an accuracy of 86.45% when classifying the healthy tomato leaves from
    the diseased tomato leaves and an accuracy of 97.39% when classifying the type
    of disease suffered by a diseased leaf. Sharad et al. developed a system based
    on a LAB (L*: lightness, a*: red/green value, b*: blue/yellow value) space-based
    color histogram, k-nearest neighbors, and random forests to detect the quality
    of apple leaves. This approach has achieved an accuracy of 98.63%. These models
    have made great contributions to literature, but some limitations are observed.
    For instance, most models have used images belonging to one scenario. Either they
    are taken in a lab environment (indoor) or outdoors in open-air fields. Secondly,
    some models have used non-destructive chemical approaches to collect the preliminary
    data, particularly while assessing the quality of plants based on chlorophyll
    content, nitrogen level, or nutrient deficiency. Considering the aforementioned,
    in this study, a fully automated, low-cost, and non-destructive model is proposed
    that is built while considering a variety of lettuce images from different sources.
    3. Research Methodology The block diagram, illustrating the five sequential modules
    of research methodology, is shown in Figure 1. Each module, along with its elements,
    is described in the next subsections. Figure 1. Research methodology outline.
    3.1. Data Preparation The image dataset is constructed using a variety of little
    gem romaine lettuce images from diverse sources. This involves top-view images
    of lettuce grown in Allfactory 4.0, an NFT-based aquaponics facility at the University
    of Alberta, Canada, focusing on smart indoor farming [2]. These images are divided
    into two classes based on the color of foliage: green foliage (no leaf chlorosis)
    and yellow foliage (leaf chlorosis). To increase the model flexibility to segment
    lettuce foliage, irrespective of background, and to ensure it correctly determines
    the plant’s health, the dataset is complemented with more lettuce images obtained
    from Ecosia, a search engine based in Berlin, Germany [22]. Figure 2 shows examples
    of some of the images. Figure 2. Image dataset: (a,b) acquired from aquaponics
    facility, and (c–f) are downloaded from ecosia.org. Next, the image augmentation
    process is performed to increase the dataset and reliability of the segmentation
    process, despite the location and orientation of the objects in the image, by
    generating new images from existing images. This study uses Albumentations, a
    Python library, for fast and flexible image augmentations [23]. The different
    augmentation techniques applied are the horizontal flip, vertical flip, 90° rotation,
    and glass noise. The new images are added to their respective classes. Figure
    3 shows examples of the augmentations. Figure 3. Data augmentation was performed
    on different images. 3.2. Image Segmentation Image segmentation was performed
    to extract the lettuce foliage from the background for further processing. This
    study uses the HSV segmentation model to segment the image [16]. There are two
    stages to the image segmentation process, which are detailed in the next two subsections.
    3.2.1. HSV Color Space The acquired images are in RGB format, where the color
    of any object in these images is represented with the combined values of the R,
    G, and B channels. The main problem with this color representation is that the
    objects’ colors are affected by variations in the illumination conditions [2].
    With the HSV color segmentation technique, as the name suggests, HSV color space
    is used, which describes the objects’ colors independent of the illumination effect
    [16]. The difference between various color spaces is usually based on color representation.
    For instance, the object’s color in the HSV color space is represented by three
    different parameters, namely the hue ( 𝐻 ), saturation ( 𝑆 ), and value ( 𝑉 ).
    H represents the color of the object, whereas the S and V values represent the
    illuminance state of the object’s color [16]. This type of description provides
    the ability to discriminate the color from the illuminance while avoiding the
    effect of the illumination changes on the object’s color. Therefore, the first
    stage of segmentation is to convert the image’s color space from RGB into HSV.
    Generally, the transformation process from RGB into HSV can be performed using
    the following Equations [24]: 𝑅 ′ = 𝑅 255  ,  𝐺 ′ = 𝐺 255  ,  𝐵 ′ = 𝐵 255   (1)
    𝑀=max( 𝑅 ′ ,  𝐺 ′ ,  𝐵 ′ ), 𝑚=min( 𝑅 ′ ,  𝐺 ′ ,  𝐵 ′ ),  (2) 𝐶=𝑀−𝑚 (3) 𝐻= ⎧ ⎩
    ⎨           0° 𝑖𝑓 𝐶=0  60°×( 𝐺 ′ − 𝐵 ′ 𝐶 𝑚𝑜𝑑 6) 𝑖𝑓 𝑀= 𝑅 ′ 60°×( 𝐺 ′
    − 𝐵 ′ 𝐶 +2) 𝑖𝑓 𝑀= 𝐺 ′ 60°×( 𝐺 ′ − 𝐵 ′ 𝐶 +4) 𝑖𝑓 𝑀= 𝐵 ′ (4) 𝑆= ⎧ ⎩ ⎨   0 𝑖𝑓 𝑀=0
    𝐶 𝑀  𝑖𝑓 𝑀≠0 (5) 𝑉=𝑀 (6) After the image transformation, a color bar is created,
    which provides intensity values for the ( 𝐻 ), ( 𝑆 ), and ( 𝑉 ) channels. These
    values are used in the next stage for segmenting the image. Figure 4 shows an
    example of the original image, its HSV channels, and the color bar format. Figure
    4. Illustration of image, its HSV channels, and color bar format. 3.2.2. Image
    Hue Thresholding The second stage of image segmentation is to determine the suitable
    threshold value to distinguish between the foreground and background. For this
    purpose, the hue image obtained in the first stage is used, as it provides a suitable
    grayscale image that can be used to classify objects based on color content. The
    upper and lower range of the hue channels is obtained from the color bar. This
    range is used to define an upper and lower threshold value for lettuce foliage
    in a hue image in the form of a mask. This mask is then applied to the R, G, and
    B channels of the original image, which are then stacked to obtain the segmented
    image. The final segmented image is saved in RGB format. In order to save time,
    the segmentation process is automated, and by the end of the process, each segmented
    image is saved in a common directory. 3.3. Foliage Color Detection Model Development
    The R, G, and B values of the lettuce foliage (foreground) are extracted from
    the segmented images. These images are represented as ( 𝑖 ) and ( 𝑗 ) for two
    classes: ( 𝑔 ) (green foliage—no chlorosis) and ( 𝑦 ) (yellow foliage—leaf chlorosis),
    respectively. The mean value of each color channel: red ( 𝜇 𝑅 ), green ( 𝜇 𝐺 ),
    and blue ( 𝜇 𝐵 ) for the two classes is computed using Equations (1) and (2).
    The elements of Equations (7) and (8) are determined using Equation (9) through
    to Equation (14). 𝜇 𝑔,𝑖 =[ 𝜇 𝑅,𝑖 , 𝜇 𝐺, 𝑖 , 𝜇 𝐵,𝑖 ] (7) 𝜇 𝑦, 𝑗 =[ 𝜇 𝑅,𝑗 , 𝜇 𝐺,𝑗
    , 𝜇 𝐵,𝑗 ] (8) where ( 𝜇 𝑔,𝑖 ) and ( 𝜇 𝑦,𝑗 ) represent the mean values of the three-color
    channels of the foreground (lettuce foliage) of two classes. Equations (9)–(14)
    are used for computing the mean values of the channels. 𝑅 𝑖/𝑗 = ∑ 𝑛 𝑅,𝑖/𝑗 1 𝑅
    𝑛,𝑖/𝑗 (9) 𝐺 𝑖/𝑗 = ∑ 𝑛 𝐺,𝑖/𝑗 1 𝐺 𝑛,𝑖/𝑗 (10) 𝐵 𝑖/𝑗 = ∑ 𝑛 𝐵,𝑖/𝑗 1 𝐵 𝑛,𝑖/𝑗 (11) 𝜇
    𝑅,𝑖/𝑗 = 𝑅 𝑖/𝑗 𝑛 𝑅,𝑖/𝑗 (12) 𝜇 𝐺,𝑖/𝑗 = 𝐺 𝑖/𝑗 𝑛 𝐺,𝑖/𝑗   (13) 𝜇 𝐵,𝑖/𝑗 = 𝐵 𝑖/𝑗 𝑛 𝐵,𝑖/𝑗
    (14) where ( 𝑅 𝑖/𝑗 ), ( 𝐺 𝑖/𝑗 ), and ( 𝐵 𝑖/𝑗 ) refer to the sum of the red, green,
    and blue values of lettuce foliage in two classes; ( 𝑖/𝑗 ) refers to either image
    belonging to the ( 𝑔 ) class or ( 𝑦 ) class; and ( 𝑛 𝑅,𝑖/𝑗 ), ( 𝑛 𝐺,𝑖/𝑗 ), and
    ( 𝑛 𝐵,𝑖/𝑗 ) represent the R, G, and B counts of lettuce foliage, respectively.
    The obtained background in segmented images is black. Hence, the R, G, and B counts
    and values of the background are not included while determining the mean value
    of the R, G, and B channels for the foreground. The process of calculating the
    mean values of the R, G, and B channels was, again, automated to save time. The
    values for each channel were automatically saved in an Excel file. While saving
    the results, it is ensured that the mean values of R, G, and B are saved for their
    respective image label and class category, ( 𝑔 ) and ( 𝑦 ). Next, the reference
    or threshold values (( 𝑔 𝑟𝑒𝑓 ) and ( 𝑦 𝑟𝑒𝑓 )) were determined for both ( 𝑔 ) and
    ( 𝑦 ) classes, using Equations (15) and (16). To compute ( 𝑔 𝑟𝑒𝑓 ), three average
    values are calculated, which are related to the mean red, mean green, and mean
    blue values of the images saved in the Excel file for the ( 𝑔 ) category. The
    total number of mean values for each channel is ( 𝑚 ). The first average value
    is obtained by summing all the green channel values and dividing the results by
    the total number of green values ( 𝑚 ). Similarly, the second and third average
    values are obtained by summing all the red channel values and all blue channel
    values of all images in the ( 𝑚 ) category and dividing the results by the number
    of red ( 𝑚 ) and blue values ( 𝑚 ), respectively. A similar computation is done
    for ( 𝑦 𝑟𝑒𝑓 ) while considering the channel values and their count ( 𝑙 ) for images
    in the ( 𝑦 ) category. Equations (17)–(22) are used to calculate ( 𝑔 𝑟𝑒𝑓 ) and
    ( 𝑦 𝑟𝑒𝑓 ). 𝑔 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑚 , 𝑥 ̲ 𝐺, 𝑚 , 𝑥 ̲ 𝐵,𝑚 ] (15) 𝑦 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑙 , 𝑥 ̲
    𝐺, 𝑙 , 𝑥 ̲ 𝐵,𝑙 ] (16) 𝑥 ̲ 𝑅,𝑚 = ∑ 𝑚 1 𝑅 𝑚 𝑚 (17) 𝑥 ̲ 𝐺,𝑚 = ∑ 𝑚 1 𝐺 𝑚 𝑚 (18) 𝑥
    ̲ 𝐵,𝑚 = ∑ 𝑚 1 𝐵 𝑚 𝑚 (19) 𝑥 ̲ 𝑅,𝑙 = ∑ 𝑙 1 𝑅 𝑙 𝑙 (20) 𝑥 ̲ 𝐺,𝑙 = ∑ 𝑙 1 𝐺 𝑙 𝑙 (21)
    𝑥 ̲ 𝐵,𝑙 = ∑ 𝑙= 1 𝐵 𝑙 𝑙 (22) where  ( 𝑥 ̲ 𝑅,𝑚 ) ,  ( 𝑥 ̲ 𝐺,𝑚 ) , and ( 𝑥 ̲ 𝐵,𝑚
    )  are the averages of three channel values in the ( 𝑔 ) category and  ( 𝑅 𝑚 )
    ,  ( 𝐺 𝑚 ) , and  ( 𝐵 𝑚 ) , are the values of three channels in the ‘g’ category.
    Likewise,  ( 𝑥 ̲ 𝑅,𝑙 ) ,  ( 𝑥 ̲ 𝐺,𝑙 ) , and ( 𝑥 ̲ 𝐵,𝑙 )  are the averages of three
    channel values in the ( 𝑦 ) category and  ( 𝑅 𝑙 ) ,  ( 𝐺 𝑙 ) , and  ( 𝐵 𝑙 ) ,
    are the values of three channels in the ( 𝑦 ) category. After determining the
    reference or threshold values, the color distance model was used to compute the
    foliage color difference from the threshold values. The Euclidean distance (ED)
    model was used in this study, and its general equation is presented below [4].
    𝑑= Δ 𝑅 2 +Δ 𝐺 2 +Δ 𝐵 2 − − − − − − − − − − − − − − √ (23) where  Δ𝑅= 𝑅 2 − 𝑅 1
    ,  Δ𝐺= 𝐺 2 − 𝐺 1 , and  Δ𝐵= 𝐵 2 − 𝐵 1 . Based on the ED model, two distances  (
    𝑑 1 )  and  ( 𝑑 2 ) , were computed using two threshold values: ( 𝑔 𝑟𝑒𝑓 ) and
    ( 𝑦 𝑟𝑒𝑓 ), respectively.  ( 𝑑 1 )  determines the distance from the green color
    threshold, whereas  ( 𝑑 2 )  determines the distance from the yellow color threshold.
    For single foliage, both  ( 𝑑 1 )  and  ( 𝑑 2 )  are determined. A lower value
    of  ( 𝑑 1 )  and a higher value of  ( 𝑑 2 )  suggests that the color patterns
    of foliage are closer to ( 𝑔 𝑟𝑒𝑓 ) or, in other words, green tones. Conversely,
    a lower value of  ( 𝑑 2 )  and a higher value of  ( 𝑑 1 )  suggests that color
    patterns of foliage are closer to ( 𝑦 𝑟𝑒𝑓 ) or, in other words, yellow tones.
    The governing equations for  ( 𝑑 1 )  and  ( 𝑑 2 )  are given below. 𝑑 1 = ( 𝑥
    𝑅 − 𝑥 ̲ 𝑅,𝑚 ) 2 + ( 𝑥 𝐺 − 𝑥 ̲ 𝐺,𝑚 ) 2 + ( 𝑥 𝐵 − 𝑥 ̲ 𝐵,𝑚 ) 2 − − − − − − − − −
    − − − − − − − − − − − − − − − − − − − − − − − − − √ (24) 𝑑 2 = ( 𝑥 𝑅 − 𝑥 ̲ 𝑅,𝑙
    ) 2 + ( 𝑥 𝐺 − 𝑥 ̲ 𝐺,𝑙 ) 2 + ( 𝑥 𝐵 − 𝑥 ̲ 𝐵,𝑙 ) 2 − − − − − − − − − − − − − − −
    − − − − − − − − − − − − − − − − − √ (25) where  ( 𝑥 𝑅 ) ,  ( 𝑥 𝐺 ) , and  ( 𝑥
    𝐵 )  are the mean values of three channels (R, G, B) of the foreground in the
    segmented image of the test samples. Lastly, the quality indicator  (𝑄)  is defined
    as a function of  ( 𝑑 1 )  and  ( 𝑑 2 )  for evaluating the plants’ quality based
    on their foliage color. In this context, when green foliage with no leaf depigmentation
    is detected, the value of  (𝑄)  is equal to 1, which implies that the crop is
    healthy. On the other hand, when yellow foliage with leaf depigmentation is detected,
    the value of  (𝑄)  is equal to 0, suggesting that the crop is unhealthy.  (𝑄)  is
    represented as below: 𝑄=𝑓( 𝑑 1 , 𝑑 2 )={ 1 𝑖𝑓  𝑑 1 < 𝑑 2 0 𝑖𝑓  𝑑 2 < 𝑑 1   (26)
    3.4. Ontology Model The complete development and details of all concepts and instances
    of an ontology model, ‘AquaONT’, is available in previous work by the authors
    [17]. AquaONT is a unified ontology model that represents and stores the essential
    knowledge of an aquaponic 4.0 system. It comprises six concepts: Consumer_Product,
    Ambient_Environment, Contextual_Data, Production_System, Product_Quality, and
    Production_Facility. In this study, two classes, ‘Consumer_Product’ and ‘Product_Quality’,
    are used for knowledge extraction. The ‘Consumer_Product’ class provides an abstract
    view of the type, growth status, and growth parameters of ready-to-harvest crops
    in an aquaponics system. Whereas the ‘Product_Quality’ class provides knowledge
    on the crop attributes related to pathology (abiotic and biotic stresses, causes,
    and the ways and means by which these can be managed or controlled), morphology
    (canopy dimensions, such as area, length, width, etc.) and foliage color. The
    lettuce crop is considered in this study. The crop growth and quality attributes
    are defined as instances of respective classes, which are extracted once the crop
    foliage is detected as yellow (or leaf chlorosis is detected). Figure 5 shows
    the hierarchical architecture of the ‘Consumer_Product’ and ‘Product_Quality’
    classes, with their instances for the lettuce crop in Protégé7 (an open-source
    ontology editor and framework developed at Stanford University) environment. Figure
    5. Ontology model showing classes, instances, and relationships between them.
    3.5. Cloud-Based Application The proposed foliage detection and ontology models
    are deployed on a cloud-based application built on Streamlit. The app’s layout
    is shown in Figure A1, Figure A2, Figure A3 and Figure A4 in Appendix A. The app
    works in six stages. The first and second stages are associated with two user
    inputs, “Select the Model” and “Upload Image”, as shown in Figure A1 in Appendix
    A. The first input allows the user to select a relevant quality evaluation model.
    This app has other quality models integrated into it, which are out of the scope
    of this study. In this study, the relevant model is “Lettuce Foliage Pigment”.
    After selecting the model, the image is selected using the second input. The third
    and fourth stages are linked with two widgets, “Preprocess and Segment Image”
    and “Determine the Crop Status”, respectively, shown in Figure A2 in Appendix
    A, that run the sub-processes associated with the model. As the name suggests,
    the first widget activates the segmentation algorithm, which preprocesses and
    segments the image selected by the user in the second stage. Likewise, the second
    widget activates the model developed in the study. The model determines the status
    of the crop and displays the results on the application panel. In the fifth stage,
    the sensor data from the dashboard is acquired and displayed to monitor the environmental
    conditions, as shown in Figure A3 in Appendix A. By clicking ‘Sensor Data’, the
    most recent data will be displayed. In the sixth stage, a widget is developed,
    ‘Causes and Treatments’, which is linked with ‘AquaONT’. This widget extracts
    knowledge from the ontology model related to the possible causes of leaf yellowing
    in the aquaponics facility. Figure A4 in Appendix A show the sixth stage of the
    app when yellow foliage is detected. 4. Results and Discussion This section first
    presents the validation of the proposed method by a case study. Then, the performance
    of the proposed method is compared with existing similar methods. To validate
    the proposed model, twenty healthy seedings were placed in NFT-based hydroponic
    systems for five weeks (plantation cycle), after which lettuce was harvested.
    A 12MP Sony Exmor RS camera sensor was used to capture the crop images during
    this period. Twenty images of 4032 × 3024 pixels (one image for one lettuce plant)
    were captured daily at 9:00 am from the top while keeping the distance between
    the camera and channel at a value of 40 cm throughout the plantation cycle, i.e.,
    five weeks. In total, 700 images of plants were collected over five weeks. During
    the first three weeks, no significant difference was observed in the color of
    the foliage. After the third week, foliage chlorosis was observed in eight lettuce
    plants. Therefore, for further processing, the images captured in the last two
    weeks of the plantation cycle were considered for model validation. In total,
    280 images were divided into two classes based on the color of the foliage: Green
    Foliage—No Leaf Chlorosis (168 images) and Yellow Foliage—Leaf Chlorosis (112
    images). The dataset is complemented with more lettuce images with green (32)
    and yellow (88) foliage, downloaded from Ecosia. The images were added to their
    respective classes. All the images were resized to 1000 × 1000 pixels and saved
    in JPG format. The augmentation process was then performed. In total, 100 images
    (50 from both classes) were selected randomly for the augmentation, which created
    100 new images. The new images were added to their respective classes, increasing
    the length of the dataset to 500 images. Half of these images belong to the (
    𝑔 ) class, and half belong to the ( 𝑦)  class and, hence, are saved in two folders
    named ( 𝑔 ) and ( 𝑦),  respectively. Out of 500 images, 100 random images (50
    from each folder) were extracted and saved in a separate validation folder to
    be used for the model evaluation. In order to complement the validation data,
    20 images were randomly selected (10 from each class), and their R, G, and B values
    were altered using Adobe Photoshop in a way that the healthy-looking lettuce appears
    yellow, and the unhealthy lettuce appears green. The validation dataset now had
    120 images in total. Figure 6 shows an example of the new images generated for
    the validation dataset. Figure 6. Example of images generated in Adobe Photoshop
    (left original images, right for the altered images). The segmentation was then
    performed on all 520 images in the dataset. Figure 7 shows an example of the segmented
    images. For the computation of the threshold values, 400 images (g and y folder)
    were used by following the process mentioned in Section 3.3. The R, G, and B values
    and their counts were computed for the foreground (lettuce foliage) of 400 segmented
    images in two classes, ( 𝑔 ) and ( 𝑦 ). The mean values of the R, G, and B channels
    were then computed. Each class has 200 foliage images, so for each class 3 × 200
    = 600 mean values (3 refers to 3 channels of an image) were obtained, which were
    automatically saved in an excel file. Figure 7. Example of segmented images ((a,c):
    segmented green lettuce; (b,d): segmented altered yellowed lettuce). Out of the
    600 means values for each class, 200 belong to the red channel, 200 belong to
    the green channel, and 200 belong to the blue channel. The threshold values (
    𝑔 𝑟𝑒𝑓 ) and ( 𝑦 𝑟𝑒𝑓 ) were obtained by dividing the mean values of the three channels
    by 200, which are given below. 𝑔 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑚 , 𝑥 ̲ 𝐺, 𝑚 , 𝑥 ̲ 𝐵,𝑚 ]=[123.4,
    138.2, 19.8] (27) 𝑦 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑙 , 𝑥 ̲ 𝐺, 𝑙 , 𝑥 ̲ 𝐵,𝑙 ]=[156.6, 155.8, 22.2]  (28)
    The model was validated using a validation dataset comprising 120 different segmented
    images belonging to two classes, ( 𝑔 ) and ( 𝑦) . The mean values of the three
    channels were computed for each image and were inserted into Equations (27) and
    (28) in place of  ( 𝑥 𝑅 ) ,  ( 𝑥 𝐺 ) , and  ( 𝑥 𝐵 ) , along with the reference
    values ( 𝑔 ) and ( 𝑦)  computed above. The  ( 𝑑 1 )  and  ( 𝑑 2 )  were determined
    for all 120 images in the validation dataset using Equations (17) and (18), respectively.
    The quality indicator,  (𝑄) , was also determined using Equation (19) for 120
    images. The performance of the model on the validation dataset was then evaluated
    by analyzing the ground truth  (𝑄)  value and predicted  (𝑄)  value. In the validation
    dataset, 60 images have a ground truth  (𝑄)  value of 1, meaning these images
    contain healthy and green lettuce foliage, and 60 images have a ground truth value
    of 0, meaning these Images contain unhealthy and yellow lettuce foliage. The performance
    is presented in the form of a confusion matrix (CM), shown in Figure 8 [2]. Figure
    8. Confusion Matrix. The different values of the CM are interpreted as: True Positive
    (TP) = 58. Thus, 58 plants were healthy, and the model correctly classified them
    healthy as well. True Negative (TN) = 57. Thus, 57 plants were unhealthy, and
    the model correctly classified them unhealthy as well. False Positive (FP) = 3.
    Thus, 3 plants were unhealthy, but the model incorrectly classified them as healthy.
    False Negative (FN) = 2. Thus, 2 plants were healthy, but the model incorrectly
    classified them as unhealthy. The performance metrics based on CM are also computed
    using the formulae given below and are summarized in Table 1. 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦= 𝑇𝑃+𝑇𝑁
    𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 (29) 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= 𝑇𝑃 𝑇𝑃+𝐹𝑃 (30) 𝑅𝑒𝑐𝑎𝑙𝑙= 𝑇𝑃 𝑇𝑃+𝐹𝑁 (31) 𝐹1−𝑆𝑐𝑜𝑟𝑒= 2×𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛×𝑅𝑒𝑐𝑎𝑙𝑙
    𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙 (32) Table 1. Summary of the performance metrics. In Table 1,
    N (truth) tells the number of actual cases in a particular class, and N (classified)
    tells the number of predicted cases belonging to a class. Table 1 shows that the
    model has achieved an average accuracy of 95%, precision of 96%, recall of 96%,
    and F1-Score of 96%. The model has correctly classified 115 cases out of a total
    of 120 cases. Figure 9 shows an example of correctly classified cases. Figure
    9. Example of correctly classified cases. To further investigate the performance
    of the proposed methodology, it was compared with the existing vision-based methods
    mentioned in Section 2. These methods were implemented on the dataset prepared
    in this study, and their performance was evaluated using the metrics based on
    CM, which are presented in Table 2. The results show that the proposed method
    has outperformed the similar existing methods, achieving an average accuracy of
    96%, precision, recall, and F1-score of 96%. The method proposed by Sharad et
    al. has shown appreciable performance when implemented on the dataset prepared
    in this study by achieving an average accuracy of 94%, a precision of 94%, a recall
    of 95%, and an F1-score of 94.45% [25]. Whereas, with their apple leaf dataset,
    they have achieved an accuracy of 98.63%. Table 2. Performance metrics of existing
    methods. The final model was then deployed in the aquaponics facility through
    a cloud-based application. This time, instead of manually taking the images, four
    ELP 1080P webcams (2.8–12 mm HD Varifocal Lens) were installed at a distance of
    40 cm from the channels for image acquisition. Each camera is programmed through
    a Raspberry Pi 4 (Model B Rev 1) controller to take one image per day at 9:00
    am, which along with the sensor values from WSM, are wirelessly uploaded to the
    ‘IoT enabled Aquaponics Dashboard’ developed by the authors in previous work [26].
    The images and sensor data are available on the cloud as well as locally, and
    the app developed in this study can access them. The ontology model discussed
    in Section 3.4 was also integrated with the proposed model and deployed on a cloud-based
    application. Once the health status of the lettuce crop was identified as ‘Yellow
    Foliage—Leaf Chlorosis’, the potential causes were automatically extracted from
    the ontology model and displayed on the application panel. Figure A1, Figure A2,
    Figure A3 and Figure A4 in Appendix A show an example of the working of the proposed
    method and application for a lettuce crop when its foliage was detected to be
    yellow. The primary causes of lettuce foliage chlorosis could be inadequate environmental
    conditions (humidity, air temperature), poor water quality (inadequate pH or EC),
    nutrient deficiency, etc. By analyzing sensor data and the possible causes of
    leaf chlorosis, it is possible to reach the specific cause of the problem. For
    instance, if sensor data show that all the parameters are within their optimal
    ranges, then the problem could be related to nutrient delivery or the design of
    the system. A reasonable treatment can be suggested after problem identification.
    The proposed model was developed using open-source frameworks, and, hence, it
    can easily be expanded or adjusted as per the requirement by adjusting the threshold
    values. The significance of the model is that it is fully automated and offers
    a non-destructive, low-cost and reliable approach to identifying leaf chlorosis
    and determining the quality of lettuce plants along with the possible causes.
    In contrast to the computer vision and machine learning-based models, the proposed
    methodology requires less data. 5. Conclusions This study discusses the major
    problem of lettuce foliage chlorosis in an aquaponics context. The ‘HSV Color
    Segmentation’ image processing approach was used to segment the lettuce images
    obtained from various resources. The segmented images were divided into two classes,
    ‘Green Foliage-No Leaf Chlorosis’ and ‘Yellow Foliage-Leaf Chlorosis’. Then, the
    foliage color detection model was developed, and a quality indicator was defined
    to identify leaf chlorosis and determine the quality of the lettuce crop. The
    model is validated, achieving an overall accuracy of 95%. The performance of the
    model was also compared with existing similar methods. The results show that the
    proposed method has outperformed these existing methods. A cloud-based application
    was then developed, where the final model was deployed. The ontology model that
    contains knowledge related to the causes of lettuce crop chlorosis was also integrated
    with the final model. The proposed system proves to be accurate and flexible enough
    to be used in real scenarios and, hence, is not limited to being disturbed by
    potentially changing conditions and environments. For future work, the system
    will be extended to include other crops. Moreover, images with complex backgrounds
    and multiple objects will also be added to the dataset. The ontology model will
    also be extended to include the specific treatments for potential causes of leaf
    chlorosis. Author Contributions Conceptualization, R.A. (Rabiya Abbasi), P.M.
    and R.A. (Rafiq Ahmad); methodology, R.A. (Rabiya Abbasi) and P.M.; validation,
    R.A. (Rabiya Abbasi); investigation, R.A. (Rabiya Abbasi); writing—original draft,
    R.A. (Rabiya Abbasi); writing—review and editing, P.M. and R.A. (Rafiq Ahmad);
    supervision, P.M. and R.A. (Rafiq Ahmad); project administration, R.A. (Rafiq
    Ahmad); funding acquisition, P.M. and R.A. (Rafiq Ahmad). All authors have read
    and agreed to the published version of the manuscript. Funding The authors acknowledge
    the financial support of this work from the Natural Sciences and Engineering Research
    Council of Canada (NSERC) (Grants File No. ALLRP 545537-19 and RGPIN-2017-04516).
    Institutional Review Board Statement Not applicable. Data Availability Statement
    The data that support the findings of this study are available from the corresponding
    author, R.A., upon reasonable request. Acknowledgments The authors would like
    the acknowledge the support from the members of the LIMDA Lab and the ALLFactory
    at the University of Alberta. Conflicts of Interest The authors declare no conflict
    of interest. Appendix A Figure A1. Stages 1 and 2 of cloud-based application.
    Figure A2. Stages 3 and 4 of cloud-based application. Figure A3. Stage 5 of cloud-based
    application. Figure A4. Stage 6 of cloud-based application. References Abbasi,
    R.; Martinez, P.; Ahmad, R. An ontology model to support the automated design
    of aquaponic grow beds. Procedia CIRP 2021, 100, 55–60. [Google Scholar] [CrossRef]
    Reyes-Yanes, A.; Martinez, P.; Ahmad, R. Real-time growth rate and fresh weight
    estimation for little gem romaine lettuce in aquaponic grow beds. Comput. Electron.
    Agric. 2020, 179, 105827. [Google Scholar] [CrossRef] Lin, K.H.; Huang, M.Y.;
    Huang, W.D.; Hsu, M.H.; Yang, Z.W.; Yang, C.M. The effects of red, blue, and white
    light-emitting diodes on the growth, development, and edible quality of hydroponically
    grown lettuce (Lactuca sativa L. var. capitata). Sci. Hortic. 2013, 150, 86–91.
    [Google Scholar] [CrossRef] Haider, T.; Farid, M.S.; Mahmood, R.; Ilyas, A.; Khan,
    M.H.; Haider, S.T.A.; Chaudhry, M.H.; Gul, M. A Computer-Vision-Based Approach
    for Nitrogen Content Estimation in Plant Leaves. Agriculture 2021, 11, 766. [Google
    Scholar] [CrossRef] Taha, M.F.; Abdalla, A.; Elmasry, G.; Gouda, M.; Zhou, L.;
    Zhao, N.; Liang, N.; Niu, Z.; Hassanein, A.; Al-Rejaie, S.; et al. Using Deep
    Convolutional Neural Network for Image-Based Diagnosis of Nutrient Deficiencies
    in Plants Grown in Aquaponics. Chemosensors 2022, 10, 45. [Google Scholar] [CrossRef]
    Matysiak, B.; Ropelewska, E.; Wrzodak, A.; Kowalski, A.; Kaniszewski, S. Yield
    and Quality of Romaine Lettuce at Different Daily Light Integral in an Indoor
    Controlled Environment. Agronomy 2022, 12, 1026. [Google Scholar] [CrossRef] Abbasi,
    R.; Martinez, P.; Ahmad, R. The digitization of agricultural industry—A systematic
    literature review on agriculture 4.0. Smart Agric. Technol. 2022, 2, 100042. [Google
    Scholar] [CrossRef] Kowalczyk, K.; Sieczko, L.; Goltsev, V.; Kalaji, H.M.; Gajc-Wolska,
    J.; Gajewski, M.; Gontar, Ł.; Orliński, P.; Niedzińska, M.; Cetner, M.D. Relationship
    between chlorophyll fluorescence parameters and quality of the fresh and stored
    lettuce (Lactuca sativa L.). Sci. Hortic. 2018, 235, 70–77. [Google Scholar] [CrossRef]
    Song, J.; Huang, H.; Hao, Y.; Song, S.; Zhang, Y.; Su, W.; Liu, H. Nutritional
    quality, mineral and antioxidant content in lettuce affected by interaction of
    light intensity and nutrient solution concentration. Sci. Rep. 2020, 10, 2796.
    [Google Scholar] [CrossRef] [Green Version] Cook, S.E.; Bramley, R.G.V. Coping
    with variability in agricultural production -implications for soil testing and
    fertiliser management. Commun. Soil Sci. Plant Anal. 2000, 31, 1531–1551. [Google
    Scholar] [CrossRef] Kjeldahl, J. Neue Methode zur Bestimmung des Stickstoffs in
    organischen Körpern. Z. Anal. Chem. 1883, 22, 366–382. [Google Scholar] [CrossRef]
    [Green Version] Yang, W.H.; Peng, S.; Huang, J.; Sanico, A.L.; Buresh, R.J.; Witt,
    C. Using Leaf Color Charts to Estimate Leaf Nitrogen Status of Rice. Agron. J.
    2003, 95, 212–217. [Google Scholar] [CrossRef] Markwell, J.; Osterman, J.C.; Mitchell,
    J.L. Calibration of the Minolta SPAD-502 leaf chlorophyll meter. Photosynth. Res.
    1995, 46, 467–472. [Google Scholar] [CrossRef] Zheng, H.; Cheng, T.; Li, D.; Zhou,
    X.; Yao, X.; Tian, Y.; Cao, W.; Zhu, Y. Evaluation of RGB, Color-Infrared and
    Multispectral Images Acquired from Unmanned Aerial Systems for the Estimation
    of Nitrogen Accumulation in Rice. Remote Sens. 2018, 10, 824. [Google Scholar]
    [CrossRef] [Green Version] Tao, M.; Ma, X.; Huang, X.; Liu, C.; Deng, R.; Liang,
    K.; Qi, L. Smartphone-based detection of leaf color levels in rice plants. Comput.
    Electron. Agric. 2020, 173, 105431. [Google Scholar] [CrossRef] Burdescu, D.D.;
    Brezovan, M.; Ganea, E.; Stanescu, L. A new method for segmentation of images
    represented in a HSV color space. In Proceedings of the Advanced Concepts for
    Intelligent Vision Systems: 11th International Conference, ACIVS 2009, Bordeaux,
    France, 28 September–2 October 2009; Springer: Berlin/Heidelberg, Germany, 2009;
    pp. 606–617. [Google Scholar] [CrossRef] Abbasi, R.; Martinez, P.; Ahmad, R. An
    ontology model to represent aquaponics 4.0 system’s knowledge. Inf. Process. Agric.
    2022, 9, 514–532. [Google Scholar] [CrossRef] Yang, R.; Wu, Z.; Fang, W.; Zhang,
    H.; Wang, W.; Fu, L.; Majeed, Y.; Li, R.; Cui, Y. Detection of abnormal hydroponic
    lettuce leaves based on image processing and machine learning. Inf. Process. Agric.
    2021, 10, 1–10. [Google Scholar] [CrossRef] Maity, S.; Sarkar, S.; Vinaba Tapadar,
    A.; Dutta, A.; Biswas, S.; Nayek, S.; Saha, P. Fault Area Detection in Leaf Diseases
    Using K-Means Clustering. In Proceedings of the 2018 2nd International Conference
    on Trends in Electronics and Informatics (ICOEI), Tirunelveli, India, 11–12 May
    2018; pp. 1538–1542. [Google Scholar] [CrossRef] [Green Version] Yang, W.; Wang,
    S.; Zhao, X.; Zhang, J.; Feng, J. Greenness identification based on HSV decision
    tree. Inf. Process. Agric. 2015, 2, 149–160. [Google Scholar] [CrossRef] [Green
    Version] Luna-Benoso, B.; Martínez-Perales, J.C.; Cortés-Galicia, J.; Flores-Carapia,
    R.; Silva-García, V.M. Detection of Diseases in Tomato Leaves by Color Analysis.
    Electronics 2021, 10, 1055. [Google Scholar] [CrossRef] Streamlit • The Fastest
    Way to Build and Share Data Apps [WWW Document], n.d. Available online: https://streamlit.io/
    (accessed on 7 June 2022). Buslaev, A.; Iglovikov, V.I.; Khvedchenya, E.; Parinov,
    A.; Druzhinin, M.; Kalinin, A.A. Albumentations: Fast and flexible image augmentations.
    Information 2020, 11, 125. [Google Scholar] [CrossRef] [Green Version] Loresco,
    P.J.M.; Valenzuela, I.C.; Dadios, E.P. Color Space Analysis Using KNN for Lettuce
    Crop Stages Identification in Smart Farm Setup. In Proceedings of the TENCON 2018-2018
    IEEE Region 10 Conference, Jeju, Republic of Korea, 28–31 October 2018; pp. 2040–2044.
    [Google Scholar] [CrossRef] Hasan, S.; Jahan, S.; Islam, M.I. Disease detection
    of apple leaf with combination of color segmentation and modified DWT. J. King
    Saud Univ.-Comput. Inf. Sci. 2022, 34, 7212–7224. [Google Scholar] [CrossRef]
    Abbasi, R.; Martinez, P.; Ahmad, R. Data acquisition and monitoring dashboard
    for IoT enabled aquaponics facility. In Proceedings of the 2022 10th International
    Conference on Control, Mechatronics and Automation (ICCMA), Luxembourg, 9–12 November
    2022; IEEE: Piscataway, NJ, USA, 2022. [Google Scholar] [CrossRef] Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Abbasi, R.; Martinez, P.; Ahmad, R. Automated
    Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic Systems.
    Agriculture 2023, 13, 615. https://doi.org/10.3390/agriculture13030615 AMA Style
    Abbasi R, Martinez P, Ahmad R. Automated Visual Identification of Foliage Chlorosis
    in Lettuce Grown in Aquaponic Systems. Agriculture. 2023; 13(3):615. https://doi.org/10.3390/agriculture13030615
    Chicago/Turabian Style Abbasi, Rabiya, Pablo Martinez, and Rafiq Ahmad. 2023.
    \"Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic
    Systems\" Agriculture 13, no. 3: 615. https://doi.org/10.3390/agriculture13030615
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   1
    Google Scholar   [click to view] Article Access Statistics Article access statistics
    Article Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28.
    Mar 0 500 1000 1500 2000 2500 For more information on the journal statistics,
    click here. Multiple requests from the same IP address are counted as one view.   Agriculture,
    EISSN 2077-0472, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Agriculture (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in
    Aquaponic Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chitra R.
  - Swetha A.
  - Vishwa M.
  - Hari Haran B.
  citation_count: '0'
  description: Agriculture is the main source of food and fodder. It demands constant
    hard work, attention to detail, responsibility, flexibility, and time to produce
    good yields. The main problems faced by the farmers are many, for example, water
    supply to the crops, farming equipment, dependence on traditional crops, storage
    facilities, etc. The foremost and important issue faced by the farmers is maintaining
    crop health, pest management and weed removal. The main focus is to manage crops
    of the same type like tomato, brinjal and green chili. The pest found in these
    types of plants is also similar. The input images captured from agricultural fields
    are subjected to image processing techniques to detect the presence of pests.
    Once pests are detected, the system classifies the pests into their respective
    types. The accuracy of the pest detection algorithm is 96.44%. The software's
    efficiency and accuracy is a viable tool for farmers and researchers for detection
    and mitigation of crop damage. The images acquired from the fields are processed
    to detect the presence of diseases in the leaves. Once a disease is detected,
    the software categorises the specific type of disease based on a diverse dataset
    of tomato leaf diseases. The accuracy of the disease detection algorithm is 80.67%.
    The system enables farmers to detect and identify plant diseases and respond to
    that promptly, preventing crop loss.
  doi: 10.1109/ICCEBS58601.2023.10448904
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 Intelligent Computing an...
    Enhancing Crop Health Monitoring and Disease Identification in Agriculture Publisher:
    IEEE Cite This PDF R. Chitra; Swetha A; Vishwa M; Hari Haran B All Authors 4 Full
    Text Views Abstract Document Sections I. Introduction II. Literature Review III.
    Methodology IV. Results and Discussion Authors Figures References Keywords Metrics
    Abstract: Agriculture is the main source of food and fodder. It demands constant
    hard work, attention to detail, responsibility, flexibility, and time to produce
    good yields. The main problems faced by the farmers are many, for example, water
    supply to the crops, farming equipment, dependence on traditional crops, storage
    facilities, etc. The foremost and important issue faced by the farmers is maintaining
    crop health, pest management and weed removal. The main focus is to manage crops
    of the same type like tomato, brinjal and green chili. The pest found in these
    types of plants is also similar. The input images captured from agricultural fields
    are subjected to image processing techniques to detect the presence of pests.
    Once pests are detected, the system classifies the pests into their respective
    types. The accuracy of the pest detection algorithm is 96.44%. The software''s
    efficiency and accuracy is a viable tool for farmers and researchers for detection
    and mitigation of crop damage. The images acquired from the fields are processed
    to detect the presence of diseases in the leaves. Once a disease is detected,
    the software categorises the specific type of disease based on a diverse dataset
    of tomato leaf diseases. The accuracy of the disease detection algorithm is 80.67%.
    The system enables farmers to detect and identify plant diseases and respond to
    that promptly, preventing crop loss. Published in: 2023 Intelligent Computing
    and Control for Engineering and Business Systems (ICCEBS) Date of Conference:
    14-15 December 2023 Date Added to IEEE Xplore: 19 March 2024 ISBN Information:
    DOI: 10.1109/ICCEBS58601.2023.10448904 Publisher: IEEE Conference Location: Chennai,
    India SECTION I. Introduction Agriculture is a primary source of livelihood in
    our country. Agriculture greatly impacts the health of the economy and the nation''s
    food security. Agriculture can help reduce poverty, raise incomes and improve
    food security for 80% of the world''s poor, who live in rural areas and work mainly
    in farming. Agriculture is not an easy occupation as the challenges in it grow
    day by day. Some of the challenges being faced are crop yields, agricultural productivity,
    management of crops, time management, pest and weed management. Most of the growing
    and grown crops are subjected to the presence of pests. The pests damage the crops
    and reducecrop productivity and improper detection of presence of crops leads
    to unessential use of harmful pesticides. Hence, the detection and termination
    of pests are mandatory[1]. Among many crops, tomato stands to be a fundamental
    ingredient in diets worldwide which makes its protection and production a top
    priority for farmers. The detection and classification of the type of disease
    aids farmers to protect their crops and optimize yields. It focuses on, A. Detection
    of Identification of Pests The images acquired from agricultural fields are subjected
    to image processing algorithms and the presence and classification of pests within
    the crops are accomplished by the system [2] B. Detection and Identification of
    Leaf Diseases Advanced image processing algorithms are applied on the images to
    improve the image quality and to detect the presence of diseases within the leaves.
    Subsequently, the detected diseases are classified into the specific type of disease
    precisely.[3] SECTION II. Literature Review Harshita Nagar et al. (2021) introduced
    a method for automatic pest detection utilizing Wavelet transformation and Oriented
    FAST and Rotated BRIEF (ORB). The proposed approach is demonstrated on images
    of fluffy caterpillar pests on mustard crop and fava bean crop farms in Rajasthan.
    The Region of Interest is extracted using wavelet transformation and image fusion
    technique.[4] Harshita Nagar et al. (2020) presents the various image processing
    techniques such as feature extraction and automatic detection for the image. The
    survey shows the efficient and simple existing methodologies. Several techniques
    are illustrated here to obtain the knowledge of different background modelling
    for pest detection such as image filtering, median filtering for noise removal,
    image extraction and detection through scanning. It also depicts some promising
    results to present enhanced methods and tools for creating fully automated pest
    identification including the extraction with detection.[5] M. I. Pavel et al.(2019)
    implemented Image processing techniques to detect and classify the affected plant
    disease. In the process, the work is divided into four portions which are image
    acquisition and preprocessing, segmentation of affected regions, feature extraction,
    and classification using a multi-class support vector machine algorithm.[6] Yogesh
    H. Bhosale et al. (2023) addresses the issues on agriculture sector in India,
    facing significant challenges due to pests, diseases, and parasites by introducing
    precision agriculture techniques such as deep machine learning (DML) which is
    used to identify plant leaf diseases and enhance agricultural output. Advantages
    of using DML include precise disease identification, enhanced operational efficiency
    and productivity, as well as the management of data dependencies and variabilities.
    Disadvantages include data dependency and variability.[7] SECTION III. Methodology
    As the backbone of the world economy, agriculture is crucial to supporting populations,
    creating jobs, and maintaining economic stability. As computer vision and image
    processing technology has advanced quickly, creative solutions have arisen to
    deal with critical issues that farmers around the world are currently facing.
    The most important of these difficulties is the prompt identification and control
    of pests and diseases in crop plants, which is essential for maintaining crop
    health and raising agricultural production. In order to do this, the focus of
    our study is on combining image processing methods for pest and leaf disease identification.
    They actively safeguard their crops. The selection of the three crops—Brinjal,
    Tomato, and Green Chillies is deliberate and based on numerous factors. These
    crops are of enormous economic importance because they are staple foods on a worldwide
    scale and the foundation of many agricultural economies. They also show increased
    susceptibility to a variety of pests and diseases, such as bacterial spot, early
    blight, and late blight, as well as armyworms, beetles, bollworms, and grasshoppers.
    As a result, there are issues with food security and yield reductions. In addition,
    data and research materials on brinjal, tomatoes, and green chilies are easily
    accessible, validating image processing models for pest and disease detection.
    Plant pests and pathogens interfere with plant growth and cause damage to cultivated
    and naturally growing plants, significantly reducing crop production. The primary
    method currently employed for pest and disease reduction is pesticide spraying.
    However, the practice has direct and indirect health implications for humans.
    Early-stage pest detection techniques can substantially reduce the need for pesticide
    application. Image detection techniques have emerged as effective tools for combating
    infestations and improving crop management, offering maximum protection to crops
    while minimizing human errors and efforts. The techniques enable automatic monitoring
    over large fields and are particularly focused on detecting leaf diseases during
    the growth and health monitoring phases of crop cultivation. A. Convolutional
    Neural Networks (CNN) An efficient approach involves the implementation of CNN
    to enhance the effectiveness of classification, specifically in the context of
    pest and leaf disease detection applications. The method encompasses fundamental
    steps, commencing with the provision of an image as input. Notably, many object
    detection problems entail training the model with image datasets that include
    bounding box annotations, achieved through techniques such as pooling. Within
    the segmented bounded region, images are cropped and supplied to the classifier
    for predictive analysis. The system effectively identifies and displays the pest
    image output in a dedicated window. Fig 2 provides a visual representation of
    the CNN process. B. Preprocessing Using Keras The Keras ‘ImageDataGenerator’ is
    frequently used to manage data loading and preprocessing chores in image processing.
    Images must be resized to fit the input size, pixel values must be normalised,
    and data augmentation for training may be used. Data batching is necessary for
    effective training. C. Process Involved in CNN The entire process is divided into
    five major phases. The phases are described in following sections whose block
    diagram is given fig 3. 1. Data Collection The dataset used is an accumulation
    of photos of crops containing pests which are common for brinjal, tomato and green
    chillies and disease affected in brinjal leaves in the agricultural fields. The
    focused pests are armyworm, beetle, bollworm, grasshopper, mites, mosquito, sawfly,
    stem borer which is given in Fig 1 and the focused diseases are tomato bacterial
    spot, tomato early blight, tomato late blight, tomato leaf mold, tomato septoria
    leaf spot, tomato spider mites two spotted spider mite, tomato target spot, tomato
    yellow leaf curl virus, tomato mosaic virus which is given in Fig 2 . Fig: 1 Pest
    dataset Show All Fig: 2 Leaf disease dataset Show All 2. Database Normalisation
    Database normalisation enables scalable and secure management of image-related
    data by reducing redundancy, improving data integrity, and optimising query performance.
    For accurate analysis and consistent findings, normalised databases fulfil the
    complex data requirements of pest and leaf disease detection. 3. Feature Extraction
    Images with cluttered backgrounds and little contrast between the pest/leaf and
    its surroundings are a common difficulty in the field of image processing for
    pest and disease identification. It becomes crucial to first extract pertinent
    regions of interest from the photos before using key point detection algorithms.
    The purpose of this preprocessing stage is to isolate and highlight the areas
    that have pests and damaged leaves, which will enhance the upcoming detection
    procedure. 4. Training In the training phase, the extracted features are stored
    in the form of a labeled dataset along with image names. 5. Testing The testing
    is done using the Dynamic Time Warping Algorithm (DTW). DTW is a time series alignment
    algorithm developed originally for speech recognition. It aims at aligning two
    sequences of feature vectors by warping the time axis iteratively until an optimal
    match (according to a suitable metrics) between the two sequences is found. The
    obtained feature descriptor is compared with feature descriptor stored in the
    database by using dynamic time warping algorithm and it is thus checked if the
    leaf has pest and disease on it or not. Fig: 3 Blocks of CNN Show All SECTION
    IV. Results and Discussion 1. A. Result The efficiency and the high accuracy of
    the pest detection model render it an invaluable tool for farmers and researchers.
    The system''s performance goes beyond mere convenience; it plays a pivotal role
    in addressing one of agriculture''s most persistent challenges – pest infestations.
    With an exceptional accuracy rate of 96.44%, our pest detection algorithm stands
    as a beacon of precision. This remarkable accuracy ensures the precise identification
    of pests, allowing farmers to act swiftly and effectively in mitigating potential
    crop damage caused by these harmful organisms. By providing farmers with early
    detection and precise categorization of pests, our system empowers them to proactively
    manage and protect their crops. This capability is particularly critical in modern
    agriculture, where timely interventions can mean the difference between a successful
    harvest and substantial crop loss. The robustness of our pest detection model
    transforms farmers into vigilant stewards of their fields, enabling them to employ
    targeted strategies for pest control. This not only minimizes crop damage but
    also reduces the need for excessive pesticide use, promoting sustainable and environmentally
    friendly farming practices. Furthermore, the system extends its capabilities to
    address another significant concern in agriculture – plant disease detection.
    Images captured from agricultural fields are subjected to thorough analysis to
    identify the presence of diseases in the leaves. Leveraging a diverse dataset
    of tomato leaf diseases, our disease detection algorithm achieves an impressive
    accuracy rate of 80.67%. This level of accuracy enables farmers to promptly detect
    and identify specific plant diseases, facilitating rapid responses to prevent
    crop loss. In essence, the software stands as a comprehensive solution for the
    multifaceted challenges faced by the agricultural community. It not only excels
    in pest detection but also lends a helping hand in the early diagnosis of plant
    diseases. By providing farmers with the tools to identify, categorize, and respond
    to these threats efficiently, our system contributes significantly to increased
    crop yields and the long-term sustainability of agriculture. It exemplifies how
    cutting-edge technology can be harnessed to address age-old challenges, making
    modern farming practices more efficient, precise, and environmentally responsible.
    Fig 4 to 6 shows the detection of pests and figure 6,7 shows the detection of
    leaf diseases in tomato plants. Fig: 4 Detection of bollworm Show All Fig: 5 Detection
    of stem borer Show All Fig: 6 Detection of mites Show All Fig: 7 Detection of
    tomato mosaic virus disease Show All Fig: 8 Detection of tomato target spot Show
    All The accuracy of the pest detection is 96.44% which indicates the effectiveness
    and accuracy of the software to correctly detect the presence of pests and classify
    them based on actual instances of pests in the test datasets. The X axis of the
    graph represents the independent variable, which is the number of iterations or
    training cycles of the algorithm over a period and the Y axis represents the dependent
    variable, which is the accuracy of the pest detection algorithm. Initially, the
    algorithm''s accuracy was quite low, measuring below 20%. This low accuracy might
    be due to several reasons, such as inadequate training data, ineffective feature
    extraction, or an insufficiently trained model. To improve the accuracy of the
    algorithm, multiple iterations of training and refinement were performed. As the
    algorithm underwent several training cycles and refined its internal representation
    of pest detection patterns, its accuracy steadily improved. This gradual increase
    in accuracy is likely reflected in the graph as an upward trend. The algorithm
    managed to achieve an impressive accuracy rate of 96.44%. The accuracy versus
    v-accuracy graph was generated by recording accuracy on the training set and v-accuracy
    on the validation set after each training epoch. This visualization illustrated
    training and validation performance The accuracy of the leaf disease detection
    is 80.67% which represents the ability of the software to precisely identify specific
    leaf diseases. The Y-axis typically shows accuracy values ranging from 0% (completely
    inaccurate) to 100% (perfect accuracy). The values in the upward trend indicates
    improved accuracy while the values in the lower trend represents lower accuracy.
    At the outset of the algorithm''s development, its accuracy was below 30%. This
    low accuracy is due to insufficient training data and inefficient feature extraction.
    As the algorithm underwent multiple iterations, the accuracy gradually started
    to increase contributing to an upward trend. After a series of iterations, the
    algorithm achieved an accuracy rate of 80.67%. This signifies that the algorithm
    has become proficient at detecting tomato leaf diseases, with the majority of
    test cases correctly classified. This graph reflects the algorithm''s potential
    for practical use in the field of plant disease management, with further opportunities
    for optimization and enhancement. Fig: 9 Accuracy graph of pest detection Show
    All Fig: 10 Accuracy graph of leaf disease detection Show All B. Future Enhancement
    An algorithm can be developed to monitor the health and the growth of crops [8],[9],
    in addition to plant disease detection and classification. Factors determining
    the growth and health of crops such as height of a plant, stem width, chlorophyll
    content by multispectral images, density of leaves can be extracted from the acquired
    images and stored on a periodic basis to determine the periodic growth of crops
    which could aid farmers in comprehending the results of their farming practices.
    Authors Figures References Keywords Metrics More Like This Smart Agriculture System
    for Plant Disease Detection and Irrigation Management Using Machine Learning and
    IoT 2023 5th International Conference on Sustainable Technologies for Industry
    5.0 (STI) Published: 2023 Deep Learning for Plant Disease Detection and Crop Yield
    Prediction based on NPP-WPF Analysis in Smart Agriculture 2023 7th International
    Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC) Published:
    2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 Intelligent Computing and Control for Engineering and Business Systems,
    ICCEBS 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Enhancing Crop Health Monitoring and Disease Identification in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Biradar V.S.
  - Al-Jiboory A.K.
  - Sahu G.
  - Tilak Babu S.B.G.
  - Mahender K.
  - Natrayan L.
  citation_count: '0'
  description: Intelligent control systems are a game-changer for robotics and industrial
    automation. This abstract explores how computer vision (CV) and artificial neural
    network (ANN) algorithms may be used to improve automation and robotics in terms
    of efficiency, accuracy, and flexibility. The goal of industrial automation is
    to streamline processes, decrease the need for human interaction, and increase
    output, all of which have progressed dramatically over the years. Integrating
    intelligent control systems that make use of computer vision and artificial neural
    networks is crucial to this transformation. The ability of these systems to detect
    and understand their environments is made possible by computer vision (CV). In
    order to make sound judgements, CV algorithms analyse data captured by cameras
    and other sensors. CV lets robots recognise things, navigate hazardous terrain,
    and carry out precise industrial tasks. CV has become an integral part of industrial
    automation, used for anything from monitoring production quality to navigating
    warehouses autonomously. Artificial neural networks (ANN s) mimic the human brain
    in many ways, including their ability to learn and make decisions on their own.
    ANNs are built from networks of nodes (neurons) that work together to analyse
    and process information. ANN s may learn to identify patterns, refine their control
    settings, and adjust to new circumstances. Predictive maintenance, problem identification,
    and control strategy optimisation are just some of the ways in which ANN s are
    put to use in industrial automation and robotics. Combining CV with ANN algorithms
    makes for a formidable tool with many practical uses in industry. The automated
    examination of produced goods is one significant use. Cracks and imperfections
    are easy targets for CVs, while ANNs can analyse the data in real time to make
    judgements about the product's quality. As a result, we can maintain constant
    quality control, cut down on waste, and boost output. Combining CV with ANN has
    been incredibly useful for robotics in industrial automation. Robots using CV
    systems can accurately pick up and place things from their surroundings without
    human intervention. By allowing robots to learn from their environments, ANNs
    increase their flexibility and usefulness in the workplace. The combination of
    CV with ANNs has improved the viability of 'cobots' in production, in which robots
    and humans work together in harmony. Autonomous navigation is another important
    field where CVs and ANNs excel. AGVs and drones need to be able to efficiently
    handle complicated layouts in large warehouses and factories. Using CV, these
    systems are better able to perceive and map their environments, while ANNs allow
    them to plan ideal courses, avoid obstacles, and adapt to a constantly shifting
    landscape. The advantages of combining CV with ANN go well beyond those of conventional
    industrial automation. For instance, these technologies are used for precision
    farming in the agriculture industry. Increased yields and efficient use of resources
    are the consequence of the combination of CV systems for identifying crop health
    and pest infestations and ANNs for making data-driven decisions regarding irrigation,
    fertiliser application, and harvesting. In conclusion, the advent of a new era
    of intelligent control systems has been heralded by the incorporation of computer
    vision and artificial neural networks into industrial automation and robotics.
    From autonomous navigation to precision agriculture, these technologies improve
    efficiency, accuracy, and flexibility in a variety of fields. The future of industrial
    automation and robotics will be significantly influenced by the complementary
    nature of CVs and ANNs.
  doi: 10.1109/UPCON59197.2023.10434927
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 10th IEEE Uttar Pradesh ...
    Intelligent Control Systems for Industrial Automation and Robotics Publisher:
    IEEE Cite This PDF Vijaykumar S. Biradar; Ali Khudhair Al-Jiboory; Gaurav Sahu;
    S. B G Tilak Babu; Kommabatla Mahender; Natrayan L All Authors 27 Full Text Views
    Abstract Document Sections I. Introduction II. Related Study III. Methodology
    IV. Results and Discussions V. Conclusion Show Full Outline Authors Figures References
    Keywords Metrics Abstract: Intelligent control systems are a game-changer for
    robotics and industrial automation. This abstract explores how computer vision
    (CV) and artificial neural network (ANN) algorithms may be used to improve automation
    and robotics in terms of efficiency, accuracy, and flexibility. The goal of industrial
    automation is to streamline processes, decrease the need for human interaction,
    and increase output, all of which have progressed dramatically over the years.
    Integrating intelligent control systems that make use of computer vision and artificial
    neural networks is crucial to this transformation. The ability of these systems
    to detect and understand their environments is made possible by computer vision
    (CV). In order to make sound judgements, CV algorithms analyse data captured by
    cameras and other sensors. CV lets robots recognise things, navigate hazardous
    terrain, and carry out precise industrial tasks. CV has become an integral part
    of industrial automation, used for anything from monitoring production quality
    to navigating warehouses autonomously. Artificial neural networks (ANN s) mimic
    the human brain in many ways, including their ability to learn and make decisions
    on their own. ANNs are built from networks of nodes (neurons) that work together
    to analyse and process information. ANN s may learn to identify patterns, refine
    their control settings, and adjust to new circumstances. Predictive maintenance,
    problem identification, and control strategy optimisation are just some of the
    ways in which ANN s are put to use in industrial automation and robotics. Combining
    CV with ANN algorithms makes for a formidable tool with many practical uses in
    industry. The automated examination of produced goods is one significant use.
    Cracks and imperfections are easy targets for CVs, while ANNs can analyse the
    data in real time to make judgements about the product''s quality. As a result,
    we can maintain constant quality control, cut down on waste, and boost output.
    Combining C... (Show More) Published in: 2023 10th IEEE Uttar Pradesh Section
    International Conference on Electrical, Electronics and Computer Engineering (UPCON)
    Date of Conference: 01-03 December 2023 Date Added to IEEE Xplore: 26 February
    2024 ISBN Information: ISSN Information: DOI: 10.1109/UPCON59197.2023.10434927
    Publisher: IEEE Conference Location: Gautam Buddha Nagar, India SECTION I. Introduction
    Incorporating cutting-edge technologies like CV and ANN has propelled the rapid
    development of industrial automation and robotics in recent years. Intelligent
    control systems made possible by these advancements have revolutionised the industrial
    sector, vastly improving productivity, accuracy, and malleability across a wide
    range of sectors. This introductory section gives a comprehensive look at the
    significance and effects of CV and ANN algorithms in industrial automation and
    robotics. [15] A. Manufacturing and Production Processes Manufacturing and production
    processes have relied heavily on industrial automation for quite some time. Simplifying
    processes, reducing the need for human involvement, and increasing output have
    long been primary objectives. Rule-based automation systems used to rely on these
    pre-programmed instructions to carry out mundane chores. However, robotics looked
    to provide mechanical aid in completing jobs that were either too risky or too
    precise for human beings to handle alone. However, there were restrictions with
    the conventional automation methods. They were not flexible and, hence, had difficulty
    in unpredictable situations. They weren''t designed to handle shifts in production
    or to make judgements on the fly based on sensory data. The combination of CV
    and ANN has proven to be revolutionary in this regard. B. Using CV to Its Full
    Potential The goal of computer vision, a branch of AI, is to teach computers to
    recognise objects and scenes in their environment. It entails the creation of
    algorithms and methods that enable computers to analyse visual data, draw conclusions,
    and act accordingly. CV systems in the realm of industrial automation and robotics
    rely on cameras and sensors to gather information about their surroundings and
    make decisions based on that information. CV has altered several facets of manufacturing.
    It allows for very precise flaw, anomaly, and quality issue detection using automated
    inspection systems in the industrial sector. Products, their assembly, and their
    conformity to quality requirements may all be evaluated with the use of such systems.
    This has resulted in less need for human inspection of products, which in turn
    has helped keep quality stable and minimised the possibility of human error. Robots
    can already recognise and control items in dynamic, unstructured settings, thanks
    in large part to CV''s function in object identification and tracking. When robots
    are required to navigate complicated layouts and interact with a wide variety
    of things, such a skill is beneficial in the fields of logistics, storage, and
    material handling. C. Artificial Neural Networks (anns) and Their Significance
    Inspired by the intricate neural architecture of the human brain, artificial neural
    networks are a type of machine learning method. Data processing and analysis are
    performed by layers of linked nodes (neurons). ANNs may be trained to learn from
    data, making them flexible and capable of recognising a wide variety of patterns.
    The ability to learn and make decisions is essential for intelligent control systems,
    and ANN s provide this capability in the context of industrial automation and
    robotics. ANNs can analyse sensor data to foresee when machinery may break down,
    making them useful for predictive maintenance. Because of this, preventative maintenance
    may be performed, cutting down on downtime and expenses. Adaptive control techniques
    use ANNs to optimise control settings in response to both incoming data and the
    environment''s dynamic nature. This flexibility is especially useful in sectors
    with highly variable processes, such as chemical manufacturing, where external
    factors can significantly alter product quality. The combination of CV and ANN
    unites the strengths of both fields to improve perception and decision-making.
    While ANNs evaluate the visual data captured by CV systems, they may then identify
    patterns and make educated judgements based on this information. Many innovations
    in industrial automation and robotics can be attributed to the cooperation between
    these two fields. The automated examination of produced goods is one significant
    use. While ANN s analyse the data to decide if the product is up to par, CV s
    can spot flaws and outliers in real time. Integrating these processes makes it
    easier to maintain high standards of quality, cuts down on waste, and boosts output.
    Autonomous navigation is another important field where CVs and ANNs shine. CV
    systems allow robots, drones, and AGVs to see and map their environments, allowing
    them to detect hazards and adapt to changing conditions.[16]. These systems can
    use ANNs to plot out efficient routes, avoid obstacles, and adjust to changing
    conditions. This is especially helpful in big facilities, where swift movement
    around the building is crucial to maximising output. The ability of autonomous
    cars to sense their environments, make split-second judgements, and travel safely
    relies heavily on CV and ANN algorithms. Transportation might be made much more
    effective and secure with the use of this technology. In summary, the use of computer
    vision and artificial neural networks has revolutionised robotics and automation
    in the manufacturing sector. These innovations have increased accuracy, flexibility,
    and productivity across many sectors. As we learn more about CV and ANN, it becomes
    evident that these technologies will have far-reaching effects on the future of
    automation, robotics, and other areas, fostering development and innovation. SECTION
    II. Related Study This study proposes a paradigm for dealing with the difficulties
    encountered while creating automation systems that make use of collaborative robots
    and other devices with a degree of autonomy. These robots attain their extraordinary
    adaptability through the use of online algorithms for detecting and acting. Control
    systems, in order to make use of this modern equipment and algorithm, need to
    be progressively adaptable. In this study, we introduce Sequence Planner (SP),
    a framework for controlling the emerging category of intelligent automation systems
    that aids in the management of both conventional automation tools and autonomous
    machines. SP utilises auxiliary algorithms for control logic synthesis and online
    planning to facilitate the difficult process of designing automation control solutions.[17].
    The Robot Operating System (ROS) has been used to develop SP with plug-in support,
    and it has been applied to a working industrial demonstration. In this presentation,
    we discuss the results of using SP as the control system for this demonstration,
    demonstrating that this method is a suitable way to automate a very versatile
    single-station setup. We expect that our work will serve as a starting point for
    the development of intelligent automation systems, as there is currently no standardised
    approach to automating such systems [1]. Automation and robots have revolutionised
    the automobile industry over the past 50 years, boosting productivity and raising
    standards across the board. The autonomous, electrified, and networked vehicles
    of the future, however, will necessitate a new level of adaptability and intelligence
    in the manufacturing process, particularly in the final assembly phase. [18].
    The need for collaborative and intelligent automation systems during final assembly
    has arisen in response to the growing complexity of goods, industrial processes,
    and logistics networks. Together, sophisticated vision-based control, adaptive
    safety systems, online optimisation and learning algorithms, and networked and
    well-informed human operators will constitute these systems. Transforming the
    present trucking business so that it can develop, deploy, and operate large-scale
    collaborative and intelligent automated systems will be a massive job, though.
    In this article, we discuss the problems that arise throughout the current steps
    of planning and preparing for final assembly, as well as the necessity of these
    steps and some potential remedies. The suggested planning and preparation methods
    are evaluated using an industrial use case at Volvo Trucks built using Sequence
    Planner and ROS2 [2]. The challenges of adapting to digital technology in agriculture
    with the hopes of boosting productivity and competitiveness are discussed. State
    assistance for the digitization of agriculture has led to some triumphs in the
    agricultural industry, but there are still many technological challenges that
    have yet to be overcome. Automating the whole agricultural process, from planting
    seeds to gathering harvested produce, is the current fad in the usage of digital
    technology in this sector. This article summarises the findings from an investigation
    into the robotization and automation of the agro-industrial sector, outlining
    the most promising technological developments in this space. Intelligent control
    information systems'' practicality is examined. The potential use of unmanned
    aerial vehicles is given particular consideration. Despite their obvious benefits,
    agricultural robots are plagued by unrealistic expectations and a host of other
    drawbacks. Concerns about how to legislate the agro-industrial complex''s transition
    to digitalization are discussed. Key strategies for boosting agricultural productivity
    were uncovered through an examination of the agricultural market and associated
    technology. Among these are the education of new professionals in order to create
    and execute cutting-edge technology, the greening of agricultural practices, the
    design and implementation of intelligent systems, and the advancement of robots
    [3]. Qualitative criteria have been developed to map fabric qualities to the optimal
    sewing machine settings for smart sewing machines by observing how machines interact
    with cloth at various speeds. Fuzzy logic inference processes have been included
    in a neural network, enabling the optimisation of membership function outputs
    and, ultimately, self-learning. The method has been successfully applied to the
    creation of smart sewing machines and is now widely used in the garment industry.
    Using the Neuro-Fuzzy model''s feedback closed loop, a system has been proposed
    for intelligent manufacturing in which fabric characteristics can forecast the
    sewability of any fabric, ascertain the smallest change in fabric properties necessary,
    and regulate the stitching of a garment in real time. The technology has been
    put through its paces in a commercial context with positive results. Optimal settings
    were attained over the whole speed range of the sewing machine, accounting for
    the qualities of challenging materials and the operator''s mishandling [4]. SECTION
    III. Methodology Using CV and ANN algorithms, the methodology section describes
    how intelligent control systems were implemented for industrial automation and
    robotics. This section describes in depth the methods, resources, and processes
    that were employed to accomplish the aims of the study or project. A. Data Collection
    and Preprocessing Gathering and preprocessing data is the first stage in creating
    an intelligent control system. Visual data from cameras and sensors, as well as
    historical data pertaining to automated operations, may fall into this category
    in the context of industrial automation and robotics. High-resolution cameras
    and sensors are only two examples of the data-collection equipment used to collect
    data in real time. The gathered information is then “pre-processed” to eliminate
    unwanted elements, rectify any errors, and transform it into a format ready for
    analysis.[19]. B. Computer Vision (cv) Implementation The computer vision (CV)
    subsystem of an intelligent control system processes and makes sense of visual
    information. Specifically, this entails using CV algorithms to glean useful data
    from visual sources. Object identification, tracking, and picture segmentation
    are all typical CV tasks. Algorithm development typically makes use of open-source
    computer vision libraries like OpenCV or deep learning frameworks like TensorFlow
    or PyTorch. To discover and identify things of interest in visual data, object
    identification algorithms like YOLO (You Only Look Once) and Faster R-CNN are
    used. In order to detect things important to the automation job, such as product
    faults or workpiece placements, these algorithms are trained using labeled datasets.
    [20] Segmentation methods are applied to pictures in order to separate out individual
    sections for closer examination. In quality control, where flaws must be accurately
    localized, this can be very helpful. C. Artificial Neural Networks (ann) Implementation
    In order to automate tasks that involve learning and decision-making, intelligent
    control systems rely on ANN algorithms. Different forms of ANNs, such as feedforward
    neural networks, convolutional neural networks (CNNs), and recurrent neural networks
    (RNN s), may be utilized for various tasks. Artificial neural networks (ANN s)
    rely heavily on training datasets to gain the ability to draw meaningful conclusions
    from past data. To anticipate when machinery may break down, ANN s are trained
    using data collected from the equipment''s sensors in predictive maintenance.
    Adjusting network parameters iteratively during training helps reduce prediction
    mistakes. In order to optimize control parameters in real time, ANN s are frequently
    used in adaptive control techniques. Deep Q-networks (DQNs) and other reinforcement
    learning approaches can be used to fine-tune control rules in response to external
    feedback. Intelligent control systems rely heavily on the interplay between CV
    and ANN. CV algorithms analyze visual data to feed ANNs, which in turn makes it
    possible to make decisions based on empirical evidence. Integration entails setting
    up a channel of communication between the two halves, with the goal of having
    ANN s correctly comprehend and respond to CV output. For example, in automated
    inspection, CV finds product flaws and sends that data to ANNs for analysis and
    decision-making. Automatic rejection or acceptance is made possible by ANNs based
    on their assessments of defect severity and compliance with quality requirements.
    D. Testing and Validation Extensive testing and validation are performed on the
    intelligent control system to assure its dependability and accuracy. This encompasses
    both virtual and actual testing environments. It is common practice to assess
    the system''s performance in a variety of scenarios using simulation environments
    like MATLAB/Simulink or ROS (Robot Operating System).During real-world testing,
    the technology is actually implemented in a robotic or industrial environment.
    Adjustments to algorithms and solutions to real-world problems, such as varying
    illumination or noise, might be made during this stage. E. Evaluation and Metrics
    for Success Metrics for measuring the system''s performance are established. A
    few examples of often used metrics are F 1 score, response time, accuracy, and
    recall. The success of the intelligent control system in achieving its goals in
    quality control, predictive maintenance, or autonomous navigation is measured
    using these indicators. Intelligent control systems for industrial automation
    and robotics using CV and ANN algorithms necessitate a methodical approach that
    incorporates data acquisition, preprocessing, algorithm implementation, integration,
    real-time control, testing, and evaluation. The goal of this approach is to use
    CV and ANN to improve the accuracy, precision, and flexibility of manufacturing
    processes, which has far-reaching implications. Equations: Object Detection Probability
    (P _detection): CV+ANNCombination; P − detection=TruePositives/(TruePositives+
    FalseNegatives)− (1) View Source This equation provides a quantitative measure
    for the likelihood of identifying items or abnormalities in a dataset, which may
    be used for quality control purposes. Detections that are true positives are right,
    whereas detections that are false negatives are overlooked. The Reward Function
    (Reward) in Reinforcement Learning: Reward=R(s, a)− (2) View Source The reward
    function in a reinforcement learning scenario specifies the instantaneous payoff
    for an agent (robot or system) performing action an in state s. This incentive
    directs learning towards increasing long-term benefits. SECTION IV. Results and
    Discussions In-depth presentation, analysis, and discussion of the study''s findings
    all go into the “Results and Discussion” portion of a research paper or project
    report. The meat of the study may be found here, including a detailed analysis
    of the study''s data, conclusions, and implications. In this introductory section,
    we outline the aims of the study, emphasize the methods employed, and stress the
    importance of the subsequent discussion. Fig. 1. Proposed System Architecture
    Show All Fig. 2. Accuracy comparison of proposed method with existing algorithms
    Show All In the context of object identification, image classification, and other
    machine learning tasks, accuracy is a popular performance parameter used to quantify
    the overall correctness of a classification or prediction system. Accuracy is
    determined by contrasting the fraction of training data that was properly categorised
    with the total number of training data instances. As shown in figure 4.1,the graph
    shows that accuracy comparison between the proposed method with other existing
    methods. Here the accuracy rate of LSTM is 75%,Support Vetor Machine has 86% followed
    by K-Nearest neighbour records 89%.Finally our proposed method records the accuracy
    of95%. The F 1 Score takes into account both the precision and recall of a model.
    When one class greatly outnumbers the other in a dataset, this method comes in
    quite handy. The Fl Score is a numeric value between 0 and 1 that is the harmonic
    mean of the recall and accuracy scores.Here the F 1 Score is measured for LSTM
    is 65%,followed by the Support Vector Machine is 69%,KNN have recorded 74% and
    finally our proposed method has 78%. Sensitivity, also known as Recall or True
    Positive Rate, is a metric for evaluating a model''s efficacy in properly identifying
    positive events. When false negatives might have serious effects, like in medical
    diagnostics or safety-critical applications, this is of paramount importance The
    Recall value of LSTM is 73%,where Support Vector Machine is 84%,followed by the
    K-Nearest Neighbour has 87% and Finally our Proposed method is 91 %. Figure 4.1,
    shows the Response time comparison of the proposed method with other existing
    algorithms. Here our Proposed Method has more faster in responding. SECTION V.
    Conclusion Finally, a new age of intelligent control systems has begun with the
    use of Computer Vision (CV) and Artificial Neural Networks (ANN) in industrial
    automation and robotics. The convergence of perception and reasoning has resulted
    in revolutionary improvements in productivity, accuracy, and flexibility in a
    wide range of fields. Automated systems now have extraordinary perceptual and
    interpretive acuity thanks to the widespread use of CV algorithms for object identification,
    tracking, and picture processing. The learning and decision-making skills afforded
    by ANN s, on the other hand, have allowed systems to respond to changing situations,
    fine-tune their controls, and improve their overall performance. CV and ANN algorithms
    have had a significant influence in many areas, from quality control in manufacturing
    to autonomous navigation in warehouses. The use of these innovations has decreased
    the need for human involvement while simultaneously raising product quality, output,
    and security. In addition, its use may be seen in other areas, such as precision
    agriculture and healthcare, where it is transforming procedures and broadening
    the scope of automation. Industrial automation and robotics will continue to benefit
    from the synergy between CV and ANN as technology develops. It is certain that
    these intelligent control systems will continue to develop, providing new answers
    to difficult problems in industry and paving the way for more complicated uses
    down the road. The future looks bright for advancing automation''s role in increasing
    efficiency, quality, and sustainability across many different industrial areas.
    Fig. 3. Response time comparison Show All Table I. Performance metrices comparison
    SECTION VI. Future Work The goal of future research in this area should be to
    further automate and generalize the capabilities of artificial neural networks
    and computer vision. Improving human-robot cooperation involves, among other things,
    creating more reliable and real-time CV algorithms, optimizing ANN topologies
    for specific industrial applications, and so on. Improving the interpretability
    and explain ability of AI-driven judgments and investigating the possibilities
    of edge computing for on-device processing are also important. To guarantee the
    safe and responsible use of these technologies, additional study is needed into
    the cybersecurity elements of intelligent control systems as well as the ethical
    considerations associated with automation in sensitive fields. Authors Figures
    References Keywords Metrics More Like This Intelligent control of robot arm using
    artificial neural networks Proceedings of 8th Mediterranean Electrotechnical Conference
    on Industrial Applications in Power Systems, Computer Science and Telecommunications
    (MELECON 96) Published: 1996 Mobile Robot Navigation Control in Moving Obstacle
    Environment Using Genetic Algorithm, Artificial Neural Networks and A* Algorithm
    2009 WRI World Congress on Computer Science and Information Engineering Published:
    2009 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering, UPCON 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Intelligent Control Systems for Industrial Automation and Robotics
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Moses D.
  - Kumar T.P.
  - Varalakshmi S.
  - Pamulaparty L.
  citation_count: '0'
  description: In this proposal, we study the advances of major core technologies
    and their applicability in creating an Intelligent farming System (IFS). As the
    world is trending into new technologies and implementations it is a necessary
    goal to trend up with agriculture also. Cyber Physical System (CPS) plays a very
    important role in Smart Farming. IOT sensors are capable of providing information
    about agriculture or Farming fields. We have proposed a Cyber Physical System
    (CPS) enabled smart agriculture system using different technologies like AI&ML,
    Data Science and Cloud Computing. This CPS based Intelligent Farming system makes
    use of sensor networks that collects data from different sensors which as a result
    develop an Intelligent Village Farming. Several Utilities such as Pest management,
    Crop Stress management, Nutrient management, Water management and Deep Analysis
    can be done to suggest the farmer regarding the crop and climatic conditions.
    This smart agriculture or Smart Farming using Cyber Physical System (CPS) is powered
    by advances in sensor technology, wireless communication technologies and their
    applicability to farming Chatbot, Computer vision, technology enabling farming,
    it consists of sensor followed by technological techniques.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 14th International Conference on Advances in Computing, Control, and Telecommunication
    Technologies, ACT 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Cyber Physical System Enabled Intelligent Farming System with Artificial
    Intelligence, Machine Learning and Cloud Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Arockia Doss A.S.
  - Jeyabalan A.
  - Borah P.R.
  - Lingampally P.K.
  - Schilberg I.D.
  citation_count: '0'
  description: This paper discusses about the modern techniques implemented in the
    field of agriculture which shaped the traditional farming to the smart farming
    (Agriculture 4.0). The rapid rise in the global population is one among the main
    reasons, besides monitoring of crops health and yield requires a huge labor force,
    which is also another reason for promoting intelligent systems into agriculture
    sector. Conventional farming methods are not suitable to meet this demand, which
    led robotics to associate with on-field agriculture by means of robot-based technologies
    like wheeled robots, ground vehicles (manned and unmanned) and aerial vehicles
    (manned and unmanned) that led to explore the possible advancement in agriculture
    3.0. Currently, the evolutionary techniques, such as Artificial Intelligence (AI)
    and IoT, are implemented in robotic vehicles to make them intelligent systems.
    Due to unprecedented climatic change and polluted ground water for the past few
    decades, the crops are being infested with new varieties of diseases. This requires
    new image processing techniques to classify the diseases based on color, texture
    and, shape of leaves. The incorporation of image processing technique into AI
    aids in deciding the appropriate amount of herbicide supplement to the plant based
    on the prediction of plant growth.
  doi: 10.1142/S0219843623500123
  full_citation: '>'
  full_text: '>

    "brought to you by UNIVERSITY OF NEBRASKA-LINCOLN Search My Cart Sign in    Institutional
    Access Skip main navigation Subject Journals Books Major Reference Works Resources
    For Partners Open Access About Us Help Cookies Notification We use cookies on
    this site to enhance your user experience. By continuing to browse the site, you
    consent to the use of our cookies. Learn More ×   International Journal of Humanoid
    RoboticsOnline Ready No Access Advancements in Agricultural Automation: A Comprehensive
    Review of Artificial Intelligence and Humanoid Robotics in Farming Arockia Selvakumar
    Arockia Doss , Abarna Jeyabalan , Priti Rekha Borah , Pavan Kalyan Lingampally
    , and Ing. Daniel Schilberg https://doi.org/10.1142/S0219843623500123Cited by:0
    (Source: Crossref) Previous Next PDF/EPUB Tools Share Cite Recommend To Library
    Abstract This paper discusses about the modern techniques implemented in the field
    of agriculture which shaped the traditional farming to the smart farming (Agriculture
    4.0). The rapid rise in the global population is one among the main reasons, besides
    monitoring of crops health and yield requires a huge labor force, which is also
    another reason for promoting intelligent systems into agriculture sector. Conventional
    farming methods are not suitable to meet this demand, which led robotics to associate
    with on-field agriculture by means of robot-based technologies like wheeled robots,
    ground vehicles (manned and unmanned) and aerial vehicles (manned and unmanned)
    that led to explore the possible advancement in agriculture 3.0. Currently, the
    evolutionary techniques, such as Artificial Intelligence (AI) and IoT, are implemented
    in robotic vehicles to make them intelligent systems. Due to unprecedented climatic
    change and polluted ground water for the past few decades, the crops are being
    infested with new varieties of diseases. This requires new image processing techniques
    to classify the diseases based on color, texture and, shape of leaves. The incorporation
    of image processing technique into AI aids in deciding the appropriate amount
    of herbicide supplement to the plant based on the prediction of plant growth.
    Keywords: Unmanned robotic systemagricultural roboticssmart farmingartificial
    intelligenceimage processing Remember to check out the Most Cited Articles! Check
    out these Notable Titles in Robotics We recommend Book Series: New Frontiers in
    Robotics Shoudong Huang et al., World Scientific Book Intelligent Control Techniques
    for Robotic Contact Tasks World Scientific Book The Application of Interactive
    Humanoid Robots in the History Education of Museums Under Artificial Intelligence
    Kuan Yang et al., International Journal of Humanoid Robotics, 2022 SOFTWARE AND
    COMMUNICATION INFRASTRUCTURE DESIGN OF THE HUMANOID ROBOT RH-1 World Scientific
    Book HUMANOID ROBOTICS RESEARCH IN IS/AIST World Scientific Book Humanoid robots
    to take centre stage at UN meet on AI TechXplore.com, 2023 The appearance of robots
    affects our perception of the morality of their decisions by University of Helsinki,
    TechXplore.com, 2021 AI ''good for the world''... says ultra-lifelike robot Phys.org
    Team programs a humanoid robot to communicate in sign language Phys.org, 2019
    Robot preachers, AI programs may undermine credibility of religious groups, study
    finds by American Psychological Association, MedicalXpress, 2023 Powered by Figures
    References Related Details Online Ready Metrics Downloaded 4 times History Received
    12 October 2022 Revised 26 June 2023 Accepted 19 July 2023 Published: 26 September
    2023 Keywords Unmanned robotic system agricultural robotics smart farming artificial
    intelligence image processing PDF download Resources For Authors For Booksellers
    For Librarians Copyright & Permissions Translation Rights How to Order Contact
    Us Sitemap    About Us & Help About Us News Author Services Help Links World Scientific
    Europe World Scientific China 世界科技 WS Education (K-12) Global Publishing 八方文化
    Asia-Pacific Biotech News World Century Privacy policy © 2024 World Scientific
    Publishing Co Pte Ltd Powered by Atypon® Literatum"'
  inline_citation: '>'
  journal: International Journal of Humanoid Robotics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Advancements in Agricultural Automation: A Comprehensive Review of Artificial
    Intelligence and Humanoid Robotics in Farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gurunathan K.
  - Bharathkumar V.
  - Meeran M.H.A.
  - Hariprasath K.
  - Jidendiran R.
  citation_count: '2'
  description: The recognition of hybrid fruits by humans is considered a challenging
    task as fruits exist in various colors, sizes, shapes, and textures. In the marketplace,
    where prediction of fruits, vegetables, and pulses by retailers and the general
    public is very difficult. In this paper, we have recognised two different classes
    of fruits, vegetables, and pulses and categorised them based on the stages of
    their existence and maturity in a procedure called fruit maturity classification.
    Crop distribution, good fruit counts, crop harvesting, crop disease detection,
    weed management, and production forecasting are all aspects of managing water
    and soil and are just a few of the smart agricultural applications that employ
    robust learning (DL). The finest deep learning is what this project seeks to produce
    in algorithms for estimating fruit quality and ripeness in order to forecast fruit
    shelf life. Manual vegetable and fruit detection is a hard process when done in
    large numbers, but it becomes easy when done in small amounts. Automated detection
    of these is thus used. Fruit, crop, and pulse images served as the input for the
    first stage of processing, which included detection. Background removal, extraction
    of colour and texture properties, and categorization comprised the three steps
    of the process. The K-means clustering method was used for background subtraction.
    Statistical attributes were used to pinpoint colour characteristics. This research
    proposes a simple and effective method for detecting fruits and predicting their
    nutrition information using deep Alex networks (DAN). The datasets used in the
    investigation were obtained from the Fruit 360 library of image processing problems.
    Apples, berries, bananas, grapes, papaya, peaches, avocados, and various apple
    tastes are among the fruit groups. The trials are also carried out on a variety
    of additional fruit samples gathered from various Web archives. The network design
    consists of three fully linked layers, including the max pooling and RELU levels,
    and five convolution layers. 227∗227∗3 photos should be used as input for 96 filters
    that are 11∗11∗3 and have a stride length of 4.
  doi: 10.1109/ICBSII58188.2023.10181087
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    Classification Of Cultivars Employing The Alexnet Technique Using Deep Learning
    Publisher: IEEE Cite This PDF K. Gurunathan; V. Bharathkumar; M.Haji Ali Meeran;
    K. Hariprasath; R. Jidendiran All Authors 2 Cites in Papers 68 Full Text Views
    Abstract Document Sections I. Introduction II. Related Work III. Existing System
    IV. Proposed Model V. Experiment Results Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: The recognition of hybrid fruits by humans
    is considered a challenging task as fruits exist in various colors, sizes, shapes,
    and textures. In the marketplace, where prediction of fruits, vegetables, and
    pulses by retailers and the general public is very difficult. In this paper, we
    have recognised two different classes of fruits, vegetables, and pulses and categorised
    them based on the stages of their existence and maturity in a procedure called
    fruit maturity classification. Crop distribution, good fruit counts, crop harvesting,
    crop disease detection, weed management, and production forecasting are all aspects
    of managing water and soil and are just a few of the smart agricultural applications
    that employ robust learning (DL). The finest deep learning is what this project
    seeks to produce in algorithms for estimating fruit quality and ripeness in order
    to forecast fruit shelf life. Manual vegetable and fruit detection is a hard process
    when done in large numbers, but it becomes easy when done in small amounts. Automated
    detection of these is thus used. Fruit, crop, and pulse images served as the input
    for the first stage of processing, which included detection. Background removal,
    extraction of colour and texture properties, and categorization comprised the
    three steps of the process. The K-means clustering method was used for background
    subtraction. Statistical attributes were used to pinpoint colour characteristics.
    This research proposes a simple and effective method for detecting fruits and
    predicting their nutrition information using deep Alex networks (DAN). The datasets
    used in the investigation were obtained from the Fruit 360 library of image processing
    problems. Apples, berries, bananas, grapes, papaya, peaches, avocados, and various
    apple tastes are among the fruit groups. The trials are also carried out on a
    variety of additional fruit samples gathered from various Web archives. The network
    design consists of three fully linked layers, including the m... (Show More) Published
    in: 2023 International Conference on Bio Signals, Images, and Instrumentation
    (ICBSII) Date of Conference: 16-17 March 2023 Date Added to IEEE Xplore: 18 July
    2023 ISBN Information: ISSN Information: DOI: 10.1109/ICBSII58188.2023.10181087
    Publisher: IEEE Conference Location: Chennai, India SECTION I. Introduction Fruit,
    crop, and vegetable grading is necessary yet time-consuming given the significance
    of food in our everyday lives. Computerized methods of automatically grading are
    seen as the answer to this issue, reducing the need for human work. There is some
    evidence to suggest that as fruit ages, it goes through a series of metabolic
    changes that alter its physical characteristics and chemical make-up, including
    its nutritional content. There are two categories of fruit grading techniques:
    non-visual and visual. The main non-visual grading criteria are chemistry, scent,
    and tactile perception. Only when the fruit is still clinging to the tree during
    development does it reach maturity, which is evidenced by a stop in cell division
    and a buildup of dry material. Along the postharvest value chain, the quality
    of all fruits and vegetables is significantly impacted by their ripeness at harvest.
    An efficient and effective automated model that can recognise and categorise the
    fruits based on their maturity degree in a short amount of time is desperately
    needed. Big data technologies and highly effective computers have given birth
    to DL technology, opening up crop management and crop harvesting prospects in
    the context of agricultural activities. SECTION II. Related Work In [1], they
    have identified nine distinct fruit classes. Fruit picture datasets may be found
    online, and some photographs can be found simply by utilising a phone’s camera.
    Pre-processing was used on these images to crop out the background and isolate
    the blob that represents the fruit. Fruits are shown, and their visual characteristics
    are captured through combinations of colour, shape, and texture factors. Multiclass
    SVM and KNN classifiers provide additional input for these feature datasets. The
    colour image is first converted to grayscale via GLCM (Gray Level Concurrence
    Matrix). The image is then changed to a binary version. The biggest blob or item,
    which would also be regarded as fruit, is extracted from the picture using morphological
    processes, which are also utilised to fill in the gaps in the image. After cropping
    the largest lump, the binary numbers are reset to their original intensity levels.
    According to the research, combining colour, texture, and form produces outcomes
    that are superior to or on par with those obtained when using any two feature
    categories. The second inference that can be made is that KNN outperforms SVM
    in this scenario. In [2], there are various stages of the training process, which
    include the following: Collecting fruit images is the first step. Then, using
    the FCH and MI methods, feature extraction is used to extract the fruit’s characteristics,
    which are then converted into vector feature forms that can be stored in databases.
    On the vector of the database’s image of fruits, a later clustering method called
    K Means Clustering is used to carry out the procedure. The following actions were
    taken throughout this study’s testing phase: Find the fruit by opening the file
    picture. The next step is to extract the features from the facial picture, which
    are then converted into a vector feature form using the same training process.
    Following that, the Euclidian distance between the new fruit picture features
    and features already present in the database was calculated as part of the recognition
    process using the KNN approach, and the results were compared with the clustering
    findings. In [3], Zhang et al. (2015) used a high-end dual camera setup to collect
    both visible (RGB) and infrared (IR) pictures. They gathered 1088 RGB + IR-matched
    pictures from six different sources. They named this dataset VAIS and made it
    available to the public for use. Their goal for employing infrared imaging is
    to improve nighttime performance. They used SIFT characteristics to train VGG-16
    and Gnostic Fields. They achieved 87.4 percent daytime accuracy and 61.0 percent
    nighttime accuracy using those classifiers in combination. Fruit identification
    was demonstrated by Patel, Jain, and Joshi [4] using an enhanced multiple feature-based
    approach. Effective feature extraction is trained into an image processing method
    in order to identify the fruit. The algorithm’s design is to determine various
    weights for the input test image’s properties, such as intensity, colour, orientation,
    and edge. The approaches for fruit processing’s sorting and grading were introduced
    by Nagganaur and Sannanki [5]. The machine begins the procedure by taking a picture
    of the fruit. Following that, the picture is sent to Matlab for feature extraction,
    categorization, and grading, all of which are accomplished using a fuzzy logic
    technique. The literature review contains several recognition and classification
    systems that may automatically examine the fruits for illnesses, a maturity phase,
    category recognition, etc. The approach taken by [6] to categorise the bananas
    using the CIE Lab and hue channel The fuzzy parameters were modified as part of
    the particle swarm optimization (PSO) process. [7] classified fruits using a kernel
    support vector machine with several classes (KSVM). SVMs were trained using the
    reduced feature vector and 5-fold stratified cross-validation. During the categorization
    process, a mix of color, texture, and form characteristics were employed. A unique
    multi-layered feed-forward unsupervised neural network called a convolutional
    neural network (CNN) was created to handle picture categorization. The feature
    extraction layer is another name for the convolution layer of a convolutional
    neural network (CNN). SECTION III. Existing System The CNN algorithm was used
    by the authors to distinguish between yellowish-green, unripe, medium, and ripe
    bananas. Before submitting them to training, image noise was removed using a bilateral
    filter, and for variants, data augmentation was used. In terms of accuracy (96.18%)
    and execution time, the recommended model surpassed NASNet Mobile. The authors
    experimented with various CNN hyper-parameters to sort ripe Medjool dates using
    CNN from scratch architectures, ResNet50, ResNet101, ResNet152, VGG16, VGG19,
    InceptionV3, etc. The effectiveness of CNN architectures in classifying the maturity
    of Medjool dates was rated in terms of accuracy and processing speed. The Adam
    optimizer’s 0.01 learning rate with 128 batch sizes gave the VGG19 model a maximum
    accuracy of 99.32%. SECTION IV. Proposed Model An example of a deep learning system
    is a CNN, which is made up of neurons and uses trainable weights and biases to
    classify incoming images. A CNN may comprise tens or even hundreds of layers,
    each of which may be trained to recognise certain aspects of an image. A CNN may
    automatically and adaptively learn spatial hierarchies of information by using
    convolutional layers. For example, pooling layers and completely linking layers
    are a few of the building blocks. The output of each training picture’s consolation
    is used as the input for the next layer after each image is convolved using a
    range of resolution filters. With the input picture size set to 112*112*3 for
    the supplemented dataset and 227*227*3 for the original dataset, this study uses
    three convolutional layers with two max-pooling layers. The loss function used
    is the cross-entropy function. while Adam is selected as the optimizer. so that
    weight and offset modifications may be made more steadily thanks to Adam’s method.
    To balance training and validation accuracy and loss, a 20% dropout was employed.
    The output layer uses the softmax activation function as a last step. Fig 1: CNN
    ARCHITECTURE Show All A. AlexNet The AlexNet architecture is divided into eight
    layers, three of which are completely connected and five of which are convolutional.
    The convolution layer, which is the foundational component of the network, constitutes
    the top layer. The first layer of AlexNet’s convolution window is 11 by 11. Objects
    in ImageNet data frequently occupy pixels that are times wider and higher because
    ImageNet photographs are eight times larger than MNIST photographs.and include
    more visual information. We need a larger convolution window to catch the item.
    Electronics 4100, 2022 The convolution window shapes of 8 of the 13 are altered
    to 5*5 and then 3*3. Fig 2: ALEXNET CLASSIFICATION Show All The final convolutional
    layer is immediately followed by two gigantic, fully connected layers with a total
    of 4096 outputs. The input picture, which has pixel values of 227 for width and
    227 for height, is transferred to the input layer along with further 3D colours
    that are RGB-saved images. The concealed layers are then transmitted with the
    image, where it is processed before moving on to the fully connected layers, which
    include convolutional layers of different filters, and lastly to the layer of
    output. The first complicated layer is reached after the input, and the picture
    is transmitted. is reduced in size, depending on whether padding is there or not.
    The pooling layer’s input image will be 55*55*96 in size since the first layer’s
    filter size is 96. The two formulas stated above are used to determine the values
    for each input size. When using the supplemented dataset, the picture size is
    decreased to 112 by 112 pixels since training a model with a large dataset consumes
    more RAM than is available. As a result, by scaling down the image to “112,\"
    the AlexNet model is trained, verified, and tested. Fig 3: PROPOSED FRAMEWORK
    Show All B. Preprocessing To extract the image’s features, we used three different
    learning algorithms. Preprocessing is followed by image resizing and rgb to grey
    conversion, and three learning algorithms are available: preprocessing followed
    by SVM, bag of features, and custom-trained convolutional neural networks using
    transfer learning. C. Segmentation A method for segmenting a collection of data
    into a preset number of groups is called “K-mean segmentation clustering.\" The
    most widely used method is k, which stands for clustering.It divides a group of
    items using the K-means clustering algorithm. It creates k separate clusters from
    a given amount of data. The K-means algorithm consists of two sections. After
    computing the k centroid in the first phase, each data point is assigned to the
    cluster with the centroid that is closest to it in the second phase data into
    a group of k numbers of data. D. Hybrid Method CNN with SVM Hybrid fruit picture
    categorization using SVM (support vector machines). A hybrid CNN-SVM model is
    recommended for the Maraval dataset’s ship categorization. The recommended method
    offers the best of both worlds by combining SVM and CNN classifiers. Convolutional
    neural networks (CNNs), which are used for supervised learning, consist of multiple
    completely connected layers.. CNN is able to learn invariant local properties
    and functions similarly to humans. It can extract the most discriminating information
    from unprocessed ship photos. The suggested method extracts the most recognisable
    characteristics from the raw input photos using a 5x5 kernel/filter. The mm filter
    in the convolutional layer is convolved with the cnn input neurons in the input
    layer. E. Tensorflow Open source software for numerical computations is called
    Tensor flow. It was initially intended to be used for machine learning and deep
    neural network research. Users who wish to employ neural networks in various scenarios
    may find neural network topologies in Tensor Flow along with retraining scripts.
    Fig 4: HYBRID FRUIT IMAGES Show All F. Keras To prepare, model, evaluate, and
    optimise neural networks, one uses the open-source Keras Python-based neural network
    library. It has the ability to run on top of TensorFlow. Given that the backend
    is responsible for high-level APIs utilise it for management.. It is designed
    for both the training process with a fit function and the development of a model.
    It is intended for low-level computing using tensors or TensorFlow and backend
    convolution. Preprocessing, modelling, optimization, testing and presentation
    python libraries imported. SECTION V. Experiment Results The deep learning framework
    PyTorch has an inbuilt tensor data format (a multidimensional tensor). In this
    paper, MobileNet was applied to the Fruit Dataset to determine how well the network
    performs at classification. The Fruits dataset was used to generate these 1260
    images, which are divided into 7 categories: 15% of these images are used to test
    the model, and 85% are used for training and overall 98.74%. The network is trained
    using a 14-batch size across 10 epochs. The results demonstrate that the suggested
    paradigm operates well. when compared to established models and show promise for
    use in practical situations. This type of increased precision and accuracy will
    help increase the machine’s overall fruit recognition effectiveness. Fig 5: UPLOADING
    IMAGES Show All Fig 6: PREDICTION Show All SECTION VI. Conclusion The fruit may
    be identified by properties like form, colour, and texture in the suggested project.
    This expands people’s understanding of certain uncommon and unusual fruits. The
    project’s major focus is on minimising human effort and simplifying human existence.
    Fruit identification might decrease the continuous issues that are present. It
    reduces misunderstandings about the particular fruit. Future work that might be
    done on this project includes the development of a web application. This software
    is available to users 24/7, from any location. Authors Figures References Citations
    Keywords Metrics More Like This Technical analysis of crop production prediction
    using Machine Learning and Deep Learning Algorithms 2022 International Conference
    on Innovative Computing, Intelligent Communication and Smart Electrical Systems
    (ICSES) Published: 2022 A Deep Learning Approach for Yield Estimation and Phenotype
    Analysis in Rice Crops 2021 International Conference on Advancements in Electrical,
    Electronics, Communication, Computing and Automation (ICAECA) Published: 2021
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the 9th International Conference on Biosignals, Images,
    and Instrumentation, ICBSII 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Classification of Cultivars Employing the Alexnet Technique Using Deep Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gohad P.R.
  - Khan S.S.
  citation_count: '0'
  description: Crop health has been an important aspect of farming as the produce
    and income depend on it. Large amount of research work has been carried out in
    crop health diagnosis. This paper suggests the using thermal imaging for identifying
    the occurrence of disease on the crop by using convolutional neural networks approach.
    Thermal images are initially preprocessed using Thresholding and morphological
    operations. Furthermore, group convolution strategy is used to train the model
    with the use of a number of kernels for processing the data.
  doi: 10.1109/ICRAIE52900.2021.9703903
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2021 6th IEEE International C...
    Diagnosis of leaf health using grape leaf thermal imaging and convolutional neural
    networks Publisher: IEEE Cite This PDF Poonam R. Gohad; S.S. Khan All Authors
    68 Full Text Views Abstract Document Sections I. Introduction II. Thermal Image
    in Agriculture III. Literature Survey IV. Crop Disease Detection IV. Proposed
    System Show Full Outline Authors Figures References Keywords Metrics Abstract:
    Crop health has been an important aspect of farming as the produce and income
    depend on it. Large amount of research work has been carried out in crop health
    diagnosis. This paper suggests the using thermal imaging for identifying the occurrence
    of disease on the crop by using convolutional neural networks approach. Thermal
    images are initially preprocessed using Thresholding and morphological operations.
    Furthermore, group convolution strategy is used to train the model with the use
    of a number of kernels for processing the data. Published in: 2021 6th IEEE International
    Conference on Recent Advances and Innovations in Engineering (ICRAIE) Date of
    Conference: 01-03 December 2021 Date Added to IEEE Xplore: 14 February 2022 ISBN
    Information: DOI: 10.1109/ICRAIE52900.2021.9703903 Publisher: IEEE Conference
    Location: Kedah, Malaysia SECTION I. Introduction Infrared thermal imaging has
    several uses in both the medical and industrial fields. Because of the excellent
    resolution of thermal picture, modern infrared cameras have many applications.
    Thermal cameras with high speed, resolution improved, image quality as well as
    accuracy. The use of image processing methods on thermal pictures has opened up
    new possibilities for assessing the health of various pieces of equipment [1].
    Picture processing can help with noise reduction and image enhancement. Image
    segmentation aids in the rapid detection of hot and cool spots in thermal images.
    The temperatures distribution in images can be utilised to identify equipment
    malfunction conditions. Different applications in the industrial field result
    from advancements in thermal image recording and image processing, such as hot
    spot detection in bearing health monitoring, electrical equipment, heat loss evaluation,
    overheating identification and so on. Infrared cameras can obtain thermal images,
    which is a non-contact technology that absorbs the infrared radiation released
    by the object. The number of elements in the detector determines the image resolution
    [2]. Single element detectors were employed in the first generation cameras, which
    were cooled to cryogenic temperatures. Infrared Focal Plane Arrays are utilised
    in current thermal imaging cameras to create a two-dimensional image from a two-dimensional
    array of detectors without the usage of a scanning motor. The number of elements
    in detectors can be increased to improve image quality. The primary distinction
    between visible as well as infrared images is that visible images depict light
    reflected from the scene, but infrared rays emitted by the scene or object are
    collected by infrared cameras. The visible image camera captures the object''s
    reflected visible light and transforms it to digital form. The pixel is the smallest
    unit of a computer picture. Each pixel in the viewable picture has three values,
    which reflect the Red, Green, and Blue components'' values. The temperature pattern
    of an item can be seen in an infrared picture. Infrared radiation is converted
    into a false colour image by an infrared camera. The agricultural sector is the
    backbone of the Indian economy. Traditional methods for determining agricultural
    factors are reliable; nevertheless, they are time-consuming, entail more labour,
    and are limited to small areas. Thermal remote sensing provides more exact information
    for parameter representation than traditional approaches since it provides continuous
    aerial scope across a large area at a specific time interval. In the late 1960s,
    Texas Instruments began producing thermal imaging for military uses. It''s a technique
    for creating a picture of or finding an object by exploiting the heat radiated
    by it. These images can be obtained with the help of small, handheld sensors as
    well as heat sensors fixed on a satellite or a flying machine. The quality of
    an object''s radiation is determined by its surface temperature. SECTION II. Thermal
    Image in Agriculture Agriculture in India has always played a pivotal role in
    the country''s economy. Large number of farmers suffer losses due to disease occurrences
    in their crops causing the wastage in time, money and energy. This could be restrained
    to an extent if detected at an earlier stage. Hence, Thermal imaging can be used
    in farms as it helps in mapping the changes invisible to human eyes. In most image
    studies where operations need to be carried out, the objects need to be divided
    from the image individually, so the further the details of those objects will
    be transformed in a proper structure for processing by computer. For many computer
    vision based algorithms as well as image processing, segmentation of image is
    a highly important step. It is observed that over a large spectrum of topics it
    has applications [3]. Some examples are analyzing the various regions of a remote
    sensed photo for understanding land or plant distribution. The object of interest
    extraction from the image''s background is important in order to build intelligent
    machines like factory automation systems. The quality of the vegetation reflectance
    spectrum is determined by the attributes of the leaves; however, the measurement
    of reflected energy for a wavelength is determined by the thickness and pigmentation
    of the leaves. [4]. Yield mapping, crop maturity, field tile mapping, Irrigation
    scheduling and soil properties mapping, agricultural plastic waste estimate, crop
    residue cover, and tillage mapping and plant disease detection are the seven parts
    of thermal imaging in agricultural applications. [5]. Soil moisture sensing, evapotranspiration,
    crop water stress monitoring, and drought stress monitoring are types of irrigation
    scheduling. In numerous publications and books, the challenges and possibilities
    of satellite and UAV (Unmanned Aerial Vehicle) based agricultural applications
    have been discussed. SECTION III. Literature Survey At the early stage, segmentation
    methods began in the 70s, which was based on thresholding gradients and histograms.
    These techniques transformed a grayscale image into a binary image and, for others,
    on the presumption that only two types of pixels are included. The goal is to
    find an optimum threshold for separating the groups of foreground and background.
    K-means is algorithm which partitions the data in to ‘k’ clusters. It requires
    a parameter that represents number of clusters. Before going to the cluster analysis
    it is necessary to identify the parameter value apriori which has to be fixed.
    Gavhale et al. [6], proposed framework where model is divided into different parts
    of image processing like colour space conversion from RGB to other, enhancement
    of image, region of interest segmentation with K means clustering algorithm to
    determine the defect as well as identify plant leaves areas severity, extraction
    of features and classification. The texture features are extracted using the statistical
    method of GLCM and color feature using mean values. Basic clustering k-means algorithm
    is used for segmentation in textured images. Considering some problems with K-Means
    to overcome them, s algorithm. The results of the algorithm proposed are compared
    with the M Kass snake model. Considering thermal images, they help to spot the
    invisible pattern in object using infrared light [6]. Certain threshold-based
    segmentation techniques have been experimented and applied for successful disease
    identification [7]. An approach with machine learning by combining thermal image
    data as well as visible light images which provide higher precision for improving
    remote images use. Similar methods can be applied to plants with other diseases
    as well which consisted of steps like image registration, depth estimation, feature
    extraction and classification. [8]. Features such as temperature were extracted
    using the software by flir while texture features using GLCM. The probability
    density functions and summary statistics help to analyze variation in these features.
    The results indicate according to statistics, the temperature variation can be
    noticed for following, stages of disease only which are normal stage, primary
    stage and highly infected [9]. Hence, in rice blight forecast, thermal features
    depict the statistical significance with a p-value less than 0.05. Tomato plants
    infected with O. neolycopersici may be recognised using a combination of thermal
    imaging and stereo visible light pictures, as well as machine learning approaches
    that give high accuracy to enhance remote image utilisation. Plants with different
    disease can also benefit from similar treatment. Image registration, depth estimation,
    feature extraction, and classification are all part of the proposed approach.
    Support vector machines were used to classify the data [9]. Fuzzy C Means is an
    algorithm for fuzzy data clustering where objects are not only cluster members
    but are multiple clusters members. By such way, the object on boundaries are not
    compelled to belong to a particular cluster fully, rather it can become member
    of multiple clusters with the membership degree between 0 and 1 which is partial
    [3]. Fuzzy C means is said to highly efficient in analyzing fuzzy data. Young
    won lim and sang uk lee [7], proposed method that has a coarse-fine concept for
    reducing the burden of computing required for Fuzzy C Means. Using the technique
    of thresholding, the segmentation stage used here segments coarsely. By application
    of the scale space filtering to histograms, the number of search regions and thresholds
    are to be identified automatically which essential to the success of the FCM.
    The proposed algorithm produced accurate segmentation of the test images. In the
    natural light illumination, Objects having bright surface area like yellow or
    green usually appear clear. But objects having darker surface colors mix with
    its own shadow in background. So, techniques such as modified thresholding based
    inverse technique need to be applied. Kass et al.[5], proposed the idea of snakes
    or active contours. Splines that minimize energy driven by external constraints
    are snakes, and are also affected by the image forces that pull them to features
    such as the lines and edges. The greedy snake algorithm was used to segment the
    leaves of several plants such as Jackfruit, Banana, Cotton, and others, and it
    was compared to the M-kass snake method. In terms of the number of iterations
    required to obtain the desired contour of a picture, the greedy method is quicker
    and more effective than the Kas SECTION IV. Crop Disease Detection A. Role of
    Temperature in Disease Detection Plant disease should be controlled in agriculture
    for optimal economic growth and productivity. Thermal imaging aids in determining
    the pre-symptomatic influence of infection on the plant, according to several
    studies [10]. During plant-pathogen disease, the physiological state of the contaminated
    tissue is changed, such as photosynthesis, transpiration, and stomatal conductance.
    While evaluating scab illness on apple leaves using thermography, they discovered
    that the maximum temperature difference (MTD) increased with scab progression
    and was highly related to the measure of disease sites. This MTD may also be used
    to assess infection levels. Even though just a few research have used thermal
    imaging to consider and assess disease and pathogen detection, it is an important
    choice for providing data to plant disease. Thermal imaging has been shown in
    studies to be useful in detecting bruising early on, as well as determining insect
    infestation with a high degree of accuracy. When considering leaf disease detection
    in thermal images, it is important to understand that, in thermal image color
    vary according to the temperature of objects in the image. For classification
    of a leaf to be healthy or diseased it is important to understand if the temperature
    distribution in leaf image is uniform with slight variations, whereas in case
    of diseased leaf the temperature variation is more depicted by the color in the
    image. Consider the following images, Here, scale on the right side depicts various
    temperatures in the images, purple being the lowest. If these images are directly
    considered for classification both will be classified as diseased. But the actual
    fact is that image Fig. 1.(a.) is healthy leaf image wrongly classified due to
    its background temperature variation. Hence, it becomes important to segment the
    image for correct identification of the region of interest and further accurate
    classification. Fig. 1. a.) Healthy leaf 1.b.) watershed segmentation Show All
    SECTION IV. Proposed System In this research, the mainly focus on disease classification
    of leaf thermal images. The thermal image were collected using Testo 875-2i thermal
    camera at grape farm located in Nashik city, Maharashtra, India. Nashik is popularly
    known to be the Wine Capital of India and has large production of grapes. Among
    these collected images a large set of images were selected for processing, in
    which categories of images were of diseased leaves and healthy leaves. Random
    backgrounds and varying phases of infection/damage were carefully considered.
    The proposed system works in the following manner: Step1:The collected data needs
    to be pre-processed in order to be given as an input to the group Convolution
    neural network. At first thresholding is performed, Here Otsu and binary thresholding
    has been used for the thermal images. Step 2:The threshold images will be then
    given as an input for morphological operation where opening and dilation is performed
    on the images. Step 3:The next step is segmentation in order to separate the foreground
    as well as background of the image it is important to perform segmentation which
    is in the form of watershed transformation. Step 4:The processed image is given
    as an input to the group convolution neural network block, where at first, the
    image is partitioned into multiple parts and then passed to the convolution kernels.
    Step 5:The output of group convolution is then clustered and passed for convolution
    again. This convolution go through max pooling, dropout and then fully connected
    layers similar to other CNN layers. Step 6:The softmax function used in this model
    identifies the class of the image for example here it will be either diseased
    or healthy and then categorizes the input, producing required output. While implementing
    the group convolution, minimum two continuous convolution layers exist. Multiple
    independent group partitions of convolution are made as the amount of convolution
    kernel in each convolution layer are split. The figure above shows an example
    of group convolution. A CNN model is made up of two continuous convolution layers,
    each with m (width) and 3X3 convolution kernels [18]. Each convolution layer is
    divided into two partition convolution units in the group convolution technique,
    resulting in a half convolution kernel. Fig 2. Proposed system flow Show All Further
    a cluster convolution is added after group convolution in group block. Hence,
    The classification performance of group block improves efficiently as resulted
    to other CNN architectures. This block contains six layers which are input (processed
    image), group block, cluster convolution, max pooling, droup out, fully connected
    layer and softmax. The group block uses small convolution kernel to integrated
    classification performance and save computation time. The kernel size used here
    is 3x3 as it obtains better performance than other kernel sizes used for convolution
    in this model. Softmax function that is a standard classifier for detailed learning,
    is used here as classifier. Fig 3. Group convolution block Show All Overfiiting
    of the CNN can be avoided by using Dropout technique. The dropout layer is set
    between the maximum pooling layer and completely connected layer as well as drop
    rate for this research work is organised to 0.1. Cross-entropy loss, often known
    as log loss, is a measure of the performance of a classification model whose output
    is a probability value between 0 and 1. Cross-entropy loss increases as the expected
    probability differs from the actual label. SECTION V. Results A. Hyper Parameter
    Selection When training CNN models many hyper parameters need to be adjusted or
    optimized. Choosing the best hyper parameters consumes a lot of time and is tough.
    The graph below shows how the dropout rate affect the classification performance
    of the model. It is observed that among 0.1,0.3,0.5 the best rate is provided
    by 0.1 dropout value. It is difficult to find a considerate dropout value as it
    can only be done by experiment and not theoretically. Optimizers are algorithms
    for altering the parameters of a neural network, like learning rate and weights,
    in order to minimise losses. By minimising the function, optimizers are used to
    solve optimization issues. Fig 4. Relationship between performance and dropout
    rate Show All Fig 5. Comparison of optimizers Show All Fig 6. a) SGD classification
    report Show All Fig 6. b) RMSprop classification report Show All From the table
    below it is observed that Adams optimiser outperforms SGD and RMSprop. Furthermore,
    Considering 20 epochs it is observed that SGD fails to perform compared to the
    other two algorithms. With 91% accuracy on test data, Adams is selected as the
    optimizer for the model. Table 1. Hyperparameter settings Fig. 7. Classification
    report on the basis of hyper parameter settings. Show All Conclusion Disease occurrence
    of a leaf changes its temperature right from the early stages. Thus when thermal
    images are captured, the temperature variations can be seen on the images. Thresholding
    and watershed segmentation algorithms are used for pre-processing the image. These
    can then be used to train neural network for classification of thermal images,
    which is done by using a deep learning technique called group convolution neural
    networks. Thus, this strategy proves to give higher accuracy results and successfully
    classify thermal images. As a leaf can go through various stages of disease evolution,
    categories of disease severity can be considered in future work. As we have a
    variety of fruits and diseases affecting them, this strategy can be experimented
    on other fruit and vegetables plants as well. Authors Figures References Keywords
    Metrics More Like This Modelling New Crop Disease Indices for Crop Species Classification
    using Restructured Convolution Neural Network on hyperspectral Satellite images
    2022 IEEE 7th International Conference on Recent Advances and Innovations in Engineering
    (ICRAIE) Published: 2022 Optimization of Crop Disease Classification using Convolution
    Neural Network 2021 IEEE International Conference on Artificial Intelligence in
    Engineering and Technology (IICAIET) Published: 2021 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2021 6th IEEE International Conference on Recent Advances and Innovations
    in Engineering, ICRAIE 2021
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Diagnosis of leaf health using grape leaf thermal imaging and convolutional
    neural networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Peddi P.
  - Dasgupta A.
  - Gaidhane V.H.
  citation_count: '2'
  description: Smart farming is an evolving concept in the field of information and
    communications technology. In this, the IoT sensors and image processing is used
    to establish transparent mechanisms of feedback about the growth and productivity
    of crops and the environmental surrounding conditions. In this paper, the solution
    of the aforementioned problem statement in the form of an accountable live information
    system of the cultivated crops to yield efficiency has been presented. The feedback
    mechanism consists of monitoring parameters like temperature, humidity, weather,
    soil and crop moisture, crop health, etc. It provides the information between
    the planting phase and the harvesting phase to facilitate soil management and
    climate forecasting in real time. The proposed paper suggests the use of an open
    data platform, namely Adafruit IO, for visualizing and analyzing real-Time in
    the IoT integrated system. Further, image processing approach has been used for
    crop remotely health monitoring for 2 widespread diseases namely, Glomeralla Cingulata
    and Phaeoisariopsis Bataticola. Owing to the economical nature and the ergonomic
    design of the proposed system, it has the feasibility of being implemented on
    a large scale in water scarce economies aiming to build a sustainable smart farming
    infrastructure by automating existing irrigation systems.
  doi: 10.1109/IEMTRONICS55184.2022.9795747
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 IEEE International IOT, ...
    Smart Irrigation Systems: Soil Monitoring and Disease Detection for Precision
    Agriculture Publisher: IEEE Cite This PDF Premsai Peddi; Anuragh Dasgupta; Vilas
    H. Gaidhane All Authors 2 Cites in Papers 184 Full Text Views Abstract Document
    Sections I. Introduction II. Methodology III. Proposed System Design IV. Experimental
    Results and Analysis V. Conclusion Authors Figures References Citations Keywords
    Metrics Abstract: Smart farming is an evolving concept in the field of information
    and communications technology. In this, the IoT sensors and image processing is
    used to establish transparent mechanisms of feedback about the growth and productivity
    of crops and the environmental surrounding conditions. In this paper, the solution
    of the aforementioned problem statement in the form of an accountable live information
    system of the cultivated crops to yield efficiency has been presented. The feedback
    mechanism consists of monitoring parameters like temperature, humidity, weather,
    soil and crop moisture, crop health, etc. It provides the information between
    the planting phase and the harvesting phase to facilitate soil management and
    climate forecasting in real time. The proposed paper suggests the use of an open
    data platform, namely Adafruit IO, for visualizing and analyzing real-time in
    the IoT integrated system. Further, image processing approach has been used for
    crop remotely health monitoring for 2 widespread diseases namely, Glomeralla Cingulata
    and Phaeoisariopsis Bataticola. Owing to the economical nature and the ergonomic
    design of the proposed system, it has the feasibility of being implemented on
    a large scale in water scarce economies aiming to build a sustainable smart farming
    infrastructure by automating existing irrigation systems. Published in: 2022 IEEE
    International IOT, Electronics and Mechatronics Conference (IEMTRONICS) Date of
    Conference: 01-04 June 2022 Date Added to IEEE Xplore: 20 June 2022 ISBN Information:
    DOI: 10.1109/IEMTRONICS55184.2022.9795747 Publisher: IEEE Conference Location:
    Toronto, ON, Canada SECTION I. Introduction The internet of things (IoT) comprises
    of all the physical electronic devices that are connected to the internet, all
    of which are actively collecting and sharing information. These devices are likely
    to be found embedded with sensors, software, and processors that enable them to
    connect with other devices and systems over numerous communication networks. IoT
    has been considered a misnomer, given that no device needs to be connected to
    the public accessed internet. Today’s smart computing is mostly based on the Internet
    of Things and over the 46% of the world population is using this technology. It
    plays a vital role in transforming conventional forms of technology into next
    generation technology. IoT has already gained a critical role in areas of research
    globally and specifically in the area of advanced wireless communication technology.
    It has seen exponential growth in usage in a very short period of time. In the
    view of a normal user, IoT has laid the foundation for products that uses wireless
    technology extensively. For example, it is being used in products like smart living,
    smart education, automation, and process controls [1]-[3]. Commercially, it is
    being used in manufacturing, transportation, agriculture, and business management
    as well [4]-[7]. The future of agricultural technology is precision agriculture.
    The data that is being generated from multiple sensors on the field can be used
    for data analytics. Therefore, assisting farmers in improving crop yield. The
    aim of this paper is to design a working product which will enable farmers to
    access real time soil, crop, and environmental data. The anticipated advantages
    of smart farming include remote monitoring for farmers, handling water supply
    and natural resource conservation. Real time data allows for necessary manipulations
    of variables that can be handled by man. Integrating an image processing mechanism
    guarantees information about crop diseases as well. This would allow the farmer
    to take quick action and stop the disease from spreading to other crops in the
    field. Some disadvantages of smart farming are the requirements. Full– time availability
    of the internet is is a major challenge in the rural areas. Unfortunately, most
    of the farming in India happens in rural areas. However, there has been a lot
    of improvement in last few years. Multiple segments of Indian states have been
    getting access to the public internet. The smart farming-based equipment require
    farmers to understand and learn the use of technology. A mobile application with
    a sophisticated user interface however might enable farmers to understand how
    this concept works. Integration of image processing and computer vision enhances
    the potential of this system. With computer vision, the agricultural industry
    greatly benefits by further productivity along with lower capital costs surrounding
    production capacities [8]. This is done via the detection and analysis of objects
    and presenting valid hypotheses based on meaningful interpretations out of a sequence
    of images. Computer vision AI models have immeasurable uses in the fields of planting,
    harvesting, analysis of weather, weeding and crop health detection and real time
    feedback for monitoring [9]. This paper presented an agricultural system with
    an IOT environment which requires adequate manpower. It employs IOT and cloud
    computing globally to remove the inadequacy and lack of management, which are
    considered to be the key factors responsible for the decline in quality agriculture.
    SECTION II. Methodology Varying crops require different levels of water levels
    for cultivation between the plantation and the harvesting phase depending upon
    the week of harvest. Based on the standardized template of the sample crops, information
    about the minimum threshold and maximum capacity of water required along with
    live feed of additional parameters like pesticides, seed monitoring, sunshine
    and humidity on the proposed automated irrigation network would be fed into the
    system. The water supply from the submersible water pump would be released at
    regular intervals based on the inputs from the sensors in the soil. Moreover,
    the quantity and the type of pesticides can be decided using the concepts of image
    processing approaches. The image processing techniques such as morphology, binarization
    and segmentation can be used for the identification of anomalies over a crop area
    to estimate and monitor plant growth. SECTION III. Proposed System Design The
    proposed system for smart irrigation is shown in Fig. 1. It consists of Sensors,
    Node MCU, LDR and camera. The principal framework comprises of a Wi-Fi Module,
    specifically, an ESP8266 Node MCU configured with numerous sensors such as the
    DHT11 humidity sensor, DS18B20 temperature sensor probe, soil moisture sensor,
    a light dependant resistor (LDR) and a water pump as shown in Fig. 1. The descriptions
    of each sensor implemented in this design are given below. Fig. 1. Hardware Diagram
    Show All A. NodeMCU ESP8266 The ESP8266 is a low-cost Wi-Fi microchip, with built-in
    TCP/IP networking software, and microcontroller capability. This Wi-Fi module
    is used in the design to connect all the sensors to an online IO, Adafruit to
    share, collect and analyse the data. Fig. 2. NodeMCU Module Show All B. Soil Moisture
    Sensor Soil moisture sensors are globally used to estimate the content of water
    in the soil. The moisture sensor used in this system is a capacitive sensor. It
    calculates the change in capacitance caused due to the dielectric. It cannot measure
    moisture directly as pure water does not conduct electricity. Some advantages
    of using a capacitive sensor are that corrosion is avoided and gives an accurate
    reading of the moisture content in the soil. Fig. 3. Capacitive Moisture Sensor
    Show All C. DHT11 Humidity Sensor The DHT11 Sensor is the most frequently used
    temperature and humidity sensors in the field of IoT, owing to its extremely low
    price. It uses a capacitive humidity sensor and a thermistor to measure the air
    temperature. It also directly produces a digital signal without the need of an
    ADC. Fig. 4. DHT11 Sensor Show All D. Light Dependent Resistor(LDR) LDRs are also
    called photoresistors since the resistance produced is dependent on the amount
    of light. Hence, this module is used in our system to monitor the amount of sunlight
    during the day. The resistance of this LDR is indirectly proportional to the intensity
    of light, hence, when the light intensity increases, the resistance offered by
    the LDR decreases. Fig. 5. Light Dependent Resistor Show All E. Water Pump The
    mini water pump that is used in this model is a 3-5V DC Pump. It is programmed
    to turn on when the moisture content in the soil is lower than the configured
    value. Fig. 6. Mini Water Pump Show All This model also employs an open-source
    Input/Output (I/O) cloud service, namely, Adafruit I/O. It is a platform that
    permits aggregation, visualisation, and analyzation of live information on the
    cloud. Adafruit I/O also enables motor controls and reading data via sensors.
    The cloud service has multiple feeds that are used to monitor various data being
    captured by the sensors. A minima and maxima are predefined during configuration,
    below or over which the farmer is notified for corrective measure. The ESP8266
    NodeMCU governs the communication between the sensors on the board and behaves
    as the IOT Gateway to the cloud. All the sensors detect the physical parameters
    and convert the analogue value into a digital value. This is achieved using an
    in-built Analog-Digital-Converter (ADC) in the sensor modules. The humidity sensor,
    DHT11, is used to compute the environmental humidity. The temperature sensor probe
    and the OpenWeatherAPI is used to monitor the soil temperature and get live environmental
    temperature, respectively. The soil moisture sensor is a capacitive sensor which
    estimates the amount of water in the soil. It works on the principle of open and
    short circuit. In simpler words, it acts as a switch with an ON/OFF mechanism
    [11]. Whether the output is high or low indicated by the inbuilt LED. When the
    soil is dry, the current is not conducted, hence, acting as an open circuit, with
    the output being high. When the soil is wet, the current flows from one terminal
    to the other and the circuit is shorted. Consequently, the output is low. Therefore,
    when the moisture sensed is below threshold, the water pump turns on and provides
    continuous water flow till the threshold value is met [10]. The cloud service,
    Adafruit I/O, deployed in this system will provide a dashboard of multiple feeds
    depending on the parameters that have to be analysed. In this case, a total of
    8-9 feeds are set up which include all the data acquired from the installed sensors.
    This will also comprise of a system which will alert the farmer or the user when
    the environmental factors are extreme. For example, whenever the temperature measured
    is above the set point, an output from a decision logic notifies the farmer. A
    model Adafruit I/O dashboard from an run is shown in Fig. 7. Fig. 7. Adafruit
    Dashboard Show All The IoT based system is implemented by using Arduino IDE. The
    sensing phase is concerned with the estimates of all the physical parameters which
    comprise of humidity, moisture, temperature, and light. Although the ESP8266 module
    acts as the IoT gateway, Arduino is used to program the sensors to it. The basic
    flow of the program is as follows: F. Image Processing The following parameters
    can be broadly classified as the major criteria involved surrounding the productivity
    of crops and plants in 4 respective areas: Identifying Plant/Crop diseases Monitoring
    the growth of crop/plant Monitoring the health aspects of crop/plant throughout
    its plantation timeline The following flowchart in Fig. 8. lays down the foundations
    of the steps involved ranging from acquiring our image to the final classification
    of the disease detected: Fig. 8. Basic Steps of Image Processing in Leaf Disease
    Detection Show All The Images of the target plants during their harvesting phase
    are captured via a webcam. The images are preprocessed in order to eliminate any
    distortions or impurities and noise which might be present in the images extracted
    and are prepared for the upcoming processing methods like extraction of features
    required in the later stages. Segmentation is primarily done to separate the respective
    area of interest by filtering out from the image captured. The basic purpose of
    segmentation is to create a collection of segments that are combined overall to
    represent the entire image into a set of contours which are obtained from the
    captured image. Feature Extraction is mainly used to extract features from the
    processed image after which we use the respective features for classification
    purposes. Its main use is to reduce dimensions in the image and compress the data
    which is to be processed in order to target the specific features which help us
    in disease classification. This is done in order to filter out the input data
    and eliminate redundant data. Classification is done on the basis of spectral
    features in the features created. And classification aids in the process of dividing
    the feature target space into various classes according to the input decision
    rule. Initially after high resolution images of plants/crops are captured via
    a webcam, image pre-processing and processing techniques are implemented in order
    to get features which would be needed for analysis at a later time. The image,
    which is in the form of Red, Green and Blue (RGB) is converted to a Hue, Intensity,
    Saturation (HIS) model for increasing luminance of every frame of the image [11].
    Further for the purpose of smoothening and filtering out noise. This is done by
    enhancing contrast in the image for increasing the accuracy of output and better
    implementation of segmentation on the image. Furthermore, Image segmentation is
    carried out using thresholding which is an efficient method in order to separate
    the background from the foreground and also masking pixels which are green indicative
    of healthy parts of a leaf or crop [12]. Segmentation is done using means clustering,
    with standard Euclidean distance as the measuring parameter for calculating extent
    of similarity, which is an unsupervised machine learning algorithm where denotes
    the number of centroids (which represents the center of the cluster) that are
    present in our dataset. For our respective algorithm, a structure was created
    for color transformation which is based on the model proposed by Oo and Htun[13]
    in his research. This is done mainly to mask the green pixels of the leaf image
    which is to separate out parts of the image. Our proposed algorithm creates =
    3 clusters in no particular order- One cluster for separating out the leaf from
    the background, one cluster to segment out the healthy part of the leaf and the
    final cluster dedicated to segment out the infected and diseased part of the leaf,
    if there is any. Since is small, the computational speed of the algorithm increases
    exponentially than hierarchal clustering. The Infected cluster is converted to
    HSI from the RGB format. In this work, the SVM (Support Vector Machine) classification
    method is used because it is much easier due to less processing time [14]-[17].
    SECTION IV. Experimental Results and Analysis A. Irrigation System The various
    reading has been taken from the design system to facilitate and validate the proposed
    system. It has been observed that the water pump starts automatically at the particular
    soil conditions. A prototype for monitoring the soil condition is shown in Fig.
    9. The setup acquires the values of moisture, humidity and temperature and transmits
    it to the cloud via the NodeMCU Module. Fig. 9 Physical Setup Show All Table 1
    shows the status of the pump corresponding to the threshold value that has been
    set, along with other measured data. A tulsi plant was used for the purpose of
    testing and threshold range coded for moisture content used was between 60 to
    63, i.e below 60% moisture, the motor is on and above 63%, the motor is turned
    off, between 60 and 63, there is no change in the motor status. However, the threshold
    values for light and moisture can be changed in the program according to the needs
    during field implementations. Fig. 10 to Fig. 13, the variation of soil Moisture,
    Soil Temperature, Environment Humidity, Environment Temperature, respectively.
    TABLE 1. Sensor measured data distributed Fig 10. Soil Temperature sensed over
    a period of 8 hours Show All Fig 11. Environmental Humidity sensed over a period
    of 8 hours Show All Fig 12. Soil Moisture data of over 8 hours Show All Fig 13.
    Environment Temperature sensed over 8 hours Show All The graphs attached above
    were obtained for a testing period of 8 hours. All the data was also published
    on the online service Adafruit. This has been shown in the Fig. 14 below. Fig
    14. Published data on Adafruit IO Show All B. Disease Detection using Image Processing
    The presented model has been validated using the available dataset [18]. The images
    are initially pre-processed using the filter to remove the noise. After pre-processing
    edge extraction is carried out using canny edge detection approach to preserve
    main features and remove the remaining features as shown in Fig. 15. It is observed
    that Canny edge detection method performs better as compared Sobel approach. The
    feature extraction on the region of interest gives information whether the plant
    or crop is healthy or unhealthy. This work has been carried out for two widespread
    diseases – Glomeralla Cingulate and Phaeoisariopsis Bataticola. The various feature
    metrics then calculated and used as an input parameter for further classification.
    The various average values of parameters are shown in Table I. TABLE 2. Extracted
    Features Parameters The leaf disease detection GUI is also shown in the figure
    below. The images captured have vertical resolution and horizontal resolution
    of 96 dpi with bit depth equal to 24. The dimensions of the captured image is
    resized in MATLAB for processing. The webcam to capture query images to test our
    model has the following specifications: Lenovo 300 FHD Flexible Mount Webcam FHD
    1080P 2.1 Megapixel CMOS Camera. Fig. 15. (a) leaf with anthracnose, (b) contrasted
    image, (c) segmented image, (d, e, f) edge detected using Canny method for feature
    extraction, (g, h, i) edge detected using Sobel method for feature extraction
    Show All Fig. 16. Sample output of classification from a query image specifying
    affected region and accuracy of output which does 500 iterations Show All SECTION
    V. Conclusion This paper proposes and implements the smart farming- soil monitoring
    and disease detection system. The presented may be useful for automatic irrigation
    system which increases the efficiency of the harvesting process by tracking real
    time plant/crop growth. It enables the farmers at all times requiring his intervention
    in the process only in the case of any anomalies. The plant disease classifier
    classifies two diseases: Glomeralla Cingulate and Phaeoisariopsis Bataticola.
    The design can further be improved by increasing and training the dataset on new
    diseases and real time images. The proposed system reduces human labor and workload
    of the farmers. The process is economical owing to its low budget and highly feasible
    considering the processing of low-resolution images captured via webcam which
    implies its suitability for farmers’ uses. Authors Figures References Citations
    Keywords Metrics More Like This Soil Macro-Nutrients Detection, Crop and Fertilizer
    Recommendation with Irrigation System 2023 International Conference on Advances
    in Electronics, Communication, Computing and Intelligent Information Systems (ICAECIS)
    Published: 2023 An IOT-Based Soil Moisture Management System for Precision Agriculture:
    Real-Time Monitoring and Automated Irrigation Control 2023 4th International Conference
    on Smart Electronics and Communication (ICOSEC) Published: 2023 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS
    2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Smart Irrigation Systems: Soil Monitoring and Disease Detection for Precision
    Agriculture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 50 papers. The special focus in this conference
    is on Microelectronics, Electromagnetics, and Telecommunications. The topics include:
    Optical Letter Recognition for Roman-Text; intelligent Noise Detection and Correction
    with Kriging on Fundus Images of Diabetic Retinopathy; a Nanoplasmonic Ultra-wideband
    Antenna for Wireless Communications; antenna Array Synthesis of Shaped Beam Using
    Deterministic Method; a Novel Semi-blind Digital Image Watermarking Using Fire
    Fly Algorithm; Real-Time Image Enhancement Using DCT Techniques for Video Surveillance;
    performance Analysis of Automatic Modulation Recognition Using Convolutional Neural
    Network; design of Wearable Microstrip Patch Antenna for Biomedical Application
    with a Metamaterial; estimation of Gender Using Convolutional Neural Network;
    automatic Modulation Recognition of Analog Modulation Signals Using Convolutional
    Neural Network; taxonomy on Breast Cancer Analysis Using Neural Networks; a Novel
    Cuckoo Search with Levy Distribution-Optimized Density-Based Clustering Model
    on MapReduce for Big Data Environment; drowsiness Detection System for Drivers
    Using 68 Coordinate System; human Action Recognition in Videos Using Deep Neural
    Network; Performance Analysis of Underwater Acoustic Communication System with
    Massive MIMO-OFDM; 360° Video Summarization: Research Scope and Trends; Construing
    Crop Health Dynamics Using UAV-RGB based SpaceTech Analytics and Image Processing;
    review of Different Binarization Techniques Used in Different Areas of Image Analysis;
    An Improved Unsharp Masking (UM) Filter with GL Mask; on Performance Improvement
    of Wireless Push Systems Via Smart Antennas; classification of Non-fluctuating
    Radar Target Using ReliefF Feature Selection Algorithm; Design of Arrayed Rectangular
    Probe Patch Antenna at 6.2 GHz for 5G Small Cell Applications; Visual Words based
    Static Indian Sign Language Alphabet Recognition using KAZE Descriptors; preface.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 6th International Conference on Microelectronics, Electromagnetics, and Telecommunications,
    ICMEET 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 50 papers. The special focus in this conference
    is on Microelectronics, Electromagnetics, and Telecommunications. The topics include:
    Optical Letter Recognition for Roman-Text; intelligent Noise Detection and Correction
    with Kriging on Fundus Images of Diabetic Retinopathy; a Nanoplasmonic Ultra-wideband
    Antenna for Wireless Communications; antenna Array Synthesis of Shaped Beam Using
    Deterministic Method; a Novel Semi-blind Digital Image Watermarking Using Fire
    Fly Algorithm; Real-Time Image Enhancement Using DCT Techniques for Video Surveillance;
    performance Analysis of Automatic Modulation Recognition Using Convolutional Neural
    Network; design of Wearable Microstrip Patch Antenna for Biomedical Application
    with a Metamaterial; estimation of Gender Using Convolutional Neural Network;
    automatic Modulation Recognition of Analog Modulation Signals Using Convolutional
    Neural Network; taxonomy on Breast Cancer Analysis Using Neural Networks; a Novel
    Cuckoo Search with Levy Distribution-Optimized Density-Based Clustering Model
    on MapReduce for Big Data Environment; drowsiness Detection System for Drivers
    Using 68 Coordinate System; human Action Recognition in Videos Using Deep Neural
    Network; Performance Analysis of Underwater Acoustic Communication System with
    Massive MIMO-OFDM; 360° Video Summarization: Research Scope and Trends; Construing
    Crop Health Dynamics Using UAV-RGB based SpaceTech Analytics and Image Processing;
    review of Different Binarization Techniques Used in Different Areas of Image Analysis;
    An Improved Unsharp Masking (UM) Filter with GL Mask; on Performance Improvement
    of Wireless Push Systems Via Smart Antennas; classification of Non-fluctuating
    Radar Target Using ReliefF Feature Selection Algorithm; Design of Arrayed Rectangular
    Probe Patch Antenna at 6.2 GHz for 5G Small Cell Applications; Visual Words based
    Static Indian Sign Language Alphabet Recognition using KAZE Descriptors; preface.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 6th International Conference on Microelectronics, Electromagnetics, and Telecommunications,
    ICMEET 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Meng Q.
  - Yang X.
  - Zhang M.
  - Guan H.
  citation_count: '10'
  description: Environmental information perception has been one of the most important
    technologies in agricultural automatic navigation tasks, such as plant fertilization,
    crop disease detection, automatic harvesting, and cultivation. Among them, the
    complex environment of a field road is characterized by the fuzzy road edge, uneven
    road surface, and irregular shape. It is necessary to accurately and rapidly identify
    the passable areas and obstacles when the agricultural machinery makes path planning
    and decision control. In this study, a lightweight semantic segmentation model
    was proposed to recognize the unstructured roads in fields using a channel attention
    mechanism combined with the multi-scale features fusion. Some environmental objects
    were also classified into 12 categories, including building, person, vehicles,
    sky, waters, plants, road, soil, pole, sign, coverings, and background, according
    to the static and dynamic properties. Furthermore, a mobile architecture named
    MobileNetV2 was adopted to obtain the image feature information, in order to reduce
    the model parameters for a higher reasoning speed. Specifically, an inverted residual
    structure with lightweight depth-wise convolutions was utilized to filter the
    features in the intermediate expansion layer. In addition, the last two stages
    of the backbone network were combined with the Hybrid Dilated Convolution (HDC),
    aiming to increase the receptive fields and maintain the resolution of the feature
    map. The hybrid dilated convolution with the dilation rate of 1, 2, and 3 was
    used to effectively expand the receptive fields, thereby alleviating the "gridding
    problem" caused by the standard dilated convolution. A Channel Attention Block
    (CAB) was also introduced to change the weight of each stage feature, in order
    to enhance the class consistency. The channel attention block was used to strengthen
    both the higher and lower level features of each stage for a better prediction.
    In addition, some errors of semantic segmentation were partially or completely
    attributed to the contextual relationship. A pyramid pooling module was empirically
    adopted to fuse three scale feature maps for the global contextual prior. There
    was the global context information in the first image level, where the feature
    vector was produced by a global average pooling. The pooled representation was
    then generated for different locations, where the rest pyramid levels separated
    the feature maps into different sub-regions. As such, the output of different
    levels in the pyramid module contained the feature maps with varied sizes, followed
    by up sampling and concatenation to form the final output. The results showed
    that the objects in the complex roads were effectively segmented with Pixel Accuracy
    (PA) and Mean Pixel Accuracy (MPA) of 94.85% and 90.38%, respectively. Furthermore,
    the single category pixel accuracy of some objects was more than 90%, such as
    road, plants, building, waters, sky, and soil, indicating a higher accuracy, strong
    robustness, and excellent generalization. An evaluation was also made to verify
    the efficiency and superiority of the model, where the mean intersection over
    union (MIoU), segmentation speed, and parameter scale were adopted as the indexes.
    The FCN-8S, SegNet, DeeplabV3+ and BiseNet networks were also developed on the
    same training and test datasets. It was found that the MIoU of the model was 85.51%,
    indicating a higher accuracy than others. The parameter quantity of the model
    was 2.41×106, smaller than FCN-8S, SegNet, DeeplabV3+, and BiseNet. In terms of
    an image with a resolution of 512×512 pixels, the reasoning speed of the model
    reached 8.19 frames per second, indicating an excellent balance between speed
    and accuracy. Consequently, the lightweight semantic segmentation model was achieved
    to accurately and rapidly segment the multiple road scenes in the field environment.
    The finding can provide a strong technical reference for the safe and reliable
    operation of intelligent agricultural machinery on unstructured roads.
  doi: 10.11975/j.issn.1002-6819.2021.22.017
  full_citation: '>'
  full_text: '>

    "EI CSA CABI 卓越期刊 CA Scopus CSCD 核心期刊 首页 关于我刊 编委会 投稿指南 期刊浏览 获奖文章 农业工程期刊 期刊订阅 联系我们
    EI收录本刊数据 English 文章导航 >  农业工程学报  > 2021  >  37(22) : 152-160.  > DOI: 10.11975/j.issn.1002-6819.2021.22.017
    引用本文: 孟庆宽, 杨晓霞, 张漫, 关海鸥. 基于语义分割的非结构化田间道路场景识别[J]. 农业工程学报, 2021, 37(22): 152-160.
    DOI: 10.11975/j.issn.1002-6819.2021.22.017 Citation: Meng Qingkuan, Yang Xiaoxia,
    Zhang Man, Guan Haiou. Recognition of unstructured field road scene based on semantic
    segmentation model[J]. Transactions of the Chinese Society of Agricultural Engineering
    (Transactions of the CSAE), 2021, 37(22): 152-160. DOI: 10.11975/j.issn.1002-6819.2021.22.017
    基于语义分割的非结构化田间道路场景识别 孟庆宽1,  杨晓霞1,  张漫2,  关海鸥3 1. 天津职业技术师范大学自动化与电气工程学院，天津市信息传感与智能控制重点实验室，天津
    300222 2. 中国农业大学现代精细农业系统集成研究教育部重点实验室，北京 100083 3. 黑龙江八一农垦大学电气与信息学院，大庆 163319 基金项目:
    国家自然科学基金项目（31571570、62001329）；天津市自然科学基金项目（18JCQNJC04500、19JCQNJC01700）；天津职业技术师范大学校级预研项目（KJ2009、KYQD1706）
    Recognition of unstructured field road scene based on semantic segmentation model
    Meng Qingkuan1,  Yang Xiaoxia1,  Zhang Man2,  Guan Haiou3 1. College of Automation
    and Electrical Eengineering, Tianjin University of Technology and Education, Tianjin
    Key Laboratory of Information Sensing and Intelligent Control, Tianjin 300222,
    China 2. Key Laboratory of Modern Precision Agriculture System Integration Research,
    Ministry of Education, China Agricultural University, Beijing 10083, China 3.
    College of Electrical and Information, Heilongjiang Bayi Agricultural University,
    Daqing 163319, China 摘要 摘要 HTML全文 图(0) 表(0) 参考文献(27) 相关文章 施引文献(21) 资源附件(0) 摘要:
    环境信息感知是智能农业装备系统自主导航作业的关键技术之一。农业田间道路复杂多变，快速准确地识别可通行区域，辨析障碍物类别，可为农业装备系统高效安全地进行路径规划和决策控制提供依据。该研究以非结构化农业田间道路场景为研究对象，根据环境对象动、静态属性进行类别划分，提出一种基于通道注意力结合多尺度特征融合的轻量化语义分割模型。首先采用Mobilenet
    V2轻量卷积神经网络提取图像特征，将混合扩张卷积融入特征提取网络最后2个阶段，在保证特征图分辨率的基础上增加感受野并保持信息的连续性与完整性；然后引入通道注意力模块对特征提取网络各阶段特征通道依据重要程度重新标定；最后通过空间金字塔池化模块将多尺度池化特征进行融合，获取更加有效的全局场景上下文信息，增强对复杂道路场景识别的准确性。语义分割试验表明，不同道路环境下本文模型可以对场景对象进行有效识别解析，像素准确率和平均像素准确率分别为94.85%、90.38%，具有准确率高、鲁棒性强的特点。基于相同测试集将该文模型与FCN-8S、SegNet、DeeplabV3+、BiseNet模型进行对比试验，该文模型的平均区域重合度为85.51%，检测速度达到8.19帧/s，参数数量为2.41×106，相比于其他模型具有准确性高、推理速度快、参数量小等优点，能够较好地实现精度与速度的均衡。研究成果可为智能农业装备在非结构化道路环境下安全可靠运行提供技术参考。   关键词:
    机器视觉  /  语义分割  /  环境感知  /  非结构化道路  /  轻量卷积  /  注意力机制  /  特征融合   Abstract: Abstract:
    Environmental information perception has been one of the most important technologies
    in agricultural automatic navigation tasks, such as plant fertilization, crop
    disease detection, automatic harvesting, and cultivation. Among them, the complex
    environment of a field road is characterized by the fuzzy road edge, uneven road
    surface, and irregular shape. It is necessary to accurately and rapidly identify
    the passable areas and obstacles when the agricultural machinery makes path planning
    and decision control. In this study, a lightweight semantic segmentation model
    was proposed to recognize the unstructured roads in fields using a channel attention
    mechanism combined with the multi-scale features fusion. Some environmental objects
    were also classified into 12 categories, including building, person, vehicles,
    sky, waters, plants, road, soil, pole, sign, coverings, and background, according
    to the static and dynamic properties. Furthermore, a mobile architecture named
    MobileNetV2 was adopted to obtain the image feature information, in order to reduce
    the model parameters for a higher reasoning speed. Specifically, an inverted residual
    structure with lightweight depth-wise convolutions was utilized to filter the
    features in the intermediate expansion layer. In addition, the last two stages
    of the backbone network were combined with the Hybrid Dilated Convolution (HDC),
    aiming to increase the receptive fields and maintain the resolution of the feature
    map. The hybrid dilated convolution with the dilation rate of 1, 2, and 3 was
    used to effectively expand the receptive fields, thereby alleviating the \"gridding
    problem\" caused by the standard dilated convolution. A Channel Attention Block
    (CAB) was also introduced to change the weight of each stage feature, in order
    to enhance the class consistency. The channel attention block was used to strengthen
    both the higher and lower level features of each stage for a better prediction.
    In addition, some errors of semantic segmentation were partially or completely
    attributed to the contextual relationship. A pyramid pooling module was empirically
    adopted to fuse three scale feature maps for the global contextual prior. There
    was the global context information in the first image level, where the feature
    vector was produced by a global average pooling. The pooled representation was
    then generated for different locations, where the rest pyramid levels separated
    the feature maps into different sub-regions. As such, the output of different
    levels in the pyramid module contained the feature maps with varied sizes, followed
    by up sampling and concatenation to form the final output. The results showed
    that the objects in the complex roads were effectively segmented with Pixel Accuracy
    (PA) and Mean Pixel Accuracy (MPA) of 94.85% and 90.38%, respectively. Furthermore,
    the single category pixel accuracy of some objects was more than 90%, such as
    road, plants, building, waters, sky, and soil, indicating a higher accuracy, strong
    robustness, and excellent generalization. An evaluation was also made to verify
    the efficiency and superiority of the model, where the mean intersection over
    union (MIoU), segmentation speed, and parameter scale were adopted as the indexes.
    The FCN-8S, SegNet, DeeplabV3+ and BiseNet networks were also developed on the
    same training and test datasets. It was found that the MIoU of the model was 85.51%,
    indicating a higher accuracy than others. The parameter quantity of the model
    was 2.41×106, smaller than FCN-8S, SegNet, DeeplabV3+, and BiseNet. In terms of
    an image with a resolution of 512×512 pixels, the reasoning speed of the model
    reached 8.19 frames per second, indicating an excellent balance between speed
    and accuracy. Consequently, the lightweight semantic segmentation model was achieved
    to accurately and rapidly segment the multiple road scenes in the field environment.
    The finding can provide a strong technical reference for the safe and reliable
    operation of intelligent agricultural machinery on unstructured roads.   Keywords:
    machine vision  /  semantic segmentation  /  environmental perception  /  unstructured
    field roads  /  lightweight convolution  /  attention mechanism  /  feature fusion   We
    recommend Real-time semantic segmentation method for field grapes based on channel
    feature pyramid Sun Jun et al., Transactions of the Chinese Society of Agricultural
    Engineering, 2022 Field road scene recognition in hilly regions based on improved
    dilated convolutional networks Li Yunwu et al., Transactions of the Chinese Society
    of Agricultural Engineering, 2019 Segmenting field rice panicle images using DBSE-Net
    Song Yuqing et al., Transactions of the Chinese Society of Agricultural Engineering,
    2022 Constructing VED SegNet segmentation model to extract fish phenotype proportions
    LI Jianyuan et al., Transactions of the Chinese Society of Agricultural Engineering
    Image segmentation method for Lingwu long jujubes based on improved FCN-8s Xue
    Junrui et al., Transactions of the Chinese Society of Agricultural Engineering,
    2020 New motion blur restoration approach for improved weed detection in crop
    fields by NanJing Agricultural University, Phys.org, 2023 Using AI, cars can detect
    potholes in real time by National Research Council of Science et al., TechXplore.com,
    2021 Improving root senescence recognition with a new semantic segmentation model
    by NanJing Agricultural University, Phys.org, 2024 New deep-learning approach
    gets to the bottom of colonoscopy by Tsinghua University Press, MedicalXpress,
    2023 Team develops new geometric deep learning model for detecting stroke lesions
    by SPIE, MedicalXpress, 2023 Powered by PDF下载 ( 4765 KB) XML下载 导出引用 点击查看大图 计量
    文章访问数:  603 HTML全文浏览量:  7 PDF下载量:  1400 被引次数: 21 出版历程 收稿日期:  2021-05-31 修回日期:  2021-09-15
    发布日期:  2021-11-14 分享 友情链接> Biomass & Bioenergy Biosystems Engineering Aquacultural
    Engineering International Journal of Agricultural and Biological Engineering 版权所有
    © 农业工程学报 京ICP备06025802号-3 地址：北京朝阳区麦子店街41号（100125） 电话：010-59197078/7077/7076 邮箱：tcsae@tcsae.org
    邮件订阅 RSS 今日头条 抖音号 视频号 淘宝 微店 本系统由北京仁和汇智信息技术有限公司开发  "'
  inline_citation: '>'
  journal: Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural
    Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Recognition of unstructured field road scene based on semantic segmentation
    model
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Neupane K.
  - Baysal-Gurel F.
  citation_count: '68'
  description: Disease diagnosis is one of the major tasks for increasing food production
    in agriculture. Although precision agriculture (PA) takes less time and provides
    a more precise application of agricultural activities, the detection of disease
    using an Unmanned Aerial System (UAS) is a challenging task. Several Unmanned
    Aerial Vehicles (UAVs) and sensors have been used for this purpose. The UAVs’
    platforms and their peripherals have their own limitations in accurately diagnosing
    plant diseases. Several types of image processing software are available for vignetting
    and orthorectifica-tion. The training and validation of datasets are important
    characteristics of data analysis. Currently, different algorithms and architectures
    of machine learning models are used to classify and detect plant diseases. These
    models help in image segmentation and feature extractions to interpret results.
    Researchers also use the values of vegetative indices, such as Normalized Difference
    Vegetative Index (NDVI), Crop Water Stress Index (CWSI), etc., acquired from different
    multispectral and hyperspec-tral sensors to fit into the statistical models to
    deliver results. There are still various drifts in the automatic detection of
    plant diseases as imaging sensors are limited by their own spectral bandwidth,
    resolution, background noise of the image, etc. The future of crop health monitoring
    using UAVs should include a gimble consisting of multiple sensors, large datasets
    for training and validation, the development of site-specific irradiance systems,
    and so on. This review briefly highlights the advantages of automatic detection
    of plant diseases to the growers.
  doi: 10.3390/rs13193841
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals
    Remote Sensing Volume 13 Issue 19 10.3390/rs13193841 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Michael
    Schirrmann Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 11131 Citations 65 Table of Contents Abstract Introduction
    Types of UAVs, Their Platforms and Peripherals Used in Disease Monitoring and
    Identification Cameras and Sensors Image Pre-Processing Data Processing Deep Learning
    Models Challenges of Automatic Plant Disease Identification Using UAVs Future
    Considerations Conclusions Author Contributions Funding Institutional Review Board
    Statement Informed Consent Statement Conflicts of Interest References Altmetric
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessReview Automatic Identification and Monitoring of Plant Diseases Using Unmanned
    Aerial Vehicles: A Review by Krishna Neupane and Fulya Baysal-Gurel * Otis L.
    Floyd Nursery Research Center, Department of Agricultural and Environmental Sciences,
    Tennessee State University, McMinnville, TN 37110, USA * Author to whom correspondence
    should be addressed. Remote Sens. 2021, 13(19), 3841; https://doi.org/10.3390/rs13193841
    Submission received: 21 July 2021 / Revised: 17 September 2021 / Accepted: 21
    September 2021 / Published: 25 September 2021 (This article belongs to the Special
    Issue UAV Imagery for Precision Agriculture) Download keyboard_arrow_down     Browse
    Figure Versions Notes Abstract Disease diagnosis is one of the major tasks for
    increasing food production in agriculture. Although precision agriculture (PA)
    takes less time and provides a more precise application of agricultural activities,
    the detection of disease using an Unmanned Aerial System (UAS) is a challenging
    task. Several Unmanned Aerial Vehicles (UAVs) and sensors have been used for this
    purpose. The UAVs’ platforms and their peripherals have their own limitations
    in accurately diagnosing plant diseases. Several types of image processing software
    are available for vignetting and orthorectification. The training and validation
    of datasets are important characteristics of data analysis. Currently, different
    algorithms and architectures of machine learning models are used to classify and
    detect plant diseases. These models help in image segmentation and feature extractions
    to interpret results. Researchers also use the values of vegetative indices, such
    as Normalized Difference Vegetative Index (NDVI), Crop Water Stress Index (CWSI),
    etc., acquired from different multispectral and hyperspectral sensors to fit into
    the statistical models to deliver results. There are still various drifts in the
    automatic detection of plant diseases as imaging sensors are limited by their
    own spectral bandwidth, resolution, background noise of the image, etc. The future
    of crop health monitoring using UAVs should include a gimble consisting of multiple
    sensors, large datasets for training and validation, the development of site-specific
    irradiance systems, and so on. This review briefly highlights the advantages of
    automatic detection of plant diseases to the growers. Keywords: UAS; UAVs; plant
    disease detection; plant monitoring; convolutional neural networks (CNNs); deep
    learning; machine learning 1. Introduction Simply, UAVs are any aerial vehicles
    that are remotely driven, meaning no pilot on board. They are considered one of
    the important innovations of present-day precision agriculture [1,2,3,4,5,6,7,8].
    Precision agriculture (PA) is a method to transform agriculture by reducing time
    and labor and increasing production and management efficiency [9]. With the development
    in technology and computational skills, there have been changes in agricultural
    patterns such as using digital planters, harvesters, sprayers, etc. Agriculture
    has transformed over time from being carried out by manual labor to mechanical
    labor due to the adoption of technological changes. Previously, plant diseases
    in agriculture fields were monitored visually by people who have experience scouting
    and monitoring plant diseases. This type of observation is psychological and subject
    to bias, optical illusion, and error [1]. This generated the need for external
    image-based tools that can replace unreliable human observations. This also allows
    for extended coverage in a limited amount of time. The potential of UAVs for conducting
    detailed surveys in precision agriculture has been demonstrated for a range of
    applications such as crop monitoring [10,11], field mapping [12], biomass estimation
    [13,14], weed management [15,16], plant population counting [17,18,19], and spraying
    [20]. A large amount of data and information is collected by UAVs to improve agricultural
    practices [21]. Different kinds of data recording instruments, cameras, and sensor
    installation equipment have been developed for agricultural purposes [4]. Some
    additional reasons for increasing usage of UAVs and drones in agriculture [22]
    include UAV prices decreasing gradually [2,23], agriculture operations being carried
    out in areas with low population and activities [1], and UAVs having a large occupancy
    and great scouting capability. Although the images can be obtained from various
    sources, such as satellites and aircrafts [4], and can cover a large area compared
    to those of an unmanned aerial system, the resolution in the images is not conducive
    to drawing significant conclusions. This leads to other advantages of using drones
    for high-resolution aerial images [1,24]. Increased efficiency, stability, accuracy,
    and productivity are other advantages of UAVs [25] that allow growers and experts
    to make better, more timely management decisions [26,27]. The application of PA
    technology not only increases economic profit but social benefits measured in
    sustainability. A study carried out by Van Evert et al. [28] reported that the
    application of PA in potato cultivation increased economic profit by 21% and social
    profit by 26% compared to agricultural practices without precision agriculture.
    The use of agriculture UAVs is hindered by many challenges such as battery efficiency
    [25,29], low flight time [7], communication distance, and payload [25,30]. The
    limited flight duration due to increased payload and decreased efficiency of batteries
    obstructs when performing critical agricultural activities in larger fields, such
    as pesticide and nutrient application. Other challenges associated with its use
    are engine power, stability, maintaining altitude, and maneuverability in wind
    and turbulence [7,31]. UAVs have been used for a variety of reasons in agriculture.
    Traditionally, visual observations would determine crop nutrient status, their
    pests, diseases, and environmental stress [32]. Currently, most UAVs are used
    for detecting stress in plants, for quantifying biomass, vegetation classification,
    canopy cover estimation, yield prediction, plant height assessment, and lodging
    assessment. Agriculture UAVs have been used for mapping agriculture fields, spraying
    chemicals, planting, crop monitoring, irrigation, diagnosis of insects and pests,
    artificial pollination [25,33], and livestock population dynamics [34]. A combination
    of UAVs with hyper and multispectral cameras has been predominant in a wide range
    of agricultural operations [33] for disease identification purposes. Recently,
    Chang, Zhou, Kira, Marri, Skovira, Gu, and Sun [32] used UAVs to measure solar-induced
    chlorophyll fluorescence and photo-chemical reflectance through single bifurcated
    fiber and a motorized arm to measure radiance. Artificial Intelligence (AI) and
    deep learning are incorporated with UAVs to increase the precision of crop disease
    identification and monitoring. Penn State University, Food and Agriculture Organization,
    International Institute of Tropical Agriculture, International Maize and Wheat
    Improvement Center and others have developed PlantVillage Nuru to identify viral
    diseases in cassava plants [35]. Nowadays, AI and DL are collaborated to ease
    the process of plant disease detection. PlantVillage Nuru has been integrated
    with the West African viral epidemiology platform (WAVE 2) to track the spread
    of cassava brown streak disease. Out of these applications, the most extensive
    use of UAVs in agriculture has been for the detection of stress in plants and
    quantification. This might be because of the impact that early detection renders
    in overall agriculture activity [1]. Early detection of the plant’s biotic and
    abiotic stresses helps to understand the changing physiology of the plant. If
    the stresses are detected early, suitable treatments can be used to protect plants
    and reduce loss. This review paper intends to cover the basic areas of the UAVs
    in the automatic detection of plant diseases, their components, image and data
    processing, and different models of data analysis. The major objective of this
    paper is to provide a holistic explanation of how UAVs can be used to automatically
    monitor the health status of the crop in the field. It provides insight into basic
    concepts of UAV peripherals, sensors, and cameras along with their limitations
    and applicability. In that regard, it further explains the performance, advantages
    and limitations of different deep learning models. Nonetheless, in situ detection
    of plant diseases using UAVs is expanding with many challenges. This paper tries
    to explore different UAV platforms and their limitations and advantages, cameras,
    and sensors with their spectral specifications to capture images and acquire data
    for monitoring and detecting plant diseases. It also reflects different methods
    of processing acquired data, the challenges in the process and the prospects of
    autonomous identification of plant diseases using Unmanned Aerial Vehicles. 2.
    Types of UAVs, Their Platforms and Peripherals Used in Disease Monitoring and
    Identification Nowadays, as discussed earlier, different agricultural activities
    have been carried out by UAVs. Among them, UAVs are increasingly used for disease
    monitoring and identification. They work together with different components such
    as cameras, sensors, motors, rotors, controllers, etc. One of the basic uses of
    UAV is to capture images. The information contained in images is extracted and
    transformed into useful information by image processing and deep learning tools
    [1,26]. Electromagnetic spectra also provide useful information, which is used
    to make decisions regarding plant physiological stress [33,36]. The comparison
    between the spectroscopy in the specific region helps to assess the condition
    of the plants in real-time under field conditions [33]. Plant disease is identified
    by observing the physiological disturbances caused by foliar reflectance in a
    near-infrared portion of the spectrum in a UAV captured image [36]. In addition,
    the disturbances in the photosynthetic activities of the crops caused by many
    diseases are also observed as reflectance in the red wavelength range. There are
    various types of UAVs for different agricultural operation purposes. In relation
    to UAVs, these UAV structures are called platforms [4]. Primarily, there are two
    types of agricultural UAV platforms: fixed wing and rotatory wing [25]. A fixed-wing
    UAV is comparatively larger in size and used for large-area coverage [4,25,37,38].
    Rotary-wing UAVs are further divided into two types: helicopter and multirotor.
    Helicopters have a large propeller at the top of the aircraft and are used for
    aerial photography and spraying. Similarly, multirotors have different varieties
    depending upon the number of motors the aircraft possesses. Different types of
    multirotors are the quadcopter (four rotors) [39,40,41], hexacopter (six rotors)
    [42], and octocopter (eight rotors) [43]. For the purpose of plant disease monitoring
    and identification, both fixed-wing and rotary UAVs have been used. To monitor
    leaf stripe disease of grapevine, one of the diseases of the esca complex, Di
    Gennaro and his colleagues used a modified multirotor Mikrocopter OktoXL (HiSystems
    GmbH, Moomerland, Germany) [36]. Similarly, target spot and powdery mildew in
    soybean have been identified using a popular quadcopter, Phantom-3 (Shenzhen DJI
    Sciences and Technologies Ltd., Shenzhen, China) [26]. Xavier and colleagues (2019)
    used a multirotor UAV to identify Rumaria leaf blight disease in cotton [44].
    A hexacopter, DJI Matrice 600 pro (Shenzhen DJI Sciences and Technologies Ltd.,
    Shenzhen, China), was used to detect target spot and bacterial spot in tomato
    leaves [45]. In 2013, Garcia-Ruiz and colleagues used fixed-wing multirotor UAVs
    to compare the aerial imaging platforms for identifying citrus greening disease
    in Florida [33]. Gomez Selvaraj et al. [46] used a multicopter, UAV Phantom 4
    Pro (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen, China), to identify
    major diseases in bananas. An Altura X8 octocopter (Aerialtonics DV B.V., Katwijk,
    The Netherlands) was used in monitoring fire blight in pear orchards in Belgium
    [47]. In a similar fashion, an octocopter (DJI S1000) (Shenzhen DJI Sciences and
    Technologies Ltd., Shenzhen, China) was used in spatio-temporal monitoring of
    yellow rust in wheat in China [48]. A helicopter was modified to carry a camera
    system, an autopilot, and sensors to obtain thermal imagery and multispectral
    imagery over the agricultural fields [49]. A package of sensors was built into
    the DJI S800 hexacopter (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen,
    China) to identify citrus greening disease [50]. Özgüven [51] used a rotary UAV–DJI
    Phantom 3 Advanced Brand (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen,
    China) to determine Cercospora leaf spot in sugar beet. Valasek et al. [52] used
    rotary UAV to distinguish leaf spot (Cercosporidium personatum) among healthy
    and diseased peanuts. Sugiura et al. [53] used a multirotor UAV to determine late
    blight resistant cultivars of potato. As we can see, both fixed and rotary UAVs
    have been used for crop monitoring purposes. The choice of the platform simply
    depends upon the area to be covered by the UAVs, payload, and nature of the study.
    For example, fixed-wing UAVs can cover a larger area than rotary-wing UAVs in
    the same amount of time. Additionally, fixed-winged UAVs have faster flight speed
    and can take comparatively bigger payloads than rotary UAVs. Rotary-winged UAVs
    are small and confined to easily access the congested area. Rotary UAVs are mostly
    used for aerial photography and videography, whereas fixed-winged UAVs are best
    suited for aerial mapping. Because of their shape and size, fixed-wing UAVs cannot
    hover in place and need definite spots to launch and land. Moreover, multirotors
    are cost-effective in terms of total area coverage and easy to handle as compared
    to fixed-winged rotors. Despite their larger size and difficulty in maintaining
    control, larger and fixed-winged UAVs can be effectively used in large fields
    because of their cost-efficiency in the total field coverage per flight duration
    [54]. 3. Cameras and Sensors Aerial imaging is one of the most important factors
    when it comes to the application of UAVs. Usually, the image quality determines
    the selection of UAVs. However, the type of sensor and the purpose of the study
    also dictate the choice of platform. Thus, the UAV platforms are loaded with different
    cameras and sensors. For example, a DJI Mavic 2 UAV is upgraded with a Sentera
    single NDVI camera (Sentera, Saint Paul, MN, USA) to monitor the drought stress
    in crapemyrtle plants (Figure 1). However, UAV platforms are limited to their
    payload. An increase in payload decreases the speed, stability, and flight time
    of the UAV [4,23]. The choice of sensors depends upon the purpose of the study.
    Drought stress is better observed using thermal sensors in the early stages [55,56],
    while multispectral and hyperspectral sensors are used for long-term results.
    Pathogen infections in crops are better diagnosed using hyperspectral and thermal
    sensors in the early stages, but RGB sensors, multispectral, and hyperspectral
    can be used to detect the severity of infection in later stages [57]. In this
    subsection, we will discuss different cameras and sensors used in plant disease
    monitoring and observation. Figure 1. DJI Mavic air UAV with single NDVI camera
    over the crapemyrtle (Lagerstroemia spp.) field at Tennessee State University,
    Otis L. Floyd Nursery Research Center, McMinnville, TN for automatic detection
    of drought stress in the plants. 3.1. RGB Camera RGB (Red–green–blue) cameras
    are commonly used types of cameras that produce images that measure the intensity
    of three colors and define values of each color in pixels: red, green, and blue.
    RGB cameras are used to generate 3-dimensional (3D) models of agricultural crops
    [58,59,60] and provide an estimation of crop biomass [61,62,63]. RGB cameras are
    also used with NIR and multispectral cameras to improve accuracy while calculating
    the biomass [62]. If the near-infrared filter is replaced by a red filter, it
    is called a modified RGB camera [64,65]. Commercial RGB cameras are cheap and
    have poor spectral resolution [65]. However, Grüner, Astor, and Wachendorf [61]
    calculated the biomass of grassland with an RGB camera using only SfM processing.
    RGB cameras have also been successfully used to identify diseases in plants. It
    is worth mentioning that RGB cameras only have the electromagnetic spectrum range
    of 380 nm to 750 nm, and not all of these wavelengths are suitable for appropriate
    crop disease detection [66]. The optical properties and the spectral range of
    the camera are considerable factors in plant disease detection. Mattupalli and
    his colleagues confirmed Phymatotrichopsis root rot disease in an alfalfa field
    using RGB imaging with a maximum likelihood classification algorithm [67]. RGB
    images were used to detect banana bunchy top virus and banana Xanthomonas wilt
    diseases in the African landscape of the Congo [46]. Similarly, an RGB camera
    with 12 megapixels was used to classify leaf spot (Cercospora beticola Sacc.)
    severity in sugarbeets in Turkey [51]. Grapevines were classified as diseased
    or healthy in France using RGB sensors [68]. Valasek, Thomasson, Balota, and Oakes
    [52] classified the leaf caused by Cercosporidium personatum spot of peanut using
    RGB cameras. Potato late-blight-resistant cultivars are screened using RGB cameras
    [53]. In a study conducted by Ashourloo et al. [69], wheat leaf rust (caused by
    Puccinia triticina) was detected by using RGB digital images with varying reflectance
    at 605, 695 and 455 nm wavelengths. RGB images also provides information on LAB
    (L = lightness, A and B are color opponent dimensions), YCBCR (Y = luma component,
    CB = blue difference and CR = red difference chroma components), HSV (hue, saturation,
    value), etc., which are very helpful in recognizing plant diseases. RGB cameras
    are readily available and are typically less expensive. They can also be used
    to collect high-resolution still images. However, they can only measure three
    bands (red, green and blue) of the electromagnetic spectrum. This causes RGB images
    to be less accurate than multispectral or hyperspectral images in terms of the
    spectral resolution of the camera system. Images from RGB cameras do not provide
    sufficient information to differentiate levels of sheath blight in rice [70].
    However, one of the major advantages of RGB cameras is their ability to capture
    high spatial resolution images in comparison to multispectral systems and, in
    turn, provide finer spatial details for plant disease detection and monitoring.
    RGB cameras should be carefully operated in order to have uniform coloring and
    lighting in the images. Uniform images will reflect fewer errors in differentiating
    healthy and diseased plants. 3.2. Multispectral Cameras Multispectral cameras
    are considered to be one of the most appropriate sensors for agricultural analytics,
    as they have the capacity to capture images in high spatial resolution and determine
    reflectance in near infrared bands [71]. Multispectral and NIR cameras create
    vegetative indices that rely on near-infrared or other bands of light [72,73,74].
    Multispectral cameras use different spectral bands: mostly red, blue, green, red-edge,
    and near-infrared. They can be differentiated into two groups based on bandwidth:
    narrowband and broadband [75]. Most of the aerial images for monitoring crop health
    issues use multispectral cameras [4] as they are used to calculate indices such
    as NDVI and others including NIR [63,71,73,76,77]. The absence of multispectral
    sensors in agricultural UAVs will hinder the early detection of plant diseases.
    The evaluation of multispectral image bands captured in an aerial image at different
    heights was carried out in 2019 by Xavier and colleagues to detect Ramularia leaf
    blight in cotton. However, the differentiation in the severity index was not significant
    [44]. In a study conducted by Abdulridha, Ampatzidis, Kakarla, and Roberts [45],
    35 vegetative indices were used to detect target spot and bacterial spot in tomatoes
    under laboratory and field conditions. A fixed-wing UAV capable of capturing hyperspectral
    images equipped with multiband sensors was used in 2012 in Florida to compare
    aerial imaging platforms in identifying citrus greening disease [33]. In the same
    study, both the multispectral and hyperspectral cameras were installed in the
    UAV. The multispectral camera used was six narrow-band cameras (miniMCA6, Tetracam,
    Inc., Chatsworth, CA, USA), having six digital sensors with customizable bands
    of 10 nm and arranged in a 3 × 2 array. Similarly, the hyperspectral camera used
    in the study was the AISA EAGLE Very Near Infra-red (VNIR) hyperspectral imaging
    sensor (Specim Ltd., Oulu, Finland). This imaging sensor has a spectral range
    of 398–998 nm and a resolution of around 5 nm. The same camera had 128 spectral
    bands for the VNIR region [33]. A combination of RGB images and multispectral
    images captured aerially using UAVs were used for pixel-based classification of
    banana diseases in the Democratic Republic of Congo. The camera system, Micasense
    RedEdge (MicaSense, Inc., Seattle, WA, USA), was capable of acquiring a 16-bit
    raw image in five narrow bands. Su, Liu, Hu, Xu, Guo, and Chen [48] used a multispectral
    camera—RedEdge—having a resolution of 1280 × 960 pixels and five narrow bands.
    A six-band multispectral camera (MCA-6, Tetracam, Inc., Chatsworth, CA, USA) was
    used, having an image resolution of 1280 × 1024 pixels, 10-bit radiometric, and
    optical focal length of 8.5 mm [49]. An RGB-Depth (RGB-D) camera, employed with
    two grayscale cameras (mvBlueFOX-MLC202bG), covering light sources with polarizing
    films and the multispectral sensors mounted to UAV platform, was used to monitor
    orange orchards for detecting citrus greening disease in Florida [50]. Al-Saddik
    et al. [78] used multispectral sensors to differentiate Flavescence dorée diseased
    and healthy grapevines in a vineyard without using common vegetative indices.
    Several other studies also used multispectral cameras [36,48,70,79,80,81,82,83].
    A combination of visible and infrared images was used to form a multispectral
    approach for the identification of vine diseases [84]. The platform was combined
    using an RGB camera and an infrared light sensor with a wavelength of 850 nm;
    both cameras had 16-megapixel high-resolution properties. In the study, the accuracy
    was varied between 70% to 90% depending upon the surface area, with a larger surface
    having higher accuarcy. Similarly, a multispctral sensor, OptRxTM -Ag Leader,
    was used at visual range of 670 nm, RedEdge 730 nm, and NIR 775 nm to identify
    the esca complex and Flavescence dorée in the vineyard [85]. A multispectral camera
    consisiting spectra of blue (475 nm), green (560 nm), red egde (720 nm), and near
    red (840 nm) was used to identify pine wood nematode caused by Bursaphelenchus
    xylophilus [86]. The accuracy observed in the study was 79%. Ye et al. [87] used
    a five-band multispectral camera having blue (465–485 nm), green (550–570 nm),
    red (653–673 nm), red edge (712–722 nm), and near infrared (800–880 nm) spectral
    ranges to classify banana wilt disease and obtained an accuracy of 80%. Multispectral
    sensors have high practicability for the new innovations of the automatic identification
    of plants. They can capture images in both visible and near infrared regions,
    but they may be limited while detecting subtle changes in the biophysical and
    biochemical parameters. Various studies have reported that multispectral cameras
    are best suited to identify plant diseases and pests in the field. The principle
    behind high accuracy is multiple bands of the electromagnetic spectrum. They not
    only provide additional information to the acquired images but can also provide
    vegetative indices. Vegetative indices are one of the most important factors in
    the identification of crop diseases. There are a few disadvantages of multispectral
    cameras, which include expensiveness and increased effort for calibration for
    specific tasks, such as disease identification, image processing, etc. 3.3. Hyperspectral
    Cameras The major difference between multispectral and hyperspectral cameras is
    that hyperspectral cameras collect light of different narrow size bands for every
    pixel in the image captured [72,88]. Though multispectral cameras are able to
    capture light reflected by biomolecules, the differences lie in the bandwidth
    and placement of the light, which helps us to isolate responses from the different
    molecules. These cameras have particular properties in detecting lights emitted
    from biomolecules such as chlorophyll [89,90], mesophyll [88], xanthophyll [91],
    and carotenoids [89,90]. The major drawback of using a hyperspectral camera is
    the high cost of cameras [72,92] and the huge amount of unnecessary data when
    not properly calibrated [88,93,94,95]. Abdulridha and colleagues [45] used a hyperspectral
    imaging system (line-scan imager system), Pika L 2.4 (Resonon Inc., Bozeman, MT,
    USA) with a 23 mm lens, that had a spectral range of 380–1020 nm and 281 spectral
    channels to detect diseases in tomato leaves. Fire blight in pears was monitored
    using a hyperspectral camera (frame-based system) (COSI-cam, VITO NV, Boeretang,
    Belgium) with a spectral range of 600–900 nm [47,96]. Images were captured in
    rapid succession of 340 frames/s in an 8-bit mode [47,97]. The overall accuracy
    was found to be 52% for the detection of healthy and infected trees. However,
    red wavelength (611 nm) and NIR (784 nm) were found to have an accuracy of 85%
    in distinguishing healthy and diseased trees [47]. Calderón et al. [98] and Sandino
    et al. [99] both used line-scan imagers hyperspectral cameras in their experiments
    to monitor crop health. The thermal, multispectral, and hyperspectral cameras
    were able to successfully detect the crop crown temperature, structural indices,
    fluorescence, and health index of the olive tree [100]. Similarly, in the study
    conducted by Sandino, Pegg, Gonzalez, and Smith [99], the identification of diseased
    trees was achieved at 97% accuracy and for healthy trees were at 95% accuracy,
    whereas the global multiclass detection rate was 97%. The main reason why hyperspectral
    cameras are used is to reduce the shortcomings of multispectral cameras. Hyperspectral
    cameras are used to capture details in fewer spectral differences and to identify
    and discriminate target objects. The major breakthrough that hyperspectral cameras
    have provided is that, unlike other cameras [101], they are able to detect plant
    stress with possible causative agents (pathogen/disease). Hyperspectral sensors
    basically measure several hundred bands of the electromagnetic spectrum to derive
    accurate results. They are able to measure visible spectrum (400–700 nm), near
    infrared (700–1000 nm), and also short-wave infrared (1000–2500 nm). As a result,
    they not only collect images but a huge amount of spectral data as well, which
    causes difficulties in extracting relevant information. 3.4. Thermal Cameras Thermal
    cameras capture infra-red lights in the range of 0.75 to 1000 µm and provide the
    temperatures of the objects in the form of a thermal image [102]. The advantages
    of thermal cameras are the low cost as compared to other spectral cameras and
    that RGB cameras can be converted to thermal cameras with certain modifications
    [103]. Originally, thermal cameras were used for inspecting drought stress in
    crops [92,103,104,105]. Thermal images include the temperature of the surrounding
    objects and have low resolution compared to images captured by other major cameras
    [102]. Thermal sensors are also used in detecting crop diseases and monitoring
    crops [80,98,106]. Thermal cameras are capable of identifying responsible agents
    for plant stress. As the pathogen infects, the structure and metabolism of the
    plant change, which can be detected by thermal sensors [107,108]. Baranowski et
    al. [109] reported that fungal stress caused by Alternaria in oilseed rape was
    identified using thermal imaging. Similarly, early detection of red leaf blotch
    on almonds is also conducted using hyperspectral and thermal imagery [110]. Many
    other studies carried out using thermal cameras are detection of Huanglongbing
    disease of citrus [111], tobacco mosaic virus [112], tomato powdery mildew [110],
    Fusarium wilt of cucumber [113], etc. A thermal FLIR camera was used to detect
    disease-induced spots on banana leaves with an accuracy of 92.8% [114]. Similar
    technology was used by Raza, Prince, Clarkson, and Rajpoot [108] to detect tomato
    powdery mildew caused by Oidium neolucopersici. This shows that the use of thermal
    cameras is increasing in crop health monitoring and disease identification. However,
    thermal imaging technology has not been fully explored. This may be because of
    environmental factors considered during image acquisition. Additionally, they
    contain huge amounts of information along with the image, which leads to challenges
    in deriving relevant information. The advantages of thermal imaging are low cost
    and early identification with causative agents. Yang et al. [115] developed a
    method for early detection of diseases in tea using thermal imagery. 3.5. Depth
    Sensors Depth sensors are common peripherals used in agricultural UAVs that provide
    extra feature-depth in the RGB pixels. The depth in a depth sensor is defined
    as the distance between the sensor and the point of an object at the time of image
    capture [116]. The most prevalent depth sensor technology is Light Detection and
    Ranging (LiDAR). The major difference between RGB-D sensors and LiDAR is that
    RGB-D sensors depend upon the light reflection intensities, whereas LiDAR uses
    laser pulses to calculate distance [23]. However, RGB-D sensors are one of the
    least commonly used sensors in the case of plant health monitoring but are commonly
    used in spraying [4], 3D modelling [117,118], and phenotyping [116]. Nowadays,
    depth sensors are used to increase the accuracy of the sensors. Sarkar et al.
    [50] used RGB-D cameras for detecting citrus greening disease. Similarly, Xia
    et al. [119] used depth sensors to create 3D segmentation of the individual leaf.
    There are many sensors available in the market to provide accurate 3D information,
    such as LiDAR and time of flight (ToF), but these are highly expensive and cannot
    be used for general agriculture practices. The use of depth cameras provides both
    the pixel intensity and depth in the captured image to develop a classifier during
    disease identification. Moreover, depth sensors provide data on the intensity
    of the light reflected from stressed objects or plants [120]. Paulus et al. [121]
    used 3-D laser technology to detect Fusarium spp. in kernels of wheat. The challenge
    of using depth sensors is that the sensors may not detect objects after a certain
    distance, which may result in a lower intensity count. This can be eliminated
    by using a configurable camera and calibrating it with depth sensors. 4. Image
    Pre-Processing Image pre-processing involves a series of steps before extracting
    data from the images. The major objective of image pre-processing is to reduce
    errors and to prepare to extract data from the image. After the high spatial geo-referenced
    aerial images are captured using UAVs, a large amount of data has to be extracted.
    Thus, it is very important that the images are error-free. The images may have
    been degraded while capturing due to noise, shadow, etc. It is very important
    to have critical knowledge of plant diseases for pre-processing the images and
    choosing an appropriate method to increase the accuracy of identification [122].
    Image pre-processing includes a series of steps to make it appropriate to extract
    data from the images. These include enhancement of images, their segmentation,
    color space conversion, and filters [123]. However, Sonka et al. [124] categorize
    the steps of image pre-processing as pixel brightness transformation, geometric
    transformation, local pre-processing, and image restoration. Pixel brightness
    transformation modifies the brightness of the image depending on the pixel of
    the image, for which they have two classes: brightness correction and grey scale
    transformation. Similarly, geometric transformation correlates the coordinates
    of the input image pixel with the points in the output image and determines the
    brightness of the transformed point in the digital raster. Local pre-processing
    utilizes the neighborhood of the pixel to generate the new brightness value in
    the output image. Finally, image restoration is the method in pre-processing that
    is used to discard degradation factors, such as lens defects, wrong focus, etc.,
    in the image. After image pre-processing, data from the images are extracted to
    use for data processing. The accuracy of the image classification depends on the
    type of image pre-processing and extraction techniques used. Studies have reported
    that image processing helps to improve the information in the image and can be
    more easily interpreted than non-processed images [70]. The images are generally
    pre-processed using different available software to orthorectify spectral bands
    in reflectance. There are varieties of software available to process raw images
    depending upon the sensors and cameras used. Ghosal et al. [125] used an unsupervised
    explanation framework to isolate symptoms during image pre-processing. In the
    study conducted by Ferentinos, a strange result was found while detecting diseases
    using open dataset images. The overall accuracy of real images was higher than
    the pre-preprocessed images [126]. Headwall SpectralView® (Headwall Photonics
    Inc., Bolton, MA, USA) and CSIRO|Data61 Scyven 1.3.0. (Scyllarus Inc., Canberra,
    Australia) software was used for isolating regions of interest and reflectance
    data by Sandina and colleagues [99,127,128]. The Simple Linear Iterative Clustering
    (SLIC) super pixels method was used by Tetila, Machado, Menezes, Da Silva Oliveira,
    Alvarez, Amorim, De Souza Belete, Da Silva, and Pistori [27] to segment leaves
    from the UAV captured images. Environment for Visualizing Images (ENVI) software
    (version 4.7, ITT VSI, White Plains, NY, USA) was used to reflect bands in the
    images from UAVs by Garcia-Ruiz, Sankaran, Maja, Lee, Rasmussen, and Ehsani [33].
    PixelWrench 2 software (Tetracam Inc., Chatsworth, CA, USA) was used for correcting
    radiometric distortion and vignetting images in identifying Ramularia leaf blight
    [44] and grape leaf stripe diseases [36]. Precision Hawk (Precision Hawk USA Inc.,
    Raleigh, NC, USA) provides an image pre-processing facility based on its cloud
    server [6]. Image processing for detection of cotton root rot was carried out
    using Pix4D software (Pix4D SA, Lausanne, Switzerland). The selection of the image
    processing features depends upon the purpose of the study. For example, if the
    diseased leaf contains an unwanted object, it can simply be removed by cropping
    to obtain the target image. This process is called image clipping, a part of image
    pre-processing. Similarly, the image enhancement can be performed using the histogram
    equalization technique to equally distribute the color intensities in the diseased
    plant images. Additionally, if the image contains shadow and has low contrast,
    this can be enhanced by removing blur [129]. Image pre-processing helps to identify
    target regions in the images and also helps to reduce noise in the images. It
    increases the reliability of the optical inspection. Images captured from the
    multispectral and hyperspectral sensors require atmospheric and radiometric calibration
    and correction of data in order to ensure consistency in the data from the image.
    5. Data Processing Data processing of the UAV images is carried out using various
    tools. Most of the researchers use vegetation indices for data processing because
    they are easy and readable. Currently, different Artificial Neural Networks (ANNs)
    are used for result demonstrations. In most of the studies, researchers also use
    numerical values from the sensors and use different statistical tools such as
    K-means clustering, Receiver Operator Characteristics (ROC) analysis, and regressions.
    However, it also depends upon the purpose of the study for the selection of data
    processing tools. Some of the data processing tools are discussed below. 5.1.
    Image Data Processing Data processing in UAVs is conducted in several ways. After
    the vignetting and orthorectification of the images, some of the results are derived
    using vegetation indices (VIs) only. However, imagery data analysis is more than
    that. It also includes image segmentation and result interpretation in the form
    of images. For those types of image data processing, various tools are used. Some
    of these tools are Artificial Neural Networks (ANNs), Decision Trees, K-means,
    k nearest neighbors, Support Vector Machines (SVMs), and Regression Analysis.
    In this review, a short description of k-means clustering and regression analysis
    is given. 5.1.1. K-Means Clustering Altas and his colleagues converted RGB images
    into L*a*b color space to identify Cercospora leaf spot in sugar beet, where a*b
    are the components in which the information about diseases in the leaves is stored.
    The colors in a*b are classified using K-means clustering [51]. K-means clustering
    is a common clustering algorithm used in various application domains, such as
    image segmentation [130], which divides a dataset into k groups [131]. In k-means
    clustering, an initial k cluster center is defined, and then the algorithm repeatedly
    selects other k values within that dataset. In the datasets for which the value
    of k is already known (such as Unique client identifiers, e.g., customer IDs),
    the same k value is used. For those where the k value is not known, the k value
    should be identified separately [132]. For the study to determine leaf spot in
    sugar beet, k value was issued as 3 since three images are obtained when pixels
    in the RGB image were separated according to color [51]. 5.1.2. Regression Analysis
    Regression analysis is one of the most common methods of analyzing UAV imagery.
    After orthorectification of images, the values for each band, such as NIR, Red,
    Green, Blue, etc., are extracted. For those extracted values, the regression model
    is run to investigate the spectral characteristics of the parameters. Generally,
    different types of regression analysis models are run depending upon the type
    of dataset acquired: linear and non-linear and simple and multiple regression
    analysis. At the end of the large dataset analysis, the cross-validation of regression
    (RA) model is important because when a model is chosen, it is predicted that the
    observation will be the same in the future as well, but it may not always be similar
    [133]. Thus, data are split, and two portions are formed. A set is used to form
    a regression analysis model, while the other set is reserved as a future observation
    to fit into the model. However, it is not required that both of the split datasets
    should be equal [133]. 5.1.3. Vegetation Indices Vegetative indices are one of
    the major analysis tools for analyzing aerial images. They are the numeric representation
    of the relationship between different wavelengths of lights that are reflected
    from the plant surface [90]. There are various vegetative indices to describe
    the status of plants. They include NDVI [4], Optimized Soil-Adjusted Vegetation
    Index (OSAVI) [134,135], and CSWI [104,136]. A detailed study of different remote
    sensing vegetation indices is conducted by Xue and Su [137]. These indices are
    correlated with different imaging sensors to derive a conclusion. Excess Red (ExR),
    Excess Green (ExG), and Excess Blue (ExB) can be used with RGB imaging. The equation
    for ExR, ExG and ExB are as below: E × R = 1.4R − G (1) E × G = 2G – R − B (2)
    E × B = 1.4B − G (3) where R represents red, G represents green, and B represents
    blue [4,15,18,138]. The physiological and morphological properties of the plant,
    such as water content, biochemical composition, nutrient status, biomass content,
    and diseased tissues, are reflected in the values of vegetative indices [70,76,139].
    The most common vegetative index to determine diseased tissue is NDVI [104]. The
    range of NIR is 780–800 nm, and R is 670–700 nm, as both come from reflected light
    [62,73,104]. The intensities of these lights are measured by multispectral or
    hyperspectral cameras and formatted to an NDVI map later. At the end of formatting,
    each pixel of the NDVI map represents the value of the crop. It is also important
    to note that vegetation indices can be combined to form other different vegetation
    indices (VIs). Therefore, we can assume that different VIs are evolving during
    the writing of this paper. Different studies using VIs to process the data include
    the esca complex in a grape vineyard detected by using NDVI [36]. Similarly, Albetis
    et al. [140] used NDVI and 10 other VIs to differentiate symptomatic and asymptomatic
    Flavescence dorée in grapevines. Analyzing and comparing vegetation indices between
    diseased and healthy crop samples is widely used in monitoring crop health. The
    use of VIs is considered simple, easy, and comparatively reliable in terms of
    disease identification and monitoring. 6. Deep Learning Models Deep Learning (DL)
    is an approach under Machine Learning (ML) where a computer model resembles the
    biological pathways of a human [141]. Deep learning includes the use of artificial
    neural networks that contain various numbers of processing layers different from
    traditional neural networks [126]. It is actually the inclusion of several steps
    from data collection to the classification of the images and interpretation of
    results. 6.1. Artificial Neural Networks (ANNs) The use of neural networks for
    disease recognition in agriculture crops is rapidly increasing [126,142,143].
    A commonly used deep learning tool for image processing and classification is
    the Artificial Neural Networks (ANNs) [8]. ANNs are mathematical models that work
    in the same fashion as the human brain does with neurons and synapses to connect
    each other [126]. The neural networks are trained into a model using previously
    known data and are programmed to work on a similar set of data. There are different
    kinds of artificial neural networks for image classification: Recurrent Neural
    Networks (RNNs), Convolutional Neural Networks (CNNs), and Generative Adversarial
    Networks (GANs). The most common neural networks used for plant disease detection
    and classification are convolutional neural networks (CNNs). 6.2. Convolutional
    Neural Networks (CNNs) CNNs are basic deep learning tools to identify plant diseases
    using aerial imagery [144]. They consist of powerful modelling techniques performing
    complex pattern recognition that have large amounts of data, such as in image
    recognition [126]. Studies that do not have large amounts of data to run neural
    networks augment the data [27]. CNNs are the successors of previous ANNs. ANNs
    were designed to apply in fields, having repeating patterns such as image recognition
    of diseased plants. For the identification of diseases in plants using CNNs, several
    algorithms have been successfully applied, making crop health monitoring easier
    than before. The major CNN algorithms or architectures used are AlexNet [145,146],
    AlexNetOWTBn [147], GoogLeNet [146], Overfeat [148], and VGG [149]. The different
    studies that used CNNs to identify plant diseases are listed in Table 1 below.
    Table 1. Different deep learning CNN models used for the automatic identification
    of diseases in plants. The performance of the deep architectures also depends
    upon the number of modifying images, minibatch sizes, weight differences, and
    bias learning rate [158]. A study by Too et al. [159] reported that DenseNets
    showed higher accuracy with no overfitting and degraded performance when compared
    with VGG 16, Inception V4, and ResNet. Similarly, AlexNet had higher accuracy
    when compared with SqueezeNet in classifying tomato diseases [160] whereas, AlexNet
    and VGG16 had similar accuracy in classifying tomato disease [158]. Mohanty, Hughes,
    and Salathé [151] used AlexNet and GoogLeNet to classify plant diseases and achieved
    an accuracy of 99.35%; however, they performed poorly when tested in different
    sets of images. Deep learning architectures are considered accurate as compared
    to the previously used models such as SVM and random forest methods [158]. The
    correct prediction percentage of CNN was reported 1–4% higher than SVM [161,162,163,164]
    and 6% higher than random forests [165]. However, Song et al. [166] reported that
    CNN models are 18% lower in correct prediction as compared to the Root Mean Square
    Error (RMSE). CNNs are widely used in the identification and classification of
    plant diseases using images. Crop classification using deep learning models helps
    for pest control, cropping activities, yield prediction, etc. [167]. Deep learning
    models have eased the work for the growers in the way that they can click the
    image of the picture in the field and identify the disease by uploading it to
    the software. Feature engineering, a complex process, is eliminated in CNN models
    as the important features are located during the training of the dataset. However,
    different architectures have their own pros and cons. As the layers extend, neural
    networks suffer from performance degradation, resulting in less accuracy. It is
    also time-consuming during training of the images as a large number of images
    are required. Deep networks experience an internal covariant shift, which causes
    disruption in input data and the training layer. However, different techniques,
    such as skip connections [168], layer-wise training [169], transfer learning,
    initialization strategies, and batch normalization, are used these days to overcome
    those challenges [159]. 7. Challenges of Automatic Plant Disease Identification
    Using UAVs Automatic plant disease detection primarily means the identification
    of biotic injury caused by pathogens in plants involving no direct human resources
    in the field. One way of automatically identifying plant disease is deploying
    UAVs with machine learning algorithms installed in them. Information gathered
    in the algorithms not only helps in identifying diseases but also estimating the
    severity of the disease [17]. Plants are infected with hundreds of pathogens in
    the field, and most of them exhibit similar symptoms. Appropriate identification
    of plant disease is one of the basic but challenging tasks in agricultural activities.
    Manually identifying plant disease is subject to bias and optical illusions, which
    result in errors [170]. It involves intensive labor with economic costs. Nevertheless,
    automatic disease identification also requires expert opinion for disease confirmation
    in specific cases. Scouting each plant using laboratory and molecular techniques
    is not practically possible for disease identification in a large area. Large
    and complex data obtained from optical sensors are able to detect disease quickly
    and classify between diseases, stress, and intensity of the diseases [120]. This
    incentivizes scientists and researchers to develop tools that are programmable
    and can read every plant through images to detect diseases. There has been progress
    in automatic monitoring of crop health using UAVs and imagery. However, the system
    of automatic detection and identification of plant diseases has still been experiencing
    difficulty with programming accuracy. Some of the challenges during automatic
    identification of plant diseases using UAVs are discussed by Barbedo [1,170],
    which include background noise, unfavourable field conditions, sensor limitations,
    symptom variations, limitation of resources (peripherals and cameras), and ‘Training’–validation
    discrepancy. In addition, most of the crop health monitoring activities using
    UAVs are dominated by RGB and NIR sensors or their combination. The accurate detection
    of plant disease requires more advanced sensors with higher spectral ranges, such
    as hyperspectral sensors. These sensors have the potentiality to distinguish specific
    features of the object with several hundreds of narrow spectral bands [171]. Moreover,
    the platform used and the image captured also require a considerable value in
    accurate crop health monitoring. There are different platforms and sources to
    acquire the images. Satellite images such as Landsat are available for free however
    have less resolution to correctly detect plant disease. Even the satellite images
    that are available with costs fall under the Visual-NIR region. Development of
    sensors to provide high-quality spatial, temporal and spectral information are
    undergoing [171,172]. Different new technologies have also been introduced to
    monitor crop phenology, such as sun-induced fluorescence and short-wave infrared
    for greenhouse gas monitoring. However, these sensors are highly affected by climatic
    conditions, such as clouds, sunlight, etc., and are only available on regional
    or global scales [173]. The limitations are extended to image processing, segmentation
    and classification as well. Different machine learning tools and architectures
    are available but contain limitations. Some of the major limitations during image
    processing, classification, and segmentation are low-resolution images, less accuracy,
    unavailability of large datasets to train the models, etc. Nguyen and Shah [174]
    found a huge discrepancy in the accuracy of their dataset and the PlantDisease
    dataset. The author recommends the semi-supervised approach to classify disease,
    which will create more diverse images. Similarly, Arsenovic et al. [175] suggested
    an improvement in the decision-making process. In a nutshell, the process of automatic
    identification of plant diseases using UAVs and deep learning can be improved
    by choosing a high-quality image capturing camera, appropriate sensors (RGB, multispectral
    or hyperspectral) depending upon the purpose of study, enough datasets to accurately
    train the model, and selecting appropriate architecture for the deep learning
    model. Apart from these various challenges regarding image analysis and result
    interpretation during automatic detection of plant diseases using UAVs, challenges
    regarding their usage and application in the field exist. The regulatory body
    for controlling UAVs’ application, Federal Aviation Administration (FAA), has
    various regulations such as limits on the height, operating areas, and zones.
    In 2018, FAA applied legal clauses for the application of pesticides using UAVs
    that the operator should receive permission following three exemptions and a waiver
    process [176]. The privacy around the operating and surrounding areas is strictly
    addressed [177]. Regulations on UAVs’ application and their handling in the international
    context is driven by International Civil Aviation Authority (ICAO). More details
    about the different authorities, protocols, and information about UAVs and their
    regulations in the global context are available in Stöcker et al. [178]. The UAV
    technology is expanding and becoming cheaper over time but still is not very applicable
    to many smallholding growers. The addition of hyperspectral and multispectral
    sensors, which are of the utmost importance for monitoring plant health in an
    already purchased UAV, adds more than $10,000 to the cost [4,72]. Similarly, the
    affordability of trained manpower for a small-scale farmer is still unrealistic.
    8. Future Considerations The Unmanned Aerial Vehicle has been a boon to automatic
    monitoring of crop status. Nonetheless, identifying the type of stress, either
    biotic and abiotic, is still vague. Researchers and scientists working together
    with UAV system manufacturers are merging to broaden possibilities. Many challenges
    are limiting progress. Different platforms and the types of sensors are already
    discussed in the sections above; however, all of them have their own pros and
    cons. Lightweight UAVs with high-resolution cameras can capture a better image
    that helps for the proper detection of diseases and reduces chances of error.
    Sensors also play a vital role in disease detection. Abdulridha, Ampatzidis, Kakarla,
    and Roberts [45] used a benchtop hyperspectral imaging system with a 23 mm lens
    having a spectral range of 380–1030 nm, 281 spectral channels 15.3° field view,
    and a spectral resolution of 2.1 nm to detect powdery mildew in squash. The wider
    the spectral ranges, the better the differentiation of disease symptoms and eventually
    helps to reduce the error. The selection of sensors and their spectral range is
    aided by the nature of the disease. For example, Xu et al. [179] used NIR spectroscopy
    and found that the best range for disease monitoring was 1450 nm and 1900 nm in
    tomatoes. Similarly, Al-Ahmadi et al. [180] utilized the same technique with the
    range of 900–2400 nm to monitor charcoal rot (Macrophomina phaseolina in soybean
    (Glycine max)). Scientists are working to develop a hybrid UAV that works as both
    fixed-wing and multirotor systems simultaneously. ALTI transition is a system
    [181,182] that serves both fixed-wing and multirotor systems when and where needed.
    A combined platform constituent of multiple sensors, such as RGB, NIR, RE infrared,
    and many others could be in the future, which will decrease the payload and could
    measure a variety of physiological parameters from the same sensor [183]. The
    increased flight duration of the UAV is the future of agricultural UAVs. With
    limited flight duration, it is challenging to distinguish the overall status of
    the crop. Interpreting results in the form of images rather than in the parametric
    value such as NDVI or other indices could be beneficial for growers to learn and
    execute management practices. Most of the decisions regarding crop health are
    based on the values of vegetative indices. Recent development in crop monitoring
    integrating UAVs and deep learning techniques offers concomitant crop counting,
    yield predictions, crop disease and nutrient deficiency detection [184]. Nebiker,
    Lack, Abächerli, and Läderach [83] utilized low-weight multispectral UAV sensors
    to predict grain yield and diseases in rape and barley. However, most of these
    integrations of concomitant yield and disease monitoring are only prototypes and
    are not available for commercial purposes [185]. The universal system developed
    by the American Society for Testing and Materials (ASTM) is called the G173 standard.
    The G173 standard system is used by different software to derive vegetation indices,
    such asNDVI, Green NDVI (GNDVI), Soil Adjusted Vegetation Index (SAVI), etc. However,
    the G173 standard accounts for the sun facing 37°, an average latitude of the
    continental states of the United States [186]. This develops a site-specific irradiance
    system that can provide a precise description of vegetation indices that will
    help in getting closer to a more accurate result using the Simple Model of Atmospheric
    Radiative Transfer of Sunshine (SMARTS). SMARTS stands on the base of the ASTM
    G173/G177 standard to provide more local solar irradiance spectra [187]. Scientists
    from Israel and Italy have launched hyperspectral imaging sensors to the orbit
    of the earth named the Space-born Hyperspectral Applicative Land and Ocean Mission
    (SHALOM). SHALOM is expected to work in the field of environmental quality and
    assist in precision agriculture in Israel and Italy [9,188]. Similarly, Fluorescence
    Explorer (FLEX) is a satellite to be launched by the European Space Agency (ESA)
    in 2022 [189,190]. FLEX is comprised of three instrumental arrays of fluorescence,
    hyperspectral reflectance, and canopy temperature. This satellite is expected
    to observe the vegetative fluorescence of crops at the global level [189,191,192].
    Further emphasis on chemometric or spectral decomposition should be given for
    the derivative method of analysis. Many researchers are creating their own datasets
    for training and validation. It has become important to develop agricultural datasets
    that will aid machine learning algorithms and help with accurate disease diagnosis
    [163]. In this review, we saw that the future of automatic detection of plant
    diseases is a combination of agriculture with machine learning. The development
    of robotic arms facilities in UAS, which can retrieve samples and return them
    for confirmation in the case of confusion, will help better diagnose plant diseases.
    Thus, machine learning, agriculture, and UAVs can act together to extend the realm
    of food security by limiting food loss due to pests and diseases. 9. Conclusions
    Rapid population growth and climate change are the leading causes of food insecurity.
    The advancements in UAVs and their systems to diagnose crop stress, pests, and
    diseases have greatly benefitted growers. Increasing farm productivity and lowering
    the cost of production using advanced technology is helping growers to increase
    yields and sustainability on their farms. The development in the automatic detection
    of plant diseases using UAVs has emerged as a novel technology of precision agriculture.
    UAVs are accurate and provide large amounts of data regarding crop status, which
    aids in making management decisions. However, there is still immense opportunity
    in plant disease diagnosis. As discussed in the future considerations section,
    the development of various algorithms of machine learning and collaboration with
    the other stems will help to reach this milestone. Author Contributions Writing—original
    draft preparation, K.N.; writing—review and editing, F.B.-G.; funding—received,
    F.B.-G. All authors have read and agreed to the published version of the manuscript.
    Funding This work was supported by the National Institute of Food and Agriculture,
    United States Department of Agriculture Capacity Building grant, under award number
    2019-38821-29062. Institutional Review Board Statement Not applicable. Informed
    Consent Statement Not applicable. Conflicts of Interest The authors declare no
    conflict of interest. References Barbedo, J.G.A. A review on the use of unmanned
    aerial vehicles and imaging sensors for monitoring and assessing plant stresses.
    Drones 2019, 3, 40. [Google Scholar] [CrossRef] [Green Version] Barbedo, J.G.A.;
    Koenigkan, L.V. Perspectives on the use of unmanned aerial systems to monitor
    cattle. Outlook Agric. 2018, 47, 214–222. [Google Scholar] [CrossRef] [Green Version]
    Beloev, I.H. A review on current and emerging application possibilities for unmanned
    aerial vehicles. Acta Technol. Agric. 2016, 19, 70–76. [Google Scholar] [CrossRef]
    [Green Version] Hassler, S.C.; Baysal-Gurel, F. Unmanned aircraft system (UAS)
    technology and applications in agriculture. Agronomy 2019, 9, 618. [Google Scholar]
    [CrossRef] [Green Version] Hunt, E.R.; Daughtry, C.S.T.; Mirsky, S.B.; Hively,
    W.D. Remote sensing with simulated unmanned aircraft imagery for precision agriculture
    applications. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 4566–4571.
    [Google Scholar] [CrossRef] Shi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.;
    Rooney, W.L.; Shafian, S.; Rajan, N.; Rouze, G.; Morgan, C.L.; Neely, H.L.; et
    al. Unmanned aerial vehicles for high-throughput phenotyping and agronomic research.
    PLoS ONE 2016, 11, e0159781. [Google Scholar] [CrossRef] [Green Version] Zhang,
    C.; Kovacs, J.M. The application of small unmanned aerial systems for precision
    agriculture: A review. Precis. Agric. 2012, 13, 693–712. [Google Scholar] [CrossRef]
    Barbedo, J.G.A. Factors influencing the use of deep learning for plant disease
    recognition. Biosyst. Eng. 2018, 172, 84–91. [Google Scholar] [CrossRef] Singh,
    P.; Pandey, P.C.; Petropoulos, G.P.; Pavlides, A.; Srivastava, P.K.; Koutsias,
    N.; Deng, K.A.K.; Bao, Y. Hyperspectral remote sensing in precision agriculture:
    Present status, challenges, and future trends. In Hyperspectral Remote Sensing;
    Elsevier: Amsterdam, The Netherlands, 2020; pp. 121–146. [Google Scholar] Hashimoto,
    N.; Saito, Y.; Maki, M.; Homma, K. Simulation of reflectance and vegetation indices
    for unmanned aerial vehicle (UAV) monitoring of paddy fields. Remote Sens. 2019,
    11, 2119. [Google Scholar] [CrossRef] [Green Version] Oliveira, H.C.; Guizilini,
    V.C.; Nunes, I.P.; Souza, J.R. Failure detection in row crops from UAV images
    using morphological operators. IEEE Geosci. Remote Sens. Lett. 2018, 15, 991–995.
    [Google Scholar] [CrossRef] Murugan, D.; Garg, A.; Singh, D. Development of an
    adaptive approach for precision agriculture monitoring with drone and satellite
    data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2017, 10, 5322–5328. [Google
    Scholar] [CrossRef] Duan, S.-B.; Li, Z.-L.; Wu, H.; Tang, B.-H.; Ma, L.; Zhao,
    E.; Li, C. Inversion of the PROSAIL model to estimate leaf area index of maize,
    potato, and sunflower fields from unmanned aerial vehicle hyperspectral data.
    Int. J. Appl. Earth Obs. Geoinf. 2014, 26, 12–20. [Google Scholar] [CrossRef]
    Verger, A.; Vigneau, N.; Chéron, C.; Gilliot, J.-M.; Comar, A.; Baret, F. Green
    area index from an unmanned aerial system over wheat and rapeseed crops. Remote
    Sens. Environ. 2014, 152, 654–664. [Google Scholar] [CrossRef] Rasmussen, J.;
    Nielsen, J.; Garcia-Ruiz, F.; Christensen, S.; Streibig, J. Potential uses of
    small unmanned aircraft systems (UAS) in weed research. Weed Res. 2013, 53, 242–248.
    [Google Scholar] [CrossRef] Sandler, H.A. Weed management in cranberries: A historical
    perspective and a look to the future. Agriculture 2018, 8, 138. [Google Scholar]
    [CrossRef] [Green Version] Abdu, A.M.; Mokji, M.M.; Sheikh, U.U. Automatic vegetable
    disease identification approach using individual lesion features. Comput. Electron.
    Agric. 2020, 176, 105660. [Google Scholar] [CrossRef] She, Y.; Ehsani, R.; Robbins,
    J.; Nahún Leiva, J.; Owen, J. Applications of high-resolution imaging for open
    field container nursery counting. Remote Sens. 2018, 10, 2018. [Google Scholar]
    [CrossRef] [Green Version] Zortea, M.; Macedo, M.M.; Mattos, A.B.; Ruga, B.C.;
    Gemignani, B.H. Automatic citrus tree detection from UAV images based on convolutional
    neural networks. In Proceedings of the 2018 31th SIBGRAPI Conference on Graphics,
    Patterns and Images (SIBGRAPI), Paraná, Brazil, 29 October–1 November 2018. [Google
    Scholar] Yanliang, Z.; Qi, L.; Wei, Z. Design and test of a six-rotor Unmanned
    Aerial Vehicle (UAV) electrostatic spraying system for crop protection. Int. J.
    Agric. Biol. Eng. 2017, 10, 68–76. [Google Scholar] [CrossRef] Mulla, D.J. Twenty
    five years of remote sensing in precision agriculture: Key advances and remaining
    knowledge gaps. Biosyst. Eng. 2013, 114, 358–371. [Google Scholar] [CrossRef]
    Gabriel, J.L.; Zarco-Tejada, P.J.; López-Herrera, P.J.; Pérez-Martín, E.; Alonso-Ayuso,
    M.; Quemada, M. Airborne and ground level sensors for monitoring nitrogen status
    in a maize crop. Biosyst. Eng. 2017, 160, 124–133. [Google Scholar] [CrossRef]
    Chen, Y.; Stark, B.; Kelly, M.; Hogan, S.D. Unmanned aerial systems for agriculture
    and natural resources. Calif. Agric. 2017, 71, 5–14. [Google Scholar] [CrossRef]
    [Green Version] Anderson, K.; Gaston, K.J. Lightweight unmanned aerial vehicles
    will revolutionize spatial ecology. Front. Ecol. Environ. 2013, 11, 138–146. [Google
    Scholar] [CrossRef] [Green Version] Kim, J.; Kim, S.; Ju, C.; Son, H.I. Unmanned
    aerial vehicles in agriculture: A review of perspective of platform, control,
    and applications. IEEE Access 2019, 7, 105100–105115. [Google Scholar] [CrossRef]
    Castelao Tetila, E.; Brandoli Machado, B.; Belete, N.A.d.S.; Guimaraes, D.A.;
    Pistori, H. Identification of soybean foliar diseases using unmanned aerial vehicle
    images. IEEE Geosci. Remote Sens. Lett. 2017, 14, 2190–2194. [Google Scholar]
    [CrossRef] Tetila, E.C.; Machado, B.B.; Menezes, G.K.; Da Silva Oliveira, A.;
    Alvarez, M.; Amorim, W.P.; De Souza Belete, N.A.; Da Silva, G.G.; Pistori, H.
    Automatic recognition of soybean leaf diseases using UAV images and deep convolutional
    neural networks. IEEE Geosci. Remote Sens. Lett. 2020, 17, 903–907. [Google Scholar]
    [CrossRef] Van Evert, F.K.; Gaitán-Cremaschi, D.; Fountas, S.; Kempenaar, C. Can
    precision agriculture increase the profitability and sustainability of the production
    of potatoes and olives? Sustainability 2017, 9, 1863. [Google Scholar] [CrossRef]
    [Green Version] Lee, B.; Park, P.; Kim, C.; Yang, S.; Ahn, S. Power managements
    of a hybrid electric propulsion system for UAVs. J. Mech. Sci. Technol. 2012,
    26, 2291–2299. [Google Scholar] [CrossRef] von Bueren, S.K.; Burkart, A.; Hueni,
    A.; Rascher, U.; Tuohy, M.P.; Yule, I.J. Deploying four optical UAV-based sensors
    over grassland: Challenges and limitations. Biogeosciences 2015, 12, 163–175.
    [Google Scholar] [CrossRef] [Green Version] Hardin, P.J.; Hardin, T.J. Small-scale
    remotely piloted vehicles in environmental research. Geogr. Compass 2010, 4, 1297–1311.
    [Google Scholar] [CrossRef] Chang, C.Y.; Zhou, R.; Kira, O.; Marri, S.; Skovira,
    J.; Gu, L.; Sun, Y. An Unmanned Aerial System (UAS) for concurrent measurements
    of solar-induced chlorophyll fluorescence and hyperspectral reflectance toward
    improving crop monitoring. Agric. For. Meteorol. 2020, 294, 108145. [Google Scholar]
    [CrossRef] Garcia-Ruiz, F.; Sankaran, S.; Maja, J.M.; Lee, W.S.; Rasmussen, J.;
    Ehsani, R. Comparison of two aerial imaging platforms for identification of Huanglongbing-infected
    citrus trees. Comput. Electron. Agric. 2013, 91, 106–115. [Google Scholar] [CrossRef]
    Wang, D.; Song, Q.; Liao, X.; Ye, H.; Shao, Q.; Fan, J.; Cong, N.; Xin, X.; Yue,
    H.; Zhang, H. Integrating satellite and Unmanned Aircraft System (UAS) imagery
    to model livestock population dynamics in the Longbao Wetland National Nature
    Reserve, China. Sci. Total Environ. 2020, 746, 140327. [Google Scholar] [CrossRef]
    Mrisho, L.M.; Mbilinyi, N.A.; Ndalahwa, M.; Ramcharan, A.M.; Kehs, A.K.; McCloskey,
    P.C.; Murithi, H.; Hughes, D.P.; Legg, J.P. Accuracy of a smartphone-based object
    detection model, PlantVillage Nuru, in identifying the foliar symptoms of the
    viral diseases of cassava–CMD and CBSD. Front. Plant Sci. 2020, 11, 1964. [Google
    Scholar] [CrossRef] Di Gennaro, S.F.; Battiston, E.; Di Marco, S.; Facini, O.;
    Matese, A.; Nocentini, M.; Palliotti, A.; Mugnai, L. Unmanned Aerial Vehicle (UAV)-based
    remote sensing to monitor grapevine leaf stripe disease within a vineyard affected
    by esca complex. Phytopathol. Mediterr. 2016, 55, 262–275. [Google Scholar] Pederi,
    Y.A.; Cheporniuk, H.S. Unmanned aerial vehicles and new technological methods
    of monitoring and crop protection in precision agriculture. In Proceedings of
    the 2015 IEEE 3rd International Conference Actual Problems of Unmanned Aerial
    Vehicles Developments (APUAVD), Kyiv, Ukraine, 13–15 October 2015; IEEE: Piscataway,
    NJ, USA, 2015; pp. 298–301. [Google Scholar] Zarco-Tejada, P.J.; Guillén-Climent,
    M.L.; Hernández-Clemente, R.; Catalina, A.; González, M.R.; Martín, P. Estimating
    leaf carotenoid content in vineyards using high resolution hyperspectral imagery
    acquired from an Unmanned Aerial Vehicle (UAV). Agric. For. Meteorol. 2013, 171,
    281–294. [Google Scholar] [CrossRef] [Green Version] Gómez-Candón, D.; De Castro,
    A.I.; López-Granados, F. Assessing the accuracy of mosaics from Unmanned Aerial
    Vehicle (UAV) imagery for precision agriculture purposes in wheat. Precis. Agric.
    2013, 15, 44–56. [Google Scholar] [CrossRef] [Green Version] Torres-Sanchez, J.;
    Lopez-Granados, F.; De Castro, A.I.; Pena-Barragan, J.M. Configuration and specifications
    of an Unmanned Aerial Vehicle (UAV) for early site specific weed management. PLoS
    ONE 2013, 8, e58210. [Google Scholar] [CrossRef] [Green Version] Torres-Sanchez,
    J.; Lopez-Granados, F.; Serrano, N.; Arquero, O.; Pena, J.M. High-throughput 3-D
    monitoring of agricultural-tree plantations with Unmanned Aerial Vehicle (UAV)
    technology. PLoS ONE 2015, 10, e0130479. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Jannoura, R.; Brinkmann, K.; Uteau, D.; Bruns, C.; Joergensen,
    R.G. Monitoring of crop biomass using true colour aerial photographs taken from
    a remote controlled hexacopter. Biosyst. Eng. 2015, 129, 341–351. [Google Scholar]
    [CrossRef] Dai, B.; He, Y.; Gu, F.; Yang, L.; Han, J.; Xu, W. A vision-based autonomous
    aerial spray system for precision agriculture. In Proceedings of the IEEE International
    Conference on Robotics and Biomimetics, Macau, Macao, 5–8 December 2017. [Google
    Scholar] Xavier, T.W.F.; Souto, R.N.V.; Statella, T.; Galbieri, R.; Santos, E.S.;
    Suli, G.S.; Zeilhofer, P. Identification of Ramularia Leaf Blight cotton disease
    infection levels by multispectral, multiscale UAV imagery. Drones 2019, 3, 33.
    [Google Scholar] [CrossRef] [Green Version] Abdulridha, J.; Ampatzidis, Y.; Kakarla,
    S.C.; Roberts, P. Detection of target spot and bacterial spot diseases in tomato
    using UAV-based and benchtop-based hyperspectral imaging techniques. Precis. Agric.
    2019, 21, 955–978. [Google Scholar] [CrossRef] Gomez Selvaraj, M.; Vergara, A.;
    Montenegro, F.; Alonso Ruiz, H.; Safari, N.; Raymaekers, D.; Ocimati, W.; Ntamwira,
    J.; Tits, L.; Omondi, A.B.; et al. Detection of banana plants and their major
    diseases through aerial images and machine learning methods: A case study in DR
    Congo and Republic of Benin. J. Photogramm. Remote Sens. 2020, 169, 110–124. [Google
    Scholar] [CrossRef] Schoofs, H.; Delalieux, S.; Deckers, T.; Bylemans, D. Fire
    Blight monitoring in pear orchards by Unmanned Airborne Vehicles (UAV) systems
    carrying spectral sensors. Agronomy 2020, 10, 615. [Google Scholar] [CrossRef]
    Su, J.; Liu, C.; Hu, X.; Xu, X.; Guo, L.; Chen, W.-H. Spatio-temporal monitoring
    of wheat yellow rust using UAV multispectral imagery. Comput. Electron. Agric.
    2019, 167, 105035. [Google Scholar] [CrossRef] Berni, J.; Zarco-Tejada, P.J.;
    Suarez, L.; Fereres, E. Thermal and narrowband multispectral remote sensing for
    vegetation monitoring from an unmanned aerial vehicle. IEEE Trans. Geosci. Remote
    Sens. 2009, 47, 722–738. [Google Scholar] [CrossRef] [Green Version] Suproteem,
    K.; Sarkara, J.D.; Ehsanib, R.; Kumara, V. Towards autonomous phytopathology:
    Outcomes and challenges of citrus greening disease detection through close-range
    remote sensing. In Proceedings of the 2016 IEEE International Conference on Robotics
    and Automation (ICRA), Stockholm, Sweden, 16–20 May 2016. [Google Scholar] Özgüven,
    M.M. Determination of sugar beet Leaf Spot disease level (Cercospora beticola
    Sacc.) with image processing technique by using drone. Curr. Investig. Agric.
    Curr. Res. 2018, 5, 621–631. [Google Scholar] [CrossRef] Valasek, J.; Thomasson,
    J.A.; Balota, M.; Oakes, J. Exploratory use of a UAV platform for variety selection
    in peanut. In Proceedings of the Autonomous Air and Ground Sensing Systems for
    Agricultural Optimization and Phenotyping, Baltimore, Maryland, 18–19 April 2016.
    98660F. [Google Scholar] [CrossRef] Sugiura, R.; Tsuda, S.; Tamiya, S.; Itoh,
    A.; Nishiwaki, K.; Murakami, N.; Shibuya, Y.; Hirafuji, M.; Nuske, S. Field phenotyping
    system for the assessment of potato late blight resistance using RGB imagery from
    an unmanned aerial vehicle. Biosyst. Eng. 2016, 148, 1–10. [Google Scholar] [CrossRef]
    Rahman, M.F.F.; Fan, S.; Zhang, Y.; Chen, L. A comparative study on application
    of unmanned aerial vehicle systems in agriculture. Agriculture 2021, 11, 22. [Google
    Scholar] [CrossRef] Ludovisi, R.; Tauro, F.; Salvati, R.; Khoury, S.; Mugnozza
    Scarascia, G.; Harfouche, A. UAV-based thermal imaging for high-throughput field
    phenotyping of black poplar response to drought. Front. Plant Sci. 2017, 8, 1681.
    [Google Scholar] [CrossRef] Zhou, J.; Zhou, J.; Ye, H.; Ali, M.L.; Nguyen, H.T.;
    Chen, P. Classification of soybean leaf wilting due to drought stress using UAV-based
    imagery. Comput. Electron. Agric. 2020, 175, 105576. [Google Scholar] [CrossRef]
    Maes, W.H.; Steppe, K. Perspectives for remote sensing with unmanned aerial vehicles
    in precision agriculture. Trends Plant Sci. 2019, 24, 152–164. [Google Scholar]
    [CrossRef] Surový, P.; Almeida Ribeiro, N.; Panagiotidis, D. Estimation of positions
    and heights from UAV-sensed imagery in tree plantations in agrosilvopastoral systems.
    Int. J. Remote Sens. 2018, 39, 4786–4800. [Google Scholar] [CrossRef] Chang, A.;
    Jung, J.; Maeda, M.M.; Landivar, J. Crop height monitoring with digital imagery
    from Unmanned Aerial System (UAS). Comput. Electron. Agric. 2017, 141, 232–237.
    [Google Scholar] [CrossRef] Torres-Sánchez, J.; de Castro, A.I.; Peña, J.M.; Jiménez-Brenes,
    F.M.; Arquero, O.; Lovera, M.; López-Granados, F. Mapping the 3D structure of
    almond trees using UAV acquired photogrammetric point clouds and object-based
    image analysis. Biosyst. Eng. 2018, 176, 172–184. [Google Scholar] [CrossRef]
    Grüner, E.; Astor, T.; Wachendorf, M. Biomass prediction of heterogeneous temperate
    grasslands using an SfM approach based on UAV imaging. Agronomy 2019, 9, 54. [Google
    Scholar] [CrossRef] [Green Version] Roth, L.; Streit, B. Predicting cover crop
    biomass by lightweight UAS-based RGB and NIR photography: An applied photogrammetric
    approach. Precis. Agric. 2017, 19, 93–114. [Google Scholar] [CrossRef] [Green
    Version] Viljanen, N.; Honkavaara, E.; Näsi, R.; Hakala, T.; Niemeläinen, O.;
    Kaivosoja, J. A novel machine learning method for estimating biomass of grass
    swards using a photogrammetric canopy height model, images and vegetation indices
    captured by a drone. Agriculture 2018, 8, 70. [Google Scholar] [CrossRef] [Green
    Version] Berra, E.F.; Gaulton, R.; Barr, S. Commercial off-the-shelf digital cameras
    on unmanned aerial vehicles for multitemporal monitoring of vegetation reflectance
    and NDVI. IEEE Trans. Geosci. Remote Sens. 2017, 55, 4878–4886. [Google Scholar]
    [CrossRef] [Green Version] Nijland, W.; De Jong, R.; De Jong, S.M.; Wulder, M.A.;
    Bater, C.W.; Coops, N.C. Monitoring plant condition and phenology using infrared
    sensitive consumer grade digital cameras. Agric. For. Meteorol. 2014, 184, 98–106.
    [Google Scholar] [CrossRef] [Green Version] Bock, C.H.; Barbedo, J.G.; Del Ponte,
    E.M.; Bohnenkamp, D.; Mahlein, A.-K. From visual estimates to fully automated
    sensor-based measurements of plant disease severity: Status and challenges for
    improving accuracy. Phytopathol. Res. 2020, 2, 1–30. [Google Scholar] [CrossRef]
    [Green Version] Mattupalli, C.; Moffet, C.; Shah, K.; Young, C. Supervised classification
    of RGB aerial imagery to evaluate the impact of a root rot disease. Remote Sens.
    2018, 10, 917. [Google Scholar] [CrossRef] [Green Version] Kerkech, M.; Hafiane,
    A.; Canals, R. Deep leaning approach with colorimetric spaces and vegetation indices
    for vine diseases detection in UAV images. Comput. Electron. Agric. 2018, 155,
    237–243. [Google Scholar] [CrossRef] Ashourloo, D.; Mobasheri, M.R.; Huete, A.
    Developing two spectral disease indices for detection of wheat leaf rust (Pucciniatriticina).
    Remote Sens. 2014, 6, 4723–4740. [Google Scholar] [CrossRef] [Green Version] Zhang,
    D.; Zhou, X.; Zhang, J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath blight
    using an unmanned aerial system with high-resolution color and multispectral imaging.
    PLoS ONE 2018, 13, e0187470. [Google Scholar] [CrossRef] [PubMed] [Green Version]
    Nhamo, L.; Ebrahim, G.Y.; Mabhaudhi, T.; Mpandeli, S.; Magombeyi, M.; Chitakira,
    M.; Magidi, J.; Sibanda, M. An assessment of groundwater use in irrigated agriculture
    using multi-spectral remote sensing. Phys. Chem. Earth Parts A/B/C 2020, 115,
    102810. [Google Scholar] [CrossRef] Adão, T.; Hruška, J.; Pádua, L.; Bessa, J.;
    Peres, E.; Morais, R.; Sousa, J. Hyperspectral Imaging: A review on UAV-based
    sensors, data processing and applications for agriculture and forestry. Remote
    Sens. 2017, 9, 1110. [Google Scholar] [CrossRef] [Green Version] Geipel, J.; Link,
    J.; Wirwahn, J.; Claupein, W. A programmable aerial multispectral camera system
    for in-season crop biomass and nitrogen content estimation. Agriculture 2016,
    6, 4. [Google Scholar] [CrossRef] [Green Version] Iqbal, F.; Lucieer, A.; Barry,
    K. Simplified radiometric calibration for UAS-mounted multispectral sensor. Eur.
    J. Remote Sens. 2018, 51, 301–313. [Google Scholar] [CrossRef] Deng, L.; Mao,
    Z.; Li, X.; Hu, Z.; Duan, F.; Yan, Y. UAV-based multispectral remote sensing for
    precision agriculture: A comparison between different cameras. J. Photogramm.
    Remote Sens. 2018, 146, 124–136. [Google Scholar] [CrossRef] Zaman-Allah, M.;
    Vergara, O.; Araus, J.L.; Tarekegne, A.; Magorokosho, C.; Zarco-Tejada, P.J.;
    Hornero, A.; Alba, A.H.; Das, B.; Craufurd, P.; et al. Unmanned aerial platform-based
    multi-spectral imaging for field phenotyping of maize. Plant Methods 2015, 11,
    35. [Google Scholar] [CrossRef] [Green Version] Kalischuk, M.; Paret, M.L.; Freeman,
    J.H.; Raj, D.; Da Silva, S.; Eubanks, S.; Wiggins, D.J.; Lollar, M.; Marois, J.J.;
    Mellinger, H.C.; et al. An improved crop scouting technique incorporating unmanned
    aerial vehicle-assisted multispectral crop imaging into conventional scouting
    practice for gummy stem blight in watermelon. Plant Dis. 2019, 103, 1642–1650.
    [Google Scholar] [CrossRef] Al-Saddik, H.; Simon, J.C.; Brousse, O.; Cointault,
    F. Multispectral band selection for imaging sensor design for vineyard disease
    detection: Case of Flavescence dorée. Adv. Anim. Biosci. 2017, 8, 150–155. [Google
    Scholar] [CrossRef] [Green Version] Albetis, J.; Jacquin, A.; Goulard, M.; Poilvé,
    H.; Rousseau, J.; Clenet, H.; Dedieu, G.; Duthoit, S. On the potentiality of UAV
    multispectral imagery to detect Flavescence dorée and grapevine trunk diseases.
    Remote Sens. 2018, 11, 23. [Google Scholar] [CrossRef] [Green Version] Calderón,
    R.; Montes-Borrego, M.; Landa, B.B.; Navas-Cortés, J.A.; Zarco-Tejada, P.J. Detection
    of downy mildew of opium poppy using high-resolution multi-spectral and thermal
    imagery acquired with an unmanned aerial vehicle. Precis. Agric. 2014, 15, 639–661.
    [Google Scholar] [CrossRef] Dash, J.; Pearse, G.; Watt, M. UAV multispectral imagery
    can complement satellite data for monitoring forest health. Remote Sens. 2018,
    10, 1216. [Google Scholar] [CrossRef] [Green Version] Khot, L.R.; Sankaran, S.;
    Carter, A.H.; Johnson, D.A.; Cummings, T.F. UAS imaging-based decision tools for
    arid winter wheat and irrigated potato production management. Int. J. Remote Sens.
    2015, 37, 125–137. [Google Scholar] [CrossRef] Nebiker, S.; Lack, N.; Abächerli,
    M.; Läderach, S. Light-weight multispectral UAV sensors and their capabilities
    for predicting grain yield and detecting plant diseases. ISPRS -Int. Arch. Photogramm.
    Remote Sens. Spat. Inf. Sci. 2016, 41, 963–970. [Google Scholar] [CrossRef] [Green
    Version] Kerkech, M.; Hafiane, A.; Canals, R. Vine disease detection in UAV multispectral
    images using optimized image registration and deep learning segmentation approach.
    Comput. Electron. Agric. 2020, 174, 105446. [Google Scholar] [CrossRef] Gallo,
    R.; Ristorto, G.; Daglio, G.; Berta, G.; Lazzari, M.; Mazzetto, F. New solutions
    for the automatic early detection of diseases in vineyards through ground sensing
    approaches integrating LiDAR and optical sensors. Chem. Eng. Trans. 2017, 58,
    673–678. [Google Scholar] Qin, J.; Wang, B.; Wu, Y.; Lu, Q.; Zhu, H. Identifying
    pine wood nematode disease using UAV images and deep learning algorithms. Remote
    Sens. 2021, 13, 162. [Google Scholar] [CrossRef] Ye, H.; Huang, W.; Huang, S.;
    Cui, B.; Dong, Y.; Guo, A.; Ren, Y.; Jin, Y. Recognition of banana fusarium wilt
    based on UAV remote sensing. Remote Sens. 2020, 12, 938. [Google Scholar] [CrossRef]
    [Green Version] Lowe, A.; Harrison, N.; French, A.P. Hyperspectral image analysis
    techniques for the detection and classification of the early onset of plant disease
    and stress. Plant Methods 2017, 13, 80. [Google Scholar] [CrossRef] Cilia, C.;
    Panigada, C.; Rossini, M.; Meroni, M.; Busetto, L.; Amaducci, S.; Boschetti, M.;
    Picchi, V.; Colombo, R. Nitrogen status assessment for variable rate fertilization
    in maize through hyperspectral imagery. Remote Sens. 2014, 6, 6549–6565. [Google
    Scholar] [CrossRef] [Green Version] Gevaert, C.M.; Suomalainen, J.; Tang, J.;
    Kooistra, L. Generation of spectral–temporal response surfaces by combining multispectral
    satellite and hyperspectral UAV imagery for precision agriculture applications.
    IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2015, 8, 3140–3146. [Google Scholar]
    [CrossRef] Proctor, C.; He, Y. Workflow for building a hyperspectral UAV: Challenges
    and opportunities. ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2015,
    40, 415–419. [Google Scholar] [CrossRef] [Green Version] Deery, D.; Jimenez-Berni,
    J.; Jones, H.; Sirault, X.; Furbank, R. proximal remote sensing buggies and potential
    applications for field-based phenotyping. Agronomy 2014, 4, 349–379. [Google Scholar]
    [CrossRef] [Green Version] Honkavaara, E.; Hakala, T.; Markelin, L.; Jaakkola,
    A.; Saari, H.; Ojanen, H.; Pölönen, I.; Tuominen, S.; Näsi, R.; Rosnell, T.; et
    al. Autonomous hyperspectral UAS photogrammetry for environmental monitoring applications.
    ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2014, 40, 155–159. [Google
    Scholar] [CrossRef] [Green Version] Honkavaara, E.; Saari, H.; Kaivosoja, J.;
    Pölönen, I.; Hakala, T.; Litkey, P.; Mäkynen, J.; Pesonen, L. Processing and assessment
    of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral
    camera for precision agriculture. Remote Sens. 2013, 5, 5006–5039. [Google Scholar]
    [CrossRef] [Green Version] Saari, H.; Akujärvi, A.; Holmlund, C.; Ojanen, H.;
    Kaivosoja, J.; Nissinen, A.; Niemeläinen, O. Visible, very near IR and short wave
    IR hyperspectral drone imaging system for agriculture and natural water applications.
    ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, 42, 165–170. [Google
    Scholar] [CrossRef] [Green Version] Tack, N.; Lambrechts, A.; Soussan, P.; Haspeslagh,
    L. A compact, high-speed, and low-cost hyperspectral imager. In Proceedings of
    the Silicon Photonics VII, 8266, San Francisco, CA, USA, 21–26 January 2012. [Google
    Scholar] Sima, A.A.; Baeck, P.; Nuyts, D.; Delalieux, S.; Livens, S.; Blommaert,
    J.; Delauré, B.; Boonen, M. Compact hyperspectral imaging system (COSI) for Small
    Remotely Piloted Aircraft Systems (RPAS) – System overview and first performance
    evaluation results. ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.
    2016, 41, 1157–1164. [Google Scholar] [CrossRef] [Green Version] Calderón, R.;
    Navas-Cortés, J.A.; Lucena, C.; Zarco-Tejada, P.J. High-resolution airborne hyperspectral
    and thermal imagery for early detection of Verticillium wilt of olive using fluorescence,
    temperature and narrow-band spectral indices. Remote Sens. Environ. 2013, 139,
    231–245. [Google Scholar] [CrossRef] Sandino, J.; Pegg, G.; Gonzalez, F.; Smith,
    G. Aerial mapping of forests affected by pathogens using UAVs, hyperspectral sensors,
    and artificial intelligence. Sensors 2018, 18, 944. [Google Scholar] [CrossRef]
    [PubMed] [Green Version] Calderón, R.; Navas-Cortés, J.; Lucena, C.; Zarco-Tejada,
    P. High-resolution hyperspectral and thermal imagery acquired from UAV platforms
    for early detection of Verticillium wilt using fluorescence, temperature and narrow-band
    indices. In Proceedings of the Workshop on UAV-basaed Remote Sensing Methods for
    Monitoring Vegetation, Cologne, Germany, 11–12 September 2013; p. 9. [Google Scholar]
    Thomas, S.; Kuska, M.T.; Bohnenkamp, D.; Brugger, A.; Alisaac, E.; Wahabzada,
    M.; Behmann, J.; Mahlein, A.-K. Benefits of hyperspectral imaging for plant disease
    detection and plant protection: A technical perspective. J. Plant Dis. Prot. 2018,
    125, 5–20. [Google Scholar] [CrossRef] Costa, J.M.; Grant, O.M.; Chaves, M.M.
    Thermography to explore plant-environment interactions. J. Exp. Bot. 2013, 64,
    3937–3949. [Google Scholar] [CrossRef] [PubMed] Mahajan, U.; Bundel, B.R. Drones
    for Normalized Difference Vegetation Index (NDVI), to estimate crop health for
    precision agriculture: A cheaper alternative for spatial satellite sensors. In
    International Conference on Innovative Research in Agriculture, Food Science,
    Forestry, Horticulture, Aquaculture, Animal Sciences, Biodiversity, Ecological
    Sciences and Climate Change; Krishi Sanskriti Publications: New Delhi, India,
    2016. [Google Scholar] Gago, J.; Douthe, C.; Coopman, R.; Gallego, P.; Ribas-Carbo,
    M.; Flexas, J.; Escalona, J.; Medrano, H. UAVs challenge to assess water stress
    for sustainable agriculture. Agric. Water Manag. 2015, 153, 9–19. [Google Scholar]
    [CrossRef] Granum, E.; Pérez-Bueno, M.L.; Calderón, C.E.; Ramos, C.; de Vicente,
    A.; Cazorla, F.M.; Barón, M. Metabolic responses of avocado plants to stress induced
    by Rosellinia necatrix analysed by fluorescence and thermal imaging. Eur. J. Plant
    Pathol. 2015, 142, 625–632. [Google Scholar] [CrossRef] Smigaj, M.; Gaulton, R.;
    Barr, S.L.; Suárez, J.C. UAV-borne thermal imaging for forest health monitoring:
    Detection of disease-induced canopy temperature increase. ISPRS-Int. Arch. Photogramm.
    Remote Sens. Spat. Inf. Sci. 2015, 40, 349–354. [Google Scholar] [CrossRef] [Green
    Version] Mahlein, A.-K.; Oerke, E.-C.; Steiner, U.; Dehne, H.-W. Recent advances
    in sensing plant diseases for precision crop protection. Eur. J. Plant Pathol.
    2012, 133, 197–209. [Google Scholar] [CrossRef] Raza, S.-e.-A.; Prince, G.; Clarkson,
    J.P.; Rajpoot, N.M. Automatic detection of diseased tomato plants using thermal
    and stereo visible light images. PLoS ONE 2015, 10, e0123262. [Google Scholar]
    Baranowski, P.; Jedryczka, M.; Mazurek, W.; Babula-Skowronska, D.; Siedliska,
    A.; Kaczmarek, J. Hyperspectral and thermal imaging of oilseed rape (Brassica
    napus) response to fungal species of the genus Alternaria. PLoS ONE 2015, 10,
    e0122913. [Google Scholar] [CrossRef] [Green Version] López-López, M.; Calderón,
    R.; González-Dugo, V.; Zarco-Tejada, P.J.; Fereres, E. Early detection and quantification
    of almond red leaf blotch using high-resolution hyperspectral and thermal imagery.
    Remote Sens. 2016, 8, 276. [Google Scholar] [CrossRef] [Green Version] Sankaran,
    S.; Maja, J.M.; Buchanon, S.; Ehsani, R. Huanglongbing (citrus greening) detection
    using visible, near infrared and thermal imaging techniques. Sensors 2013, 13,
    2117–2130. [Google Scholar] [CrossRef] [PubMed] [Green Version] Xu, H.; Zhu, S.;
    Ying, Y.; Jiang, H. Early detection of plant disease using infrared thermal imaging.
    In Optics for Natural Resources, Agriculture, and Foods; International Society
    for Optics and Photonics: Bellingham, WA, USA, 2006; p. 638110. [Google Scholar]
    Wang, M.; Xiong, Y.; Ling, N.; Feng, X.; Zhong, Z.; Shen, Q.; Guo, S. Detection
    of the dynamic response of cucumber leaves to fusaric acid using thermal imaging.
    Plant Physiol. Biochem. 2013, 66, 68–76. [Google Scholar] [CrossRef] [PubMed]
    Anasta, N.; Setyawan, F.; Fitriawan, H. Disease detection in banana trees using
    an image processing-based thermal camera. In IOP Conference Series: Earth and
    Environmental Science; IOP Publishing: Bristol, UK, 2021; p. 012088. [Google Scholar]
    Yang, N.; Yuan, M.; Wang, P.; Zhang, R.; Sun, J.; Mao, H. Tea diseases detection
    based on fast infrared thermal image processing technology. J. Sci. Food Agric.
    2019, 99, 3459–3466. [Google Scholar] [CrossRef] [PubMed] Vit, A.; Shani, G. Comparing
    RGB-D sensors for close range outdoor agricultural phenotyping. Sensors 2018,
    18, 4413. [Google Scholar] [CrossRef] [PubMed] [Green Version] Andujar, D.; Dorado,
    J.; Fernandez-Quintanilla, C.; Ribeiro, A. An approach to the use of depth cameras
    for weed volume estimation. Sensors 2016, 16, 972. [Google Scholar] [CrossRef]
    [PubMed] [Green Version] Zollhöfer, M.; Stotko, P.; Görlitz, A.; Theobalt, C.;
    Nießner, M.; Klein, R.; Kolb, A. State of the art on 3D reconstruction with RGB-D
    cameras. Comput. Graph. Forum 2018, 37, 625–652. [Google Scholar] [CrossRef] Xia,
    C.; Wang, L.; Chung, B.-K.; Lee, J.-M. In situ 3D segmentation of individual plant
    leaves using a RGB-D camera for agricultural automation. Sensors 2015, 15, 20463–20479.
    [Google Scholar] [CrossRef] Mahlein, A.-K. Plant disease detection by imaging
    sensors–parallels and specific demands for precision agriculture and plant phenotyping.
    Plant Dis. 2016, 100, 241–251. [Google Scholar] [CrossRef] [Green Version] Paulus,
    S.; Dupuis, J.; Mahlein, A.-K.; Kuhlmann, H. Surface feature based classification
    of plant organs from 3D laserscanned point clouds for plant phenotyping. BMC Bioinform.
    2013, 14, 1–12. [Google Scholar] [CrossRef] [Green Version] Singh, A.; Ganapathysubramanian,
    B.; Singh, A.K.; Sarkar, S. Machine learning for high-throughput stress phenotyping
    in plants. Trends Plant Sci. 2016, 21, 110–124. [Google Scholar] [CrossRef] [Green
    Version] Wallelign, S.; Polceanu, M.; Buche, C. Soybean plant disease identification
    using convolutional neural network. In Proceedings of the Thirty-First International
    Flairs Conference, Melbourne, FL, USA, 21–23 May 2018. [Google Scholar] Sonka,
    M.; Hlavac, V.; Boyle, R. Image pre-processing. In Image Processing, Analysis
    and Machine Vision; Springer: Berlin, Germany, 1993; pp. 56–111. [Google Scholar]
    Ghosal, S.; Blystone, D.; Singh, A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar,
    S. An explainable deep machine vision framework for plant stress phenotyping.
    Proc. Natl. Acad. Sci. USA 2018, 115, 4613–4618. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Ferentinos, K.P. Deep learning models for plant disease detection
    and diagnosis. Comput. Electron. Agric. 2018, 145, 311–318. [Google Scholar] [CrossRef]
    Gu, L.; Robles-Kelly, A.A.; Zhou, J. Efficient estimation of reflectance parameters
    from imaging spectroscopy. IEEE Trans. Image Process. 2013, 22, 3648–3663. [Google
    Scholar] Habili, N.; Oorloff, J. Scyllarus ™: From research to commercial software.
    In Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference,
    New York, NY, USA, 28 September–1 October 2015; pp. 119–122. [Google Scholar]
    Choi, H.; Baraniuk, R. Analysis of wavelet-domain Wiener filters. In Proceedings
    of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis
    (Cat. No. 98TH8380), Philadelphia, PA, USA, 25–28 October 1994; pp. 613–616. [Google
    Scholar] Marroquin, J.L.; Girosi, F. Some extensions of the K-Means algorithm
    for image segmentation and pattern classification; Massachusetts Inst of Tech
    Cambridge Artificial Intelligence Lab: Cambridge, MA, USA, 1993. [Google Scholar]
    MacQueen, J. Some methods for classification and analysis of multivariate observations.
    In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
    Probability, Los Angeles, CA, USA, 27 December 1965–7 January 1966; pp. 281–297.
    [Google Scholar] Wagstaff, K.; Cardie, C.; Rogers, S.; Schrödl, S. Constrained
    k-means clustering with background knowledge. In Proceedings of the Eighteenth
    International Conference on Machine Learning, San Francisco, CA, USA, 28 June–1
    July 2001; pp. 577–584. [Google Scholar] Picard, R.R.; Cook, R.D. Cross-validation
    of regression models. J. Am. Stat. Assoc. 1984, 79, 575–583. [Google Scholar]
    [CrossRef] Gupta, S.G.; Ghonge, D.; Jawandhiya, P.M. Review of Unmanned Aircraft
    System (UAS). Int. J. Adv. Res. Comput. Eng. Technol. 2013, 2, 1646–1658. [Google
    Scholar] [CrossRef] Marino, S.; Alvino, A. Detection of spatial and temporal variability
    of wheat cultivars by high-resolution vegetation indices. Agronomy 2019, 9, 226.
    [Google Scholar] [CrossRef] [Green Version] Ribeiro-Gomes, K.; Hernández-López,
    D.; Ortega, J.F.; Ballesteros, R.; Poblete, T.; Moreno, M.A. Uncooled thermal
    camera calibration and optimization of the photogrammetry process for UAV applications
    in agriculture. Sensors 2017, 17, 2173. [Google Scholar] [CrossRef] Xue, J.; Su,
    B. Significant remote sensing vegetation indices: A review of developments and
    applications. J. Sens. 2017, 2017, 1353691. [Google Scholar] [CrossRef] [Green
    Version] Woebbecke, D.M.; Meyer, G.E.; Von Bargen, K.; Mortensen, D.A. Color indices
    for weed identification under various soil, residue, and lighting conditions.
    Trans. ASAE 1995, 38, 259–269. [Google Scholar] [CrossRef] Patrick, A.; Pelham,
    S.; Culbreath, A.; Holbrook, C.C.; De Godoy, I.J.; Li, C. High throughput phenotyping
    of tomato spot wilt disease in peanuts using unmanned aerial systems and multispectral
    imaging. IEEE Instrum. Meas. Mag. 2017, 20, 4–12. [Google Scholar] [CrossRef]
    Albetis, J.; Duthoit, S.; Guttler, F.; Jacquin, A.; Goulard, M.; Poilvé, H.; Féret,
    J.-B.; Dedieu, G. Detection of Flavescence dorée grapevine disease using Unmanned
    Aerial Vehicle (UAV) multispectral imagery. Remote Sens. 2017, 9, 308. [Google
    Scholar] [CrossRef] [Green Version] McCulloch, W.S.; Pitts, W. A logical calculus
    of the ideas immanent in nervous activity. Bull. Math. Biophys. 1943, 5, 115–133.
    [Google Scholar] [CrossRef] Carranza-Rojas, J.; Goeau, H.; Bonnet, P.; Mata-Montero,
    E.; Joly, A. Going deeper in the automated identification of Herbarium specimens.
    BMC Evol. Biol. 2017, 17, 1–14. [Google Scholar] [CrossRef] [PubMed] [Green Version]
    Yang, X.; Guo, T. Machine learning in plant disease research. Eur. J. Biomed.
    Res. 2017, 3, 6–9. [Google Scholar] [CrossRef] [Green Version] LeCun, Y.; Bottou,
    L.; Bengio, Y.; Haffner, P. Gradient-based learning applied to document recognition.
    Proc. IEEE 1998, 86, 2278–2324. [Google Scholar] [CrossRef] [Green Version] Zhang,
    K.; Wu, Q.; Liu, A.; Meng, X. Can deep learning identify tomato leaf disease?
    Adv. Multimed. 2018, 2018, 6710865. [Google Scholar] [CrossRef] [Green Version]
    Türkoğlu, M.; Hanbay, D. Plant disease and pest detection using deep learning-based
    features. Turk. J. Electr. Eng. Comput. Sci. 2019, 27, 1636–1651. [Google Scholar]
    [CrossRef] Krizhevsky, A. One weird trick for parallelizing convolutional neural
    networks. arXiv 2014, arXiv:1404.5997. [Google Scholar] Sermanet, P.; Eigen, D.;
    Zhang, X.; Mathieu, M.; Fergus, R.; LeCun, Y. Overfeat: Integrated recognition,
    localization and detection using convolutional networks. arXiv 2013, arXiv:1312.6229.
    [Google Scholar] Simonyan, K.; Zisserman, A. Very deep convolutional networks
    for large-scale image recognition. arXiv 2014, arXiv:1409.1556. [Google Scholar]
    Hughes, D.; Salathé, M. An open access repository of images on plant health to
    enable the development of mobile disease diagnostics. arXiv 2015, arXiv:1511.08060.
    [Google Scholar] Mohanty, S.P.; Hughes, D.P.; Salathé, M. Using deep learning
    for image-based plant disease detection. Front. Plant Sci. 2016, 7, 1419. [Google
    Scholar] [CrossRef] [PubMed] [Green Version] Sibiya, M.; Sumbwanyambe, M. A computational
    procedure for the recognition and classification of maize leaf diseases out of
    healthy leaves using convolutional neural networks. AgriEngineering 2019, 1, 119–131.
    [Google Scholar] [CrossRef] [Green Version] Wang, T.; Thomasson, J.A.; Yang, C.;
    Isakeit, T.; Nichols, R.L. Automatic classification of cotton root rot disease
    based on UAV remote sensing. Remote Sens. 2020, 12, 1310. [Google Scholar] [CrossRef]
    [Green Version] Kerkech, M.; Hafiane, A.; Canals, R.; Ros, F. Vine disease detection
    by deep learning method combined with 3d depth information. In Proceedings of
    the International Conference on Image and Signal Processing, Marrakesh, Morocco,
    4–6 June 2020; Springer: Cham, Germany, 2020; pp. 82–90. [Google Scholar] Gibson-Poole,
    S.; Humphris, S.; Toth, I.; Hamilton, A. Identification of the onset of disease
    within a potato crop using a UAV equipped with un-modified and modified commercial
    off-the-shelf digital cameras. Adv. Anim. Biosci. 2017, 8, 812–816. [Google Scholar]
    [CrossRef] Sugiura, R.; Tsuda, S.; Tsuji, H.; Murakami, N. Virus-infected plant
    detection in potato seed production field by UAV imagery. In Proceedings of the
    2018 ASABE Annual International Meeting, Detroit, MI, USA, 29 July–1 August 2018;
    p. 1. [Google Scholar] Dang, L.M.; Hassan, S.I.; Suhyeon, I.; kumar Sangaiah,
    A.; Mehmood, I.; Rho, S.; Seo, S.; Moon, H. UAV based wilt detection system via
    convolutional neural networks. Sustain. Comput. Inform. Syst. 2018, 28, 100250.
    [Google Scholar] [CrossRef] [Green Version] Rangarajan, A.K.; Purushothaman, R.;
    Ramesh, A. Tomato crop disease classification using pre-trained deep learning
    algorithm. Procedia Comput. Sci. 2018, 133, 1040–1047. [Google Scholar] [CrossRef]
    Too, E.C.; Yujian, L.; Njuki, S.; Yingchun, L. A comparative study of fine-tuning
    deep learning models for plant disease identification. Comput. Electron. Agric.
    2019, 161, 272–279. [Google Scholar] [CrossRef] Durmuş, H.; Güneş, E.O.; Kırcı,
    M. Disease detection on the leaves of the tomato plants by using deep learning.
    In Proceedings of the 2017 6th International Conference on Agro-Geoinformatics,
    Fairfax, VA, USA, 7–10 August 2017; pp. 1–5. [Google Scholar] Chen, Y.; Lin, Z.;
    Zhao, X.; Wang, G.; Gu, Y. Deep learning-based classification of hyperspectral
    data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 2094–2107. [Google
    Scholar] [CrossRef] Grinblat, G.L.; Uzal, L.C.; Larese, M.G.; Granitto, P.M. Deep
    learning for plant identification using vein morphological patterns. Comput. Electron.
    Agric. 2016, 127, 418–424. [Google Scholar] [CrossRef] [Green Version] Kamilaris,
    A.; Prenafeta-Boldú, F.X. Deep learning in agriculture: A survey. Comput. Electron.
    Agric. 2018, 147, 70–90. [Google Scholar] [CrossRef] [Green Version] Lee, S.H.;
    Chan, C.S.; Wilkin, P.; Remagnino, P. Deep-plant: Plant identification with convolutional
    neural networks. In Proceedings of the 2015 IEEE international conference on image
    processing (ICIP), Quebec City, QC, Canada, 27–30 September 2015; pp. 452–456.
    [Google Scholar] Kussul, N.; Lavreniuk, M.; Skakun, S.; Shelestov, A. Deep learning
    classification of land cover and crop types using remote sensing data. IEEE Geosci.
    Remote Sens. Lett. 2017, 14, 778–782. [Google Scholar] [CrossRef] Song, X.; Zhang,
    G.; Liu, F.; Li, D.; Zhao, Y.; Yang, J. Modeling spatio-temporal distribution
    of soil moisture by deep learning-based cellular automata model. J. Arid Land
    2016, 8, 734–748. [Google Scholar] [CrossRef] [Green Version] Zhu, N.; Liu, X.;
    Liu, Z.; Hu, K.; Wang, Y.; Tan, J.; Huang, M.; Zhu, Q.; Ji, X.; Jiang, Y. Deep
    learning for smart agriculture: Concepts, tools, applications, and opportunities.
    Int. J. Agric. Biol. Eng. 2018, 11, 32–44. [Google Scholar] [CrossRef] He, K.;
    Zhang, X.; Ren, S.; Sun, J. Identity mappings in deep residual networks. In Proceedings
    of the European Conference on Computer Vision, Amsterdam, The Netherlands, 8–16
    October 2016; Springer: Cham, Germany, 2016; pp. 630–645. [Google Scholar] Yu,
    D.; Xiong, W.; Droppo, J.; Stolcke, A.; Ye, G.; Li, J.; Zweig, G. Deep convolutional
    neural networks with layer-wise context expansion and attention. In Proceedings
    of the 17th Annual Conference of the International Speech Communication Association,
    San Francisco, CA, USA, 8–12 September 2016; pp. 17–21. [Google Scholar] Barbedo,
    J.G.A. A review on the main challenges in automatic plant disease identification
    based on visible range images. Biosyst. Eng. 2016, 144, 52–60. [Google Scholar]
    [CrossRef] Khanal, S.; KC, K.; Fulton, J.P.; Shearer, S.; Ozkan, E. Remote sensing
    in agriculture—accomplishments, limitations, and opportunities. Remote Sens. 2020,
    12, 3783. [Google Scholar] [CrossRef] Hulley, G.; Hook, S.; Fisher, J.; Lee, C.
    Ecostress, a Nasa Earth-Ventures Instrument for studying links between the water
    cycle and plant health over the diurnal cycle. In Proceedings of the 2017 IEEE
    International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth, TX,
    USA, 23–28 July 2017; pp. 5494–5496. [Google Scholar] Song, L.; Guanter, L.; Guan,
    K.; You, L.; Huete, A.; Ju, W.; Zhang, Y. Satellite sun-induced chlorophyll fluorescence
    detects early response of winter wheat to heat stress in the Indian Indo-Gangetic
    Plains. Glob. Chang. Biol. 2018, 24, 4023–4037. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Nguyen, M.-T.; Shah, D. Improving Current Limitations of Deep
    Learning Based Plant Disease Identification; The Cooper Union: NewYork, NY, USA,
    22 December 2019. [Google Scholar] Arsenovic, M.; Karanovic, M.; Sladojevic, S.;
    Anderla, A.; Stefanovic, D. Solving current limitations of deep learning based
    approaches for plant disease detection. Symmetry 2019, 11, 939. [Google Scholar]
    [CrossRef] [Green Version] Petty, R.V.; Chang, E.B.E. Drone use in aerial pesticide
    application faces outdated regulatory hurdles. Harvard J. Law Technol. Dig. 2018,
    1–14. [Google Scholar] Stoica, A.-A. Emerging legal issues regarding civilian
    drone usage. Chall. Knowl. Soc. 2018, 692–699. [Google Scholar] Stöcker, C.; Bennett,
    R.; Nex, F.; Gerke, M.; Zevenbergen, J. Review of the current state of UAV regulations.
    Remote Sens. 2017, 9, 459. [Google Scholar] [CrossRef] [Green Version] Xu, H.;
    Ying, Y.; Fu, X.; Zhu, S. Near-infrared spectroscopy in detecting leaf miner damage
    on tomato leaf. Biosyst. Eng. 2007, 96, 447–454. [Google Scholar] [CrossRef] Al-Ahmadi,
    A.H.; Subedi, A.; Wang, G.; Choudhary, R.; Fakhoury, A.; Watson, D.G. Detection
    of charcoal rot (Macrophomina phaseolina) toxin effects in soybean (Glycine max)
    seedlings using hyperspectral spectroscopy. Comput. Electron. Agric. 2018, 150,
    188–195. [Google Scholar] [CrossRef] Oosedo, A.; Abiko, S.; Konno, A.; Uchiyama,
    M. Optimal transition from hovering to level-flight of a quadrotor tail-sitter
    UAV. Auton. Robot. 2017, 41, 1143–1159. [Google Scholar] [CrossRef] Theys, B.;
    De Vos, G.; De Schutter, J. A control approach for transitioning VTOL UAVs with
    continuously varying transition angle and controlled by differential thrust. In
    Proceedings of the 2016 International Conference on Unmanned Aircraft Systems
    (ICUAS), Arlington, VA, USA, 7–10 June 2016; IEEE: Piscataway, NJ, USA, 2016;
    pp. 118–125. [Google Scholar] Latif, M.A. An agricultural perspective on flying
    sensors: State of the art, challenges, and future directions. IEEE Geosci. Remote
    Sens. Mag. 2018, 6, 10–22. [Google Scholar] [CrossRef] Oghaz, M.M.D.; Razaak,
    M.; Kerdegari, H.; Argyriou, V.; Remagnino, P. Scene and environment monitoring
    using aerial imagery and deep learning. In Proceedings of the 2019 15th International
    Conference on Distributed Computing in Sensor Systems (DCOSS), Los Angeles, CA,
    USA, 29–31 May 2019; pp. 362–369. [Google Scholar] Boursianis, A.D.; Papadopoulou,
    M.S.; Diamantoulakis, P.; Liopa-Tsakalidi, A.; Barouchas, P.; Salahas, G.; Karagiannidis,
    G.; Wan, S.; Goudos, S.K. Internet of Things (IoT) and Agricultural Unmanned Aerial
    Vehicles (UAVs) in smart farming: A comprehensive review. Internet Things 2020,
    100187. [Google Scholar] [CrossRef] Ernst, M.; Holst, H.; Winter, M.; Altermatt,
    P.P. SunCalculator: A program to calculate the angular and spectral distribution
    of direct and diffuse solar radiation. Sol. Energy Mater. Sol. Cells 2016, 157,
    913–922. [Google Scholar] [CrossRef] Fernández, E.F.; Soria-Moya, A.; Almonacid,
    F.; Aguilera, J. Comparative assessment of the spectral impact on the energy yield
    of high concentrator and conventional photovoltaic technology. Sol. Energy Mater.
    Sol. Cells 2016, 147, 185–197. [Google Scholar] [CrossRef] Ben-Dor, E.; Schläpfer,
    D.; Plaza, A.J.; Malthus, T. Hyperspectral remote sensing. In Airborne Measurements
    for Environmental Research: Methods and Instruments; Wiley-VCH Verlag & Co. KGaA:
    Weinheim, Germany, 2013; Volume 413, p. 456. [Google Scholar] Colombo, R.; Celesti,
    M.; Bianchi, R.; Campbell, P.K.; Cogliati, S.; Cook, B.D.; Corp, L.A.; Damm, A.;
    Domec, J.C.; Guanter, L. Variability of sun-induced chlorophyll fluorescence according
    to stand age-related processes in a managed loblolly pine forest. Glob. Chang.
    Biol. 2018, 24, 2980–2996. [Google Scholar] [CrossRef] [Green Version] Middleton,
    E.M.; Rascher, U.; Huemmrich, K.F.; Cook, B.D.; Noormets, A.; Schickling, A.;
    Pinto, F.; Alonso, L.; Damm, A.; Guanter, L. The 2013 FLEX-US airborne campaign
    at the Parker Tract Loblolly Pine Plantation in North Carolina, USA. Remote Sens.
    2017, 9, 612. [Google Scholar] [CrossRef] [Green Version] Bovensmann, H.; Bösch,
    H.; Brunner, D.; Ciais, P.; Crisp, D.; Dolman, H.; Hayman, G.; Houweling, S.;
    Lichtenberg, L. Report for Mission Selection: CarbonSat-An Earth Explorer to Observe
    Greenhouse Gases; European Space Agency: Noordwijk, The Netherlands, 2015. [Google
    Scholar] Pandley, P.; Manevski, K.; Srivastava, P.K.; Petropoulos, G.P. The use
    of hyperspectral earth observation data for land use/cover classification: Present
    status, challenges and future outlook. In Hyperspectral Remote Sensing of Vegetation,
    1st ed.; Thenkabail, P., Ed.; CRC Press: Boca Raton, FL, USA, 2018; pp. 147–173.
    [Google Scholar]      Publisher’s Note: MDPI stays neutral with regard to jurisdictional
    claims in published maps and institutional affiliations.  © 2021 by the authors.
    Licensee MDPI, Basel, Switzerland. This article is an open access article distributed
    under the terms and conditions of the Creative Commons Attribution (CC BY) license
    (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style
    Neupane, K.; Baysal-Gurel, F. Automatic Identification and Monitoring of Plant
    Diseases Using Unmanned Aerial Vehicles: A Review. Remote Sens. 2021, 13, 3841.
    https://doi.org/10.3390/rs13193841 AMA Style Neupane K, Baysal-Gurel F. Automatic
    Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles:
    A Review. Remote Sensing. 2021; 13(19):3841. https://doi.org/10.3390/rs13193841
    Chicago/Turabian Style Neupane, Krishna, and Fulya Baysal-Gurel. 2021. \"Automatic
    Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles:
    A Review\" Remote Sensing 13, no. 19: 3841. https://doi.org/10.3390/rs13193841
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   60
    Scopus   65 Web of Science   49 ads   16 Google Scholar   [click to view] Article
    Access Statistics Article access statistics Article Views 8. Jan 18. Jan 28. Jan
    7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0k 2.5k 5k 7.5k 10k 12.5k For more
    information on the journal statistics, click here. Multiple requests from the
    same IP address are counted as one view.   Remote Sens., EISSN 2072-4292, Published
    by MDPI RSS Content Alert Further Information Article Processing Charges Pay an
    Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For
    Reviewers For Editors For Librarians For Publishers For Societies For Conference
    Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles
    Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe
    to receive issue release notifications and newsletters from MDPI journals Select
    options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated
    Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Automatic identification and monitoring of plant diseases using unmanned
    aerial vehicles: A review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sadiq M.I.
  - Pramito Rahman S.M.
  - Kayes S.
  - Sumaita A.H.
  - Chisty N.A.
  citation_count: '1'
  description: Climate change, droughts, and growing food demands are the rising problems
    in the field of agriculture. Agricultural methodologies need acclimatization to
    these growing problems with technological innovations. The infrared and visible
    imaging approaches are substantial to ascertain crop health, temperature and humidity
    distributions, salinity, water stress, and visible pattern recognition for the
    widespread areas of agricultural lands. The paper has put forward a review on
    the viable image processing methods incorporating computer vision, machine learning,
    infrared and visible imaging, and complex crop and soil nutrient sensing technologies,
    which are being used to detect the pivotal elements required for the monitoring
    systems. The discussed approaches and analyses, if implemented, have the prospects
    of expanding the field of agriculture with the emerging technologies, which would
    be able to adapt to the rising demand of quality crop productions.
  doi: 10.1109/ICDS53782.2021.9626711
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2021 Fifth International Conf...
    A review on the Imaging Approaches in Agriculture with Crop and Soil Sensing Methodologies
    Publisher: IEEE Cite This PDF Md Ijaj Sadiq; S. M. Pramito Rahman; Shakiyen Kayes;
    Afnan Hossain Sumaita; Nafiz Ahmed Chisty All Authors 1 Cites in Paper 150 Full
    Text Views Abstract Document Sections I. Introduction II. Computer Vision In Agriculture
    III. Visible and thermal Imaging IV. Crop and Soil Sensing Methodologies V. Data
    Analysing Methodologies Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: Climate change, droughts, and growing food demands
    are the rising problems in the field of agriculture. Agricultural methodologies
    need acclimatization to these growing problems with technological innovations.
    The infrared and visible imaging approaches are substantial to ascertain crop
    health, temperature and humidity distributions, salinity, water stress, and visible
    pattern recognition for the widespread areas of agricultural lands. The paper
    has put forward a review on the viable image processing methods incorporating
    computer vision, machine learning, infrared and visible imaging, and complex crop
    and soil nutrient sensing technologies, which are being used to detect the pivotal
    elements required for the monitoring systems. The discussed approaches and analyses,
    if implemented, have the prospects of expanding the field of agriculture with
    the emerging technologies, which would be able to adapt to the rising demand of
    quality crop productions. Published in: 2021 Fifth International Conference On
    Intelligent Computing in Data Sciences (ICDS) Date of Conference: 20-22 October
    2021 Date Added to IEEE Xplore: 01 December 2021 ISBN Information: DOI: 10.1109/ICDS53782.2021.9626711
    Publisher: IEEE Conference Location: Fez, Morocco SECTION I. Introduction Agriculture
    has long been the primary occupation of many nations and has contributed significantly
    to their economic developments. When it comes to cultivating crops, advancements
    in agricultural production are very welcomed in the modern era. Thanks to technological
    advancements, crop fields can now be monitored automatically. It would benefit
    both the farmers and the research organizations. It is difficult for a human being
    to monitor the crop field and determine the conditions physically. There have
    been numerous efforts to improve the crop’s health management systems. It is also
    important to track agricultural yield and chemical fertilizers. Additionally,
    if a crop is cultivated several times in a limited farming area, the soil nutrients
    may be depleted. Crop growth can be enhanced if soil quality is improved. A variety
    of soil testing techniques could be conducted on the fields. However, some thermal
    imaging cameras were used in the past that permitted image processing only when
    a predetermined range was achieved. On the other hand, many upgraded thermal cameras
    may now be used to process photographs with ease. Factors like air quality, soil
    moisture, and pH level are significant for farming and can be monitored for optimal
    production. After undertaking a thorough examination of the discipline, a sample
    of approximately 27 papers was researched and analyzed in detail to provide an
    insight into the existing studies. Four terms were chosen for discussion: visible
    and thermal imaging, crop and soil sensing methodologies, data analysing methodologies,
    and applications. SECTION II. Computer Vision In Agriculture Computer Vision works
    in two stages: image acquisition and image processing. The marvels of Computer
    Vision are the acquisition of image data, processing of the images, and analysis
    of the data using Machine Learning, Artificial Intelligence and Convolution Networks.
    Diego Inácio Patrício, et al. (2018) classified some papers into groups, year,
    crop, device, target, classifier for systematic review. Diseases and pests, Phenology
    and Phenotyping, and Grain quality were the parameters of the classifications.
    The broader applications and techniques of machine learning, video, and image
    processing in detecting agricultural patterns were discussed. Their study focused
    and emphasized the new approaches in computer vision and artificial intelligence
    that can further the field of agriculture for better food production, quality,
    and safety [1]. Mukesh Kumar Tripathi, et al. (2016) described the significance
    of computer vision in vegetables and fruits and the corresponding methods required
    to classify and detect the diseases. The research of computer vision in fruits
    and vegetables and their overall performance, comparison of performance, advantages,
    and disadvantages were analyzed, including a framework for classification and
    recognition of vegetables and fruits [2]. Alan Bauer et al. (2019) analysis shows
    that Aerial imaging is widely used for crop monitoring during growth seasons.
    Computer vision, Machine learning, and Deep learning were used to decipher the
    large dataset of images to obtain phenotypic information. AirSurf - an analytical
    platform was introduced to determine normalized difference vegetation index (NDVI),
    aerial imagery for data collection, crop counting using deep learning, image processing
    using computer vision, and crop quality assessment using machine learning. The
    heads and plantation layouts of Lettuce were measured with the help of ultra-large
    NDVI images. Their research and study improved lettuce production and quantified
    reliable crop quality by ameliorating the yield of lettuces [3]. Computer Vision
    needs thermal and visible imaging data to work, the following section discusses
    the methodologies behind the image acquisition technologies. SECTION III. Visible
    and thermal Imaging Thermal and visible imaging is being used in the new era of
    crop and soil health monitoring systems. The visible and infrared radiation patterns
    of objects are captured by camera sensors and turned into images. These images
    can be analyzed for making decisions on crop health. To monitor crops with imaging
    sensors, different methods are being used. Effective imaging methods could include
    measuring and analyzing data taken wirelessly from thermal sensors and visible
    cameras placed in a drone or a satellite. R. Vadivambal et al. (2011) shared an
    analytical review that thermometers, thermocouples, thermistors, resistance, thermal
    detectors cannot be used without a physical connection with the plants. However,
    the thermal imaging process can solve the barrier which can be used without physical
    attachment. The signal processing unit receives the electrical impulse and transforms
    the data into a thermal image in their mentioned process [4]. Yasin Osroosh et
    al. (2018) discussed thermal imaging and analysis through the data collected by
    low-cost thermal RGB cameras. The thermal-based approach can detect the effects
    of salinity and water stress on crops. Thermal sensing technologies on the horizon
    combine reliability, precision, and low cost. On the other hand, affordable single-board
    computers like the Raspberry Pi have made onboard image processing possible. One
    of the drawbacks with imagers was optimizing power usage or maintaining the power
    supply for operations that required uninterrupted monitoring for a long duration
    of time [5]. Ruwaida Ramly et al. (2020) were effective in detecting plant’s surface
    temperatures. The consumer can observe the plants using live streaming throughout
    the day. Since the device is totally automated, customers are not required to
    water their plants when the temperature rises. Thus, their primary objective of
    detecting plant water irrigation over an agricultural plot using thermal imaging
    was accomplished successfully. However, certain parameters would yield more significant
    results, hence improve performance. The thermal camera’s resolution and specifications
    need to be upgraded to identify a wider range of area with greater precision.
    Additionally, the framework should include a machine learning or artificial intelligence
    application that enables two nodes to communicate [6]. A. K. Saha et al. (2018)
    enlightened that drones outfitted with appropriate cameras, sensors, and integrating
    modules would aid in achieving simple, efficient, and precise agriculture. They
    used different kinds of sensors such as RGB D sensors, gas sensors, and GPS modules.
    Through these sensors, they collected data and analyzed the data in the Raspberry
    Pi model to operate the seed and pesticide spray. Cloud-based storage was used
    to store the data. The prototype is complicated for implementation in developing
    countries. There are some necessary improvements required because they are collecting
    data with the help of a drone. The drones require a lot of power supply for a
    certain amount of time to collect and analyze the necessary data [7]. Kshitij
    Shinghal et al. (2011) built an intelligent humidity sensor. They combined all
    functions from sensor to bus interface on a single chip, resulting in an integrated
    intelligent sensor. The air humidity unit was calibrated using reference data
    obtained from Honeywell’s HCH 1000 series air humidity sensor, and it worked with
    an accuracy better than 1%. Hence their objective was achieved. Precision irrigation,
    automated weather stations, and agro-meteorological and microclimatic stations
    were the key applications for their intelligent humidity sensors [8]. Vasit Sagan
    et al. (2019) constructed a UAV-Based High-Resolution Thermal Imager for Vegetation
    Monitoring. The thermal camera software used in the study is specifically tailored
    for UAV applications. The FLIR mobile application, which connects to the camera
    through Bluetooth to set all camera and data capture settings, was used. The FLIR
    camera does not collect geotagged data and does not record GPS positions for each
    image during collection. Overall, the three thermal cameras tested (ICI 8640 P,
    FLIR Vue Pro R 640, and thermoMap) proved effective in vegetation monitoring and
    phenotyping. ICI was the best performer among the thermal cameras, but other cameras
    performed closely [9]. Gaetano Messina et al. (2020) analysis covered the most
    up-to-date thermal UAV RS technology [10]. UAV and Satellites Multispectral Imagery
    were discussed and analyzed thoroughly. Sentinel-2 satellite data was acquired
    in their analysis from Copernicus Open Access Hub which contained 13 bands in
    the visible, Near-infrared (NIR), and short-wavelength infrared (SWIR) spectral
    range [28,30,31,32]. PlanetScope’s imagery was also acquired for their study [29]
    and the images were radiometrically and geometrically corrected [29,33]. The characteristics
    of the multispectral and satellite images obtained through Parrot Disco-Pro AG(UAV),
    PlanetScope, and Sentinel-2(satellite) were discussed, including a comparison
    of their vegetation indices [34,35,36]. Brandon Stark et al. (2014) focused their
    work on thermal infrared remote sensing. Experiments were done with the thermal
    infrared (TIR) camera modules, which are costly but provides considerable information.
    A TIR framework is best used as a part of the sensor suite for research purposes.
    Unmanned Aerial Systems (UASs) were also described for environmental monitoring
    purposes [11]. Roselyne Ishim et al. (2014) discussed a wide range of thermal
    imaging applications, including nursery monitoring, irrigation scheduling, soil
    salinity detection, disease, and pathogen detection, yield estimation and forecasting,
    maturity evaluation, and bruise detection [12]. D.M. Bulanon et al. (2009) illustrated
    the fusion of visible and thermal images to improve fruit detection. Image fusion
    is an important aspect in remote sensing, surveillance, agriculture, human identification,
    and medical applications. The FLIR infrared camera was used for their objective,
    and the post-image processing was done using MATLAB [13]. Labbé S et al. (2012)
    combined geometric and radiometric preprocessing data from temperature sensors
    to create spatial irrigation models. A matrix of uncooled microbolometers was
    used to acquire the thermal image. Commercial visible and near-infrared cameras,
    for example, Sony A850 with 50 mm lens (Visible) and 320x240 pixels (FLIR B20)
    (Thermal) cameras were geometrically and radiometrically corrected for irrigation
    monitoring, better conservation, and management of water resources [14]. Mang
    Tik Chiu et al. (2020) provided a thorough analysis of deep learning in visual
    pattern recognition on farmlands. They discussed the use of computer vision to
    compute and analyze large agricultural image datasets, as well as the semantic
    segmentation of agricultural patterns. Agriculture-Vision, a database of 94,986
    high-quality aerial images across the United States was analyzed using their system
    [15]. Marek Wojtowicz et al. (2016) researched remote sensing, a procedure that
    utilizes visible light (VIS), near-infrared (NIR), shortwave infrared (SWIR),
    thermal infrared (TIR), and microwave wavelengths. Vegetation indices compilation,
    forecasting of yield, determination of nutritional requirements of plants, detection
    of disease and pest damage, assessment of water demands of plants, and weed control
    were the applications achieved through the remote sensing method [16]. Hung-Yu
    Chien et al. (2018) used IoT to analyze the infrared thermal images of plants
    and fruits. RethinkDB was used to upload and analyze the data, and the FLIR thermal
    camera was used to capture the images [17]. Determining crop and soil health is
    also essential because necessary nutrients supplied by the soil are the key element
    required for proper crop growth. SECTION IV. Crop and Soil Sensing Methodologies
    L. Testi et al. (2008) primary objective was to measure the temperature and Crop
    Water Stress Index (CWSI) to determine the water stress level based on the temperature
    measurement of the canopy. The initial attempt was to obtain the CWSI that can
    be determined using the two Tc limits for potential and null crop transpiration
    under specified climate factors [18]. Barry Allred et al. (2018) created an agricultural
    water draining system. An unmanned aircraft system (UAS) drainage pipeline modeling
    survey was conducted on a farm field in the United States. At a position over
    a drain line, monitoring equipment was developed to detect temperature and evaluate
    rainfall quantities and soil surface moisture content. The Thermo Map TIR image
    accurately identified approximately 60% of the subsurface drainage facilities
    existing throughout the surrounded territory using the Sequoia camera, which obtained
    VIS/NIR images. The mentioned technique can also be used for different kinds of
    soil fields [19]. Pulkit Hanswal et al. (2013) explained constructing a centralized
    controller unit and an automatic irrigation system regarding the soil moisture
    sensors. For long-distance wireless communication, GSM technology was used. The
    delayed message delivery posed a challenge. The device solved the problem by searching
    for a reliable response from the central controller while also awaiting a response
    from the GSM to ensure network availability. By adjusting the potentiometer, the
    same device can also be used for different crops with differing standards. The
    starting of the motor could be automated to improve the project. Low power consumption,
    less complexity, and low cost were the key factors of the system [20]. Aleksander
    Maria Astel et.al (2011) discussion was about Principal components analysis (PCA)
    and Hierarchical cluster analysis (HCA) approaches, which were used to monitor
    soil health [21]. K. Spandana1 et al. (2020) primary objective was measuring and
    analyzing soil moisture levels, soil type, and soil quality with the help of a
    smart farming application. Another objective was to assist farmers in determining
    the type of fertilizer required for their harvesting operations based on the type
    of plants and the various weather conditions [22]. The next important element
    is the collection of data from the environment effectively as data analysis depends
    on it. SECTION V. Data Analysing Methodologies Zheng Ma et al. (2012) built a
    sensor node to collect agricultural data from the environment. The system utilized
    star networking and SHT11, which benefited from the system’s low power consumption.
    The system enabled real-time processing and monitoring of environmental data such
    as temperature and humidity in greenhouses. Further use of this technology enables
    intelligent perception, intelligent alarming, and rational decision-making [23].
    Zhu Yao-lin et al. (2011) suggested a remote multi-point temperature transmission
    system based on the nRF24L01 radio frequency chip, the C8051F340 microcontroller,
    and the DS18B20 temperature sensor. This technology is also capable of real-time
    temperature detection and communication. Additionally, the technique can be adapted
    for wireless data transmission [24]. According to S. Shibusawa et al. the fundamental
    objective of field experimentation is to generate useful knowledge for farmers
    and to assess the environmental impact of agricultural activities. The GPS facilitates
    the acquisition of geo-referenced data, whilst the GIS permits spatial overviews
    of interpolated maps. To acquire data, an autonomous mobile soil sampler was built.
    The study explored a variety of techniques, including mobile soil sampling, remote
    sensing, and imaging spectroscopy [25]. The collected data can be utilized to
    make vital decisions in different firming methodologies. TABLE I Literature Comparison
    SECTION VI. Green House Farming Yousef E. M. Hamouda et al. (2017) developed a
    system where GSMS was established to facilitate agricultural characteristics such
    as temperature, relative humidity, autonomously and effectively managing greenhouse
    irrigation and cooling. The GSMS application program is built on the android platform
    and can be used on mobile devices. By performing cooling and irrigation actions
    for a calculated amount of time, GSMS increases agricultural productivity and
    reduces agricultural resource waste. Hence, the system’s purpose was successfully
    achieved. Besides, monitoring pH levels, and CO2 level is significant for irrigation
    systems, which could be included to improve the overall monitoring system [26].
    Regardless of the approach, microcontrollers are frequently used in monitoring
    systems. SECTION VII. Microcontroller Applications N V Titovskaya et al. (2019)
    discussed the existing problems and suitable solutions of the firming systems.
    Microcontrollers can play a significant part in the development of devices that
    aid in the development of intelligent and effective firming systems. The agricultural
    application of these automated devices requires that each device have its own
    portion or specialized space for specific tasks. Additionally, microcontrollers
    have the advantage of being far less expensive and having a broad range of applications
    in agriculture and training programs, which makes them ideal for the digital agricultural
    systems [27]. SECTION VIII. Tabulation and Diagrammatic Representation Figure
    1 shows the block diagram representation of the mentioned concepts. A brief discussion
    of the previously presented literatures in the paper are summarized in tabular
    form in Table 1 for the ease of comparison and analysis. Fig. 1. Block diagram
    representation of the mentioned concepts. Show All SECTION IX. Conclusion In agricultural
    monitoring systems, monitoring crop growth and health are essential, soil nutrients
    are vital elements that help maintain crop growth. Several studies were conducted
    over the years to determine these parameters to provide analytical data to ensure
    quality food production and proper crop management. A comprehensive review of
    the papers exhibited that visible and infrared camera modules are pivotal in data
    collection and image processing. The higher the sensor resolution, the more accurate
    and precise the data. Machine learning, Artificial Intelligence, and Computer
    Vision are all being used to extract data from large datasets of images over the
    world. Thermal camera modules with better resolutions are expensive in nature.
    In that circumstance, equipment costs rise, making them unaffordable for large-scale
    implementation in developing countries. Crop and soil health monitoring could
    be maneuvered with less expensive thermal camera models by incorporating improved
    image processing algorithms with the help of acquiring large data sets of images
    and the utilization of deep learning. Soil health and nutrients play an essential
    role in crop development. Laboratory research and soil spectroscopy are time-consuming
    and costly approaches whereas specific detectors, such as soil moisture sensors,
    humidity sensors, pH level detectors, and temperature sensors could be utilized.
    Furthermore, satellite-based, and unmanned aerial imaging systems are much more
    intricate and sophisticated approaches in acquiring uninterrupted wide-ranging
    aerial data compared to regular imaging modules. An uninterruptible power supply
    model is required to power the UAVs to monitor and collect enough data from the
    fields for an extended period of time. A solar-powered system could be proposed
    for the operations of multiple drones in the acquisition of large-scale field
    data. Moreover, crop and soil health could be monitored in real-time using IoT-based
    applications. A number of microcontroller-based embedded systems have been developed
    throughout the years, which included several types of electronic sensors to detect
    the required parameters. Most data collection mediums consisted of FTP servers,
    remote access using Wi-Fi, and cloud storage services. Finally, further studies
    are required in this discipline to bolster agricultural inventions to meet the
    rising quality crop demands across the globe. ACKNOWLEDGMENT We thank Mr. Nafiz
    Ahmed Chisty for his humbling and inspiring guidance to conduct this study. His
    supervision enabled us to conduct in-depth research into this discipline. Authors
    Figures References Citations Keywords Metrics More Like This Monitoring of Plant
    Growth Using Soil Moisture and Temperature Sensor and Camera 2022 45th Jubilee
    International Convention on Information, Communication and Electronic Technology
    (MIPRO) Published: 2022 An AI-Based Prediction Model for Climate Change Effects
    on Crop production using IoT 2023 International Telecommunications Conference
    (ITC-Egypt) Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 5th International Conference on Intelligent Computing in Data Sciences,
    ICDS 2021
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A review on the Imaging Approaches in Agriculture with Crop and Soil Sensing
    Methodologies
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Moazzam S.I.
  - Khan U.S.
  - Qureshi W.S.
  - Tiwana M.I.
  - Rashid N.
  - Alasmary W.S.
  - Iqbal J.
  - Hamza A.
  citation_count: '16'
  description: Weeds affects crops health as it shares water and nutrients from the
    soil, as a result it decreases crop yield. Manual weedicide spray through bag-pack
    is hazardous to human health. Localized autonomous weedicide spray through aerial
    spraying units can help save water, weedicide chemical and effect less on human
    health. Such systems require multi-spectral cues to classify crop, weed, and soil
    surface. Our focus in this paper is on the detection of weeds in the sugar beet
    crop, using air-borne multispectral camera sensors, which is considered as an
    alternative crop to sugarcane to obtain sugar in Pakistan. We developed a new
    framework for weed identification; a patch-based classification approach as appose
    to semantic segmentation that is more realistic for real-time intelligent aerial
    spraying systems. Our approach converts 3-class pixel classification problem into
    a 2-class crop-weed patch classification problem which in turns improves crop
    and weed classification accuracy. For classification, we developed a new VGG-Beet
    convolutional neural network (CNN), which is based on generic VGG16 (visual graphics
    group) CNN model with 11 convolutional layers. For experiments, we captured a
    sugar beet dataset with 3-channel multispectral sensor with a ground sampling
    distance (GSD) of 0.2 cm/pixel and a height of 4 meters. For better comparison,
    we used two publicly available sugar beet crop aerial imagery datasets, captured
    using a 5-channel multispectral sensor and a 4-Channel multispectral sensor with
    a ground sampling distance of 1cm and a height of 10 meters. We observed that
    patch-based method is more robust to different lighting conditions. To produce
    low cost weed detection system usage of Agrocam sensor is recommended, for higher
    accuracy Red Edge and Sequoia multispectral sensors with more channels should
    be deployed. We observed higher crop-weed accuracy and lower testing time for
    our patch-based approach as compared to U-Net and Deeplab based semantic segmentation
    networks.
  doi: 10.1109/ACCESS.2021.3109015
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 9
    A Patch-Image Based Classification Approach for Detection of Weeds in Sugar Beet
    Crop Publisher: IEEE Cite This PDF S. Imran Moazzam; Umar S. Khan; Waqar S. Qureshi;
    Mohsin I. Tiwana; Nasir Rashid; Waleed S. Alasmary; Javaid Iqbal; Amir Hamza All
    Authors 18 Cites in Papers 2193 Full Text Views Open Access Comment(s) Under a
    Creative Commons License Abstract Document Sections I. Introduction II. Materials
    and Methods III. System Architecture IV. Implementation V. Results & Discussion
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    Weeds affects crops health as it shares water and nutrients from the soil, as
    a result it decreases crop yield. Manual weedicide spray through bag-pack is hazardous
    to human health. Localized autonomous weedicide spray through aerial spraying
    units can help save water, weedicide chemical and effect less on human health.
    Such systems require multi-spectral cues to classify crop, weed, and soil surface.
    Our focus in this paper is on the detection of weeds in the sugar beet crop, using
    air-borne multispectral camera sensors, which is considered as an alternative
    crop to sugarcane to obtain sugar in Pakistan. We developed a new framework for
    weed identification; a patch-based classification approach as appose to semantic
    segmentation that is more realistic for real-time intelligent aerial spraying
    systems. Our approach converts 3-class pixel classification problem into a 2-class
    crop-weed patch classification problem which in turns improves crop and weed classification
    accuracy. For classification, we developed a new VGG-Beet convolutional neural
    network (CNN), which is based on generic VGG16 (visual graphics group) CNN model
    with 11 convolutional layers. For experiments, we captured a sugar beet dataset
    with 3-channel multispectral sensor with a ground sampling distance (GSD) of 0.2
    cm/pixel and a height of 4 meters. For better comparison, we used two publicly
    available sugar beet crop aerial imagery datasets, captured using a 5-channel
    multispectral sensor and a 4-Channel multispectral sensor with a ground sampling
    distance of 1cm and a height of 10 meters. We observed that patch-based method
    is more robust to different lighting conditions. To produce low cost weed detection
    system usage of Agrocam sensor is recommended, for higher accuracy Red Edge and
    Sequoia multispectral sensors with more channels should be deployed. We observed
    higher crop-weed accuracy and lower testing time for our patch-based approach
    as compared to U-Net and Deeplab based semantic segmentatio... (Show More) System
    flow diagram. Published in: IEEE Access ( Volume: 9) Page(s): 121698 - 121715
    Date of Publication: 30 August 2021 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2021.3109015
    Publisher: IEEE Funding Agency: SECTION I. Introduction Weeds affects crops health
    as it shares water and nutrients from the soil, as a result it decreases crop
    yield [1], [2]. Weeds should be removed timely for good crop health [3]. Weed
    removal is done in two ways in developing countries such as Pakistan, either manually
    through human workers or using weedicides. Manual weeding with hand tools is time-consuming
    and tedious. Weedicide spray using backpack using human labor affects human health.
    Spraying with tractor causes pollution and is not possible in cases where the
    tractors do not have access. Sometimes equal spray on crop and weed affects crop
    health badly and crop could also absorb hazardous chemicals. Autonomous weed removal
    [4] using robots is widely investigated by researchers. For efficient weed removal,
    weed detection is important. Autonomous aerial spraying units are also developed,
    and field tested for spraying weeds. Weed detection for spraying using aerial
    spraying units (ASU) [5] is better than spraying with ground-based robots as spraying
    with drones is time efficient and become essential in the case where the ground-mounted
    spray unit cannot reach. Currently commercially available developed drones perform
    spraying by using GPS coordinates without distinguishing weeds and crops. To effectively
    spray using drone i.e. to spray only on weeds, the weed profile information must
    be taken out and provided to the drone. In Pakistan, losses caused by weeds in
    our major crops are estimated to be around 388 million US dollars and further
    around 149 million US dollars are spent to control weeds with herbicides, machinery
    or farm labor [6]. Sugar beet is considered as a second major crop for producing
    sugar in Pakistan. Multi-spectral cameras with four or five channels spectral
    bands provide more information about vegetation as compared to 3-channel RGB image
    sensors. The cameras which have NIR and R channels provide NDVI images, are best
    suitable for vegetation detection [7]. Differentiation between weed and sugarcane
    crop is done in [8], the authors applied morphological operation combined with
    Fuzzy real-time classifier. Leaf texture is extracted from morphology and features
    are computed using RGB channel for detection. The limitation of the technique
    is poor performance when the dataset is changed, lighting conditions are changed
    or if there is a different phenotype of crop. Tang et al. [9] identified weeds
    in soybean crop by k-means feature learning combined with a convolutional neural
    network using RGB camera sensor. Maize and weed classification by Zheng et al.
    [10] use support vector machine classifier on RGB texture cues. Support vector
    machines performs better in many cases as compared to other machine learning techniques,
    yet they used handcrafted feature detectors as compared to deep learning-based
    techniques where features are automatically learned. Fawakherji et al. [11] applied
    semantic segmentation for detection of weeds in sunflower crop. In first step
    they applied UNet with VGG-16 backbone to separate vegetation and background.
    In second step they extracted vegetation blobs and afterwards in third step they
    reapplied deep learning using VGG-16 neural network to classify crop and weed
    blobs. Application of deep learning two times and application of semantic segmentation
    in first step has made this approach computationally expensive and time-consuming.
    To detect weeds in sugar beet crop Milioto et al. [12] used a convolutional neural
    network (CNN) based semantic segmentation. Two main drawbacks of the algorithm
    are the usage of RGB images as other multispectral data would have given better
    accuracies and the use of semantic segmentation which is a computational complex
    neural network arrangement. Sa et al. [13] applied semantic segmentation to cluster
    sugar beet and weeds using VGG16 layers in encoder and decoder of neural network.
    Their two datasets contain multispectral images. Usage of semantic segmentation
    makes neural network computationally complex. The dataset used have a ground sampling
    distance around 1 cm. A faster detection is possible if a patch-based technique
    is used as appose to pixel-wise classification. Usually, semantic segmentation
    approach is applied for crop weed detection such as, [11]–[13]. Image Classification
    approach is simple and less computationally expensive approach as compared to
    semantic segmentation where each pixel is classified. Semantic segmentation could
    be applied in the field of crop and weed classification with various semantic
    neural network approaches like SegNet, FCN, U-Net, Deep Lab and Global Convolution
    Network. All these approaches use an encoder-decoder structure. The decoder is
    used to preserve localization information of objects in images, it helps in mapping
    back predictions to individual pixels. Decoder structure has its own computational
    complexity and error associated with it according to the up-sampling approach
    applied. SegNet [14] has an encoder structure like VGG16, it has 13 convolutional
    layers and its decoder structure has up sampling layers for each counterpart in
    the encoder. SegNet is widely used in crop weed semantic segmentation. Both SegNet
    and FCN [15] output is rough due to the loss of information because of heavy down
    sampling. U-Net tries to address the information loss problem in FCN. Information
    is sent to every up-sampling layer in the decoder from the corresponding down
    sampling layer in the encoder making structure more complex than simple FCN and
    SegNet architectures. Deeplab [16] deployed dilated convolutions by increasing
    the filter size and its last pooling layers had stride one which kept down sampling
    rate to only 8 times. Then a series of atrous convolutions is applied in this
    technique. This technique still has loss of information due to down sampling and
    a series of atrous convolutions make this approach heavy. Global Convolution Network
    [17] keeps a record of classification and localization at the same time and uses
    large kernels which make dense connections and hence more complex structure. In
    this paper, we proposed VGG-Beet a convolutional neural network similar to the
    generic design of VGG16 [18] model with 11 weight (10 convolutional layers + 1
    fully connected layer) layers that can classify sugar beet and weeds more accurately
    and faster. The images are broken into smaller square patches of size 21×21 -pixel
    and 41×41 -pixel to feed the CNN network for training and classification. Flow
    diagram of our system is shown in Fig. 1. We have applied our proposed approach
    on our new captured multispectral sugar beet dataset. For a better comparison
    of our classification algorithm we used publicly available sugar beet crop aerial
    images datasets [19]. FIGURE 1. System flow diagram. Show All Our approach removes
    the need for decoder structure to preserve localization information. Our approach
    divides the bigger image into small uniform patches and keeps a record of the
    location of patches. These small patch images are fed to a smaller classification
    model to be classified. Our approach removes the need for up sampling which involves
    heavy computations and errors. We observed that for weed detection in sugar beet
    using aerial images, the image classification approach performs better than semantic
    segmentation. The images are broken into smaller square patches of size 21×21
    -pixel and 41×41 -pixel. These patch images are classified into two classes i.e.
    sugar beet and weed. The rest of the paper is divided into five sections. Section
    2 is about material and methods. Section 3 and 4 are about system architecture
    and implementation. Section 5 and 6 are results & discussion and conclusion. SECTION
    II. Materials and Methods This study uses three different datasets, the details
    are as follows: A sugar beet local dataset has been acquired using Agrocam sensor
    by flying phantom 4 drone. This crop is located at geographical coordinates of
    E72.857886 and N31.832084. It was infested with fumaria indica weed which is found
    in many fields of Punjab, Pakistan. The sowing date of this field was 14 October
    2020 and dataset is collected on 27 December 2020 at the crop age of 75 days.
    The Agrocam Geo is a camera sensor which is designed to monitor crop health, it
    provides NGB images with GPS coordinates. NGB images have three channels i.e.,
    NIR, G and B channels. NDVI image is computed with B and NIR channels. Agrocam
    setting is set to capture 12-megapixel images of resolution 4000×3000 every 0.5
    second. Agrocam is attached to phantom 4 facing downward while its GPS module
    is fixed at the top of the drone. The drone height is kept 4 meters above ground
    and drone moving speed is kept at 2 km/h (0.5 m/s). After dataset collection these
    images are manually labelled with Matlab image labelling app. The Agrocam dataset
    has a train and test images percentage of 77.7% and 22.2% respectively. This newly
    captured sugar-beet multispectral dataset, captured in Lalian, Chiniot, Punjab,
    Pakistan has been made public [20]. The second dataset which is used in this research
    is publicly available [19]. It was captured using Parrot Sequoia camera using
    Mavic pro drone. The images were captured at the altitude of 10 meters with ground
    sampling distance of around 1cm of the different sugar beet crop fields. Parrot
    Sequoia camera has 4 channels (R, G, NIR and RE (red-edge channel)), NDVI and
    CIR composite images are derived from these channels. Field number 005 (210 images
    of resolution 360×480 ) and a small portion of field number 007 (31 images of
    resolution 360×480 ) are used for training and a big portion of field number 007
    (61 images of resolution 360×480 ) is used in testing. Field number 006 is not
    used in training or testing because this field is very much different than the
    other two parrot sequoia fields when ground sample distance (GSD) is compared.
    The parrot sequoia dataset has a train and test images percentage of 79.8% and
    20.2% respectively. For better comparison a third publicly available dataset is
    used. It was captured with Inspire 2 drone using Red Edge camera sensor. This
    dataset was provided by Sa et al. [19]. Red Edge is a five channels multispectral
    camera. The images were captured at the altitude of 10 meters and ground sampling
    distance (GSD) of around 1cm of a sugar beet crop in Rhein Bach, Germany, on 18
    September 2017. The five channels of the camera are R, G, B, RE and NIR, NDVI
    is computed with R and NIR channels, RGB composite image data is achieved by combining
    R, G and B channels and CIR (color infrared) is obtained by stacking R, G, and
    NIR channels. In total there are eight types of images of the same fields i.e.
    R, G, B, RE, NIR, NDVI, CIR and RGB using Red Edge sensor. The ground truth of
    this dataset is also provided. This dataset contains five fields of sugar beet
    crop, (Field 000-004), Field number 000, field number 001, field number 002 and
    field number 004 are used in training. Field number 000, 001, 002 and 004 have
    107, 90, 145 and 61 images of resolution 360×480 respectively. Field number 003
    have 94 images of resolution 360×480 , is reserved to test the approach on unseen
    data. The overall percentage of images used for training and testing are 81.1%
    and 18.9% respectively for Red Edge Dataset. In Parrot Sequoia and Red Edge datasets,
    treating each channel as an image, results in a total of 1.76 billion pixels.
    According to Sa et al. [13], most probably this is the largest sugar beet multispectral
    dataset which is publicly available. Total area covered in these two datasets
    is 0.8934 hectares and 0.762 hectares respectively for Red Edge and Sequoia dataset
    [19]. The three datasets have been captured at different lighting conditions,
    geographically distant locations, and soil conditions. Agrocam dataset is taken
    in Punjab, Pakistan, Parrot Sequoia dataset is taken in Eschikon, Switzerland
    and Red Edge dataset is taken in Rheinbach, Germany. Agrocam dataset is taken
    around 4:00 PM Sequoia data is taken around 12:00 PM and Red Edge data is taken
    early in the morning at around 9:30 AM hence different lighting conditions are
    present in these datasets. There was almost six months difference in Parrot Sequoia
    and Red Edge datasets capture implies that these datasets were taken in different
    seasons. Also, growth stages are different in all datasets, In Agrocam dataset,
    size of crop and weed plants are 28–62 cm and 18–113 cm respectively, the sizes
    of crops and weeds exhibited 8–10 cm and 5–10 cm respectively in the Sequoia dataset
    while in Red Edge dataset size of crops and weeds are 15–20 cm and 5–10 cm, respectively.
    There are five fields in the Red Edge dataset and three fields in the Sequoia
    dataset which are located at different places. These different fields have different
    GSD (ground sample distance) means that these different datasets are taken with
    various heights. Agrocam plant sizes are bigger than other two datasets as this
    dataset is taken at lower altitude of 4 meters. Another reason of bigger plant
    sizes of this dataset is growth stage, this dataset is captured lately as compared
    to other two datasets at the stage when field was heavily infested by weeds. In
    dataset overlap between crop and weed plants is visible making this dataset classification
    more challenging than other two datasets which are taken at early crop stages
    and there is less overlap between crop and weeds. Agrocam, Parrot Sequoia and
    Red Edge sensors operate on different wavelengths for same channels and therefore
    we trained separate models for each sensor. These sensors are different from each
    other in terms of focal length, field of view, number of spectral bands and wavelengths
    of same channels and therefore separate trained models will be required for each
    sensor. Fig. 2 (a)-(b) shows two image segments which are taken from two different
    fields of Red Edge dataset, different lighting conditions are visible on RGB data.
    (Note that these datasets are normalized using sunshine sensor, which is part
    of Red Edge and Sequoia sensors, but still different lighting conditions are visible
    to naked eye after normalization). Fig. 2 (c)-(d) show two NDVI data image segments
    from Parrot sequoia two different fields, the lighting conditions are more visible
    in NDVI data. So, lighting conditions are already addressed in this research.
    FIGURE 2. Different lighting conditions: (a) An image segment from Red Edge field
    number 000; (b) An image segment from Red Edge field number 001; (c) An image
    segment from Sequoia field number 005; (d) An image segment from Sequoia field
    number 007; ((a)-(b) show different lighting conditions in RGB Red Edge dataset,
    (c)-(d) show different lighting conditions in NDVI Sequoia dataset.) Show All
    SECTION III. System Architecture Proposed weed classification method classifies
    small patches instead of pixel-based classification. The input image is first
    segmented into smaller square patches. These patches are then fed to the system
    for further processing. The proposed methodology performs training and testing
    separately for Agrocam, Parrot Sequoia and Red Edge sensors. Training is done
    using available masks or manually labelled images. The first step is to compute
    NDVI images and detection of vegetation. NDVI for each pixel is calculated using
    equation (1). NDVI=(NIR−RED)/(NIR+RED) (1) View Source Red channel is not present
    in Agrocam so NDVI value for each pixel of Agrocam image is computed by using
    Blue and NIR channel (RED is replaced with BLUE in equation-1 for Agrocam sensor).
    The computed NDVI images are binarized to generate mask images with threshold
    value of 0.5 for Red Edge and Sequoia sensors, while for Agrocam sensor the threshold
    is kept at 0.3. The original image (single or multiple channel) is then divided
    into a grid of small patches and only those patches are considered for further
    processing in which vegetation is detected. A patch is considered to have vegetation
    if its corresponding vegetation mask have at least five pixels labelled as vegetation.
    The threshold values are computed heuristically. Experiments are done with two
    different patch sizes i.e., 21px ×21 px – pixels and 41px ×41 px – pixels. The
    flow of testing and training is same to this stage (see Fig. 1). For training,
    crop and weed patches are separated manually, patches containing both classes
    are dropped and are not used for training our proposed VGG-Beet CNN model (see
    Fig. 3 and Fig. 4). Cross validation data is used to validate the results and
    to stop model training on ideal weights. FIGURE 3. Applied deep learning model
    flow diagram. Show All FIGURE 4. 2-inputs model flow diagram. Show All For testing
    our system compute vegetation patches as explained earlier, which are then fed
    to the trained model of respective sensor for classification. The detailed system
    flow diagram is shown in Fig. 1. The procedure of computing vegetation patches
    for different channels used is the same (see Table 3, 6, and 5). Table 1 and 2
    show original dataset images and the number of patch images cropped with our method
    and their utilization in training and testing. TABLE 1 Dataset Details (Training)
    TABLE 2 Dataset Details (Testing) TABLE 3 Testing Results of Agrocam Dataset With
    Different Combination of Channels TABLE 4 Testing Results of Parrot Sequoia Dataset
    With Different Combination of Channels TABLE 5 Testing Results of Red Edge Dataset
    With Different Combination of Channels TABLE 6 Accuracy on Single Class Red Edge
    Dataset Patches— When Input Data is (a) RGB; (b) CIR; (c) RGB + NIR; (d) RGB +
    CIR. (S Represents Sugar Beet, W Represents Weeds, and N is Total Number of Single
    Class Patches) A. Neural Network Architecture Smaller image patch requires a smaller
    deep learning model, we used the generic VGG network to design our VGG-BEET network
    with 11 weight layers (containing 10 convolutional and 1 fully connected layer
    (total 18 layers)). The deep learning model applied is shown in Fig. 3. The network
    is retrained for different inputs (individual channels or composite images). For
    two inputs, two similar networks as shown in Fig. 3 are concatenated in such a
    way that the new network contains separate first 16 layers for both inputs. The
    last two layers (fully connected layer and classification layer) are shared (see
    Fig. 4). Same strategy is applied to create models to process more than 2-inputs
    in this paper i.e., every new input pass through first 16 layers separately and
    then passed through last two common layers. SECTION IV. Implementation Models
    were trained separately for Agrocam, Sequoia and Red Edge datasets. We used Keras
    with Tensorflow-GPU libraries for implementation of our framework in python. The
    hardware used is a laptop with core i5 eighth generation CPU having 16 GB RAM
    and NVIDIA GTX 1050 graphics processing unit. A learning rate of 0.00010 is applied
    with Adam optimizer using a categorical cross-entropy loss function, data augmentation
    of horizontal and vertical flips are applied with a validation split of 0.20.
    The deep learning model always converges for all combinations of channels. Convergence
    problems occur when further 2–3 convolutional layers are removed from reported
    network. The training is monitored to stop it on ideal weight values so that to
    prevent the model from overfitting. For that purpose, validation loss is monitored
    at every epoch, if at any epoch validation loss is decreased then model is saved
    at that epoch as best model. Heuristically a patience parameter of 20 is applied
    to see if validation loss will further decrease, if validation loss does not decrease
    anymore within 20 epochs then further training is stopped as the model is now
    overfitting. Epoch with minimum validation loss is saved as the best epoch and
    corresponding deep learning model state is saved as the best-trained model. We
    have used accuracy, Matthews correlation coefficient (MCC), AUROC (area under
    the receiver operating curve (This curve has true positive rate on y-axis and
    false positive rate on x-axis)) and AUC (area under the precision recall curve
    (This curve has precision on y-axis and recall on x-axis)) as evaluation metrics.
    Equation (2) and (5) represent accuracy and Matthew’s correlation coefficient.
    ACC= A= B= MCC= TPRate= FPRate= Recall= Precision= (TP+TN)/(P+N) TP×TN−FP×FN (TP+FP)(TP+FN)(TN+FP)(TN+FN)
    − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − √ A/B
    TP/(TP+FN) FP/(FP+TN) TP/(TP+FN) TP/(TP+FP) (2) (3) (4) (5) (6) (7) (8) (9) View
    Source SECTION V. Results & Discussion After training, prediction is performed
    on unseen testing data. After prediction actual labels are read to compute performance.
    Results of our proposed approach on our acquired sugar beet dataset are summarized
    in Table 3. Our acquired Agrocam dataset is very challenging as there is highly
    cluttered vegetation. Weeds and sugar beet are overlapping in the dataset making
    classification very challenging. Table 3 shows quantitative results on 1,03,030
    labelled 21×21 pixel size patch images from our unseen test data on single channels
    and NGB composite images. We have achieved 86.3% accuracy on our newly captured
    Agrocam dataset of sugar beet crop on NGB composite images data. More accuracy
    is observed on composite 3-channel images as compared to single channels. Testing
    results of Parrot Sequoia dataset with different combination of channels are shown
    in Table 4. More accuracy is observed on CIR 3-channel composite images as compared
    to single channel NDVI images. When the number of input channels are further increased
    to five then maximum accuracy of 90.7% is achieved. This shows a direct relation
    of number of channels and accuracy up to eight channels, the accuracy decrease
    if we increase the channels more than eight (see Table 4). Minimum validation
    loss, overall testing accuracy and AUROC are summarized in Table 5 for all separate
    channels, composite images and combination of channels of Red Edge dataset. The
    trend of better accuracy on 3 channel composite images as compared to single channels
    is again observed by looking at minimum validation loss and AUROC in Table 5.
    Multiple inputs are tested to see any improvement of classification accuracy and
    it was observed that certain combinations of channels give higher accuracy, in
    our case combination of RGB + CIR data gave the best accuracy of 0.92 for Red
    Edge dataset. Maximum Overall Testing Accuracy (ACC) of Agrocam and Red Edge datasets
    are 86.3 and 92.4 respectively which could lead to the conclusion that flying
    low was not very effective however, 86.3 % accuracy of Agrocam is obtained with
    3-channel data, while 92.4 % accuracy of Red Edge dataset is obtained with 6-channels
    data, as input to the system. If we look at accuracy of 3-channel CIR input of
    Red Edge data, which is 88.4 % in Table 5, we see that this accuracy is comparable
    to Agrocam 3-channel accuracy. Therefore, accuracy of the system improves when
    a greater number of channels are used. Agrocam, Parrot Sequoia and Reg Edge sensors
    cost approximately $ 380, $ 3,500, and $ 5,500 respectively, so better accuracies
    come with greater system price. Total there were 17,660, 21px ×21 px- pixel non-overlapping
    vegetation patches in the testing field number 003 in Red Edge dataset. Out of
    total 17,660 patches, 12138 patch images were sugar beet and 4447 patch images
    were containing weeds. There are 1075 patches which contain both classes. In some
    cases, only one pixel of any class was present with other majority class. Out
    of 1075 patches, in 610 patches sugar beet is majority class and in 465 patches
    weed is majority class. For overall accuracy, majority class is defined as true
    class. Majority class is calculated by counting number of labeled pixels in patch
    images. We have also computed separate accuracy for 16585 (93.9% patches) single
    class patches, where both classes don’t merge, and for 1075 (6.1% patches) problematic
    patches which contain both classes to see if algorithm decide majority class or
    otherwise on 4 different best input cases where validation loss is minimum. From
    Table 5 best input cases where validation loss is minimum are numbers 7, 8, 12
    and 15. We have evaluated these four best cases further in detail to find out
    the best input case. Table 6 shows confusion matrices and performance metrics
    results of 16585 (93.9% patches) single class patches. Table 7 shows confusion
    matrices and performance metrics results of 1075 (6.1% patches) mix class patches.
    Table 6 is important as it decides the fate of the majority of patches (93.9%
    patches). The best input case will be the one in which weeds are efficiently classified.
    The case where fewer weeds are classified as crop. Another attribute of the best
    case will be minimum classification of crop as weed. According to these two criteria’s
    input case (d) is best in Table 6 where RGB and CIR patch images are used as input.
    TABLE 7 Limited Accuracy on Mix Class Red Edge Dataset Patches— When Input Data
    is (a) RGB; (b) CIR; (c) RGB + NIR; (d) RGB + CIR. (S Represents Patches Classified
    as Sugar Beet, W Represents Patches Classified as Weeds, mS Represent Number of
    Patches With More Sugar Beet Area While mW Represent Number of Patches With More
    Weed Area and N is Total Number of Mix Class Patches) Combination of RGB + CIR
    provides better results when it comes to the classification of mixed class patches
    as compared to other three cases which are evaluated. Table 7 provides the results
    on mixed class patches (6.1% patches). The weeds close to the crop pose a limitation,
    in our patch-based technique. we get problematic patches where both sugar beet
    and weed are present. Such mix class patches which are 6.1% in the case of test
    field number 003 in Red Edge data, the accuracy is limited as shown in Table 7.
    However, the combined accuracy is still better than pixel-wise (semantic segmentation)
    approach. We have compared our technique and results with the benchmark paper
    which in our case is Sa et al. [13]. They applied semantic segmentation with SegNet
    architecture in which encoding part was VGG16 layers followed by decoding layers
    for each counterpart. Their technique classifies into background, sugar beet and
    weeds. Our technique is based on classification of vegetation patches using only
    10 convolutional layers deep learning model described above. Our technique is
    focused on detecting vegetation (sugar beet + weeds), cropping patch images and
    then patch classification. A comparison is drawn in Fig. 5 using different channels
    as input on test field number 003 between Sa et al. [13] average AUC of sugar
    beet and weed using pixel-wise semantic segmentation approach and our AUC of sugar
    beet and weed using patch classification-based approach. Results show better performance
    on our patch-based approach. In all input cases our technique has performed well
    as compared to the set benchmark. In the case when NDVI images are used both techniques
    performed considerably good, the performance difference is lower comparably, this
    is because NDVI images give best distinguishing parameters, which means that NDVI
    images are most suitable data type to be processed when targeted classification
    task is vegetation. The performance difference is highest with NIR channel images,
    where our technique performed very well as compared to the set benchmark. FIGURE
    5. Comparison between our and benchmark paper AUCs: Result comparison when Red
    Edge dataset is used (Test data is same as [13] i.e. Field number 003). Show All
    The Qualitative results are shown in Fig. 6 and 7. FIGURE 6. Qualitative predicted
    results of some cropped patch images: (a) Cropped patch images of our Agrocam
    dataset; (b) Predicted patches output of Agrocam dataset; (c) Cropped patch images
    of Sequoia dataset; (d) Predicted patches output of Sequoia dataset; (e) Cropped
    patch images of Red Edge dataset; (f) Predicted patches output of Red Edge dataset,
    (Blue color boxes are patches which are input to the system, Red color patches
    are predicted as weeds while Green color patches are predicted sugar beet output).
    Show All FIGURE 7. Qualitative classification results of some cropped patches
    after reading labels: (a) Cropped patches of our Agrocam dataset; (b) Classified
    patches output of Agrocam dataset; (c) Cropped patches of Sequoia dataset; (d)
    Classified patches output of Sequoia dataset; (e) Cropped patchs of Red Edge dataset;
    (f) Classified patches output of Red Edge dataset, (Blue color boxes are labelled
    patches which are input to the system, Red color patches are correctly classified
    weeds, Green color patches are correctly classified sugar beet, Yellow color patches
    are actually weeds which are wrongly classified as sugar beet and Cyan color patches
    are actually sugar beet which are wrongly classified as weeds). Show All The best
    performance of Sa et al. [13] technique achieved 0.82 AUC using (B + CIR + G +
    NDVI + NIR + R + RE) 9 channels, when the same channels are passed, and our technique
    is applied, 0.90 AUC is achieved (shown in Fig. 5). One other main advantage of
    our technique is it gives better performance than Sa et al. [13] using fewer input
    channels. The results show better classification accuracy using our approach as
    compared to Sa et. al. [13] semantic segmentation application on the same data.
    Fig. 6 shows input patches to the system and respective predicted output while
    Fig. 7 shows labelled patches input to the system and respective classification
    results. Fig. 7 show classification results after matching predictions with labels.
    Note that Fig. 6 and 7 are same except one thing, Fig. 6 gives prediction results
    and Fig. 7 provide classification results after reading patch labels. Now the
    problem is our dataset is complex and at some locations weed and crop is confusing
    and therefore cannot be labelled, this is why only labeled patches are cropped
    in Fig. 7(a) to check if they are classified correctly. Figure 6(a) is the same
    one as Fig. 7(a), it is seen that all vegetation is detected in Fig. 6(a). Our
    captured Agrocam sensor data is heavily infested where crop and weeds are overlapping
    while Red Edge and Sequoia datasets are taken at very early stage of crop and
    are less infested by weed as compared to our captured dataset. There are different
    trained models for each sensor. The system has been tested on all three datasets.
    More visual results are uploaded to [21]. Weed profiles output on complete field
    of Red Edge and Parrot Sequoia tested datasets are shown in Appendix A. Table
    8 provides comparison between state-of-the-art semantic segmentation techniques
    and our patch-based approach with different patch sizes. UNet and Deeplab v3 are
    state of the art semantic segmentation algorithms and these deep learning algorithms
    are tested and compared with our approach. We observed Higher crop-weed accuracy
    and lower testing time for our patch-based approach. Confusion matrices using
    UNet and Deeplab techniques are shown first for all three datasets in the Table
    8. We observed a common problem in all these confusion matrices and that is many
    crop and weed pixels are wrongly classified into background and the most probable
    reason of this problem is class imbalance. As background cover most of the area
    in the dataset the predictions are inclined towards background class. This problem
    has resulted in higher background prediction accuracy and lower crop-weed classification
    accuracy. Our patch-based approach has eliminated this problem by taking out background
    patches first. In Table 8, we can see that when our patch-based approach is applied,
    none of the crop or weed patches are wrongly classified into background, this
    is due to removal of background patches prior to classification of crop and weed.
    TABLE 8 Comparison of Confusion Matrices Between State-of-the-Art Techniques and
    Our Patch Based Approach (for Semantic Segmentation Approaches Values in the Confusion
    Matrices Denotes Pixels and for Patch Based Approach Values in the Confusion Matrices
    Denotes Vegetation Patches) Semantic segmentation addresses 3-class problem, and
    our patch-based approach detects background and removes it, this way vegetation
    is separated out and vegetation (sugar beet or weed) patches are cropped. As a
    result, a 3-class pixel classification problem is converted to a 2-class crop-weed
    patch classification problem. By converting 3-class problem to 2-class problem
    we eliminate the risk of vegetation (crop or weed) classification into background
    and vice versa. We see in the Table 8 much vegetation (crop or weed) pixels are
    classified into background and vice-versa for semantic segmentation algorithms.
    In our patch-based approach case, background patches are removed first and as
    a result none of the crop or weeds patches are classified into background and
    vice versa, this practice in turns improves crop and weed classification accuracy.
    Table 9 shows Comparison of sugar beet, weed and mean sugar beet weed accuracy
    of Agrocam, Sequoia and Red Edge datasets using Unet, Deeplab and Our Patch based
    technique. We observed that sugar beet and weed classification accuracy is improved
    using our patch-based approach. Table 9 also gives us comparison of using small
    ( 21×21 ) and bigger ( 41×41 ) patch sizes and we have seen better accuracy in
    the case of bigger patch sizes. Although using bigger patch size seems more accurate,
    we recommend usage of small patch size for Sequoia and Red Edge datasets as we
    know in these datasets vegetation sizes are less than 20 pixel across so small
    ( 21×21 ) patch size suits more. TABLE 9 Comparison of Sugar Beet, Weed and Mean
    Sugarbeet-Weed Accuracy of Agrocam, Sequoia and Red Edge Datasets Using Unet,
    Deeplab and Our Patch Based Technique With Different Patch Sizes. (%CWA Shows
    Percentage Class Wise Accuracy, PBA Stands for Patch Based Approach) Another advantage
    of our patches-based approach is dealing with small size patch images while for
    semantic segmentation bigger image sizes are required according to the backbone
    encoder size of neural network, this leads to less memory requirement and lower
    computations in the case of our approach. Table 10 highlights lower testing time
    using our patch-based approach due to less computational complexity as compared
    to semantic segmentation by using same processing hardware and same datasets.
    We have also observed that bigger patch size is less time taking. TABLE 10 Comparison
    of Testing Time of Agrocam, Sequoia and Red Edge Complete Datasets Using Unet,
    Deeplab and Proposed Patch Based Technique With Different Patch Sizes. (Testing
    Time in Seconds on Complete Datasets Using Intel Core i5 8th Generation Processor)
    An additional advantage of our approach is it could be used to spray individual
    patch areas and spray on crop or soil can be avoided. Our patch-based approach
    can be used for directed spray on weed patches by saving spray on soil and crop
    plants while in the case of semantic segmentation directed spray is impossible
    as predicted weed pixel locations are random and single pixels are hard to target.
    Table 11 lists results of sensitivity analysis for several configuration of patches
    generations in the test images of Parrot Sequoia dataset. TABLE 11 Sensitivity
    Analysis for Several Configuration of Patches Generations in the Test Images of
    Parrot Sequoia Dataset (N is Total Number of Vegetation Patches, CLS Stands for
    Class, PRE Stands for Predicted, CONF Stands for Configuration, S Stands for Sugarbeet
    and W stands for Weed) We have used three different configuration of patch generation
    (Note that with different configuration, slightly different number of vegetation
    patches are achieved). In configuration 1 the whole image is cropped to small
    patches without overlap and without any bias. The second configuration of patch
    cropping is done by adding a bias of 10 pixels in rows and columns of cropped
    test patch images and third configuration of patch cropping is generated by adding
    a bias of 10 pixels in rows of test patch images. We have produced these 3 configurations
    for both 21×21 and 41×41 patch sizes of parrot sequoia dataset. The Sensitivity
    results show satisfactory performance. The GSD of our Agrocam dataset is 0.2 while
    the GSD of Red Edge and Parrot Sequoia datasets is around 1cm/pixel, this is due
    to different dataset capturing height and image sensors resolution. The networks
    are trained and tested on the minimum GSD range of 0.83cm/pixel to a maximum GSD
    range of 1.07cm/pixel for Red Edge and Parrot Sequoia datasets. For Agrocam dataset
    the network is trained and tested on GSD around 0.2 cm/pixel. If any new test
    dataset will be tested with out-of-range GSD, then classification results could
    become worse. In that case, new training data with similar GSD will be required
    to retrain the network. SECTION VI. Conclusion A new framework for weed identification
    is developed; a patch-based classification approach as appose to semantic segmentation
    that is more realistic for real-time intelligent aerial spraying systems. For
    classification, we developed a new VGG-Beet convolutional neural network (CNN),
    which is based on generic CNN (VGG) model with 11 convolutional layers. For experiments,
    we acquired a sugar beet dataset with 3-channel multispectral sensor with a ground
    sampling distance of 0.2 cm/pixel and a height of 4 meters. For better comparison,
    we used two publicly available sugar beet crop aerial imagery datasets, captured
    using a 5-channel multispectral sensor and 4-Channel multispectral sensor with
    a ground sampling distance of 1cm and a height of 10 meters. Three different multispectral
    sensors datasets are used in the experiments, we observed that the same channels
    in these sensors have different wavelengths and require separate trained model
    for each sensor. Agrocam, Parrot Sequoia and Red Edge datasets have various natural
    conditions and we observed that our patch-based method is robust to lighting conditions.
    We designed experiments to see performance on individual and multiple channels
    for each sensor used. The trend of better accuracy on 3 channel composite images
    as compared to single channels is observed. In general, using 3-channel images
    are better than single channels for all three sensors. For Red Edge sensor we
    observed that using 6-channels (RGB + CIR) performed better than individual 3-channel
    RGB or CIR input data. We have compared state-of-the-art UNet and Deeplab v3 semantic
    segmentation techniques and our patch-based approach with different patch sizes
    and we observed higher crop-weed accuracy and lower testing time for our patch-based
    approach. Our approach converts 3-class pixel classification problem into a 2-class
    crop-weed patch classification problem which in turns improves crop and weed classification
    accuracy. Our patches-based approach deals with small size patch images while
    for semantic segmentation bigger image sizes are required according to the backbone
    network encoder size, this leads to less memory requirement and lower computations
    in the case of our approach. We observed better accuracy and less testing time
    in the case of bigger patch sizes. Although using bigger patch size seems more
    accurate and less time taking, we recommend usage of small patch size for small
    sized vegetation and bigger patch size for bigger sized vegetation. Our future
    work is focused on developing a technique for mixed class patch images classification
    or reducing mix class patches, increasing the accuracy of weed detection and reducing
    crop classification as weeds. ACKNOWLEDGMENT The authors would like to thank the
    Autonomous Systems Laboratory (ASL) Datasets for provision of publicly available
    database. Appendix A See Figs. 8–11. FIGURE 8. Complete ground truth of sugar
    beet and weeds of testing field number 003 of Red Edge dataset. Red color represents
    weeds and green color represents sugar beet. Show All FIGURE 9. Complete weed
    profile results shown over RGB image data (while using RGB + CIR as input). Showing
    full testing field number 003 of Red Edge dataset. Patches with red color are
    correctly classified weeds while magenta color patches are also weeds which are
    wrongly classified as sugar beet. Partly clustered weed regions in a field are
    obvious. These regions could be efficiently sprayed using drones. Show All FIGURE
    10. Complete ground truth of sugar beet and weeds of testing field number 007
    of Parrot Sequoia dataset. Red color represents weeds and green color represents
    sugar beet. Show All FIGURE 11. Complete weed profile results shown over CIR image
    data (while using CIR as input). Showing testing field number 007 of Parrot Sequoia
    dataset. Patches with red color are correctly classified weeds while magenta color
    patches are also weeds which are wrongly classified as sugar beet. Show All Authors
    Figures References Citations Keywords Metrics More Like This Combination of a
    wireless sensor network and drone using infrared thermometers for smart agriculture
    2018 15th IEEE Annual Consumer Communications & Networking Conference (CCNC) Published:
    2018 Sensors Enabling Precision Spraying in Agriculture: A Case Study 2023 16th
    International Conference on Sensing Technology (ICST) Published: 2023 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Patch-Image Based Classification Approach for Detection of Weeds in Sugar
    Beet Crop
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shiva R.
  - Vimal G.
  - Kaviyarasu M.
  - Lakshmi Joshitha K.
  citation_count: '8'
  description: Robots play helping role in small scale farming and gardening. This
    can be done with help of Intelligence introduced in robots thereby assisting the
    farmers. Drones fitted with thermal camera can interact with crops by monitoring
    the ambient temperature to sense and sprays water, organic fertilizers and pesticides
    automatically with Artificial Intelligence (AI) process. The proposed model also
    adds a feature that whenever birds attack the crops, an active piezoelectric buzzer
    along with a controller enables the drones to detect and move against the birds
    with a loud noise to drive them away preventing the crop damage. The third feature
    of the work is crop health monitoring. Health condition of the crop is analyzed
    with a robotic arm mounted on a moving vehicle along with an image sensor that
    moves along in the field or garden. The arm takes the picture of the crops, analyzes
    the crop patterns and identifies the bugs and pests using image processing (binary
    inversion, dilation). The database for the same is created consists of possible
    information about pests, diseases, growth conditions, climatic factors. Machine
    Learning technique is used to train the drone for decision making and to spray
    the pesticides automatically. Finally the harvested vegetables and fruits are
    freshly packed with the help of delta robots and robotic arms. This prevents processing
    stage and adulteration, thus it retains 100% of nutrition. This method will revolutionize
    the impact of organic farming in the future.
  doi: 10.1109/ICPECTS49113.2020.9337002
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 International Conference...
    Intelligent Farming using Delta Robot Publisher: IEEE Cite This PDF Shiva R; Vimal
    G; Kaviyarasu M; Lakshmi Joshitha K All Authors 7 Cites in Papers 671 Full Text
    Views Abstract Document Sections I. Introduction II. Related Works III. Workflow
    IV. Proposed System V. Hardware Components and Software Used Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: Robots play helping
    role in small scale farming and gardening. This can be done with help of Intelligence
    introduced in robots thereby assisting the farmers. Drones fitted with thermal
    camera can interact with crops by monitoring the ambient temperature to sense
    and sprays water, organic fertilizers and pesticides automatically with Artificial
    Intelligence (AI) process. The proposed model also adds a feature that whenever
    birds attack the crops, an active piezoelectric buzzer along with a controller
    enables the drones to detect and move against the birds with a loud noise to drive
    them away preventing the crop damage. The third feature of the work is crop health
    monitoring. Health condition of the crop is analyzed with a robotic arm mounted
    on a moving vehicle along with an image sensor that moves along in the field or
    garden. The arm takes the picture of the crops, analyzes the crop patterns and
    identifies the bugs and pests using image processing (binary inversion, dilation).
    The database for the same is created consists of possible information about pests,
    diseases, growth conditions, climatic factors. Machine Learning technique is used
    to train the drone for decision making and to spray the pesticides automatically.
    Finally the harvested vegetables and fruits are freshly packed with the help of
    delta robots and robotic arms. This prevents processing stage and adulteration,
    thus it retains 100% of nutrition. This method will revolutionize the impact of
    organic farming in the future. Published in: 2020 International Conference on
    Power, Energy, Control and Transmission Systems (ICPECTS) Date of Conference:
    10-11 December 2020 Date Added to IEEE Xplore: 09 February 2021 ISBN Information:
    DOI: 10.1109/ICPECTS49113.2020.9337002 Publisher: IEEE Conference Location: Chennai,
    India SECTION I. Introduction In the current scenario, technology plays a vital
    role in the development of the every field by means of artificial intelligence.
    This technique can be implemented in the small scale farming, here robots plays
    the key role for farming through AI. Integrated, adaptive conditions are achieved
    by training our model through machine learning. Hence different techniques and
    strategies are followed by robots to grow and analyse the plants. Nowadays food
    processing, adulteration and spraying harmful pesticides cause serious health
    issues. Awareness about these problems are increasing day by day with people relying
    on ancient food habits. According to Global share of Organic farming, Asia contributes
    only by 4% in the field of organic farming. The work here proposes a robotic system
    with intelligence to analyse the plants health conditions and to take the remedial
    measures. This idea will definitely help out the people leading sedentary lifestyle
    and following terrace farming. By implementing this idea the global contribution
    of organic farming can be increased. Drone companies like Precision Hawk offer
    farmers combined packages which include robotic hardware and analysis software.
    The farmer can then move the drone to the field, initiate the software via a tablet
    or smartphone, and view the collected crop data in real time. Robots are designed
    such a way that it directly targets the base of the plant. Several autonomous
    tractors have been implemented for harvesting purpose. Autonomous precision seeding
    with geo mapping is used and drones are used to analyses the soil''s conditions
    etc. HETO agro technics provides the automation for seeding, nursing and harvesting
    of plants. The proposed solution can be implemented for the organic farming which
    can be used as an idea for the startups, which is greatly affordable and the installation
    setup requires only few hectares of land. Section 2 deals with related works,
    section 3 deals with workflow, section 4 deals with hardware setup, section 5
    and 6 deals with component and software description, section 7 gives results and
    section 8 concludes the paper with the future work. SECTION II. Related Works
    Literature shows many works related to the image processing and robotics especially
    for the agricultural domain. Author of [1] gives an insight about the high performance
    computing, the parallel and distributed processing of the image. Work of [2] helps
    out the farmer to have a clear idea about the diseases that affects the plants
    and the remedial measures for the same using ANN technique. The work uses the
    technique of segmentation and filtering to achieve the accuracy of detection reaching
    91 percentage. It works with about 140 samples of three different diseases with
    images having uniform and various backgrounds. The different image processing
    techniques the issues like the blur noise in it and the way the restoration can
    be done are discussed in detail in work [3]. Bhode and Anup Vibuthe in [4] discusses
    how the image processing tool can effectively be used in the agricultural field
    for weed detection and fruit grading. The author also discusses about the measures
    like canopy, yield and the product quality and the ways to improve the same. The
    details of the application of ANN to many fields is discussed by the author in
    [5]. The work in [7] aims at providing a comfortable climate using the fuzzy system
    for the green house environment. The variable rate application is used to reduce
    the inputs given to the field and improve the yield in the precision agriculture
    [8]. The optimization of the nitrogen distribution is done through the GA algorithm.
    The author of [9] has used the neural network for the identification of the diseases
    in the leaves of tomato plant. The K mean clustering and classifiers are used
    along with the image processing steps to achieve an accuracy of 90 percent to
    identify the spots in the leaves. The work in [6], [10] discusses on the data
    analysis and different machine learning techniques in detail. Kale and Patil in
    [11] has used the regression model to predict the crop growth for various crops
    grown during the entire year plantation. The author has worked with the multi-layered
    Neural Network model. SECTION III. Workflow Fig 1. Processing of the acquired
    image Show All Fig 2. Work flow Show All The proposed work aims at identifying
    the pests and weeds in the crop using the image processing technique and to activate
    the arm of the Delta Robot designed to remove the same. The work is done in an
    iterative manner to achieve an optimized condition as shown in fig 1, 2. The image
    of the field acquired is compared with the shape and size of the crop already
    stored in the data base. Thus the pests and weeds are identified and the arm of
    the robot that is designed to move in a predefined path is activated to do the
    necessary action. SECTION IV. Proposed System We propose that organic products
    can be obtained by complete robotized farming in small scale. The implementation
    can be done in three ways using robots. First, Plants are being grown in batches
    of uniform rows and columns that are easy to monitor. The three robots employed
    are, one for spraying pesticides, harvesting, the second one for analyzing the
    status of the plants and the third one for seeding that are basically drones moving
    in a fashion tailored. A autonomous vehicle that moves carries the robotic arm
    fixed to it. This vehicle is equipped with the microcontroller, receiver, camera,
    sensors such as ultrasonic sensors and power supply. When transmitter transmits
    the signal, the microcontroller in vehicle receives the signal through receiver
    and vary the speed of the motors to move against the batch. The transmitters are
    fixed at specific areas in the field and they are responsible for controlling
    the movement of the autonomous vehicle. The turning of vehicle can be done by
    transmitting a particular frequency, so that wheels of the vehicle turns left
    or right. For analyzing part, robotic arm is used. Fig 3. Block diagram self-driving
    vehicle Show All An image sensor placed at the end of manipulator takes the picture
    of the plant''s specific part such as leaves as in Fig 3. The image taken often
    consists noise due to the misalignment of the lenses with sensors. These noise
    can be eliminated by using a appropriate filters in the preprocessing stage. The
    leaves are analyzed for any holes or decay. A separate database is maintained
    which consists of information about the pests, diseases etc. The captured image
    is compared with images in the database to identify the pests or diseases. Fig
    4 shows the plant analyzing unit. This can be achieved by training the arm with
    the help of classifier using machine learning. This information is sent to the
    main computer which decides what has to be done. Fig 4. Plant analyzing unit Show
    All The main computer process the obtained information and calculate the amount
    of pesticide to be sprayed on the batches of plants. Another role of arm is that
    it detects the weeds through object detection. Artificial intelligence is used
    for decision making. This commands the other arm to spray the pesticides. It is
    also mounted on a moving vehicle and the sprayer is installed at the manipulator.
    The spraying mechanism consist of storage tank, embedded circuit and a motor for
    spraying via manipulator. Here the transmitters are programmed in such a way that
    it is used for controlling the speed of the motors connected to w heels via microcontroller.
    The whole set up is shown in Fig 5. Fig 5 Overall block diagram Show All The drones
    are employed with an autopilot system, where the sensors collects the input from
    the environment and it is sent to the microcontroller. Then the microcontroller
    varies the speed of the propellers. A closed loop system is maintained by using
    a feedback to the inputs of the sensors. Hence the direction of the can be controlled.
    It is fitted with the thermal camera which monitors the indoor farm for the availability
    of water, where the blue region denotes the sufficient water and red region denotes
    the scares in water. After analyzing the farm, the drone sends the information
    to the main computer. It checks the information and orders the drone to spray
    water and fertilizers. The spraying mechanism consist of storage tank, embedded
    circuit and a motor for spraying. Hence the main computer is the brain of all
    the robots. High resolution camera installed on every sides of the indoor farm
    to monitor the external agents such as birds or animals entering into the farm
    as in Fig 6. The external agents are detected by object detection technique by
    using machine learning, the software used are tensor flow and open cv. It sends
    the detected object to main computer. Fig 6. Monitoring of the field Show All
    This commands the drones which consist camera and buzzer system. When drones detects
    the external agents, input is given to the controller. The microcontroller activates
    piezoelectric buzzer and hence the drones move against the entered one and force
    them by scaring and to move away from the farm by producing intolerable noise.
    The noise level of the buzzer can be improved by high voltage supply and amplifying
    the noise signal through transistors. In this way there will be no external disturbances
    for plants for their effective growth. The farm is enclosed within the glass to
    produce greenhouse effect. The ambience of the farm is monitored using different
    sensors. SECTION V. Hardware Components and Software Used The Raspberry pi is
    series of small single board computed device. It does not include peripherals
    (such as keyboard and mouse). It consist Speed ranges from 700MHz to 1.4GHz. Its
    memory range is 256MB to 1GB RAM. Fig 7. Raspberry pi Show All The motor used
    is the stepper motor as in Fig 8 is used for every movement of the robotic arm.
    The angle of the movement is calculated after acquisition of the location of the
    weed or pest. The Raspberry pi can act as the master controlling two or more slave
    Arduino boards through the I2C interface. Fig 8. Stepper motor Show All The system
    also uses the sensors like the piezo electric sensor, image sensor, humidity sensor,
    moisture sensor as in fig 9. Fig 9. Sensors used Show All The software used includes
    the Tensor flow and the open CV with the library functions. SECTION VI. Results
    and Conclusion The scenario has a uniform rows and columns of batches of plants
    as shown in the figure. Plants are classified as good or bad based on the acquired
    features, shape and size. Fig 10. Scenario of the field Show All The example scenario
    was taken with the height of the soya plantation and the weeds in it and the size
    based weed removal was simulated. The designed Delta Robot original picture and
    the ugly model designed are shown in Fig 11. The 3D printing was used for the
    development of the robot arm. Table I and II gives details of the anomaly detection
    and the angle calculation of the robot arm. Fig 11. Model of delta robot designed
    Show All Table I. Anamoly detection using z-score analysis The future direction
    of the work involves construction of an autonomous vehicle and a delta robot that
    serves for the purpose of intelligent farming. Table II. Inverse kinematics used
    to design the delta robot Authors Figures References Citations Keywords Metrics
    More Like This Development of an Adaptive Approach for Precision Agriculture Monitoring
    with Drone and Satellite Data IEEE Journal of Selected Topics in Applied Earth
    Observations and Remote Sensing Published: 2017 On-site monitoring of soil condition
    for precision agriculture by using multimodal microchip integrated with EC and
    temperature sensors 2013 Transducers & Eurosensors XXVII: The 17th International
    Conference on Solid-State Sensors, Actuators and Microsystems (TRANSDUCERS & EUROSENSORS
    XXVII) Published: 2013 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: ICPECTS 2020 - IEEE 2nd International Conference on Power, Energy, Control
    and Transmission Systems, Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Intelligent Farming using Delta Robot
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Guerrouj F.Z.
  - Latif R.
  - Saddik A.
  citation_count: '2'
  description: 'Artificial intelligence is a field in full development, from facial
    recognition to autonomous vehicles and referral systems for online shopping, passing
    by smart farming, these new technologies are invading our daily lives. Nowadays,
    agricultural applications require more and more computer vision technologies for
    continuous monitoring and analysis of crop health and yield. That is why machine
    learning has become one of the mechanisms that make farming more efficient by
    using high-precision algorithms. This article deals with the Normalized Difference
    Vegetation Index (NDVI) and the Normalized Difference Water Index (NDWI), which
    are the most widely used indices in precision agriculture. In this work, we adopt
    GPU-based heterogeneous architecture using parallel programming with the CUDA
    language. The algorithm is evaluated on several platforms: NVIDIA Jetson TX1,
    DELL-desktop, and XU4 board. It has been discovered that the execution time of
    the two NDVI and NDWI indices on the embedded TX1 card is more optimized and improved
    compared to the execution time on the XU4 card and the Desktop.'
  doi: 10.1109/CloudTech49835.2020.9365888
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 5th International Confer...
    Evaluation of NDVI and NDWI parameters in CPU-GPU Heterogeneous Platforms based
    CUDA Publisher: IEEE Cite This PDF Fatima Zahra GUERROUJ; Rachid LATIF; Amine
    SADDIK All Authors 1 Cites in Paper 93 Full Text Views Abstract Document Sections
    I. Introduction II. Related work III. Methodology IV. Results and Discussions
    V. Conclusion Authors Figures References Citations Keywords Metrics Abstract:
    Artificial intelligence is a field in full development, from facial recognition
    to autonomous vehicles and referral systems for online shopping, passing by smart
    farming, these new technologies are invading our daily lives.Nowadays, agricultural
    applications require more and more computer vision technologies for continuous
    monitoring and analysis of crop health and yield. That is why machine learning
    has become one of the mechanisms that make farming more efficient by using high-precision
    algorithms. This article deals with the Normalized Difference Vegetation Index
    (NDVI) and the Normalized Difference Water Index (NDWI), which are the most widely
    used indices in precision agriculture. In this work, we adopt GPU-based heterogeneous
    architecture using parallel programming with the CUDA language. The algorithm
    is evaluated on several platforms: NVIDIA Jetson TX1, DELL-desktop, and XU4 board.
    It has been discovered that the execution time of the two NDVI and NDWI indices
    on the embedded TX1 card is more optimized and improved compared to the execution
    time on the XU4 card and the Desktop. Published in: 2020 5th International Conference
    on Cloud Computing and Artificial Intelligence: Technologies and Applications
    (CloudTech) Date of Conference: 24-26 November 2020 Date Added to IEEE Xplore:
    02 March 2021 ISBN Information: DOI: 10.1109/CloudTech49835.2020.9365888 Publisher:
    IEEE Conference Location: Marrakesh, Morocco SECTION I. Introduction The world''s
    population continues to grow over the years, from 7.5 billion in 2017 to 9.8 billion
    in 2050 [1]. As a result, agriculture faces major challenges such as quantitative
    production, good production and sustainable production. To cope with the growing
    challenges of agricultural production, it is necessary to find solutions to produce
    more and better by consuming fewer inputs. Since the 1990s, many studies and initiatives
    have been launched to address these challenges so that agro-ecosystems must be
    understood by continually measuring and analyzing diverse physiological characteristics
    and phenomena. The use of new information and communication technologies (ICTs)
    for the management of crops and farms at the field level facilitates this task
    and extends the concept of precision agriculture, as well as strengthening the
    management of tasks and decision making. Today, the emergence of digital technologies
    such as data analysis [2] [3], cloud computing [4], the Internet of Things IoT
    [5] and remote sensing [6] have supported agricultural practices; all these new
    technologies are leading to the notion of \"intelligent agriculture\". Remote
    sensing can potentially provide information on land use, crop type, crop water
    requirements, salinity and crop yield. Remote sensing covers a large area, including
    areas inaccessible to human exploration using satellites, airplanes and unmanned
    aerial vehicles UAV (eg drones) and allows systematic data collection, allowing
    time series and comparisons between systems. The IoT in intelligent agriculture
    is a system based on advanced sensors (humidity, temperature, light, etc.) to
    monitor the crop field and automate the irrigation system. IoT technologies will
    allow farmers to monitor field conditions from anywhere and improve their productivity.
    Whereas cloud computing is mainly used to store and collect data. It can also
    facilitate real-time computing and data access for users. Cloud computing can
    therefore help farmers make crop decisions based on storing information on newly
    developed crops. In addition, big data analysis used successfully in various sectors;
    it was recently applied to agriculture [7] and enables real-time, large-scale
    analysis of stored data in the cloud.. There are several ways to detect images,
    the most common one depends on satellites using multispectral and hyperspectral
    images. Other methods are being used to a lesser extent, but increasingly, such
    as Synthetic Aperture Radar (SAR), Thermal Imagers and Near Infrared (NIR). Moreover,
    others methods are applied in the classification of fruits and packaged foods,
    such as optical and X-ray imaging. Therefore, the use of imaging analysis techniques
    is important in the field of agriculture for the identification, classification,
    and detection of anomalies. These techniques include machine learning such as
    (K-means, Decision Trees, Support Vector Machine (SVM), Artificial Neural Networks
    (ANN) and others), wavelet filtering, vegetation indices (NDVI) and regression
    analysis [8]. In addition to the above techniques, deep (ANN), also called deep
    learning or deep neural networks (DNN), is a new technology belonging to field
    machine learning (ML). Deep learning (DL) is based on the use of deep neural networks
    to deal with complex problems; the main advantage of this technique is that networks
    can extract these characteristics themselves from the raw data [9] [10]. Artificial
    intelligence (AI), machine learning (ML), and deep learning (DL) are areas that
    require intense computing. So a high performance architecture must be chosen.
    The use of a CPU for such workload might not be the best choice because it contains
    fewer cores, whereas GPUs can handle the tasks of AI, ML, and DL with an efficient
    and a faster way due to the thousands of cores that it integrates. The problems
    of IA, ML, and DL involve extensive matrix operations that can easily be paralleled
    on the GPU. Therefore, heterogeneous architectures such as (CPU-GPU) and (CPU-FPGA)
    are the trend of the ongoing research challenge, where the host is the CPU and
    the device is the GPU or the FPGA. There are several programming platforms available
    to improve GPU performance, such as OpenCL, OpenACC, but the most popular is Compute
    Unified Device Architecture (CUDA) developed by NVIDIA. CUDA is essentially an
    extension of the C/C ++ language that allows to use graphical cards to execute
    highly parallel computing programs [11]. Our contribution: Smart farming or precision
    agriculture aims to support sustainable agriculture by accurately observing and
    measuring. For this purpose, the heterogeneous systems are suitable choice to
    deal with time-consuming and computing-intensive tasks. In our work, we propose
    a parallel computation of the vegetation index (NDVI) and the water index (NDWI)
    most widely used in precision agriculture, using CUDA technology. The main contributions
    are: Our work aims to target heterogeneous architecture such as Jetson TX1 (CPU-GPU)
    in the precision agriculture domain. A parallel implementation is achieved using
    CUDA to optimize the execution time. Efficient implementation of vegetation indexes
    merging two or more of them in single GPU execution. The remaining part of the
    paper is structured as follows : Section II provides a brief description of the
    vegetation and includes related work done in agriculture basing on the vegetation
    indices. Section III describes the proposed algorithm and methodology that was
    used to solve the problem. Section IV deals with the experiments performed and
    the results achieved. Finally, the results are concluded, which includes a comparison
    between the different platforms and future work. SECTION II. Related work Vegetation
    has a significant role in the establishment of ecosystems. It is also of vital
    importance for the economy and health by being at the base of the entire food
    chain. The vital role of vegetation has long been studied, but the remote sensing
    era has opened new opportunities to better understand how it works. Remote sensing
    technologies are increasingly being used in the field of agriculture for a straightforward
    reason: the variables to be measured and monitored are widely dispersed in remote
    areas with limited wireless communications or no power supply, etc… Sensors may
    be multispectral cameras on satellites or mounted on unmanned aerial vehicles
    (UAVs) [12]. New advances in unmanned aerial vehicles (UAVs) have made affordable
    a new set of capabilities for monitoring and extracting useful information from
    remote locations. Furthermore, Images obtained from crops or forests can be processed
    to extract information on productivity, farm health, and the condition of the
    soil [12]. Today, much of the research in the field of precision agriculture has
    focused on vegetation indices, which are widely used to identify and monitor vegetation
    dynamics. At the same time, they are also used in the context of food security,
    prediction of agricultural production or estimation of the probability of forest
    fires. For this purpose, the authors in [14] proposed a parallel extraction algorithm
    of the normalized vegetation index (NDVI) based on a multi-core processor. In
    this work, the authors used the functions of the OpenCV and OpenMP libraries to
    compute and parallelize the NDVI algorithm. They used atmospheric correction in
    the pre-processing phase to remove the ground reflection image due to atmospheric
    and light factors. This is followed by two image enhancement methods, such as
    the minimum and maximum stretch histogram and the equalization histogram. The
    image enhancement method can improve the visual effects of certain image features.
    The experimental environment used in this article [14], is the Intel Xeon (Ultra)
    E5-2697 v3 @ 2.60 GHz (14 cores 28 wires). Afterward, they evaluate the performance
    of the algorithm by comparing the execution time, acceleration rate and parallel
    efficiency, by increasing the number of threads: single thread, 2 threads, 4 threads,
    8 threads, 12 threads, 16 threads, 20 threads, 24 threads, 28 threads. The parallel
    NDVI calculation method proposed in this work achieved a significant acceleration
    rate. However, their work might be more attractive if they joined various indexes
    such as NDWI and OSAVI for extended applications. In the same context, the work
    of [15] proposes an integrated and cost-effective solution (hardware and software)
    for the acquisition and processing of spectral data to estimate the state of vegetation.
    The work of [15] is based on a multispectral camera mounted on an unmanned aerial
    vehicle (UAV) to acquire images of the surveyed field using OpenDroneMap to generate
    georeferenced orthophotomaps that correspond to the multispectral camera wavelength
    (NIR bands, Red Edge, Red + Green + Blue). . Then, these data are transmitted
    to a Small Board Computer (SBC) Raspberry Pi 3, in order to compute different
    vegetation indices such as NDVI, NDWI, CVI (Chlorophyll Vegetation Index), and
    CCCI (Canopy Chlorophyll Content Index) as well as to generate the corresponding
    maps. Likewise [16] aimed to implement unsupervised approaches and data summarization
    in a Lenovo ThinkStation P320 Tiny edge laptop equipped with Nvidia Quadro P600
    GPU, using python3 with the Pytorch GPU acceleration library. The unsupervised
    approaches used in their experiment are NDVI, Enhanced Vegetation Index (EVI),
    and Standardized Vegetation Index (SVI) as feature extraction, while K-means clustering
    and Gaussian Mixture Model (GMM) clustering as data summarization. Then, they
    compared the execution time of these complex models. Through this, the results
    of the work show that the execution time does not satisfy the real-time processing
    requirements of the GPU, in the case of large images and intensive computing tasks.
    In addition, when the size of the images is large (2048 x 2048), even a single
    image is still too large to fit in the GPU’s memory, which leads to an OutOfMemory
    error. The aforementioned works present several studies concerning vegetation
    indices. The first work [14] aims to parallelize the NDVI index computation on
    many threads by using the OpenMP library. Besides, the other works attempt to
    implement various vegetation indices into different platforms such as Raspberry
    [15], while the work [16] implements those indices on a laptop and accelerates
    the processing with GPU accelerator library Pytorch. In contrast, our work aims
    to implement and evaluate various vegetation indices into CPU-GPU heterogeneous
    architecture to achieve real-time execution and performs intensive calculations.
    SECTION III. Methodology In this paper, the implementation is done in different
    phases as follows: dataset collection, preprocessing dataset, parallel computing
    using Cuda technology. A. Precision agriculture Precision Agriculture (PA) provides
    tools and technologies to improve the productivity, quality, and sustainability
    of agricultural production. As well as to make the best technical intervention
    at the right place and at the right time. Using remote sensing can help to map
    soil properties, classify crop species, monitor crop water stress, and detect
    weeds and crop diseases. These data enrich agricultural decision-making, which
    could be obtained from several sources, such as drones and satellites. Through
    the employment of high-resolution spectral tools, the number of bands obtained
    by remote sensing is increased, whereas the bandwidth is decreased [17]. The most
    commonly used and implemented indices are NDVI and NDWI, which are calculated
    from multispectral information as a normalized difference between the red (RED)
    / green (GREEN) and near infrared (NIR) bands, that are expressed as follows:
    NDVI= NIR−RED NIR+RED NDWI= GREEN−NIR GREEN+NIR (1) (2) View Source The NDVI varies
    between 0.1 and 0.8 for vegetation pixels, while ground pixels take values slightly
    above 0, whereas clouds take values below 0. Similarly, the interpretation of
    the NDWI is similar to that of the NDVI. Pixel values less than 0 indicate a bright
    surface without vegetation or water, whereas pixels with values greater than 1
    represent water content [18]. B. The Algorithm study The proposed algorithm is
    separated into four blocks as shown in Figure. 1. The first block aims to acquire
    images transmitted by multispectral cameras, generally separated into two types;
    the first type provides images with separate bands. While the second type provides
    images with non-separated bands, among this type is the TetraCam camera that provides
    two images. The first image contains the three red, blue and green bands and the
    other image contains the NIR band. Besides, this type of camera offers a real-time
    processing advantage due to the acquisition time at one image/second. In our work,
    we used a dataset [19] instead of the camera to evaluate the proposed optimization,
    which contains 100 RGB and NIR images of sugar beet crop with a size of 1296 ×
    966. Subsequently, the second block is the preprocessing phase, which provides
    a test to check the number of bands in the image, if the image contains all bands,
    it is necessary to separate them to the bands (i.e., Red, Green, and NIR) required
    by the NDVI (1) and NDWI (2) as shown in Figure. 2. Otherwise, the conversion
    step starts to convert images to grayscale and in double precision respectively.
    Afterward, the third block is the core of the algorithm, which processes both
    NDVI and NDWI indices simultaneously. Finally, the last block is based on a thresholding
    operation to make decision and store the resulting images. Furthermore, Figure1
    shows the implementation of the proposed optimization on heterogeneous architecture
    using Cuda, while the CPU takes the first, the second, and the fourth block, whereas
    the core of the algorithm is executed on the GPU in a parallel manner, benefiting
    with its capability to process data massively in a suitable time thanks to its
    architecture. Fig. 1. The proposed architecture Show All Fig. 2. Preprocessing
    of image: Original RGB (a), Red band (b) and NIR band (c) Show All C. Cuda Architecture
    The CUDA architecture is composed of associated blocks in a grid, which are constituted
    of threads with a 1D, 2D, 3D block identifier. In the same block, all the threads
    communicate with each other through the shared memory and coordinate to schedule
    their access to the memory. The CUDA program comprises the host code and the device
    code, in which the host code is responsible for the execution of the device code
    and the communication to and from the host memory and the device memory. This
    communication is done by PCI-express protocol. Figure 3 shows the Cuda architecture
    [20]. D. Hardware Implementation On an experimental basis, we implemented the
    algorithm on two CPU-GPU platforms: the DELL desktop and the NVIDIA Jetson Tegra
    X1. Table I presents its technical specifications. The DELL desktop provides two
    processor cores with Hyper-Threading clocked from @ 2.2 to 2.7 GHz (2 cores: @
    2.5 GHz). The architecture of the CPU offers a 3 MB cache memory and 8 GB in the
    RAM. This system also integrates an NVIDIA GeForce GT 920M with 384 shader cores
    clocked at @ 954 MHz. Besides, Tegra X1 is equipped with a large ARM Cortex A57
    quad-core processor, coupled with a small Cortex A53 processor, all at @ 2.0GHz.
    Physically, eight cores are present in the SoC (System on Chip) but are never
    exploited simultaneously. For heavy applications and complex calculations, the
    four A57 cores are activated, while the four A53 cores are inactive. While the
    Cortex A53 is activated, the Cortex A57 goes into sleep mode for small tasks.
    However, the CPU A57 operates at 1.9 GHz with 2 MB of shared cache L2, a 48KB
    instruction cache L1, and a L1 data cache per core of 32-KB. Processor group A53
    runs at 1.3 GHz with a L2 cache shared memory of 512 KB, 32 KB of instructions,
    and 32 KB of L1 data cache per core. Therefore, this technique saves energy. Graphically,
    Nvidia Tegra X1 is based on the Maxwell architecture, which incorporates 256-core
    CUDA, clocks at 1 GHz. Also, the memory of the graphics processor offers a bandwidth
    of 25.6 GB / s and a global memory of 2 GB. The core of the processor itself is
    compatible with 64 bits with a computing power doubled from 512 GFLOPS to 1024
    GFLOPS, 1 TFLOPS. Fig. 3. Cuda architecture [20] Show All TABLE I. Architecture
    Specifications: NVIDIA JETSON Tegra X1, Desktop DELL SECTION IV. Results and Discussions
    In this section, we will examine the results of the experiment on the two platforms
    mentioned. Then we will compare with the work [21], which proposed an implementation
    of NDVI and NDWI on an embedded board XU4, based on two ARM Cortex-A15 Quad @
    2.0GHz and Cortex-A7 Quad @ 1.4GHz processors with a Mali-T628 MP6 GPU with 2Gbyte
    of LPDDR3 RAM. The algorithm takes an average of 91 ms, giving a processing time
    of 10 frames/sec. However, the processing time obtained in [21] can only process
    images sent from cameras that provide less than 10 frames / s for real-time processing.
    In addition, the implementation is no longer real time in the case of cameras
    such as the Parrot Sequoia, which gives 60FPS images. In our contribution, we
    present an implementation that is more optimal in terms of execution time in a
    heterogeneous CPU-GPU architecture. The heterogeneous CPU-GPU architecture is
    taken into account in our work due to its popularity in embedded computing platforms.
    We evaluate the average processing times of the algorithm on 80 images with a
    resolution of 1296×966 pixels. All timings are given in milliseconds. Table II
    shows the timing of the algorithm implemented on different platforms. By unloading
    the processing to the GPU. The algorithm on a DELL desktop works at 50 ms per
    image, which means 20 images/second. Also, on the TX1 embedded platform, the algorithm
    operates at 80 ms per image, which provides 12 images/second. However, the XU4
    card provides 10 frames / second with an algorithm execution time of 91 ms [21].
    Then, with the GPU implementation, on the DELL desktop, we have a better performance
    in which the average execution time of the algorithm is reduced to 0.063 ms per
    image. While for TX1, the processing is slower than that of the DELL desktop.
    The algorithm takes an average of 0.77 ms. Experience shows that the high-end
    NVIDIA GeForce 920M GPU on the DELL desktop achieves a speed 7 times faster than
    the Tegra TX1 GPU, due to the high CPU and GPU frequency, multi-core integrity
    and large cache memory. Moreover, using the GPU architecture for tasks requiring
    intensive data performs more efficiently than the CPU, thanks to parallel computing.
    Table II. MEAN OF EXECUTION TIME (MILLISECONDS) ON THE DELL DESKTOP AND THE TX1
    AND XU4 SECTION V. Conclusion The vegetation indices are popular and widely used
    in precision farming, in order to maximize the sensitivity of the vegetation characteristics
    while minimizing confounding factors such as background soil reflectance, directional
    or atmospheric effects. This work aims to optimize the compute-intensive of NDVI
    and NDWI index benefiting of parallel nature of CUDA language using CPU-GPU heterogeneous
    platforms. The implementation of this algorithm is done on different platforms:
    DELL and the NVIDIA TX1 embedded card. The results show an effective speed-up
    for heterogeneous CPU-GPU systems compared to homogeneous CPU systems. The overall
    work shows that GPU / CUDA frameworks with their thread organization are suitable
    for parallel implementation to compute the various indices required in precision
    agriculture through large images. In general, precision agriculture is under development
    due to the technological level; however, these techniques are extremely promising
    and represent an important turning point for the future of agriculture. The results
    presented in this work show that the resources of the TX1 board are not completely
    exploited, which implies a detailed study of the Hardware/Software Co-Design approach
    for an optimal implementation.As part of future work, we will intend to use this
    work as features extraction for deep learning to detect and classify plant diseases.
    Authors Figures References Citations Keywords Metrics More Like This Optimization
    of Refractive Index Sensitivity in Nanofilm-Coated Long-Period Fiber Gratings
    Near the Dispersion Turning Point Journal of Lightwave Technology Published: 2020
    Despeckling Synthetic Aperture Radar images with cloud computing using graphics
    processing units 5th International Conference on Pervasive Computing and Applications
    Published: 2010 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 'Proceedings of 2020 5th International Conference on Cloud Computing and
    Artificial Intelligence: Technologies and Applications, CloudTech 2020'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Evaluation of NDVI and NDWI parameters in CPU-GPU Heterogeneous Platforms
    based CUDA
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kumar A.
  - Taparia M.
  - Rajalakshmi P.
  - Guo W.
  - Balaji Naik B.
  - Marathi B.
  - Desai U.B.
  citation_count: '2'
  description: The health and yield of crops depend on the use of water, nutrients,
    and fertilizers. Due to climatic changes and reduction in rainfall, farmers are
    relying on groundwater for irrigation, which should be used optimally. The use
    of water and other agronomic inputs can be optimized by monitoring the health
    of crops and soil. Usually, it is done by manual observation, which is labor-intensive
    and time-consuming. In this paper, we propose Chlorophyll Index Green (CIG) vegetative
    index-based method for monitoring the crop health using near-infrared, green,
    and red band images acquired using a multispectral camera mounted on Unmanned
    Ariel Vehicle (UAV). The proposed method clearly classifies the water-stressed
    area of the field and helps in optimizing the irrigation process and monitoring
    the crop-health.
  doi: 10.1109/SAS48726.2020.9220016
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 IEEE Sensors Application...
    CIG based Stress Identification Method for Maize Crop using UAV based Remote Sensing
    Publisher: IEEE Cite This PDF Ajay Kumar; Mahesh Taparia; P. Rajalakshmi; Wei
    Guo; Balaji Naik B; Balram Marathi; U. B. Desai All Authors 3 Cites in Papers
    188 Full Text Views Abstract Document Sections I. Introduction II. Data Acquisition
    and Pre-Processing III. Proposed Method IV. Results and Discussion V. Conclusion
    Authors Figures References Citations Keywords Metrics Footnotes Abstract: The
    health and yield of crops depend on the use of water, nutrients, and fertilizers.
    Due to climatic changes and reduction in rainfall, farmers are relying on groundwater
    for irrigation, which should be used optimally. The use of water and other agronomic
    inputs can be optimized by monitoring the health of crops and soil. Usually, it
    is done by manual observation, which is labor-intensive and time-consuming. In
    this paper, we propose Chlorophyll Index Green (CIG) vegetative index-based method
    for monitoring the crop health using near-infrared, green, and red band images
    acquired using a multispectral camera mounted on Unmanned Ariel Vehicle (UAV).
    The proposed method clearly classifies the water-stressed area of the field and
    helps in optimizing the irrigation process and monitoring the crop-health. Published
    in: 2020 IEEE Sensors Applications Symposium (SAS) Date of Conference: 09-11 March
    2020 Date Added to IEEE Xplore: 12 October 2020 ISBN Information: DOI: 10.1109/SAS48726.2020.9220016
    Publisher: IEEE Conference Location: Kuala Lumpur, Malaysia SECTION I. Introduction
    The world’s population will increase by 33 %, and it will be around 10 billion
    by 2050 [1]. In order to fulfill future food demands, food production should also
    be increased by 70 % [2]–[4]. However, due to urbanization and industrialization,
    there are changes and reductions in agricultural land that are affecting food
    production [5]. Also, there is a reduction in rainfall due to climatic changes
    that are forcing farmers to use groundwater for irrigation. In the world, on an
    average 70% of the freshwater is being utilized for agricultural purposes. Hence,
    agricultural scientists are developing new crop varieties with short growth-period,
    provides a high yield while sustaining with climatic changes, and requires less
    amount of agronomic inputs like water, nutrients, and fertilizers. These new developed
    varieties aid farmers in having multiple crop-cycles and high yields [6], [7].
    The crop-health and yield depend on the use of water, nutrients, and fertilizers
    [8]. Most of the farmers mishandle the use of water and nutrient because of improper
    knowledge regarding the health of crops and soil. The use of agronomic inputs
    can be optimized by monitoring crop health and soil. Usually, this is performed
    by the farmer’s experience and manual observation, which is time-consuming and
    labor-intensive. Hence, there is a need to automate this process. To ease and
    automate this process, the researchers have proposed sensors and robots to observe
    the health status of crop and soil. For measuring soil moisture, there are many
    sensors proposed by researchers [9]-[11]. However, the requirement of irrigation
    also depends on the health and growth stage of the crop. The crop needs less amount
    of water at the maturity stage compared to the initial growth stage [12]. The
    satellite-based techniques have been used to monitor the soil moisture and health
    of the crop [13]-[15]. However, the images of a field from satellite highly depend
    on the frequency of satellite, and its quality depends on weather conditions,
    which can result in erroneous information. The static cameras can improve the
    quality of the image, and data can be acquired frequently compared to satellite.
    In [16], the authors have proposed a design of rover with a fixed camera, which
    helps in identifying the condition of a crop and in optimizing the use of fertilizers
    and pesticides. However, the design of rover depends on the land and crop-structure,
    and it is challenging to cover a large field with a static camera. In [17]-[19],
    hyperspectral imaging and spectral signature have been used for the water-stress
    analysis. In [17], a hyperspectral imaging system with a digital camera was mounted
    on static frame-structure to monitor the apple trees in a greenhouse environment.
    The researchers, [18], used a miniature fiber optic spectrometer to collect spectral
    data from a tomato plant. In these research, various vegetative indices like normalized
    difference vegetation index (NDVI), red edge NDVI, relative leaf water content
    (RWC), Renormalized Difference Vegetation Index (RDVI), and water index (WI) are
    used to monitor and analyze the stress and health of the crops. However, there
    was a finding that the chlorophyll content is present more in healthy or normal
    plants as compared to stressed plants [20], and there was a difference in the
    greenness of leaves of stressed and healthy plants in a collected dataset. This
    has motivated us to use CIG vegetative index for the identification of stressed
    plots. In past research, static and handheld imaging systems are used for analysis,
    and most of the experiments are conducted in a greenhouse or indoor environments.
    As Unmanned Ariel Vehicle (UAV) can carry different sensors and can cover a broad
    area, UAV based remote sensing can be useful to solve these problems and can help
    analysis in real field environments. The authors, in [21], have used UAV with
    an RGB camera for tassel detection and growth stage monitoring. In [22], a fixed-wing
    UAV with a multispectral camera has been used to identify vegetation on the ground.
    In [23], the green-red vegetation index (GRVI) has been used to estimate canopy
    coverage and to manage the irrigation. However, the health of the crop plays an
    essential role in managing irrigation. To address these issues, in this paper,
    the stress on the plant has been considered for optimizing irrigation. A method
    based on CIG vegetative index and UAV based remote sensing has been proposed.
    The proposed method uses near-infrared (NIR), green, and red band images to identify
    stressed areas in crop-field and helps in optimizing the irrigation process. To
    best of our knowledge, this is the first study to use CIG vegetation index to
    identify stress in crops. Fig. 1: (a) Top view of maize field. (b) Map of field
    experiment on maize crop-field. Show All The rest of this paper is organized as
    follows: Section II discusses data acquisition from crop-field, and it’s pre-processing.
    Section III describes the proposed method. The performance analysis of the proposed
    method has been discussed in section IV. The paper has been concluded in section
    V. SECTION II. Data Acquisition and Pre-Processing The experiment was conducted
    by agricultural scientists for maize crops at Agro Climate Research Center, Professor
    Jayashankar Telangana State Agriculture University (PJT-SAU), Hyderabad, India,
    during a season of 2018 − 19 as shown in Fig. 1(a). The maize crop was studied
    to monitor its growth (health, growth rate, and yield) in three different levels
    of treatments of water and nitrogen. The area of the experimental field was 10m
    × 10m which had 27 plots and includes: (i) the variation of three different levels
    of irrigation I1, I2, and I3, (ii) variation of three different nitrogen treatments
    N1, N2, and N3, and (iii) three replications of these plots R1, R2, and R3 as
    shown in the Fig. 1 (b). The fixed amount of water was applied to each plot when
    the ratio of irrigation water and cumulative pan evaporation (IW/CPE) arrives
    at pre-determined levels as per the treatments. To ensure the stressed environment,
    the crop was subjected to three irrigation (IW/CPE: I1 = 0.60, I2 = 0.8 and I3
    = 1.2) and three nitrogen levels (N1 = 100, N2 = 200, and N3 = 300 kg of nitrogen
    ha−1). Fig. 2: UAV with RGB and multispectral camera which is used for data collection
    Show All For our study, the images were acquired at 11 A.M. by flying UAV at the
    speed of 4 km/hr periodically at the height of 10m over the field. The DJI Inspire-1
    Pro UAV, shown in Fig. 2, was used for data collection from the field [24]. It
    was flown in an autopilot mode, and a flight path was set with 80% overlapping
    between two consecutive images. The data set is comprised of RGB and multispectral
    images, and the entire growth period from germination to the harvesting of maize
    crop has been covered. RGB images with a size of 4608 × 3456 pixels were captured
    by zenmuse X5 (type: CMOS, 16.0 megapixel, ISO range: 100 25600) camera [25].
    MicaSense RedEdge camera (MicaSense Inc., Seattle, WA, USA), [26], is used to
    capture multispectral images with a size of 1260 × 960 pixels, which consist of
    five 3.6MP, MP, 12-bit sensors with discrete and narrowband filters. It has 5
    bands: blue (475 nm), green (560 nm), red (668 nm), near-infrared (840 nm), and
    rededge (717 nm). The images were captured at every one and two seconds interval
    with a multispectral and RGB camera, respectively, during the entire flight of
    UAV. The first step in the UAV based remote sensing technique is to acquire a
    proper plot image for processing from the originally captured images with UAV.
    For this, an orthomosaic (which is a stitched and geometrically corrected panoramic
    view of an area covered by all raw images) of the entire field is created from
    the images acquired via UAV. Agisoft Photoscan software [27] is used to create
    an orthomosaic of the field. The orthomosaic is further segmented to obtain plot-wise
    images, as shown in Fig. 3(a), which are further used for the analysis of crop-health.
    For this study, only the net area of the plot, Fig. 3(c), which is the area after
    excluding the borders, is considered, as shown in Fig. 3 to avoid the boundary
    effect in the analysis. SECTION III. Proposed Method When light incidents on leaves,
    the stressed and non-stressed/healthy leaves behave differently for different
    wavelengths. When a plant is in stress condition due to lack of water, the plant
    tries to reduce the transpiration process. The plant reduces its surface area
    of leaves by twisting it in a spiral way, as shown in Fig. 4 and 5. If a plant
    is in stress, it’s canopy temperature is also high compared to a healthy or normal
    plant [28]. It is found that the chlorophyll content is present more in healthy
    or normal plants as compared to stressed plant [20]. There was a difference in
    the greenness of leaves of stressed and healthy plants in a collected dataset.
    These facts have motivated us to investigate multispectral images and Chlorophyll
    Index Green (CIG) vegetative index to identify the stressed areas in the field
    and to optimize the irrigation process. Fig. 3: (a) Orthomosaic. (b) Segmented
    plot of maize crop. (c) Net area of plot selected for health monitoring. Show
    All Fig. 4: RGB images of (a). Non-stressed area of a plot. (b). Stressed area
    of a plot. Show All The near infra-red (NIR), green, and red band images are used
    for this study. As discussed earlier, the chlorophyll content is found to be less
    in stressed plants compared to normal plants. The chlorophyll index green (CIG)
    vegetative index, [29], is used for identifying the stressed area in the field
    of maize crops. The first step in the proposed method is to segment leaves from
    the background (soil). OSAVI (Optimized Soil Adjusted Vegetative Index) [30],
    which can be estimated by Eq. (1), was used to separate background (soil) from
    NIR and green band images of the maize plots. The resultant images after removing
    soil are shown in Fig. 6 and 7. OSAVI= 1.16∗( ρ NIR − ρ Red ) ( ρ NIR − ρ Red
    +0.16) (1) View Source Fig. 5: near infra-red (NIR) band image of (a). Non-stressed
    area of a plot. (b). Stressed area of a plot. Show All Fig. 6: (a) Input image
    (NIR band) of normal plot. (b) Resultant image after soil or background removal.
    Show All Fig. 7: (a) Input image (NIR band) of stressed plot. (b) Resultant image
    after soil or background removal. Show All After removing the background (soil),
    the images were classified as stressed or normal. Chlorophyll index green (CIG)
    vegetative index was measured by processing images of NIR and green bands and
    following Eq. (2). CIG= ρ NIR ρ Green −1 (2) View Source The averaged value of
    CIG (Avg CIG) is used to classify plots as stressed or normal/healthy. The averaged
    value of CIG (Avg CIG) value of plots with ideal irrigation treatment I3 is used
    to distinguish between stressed and normal/healthy. The steps which are followed
    in the proposed method are summarized in algorithm 1. SECTION IV. Results and
    Discussion As discussed earlier in section II, the three irrigation and nitrogen
    treatments have been replicated three places R1, R2, and R3 on the field to nullify
    non-uniformity of other soil nutrients and soil structure. The irrigation level
    I3 refers to ideal irrigation treatment, I2 refers to medium irrigation-treatment,
    and irrigation I1 refers to deficit irrigation-treatment. The performance of proposed
    method was analysed to classify plots of I1 and I2 irrigation levels. The maize
    crop plots with ideal irrigation I3 is used as detection or classification boundary.
    After using the plots of I3 irrigation, which were 9 plots as boundary for classification,
    the total 18 plots of I1 and I2 irrigation levels are used for performance evaluation.
    The dataset of four dates 10 Dec 2018, 12 Dec 2018, 17 Dec 2018, and 20 Dec 2018
    were used for this analysis. In Fig. 9-12, RiNi refers to combination of repetitions
    and nitrogen levels as shown in Fig. 8 for first repetition. One of the decision
    rule can be the plots which is having Avg CIG higher compared to Avg CIG of I3
    irrigation plot is stressed otherwise plot is normal. The problem with this consideration
    was that the plot of I2 irrigation was misclassified as stressed. Therefore, in
    this analysis, the decision rule was the plot having highest value of Avg CIG
    is a stressed plot otherwise plot is normal. From Table I, it is clear that out
    of total 72 plots of four days only 1 plot on 12 Dec 2018 is misclassified as
    normal plot. Algorithm 1: Crop-Stress Monitoring The error in classification,
    as shown in Fig. 10 with red circle, occurred due to the presence of area with
    healthy leaves between the plots, as shown in Fig. 13 with red circle. From Fig.
    9-12 it is clear that the averaged value of CIG value is always high for stressed
    plots RiI1Ni compared to normal or non-stressed plots RiI2Ni. Therefore the error
    can be removed by changing the decision rule. Avg CIG values of stressed plots
    and normal plots can be compared and the plots having higher value of Avg CIG
    can be classified as stressed plot. The change in the value of Avg CIG of stressed
    plot occurred due to error in irrigation process during experiment as the averaged
    value of CIG value depends on stress level and stage of the crop. The error can
    also be addressed by considering small patches of plots or with pixel-wise classification.
    This can be useful to classify plots in different levels I1, I2, and I3 of irrigation
    also. SECTION V. Conclusion In this paper, we tried to address the problem of
    the optimal use of agronomic inputs for a crop-growth. The proposed method estimate
    the averaged chlorophyll index green (Avg CIG) to check the status of maize crop-health.
    It used images of NIR, green, and red bands, which were acquired from a multispectral
    camera. The performance analysis showed that the proposed method clearly classified
    stressed and normal plots of maize crop. It eases the process of monitoring the
    health of a crop and helps in optimizing the irrigation process. Also, it aids
    agricultural scientists in their analysis for developing new varieties of the
    crop which can sustain with the climatic changes and require less amount of agronomic
    inputs. In the future, we develop a pixel-wise classification method to classify
    small portions of plots and build a complete end-to-end system that can predict
    and helps farmers in scheduling the use of water and other agronomic inputs. TABLE
    I: The performance analysis of proposed method Fig. 8: Irrigation treatments with
    combination of nitrogen treatments on first replication R1. Show All Fig. 9: Averaged
    CIG values of different plots on 10 December 2018. Show All Fig. 10: Averaged
    CIG values of different plots on 12 Decem-ber 2018. Show All Fig. 11: Averaged
    CIG values of different plots on 17 Decem-ber 2018. Show All Fig. 12: Averaged
    CIG values of different plots on 20 Decem-ber 2018. Show All Fig. 13: Presence
    of normal leaves in the stressed plot. Show All ACKNOWLEDGMENT This work was supported
    and funded by the Department of Science and Technology (DST) India under the project
    \"Data Science-based Farming Support System For Sustainable Crop Production Under
    Climatic Changes(DSFS)\" project no: MST/IBCD/EE/F066/2016-17G48. Authors Figures
    References Citations Keywords Metrics Footnotes More Like This Fast Target Detection
    in Synthetic Aperture Sonar Imagery: A New Algorithm and Large-Scale Performance
    Analysis IEEE Journal of Oceanic Engineering Published: 2015 Challenges in Seafloor
    Imaging and Mapping With Synthetic Aperture Sonar IEEE Transactions on Geoscience
    and Remote Sensing Published: 2011 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2020 IEEE Sensors Applications Symposium, SAS 2020 - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: CIG based Stress Identification Method for Maize Crop using UAV based Remote
    Sensing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rao A.J.
  - Bekal C.
  - Manoj Y.R.
  - Rakshitha R.
  - Poornima N.
  citation_count: '0'
  description: Water wastage in agricultural fields has been one of the major issues
    in various countries especially in India. Hence it is very important to reduce
    water loss in different situations due to various factors like pipe leakage or
    leaving excess water into the farms without knowing. This paper provides various
    insights on the comparison of different methods to reduce water loss using various
    machine learning techniques. Diseases in crops, reduces the quality of each product
    and the quantity of agricultural product. Thus we require image processing techniques,
    as it will help in accurate and timely detection of diseases and helps in reducing
    the errors of humans. Production of crops can be increased by detecting the disease
    well in time. Automatic detection of plant sickness helps in analyzing the crop
    and robotically detects the sign of the alignments as soon as they appear on plant
    leaves in order to prevent the loss of crops.
  doi: 10.1007/978-3-030-43192-1_65
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Proceeding of the International
    Conference on Computer Networks, Big Data and IoT (ICCBI - 2019) Conference paper
    Smart Irrigation and Crop Disease Detection Using Machine Learning – A Survey
    Conference paper First Online: 05 March 2020 pp 575–581 Cite this conference paper
    Access provided by University of Nebraska-Lincoln Download book PDF Download book
    EPUB Proceeding of the International Conference on Computer Networks, Big Data
    and IoT (ICCBI - 2019) (ICCBI 2019) Anushree Janardhan Rao, Chaithra Bekal, Y.
    R. Manoj, R. Rakshitha & N. Poornima  Part of the book series: Lecture Notes on
    Data Engineering and Communications Technologies ((LNDECT,volume 49)) Included
    in the following conference series: International conference on Computer Networks,
    Big data and IoT 1272 Accesses 1 Citations Abstract Water wastage in agricultural
    fields has been one of the major issues in various countries especially in India.
    Hence it is very important to reduce water loss in different situations due to
    various factors like pipe leakage or leaving excess water into the farms without
    knowing. This paper provides various insights on the comparison of different methods
    to reduce water loss using various machine learning techniques. Diseases in crops,
    reduces the quality of each product and the quantity of agricultural product.
    Thus we require image processing techniques, as it will help in accurate and timely
    detection of diseases and helps in reducing the errors of humans. Production of
    crops can be increased by detecting the disease well in time. Automatic detection
    of plant sickness helps in analyzing the crop and robotically detects the sign
    of the alignments as soon as they appear on plant leaves in order to prevent the
    loss of crops. Keywords Smart irrigation Crop diseases Crop loss Machine learning
    Image processing Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF Similar content being viewed by others Artificial Intelligence in Agriculture:
    Machine Learning Based Early Detection of Insects and Diseases with Environment
    and Substance Monitoring Using IoT Chapter © 2023 Automatic Smart Irrigation Method
    for Agriculture Data Chapter © 2023 Automated Disease Detection and Classification
    of Plants Using Image Processing Approaches: A Review Chapter © 2021 1 Introduction
    Earth has 99% water in sea which is salty and is not suitable for human use. Just
    1% is freshwater and groundwater and can be used for human consumption. Shrinking
    of water reservoir, low rainfall, etc. will lead to problems in future. There
    will be not enough water resources to provide for a huge growing population in
    India [1,2,3]. So it is necessary to save water and adopt mechanisms like smart
    irrigation and smart farming to reduce the water loss in agricultural fields.
    There are various research scholars who have performed experiments on various
    soil types using various instruments like sensors, watermark, tensiometer, etc.
    [4,5,6,7,8]. These studies have revealed that a lot of water loss can be reduced
    by using Machine learning and Artificial Intelligence technologies. This paper
    provides the review on the methods of reducing water loss and how accurate each
    method is and what future work is needed to improve the existing system [9,10,11,12].
    India is a developing country. In developing country economic growth plays a vital
    role. For economic growth not only the Industrial contribution is important but
    also agriculture contribution is important and 70% of our population is depended
    on agriculture [13,14,15,16]. Crop diseases are affecting agriculture. This may
    lead to the reduction of quality and quantity of crops. Crop diseases are caused
    by microorganisms. Hence farmers cannot see the symptoms of disease on leaf by
    just looking at it. To find the disease and its measure one can use computerized
    technique followed by various methods to detects the disease. The main part of
    the plant to look for the sickness is its leaf. The diseases on leaf will cut
    back amount of crops and their growth. The simple methodology to find the plant
    diseases is with assistance of agricultural knowledge having data of plant diseases.
    However, this is manual detection of plant ailments that takes a great deal of
    your time and could be a backbreaking work. Hence, there a necessity for desktop
    gaining data of approach to become tuned into the leaf diseases. Systems will
    play a main role to develop the processed ways for the detection and classification.
    There are often variety pattern attention and movie process techniques which are
    employed in the leaf disorder detection. The plant disease detections and classifications
    of leaf diseases is the key to prevent the agricultural loss. 2 Literature Review
    In [1] the author discussed about the water scarcity problem in India and what
    are the various methods to reduce water loss. They have stated various ways in
    which water is lost through leakage and excess water that is left into farms.
    The problems created due to water loss is discussed, few of them being food stress,
    GDP problem, Energy Problem and increased carbon foot print etc. The measures
    taken at present by the Indian government and various technologies like smart
    farming, smart water system are discussed. The advantages and disadvantages of
    measures taken across various countries in the world are given in this paper and
    how it can improvise and be inculcated in India to prevent water scarcity. This
    review paper in overall gives the necessary information on present situation about
    water loss and how it can be prevented in future. In the paper [2], the authors
    concentrate on conserving water in arid regions. Intelligent irrigation system
    (IIS) is the method used to determine the crop water requirements based on the
    climatic conditions. They have taken two crops wheat and tomato into their study
    and made use of drip and sprinkle irrigation methods. The comparisons on how much
    water is necessary for both the crops was done using the hunter ET system which
    is the evapotranspiration method. The moisture content in soil is observed using
    the sensors like watermarks, tensiometers and Enviroscan. The Operation time was
    determined and results of soil analysis and water application for both tomato
    and wheat was obtained and graphs plotted. All of the technologies tested (IIS)
    managed to reduce water application resulted in water savings ranging from 18
    to 27%. In [3] the authors discussed regarding the evaluation of accuracy of soil
    water sensors for irrigation scheduling to conserve freshwater in which they have
    used low cost soil water sensors like ECH2O-5TE, Watermark 200SS and Tensiometer
    model R to determine their accuracies. They have conducted site study in a mature
    pecan field, located in the south El Paso in Texas, USA. This was followed by
    soil sampling and analysis. Considering sensors accuracy and soil water sensors
    the results of all the soil samples for various sensors were compared and graphs
    plotted for them. Tensiometer provided relatively more accurate soil water data
    compared to the other two sensors. In [4], they concentrate on optimizing the
    use of water for agricultural crops. The method that they have used consists of
    a system that has wireless distributed network of soil moisture and temperature
    sensors that was fixed in the roots of plants. They have considered different
    methods with various parameters and results. This method was tested in a greenhouse
    with organic sage as its produce. The automated irrigation was triggered immediately
    when the soil moisture value fell below the threshold value and similarly for
    soil temperature when the temperature was above the threshold value. Hence the
    automated irrigation system proves that the water can be used effectively for
    a fresh biomass production. In [5], the author stated that due to high increase
    in the demand for freshwater in the agricultural area, fresh water should be used
    effectively for irrigation purposes. The system that they used consists of a sensor
    network which is wireless for wireless controlled irrigation solution at low-cost
    and analyzing the water content of the soil. This system was implemented and tested
    in an area of 8 acres located in central Anatolia for controlling drip irrigation
    of dwarf cherry trees. The main advantage of this system is that it prevents moisture
    stress and salification. In [6], the author concentrates on efficient management
    of water in cropping areas. This paper stresses on site-specific irrigation management
    that increases their productivity and saves water. This method consists of in-field
    sensors based on site-specific irrigation which takes soil moisture, soil temperature
    and air temperature as parameters. This method had the capacity to increase the
    yield and the quality of the crops while optimizing the use of water. In [7],
    the researchers, explained the method to prevent the loss of crops in cotton leaf
    by detecting the symptoms. In cotton, the diseases show up in leaf, so the area
    of interest is leaf, as most of the diseases appears on the leaves itself. In
    cotton there are common diseases like Red Leaf Spot, Alternaria Leaf Spot and
    Cercospra Leaf Spot. These disease can be easily detected using k-means clustering
    algorithm. It classifies objects. Segmentation is done based on a set of features
    and then the image is partitioned into number of classes and finally disease can
    be detected using neural network. In [8] the author described an approach to detect
    the crop disease in large farms agriculture for instance rice. It is based on
    automated technique. Fungi are identified primarily, then the bacteria is considered
    by capturing the image of two leaves that is one of healthy and another is unhealthy
    and thus the disease is detected. The RGB image that was captured is converted
    to grey image and then grey image is resized and performs canny edge detection.
    In [9] the authors described the approach that consists of various steps. Firstly,
    the green color pixels are recognized. Then based on specific threshold values
    green pixels are covered. RGB values with zero and disease occurred leaf boundaries
    are removed. This step is important in classifying the diseases. These methods
    are used to acquire the necessary features for analysis. This technique has high
    accuracy in detecting the plant disease. In [10], the author proposed a technique
    that can be applied to different yields like orange, citrus, wheat, corn and maize
    and so forth. Fluffy framework for leaf sickness, recognition and reviewing, K-means
    implies bunching procedure that has been utilized for division, which gathers
    comparable pixels of a picture. RGB shading space is changed over to L * a * b
    space, where L is the radiance and a * b are the shading space. In [11], Picture
    handling based strategy for evaluating the leaf spot alignment in plant leaves.
    They played out an examination on all the impacting factors that were available
    during the time spent division. Otsu Technique was utilized to section the leaf
    areas. In [12], the author proposed a way to deal with recognition and grouping
    the illness in the sunflower harvest utilizing picture preparing. The exploration
    was completed utilizing the leaf pictures of the yield that were taken utilizing
    a high-goals advanced camera. 3 Comparison of Different Detection Method for Smart
    Irrigation and Detection of Crop Diseases Table 1, gives us the attractive idea
    of the detection techniques used by various authors in the field of smart irrigation
    and detection of crop diseases. It also gives the list of recommendations which
    we thought, could have been implemented in the system in future. Table 1. Comparison
    of different detection methods for abusive text. Full size table 4 Conclusion
    The survey of the different papers studied have given special identification and
    classification techniques which have been summarized above. Each paper has its
    own different methods, advantages and disadvantages, by combining various methods
    one can achieve better results. As per the survey, we have analyzed that the k-means
    method has the highest accuracy and it can be used with the aid of researchers
    for ailment identification and classification of plants. These computing device
    learning methods help agricultural specialists in detection of disorder in the
    plant in well-timed fashion, then the professionals will suggest the drugs to
    the farmer. As per pointers of agricultural experts, the farmer will supply the
    therapy for the diseased plant in a well-timed manner which will amplify the crop
    yield. We can develop a system that will include inputs of various plant leaves
    and add the best suited algorithm for more efficiency and derive the results.
    It also has the review and comparison of various papers on smart irrigation and
    how can we reduce the amount of water lost unnecessarily in agricultural fields.
    These comparisons will help in enhancing the existing system and derive a new
    model to achieve the objective. We can design a machine learning system that will
    take the input data about the surrounding from these hardware devices and then
    decide the amount of water to be left to the fields. References Gupta, A., Mishra,
    S., Bokde, N., Kulat, K.: Need of smart water systems in India. Int. J. Appl.
    Eng. Res. 11(4), 2216–2223 (2006) Google Scholar   Al-Ghobari, H.M., Mohammad,
    F.S.: Intelligent irrigation performance: evaluation and quantifying its ability
    for conserving water in arid region. Appl Water Sci. 1, 73–83 (2011) Article   Google
    Scholar   Ganjegunte, G.K., Sheng, Z., Clark, J.A.: Evaluating the accuracy of
    soil water sensors for irrigation scheduling to conserve freshwater. Appl. Water
    Sci. 2, 119–125 (2012). Smith, B.: An approach to graphs of linear forms (Unpublished
    work style) (unpublished) Google Scholar   Gutiérrez, J., Medina, J.F.V., Garibay,
    A.N., Gándara, M.A.P.: Automated irrigation system using a wireless sensor network
    and GPRS module. IEEE Trans. Instrum. Meas. 63(1), 1–11 (2014) Article   Google
    Scholar   Dursun, M., Ozden, S.: A wireless application of drip irrigation automation
    supported by soil moisture sensors. Sci. Res. Essays 6(7), 1573–1582 (2011) Google
    Scholar   Kim, Y.J., Evans, R.G., Iversen, W.M.: Remote sensing and control of
    an irrigation system using a distributed wireless sensor network. IEEE Trans.
    Instrum. Meas. 57(7), 13791387 (2008) Google Scholar   Warne, P.P., Ganorkar,
    S.R.: Detection of diseases on cotton leaves using K-mean clustering method. Int.
    Res. J. Eng. Technol. (IRJET) 02(04), 425–431 (2015) Google Scholar   Shergill,
    D., Rana, A., Singh, H.: Extraction of rice disease using image processing. Int.
    J. Eng. Sci. Res. Technol. 1, 135–143 (2015) Google Scholar   Naikwadi, S., Amoda,
    N.: Advances in image processing for detection of plant diseases. Int. J. Appl.
    Innov. Eng. Manag. (IJAIEM) 2(11), 168–175 (2013) Google Scholar   Kamlapurkar,
    S.R.: Detection of plant leaf disease using image processing approach. Int. J.
    Sci. Res. Publ. 6, 73–76 (2016). e-ISSN 2250-3153 Google Scholar   Rani, M., Kaur,
    R.: Machine learning algorithms for disease classification in crop and plants.
    Int. J. Eng. Sci. Res. Technol. 4(08), 976–981 (2018). e-ISSN 2455-2585 Google
    Scholar   Kambale, G.: Crop disease identification and classification using pattern
    recognition and digital image processing techniques. Professor of CSE MME Collage
    in India (2007). P-ISSN 2278-8727 Google Scholar   Jha, K., Doshi, A., Patel,
    P.: Intelligent irrigation system using artificial intelligence and machine learning:
    a comprehensive review. Int. J. Adv. Res. (IJAR) 6(10), 1493–1502 (2018) Article   Google
    Scholar   Aitkenhead, M.J., Dalgetty, I.A., Mullins, C.E., McDonald, A.J.S., Strachan,
    N.J.C.: Weed and crop discrimination using image analysis and artificial intelligence
    methods. Comput. Electron. Agric. 39(3), 157–171 (2003) Article   Google Scholar   Raj,
    J.S., Vijitha Ananthi, J.: Automation using IoT in greenhouse environment. J.
    Inf. Technol. 1(01), 38–47 (2019) Google Scholar   Encinas, C., Ruiz, E., Cortez,
    J., Espinoza, A.: Design and implementation of a distributed IoT system for the
    monitoring of water quality in aquaculture. In: 2017 Wireless Telecommunications
    Symposium (WTS), pp. 1–7 (2017) Google Scholar   Download references Author information
    Authors and Affiliations Department of Computer Science and Engineering, Vidyavardhaka
    College of Engineering, Mysuru, Karnataka, India Anushree Janardhan Rao, Chaithra
    Bekal, Y. R. Manoj, R. Rakshitha & N. Poornima Corresponding author Correspondence
    to Anushree Janardhan Rao . Editor information Editors and Affiliations Department
    of CSE, Vaigai College of Engineering, Melur, Tamil Nadu, India A. Pasumpon Pandian
    Department of Business Administration, The Gerald Schwartz School of Business,
    StFX University, Antigonish, NS, Canada Ram Palanisamy Electrical and Computer
    Engineering, University of Applied Sciences, Egaleo, Attiki, Greece Klimis Ntalianis
    Rights and permissions Reprints and permissions Copyright information © 2020 Springer
    Nature Switzerland AG About this paper Cite this paper Rao, A.J., Bekal, C., Manoj,
    Y.R., Rakshitha, R., Poornima, N. (2020). Smart Irrigation and Crop Disease Detection
    Using Machine Learning – A Survey. In: Pandian, A., Palanisamy, R., Ntalianis,
    K. (eds) Proceeding of the International Conference on Computer Networks, Big
    Data and IoT (ICCBI - 2019). ICCBI 2019. Lecture Notes on Data Engineering and
    Communications Technologies, vol 49. Springer, Cham. https://doi.org/10.1007/978-3-030-43192-1_65
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-030-43192-1_65
    Published 05 March 2020 Publisher Name Springer, Cham Print ISBN 978-3-030-43191-4
    Online ISBN 978-3-030-43192-1 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Sections References Abstract Introduction Literature Review Comparison
    of Different Detection Method for Smart Irrigation and Detection of Crop Diseases
    Conclusion References Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes on Data Engineering and Communications Technologies
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Smart Irrigation and Crop Disease Detection Using Machine Learning – A Survey
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nasir R.
  - Khan M.J.
  - Arshad M.
  - Khurshid K.
  citation_count: '3'
  description: 'With the advancements in precision farming, crop sensing is gaining
    importance for timely crop health management. Leaf water content (LWC) is key
    component to determine vegetation health and nourishment. Timely estimation of
    LWC could save us from hazardous damage by pre-planning: drought stress on plants,
    irrigation and prediction of woodland fire. The retrieval of LWC from visible
    to shortwave infrared (VSWIR: 0.39 to 2.5 μm) mid- and thermal-infrared (MIR and
    TIR: 2.50 to 14.0 μm) windows of electromagnetic spectrum has been investigated
    using different statistical algorithms. Deep learning is modernizing the fast
    growing field of machine learning and image processing. The convolutional neural
    network (CNN) is ultramodern technique of deep learning that learns and extracts
    features directly from data. This research is focused on the extraction of different
    features of different plant species by using CNN for Regression. The modeled CNN
    architecture automatically detects prominent features to estimate LWC in plant
    species from its reflectance spectra, recorded for varying amount of LWC. Previous
    methods applied on same dataset yielded accuracy of 93% and Root Mean Square Error
    (RMSE) of 7.1, however, CNN resulted in better and swift results with an accuracy
    of 98.4% and RMSE of 4.183. This study helps in identifying the important spectral
    regions for quantifying water stresses in vegetation. The outcomes of this study
    can enable the future space missions to foresee water content of different plant
    species on the basis of their spectral signatures for illustrating vegetation
    stresses.'
  doi: 10.1109/INTELLECT47034.2019.8954985
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 Second International Con...
    Convolutional Neural Network based Regression for Leaf Water Content Estimation
    Publisher: IEEE Cite This PDF Rida Nasir; Muhammad Jaleed Khan; Muhammad Arshad;
    Khurram Khurshid All Authors 2 Cites in Papers 155 Full Text Views Abstract Document
    Sections I. Introduction II. Related Work III. Proposed LWC Estimation Method
    IV. Experimental Results V. Conclusion Authors Figures References Citations Keywords
    Metrics Abstract: With the advancements in precision farming, crop sensing is
    gaining importance for timely crop health management. Leaf water content (LWC)
    is key component to determine vegetation health and nourishment. Timely estimation
    of LWC could save us from hazardous damage by pre-planning: drought stress on
    plants, irrigation and prediction of woodland fire. The retrieval of LWC from
    visible to shortwave infrared (VSWIR: 0.39 to 2.5 μm) mid- and thermal-infrared
    (MIR and TIR: 2.50 to 14.0 μm) windows of electromagnetic spectrum has been investigated
    using different statistical algorithms. Deep learning is modernizing the fast
    growing field of machine learning and image processing. The convolutional neural
    network (CNN) is ultramodern technique of deep learning that learns and extracts
    features directly from data. This research is focused on the extraction of different
    features of different plant species by using CNN for Regression. The modeled CNN
    architecture automatically detects prominent features to estimate LWC in plant
    species from its reflectance spectra, recorded for varying amount of LWC. Previous
    methods applied on same dataset yielded accuracy of 93% and Root Mean Square Error
    (RMSE) of 7.1, however, CNN resulted in better and swift results with an accuracy
    of 98.4% and RMSE of 4.183. This study helps in identifying the important spectral
    regions for quantifying water stresses in vegetation. The outcomes of this study
    can enable the future space missions to foresee water content of different plant
    species on the basis of their spectral signatures for illustrating vegetation
    stresses. Published in: 2019 Second International Conference on Latest trends
    in Electrical Engineering and Computing Technologies (INTELLECT) Date of Conference:
    13-14 November 2019 Date Added to IEEE Xplore: 13 January 2020 ISBN Information:
    DOI: 10.1109/INTELLECT47034.2019.8954985 Publisher: IEEE Conference Location:
    Karachi, Pakistan SECTION I. Introduction Leaf water content (LWC) is considered
    a key element in accessing the biophysical and biochemical attributes of vegetation.
    With the advancements in optical technology, optical remote sensing is non-invasive
    approach to investigate large amount of target area. Modern optical sensors are
    now capable of detecting spectral responses of objects over wide range of electromagnetic
    spectrum. [1]. Due to unique nature of every material, every object has unique
    spectral signatures [2]. Multispectral imaging technique has been used for different
    remote sensing, environmental and for land observation since 1960s [3]. However
    less number of spectral bands were acquired over the electromagnetic spectrum
    which was a major drawback. Hence, this lead to newer technologies in hyperspectral
    imaging (HSI), which could record spectral response of target area over hundreds
    and thousands of narrow bands. HSI is becoming popular with the diverse innovations
    being made and becoming a significant tool for many useful applications. The introduction
    of hyperspectral imaging paved for analysis of an object or scene with more spectral
    resolution and more spectral information [4]. Crop sensing is used to improve
    farming quality and help increase productivity of crops. With the increase of
    population, proper nourishment for plants and their growth is necessary to produce
    quantitative and qualitative yield. Further studies have made researchers to develop
    better sensors to match crops to different soils and weather conditions [14].
    For this purpose betterment in vegetation techniques have been ongoing since three
    decades. LWC is major component of plant health, for site specific on-time vegetation
    treatment, forest fire and drought forecast. Similarly, LWC is also vital for
    photosynthesis and productivity of plants. It has widespread applications in hydrology,
    agriculture and forestry. Deep learning is a subset of machine learning that makes
    computers to adapt by an example. The Convolution Neural Network (CNN) is a deep
    learning architecture which extracts major features from data automatically and
    uses its ability to adopt the training pattern to recognize objects. CNN is computationally
    a faster method to classify data especially due the availability of pre-trained
    networks. Remote sensing has made it easier monitor vegetation parameters for
    a large area, which is otherwise labor intensive and more prone to errors. For
    this we have proposed to estimate LWC in a group of plant species to by using
    deep learning method of CNN for regression. Regression techniques are widely employed
    to solve tasks where the goal is to predict continuous values. In computer vision,
    regression techniques span a large collaboration of circumstances such as: age
    estimation [21], facial landmark detection [22] head-pose estimation [23] or human
    pose estimation [24] [25] or image registration [26]. Remotely sensed spectral
    signatures fluctuate with variation in vegetation biophysical/biochemical traits
    and exhibit strong relationship between vegetation parameters and their reflectance
    signatures as shown in Figure 1. In this paper, a CNN-based novel method for prediction
    of LWC using spectral responses of leaves in proposed. The suitable CNN architecture
    for prediction of LWC is determined by experimenting different architectures with
    different number of layers with random search for suitable filter size in convolutional
    layers. The proposed optimum architecture contains four (4) convolutional 2d layers
    each with a filter size of 3 × 3. The proposed technique achieved the highest
    results (accuracy of 98.4% and RMSE of 4.183) amongst the previous techniques
    of LWC estimation on this dataset [20]. The constraints in this research include
    less amount of data with similar fluctuating values which was computationally
    difficult, nevertheless, the proposed CNN based regression model has proven its
    capability to efficiently estimate correct amount of water content to a certain
    limit. Fig. 1. Different plant species and their spectral responses. Source: [20]
    Show All Fig. 2. R2 and rmsecv against the number of variables. Source: [20] Show
    All SECTION II. Related Work HSI is used to deduce spectrum for very minute and
    pixel level details in a scene. It collects and processes information over wide
    range of electromagnetic spectrum. Large amounts of research has developed a better
    understanding of hyperspectral data which eventually turned into using remote
    sensing applications [5]. HSI has been widely used in rremote sensing, which acquires
    information about any object or different areas from a distance without any physical
    contact with the object. Remote sensing has been applied to numerous applications
    including mineralogical mapping of earth surface [6], weather forecasting [7]
    Laser and radar altimeters [8] military defense, art conservation and archeology
    [9], [10] aerial photographs [11], agricultural and water resources control [12].
    Among these is an important application of remote sensing which has been given
    importance which is known ad crop sensing [13]. Remote sensing is helpful for
    the revealing of vegetation water content as it could be an indication of fire
    happening hazards and burning at local or global scales [34]–[35]. Many practices
    have been used to forecast LWC for the purpose of on time calculation of plants
    health and timely water requirement. Areas prone to fire hazards need more attention
    for risk analysis and mitigation [16]. Most of the best yield of crops die from
    being unattended, further leading to scarcity of water in that specific areas.
    Furthermore, it leads to loss of hundreds to thousands of plants. Lately numerous
    techniques have been researched upon for estimation of leaf water content across
    the whole electromagnetic spectrum. Some of the methods use reflectance, continuous
    wavelet analysis [17], optical methods [18], spectral indices [19] and Genetic
    Algorithm (GA) coupled with Partial Least square Regression (PLSR) [20]. GA coupled
    with PLSR has been used to predict the leaf water content from mid to thermal
    infrared spectrum. GA works on the principle of survival of the fittest and it
    summarizes results to a problem in the form of strings/chromosomes. There are
    many limitations of GA varying from population size to mutation and crossover
    but GA is dependent on all these factors. Many other factors such as chromosome
    size also affects the throughput of GA. Steps such as initial population, assessment,
    selection, crossover, mutation and next generation population take place in the
    evolution process of GA. On the other hand, PLSR is a vigorous multivariate linear
    statistical analysis suitable for developing regression models from data containing
    adequate number of variables. PSLR extracts useful information by creating a correlation
    between several independent variables. PLSR is used as objective function and
    the accuracy is retrieved in the form of adjusted-R”, The accuracy of final models
    is determined in terms of R2-adjusted and root mean square error cross validation
    (RMSECV). The same dataset is used of 11 different species, a total of 402 images,
    50% are used for training and 50% for testing. Each Image is made up of 3457 bands
    and thus an accuracy of 96% is achieved and lowest is RMSECV of 6.6%. The values
    of R2 and RMSEcv are plotted against the number of variables in Figure 2. Other
    researchers have analyzed plant leaves spectral responses using deep neural networks
    such as Artificial Neural Networks (ANNs) [28] self-organizing map (SOM) neural
    network [29] and [30] ARTMAP neural network to enhance vegetation parameters estimation.
    However, these techniques are more computationally complex and causes overfitting.
    The proposed research is to address these problems and develop more robust model
    for estimation of LWC. Table I. Summary of leaf samples collected, dehydration
    stages and total number of spectra recorded per specie SECTION III. Proposed LWC
    Estimation Method A. Database and Pre-Processing Reflectance values of each leaf
    at varying amount of water content, is calculated over visible to shortwave infrared
    (VSWIR: 0.39 to 2.5 µm) & mid- and thermal-infrared (MIR and TIR: 2.50 to 14.0
    µm) windows using spectroradiometer As mentioned in Table I, there are 9 major
    species of leaves and each class is further subdivided into different stages depending
    on variation in water contnt. Water content is measured using weight balance and
    spectra is recorded after every four hour air drying stage of each leaf. This
    process is repeated untill the leaf is completely dried and no water content remains
    in the leaf as shown in Table 1. Fig. 3. Spectral reflectance data of images from
    class 1–4 Show All Fig. 4. Working of a convolutional filter with stride = 2 Show
    All B. Spectral Data Organization In order to process spectral responses in CNN,
    we need to format the spectral responses in image form [31]. The spectral response
    of each sample of plant species are 1×2111 in the case of visible to shortwave
    infrared (VSWIR: 0.39 to 2.5 µm) & 1×3457 in the case of mid- and thermal-infrared
    (MIR and TIR: 2.50 to 14.0 µm). We reshape them to 46×46 matrix by appending 5
    zeros and 59×59 matrix by appending 24 zeroes respectively as depicted in Figure
    3. C. CNN Based Regression In this research, we have used a CNN based regression
    model. The CNN has an input layer size of 46×46 pixels for 393 leaves from visible
    to shortwave infrared (VSWIR: 0.39 to 2.5 µm) and 59×59 pixels for 393 for mid-
    and thermal-infrared (MIR and TIR: 2.50 to 14.0 µm) windows of electromagnetic
    spectrum. The set of image are reshaped and appended with zeros for conversion
    to proper size ratio. The convolutional layers consist of a group of adaptive
    filters that slide over the input image along height and width of input given
    to compute the dot product of the weights and mapping of the previous layer. In
    this way the spectral signatures of leaves are trained and extraction of features
    are carried out for a set of leaf species to determine the variations and to distinguish
    characteristics in a set of leaves belonging to same species and that of a wet
    or dried leaf Stride takes control of how filter behaves or convolves around the
    given input. Stride is usually set so that the output value is a whole number
    rather a faction. In this scenario, we have set a stride of 2 which gave us best
    accuracy results as shown in Figure 4. As the input goes into the deep neural
    network, weights and other parameters makes the input values distorted. In this
    regard, batch normalization layer normalizes the data cross each mini-batch. This
    layer helps in increasing accuracy and speed of the network and also in reducing
    overfitting [32]. After every convolutional layer, a Rectified Linear Unit (ReLU)
    layer is added to introduce non-linearity. Without this layer the whole CNN architecture
    would be just matrix multiplications and convolutions and the output would just
    be linear classifier. It performs a threshold operation i.e. if any value is less
    than zero is would make it equal to zero. The pooling layers perform down sampling
    and reduce overfitting. The reason why we have used Average pooling layer over
    Max pooling is that Average pooling retains a major portion of the data and smoothens
    the features extracted whereas Max pooling only keeps the prominent features such
    as edges and discards rest of the data. For segregation of such spectral response
    of leaves from the data of the same species under different stages was a major
    challenge as there was similarity in their spectral signatures, Hence, the features
    extracted needed to be smoothed out for preserving maximum details. Dropout layer
    is mainly used in convolutional neural networks to avoid overfitting [33]. It
    randomly discards an input value and gives it a value of zero. It basically adjusts
    a value by adding noise to unknown or hidden parts of the CNN architecture. In
    the fully connected layer, the neurons are connected actually do the distinctive
    learning in the whole neural network. It takes neurons from the previous layer
    and connects to every single neuron it has. The regression layer predicts a continuous
    set of values, such as LWC in this research. After prediction, the root mean square
    error (RMSE) is calculated to assess the performance of the model. Six different
    CNN architectures, presented in Table II, are experimented in order to select
    the most suitable architecture for LWC estimation. Table II. Comparison of six
    cnn architectures for lwc estimation SECTION IV. Experimental Results MATLAB 2018b
    with was used for experimentation in this research with 8GB RAM, Intel 2.3 GHz
    and 64 bit processor. The CNN architectures shown in the Table II were trained
    with spectral signatures and water content of the leaves of different species.
    There were nine different plant species out of which 90 percent was used for training
    and 10 for testing as shown in Table I. Accuracy is computed by dividing the total
    of correctly characterized pixels by the total number of pixels in the image.
    The accuracy of each class is mentioned in the given chart. The image size of
    each plant leaf used was 59×59 and hence filter size of 3×3 was used to deduce
    best accuracy rates. Also it was noticed that gradual increase in number of filters
    lead to better results. CNN-4, presented in Figure 5, gave best results among
    other architectures with an overall accuracy of 98.4% as shown in Table II. While
    training accuracy, Root Mean Square Error (RMSE) plots were also extracted as
    shown in Figure 6. The Optimum architecture used starts to converge at 250th iteration
    but converges quickest than other architectures. It took six and a half minutes
    to train 1050 iterations with 30 epochs. A validation RMSE of 4.12 was achieved
    with validation frequency of 10. It was observed that smaller filter sizes of
    3×3 gave better results as compared to 5×5 and 7×7 sized filters. However, CNN
    proposed is pertinent to all leave plants to predict and calculate water content
    based on regression method of CNN. Any other similar set of data of plant species
    can be trained and tested under same conditions to give such accuracy which can
    be quite useful for vegetation stress studies. Fig. 5. CNN-4 block diagram Show
    All Fig. 6. CNN training RMSE and loss plots Show All SECTION V. Conclusion HSI
    has been used to increase and maintain the crop yields and managing water resources.
    Water deficiency in plants is considered an important indicator of low productivity,
    forest fire and drought forecast. Precise estimation of water content in plant
    leaf is crucial because the quantity of water content in vegetation highlight
    the stress status of vegetation. Remote sensing allows the accurate estimation
    of stress level at local, regional and global scales. To date, using remote sensing
    for predicting vegetation water status, most of the researchers have focused on
    Visible, NIR and SWIR regions of electromagnetic spectrum. Less research has been
    done in MIR and TIR for quantifying vegetation traits specially LWC. To the best
    of our knowledge, this study is first of its type in which CNN based regression
    has been employed to precisely estimate LWC from MIR and TIR spectra. The finding
    of this study suggests that there is a relationship between LWC and Visible to
    MIR-TIR spectra. The modeled CNN architecture automatically detects prominent
    features to estimate LWC in certain type of plant species from a dataset having
    spectra of 9 different plant species. Previous methods applied on same dataset
    yielded accuracy of 93% and Root Mean Square Error (RMSE) of 7.1, however, CNN
    resulted in better and swift results with an accuracy of 98.4% and RMSE of 4.183.
    Authors Figures References Citations Keywords Metrics More Like This Palm Trees
    Counting in Remote Sensing Imagery Using Regression Convolutional Neural Network
    IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
    Published: 2018 A Novel Deep Learning Framework by Combination of Subspace-Based
    Feature Extraction and Convolutional Neural Networks for Hyperspectral Images
    Classification IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing
    Symposium Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2019 2nd International Conference on Latest Trends in Electrical Engineering
    and Computing Technologies, INTELLECT 2019
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Convolutional neural network based regression for leaf water content estimation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gobhinath S.
  - Devi Darshini M.
  - Durga K.
  - Hari Priyanga R.
  citation_count: '7'
  description: Automation has maneuvered industrial advancements and has become provident
    in most of the domain. But the contribution of automation to agriculture is in
    a lower degree. So, by incorporating automation in the field of agriculture, productivity
    can be increased to manifolds. This paper brings out the ways to automate agriculture
    which enhances irrigation, protection of farmlands and health management. Irrigation
    of crops is made economical by considering the moisture level in soil, temperature
    and humidity. Health supervision is done by an autonomous agricultural rover which
    moves around the field, collecting data through a camera fixed on it. The images
    are processed using algorithms in MATLAB to identify the disease affecting or
    nutrition deficit in the field. The shortcoming is indicated to the farmer. In
    case of fire accidents, it is detected using Ultraviolet Flame sensor and the
    fire is put off. Also, the farm is protected from animal intrusion using PIR sensor
    and buzzer. These techniques are integrated to improve the standards of agricultural
    farming.
  doi: 10.1109/ICACCS.2019.8728468
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: '>'
  journal: 2019 5th International Conference on Advanced Computing and Communication
    Systems, ICACCS 2019
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Smart Irrigation with Field Protection and Crop Health Monitoring system
    using Autonomous Rover
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
