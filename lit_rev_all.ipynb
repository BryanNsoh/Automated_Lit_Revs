{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The JSON outline of the research outline follows this structure:\n",
    "\n",
    "{ \n",
    "   \"title\": \"1. Introduction\",\n",
    "   \"subsections\": [\n",
    "       {\n",
    "           \"title\": \"1.1 The Global Food Challenge and the Role of Irrigation\",\n",
    "           \"point1\": {\n",
    "               \"text\": \"...\",\n",
    "               \"query1\": \"...\",\n",
    "               \"query2\": \"...\",\n",
    "               ...\n",
    "           },\n",
    "           \"point2\": {\n",
    "               \"text\": \"...\", \n",
    "               \"query1\": \"...\",\n",
    "               \"query2\": \"...\",\n",
    "               ...\n",
    "           },\n",
    "           ...\n",
    "       },\n",
    "       ...\n",
    "   ]\n",
    "}\n",
    "\n",
    "#### The mapping from JSON to Excel is as follows:\n",
    "\n",
    "1. The `point_text` value itself, mapped to the `text` field.\n",
    "2. One or more `query` entries, mapped to the `query` field.\n",
    "3. For each `query` entry, there will be the following dependent fields:\n",
    "   * `doi`\n",
    "   * `title` \n",
    "   * `full_text`\n",
    "   * `bibtex`\n",
    "   * `pdf_location`\n",
    "   * `journal`\n",
    "   * `citation_count`\n",
    "   * `relevance_score`\n",
    "\n",
    "To reconstruct the Excel table from the JSON, for each `point_text`, its value should be repeated for each associated `query`. This means that if there are 7 queries for a particular `point_text`, that `point_text` value should be repeated 7 times in the Excel table, with each repetition accompanied by the corresponding `query` and its dependent fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from JSON files loaded and saved to documents/master.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the documents folder\n",
    "documents_folder = \"documents\"\n",
    "\n",
    "# Create an Excel writer object\n",
    "#output_file = documents_folder + \"/master.xlsx\"\n",
    "#writer = pd.ExcelWriter(output_file, engine=\"openpyxl\")\n",
    "\n",
    "# Iterate over the JSON files in the documents folder\n",
    "for filename in os.listdir(documents_folder):\n",
    "    if filename.startswith(\"outline_\") and filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(documents_folder, filename)\n",
    "        \n",
    "        # Extract the sheet name from the filename\n",
    "        sheet_name = filename.split(\"_\")[1].split(\".\")[0]\n",
    "        \n",
    "        # Load the JSON data from the file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Create an empty list to store the data for the current JSON file\n",
    "        data_list = []\n",
    "        \n",
    "        # Extract the data from the JSON structure\n",
    "        for section in json_data:\n",
    "            document_title = section[\"title\"]\n",
    "            for subsection in section.get(\"subsections\", []):\n",
    "                section_title = subsection[\"title\"]\n",
    "                for point_key, point_data in subsection.items():\n",
    "                    if point_key.startswith(\"point\"):\n",
    "                        for query_key, query_data in point_data.items():\n",
    "                            if query_key.startswith(\"query\"):\n",
    "                                data_list.append({\n",
    "                                    \"document_title\": document_title,\n",
    "                                    \"section_title\": section_title,\n",
    "                                    \"point_text\": point_data[\"text\"],\n",
    "                                    \"query\": query_data,\n",
    "                                    \"doi\": \"\",\n",
    "                                    \"title\": \"\",\n",
    "                                    \"full_text\": \"\",\n",
    "                                    \"bibtex\": \"\",\n",
    "                                    \"pdf_location\": \"\",\n",
    "                                    \"journal\": \"\",\n",
    "                                    \"citation_count\": \"\",\n",
    "                                    \"relevance_score\": \"\"\n",
    "                                })\n",
    "        \n",
    "        # Create a DataFrame from the data list\n",
    "        df = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Reorder the columns\n",
    "        column_order = [\n",
    "            \"document_title\",\n",
    "            \"section_title\",\n",
    "            \"point_text\",\n",
    "            \"query\",\n",
    "            \"doi\",\n",
    "            \"title\",\n",
    "            \"full_text\",\n",
    "            \"bibtex\",\n",
    "            \"pdf_location\",\n",
    "            \"journal\",\n",
    "            \"citation_count\",\n",
    "            \"relevance_score\"\n",
    "        ]\n",
    "        df = df[column_order]\n",
    "        \n",
    "        # Save the DataFrame to a separate sheet in the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n",
    "print(f\"Data from JSON files loaded and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import asyncio\n",
    "import json\n",
    "from llm_api_handler import LLM_APIHandler\n",
    "from prompts import return_best_results\n",
    "\n",
    "class ExcelQueryProcessor:\n",
    "    def __init__(self, file_path, api_key_path):\n",
    "        self.workbook = openpyxl.load_workbook(file_path)\n",
    "        self.api_handler = LLM_APIHandler(api_key_path)\n",
    "\n",
    "    async def process_queries(self, sheet_name, input_columns, output_columns, new_columns=None, match_column=None, match_strings=None, skip_columns=None):\n",
    "        sheet = self.workbook[sheet_name]\n",
    "        query_data = []\n",
    "\n",
    "        for row in sheet.iter_rows(values_only=True):\n",
    "            if match_column and match_strings:\n",
    "                match_column_index = sheet.column_names.index(match_column)\n",
    "                if row[match_column_index] not in match_strings:\n",
    "                    continue\n",
    "\n",
    "            if skip_columns:\n",
    "                skip = False\n",
    "                for col in skip_columns:\n",
    "                    col_index = sheet.column_names.index(col)\n",
    "                    if not row[col_index]:\n",
    "                        skip = True\n",
    "                        break\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "            query_values = {col: row[sheet.column_names.index(col)] for col in input_columns}\n",
    "            query_data.append({\"row\": row[0], \"query\": query_values})\n",
    "\n",
    "        results = await self.process_queries_async(query_data, output_columns, new_columns)\n",
    "\n",
    "        if new_columns:\n",
    "            for col in new_columns:\n",
    "                if col not in sheet.column_names:\n",
    "                    sheet.column_dimensions[openpyxl.utils.get_column_letter(sheet.max_column + 1)].width = 20\n",
    "                    sheet.cell(row=1, column=sheet.max_column, value=col)\n",
    "\n",
    "        for result in results:\n",
    "            row = result[\"row\"]\n",
    "            response = result[\"response\"]\n",
    "            for paper in response:\n",
    "                for col, value in paper.items():\n",
    "                    if col in output_columns:\n",
    "                        col_index = sheet.column_names.index(col)\n",
    "                        sheet.cell(row=row, column=col_index + 1, value=value)\n",
    "                    elif col in new_columns:\n",
    "                        col_index = sheet.max_column\n",
    "                        sheet.cell(row=row, column=col_index, value=value)\n",
    "\n",
    "        self.workbook.save(file_path)\n",
    "\n",
    "    async def process_queries_async(self, query_data, output_columns, new_columns):\n",
    "        tasks = []\n",
    "        for query in query_data:\n",
    "            task = asyncio.ensure_future(self.process_single_query(query, output_columns, new_columns))\n",
    "            tasks.append(task)\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "    async def process_single_query(self, query, output_columns, new_columns):\n",
    "        while True:\n",
    "            outline = query[\"query\"].get(\"outline\", \"\")\n",
    "            review_intention = query[\"query\"].get(\"review_intention\", \"\")\n",
    "            point = query[\"query\"].get(\"point\", \"\")\n",
    "            research_papers = query[\"query\"].get(\"research_papers\", \"\")\n",
    "\n",
    "            prompt = return_best_results(outline, review_intention, point, research_papers)\n",
    "            response = await self.api_handler.generate_gemini_content(prompt)\n",
    "\n",
    "            try:\n",
    "                response_json = json.loads(response)\n",
    "                return {\"row\": query[\"row\"], \"response\": response_json}\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON response. Retrying query: {prompt}\")\n",
    "                \n",
    "                \n",
    "# Usage example\n",
    "file_path = \"master.xlsx\"\n",
    "api_key_path = \"api_keys.json\"\n",
    "sheet_name = \"Sheet1\"\n",
    "input_columns = [\"outline\", \"review_intention\", \"point\", \"research_papers\"]\n",
    "output_columns = [\"doi\", \"title\", \"citation_count\", \"relevance_score\", \"journal\"]\n",
    "new_columns = [\"New_Column1\", \"New_Column2\"]\n",
    "match_column = \"category\"\n",
    "match_strings = [\"A\", \"B\"]\n",
    "skip_columns = [\"skip_column1\", \"skip_column2\"]\n",
    "\n",
    "processor = ExcelQueryProcessor(file_path, api_key_path)\n",
    "asyncio.run(processor.process_queries(sheet_name, input_columns, output_columns, new_columns, match_column, match_strings, skip_columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated_lit_reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
