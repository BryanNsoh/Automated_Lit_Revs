

# Contents of ./utils\copy_all_code.py
import os


def copy_py_to_txt(output_file, folder):
    with open(output_file, "w") as outfile:
        for filename in os.listdir(folder):
            if filename.endswith(".py") and not (
                filename.endswith(".yaml") or filename.endswith(".json")
            ):
                file_path = os.path.join(folder, filename)
                with open(file_path, "r") as infile:
                    outfile.write(f"\n\n# Contents of {file_path}\n")
                    outfile.write(infile.read())


def main():
    current_folder = "./utils"
    output_filename = "all_python_contents.txt"
    copy_py_to_txt(output_filename, current_folder)
    print(f"All .py file contents copied to {output_filename}")


if __name__ == "__main__":
    main()


# Contents of ./utils\extract_relevant_papers.py
import asyncio
import aiofiles
import yaml
import logging
import json
import chardet
import re
import os
from llm_api_handler import LLM_APIHandler
from prompts import (
    get_prompt,
    review_intention,
    section_intentions,
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
file_handler = logging.FileHandler("paper_ranker.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class MetricsTracker:
    def __init__(self):
        self.total_entries_processed = 0
        self.relevant_entries = 0

    def increment_total_entries_processed(self):
        self.total_entries_processed += 1

    def increment_relevant_entries(self, count):
        self.relevant_entries += count

    def log_metrics(self):
        logger.info(f"Metrics Summary:")
        logger.info(f"- Total entries processed: {self.total_entries_processed}")
        logger.info(f"- Relevant entries found: {self.relevant_entries}")


class PaperRanker:
    def __init__(self, api_key_path, max_retries=4):
        self.llm_api_handler = LLM_APIHandler(api_key_path)
        self.max_retries = max_retries
        self.metrics_tracker = MetricsTracker()

    async def process_yaml_entry(
        self,
        entry,
        subsection_title,
        point_content,
        section_intention,
        output_folder_path,
    ):
        self.metrics_tracker.increment_total_entries_processed()
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                "rank_papers",
                review_intention=review_intention,
                point_content=point_content,
                subsection_title=subsection_title,
                document_title=entry.get("title", ""),
                full_text=entry.get("full_text", ""),
                abstract=entry.get("description", ""),
                section_intention=section_intention,
            )
            try:
                response = await self.llm_api_handler.generate_gemini_content(prompt)
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping entry."
                    )
                    return None
                response = re.search(r"\{.*\}", response, re.DOTALL)
                if response:
                    response = response.group()
                else:
                    response = ""
                try:
                    json_data = json.loads(response)
                    if "relevance_score" in json_data:
                        try:
                            relevance_score = float(json_data["relevance_score"])
                            if relevance_score < 0 or relevance_score > 1:
                                raise ValueError(
                                    "Relevance score must be between 0 and 1"
                                )
                            entry.update(json_data)
                            logger.debug(f"Successfully processed entry.")
                            if relevance_score > 0.5:
                                await self.save_relevant_entry(
                                    output_folder_path, entry
                                )
                            return entry
                        except (ValueError, TypeError):
                            logger.warning(
                                f"Invalid relevance score for current entry. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"Relevance score not found in the response. Retrying..."
                        )
                        retry_count += 1
                except json.JSONDecodeError:
                    logger.warning(
                        f"Invalid JSON response for the current entry. Retrying immediately..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing entry: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for current entry. Skipping entry.")
        return None

    async def save_relevant_entry(self, output_folder_path, entry):
        output_file_name = "relevant_entries.yaml"
        output_file_path = os.path.join(output_folder_path, output_file_name)
        try:
            async with aiofiles.open(output_file_path, "a", encoding="utf-8") as file:
                yaml_data = yaml.safe_dump([entry], allow_unicode=True)
                await file.write(yaml_data)
            logger.info(f"Successfully saved relevant entry to: {output_file_path}")
        except Exception as e:
            logger.exception(f"Error saving relevant entry: {str(e)}")

    async def process_yaml_files(
        self,
        yaml_file_paths,
        output_folder_path,
        subsection_title,
        point_content,
        section_intention,
    ):
        tasks = []
        for yaml_file_path in yaml_file_paths:
            logger.info(f"Processing file: {yaml_file_path}")
            try:
                async with aiofiles.open(yaml_file_path, "r", encoding="utf-8") as file:
                    data = yaml.safe_load(await file.read())
            except FileNotFoundError:
                logger.warning(f"File not found: {yaml_file_path}. Skipping.")
                continue
            except Exception as e:
                logger.exception(f"Error loading YAML file: {str(e)}")
                continue
            for entry in data:
                task = asyncio.create_task(
                    self.process_yaml_entry(
                        entry,
                        subsection_title,
                        point_content,
                        section_intention,
                        output_folder_path,
                    )
                )
                tasks.append(task)
        processed_entries = await asyncio.gather(*tasks, return_exceptions=True)
        relevant_entries = [
            entry
            for entry in processed_entries
            if entry
            and isinstance(entry, dict)
            and float(entry.get("relevance_score", 0)) > 0.5
        ]
        self.metrics_tracker.increment_relevant_entries(len(relevant_entries))
        logger.info(f"Successfully processed files.")


class FileSystemHandler:
    def __init__(self):
        self.progress_file = "progress.yaml"

    async def process_outline(
        self,
        outline_file_path,
        output_folder_path,
        ranker,
        section_number,
    ):
        try:
            async with aiofiles.open(outline_file_path, "rb") as file:
                content = await file.read()
                encoding = chardet.detect(content)["encoding"]
                content = content.decode(encoding, errors="replace")
                outline_data = yaml.safe_load(content)
        except FileNotFoundError:
            logger.error(f"Outline file not found: {outline_file_path}")
            return
        except Exception as e:
            logger.exception(f"Error loading outline file: {str(e)}")
            return
        section_intention = section_intentions.get(section_number, "")
        tasks = []
        for subsection in outline_data["subsections"]:
            subsection_index = subsection["index"]
            subsection_title = subsection["subsection_title"]
            for point_dict in subsection["points"]:
                for point_key, point in point_dict.items():
                    point_content = point["point_content"]
                    point_folder_name = point_key.replace(" ", "_")
                    point_folder = os.path.join(
                        output_folder_path,
                        f"subsection_{subsection_index}",
                        point_folder_name,
                    )
                    os.makedirs(point_folder, exist_ok=True)
                    yaml_file_paths = []
                    for response in point["scopus_queries"] + point["alex_queries"]:
                        for response_data in response["responses"]:
                            yaml_file_path = response_data.get("yaml_path", "")
                            if yaml_file_path:
                                yaml_file_paths.append(yaml_file_path)
                    task = asyncio.create_task(
                        ranker.process_yaml_files(
                            yaml_file_paths,
                            point_folder,
                            subsection_title,
                            point_content,
                            section_intention,
                        )
                    )
                    tasks.append(task)
        await asyncio.gather(*tasks)


async def main(section_number):
    output_folder_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\processed"
    outline_file_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\outline_queries.yaml"
    api_key_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\keys\api_keys.json"
    async with LLM_APIHandler(api_key_path) as api_handler:
        ranker = PaperRanker(api_key_path)
        file_system_handler = FileSystemHandler()
        os.makedirs(output_folder_path, exist_ok=True)
        logger.info(f"Starting paper ranking process for section {section_number}...")
        await file_system_handler.process_outline(
            outline_file_path,
            output_folder_path,
            ranker,
            section_number,
        )
        logger.info(f"Paper ranking process completed for section {section_number}.")
        try:
            ranker.metrics_tracker.log_metrics()
        except Exception as e:
            logger.exception("Error logging metrics:")
            pass


if __name__ == "__main__":
    section_number = "3"  # Replace with the desired section number
    asyncio.run(main(section_number))


# Contents of ./utils\google_scholar_search.py
import requests
import yaml
from bs4 import BeautifulSoup
import logging
import time
import random

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class GoogleScholarScraper:
    def __init__(self, yaml_file, max_requests_per_minute=30, delay_range=(1, 5)):
        self.yaml_file = yaml_file
        self.max_requests_per_minute = max_requests_per_minute
        self.delay_range = delay_range
        self.request_count = 0
        self.last_request_time = time.time()

    async def get_scholar_data(self, query):
        try:
            # Rate limiting
            current_time = time.time()
            elapsed_time = current_time - self.last_request_time
            if elapsed_time < 60 / self.max_requests_per_minute:
                sleep_time = 60 / self.max_requests_per_minute - elapsed_time
                logging.info(f"Rate limiting: Sleeping for {sleep_time:.2f} seconds")
                time.sleep(sleep_time)
            self.last_request_time = time.time()

            # Random delay
            delay = random.uniform(*self.delay_range)
            logging.info(f"Random delay: Sleeping for {delay:.2f} seconds")
            time.sleep(delay)

            url = f"https://www.google.com/scholar?q={query}&hl=en"
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36"
            }
            logging.info(f"Sending request to URL: {url}")
            response = requests.get(url, headers=headers)
            logging.info(f"Received response with status code: {response.status_code}")
            soup = BeautifulSoup(response.text, "html.parser")
            scholar_results = []
            for el in soup.select(".gs_ri"):
                title = el.select(".gs_rt")[0].text if el.select(".gs_rt") else ""
                doi = (
                    el.select(".gs_a a[href*='doi.org']")[0].text
                    if el.select(".gs_a a[href*='doi.org']")
                    else ""
                )
                full_text = ""
                inline_citation = ""  # Not available in the provided HTML structure
                full_citation = ""  # Not available in the provided HTML structure
                publication_year = (
                    el.select(".gs_a")[0].text.split(" - ")[-1]
                    if " - " in el.select(".gs_a")[0].text
                    else ""
                )
                authors = (
                    [
                        author.strip()
                        for author in el.select(".gs_a")[0]
                        .text.split(" - ")[0]
                        .split(",")
                    ]
                    if " - " in el.select(".gs_a")[0].text
                    else []
                )
                citation_count = (
                    int(
                        el.select(".gs_fl a:contains('Cited by')")[0].text.split(" ")[
                            -1
                        ]
                    )
                    if el.select(".gs_fl a:contains('Cited by')")
                    else 0
                )
                pdf_link = (
                    el.select(".gs_or_ggsm a")[0]["href"]
                    if el.select(".gs_or_ggsm a")
                    else ""
                )
                journal = (
                    el.select(".gs_a")[0].text.split(" - ")[1]
                    if len(el.select(".gs_a")[0].text.split(" - ")) > 1
                    else ""
                )
                # remove "[HTML][HTML]" and "[PDF][PDF]" and "[BOOK][B]" from the title if present
                title = (
                    title.replace("[HTML][HTML]", "")
                    .replace("[PDF][PDF]", "")
                    .replace("[BOOK][B]", "")
                )
                scholar_results.append(
                    {
                        "title": title,
                        "DOI": doi,
                        "full_text": full_text,
                        "inline_citation": inline_citation,
                        "full_citation": full_citation,
                        "publication_year": publication_year,
                        "authors": authors,
                        "citation_count": citation_count,
                        "pdf_link": pdf_link,
                        "journal": journal,
                    }
                )
            logging.info(
                f"Extracted {len(scholar_results)} scholar results for query: {query}"
            )
            return scholar_results
        except Exception as e:
            logging.error(f"Error occurred while processing query: {query}")
            logging.error(f"Exception: {str(e)}")
            return None

    async def process_queries(self):
        with open(self.yaml_file, "r") as file:
            data = yaml.safe_load(file)
            print(f"Loaded YAML file: {self.yaml_file}")

    async def process_queries(self):
        with open(self.yaml_file, "r") as file:
            data = yaml.safe_load(file)

        for subsection in data["subsections"]:
            for point in subsection["points"]:
                point_title = next(iter(point))
                print(f"Processing point: {point_title}")

                if "google_queries" in point[point_title]:
                    for query_data in point[point_title]["google_queries"]:
                        if query_data["query_id"].startswith("google_"):
                            query = query_data["query"]
                            logging.info(f"Processing Google query: {query}")
                            results = await self.get_scholar_data(query)
                            query_data["responses"] = results
                            logging.info(f"Processed Google query: {query}")

                if "scopus_queries" in point[point_title]:
                    for query_data in point[point_title]["scopus_queries"]:
                        if query_data["query_id"].startswith("scopus_"):
                            query = query_data["query"]
                            logging.info(f"Processing Scopus query: {query}")
                            # Perform Scopus query processing here
                            logging.info(f"Processed Scopus query: {query}")

        with open(self.yaml_file, "w") as file:
            yaml.dump(data, file)
        logging.info(f"Updated YAML file: {self.yaml_file}")


async def main():
    yaml_file = "data.yaml"
    scraper = GoogleScholarScraper(
        yaml_file, max_requests_per_minute=30, delay_range=(1, 5)
    )
    logging.info("Starting Google Scholar scraper")
    await scraper.process_queries()
    logging.info("Google Scholar scraping completed")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())


# Contents of ./utils\llm_api_handler.py
import asyncio
import aiohttp
import logging
import google.generativeai as genai
import google.api_core.exceptions
import anthropic
import backoff
import requests
import tiktoken
import time
from collections import deque
import json

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler("llm_handler.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


def count_tokens(text, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(text))
    return num_tokens


def clip_prompt(prompt, max_tokens, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    tokens = encoding.encode(prompt)
    if len(tokens) > max_tokens:
        clipped_tokens = tokens[:max_tokens]
        clipped_prompt = encoding.decode(clipped_tokens)
        return clipped_prompt
    return prompt


class RateLimiter:
    def __init__(self, rps, window_size):
        self.rps = rps
        self.window_size = window_size
        self.window_start = time.monotonic()
        self.request_count = 0
        self.semaphore = asyncio.Semaphore(rps)

    async def acquire(self):
        current_time = time.monotonic()
        elapsed_time = current_time - self.window_start
        if elapsed_time > self.window_size:
            self.window_start = current_time
            self.request_count = 0
        if self.request_count >= self.rps:
            await asyncio.sleep(self.window_size - elapsed_time)
            self.window_start = time.monotonic()
            self.request_count = 0
        self.request_count += 1
        await self.semaphore.acquire()

    def release(self):
        self.semaphore.release()


class RequestTracker:
    def __init__(self):
        self.active_requests = 0
        self.total_requests = 0
        self.total_response_time = 0
        self.request_times = []
        self.throughputs = []

    def start_request(self):
        self.active_requests += 1
        self.total_requests += 1
        self.request_times.append(time.time())

    def end_request(self, response_time):
        self.active_requests -= 1
        self.total_response_time += response_time

    def calculate_metrics(self):
        elapsed_time = time.time() - self.request_times[0]
        throughput = self.total_requests / elapsed_time
        self.throughputs.append(throughput)
        avg_response_time = self.total_response_time / self.total_requests
        return {
            "active_requests": self.active_requests,
            "total_requests": self.total_requests,
            "throughput": throughput,
            "avg_response_time": avg_response_time,
        }


class LLM_APIHandler:
    def __init__(self, key_path, rps=1, window_size=55):
        self.load_api_keys(key_path)
        self.rate_limiters = [
            RateLimiter(rps, window_size) for _ in range(len(self.gemini_api_keys))
        ]
        self.claude_rate_limiter = RateLimiter(rps, window_size)
        self.together_rate_limiter = RateLimiter(rps, window_size)
        self.request_tracker = RequestTracker()
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_NONE",
            },
        ]
        self.gemini_clients = []
        for api_key in self.gemini_api_keys:
            genai.configure(api_key=api_key)
            client = genai.GenerativeModel(
                "gemini-pro", safety_settings=self.safety_settings
            )
            self.gemini_clients.append(client)
        self.claude_client = anthropic.Anthropic(api_key=self.claude_api_key)
        self.session = aiohttp.ClientSession()  # Create a single session
        self.client_queue = deque(range(len(self.gemini_api_keys)))  # Round-robin queue

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()  # Close the session when done

    def load_api_keys(self, key_path):
        with open(key_path, "r") as file:
            api_keys = json.load(file)
        self.gemini_api_keys = [
            api_keys[key] for key in api_keys if key.startswith("GEMINI_API_KEY")
        ]
        self.claude_api_key = api_keys["CLAUDE_API_KEY"]
        self.together_api_key = api_keys["TOGETHER_API_KEY"]

    @backoff.on_exception(
        backoff.expo,
        (
            aiohttp.ClientError,
            ValueError,
            google.api_core.exceptions.InternalServerError,
            google.api_core.exceptions.ServiceUnavailable,
            google.api_core.exceptions.Unknown,
        ),
        max_tries=5,
        max_time=60,
    )
    async def generate_gemini_content(self, prompt):
        client_index = self.client_queue.popleft()
        self.client_queue.append(
            client_index
        )  # Move the client to the end of the queue
        rate_limiter = self.rate_limiters[client_index]
        await rate_limiter.acquire()
        try:
            retry_count = 0
            while retry_count < 5:
                try:
                    clipped_prompt = clip_prompt(prompt, max_tokens=25000)
                    logging.info(
                        f"Generating content with Gemini API (client {client_index}). Prompt: {clipped_prompt}"
                    )
                    self.request_tracker.start_request()
                    start_time = time.time()
                    response = await self.gemini_clients[
                        client_index
                    ].generate_content_async(clipped_prompt)
                    end_time = time.time()
                    response_time = end_time - start_time
                    self.request_tracker.end_request(response_time)
                    if response.text:
                        metrics = self.request_tracker.calculate_metrics()
                        logger.info(f"Gemini API Metrics: {metrics}")
                        return response.text
                    else:
                        raise ValueError("Invalid response format from Gemini API.")
                except (IndexError, AttributeError, ValueError) as e:
                    retry_count += 1
                    logger.warning(
                        f"Error from Gemini API (client {client_index}). Retry count: {retry_count}. Error: {e}"
                    )
                    await asyncio.sleep(min(2**retry_count, 30))
                except google.api_core.exceptions.InternalServerError as e:
                    retry_count += 1
                    logger.warning(
                        f"InternalServerError from Gemini API (client {client_index}). Retry count: {retry_count}. Error: {e}"
                    )
                    await asyncio.sleep(min(2**retry_count, 30))
                except google.api_core.exceptions.ServiceUnavailable as e:
                    retry_count += 1
                    logger.warning(
                        f"ServiceUnavailable from Gemini API (client {client_index}). Retry count: {retry_count}. Error: {e}"
                    )
                    await asyncio.sleep(min(2**retry_count, 30))
                except google.api_core.exceptions.Unknown as e:
                    retry_count += 1
                    logger.warning(
                        f"Unknown error from Gemini API (client {client_index}). Retry count: {retry_count}. Error: {e}"
                    )
                    await asyncio.sleep(min(2**retry_count, 30))
            logger.error(
                f"Max retries reached. Unable to generate content with Gemini API (client {client_index}). Moving on."
            )
            return None
        except Exception as e:
            logger.error(
                f"Unexpected error from Gemini API (client {client_index}). Error: {e}"
            )
            raise
        finally:
            rate_limiter.release()

    @backoff.on_exception(
        backoff.expo, (anthropic.APIError, ValueError), max_tries=5, max_time=60
    )
    async def generate_claude_content(
        self,
        prompt,
        system_prompt=None,
        model="claude-3-haiku-20240307",
        max_tokens=3000,
    ):
        await self.claude_rate_limiter.acquire()
        try:
            if model not in ["claude-3-haiku-20240307"]:
                raise ValueError(f"Invalid model: {model}")
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            messages = [{"role": "user", "content": clipped_prompt}]
            if system_prompt is None:
                system_prompt = "Directly fulfill the user's request without preamble, paying very close attention to all nuances of their instructions."
            logger.info(f"Generating content with Claude API. Prompt: {clipped_prompt}")
            try:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=messages,
                )
                logger.info(f"Claude API response: {response.content[0].text}")
                return response.content[0].text
            except anthropic.APIError as e:
                logger.error(
                    f"Max retries reached. Unable to generate content with Claude API. Error: {e}. Moving on."
                )
                return None
        finally:
            self.claude_rate_limiter.release()

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, ValueError),
        max_tries=5,
        max_time=60,
    )
    async def generate_together_content(
        self,
        prompt,
        model="Qwen/Qwen1.5-72B-Chat",
        max_tokens="null",
        temperature=0.25,
        top_p=0.5,
        top_k=20,
        repetition_penalty=1.23,
        stop=None,
        messages=None,
    ):
        await self.together_rate_limiter.acquire()
        try:
            endpoint = "https://api.together.xyz/v1/chat/completions"
            if stop is None:
                stop = ["<|im_end|>", "<|im_start|>"]
            clipped_prompt = clip_prompt(prompt, max_tokens=25000)
            if messages is None:
                messages = [{"content": clipped_prompt, "role": "user"}]
            data = {
                "model": model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "repetition_penalty": repetition_penalty,
                "stop": stop,
                "messages": messages,
                "repetitive_penalty": repetition_penalty,
            }
            headers = {"Authorization": f"Bearer {self.together_api_key}"}
            retry_count = 0
            while retry_count < 5:
                try:
                    response = requests.post(endpoint, json=data, headers=headers)
                    response.raise_for_status()
                    logger.info(f"Together API response: {response.text}")
                    response_data = response.json()
                    generated_text = response_data["choices"][0]["message"]["content"]
                    return generated_text
                except requests.exceptions.RequestException as e:
                    retry_count += 1
                    logger.warning(
                        f"Error from Together API. Retry count: {retry_count}. Error: {e}"
                    )
                    await asyncio.sleep(
                        min(2**retry_count, 60)
                    )  # Exponential backoff capped at 60 seconds
            logger.error(
                "Max retries reached. Unable to generate content with Together API. Moving on."
            )
            return None
        finally:
            self.together_rate_limiter.release()


async def main():
    api_key_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\keys\api_keys.json"
    rps = 5  # Requests per second
    window_size = 60  # Window size in seconds
    async with LLM_APIHandler(api_key_path, rps, window_size) as api_handler:
        # Test Gemini API
        gemini_prompt = "What is the meaning of life?"
        gemini_response = await api_handler.generate_gemini_content(gemini_prompt)
        print("Gemini Response:")
        print(gemini_response)
        print()

        # Test Claude API
        claude_prompt = "What is the meaning of life?"
        claude_response = await api_handler.generate_claude_content(claude_prompt)
        print("Claude Response:")
        print(claude_response)
        print()

        # Test Together API
        together_prompt = "What is the meaning of life?"
        together_response = await api_handler.generate_together_content(together_prompt)
        print("Together Response:")
        print(together_response)
        print()


if __name__ == "__main__":
    asyncio.run(main())


# Contents of ./utils\misc_utils.py
import json
import asyncio


async def prepare_text_for_json(text):
    # Replace backslashes with double backslashes
    text = text.replace("\\", "\\\\")

    # Replace double quotes with escaped double quotes
    text = text.replace('"', '\\"')

    # Replace newline characters with escaped newline characters
    text = text.replace("\n", "\\n")

    # Replace tab characters with escaped tab characters
    text = text.replace("\t", "\\t")

    # Replace form feed characters with escaped form feed characters
    text = text.replace("\f", "\\f")

    # Replace backspace characters with escaped backspace characters
    text = text.replace("\b", "\\b")

    # Replace carriage return characters with escaped carriage return characters
    text = text.replace("\r", "\\r")

    # Wrap the escaped text in double quotes to make it a valid JSON string
    json_string = f'"{text}"'

    return json_string


# Contents of ./utils\openalex_search.py
import aiohttp
import asyncio
import json
import fitz
import urllib.parse
import yaml
from hashlib import sha256

import logging

# Create a logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Create a console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Create a formatter
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)

# Add the console handler to the logger
logger.addHandler(console_handler)

# Optional: Create a file handler to log messages to a file
file_handler = logging.FileHandler("alex.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class OpenAlexPaperSearch:
    def __init__(self, email, web_scraper, output_folder):
        self.base_url = "https://api.openalex.org"
        self.email = email
        self.web_scraper = web_scraper
        self.output_folder = output_folder
        self.semaphore = asyncio.Semaphore(5)  # Limit to 5 concurrent requests

    async def search_papers(self, query, query_id, response_id, max_results=30):
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=600)
        ) as session:
            if query.startswith("https://api.openalex.org/works?"):
                search_url = f"{query}&mailto={self.email}"
            else:
                encoded_query = urllib.parse.quote(query)
                search_url = f"{self.base_url}/works?search={encoded_query}&per_page={max_results}&mailto={self.email}"

            retries = 0
            max_retries = 3
            retry_delay = 1

            while retries < max_retries:
                async with self.semaphore:
                    try:
                        await asyncio.sleep(
                            0.2
                        )  # Wait for 0.2 seconds between requests to comply with rate limits
                        async with session.get(search_url) as response:
                            if response.status == 429:
                                logger.warning(
                                    f"Rate limit exceeded. Retrying in {retry_delay} seconds..."
                                )
                                await asyncio.sleep(retry_delay)
                                retries += 1
                                retry_delay *= 2  # Exponential backoff
                            elif response.status == 200:
                                if response.content_type == "application/json":
                                    data = await response.json()

                                    if "results" in data:
                                        paper_data = []
                                        for work in data["results"][:25]:
                                            paper = {
                                                "DOI": (
                                                    work["doi"] if "doi" in work else ""
                                                ),
                                                "authors": (
                                                    [
                                                        author["author"]["display_name"]
                                                        for author in work[
                                                            "authorships"
                                                        ]
                                                    ]
                                                    if "authorships" in work
                                                    else []
                                                ),
                                                "citation_count": (
                                                    work["cited_by_count"]
                                                    if "cited_by_count" in work
                                                    else 0
                                                ),
                                                "full_citation": ">",
                                                "full_text": ">",
                                                "analysis": ">",
                                                "verbatim_quote1": ">",
                                                "verbatim_quote2": ">",
                                                "verbatim_quote3": ">",
                                                "relevance_score1": 0,
                                                "relevance_score2": 0,
                                                "limitations": ">",
                                                "inline_citation": ">",
                                                "journal": (
                                                    work["primary_location"]["source"][
                                                        "display_name"
                                                    ]
                                                    if "primary_location" in work
                                                    and isinstance(
                                                        work["primary_location"], dict
                                                    )
                                                    and "source"
                                                    in work["primary_location"]
                                                    and isinstance(
                                                        work["primary_location"][
                                                            "source"
                                                        ],
                                                        dict,
                                                    )
                                                    and "display_name"
                                                    in work["primary_location"][
                                                        "source"
                                                    ]
                                                    else ""
                                                ),
                                                "pdf_link": (
                                                    work["primary_location"]["pdf_url"]
                                                    if "primary_location" in work
                                                    and isinstance(
                                                        work["primary_location"], dict
                                                    )
                                                    and "pdf_url"
                                                    in work["primary_location"]
                                                    else ""
                                                ),
                                                "publication_year": (
                                                    work["publication_year"]
                                                    if "publication_year" in work
                                                    else ""
                                                ),
                                                "title": (
                                                    work["title"]
                                                    if "title" in work
                                                    else ""
                                                ),
                                            }

                                            full_text = ""  # Initialize full_text with an empty string

                                            try:
                                                if paper["pdf_link"]:
                                                    logger.info(
                                                        f"Extracting full text from PDF URL: {paper['pdf_link']}"
                                                    )
                                                    full_text = (
                                                        await self.extract_fulltext(
                                                            paper["pdf_link"]
                                                        )
                                                    )
                                                    if not full_text:
                                                        logger.info(
                                                            f"Extracting full text from URL: {paper['pdf_link']}"
                                                        )
                                                        full_text = await self.extract_fulltext_from_url(
                                                            paper["pdf_link"]
                                                        )

                                                if not full_text and paper["DOI"]:
                                                    logger.info(
                                                        f"Extracting full text from DOI: {paper['DOI']}"
                                                    )
                                                    full_text = await self.extract_fulltext_from_doi(
                                                        paper["DOI"]
                                                    )

                                                if full_text:
                                                    logger.info(
                                                        "Full text extracted successfully."
                                                    )
                                                    paper["full_text"] = (
                                                        ">\n" + full_text
                                                    )
                                                else:
                                                    logger.warning(
                                                        "Failed to extract full text."
                                                    )
                                            except Exception as e:
                                                logger.error(
                                                    f"Error occurred while extracting full text: {str(e)}"
                                                )

                                            paper_data.append(paper)

                                        hashed_filename = self.get_hashed_filename(
                                            query, query_id, response_id
                                        )
                                        output_path = self.save_yaml(
                                            paper_data, hashed_filename
                                        )
                                        logger.info(
                                            f"Saved paper data to: {output_path}"
                                        )
                                        return output_path
                                    else:
                                        logger.warning(
                                            f"Unexpected JSON structure from OpenAlex API: {data}"
                                        )
                                        return ""
                                else:
                                    logger.error(
                                        f"Unexpected content type from OpenAlex API: {response.content_type}"
                                    )
                                    logger.error(f"URL: {search_url}")
                                    logger.error(await response.text())
                                    return ""
                            else:
                                logger.error(
                                    f"Unexpected status code from OpenAlex API: {response.status}"
                                )
                                logger.error(f"URL: {search_url}")
                                logger.error(await response.text())
                                return ""
                    except asyncio.TimeoutError:
                        logger.warning(
                            f"Request timed out. Retrying in {retry_delay} seconds..."
                        )
                        retries += 1
                        if retries < max_retries:
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # Exponential backoff
                        else:
                            logger.error(f"Max retries exceeded for URL: {search_url}")
                            return ""
                    except aiohttp.ClientError as error:
                        logger.exception(
                            f"Error occurred while making request to OpenAlex API: {str(error)}"
                        )
                        retries += 1
                        if retries < max_retries:
                            logger.warning(
                                f"Retrying request in {retry_delay} seconds..."
                            )
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # Exponential backoff
                        else:
                            logger.error(f"Max retries exceeded for URL: {search_url}")
                            return ""

            logger.error(f"Max retries exceeded for URL: {search_url}")
            return ""

    async def extract_fulltext(self, pdf_url):
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(pdf_url) as resp:
                    pdf_bytes = await resp.read()
                    try:
                        with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                            fulltext = ""
                            for page in doc:
                                fulltext += page.get_text()
                            logger.info(f"Full text extracted from PDF: {pdf_url}")
                            return fulltext
                    except fitz.FileDataError:
                        logger.error(f"Error: Cannot open PDF file from {pdf_url}")
                        return ""
            except aiohttp.ClientError as e:
                logger.error(f"Error occurred while retrieving PDF from {pdf_url}")
                logger.error(f"Error details: {str(e)}")
                return ""
            except Exception as e:
                logger.error(
                    f"Unexpected error occurred while extracting full text from {pdf_url}"
                )
                logger.error(f"Error details: {str(e)}")
                return ""

    async def extract_fulltext_from_url(self, pdf_url):
        try:
            logger.info(f"Extracting full text from URL: {pdf_url}")
            content = await self.web_scraper.get_url_content(pdf_url)
            logger.info(f"Full text extracted from URL: {pdf_url}")
            return content
        except Exception as e:
            logger.error(
                f"Error: Failed to scrape full text from PDF URL {pdf_url}. {str(e)}"
            )
            return ""

    async def extract_fulltext_from_doi(self, doi):
        try:
            logger.info(f"Extracting full text from DOI: {doi}")
            content = await self.web_scraper.get_url_content(doi)
            logger.info(f"Full text extracted from DOI: {doi}")
            return content
        except Exception as e:
            logger.error(f"Error: Failed to scrape full text from DOI {doi}. {str(e)}")
            return ""

    def get_hashed_filename(self, query, query_id, response_id):
        hash_input = f"{query}_{query_id}_{response_id}"
        hashed_filename = sha256(hash_input.encode()).hexdigest()
        return f"{hashed_filename}.yaml"

    def save_yaml(self, data, filename):
        try:
            output_path = self.output_folder / filename
            # Create the output folder if it doesn't exist
            self.output_folder.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, default_flow_style=False, allow_unicode=True)
            logger.info(f"YAML file saved successfully: {output_path}")
            return output_path.absolute()
        except Exception as e:
            logger.error(
                f"An error occurred while saving YAML file: {filename}. Error: {e}"
            )
            return ""


# Contents of ./utils\populate_search_queries.py
import yaml
import asyncio
import aiofiles
import re
import os
from pathlib import Path
import json
from llm_api_handler import LLM_APIHandler
from prompts import (
    get_prompt,
    review_intention,
    scopus_search_guide,
    alex_search_guide,
    section_intentions,
)


class QueryGenerator:
    def __init__(self, api_key_path, max_retries=10):
        self.llm_api_handler = LLM_APIHandler(api_key_path)
        self.max_retries = max_retries

    async def process_yaml(self, yaml_path, section_title, section_number):
        async with aiofiles.open(yaml_path) as file:
            self.yaml_data = yaml.safe_load(await file.read())

        # Determine the temporary output file path
        yaml_dir = Path(yaml_path).parent
        temp_output_file = yaml_dir / "temp_outline_queries.yaml"

        subsections = self.yaml_data["subsections"]
        tasks = []
        for subsection in subsections:
            subsection_title = subsection["subsection_title"]
            print(f"Processing subsection: {subsection_title}")
            points = subsection["points"]
            for point in points:
                for point_key, point_data in point.items():
                    point_content = point_data["point_content"]

                    # Generate queries for each query type
                    query_types = [
                        key for key in point_data if key.endswith("_queries")
                    ]
                    for query_type in query_types:
                        task = asyncio.create_task(
                            self.process_query_type(
                                section_title,
                                subsection_title,
                                point_content,
                                eval(query_type.replace("_queries", "_search_guide")),
                                section_number,
                                query_type,
                                point_data,
                            )
                        )
                        tasks.append(task)

        await asyncio.gather(*tasks)

        # Write the updated yaml_data to the temporary output file
        async with aiofiles.open(temp_output_file, "w") as file:
            await file.write(yaml.dump(self.yaml_data))

        # Rename the temporary output file to the final output file
        output_file = yaml_dir / "outline_queries.yaml"
        os.rename(temp_output_file, output_file)

    async def process_query_type(
        self,
        section_title,
        subsection_title,
        point_content,
        search_guidance,
        section_number,
        query_type,
        point_data,
    ):
        queries = await self.get_queries(
            section_title,
            subsection_title,
            point_content,
            search_guidance,
            section_number,
        )
        print(f"{query_type}: ", queries)
        point_data[query_type] = queries

        # Create 1 empty response for each query
        for i, query in enumerate(queries):
            query_id = f"{query_type}_{i}"
            query_data = {
                "query_id": query_id,
                "query": query,
                "responses": [
                    {
                        "response_id": f"{query_id}_response_{j}",
                    }
                    for j in range(1)
                ],
            }
            point_data[query_type][i] = query_data

    async def get_queries(
        self,
        section_title,
        subsection_title,
        point_content,
        search_guidance,
        section_number,
    ):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="generate_queries",
                review_intention=review_intention,
                section_intention=section_intentions[str(section_number)],
                point_content=point_content,
                section_title=section_title,
                subsection_title=subsection_title,
                search_guidance=search_guidance,
            )
            response = await self.llm_api_handler.generate_gemini_content(prompt)
            queries = self.parse_response(response)
            if queries:
                print(queries)
                return queries
            retry_count += 1
            print(f"Retrying... Attempt {retry_count}")
        print("Max retries reached. Returning empty queries.")
        return []

    def parse_response(self, response):
        try:
            data = json.loads(response)
            queries = None

            # Find the key ending with "_queries"
            for key in data:
                if key.endswith("_queries"):
                    queries = data[key]
                    break

            if queries:
                print(f"Parsed {key}.")
                return queries
            else:
                print("No queries found in the response.")
                return []
        except json.JSONDecodeError as exc:
            print(f"Error parsing JSON response: {exc}")
            return None


# Example usage
yaml_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\research_paper_outline.yaml"
section_title = "DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING"
section_number = 3
api_key_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\keys\api_keys.json"
processor = QueryGenerator(api_key_path)
asyncio.run(processor.process_yaml(yaml_path, section_title, section_number))


# Contents of ./utils\populate_search_results.py
import asyncio
import logging
from pathlib import Path
from openalex_search import OpenAlexPaperSearch
from scopus_search import ScopusSearch
from yaml_iterator import IrrigationData
from web_scraper import WebScraper
from ruamel.yaml import YAML

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Create a file handler to log messages to a file
file_handler = logging.FileHandler("get_results.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(
    logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
)
logger.addHandler(file_handler)


class QueryProcessor:
    def __init__(self, yaml_file, output_folder, api_key_path, email, new_yaml_file):
        self.yaml_file = yaml_file
        self.output_folder = Path(output_folder)
        self.api_key_path = api_key_path
        self.email = email
        self.new_yaml_file = new_yaml_file
        self.web_scraper = WebScraper()
        self.openalex_search = OpenAlexPaperSearch(
            email, self.web_scraper, self.output_folder
        )
        self.scopus_search = ScopusSearch(
            api_key_path, self.web_scraper, self.output_folder
        )
        self.irrigation_data = IrrigationData(yaml_file)
        self.new_data = {}

    async def process_queries(self):
        try:
            await self.irrigation_data.load_data()
            logger.info("Loaded YAML data successfully.")

            query_tasks = []
            entries_to_process = []
            async for (
                subsection,
                point_title,
                point_content,
                query_type,
                query_id,
                response_id,
                response,
                query,
            ) in self.irrigation_data.iterate_data():
                if query and "yaml_path" not in response:
                    entries_to_process.append(
                        (
                            subsection["index"],
                            subsection["subsection_title"],
                            point_title,
                            point_content,
                        )
                    )
                    query_tasks.append(
                        self.process_query(
                            subsection,
                            point_title,
                            point_content,
                            query_type,
                            query,
                            query_id,
                            response_id,
                        )
                    )

            logger.info(f"Found {len(entries_to_process)} entries to process:")
            for entry in entries_to_process:
                logger.info(
                    f"Subsection: {entry[0]}, Subsection Title: {entry[1]}, Point Title: {entry[2]}, Point Content: {entry[3]}"
                )

            logger.info(f"Processing {len(query_tasks)} queries.")
            await asyncio.gather(*query_tasks)
            logger.info("Finished processing queries.")

            await self.save_new_data()
            logger.info("Saved new YAML data.")

        except Exception as e:
            logger.exception("An error occurred during query processing.")
            raise

    async def process_query(
        self,
        subsection,
        point_title,
        point_content,
        query_type,
        query,
        query_id,
        response_id,
    ):
        if "scopus_queries" in query_type:
            output_path = await self.scopus_search.search_and_parse(
                query, query_id, response_id
            )
            logger.info(
                f"Processed Scopus query {query_id}. Output path: {output_path}"
            )
        elif "alex_queries" in query_type:
            output_path = await self.openalex_search.search_papers(
                query, query_id, response_id
            )
            logger.info(
                f"Processed OpenAlex query {query_id}. Output path: {output_path}"
            )
        else:
            logger.warning(f"Unsupported query type: {query_type}")
            return

        await self.update_new_data(
            subsection["index"],
            subsection["subsection_title"],
            point_title,
            point_content,
            str(output_path),
        )

    async def update_new_data(
        self, subsection_index, subsection_title, point_title, point_content, yaml_path
    ):
        if subsection_index not in self.new_data:
            self.new_data[subsection_index] = {
                "subsection_title": subsection_title,
                "point_content": {},
            }
        if point_title not in self.new_data[subsection_index]["point_content"]:
            self.new_data[subsection_index]["point_content"][point_title] = {
                "content": point_content,
                "yaml_paths": [],
            }
        self.new_data[subsection_index]["point_content"][point_title][
            "yaml_paths"
        ].append(yaml_path)

        await self.save_new_data()

    async def save_new_data(self):
        yaml = YAML()
        with open(self.new_yaml_file, "w") as file:
            yaml.dump(self.new_data, file)


async def main():
    yaml_file = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\outline_queries.yaml"
    output_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\search_results"
    api_key_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\keys\api_keys.json"
    email = "bnsoh2@huskers.unl.edu"
    new_yaml_file = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\new_outline_structure.yaml"

    query_processor = QueryProcessor(
        yaml_file, output_folder, api_key_path, email, new_yaml_file
    )
    await query_processor.process_queries()

    logger.info("Code execution completed successfully.")


if __name__ == "__main__":
    asyncio.run(main())


# Contents of ./utils\prompts.py
import re


outline = """ 
A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
1.1	The Global Food Challenge and the Role of Irrigation
	The challenge of feeding a growing population with finite resources
	The role and importance of irrigation in enhancing crop yields and agricultural productivity
	The necessity of scalable water-efficient practices for increasing food demand
1.2. Traditional Irrigation: Limitations and the Need for Automation
	Definition of irrigation scheduling and management
	Historical irrigation management techniques (e.g., manual and timer-based scheduling)
	Limitations: labor-intensive, inefficient water use, less adaptable to changing conditions
	Need for scalable, automated solutions for greater efficiency
1.3. The Emergence of Smart Irrigation Management and IoT
	Contrast between modern and historical irrigation management, emphasizing automation
	Technologies for smart irrigation management (e.g., remote sensing, sensor networks, weather data, and computational algorithms)
	Key point: modern approaches rely on vast data and analysis algorithms
	The role of IoT in collecting vast amounts of data through sensors, data transmission, and tailored networks
	The importance of real-time data for dynamic irrigation systems and automated management at a larger scale
	Challenges hampering the full benefit of IoT: processing diverse data sources, data integration, and lack of integrated data analysis
1.4. Machine Learning and Real-time/Automated Systems in Irrigation Management
	Machine learning (ML) in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation
	ML's potential constrained by manual steps (data interpretation, decision-making on irrigation timing and volume, and system adjustments)
	The need for automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability
	The current fragmented approach in smart irrigation due to focusing on pieces rather than the entire picture
	The need for automating and integrating each section of the irrigation management pipeline for autonomous real-time end-to-end irrigation management
	Full autonomous irrigation management pipeline: sensor/weather data -> transmission -> processing/analysis -> algorithmic decision-making -> automated action
	Benefits of automation: easing bottlenecks in data collection, insight generation, and processing; freeing up labor and fostering scalability
1.5 Interoperability and Standardization: Key Enablers for Seamless Integration
	The critical role of interoperability and standardization in enabling seamless integration across the automated irrigation management system
	Challenges of integrating heterogeneous data sources, devices, and systems
	Standardized protocols and data formats as essential for achieving seamless integration and ensuring compatibility between components
	Existing and emerging standards (e.g., OGC SensorThings API, ISO 11783) and their applicability to real-time irrigation management systems
	1.6. Objectives of the Review
	Primary Objective: To critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
	Specific Objective 1: Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
	Specific Objective 2: Analyze the effectiveness and efficiency of integrated end-to-end automated irrigation systems
	Specific Objective 3: Investigate the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline
	Specific Objective 4: Identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

2. REVIEW METHODOLOGY
	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
	Types of data most applicable to irrigation management and their typical sources
	Different data types (e.g., soil moisture, canopy temperature, weather) and their collection and use
	Considerations: volume, frequency, format, and source of the data
3.2. Edge Computing and Fog Computing
	The potential of edge computing and fog computing in real-time irrigation management
	Benefits of edge computing in reducing latency, enabling real-time decision-making, and reducing reliance on cloud connectivity
	The role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability
3.3. Automation of Data Collection
3.4. Real-Time Data Transmission Protocols and Technologies
	Exploration of MQTT and client-server IoT networks for real-time data transmission
	Comparison of application layer protocols (e.g., XMPP, CoAP, SOAP, HTTP) and their suitability for real-time irrigation management
3.5. Challenges and Solutions in Real-Time Data Transmission
	Obstacles in transmitting data in real-time
	Solutions implemented to address these challenges
3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
	Strategies for collecting and managing VRI data at scale
	Autonomous planning and scheduling of VRI using machine learning and optimization techniques
	Challenges and solutions for implementing VRI in real-time, end-to-end automated irrigation systems
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
	The importance of data quality and preprocessing in the automated irrigation management pipeline
	Techniques for data cleaning, outlier detection, and handling missing or inconsistent data in real-time
	The impact of data quality on the accuracy and reliability of ML models and decision-making processes
4.2. Containerization Strategies for Scalable and Autonomous Deployment
	Examination of containerization technologies (e.g., Docker, Kubernetes) for hosting, running, and scaling ML/data processing modules in the cloud
	Benefits and challenges of containerization in the context of real-time irrigation management
	Scalability and performance optimization techniques, such as parallel processing, distributed computing, and resource allocation strategies
4.3. Deploying ML Models for Data Processing
	Details on the deployment of ML models on cloud platforms for real-time data processing and inference
4.4. Online Learning in the Cloud
	Investigation of the use of online learning models for continuous improvement based on incoming data


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Insights
	Examination of the application of predictive models in generating actionable irrigation insights in real-time
5.2. Automated Application of Insights
	Exploration of strategies for the automated application of ML-generated insights to control irrigation systems without manual intervention
5.3. Interpretability and Explainability of ML Models
	The importance of interpretability and explainability in ML models used for irrigation management
	Techniques for enhancing the transparency and interpretability of ML models, enabling users to understand and trust the generated insights and decisions
	The role of explainable AI (XAI) in building user confidence and facilitating human-machine collaboration in automated irrigation systems

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
6.2. Integration with Existing Irrigation Infrastructure
	Challenges and strategies for integrating automated systems with existing irrigation infrastructure
	Compatibility issues and retrofit solutions for incorporating IoT devices and ML-based decision-making into legacy irrigation systems
	Economic and practical considerations of transitioning from traditional to automated irrigation management
6.3. Integration with Other Precision Agriculture Technologies
	Synergies between automated irrigation and technologies such as remote sensing, variable rate application, and yield mapping, focusing on how they contribute to real-time, automated irrigation management
	Benefits and challenges of integrating these technologies to create a holistic precision agriculture ecosystem

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance
	Strategies for ensuring the robustness and reliability of the system in the face of failures, disruptions, or unexpected events
	The role of redundancy, failover mechanisms, and self-healing capabilities in maintaining system stability and minimizing downtime
	The potential of using AI techniques for anomaly detection, fault diagnosis, and predictive maintenance in real-time irrigation systems
7.2. Importance of Monitoring in Automated Irrigation Systems
	Ensuring system stability, detecting anomalies, and preventing errors from propagating
	Consequences of system failures and the need for robust monitoring
7.3. Advanced Monitoring Techniques
	Remote monitoring using cameras and sensors integrated with the cloud
	Innovative approaches for real-time system health assessment
7.4. Closed-Loop Control and System Security
	Exploring the concept of closed-loop control in autonomous irrigation systems
	Addressing security concerns and potential risks of fully automated systems
	Strategies for managing and mitigating risks in large-scale deployments
CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS
	Detailed case studies showcasing the successful deployment of end-to-end automated irrigation systems in various agricultural settings
	Lessons learned, challenges encountered, and best practices derived from real-world implementations
	Emphasis on how each case study demonstrates the effectiveness and efficiency of integrated, real-time automated irrigation management
CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
	Based on the questions addressed, propose new research directions and unanswered questions
	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems


"""

review_intention = """  
the purpose and intention of this systematic review on automated systems for real-time irrigation management can be interpreted as follows:
Addressing the global food challenge: The review aims to explore how automated, real-time irrigation management systems can contribute to the efficient use of water resources and enhance agricultural productivity to meet the growing demand for food.
Evaluating the current state and future potential: The primary objective is to critically assess the current state of end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. The review also seeks to identify gaps and propose solutions for seamless integration across the automated irrigation management system to achieve fully autonomous, scalable irrigation management.
Examining automation across the entire pipeline: The review intends to systematically analyze the automation of each component of the irrigation management pipeline, from data collection and transmission to processing, analysis, decision-making, and automated action. It aims to investigate the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
Highlighting the role of interoperability and standardization: The review seeks to emphasize the importance of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline. It aims to identify existing and emerging standards and their applicability to real-time irrigation management systems.
Identifying challenges and proposing solutions: The review intends to uncover the challenges associated with implementing real-time, automated irrigation systems, such as data quality, scalability, reliability, and security. It aims to propose solutions and best practices based on the analysis of case studies and real-world implementations.
Guiding future research and innovation: By identifying research gaps and proposing new research questions and hypotheses, the review aims to provide a roadmap for advancing the field of real-time, automated irrigation management. It seeks to encourage collaborative research efforts across disciplines to address the complex challenges of automated irrigation systems.
In summary, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge."""

section_intentions = {
    "1": "INTRODUCTION: Sets the context for the systematic review by highlighting the global food challenge, the critical role of irrigation, and the potential for automation and integration across the irrigation management pipeline.",
    "2": "REVIEW METHODOLOGY: Outlines the question-driven framework used to guide the literature review, presenting key research questions, motivations, and hypotheses for a structured approach to assessing real-time, autonomous irrigation management systems.",
    "3": "DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING: Focuses on the initial stages of the automated irrigation management pipeline, covering data collection, edge and fog computing, real-time data transmission protocols and technologies, and the challenges and solutions associated with real-time data transmission.",
    "4": "AUTOMATED DATA PROCESSING IN THE CLOUD: Examines the importance of data quality and preprocessing in the cloud, containerization strategies for scalable and autonomous deployment, and the deployment of machine learning (ML) models for real-time data processing and inference.",
    "5": "GENERATING AND APPLYING IRRIGATION INSIGHTS: Focuses on the application of ML-generated insights to control irrigation systems without manual intervention, investigating the real-time generation and automated application of actionable irrigation insights, and the importance of interpretability and explainability in ML models.",
    "6": "INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION: Explores the challenges and strategies for integrating automated systems with existing irrigation infrastructure and other precision agriculture technologies, highlighting the importance of interoperability and standardization in enabling seamless communication and compatibility.",
    "7": "MONITORING AND ENSURING SYSTEM RELIABILITY: Focuses on strategies for ensuring the robustness and reliability of the automated irrigation system, including resilience and fault tolerance, advanced monitoring techniques, closed-loop control, and addressing security concerns and risks in large-scale deployments.",
    "8": "CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS: Presents detailed examples of successful deployments of end-to-end automated irrigation systems in various agricultural settings, highlighting lessons learned, challenges encountered, and best practices derived from real-world implementations.",
    "9": "CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS: Summarizes key insights gained from the question-driven review, identifies research gaps, proposes new research directions and unanswered questions for advancing the field, and emphasizes the need for collaborative research efforts and further innovation in real-time, automated irrigation management.",
}


scopus_search_guide = """
Syntax and Operators

Valid syntax for advanced search queries includes:

Field codes (e.g. TITLE, ABS, KEY, AUTH, AFFIL) to restrict searches to specific parts of documents
Boolean operators (AND, OR, AND NOT) to combine search terms
Proximity operators (W/n, PRE/n) to find words within a specified distance - W/n: Finds terms within "n" words of each other, regardless of order. Example: journal W/15 publishing finds articles where "journal" and "publishing" are within two words of each other. - PRE/n: Finds terms in the specified order and within "n" words of each other. Example: data PRE/50 analysis finds articles where "data" appears before "analysis" within three words. - To find terms in the same sentence, use 15. To find terms in the same paragraph, use 50 -
Quotation marks for loose/approximate phrase searches
Braces {} for exact phrase searches
Wildcards (*) to capture variations of search terms
Invalid syntax includes:

Mixing different proximity operators (e.g. W/n and PRE/n) in the same expression
Using wildcards or proximity operators with exact phrase searches
Placing AND NOT before other Boolean operators
Using wildcards on their own without any search terms
Ideal Search Structure

An ideal advanced search query should:

Use field codes to focus the search on the most relevant parts of documents
Combine related concepts using AND and OR
Exclude irrelevant terms with AND NOT at the end
Employ quotation marks and braces appropriately for phrase searching
Include wildcards to capture variations of key terms (while avoiding mixing them with other operators)
Follow the proper order of precedence for operators
Complex searches should be built up systematically, with parentheses to group related expressions as needed. The information from the provided documents on syntax rules and operators should be applied rigorously.

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

Example Advanced Searches

{{
"scopus_queries": [
"TITLE-ABS-KEY(("precision agriculture" OR "precision farming") AND ("machine learning" OR "AI") AND "water")",
"TITLE-ABS-KEY((iot OR "internet of things") AND (irrigation OR watering) AND sensor*)",
"TITLE-ABS-Key(("precision farming" OR "precision agriculture") AND ("deep learning" OR "neural networks") AND "water")",
"TITLE-ABS-KEY((crop W/5 monitor*) AND "remote sensing" AND (irrigation OR water*))",
"TITLE("precision irrigation" OR "variable rate irrigation" AND "machine learning")"
]
}}

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **. 

These example searches demonstrate different ways to effectively combine key concepts related to precision agriculture, irrigation, real-time monitoring, IoT, machine learning and related topics using advanced search operators. They make use of field codes, Boolean and proximity operators, phrase searching, and wildcards to construct targeted, comprehensive searches to surface the most relevant research. The topic focus is achieved through carefully chosen search terms covering the desired themes.
"""

alex_search_guide = """
Syntax and Operators
Valid syntax for advanced alex search queries includes:
Using quotation marks %22%22 for exact phrase matches
Adding a minus sign - before terms to exclude them
Employing the OR operator in all caps to find pages containing either term
Using the site%3A operator to limit results to a specific website
Applying the filetype%3A operator to find specific file formats like PDF, DOC, etc.
Adding the * wildcard as a placeholder for unknown words
`
Invalid syntax includes:
Putting a plus sign + before words (alex stopped supporting this)
Using other special characters like %3F, %24, %26, %23, etc. within search terms
Explicitly using the AND operator (alex's default behavior makes it redundant)

Ideal Search Structure
An effective alex search query should:
Start with the most important search terms
Use specific, descriptive keywords related to irrigation scheduling, management, and precision irrigation
Utilize exact phrases in %22quotes%22 for specific word combinations
Exclude irrelevant terms using the - minus sign
Connect related terms or synonyms with OR
Apply the * wildcard strategically for flexibility
Note:

By following these guidelines and using proper URL encoding, you can construct effective and accurate search queries for alex.

Searches should be concise yet precise, following the syntax rules carefully. 

Example Searches
{{ "alex_queries": [
"https://api.openalex.org/works?search=%22precision+irrigation%22+%2B%22soil+moisture+sensors%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22machine+learning%22+%2B%22irrigation+management%22+%2B%22crop+water+demand+prediction%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22IoT+sensors%22+%2B%22real-time%22+%2B%22soil+moisture+monitoring%22+%2B%22crop+water+stress%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22remote+sensing%22+%2B%22vegetation+indices%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"https://api.openalex.org/works?search=%22wireless+sensor+networks%22+%2B%22precision+agriculture%22+%2B%22variable+rate+irrigation%22+%2B%22irrigation+automation%22&sort=relevance_score:desc&per-page=30"
]}}

These example searches demonstrate how to create targeted, effective alex searches. They focus on specific topics, exclude irrelevant results, allow synonym flexibility, and limit to relevant domains when needed. The search terms are carefully selected to balance relevance and specificity while avoiding being overly restrictive.  By combining relevant keywords, exact phrases, and operators, these searches help generate high-quality results for the given topics.
"""


def remove_illegal_characters(text):
    if text is None:
        return ""
    illegal_chars = re.compile(r"[\000-\010]|[\013-\014]|[\016-\037]")
    return illegal_chars.sub("", str(text))


def get_prompt(template_name, **kwargs):
    prompts = {  # Soom to be deprecated
        "paper_analysis": """<instructions>  
First, carefully read through the full text of the paper provided under <full_text>. Then, analyze the paper's relevance to the specific point mentioned in <point_content> within the context of the overall literature review outline and intention provided.

Your analysis should include:

A concise summary (3-5 sentences) of the key points of the paper as they relate to the outline point.
A detailed explanation of how the specifics of the paper contribute to addressing the point within the larger context and intent of the literature review. Consider the following factors based on the paper type:
Relevance: How directly does the paper address the key issues pertaining to the outline point?
Insight: To what extent does the paper provide novel, meaningful, or valuable information for the point?
Credibility: How reliable, valid, and trustworthy are the paper's findings, methods, or arguments?
Scope: How comprehensive is the paper's coverage of topics relevant to the outline point?
Recency: How up-to-date is the information in the context of the current state of knowledge on the topic?
The two most relevant verbatim quotes from the paper, each no more than 3 sentences, demonstrating its pertinence to the outline point and review. Include the most important quote under "extract_1" and the second most important under "extract_2". If no quotes are directly relevant, leave these blank.
A relevance score between 0 and 1 representing the overall fit of the paper to the outline point and review. Use the following rubric:
0.9-1.0: Exceptionally relevant - Comprehensively addresses all key aspects of the point with highly insightful, reliable, and up-to-date information. A must-include for the review.
0.8-0.89: Highly relevant - Addresses key issues of the point with novel, credible, and meaningful information. Adds substantial value to the review.
0.7-0.79: Very relevant - Directly informs the point with reliable and valuable information, but may have minor limitations in scope, depth, or recency.
0.6-0.69: Moderately relevant - Provides useful information for the point, but has some notable gaps in addressing key issues or limitations in insight, credibility, or timeliness.
0.5-0.59: Somewhat relevant - Addresses aspects of the point, but has significant limitations in scope, depth, reliability, or value of information. May still be worth including.
0.4-0.49: Marginally relevant - Mostly tangential to the main issues of the point, with information of limited insight, credibility, or meaningfulness. Likely not essential.
0.2-0.39: Minimally relevant - Only briefly touches on the point with information that is of questionable value, reliability, or timeliness. Not recommended for inclusion.
0.0-0.19: Not relevant - Fails to address the point or provide any useful information. Should be excluded from the review.

Be uncompromising in assigning the most appropriate score based on a holistic assessment of the paper's merits and limitations.

If the paper is highly relevant (0.8+) to a different outline section, indicate the most applicable subsection under "alternate_section" in the format "1.2". If no other section applies, leave this blank.
List any important limitations of the paper for fully addressing the point and outline, such as limited scope, methodological issues, dated information, or tangential focus. If there are no major limitations, leave this blank.
Provide your analysis in the following JSON format:

<response_format>
{{
"explanation": "",
"extract_1": "",
"extract_2": "",
"relevance_score": 0.0,
"Alternate_section": "",
"limitations": "",
"inline_citation": "",
"apa_citation": ""
}}
</response_format>

Also include a suggested in-line citation for the paper under "inline_citation" in the format (Author, Year), and a full APA style reference under "apa_citation".
Leave any fields blank if not applicable.
</instructions>

<documents> <full_text> {full_text} </full_text> <context> <outline> {outline} </outline>
<review_intention>
{review_intention}
</review_intention>

<point_content>
{point_content}
</point_content>

<subsection_title>

{section_title}
</subsection_title>

<section_title>
{document_title}

</section_title>
</context>
</documents>
""",
        "generate_queries": """
<documents>
<document index="1">
<source>search_query_prompt.txt</source>
<document_content>
<instructions>
Carefully review the provided context, including the overall review intention under <review_intention>, the purpose of the current section within that context explained in <section_intention>, and the specific point that needs to be addressed by the literature search, given in <point_content>.

Your task is to generate a set of 5 highly optimized search queries that would surface the most relevant, insightful, and comprehensive set of research articles to shed light on various aspects of the particular point <point_content> which is under subsection <subsection_title>, while keeping the queries tightly focused around the intentions of the <section_title> and <review_intention>.

The queries should:
- Be thoughtfully crafted to return results that directly address the key issues and nuances of the <point_content>
- Demonstrate creativity and variety in their formulation to capture different dimensions of the topic
- Use precise terminology and logical operators to maintain a high signal-to-noise ratio in the results
- Cover a broad range of potential subtopics, perspectives, and article types related to the <point_content>
- Align closely with the stated goals of the <section_title> and <review_intention> to maximize relevance 
- Adhere strictly and diligently to any specific guidance or requirements provided in <search_guidance>. This is critical!

Provide your response strictly in the following JSON format:
{{
    "*_queries": [
        "query_1",
        "query_2",
        "query_3",
        "query_4",
        "query_5"
    ]
}}

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (\") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

The  platform will be specified in the search guidance. Replace * with the platform name (e.g., scopus_queries, alex_queries). Each query_n should be replaced with a unique, well-formulated search entry according to the instructions in <search_guidance>. No other text should be included. Any extraneous text or deviation from this exact format will result in an unusable output.
</instructions>
<resources>
<review_intention>
{review_intention}
</review_intention>
<section_intention>
{section_intention}
</section_intention>
<point_content>
{point_content}
</point_content>
<section_title>
{section_title}
</section_title>
<subsection_title>
{subsection_title}
</subsection_title>
<search_guidance>
{search_guidance}
</search_guidance>
</resources>
</document_content>
</document>
</documents>
""",
        "rank_papers": """
        <instructions>  
First, carefully read through the full text of the paper provided under <full_text>. Then, analyze the paper's relevance to the specific point mentioned in <point_content> within the context of the overall literature review intention provided and the specific section and subsection of the review.

Your analysis should include:

A concise analysis of how the specifics of the paper contribute to addressing the point within the larger context and intent of the literature review. Consider the following factors based on the paper type:
Relevance: How directly does the paper address the key issues pertaining to the outline point?
Insight: To what extent does the paper provide novel, meaningful, or valuable information for the point?
Credibility: How reliable, valid, and trustworthy are the paper's findings, methods, or arguments?
Scope: How comprehensive is the paper's coverage of topics relevant to the outline point?
Recency: How up-to-date is the information in the context of the current state of knowledge on the topic?
The three most relevant verbatim quotes from the paper, each no more than 3 sentences, demonstrating its pertinence to the outline point and review. Include the most important quote under "verbatim_quote1", the second most important under "verbatim_quote2", and the third under "verbatim_quote3". If no quotes are directly relevant, leave these blank.
A relevance score between 0 and 1 representing the overall fit of the paper to the outline point and review. Use the following rubric:
0.9-1.0: Exceptionally relevant - Comprehensively addresses all key aspects of the point with highly insightful, reliable, and up-to-date information. A must-include for the review.
0.8-0.89: Highly relevant - Addresses key issues of the point with, credible, and meaningful information. Adds substantial value to the review.
0.7-0.79: Very relevant - Directly informs the point with reliable and valuable information, but may have minor limitations in scope, depth, or recency.
0.6-0.69: Moderately relevant - Provides useful information for the point, but has some notable gaps in addressing key issues or limitations in insight, credibility, or timeliness.
0.5-0.59: Somewhat relevant - Addresses aspects of the point, but has significant limitations in scope, depth, reliability, or value of information. May still be worth including.
0.4-0.49: Marginally relevant - Mostly tangential to the main issues of the point, with information of limited insight, credibility, or meaningfulness. Likely not essential.
0.2-0.39: Minimally relevant - Only briefly touches on the point with information that is of questionable value, reliability, or timeliness. Not recommended for inclusion.
0.0-0.19: Not relevant - Fails to address the point or provide any useful information. Should be excluded from the review.

Be uncompromising in assigning the most appropriate score based on a holistic assessment of the paper's merits and limitations.

List any important limitations of the paper for fully addressing the point and outline, such as limited scope, methodological issues, dated information, or tangential focus. If there are no major limitations, leave this blank. Additionally, provide the in-line and apa citations of the paper in the relevant json field. If these are not provided, construct them based on the information in the paper.
Provide your analysis and other responses in the following JSON format:


{{
"analysis": "",
"verbatim_quote1": "",
"verbatim_quote2": "",
"relevance_score": 0.0,
"full_citation": "",
"inline_citation": "",
"limitations": ""
}}

Use double quotation marks around all field names and values.
Separate field/value pairs with a comma.
Do not add any extra characters, line breaks, indentation etc. inside the curly braces.
If a field is not applicable, set its value to an empty string "".
Set "relevance_score" to a numerical value between 0.0 and 1.0, following the provided rubric.
Follow the specified order of fields exactly.

Strict adherence to this JSON format is critical. Any deviation, even a small one like a missing quotation mark or extra space, may result in an unusable output that cannot be processed correctly.
</instructions>

<documents>
<context>
<review_intention>
{review_intention}
</review_intention>

<point_content>
{point_content}
</point_content>

<subsection_title>
{subsection_title}  
</subsection_title>

<section_intention>
{section_intention}
</section_intention>
</context>
<title>
{document_title}
</title>
<full_text>
{full_text}
</full_text>
<abstract>
{abstract}
</abstract>
</documents>
""",
    }
    return prompts[template_name].format(**kwargs)


# Contents of ./utils\research_paper_outline_generator.py
"""
research_paper_outline_generator.py

This module provides a class to generate a YAML-formatted research paper outline from a JSON input file.
"""

import json
import os
import yaml


class ResearchPaperOutlineGenerator:
    def __init__(self, json_file, output_directory):
        with open(json_file) as file:
            self.data = json.load(file)
        self.output_directory = output_directory

    def generate_outline(self):
        outline_data = []
        for i, subsection in enumerate(self.data["subsections"], start=1):
            subsection_data = {
                "index": i,
                "subsection_title": subsection["subsection_title"],
                "points": [],
            }
            for j, point in enumerate(subsection["points"], start=1):
                point_data = {
                    f"Point {j}": {
                        "point_content": point["point_content"],
                        "scopus_queries": self.generate_queries(
                            point["scopus_queries"]
                        ),
                        "alex_queries": self.generate_queries(point["google_queries"]),
                    }
                }
                subsection_data["points"].append(point_data)
            outline_data.append(
                subsection_data
            )  # Move this line inside the subsection loop

        output_file = os.path.join(self.output_directory, "research_paper_outline.yaml")
        with open(output_file, "w") as file:
            yaml.dump({"subsections": outline_data}, file, default_flow_style=False)
        return output_file

    @staticmethod
    def generate_queries(queries):
        query_data = []
        for i, query in enumerate(queries, start=1):
            query_result = []
            for j in range(1, 3):
                query_result.append(
                    {
                        f"Paper {j}": {
                            "title": "",
                            "DOI": "",
                            "full_text": "",
                            "inline_citation": "",
                            "full_citation": "",
                            "publication_year": "",
                            "authors": [],
                            "citation_count": 0,
                            "pdf_link": "",
                            "journal": "",
                            "analysis": "",
                            "verbatim_quote1": "",
                            "verbatim_quote2": "",
                            "verbatim_quote3": "",
                            "relevance_score1": 0,
                            "relevance_score2": 0,
                            "limitations": "",
                            "inline_citation": "",
                            "full_citation": "",
                        }
                    }
                )
            query_data.append(
                {
                    "query_number": i,
                    "query_content": query,
                    "query_result": query_result,
                }
            )
        return query_data


# calling the class
if __name__ == "__main__":
    json_file = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\outline.json"
    output_directory = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3"
    generator = ResearchPaperOutlineGenerator(json_file, output_directory)
    generator.generate_outline()
    print("Research paper outline generated successfully!")


# Contents of ./utils\scopus_search.py
import aiohttp
import asyncio
import json
import time
import logging
import yaml
from hashlib import sha256

from collections import deque
from misc_utils import prepare_text_for_json

# Create a logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Create a console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Create a formatter
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)

# Add the console handler to the logger
logger.addHandler(console_handler)

# Optional: Create a file handler to log messages to a file
file_handler = logging.FileHandler("scopus.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class ScopusSearch:
    def __init__(self, key_path, doi_scraper, output_folder):
        self.load_api_keys(key_path)
        self.base_url = "http://api.elsevier.com/content/search/scopus"
        self.request_times = deque(maxlen=6)
        self.scraper = doi_scraper
        self.output_folder = output_folder

    def load_api_keys(self, key_path):
        try:
            with open(key_path, "r") as file:
                api_keys = json.load(file)
            self.api_key = api_keys["SCOPUS_API_KEY"]
            logger.info("API keys loaded successfully.")
        except FileNotFoundError:
            logger.error(f"API key file not found at path: {key_path}")
        except KeyError:
            logger.error("SCOPUS_API_KEY not found in the API key file.")
        except json.JSONDecodeError:
            logger.error("Invalid JSON format in the API key file.")

    async def search(
        self, query, count=25, view="COMPLETE", response_format="json", max_retries=4
    ):
        headers = {
            "X-ELS-APIKey": self.api_key,
            "Accept": (
                "application/json"
                if response_format == "json"
                else "application/atom+xml"
            ),
        }

        params = {"query": query.replace("\\", ""), "count": count, "view": view}

        retry_count = 0
        while retry_count < max_retries:
            try:
                # Ensure compliance with the rate limit
                while True:
                    current_time = time.time()
                    if (
                        not self.request_times
                        or current_time - self.request_times[0] >= 1
                    ):
                        self.request_times.append(current_time)
                        break
                    else:
                        await asyncio.sleep(0.2)

                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        self.base_url, headers=headers, params=params
                    ) as response:
                        if response.status == 200:
                            logger.info("Scopus API request successful.")
                            if response_format == "json":
                                return await response.json()
                            else:
                                return await response.text()
                        else:
                            logger.warning(
                                f"Scopus API request failed with status code: {response.status}"
                            )
                            return None
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                retry_count += 1
                wait_time = 2**retry_count
                logger.warning(
                    f"Error occurred while making Scopus API request: {e}. Retrying in {wait_time} seconds... (Attempt {retry_count}/{max_retries})"
                )
                await asyncio.sleep(wait_time)  # Exponential backoff

        logger.error(
            f"Max retries ({max_retries}) exceeded. Unable to fetch data from the Scopus API for query: {query}"
        )
        return None

    async def search_and_parse(
        self, query, query_id, response_id, count=25, view="COMPLETE"
    ):
        try:
            results = await self.search(query, count, view, response_format="json")

            if (
                results is None
                or "search-results" not in results
                or "entry" not in results["search-results"]
            ):
                logger.warning(f"No results found for query: {query}")
                return ""
            else:
                parsed_results = []
                for entry in results["search-results"]["entry"]:
                    title = entry.get("dc:title")
                    doi = entry.get("prism:doi")
                    description = entry.get("dc:description")
                    journal = entry.get("prism:publicationName")
                    citation_count = entry.get("citedby-count", "0")
                    authors = [
                        author.get("authname")
                        for author in entry.get("author", [])
                        if author.get("authname") is not None
                    ]

                    full_text = None
                    if doi:
                        logger.info(f"Scraping full text for DOI: {doi}")
                        try:
                            full_text = await self.scraper.get_url_content(doi)
                            full_text = await prepare_text_for_json(full_text)
                            logger.info(
                                f"Full text scraped successfully for DOI: {doi}"
                            )
                        except Exception as e:
                            logger.warning(
                                f"Error occurred while scraping full text for DOI: {doi}. Error: {e}"
                            )
                    if title is not None:
                        parsed_results.append(
                            {
                                "title": title,
                                "doi": doi,
                                "description": description,
                                "journal": journal,
                                "authors": authors,
                                "citation_count": citation_count,
                                "full_text": ">\n" + full_text if full_text else ">",
                                "analysis": ">",
                                "verbatim_quote1": ">",
                                "verbatim_quote2": ">",
                                "verbatim_quote3": ">",
                                "relevance_score1": 0,
                                "relevance_score2": 0,
                                "limitations": ">",
                                "inline_citation": ">",
                                "full_citation": ">",
                            }
                        )

                hashed_filename = self.get_hashed_filename(query, query_id, response_id)
                output_path = self.save_yaml(parsed_results, hashed_filename)
                logger.info(f"Results saved successfully to: {output_path}")
                return output_path
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {query}. Error: {e}"
            )
            return ""

    def get_hashed_filename(self, query, query_id, response_id):
        hash_input = f"{query}_{query_id}_{response_id}"
        hashed_filename = sha256(hash_input.encode()).hexdigest()
        return f"{hashed_filename}.yaml"

    def save_yaml(self, data, filename):
        try:
            output_path = self.output_folder / filename
            # Create the output folder if it doesn't exist
            self.output_folder.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, default_flow_style=False, allow_unicode=True)
            logger.info(f"YAML file saved successfully: {output_path}")
            return output_path.absolute()
        except Exception as e:
            logger.error(
                f"An error occurred while saving YAML file: {filename}. Error: {e}"
            )
            return ""


# Contents of ./utils\web_scraper.py
import asyncio
import random
import aiohttp
from fake_useragent import UserAgent
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    WebDriverException,
)


class WebScraper:
    def __init__(self, max_concurrent_tasks=120):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)

    async def scrape_url(self, url, max_retries=3):
        options = uc.ChromeOptions()
        options.add_argument("--blink-settings=imagesEnabled=false")
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--disable-extensions")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        retry_count = 0
        while retry_count < max_retries:
            try:
                driver = uc.Chrome(options=options)
                await self.update_user_agent(driver)
                await self.navigate_to_url(driver, url)
                content = await self.extract_text_content(driver)
                if any(
                    s in content
                    for s in [
                        "! There was a problem providing the content you requested Please contact us via our support center for more information and provide the details below.",
                        "The requested URL was rejected. Please consult with your administrator.",
                    ]
                ):
                    return ""
                print(f"Successfully scraped URL: {url}")
                return content
            except (aiohttp.ClientConnectorError, OSError) as e:
                print(f"Network error occurred while scraping URL: {url}")
                print(f"Error details: {str(e)}")
                retry_count += 1
                await asyncio.sleep(
                    random.uniform(1, 5)
                )  # Add a random delay between retries
            except Exception as e:
                print(f"Error occurred while scraping URL: {url}")
                print(f"Error details: {str(e)}")
                return ""
            finally:
                try:
                    driver.quit()
                except Exception:
                    pass

        print(f"Max retries exceeded for URL: {url}")
        return ""

    async def get_url_content(self, url):
        async with self.semaphore:
            return await self.scrape_url(url)

    async def update_user_agent(self, driver):
        ua = UserAgent()
        user_agent = ua.random
        driver.execute_cdp_cmd(
            "Network.setUserAgentOverride", {"userAgent": user_agent}
        )

    async def navigate_to_url(self, driver, url, retry=3):
        if not url.startswith("http"):
            url = f"https://doi.org/{url}"
        await self.update_user_agent(driver)
        try:
            driver.get(url)
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except TimeoutException:
                pass
            await asyncio.sleep(1)
        except WebDriverException as e:
            if retry > 0:
                print(f"Retrying URL: {url}")
                await self.navigate_to_url(driver, url, retry - 1)
            else:
                raise e

    async def extract_text_content(self, driver):
        try:
            body = driver.find_element(By.TAG_NAME, "body")
            text_content = body.text
            return text_content.replace("\n", " ")
        except (NoSuchElementException, TimeoutException):
            return ""


# Contents of ./utils\yaml_iterator.py
"""
   Comprehensive guide on how to use the YAML iterator with the provided YAML structure.

   Initializing the IrrigationData class:
   Create an instance of the IrrigationData class by passing the path to your YAML file as a parameter.
   Example:
   >>> data = IrrigationData("path/to/your/yaml/file.yaml")

   Loading the YAML data:
   Before you can iterate through the data or perform any operations, you need to load the YAML data from the file.
   Use the load_data method to asynchronously load the data.
   Example:
   >>> await data.load_data()

   Iterating through the entire YAML structure:
   Use the iterate_data method to iterate through the entire YAML structure asynchronously.
   The method yields the subsection, point title, query type, query ID, response ID, and response for each response in the YAML structure.
   Example:
   >>> async for subsection, point_title, query_type, query_id, response_id, response in data.iterate_data():
   ...     print(f"Subsection: {subsection['subsection_title']}")
   ...     print(f"Point: {point_title}")
   ...     print(f"Query Type: {query_type}")
   ...     print(f"Query ID: {query_id}")
   ...     print(f"Response ID: {response_id}")
   ...     print(f"Response: {response}")

   Updating specific fields of a response:
   Use the update_response method to update specific fields of a response.
   Pass the query ID, response ID, field name, and new value as parameters to the method.
   The method will search for the corresponding response in the YAML structure and update the specified field with the provided value.
   Example:
   >>> await data.update_response(query_id, response_id, "DOI", "10.1234/example")
   >>> await data.update_response(query_id, response_id, "pdf_link", "https://example.com/paper.pdf")

   Saving the updated data:
   After making any updates to the responses, you need to save the modified data back to the YAML file.
   The update_response method automatically saves the data after each update.
   If you want to manually save the data, you can use the save_data method.
   Example:
   >>> await data.save_data()

   Combining iteration and updating:
   You can combine the iteration and updating operations to process and modify the responses in a single loop.
   Example:
   >>> async for subsection, point_title, query_type, query_id, response_id, response in data.iterate_data():
   ...     # Perform operations on the response
   ...     # ...
   ...
   ...     # Update specific fields of the response
   ...     await data.update_response(query_id, response_id, "DOI", "10.1234/example")
   ...     await data.update_response(query_id, response_id, "pdf_link", "https://example.com/paper.pdf")

   Accessing specific subsections, points, queries, or responses:
   If you need to access specific subsections, points, queries, or responses, you can use conditional statements within the iteration loop.
   Example (accessing a specific subsection):
   >>> async for subsection, point_title, query_type, query_id, response_id, response in data.iterate_data():
   ...     if subsection["subsection_title"] == "Desired Subsection":
   ...         # Perform operations on the desired subsection
   ...         # ...

   Handling exceptions:
   If any exceptions occur during the iteration or updating process, you can wrap the relevant code in a try-except block to handle the exceptions gracefully.
   Example:
   >>> try:
   ...     async for subsection, point_title, query_type, query_id, response_id, response in data.iterate_data():
   ...         # Perform operations
   ...         # ...
   ... except Exception as e:
   ...     print(f"An error occurred: {str(e)}")

   Closing the loop:
   After you have finished iterating through the data and performing any necessary operations, the loop will automatically end.
   You can perform any post-processing or cleanup tasks after the loop.

   Remember to use the async/await syntax appropriately when working with asynchronous methods and loops.
   """

import asyncio
import aiofiles
import shutil
import tempfile
import os
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
file_handler = logging.FileHandler("yaml_iterator.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

from ruamel.yaml import YAML


class IrrigationData:
    def __init__(self, yaml_file):
        self.yaml_file = yaml_file
        self.yaml = YAML()
        self.data = None

    async def load_data(self):
        try:
            async with aiofiles.open(self.yaml_file, "r") as file:
                content = await file.read()
                self.data = self.yaml.load(content)
        except Exception as e:
            logger.exception("An error occurred while loading YAML data.")
            raise

    async def save_data(self):
        try:
            # Create a temporary file to write the updated data
            with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
                self.yaml.dump(self.data, temp_file)
                temp_file_path = temp_file.name

            # Replace the original file with the temporary file
            shutil.move(temp_file_path, self.yaml_file)
            logger.info("Saved updated YAML data.")
        except Exception as e:
            logger.exception("An error occurred while saving YAML data.")
            # Attempt to restore from the backup file
            await self.restore_from_backup()
            raise

    async def save_data_with_backup(self):
        try:
            # Create a backup of the original YAML file
            backup_file = f"{self.yaml_file}.bak"
            shutil.copy2(self.yaml_file, backup_file)
            logger.info(f"Created backup of YAML file: {backup_file}")

            # Save the updated data
            await self.save_data()
        except Exception as e:
            logger.exception("An error occurred while saving YAML data with backup.")
            raise

    async def restore_from_backup(self):
        backup_file = f"{self.yaml_file}.bak"
        if os.path.exists(backup_file):
            shutil.move(backup_file, self.yaml_file)
            logger.info(f"Restored YAML file from backup: {backup_file}")
        else:
            logger.warning("No backup file found. Unable to restore.")

    async def iterate_data(self):
        for subsection in self.data["subsections"]:
            logger.info(f"Processing subsection: {subsection}")
            for point in subsection["points"]:
                point_title = next(iter(point))
                point_content = point[point_title]["point_content"]
                logger.info(f"Processing point: {point_title} - {point_content}")
                point_data = point[point_title]
                query_types = [
                    key for key in point_data.keys() if key.endswith("_queries")
                ]
                for query_type in query_types:
                    queries = point_data.get(query_type, [])
                    for query_mother in queries:
                        query_id = query_mother["query_id"]
                        query = query_mother["query"]
                        logger.info(f"Processing query: {query_id} - {query}")
                        for response in query_mother.get("responses", []):
                            response_id = response["response_id"]
                            yield subsection, point_title, point_content, query_type, query_id, response_id, response, query

    async def update_response(
        self,
        subsection_index,
        point_title,
        point_content,
        query_id,
        response_id,
        field,
        value,
    ):
        subsection = self.data["subsections"][subsection_index]
        for point in subsection["points"]:
            if (
                point_title in point
                and point[point_title]["point_content"] == point_content
            ):
                logger.info(f"Found matching point: {point_title} - {point_content}")
                point_data = point[point_title]
                query_types = [
                    key for key in point_data.keys() if key.endswith("_queries")
                ]
                for query_type in query_types:
                    queries = point_data.get(query_type, [])
                    for query in queries:
                        if query["query_id"] == query_id:
                            logger.info(f"Found matching query ID: {query_id}")
                            for response in query.get("responses", []):
                                if response["response_id"] == response_id:
                                    logger.info(
                                        f"Found matching response ID: {response_id}"
                                    )
                                    response[field] = value
                                    await self.save_data()
                                    return


# async def main():
#     data = IrrigationData(
#         r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Coding Projects\Automated_Lit_Revs\documents\section3\outline_queries.yaml"
#     )
#     await data.load_data()

#     async for (
#         subsection,
#         point_title,
#         query_type,
#         query_id,
#         response_id,
#         response,
#         query,
#     ) in data.iterate_data():
#         print(f"Subsection: {subsection['subsection_title']}")
#         print(f"Point: {point_title}")
#         print(f"Query Type: {query_type}")
#         print(f"Query ID: {query_id}")
#         print(f"Response ID: {response_id}")
#         print(f"Response: {response}")
#         print(f"Query: {query}")

#         # Update specific fields of the response
#         # await data.update_response(query_id, response_id, "DOI", "10.1234/example")
#         # await data.update_response(
#         #     query_id, response_id, "pdf_link", "https://example.com/paper.pdf"
#         # )


# asyncio.run(main())


# Contents of ./utils\__init__.py
"""
__init__.py
This file ensures that the 'utils' directory is treated as a Python module.
"""

"""
The YAML structure provided consists of a single top-level key named subsections, which contains a list of items. Each item in this list represents a Point, identified numerically (Point 1, Point 2, etc.). Below is a detailed breakdown of the hierarchical structure:

Top Level

subsections: A list containing items, each of which represents a subsection of content.
Second Level (Points)

Each item within subsections is a Point, denoted by - points: and further identified by a title (e.g., Point 1, Point 2, etc.).
Third Level (Content within Points)

Each Point contains:
google_queries: A list of dictionaries. Each dictionary represents a Google search query and contains a query string, a query_id string, and a responses list.
point_content: A string that describes the content or focus of the Point.
alex_queries: A list of dictionaries similar to google_queries. Each dictionary represents a alex search query and contains a query string, a query_id string, and a responses list.
Fourth Level (Query Responses)

Within each google_queries and alex_queries dictionary, the responses list contains dictionaries, each representing an individual response to the query. These dictionaries include keys such as DOI, authors, citation_count, full_citation, full_text, inline_citation, journal, pdf_link, publication_year, response_id, and title, most of which are strings or lists, and some may be integers.
This structure is repeated for each Point within the subsections. The Points are numbered sequentially and each contains its unique set of Google and alex queries along with their respective responses and a brief content description. Each query within the google_queries and alex_queries lists is identified by a unique query_id and contains multiple responses, each response structured consistently across the dataset.
"""

"""The YAML structure represents a hierarchical organization of data related to irrigation management. It consists of multiple levels of nesting, with each level providing more specific information or details.

At the top level, we have the "subsections" key, which contains a list of subsections. Each subsection represents a high-level topic or category within the overall subject of irrigation management.

Within each subsection, there is a "points" key, which holds a list of points. Each point represents a specific aspect, concept, or item related to the subsection it belongs to. Points are the main units of information within the YAML structure.

Each point is represented as a dictionary with a single key-value pair. The key of this pair is the title or name of the point, providing a brief description or identifier for that particular point. The value associated with the point title is another dictionary that contains further details and information related to that point.

Inside each point dictionary, there are two optional keys: "google_queries" and "alex_queries". These keys represent different types of queries or searches associated with the point.

The "google_queries" key, if present, contains a list of dictionaries representing Google search queries related to the point. Each dictionary within this list corresponds to a specific Google query and includes the following information:
"query": The actual query string used for the Google search.
"query_id": A unique identifier for the query, prefixed with "google_".
"responses": A list of dictionaries representing the search results or responses obtained from the Google query. Each response dictionary contains various fields such as "DOI", "authors", "citation_count", "full_citation", "full_text", "inline_citation", "journal", "pdf_link", "publication_year", "response_id", and "title".
Similarly, the "alex_queries" key, if present, contains a list of dictionaries representing alex search queries related to the point. Each dictionary within this list corresponds to a specific alex query and follows a structure similar to the Google queries, with fields like "query", "query_id", and "responses".
The "point_content" key within each point dictionary provides a brief description or summary of the content or focus of that particular point.

In the code, the process_queries function traverses this YAML structure to process the queries and retrieve relevant information. It starts by iterating over the subsections, then moves on to the points within each subsection. For each point, it checks for the presence of "google_queries" and "alex_queries" keys. If found, it processes the corresponding queries using the get_scholar_data function (for Google queries) or performs alex query processing (not shown in the code snippet). The retrieved results are stored back into the "responses" field of each query dictionary.

The hierarchy flows from subsections to points, and within each point, it branches into Google queries and alex queries. The queries are processed individually, and their results are stored within the respective query dictionaries, maintaining the overall structure and organization of the YAML data.

This hierarchical and nested structure allows for a clear and organized representation of the data, enabling easy access and retrieval of specific information related to irrigation management based on subsections, points, and associated queries."""
