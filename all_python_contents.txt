

# Contents of .\analyze_papers.py
from misc_utils import get_api_keys
import asyncio
import logging
import re
from llm_api_handler import LLM_APIHandler
from prompts import get_prompt
import aiohttp
import json

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
file_handler = logging.FileHandler("paper_ranker.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class PaperRanker:
    def __init__(self, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)
        self.max_retries = max_retries

    async def process_query(self, query_key, query_data, point_context):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="rank_papers",
                full_text=query_data.get("full_text", ""),
                point_context=point_context,
                query_rationale=query_data.get("query_rationale", ""),
            )
            try:
                print(f"Processing queries for {point_context}...")
                response = await self.llm_api_handler.generate_cohere_content(prompt)
                print(f"Response: {response}")
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping query."
                    )
                    return None

                try:
                    # Extract the relevance score using the specified token format
                    relevance_score_match = re.search(
                        r"<<relevance>>(\d+\.\d+)<<relevance>>",
                        response,
                    )

                    if relevance_score_match:
                        relevance_score_str = relevance_score_match.group(1)
                        try:
                            relevance_score = float(relevance_score_str)
                            if relevance_score > 0.5:
                                logger.debug(f"Successfully processed query.")
                                return {
                                    "DOI": query_data.get("DOI", ""),
                                    "title": query_data.get("title", ""),
                                    "analysis": response,
                                    "relevance_score": relevance_score,
                                }
                            else:
                                logger.debug(
                                    f"Relevance score {relevance_score} is below the threshold. Skipping query."
                                )
                                return None
                        except ValueError:
                            logger.warning(
                                f"Extracted relevance score '{relevance_score_str}' is not a valid float. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"No relevance score found between <|relevance|> tokens in the response for query {query_key}. Response: {response}"
                        )
                        retry_count += 1
                except Exception as e:
                    logger.warning(
                        f"Error extracting relevance score for query {query_key}: {str(e)}. Retrying..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing query {query_key}: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for query {query_key}. Skipping query.")
        return None

    async def process_queries(self, input_json, point_context):
        tasks = []
        for query_key, query_data in input_json.items():
            task = asyncio.create_task(
                self.process_query(query_key, query_data, point_context)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        output_json = {}
        for query_key, result in zip(input_json.keys(), results):
            if result and isinstance(result, dict):
                output_json[query_key] = result

        return output_json


async def main(input_json, point_context):
    async with aiohttp.ClientSession() as session:
        ranker = PaperRanker(session)
        logger.info("Starting paper ranking process...")
        output_json = await ranker.process_queries(input_json, point_context)
        logger.info("Paper ranking process completed.")
        return output_json


if __name__ == "__main__":
    input_json = {
        "query_1": {
            "DOI": "https://doi.org/10.1007/bf00281114",
            "authors": ["Jarrett Rj"],
            "citation_count": 156,
            "journal": "Diabetologia",
            "pdf_link": "https://link.springer.com/content/pdf/10.1007%2FBF00281114.pdf",
            "publication_year": 1984,
            "title": "Type 2 (non-insulin-dependent) diabetes mellitus and coronary heart disease ? chicken, egg or neither?",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query aims to understand the relationship between Type 2 diabetes and coronary heart disease in chickens.",
        },
        "query_2": {
            "DOI": "https://doi.org/10.1001/jamainternmed.2019.6969",
            "authors": [
                "Victor W. Zhong",
                "Linda Van Horn",
                "Philip Greenland",
                "Mercedes R. Carnethon",
                "Hongyan Ning",
                "John T. Wilkins",
                "Donald M. Lloyd‐Jones",
                "Norrina B. Allen",
            ],
            "citation_count": 221,
            "journal": "JAMA internal medicine",
            "pdf_link": "https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf",
            "publication_year": 2020,
            "title": "Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query investigates the associations between different types of meat intake, including poultry, and cardiovascular disease and mortality.",
        },
        "query_3": {
            "DOI": "https://doi.org/10.1016/s0034-5288(18)33737-8",
            "authors": ["S.F. Cueva", "H. Sillau", "Abel Valenzuela", "H. P. Ploog"],
            "citation_count": 181,
            "journal": "Research in Veterinary Science/Research in veterinary science",
            "publication_year": 1974,
            "title": "High Altitude Induced Pulmonary Hypertension and Right Heart Failure in Broiler Chickens",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query focuses on the effects of high altitude on pulmonary hypertension and right heart failure specifically in broiler chickens.",
        },
        "query_4": {
            "DOI": "https://doi.org/10.2307/1588087",
            "authors": ["Sherwin A. Hall", "Nicanor Machicao"],
            "citation_count": 58,
            "journal": "Avian diseases",
            "publication_year": 1968,
            "title": "Myocarditis in Broiler Chickens Reared at High Altitude",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query examines myocarditis in broiler chickens reared at high altitude, providing insight into heart disease in this specific context.",
        },
        "query_5": {
            "DOI": "https://doi.org/10.1038/srep14727",
            "authors": [
                "Huaguang Lu",
                "Yi Tang",
                "Patricia A. Dunn",
                "Eva Wallner-Pendleton",
                "Lin Lin",
                "Eric A. Knoll",
            ],
            "citation_count": 82,
            "journal": "Scientific reports",
            "pdf_link": "https://www.nature.com/articles/srep14727.pdf",
            "publication_year": 2015,
            "title": "Isolation and molecular characterization of newly emerging avian reovirus variants and novel strains in Pennsylvania, USA, 2011–2014",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query looks at the isolation and characterization of new avian reovirus variants and strains, which may have implications for chicken health.",
        },
    }
    point_context = "Heart disease in chickens."

    output_json = asyncio.run(main(input_json, point_context))
    print(json.dumps(output_json, indent=2))


# Contents of .\app.py
import sys
import os
import json
import logging
import aiohttp
from fastapi import FastAPI, Form, HTTPException
from fastapi.responses import HTMLResponse
from typing import List

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from get_search_queries import QueryGenerator
from scopus_search import ScopusSearch
from analyze_papers import PaperRanker
from synthesize_results import QueryProcessor
from web_scraper import WebScraper
from core_search import CORESearch
from misc_utils import get_api_keys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("research_query_processor.log"),
        logging.StreamHandler(sys.stdout),
    ],
)

app = FastAPI()


class ResearchQueryProcessor:
    def __init__(self):
        self.api_keys = get_api_keys()

    async def chatbot_response(self, message: str) -> str:
        async with aiohttp.ClientSession() as session:
            responses = []
            query_generator = QueryGenerator(session)
            search_queries = await query_generator.generate_queries(message)

            responses.append("Generating search queries...\n")
            responses.append("Searching in CORE...\n")
            core_search = CORESearch(max_results=5)
            search_results = await core_search.search_and_parse_json(search_queries)

            responses.append("Analyzing papers...\n")
            paper_ranker = PaperRanker(session)
            analyzed_papers = await paper_ranker.process_queries(
                search_results, message
            )

            responses.append("Synthesizing results...\n")
            query_processor = QueryProcessor(session)
            synthesized_results = await query_processor.process_query(
                message, analyzed_papers
            )

            final_response = "\n".join(responses) + f"{synthesized_results}"
            return final_response


processor = ResearchQueryProcessor()


@app.get("/", response_class=HTMLResponse)
async def get_root():
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Literature Review Agent</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            textarea { width: 300px; height: 100px; }
            #result { white-space: pre-wrap; margin-top: 20px; }
        </style>
    </head>
    <body>
        <h1>Literature Review Agent</h1>
        <form id="queryForm">
            <textarea id="queryText"></textarea><br>
            <button type="button" onclick="submitQuery()">Submit</button>
        </form>
        <div id="result"></div>

        <script>
            async function submitQuery() {
                const query = document.getElementById('queryText').value;
                const response = await fetch('/process_query/', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
                    body: 'query=' + encodeURIComponent(query)
                });
                const result = await response.json();
                document.getElementById('result').textContent = result.message;
            }
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)


@app.post("/process_query/")
async def process_query(query: str = Form(...)):
    try:
        result = await processor.chatbot_response(query)
        return {"message": result}
    except Exception as e:
        logging.error(f"Error processing query: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal Server Error")


# Contents of .\copy_all_code.py
import os


def copy_py_to_txt(output_file, folder):
    with open(output_file, "w", encoding="utf-8") as outfile:
        for filename in os.listdir(folder):
            if filename.endswith(".py"):
                file_path = os.path.join(folder, filename)
                with open(file_path, "r", encoding="utf-8") as infile:
                    outfile.write(f"\n\n# Contents of {file_path}\n")
                    outfile.write(infile.read())


def main():
    current_folder = "."
    output_filename = "all_python_contents.txt"
    copy_py_to_txt(output_filename, current_folder)
    print(f"All .py file contents copied to {output_filename}")


if __name__ == "__main__":
    main()


# Contents of .\core_search.py
import aiohttp
import asyncio
import json
import logging
from misc_utils import get_api_keys

# Create a logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Create a console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Create a formatter
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)

# Add the console handler to the logger
logger.addHandler(console_handler)

# Create a file handler to log messages to a file
file_handler = logging.FileHandler("core_search.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class CORESearch:
    def __init__(self, max_results):
        self.api_keys = get_api_keys()
        self.base_url = "https://api.core.ac.uk/v3"
        self.max_results = max_results
        self.core_api_key = self.api_keys["CORE_API_KEY"]

    async def search(self, search_query):
        headers = {
            "Authorization": f"Bearer {self.core_api_key}",
            "Accept": "application/json",
        }

        params = {
            "q": search_query,
            "limit": self.max_results,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/search/works", headers=headers, json=params
            ) as response:
                if response.status == 200:
                    logger.info("CORE API request successful.")
                    response_json = await response.json()
                    return response_json
                else:
                    logger.warning(
                        f"CORE API request failed with status code: {response.status}"
                    )
                    return None

    async def search_and_parse(self, query):
        try:
            search_query = query["search_query"]
            response = await self.search(search_query)

            if response is None:
                logger.warning(f"Empty API response for query: {search_query}")
                return {}

            results = response.get("results", [])
            parsed_result = {}

            if results:
                print(results)
                entry = results[0]
                parsed_result = {
                    "DOI": entry.get("doi", ""),
                    "authors": [author["name"] for author in entry.get("authors", [])],
                    "citation_count": entry.get("citationCount", 0),
                    "journal": entry.get("publisher", ""),
                    "pdf_link": entry.get("downloadUrl", ""),
                    "publication_year": entry.get("publicationYear"),
                    "title": entry.get("title", ""),
                    "full_text": entry.get("fullText", ""),
                    "query_rationale": query["query_rationale"],
                }

            return parsed_result
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {search_query}. Error: {e}"
            )
            return {}

    async def search_and_parse_json(self, input_json):
        try:
            updated_json = {}
            for query_id, query in input_json.items():
                parsed_result = await self.search_and_parse(query)
                updated_json[query_id] = parsed_result
            return updated_json
        except Exception as e:
            logger.error(
                f"An error occurred while processing the input JSON. Error: {e}"
            )
            return {}


async def main():
    max_results = 1

    core_search = CORESearch(max_results)

    input_json = {
        "query_1": {
            "search_query": "climate change, water resources",
            "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
        },
        "query_2": {
            "search_query": "water scarcity, (hydrologist OR water expert)",
            "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
        },
        "query_3": {
            "search_query": "sea level rise, coastal erosion",
            "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
        },
        "query_4": {
            "search_query": "water conservation, climate change mitigation, environmental studies",
            "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
        },
        "query_5": {
            "search_query": "glacier melting, cryosphere",
            "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
        },
    }

    updated_json = await core_search.search_and_parse_json(input_json)

    # Remove the "full_text" key from each query result
    for query_result in updated_json.values():
        query_result.pop("full_text", None)

    print(json.dumps(updated_json, indent=2))


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\get_search_queries.py
import json
import asyncio
import aiohttp

from llm_api_handler import LLM_APIHandler
from prompts import get_prompt, core_search_guide
from misc_utils import get_api_keys


class QueryGenerator:
    def __init__(self, session):
        self.api_keys = get_api_keys()
        self.session = session
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)

    async def generate_queries(self, user_query):
        # Generate the prompt from the user's query
        prompt = get_prompt(
            template_name="generate_queries",
            user_query=user_query,
            search_guidance=core_search_guide,
        )

        # Obtain the model's response for the generated prompt
        response = await self.llm_api_handler.generate_openai_content(prompt)

        # Parse and return the response
        return self.parse_response(response)

    def parse_response(self, response):
        try:
            # Attempt to find and extract the JSON structured data
            start_index = response.find("{")
            end_index = response.rfind("}") + 1
            if start_index != -1 and end_index != -1:
                json_string = response[start_index:end_index]
                return json.loads(json_string)
            else:
                print("No valid JSON object found.")
                return {}
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON: {e}")
            return {}


async def main():
    async with aiohttp.ClientSession() as session:
        processor = QueryGenerator(session)
        user_query = "Impact of climate change on global water resources"
        queries = await processor.generate_queries(user_query)
        print(json.dumps(queries, indent=2))


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\gunicorn.conf.py
import sys
import os

if sys.platform == "win32":
    os.environ.setdefault("GUNICORN_CMD_ARGS", "--log-level=debug")


# Contents of .\llm_api_handler.py
import asyncio
import aiohttp
import logging
import anthropic
import backoff
import tiktoken
import time
import json
import cohere
import requests
import random
from misc_utils import get_api_keys
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler("llm_handler.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


def count_tokens(text, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(text))
    return num_tokens


def clip_prompt(prompt, max_tokens, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    tokens = encoding.encode(prompt)
    if len(tokens) > max_tokens:
        clipped_tokens = tokens[:max_tokens]
        clipped_prompt = encoding.decode(clipped_tokens)
        return clipped_prompt
    return prompt


class RateLimiter:
    def __init__(self, rps, window_size):
        self.rps = rps
        self.window_size = window_size
        self.window_start = time.monotonic()
        self.request_count = 0
        self.semaphore = asyncio.Semaphore(rps)

    async def acquire(self):
        current_time = time.monotonic()
        elapsed_time = current_time - self.window_start
        if elapsed_time > self.window_size:
            self.window_start = current_time
            self.request_count = 0
        if self.request_count >= self.rps:
            await asyncio.sleep(self.window_size - elapsed_time)
            self.window_start = time.monotonic()
            self.request_count = 0
        self.request_count += 1
        await self.semaphore.acquire()

    def release(self):
        self.semaphore.release()


class LLM_APIHandler:
    def __init__(self, api_keys, session, rps=0.5, window_size=60):
        self.set_api_keys(api_keys)
        self.claude_rate_limiter = RateLimiter(rps, window_size)
        self.openai_rate_limiter = RateLimiter(rps, window_size)
        self.cohere_rate_limiter = RateLimiter(75, 60)  # 75 calls per minute
        self.llama_rate_limiter = RateLimiter(rps, window_size)
        self.qwen_rate_limiter = RateLimiter(rps, window_size)
        self.claude_client = anthropic.Anthropic(api_key=self.claude_api_key)
        self.cohere_client = cohere.Client(self.cohere_api_key)
        self.session = session

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()  # Close the session when done

    def set_api_keys(self, api_keys):
        self.claude_api_key = api_keys["CLAUDE_API_KEY"]
        self.openai_api_key = api_keys["OPENAI_API_KEY"]
        self.cohere_api_key = api_keys["COHERE_API_KEY"]
        self.together_api_key = api_keys["TOGETHER_API_KEY"]

    @backoff.on_exception(
        backoff.expo,
        (anthropic.APIError, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_opus_content(
        self,
        prompt,
        system_prompt=None,
        model="claude-3-opus-20240229",
        max_tokens=3000,
    ):
        await self.claude_rate_limiter.acquire()
        try:
            if model not in [
                "claude-3-opus-20240229",
                "claude-3-sonnet-20240229",
                "claude-3-haiku-20240307",
            ]:
                raise ValueError(f"Invalid model: {model}")
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            messages = [{"role": "user", "content": clipped_prompt}]
            if system_prompt is None:
                system_prompt = "Directly fulfill the user's request without preamble, paying very close attention to all nuances of their instructions."
            try:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=messages,
                )
                return response.content[0].text
            except anthropic.APIError as e:
                logger.error(
                    f"Max retries reached. Unable to generate content with Claude API. Error: {e}. Moving on."
                )
                return None
        finally:
            self.claude_rate_limiter.release()

    @backoff.on_exception(
        backoff.expo,
        (anthropic.APIError, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_haiku_content(
        self,
        prompt,
        system_prompt=None,
        model="claude-3-haiku-20240307",
        max_tokens=3000,
    ):
        await self.claude_rate_limiter.acquire()
        try:
            if model not in [
                "claude-3-opus-20240229",
                "claude-3-sonnet-20240229",
                "claude-3-haiku-20240307",
            ]:
                raise ValueError(f"Invalid model: {model}")
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            messages = [{"role": "user", "content": clipped_prompt}]
            if system_prompt is None:
                system_prompt = "Directly fulfill the user's request without preamble, paying very close attention to all nuances of their instructions."
            try:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=messages,
                )
                return response.content[0].text
            except anthropic.APIError as e:
                logger.error(
                    f"Max retries reached. Unable to generate content with Claude API. Error: {e}. Moving on."
                )
                return None
        finally:
            self.claude_rate_limiter.release()

    async def generate_cohere_content(
        self,
        prompt,
        model="command-r",
        temperature=0.3,
        prompt_truncation="AUTO",
        connectors=None,
    ):
        await self.cohere_rate_limiter.acquire()
        try:
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            response = self.cohere_client.chat(
                model=model,
                message=clipped_prompt,
                temperature=temperature,
                chat_history=[],
                prompt_truncation=prompt_truncation,
                connectors=connectors,
            )
            return response.text
        except cohere.CohereError as e:
            logger.error(
                f"Unable to generate content with Cohere API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.cohere_rate_limiter.release()

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_llama_content(
        self,
        prompt,
        max_tokens=1024,
        temperature=0.5,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
    ):
        endpoint = "https://api.together.xyz/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.together_api_key}",
        }
        clipped_prompt = clip_prompt(prompt, max_tokens=5000)
        messages = [{"content": clipped_prompt, "role": "user"}]
        data = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "stop": ["<|eot_id|>"],
            "messages": messages,
        }

        await self.llama_rate_limiter.acquire()  # Acquire the rate limiter
        try:
            async with self.session.post(
                endpoint, json=data, headers=headers
            ) as response:
                response.raise_for_status()
                result = await response.json()
                content = result["choices"][0]["message"]["content"]
                return content
        except (requests.exceptions.RequestException, ValueError, KeyError) as e:
            logger.error(
                f"Unable to generate content with Llama API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.llama_rate_limiter.release()  # Release the rate limiter

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_qwen_content(
        self,
        prompt,
        max_tokens=2048,
        temperature=0.1,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
    ):
        endpoint = "https://api.together.xyz/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.together_api_key}",
        }
        clipped_prompt = clip_prompt(prompt, max_tokens=25000)
        messages = [{"content": clipped_prompt, "role": "user"}]
        data = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "stop": ["<|im_end|>", "<|im_start|>"],
            "messages": messages,
        }

        await self.qwen_rate_limiter.acquire()  # Acquire the rate limiter
        try:
            async with self.session.post(
                endpoint, json=data, headers=headers
            ) as response:
                response.raise_for_status()
                result = await response.json()
                content = result["choices"][0]["message"]["content"]
                return content
        except (requests.exceptions.RequestException, ValueError, KeyError) as e:
            logger.error(
                f"Unable to generate content with Llama API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.qwen_rate_limiter.release()  # Release the rate limiter

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_openai_content(self, prompt, model="gpt-4-turbo"):
        client = AsyncOpenAI(api_key=self.openai_api_key)

        # Format the single prompt string into the correct structure expected by the API
        messages = [{"role": "user", "content": prompt}]

        try:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
                top_p=1,
                n=1,
                stream=False,
                max_tokens=None,
                presence_penalty=0,
                frequency_penalty=0,
                user=None,
            )

            if response:
                # Assuming non-streaming response for simplicity
                return response.choices[0].message.content
            else:
                return "No response from API."

        except Exception as e:
            logger.error(
                f"Unable to generate content with OpenAI API. Error: {e}. Moving on."
            )
            return None


async def main():
    api_keys = get_api_keys()
    rps = 5  # Requests per second
    window_size = 60  # Window size in seconds
    async with aiohttp.ClientSession() as session:
        async with LLM_APIHandler(api_keys, session, rps, window_size) as api_handler:
            tasks = []

            # Llama API
            test_qwen = False
            if test_qwen:
                qwen_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_qwen_content(prompt=qwen_prompt))

            # OpenAI API
            test_openai = True
            if test_openai:
                openai_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_openai_content(openai_prompt))

            # Claude API
            test_claude = False
            if test_claude:
                claude_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_opus_content(claude_prompt))

            # Cohere API
            test_cohere = False
            if test_cohere:
                cohere_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_cohere_content(cohere_prompt))

            # Llama API
            test_llama = False
            if test_llama:
                llama_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_llama_content(prompt=llama_prompt))

            responses = await asyncio.gather(*tasks)

            for response, api_name in zip(
                responses, ["Llama", "Cohere", "Claude", "OpenAI", "qwen"]
            ):
                if response is not None:
                    print(f"{api_name} Response:", response)


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\main.py
# main.py
import os
from user_interface import create_app

app = create_app()

if __name__ == "__main__":
    if os.environ.get("ENV") == "production":
        # In production, we expect gunicorn to serve the app from wsgi.py
        pass
    else:
        # Running locally
        app.launch(share=True)


# Contents of .\misc_utils.py
import os
import json
from google.cloud import secretmanager


async def prepare_text_for_json(text):
    # Replace backslashes with double backslashes
    text = text.replace("\\", "\\\\")

    # Replace double quotes with escaped double quotes
    text = text.replace('"', '\\"')

    # Replace newline characters with escaped newline characters
    text = text.replace("\n", "\\n")

    # Replace tab characters with escaped tab characters
    text = text.replace("\t", "\\t")

    # Replace form feed characters with escaped form feed characters
    text = text.replace("\f", "\\f")

    # Replace backspace characters with escaped backspace characters
    text = text.replace("\b", "\\b")

    # Replace carriage return characters with escaped carriage return characters
    text = text.replace("\r", "\\r")

    # Wrap the escaped text in double quotes to make it a valid JSON string
    json_string = f'"{text}"'

    return json_string


def get_api_keys(source="cloud"):
    if source == "cloud":
        client = secretmanager.SecretManagerServiceClient()
        name = f"projects/crop2cloud24/secrets/api-keys/versions/latest"
        response = client.access_secret_version(request={"name": name})
        api_keys = response.payload.data.decode("UTF-8")
        return api_keys
    else:
        with open(
            os.path.expanduser(
                r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\keys\api_keys.json"
            ),
            "r",
        ) as file:
            api_keys = file.read()
            api_keys = json.loads(api_keys)
        return api_keys


# Contents of .\openalex_search.py
from misc_utils import get_api_keys
import asyncio
import logging
import re
from llm_api_handler import LLM_APIHandler
from prompts import get_prompt
import aiohttp
import json

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
file_handler = logging.FileHandler("paper_ranker.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class PaperRanker:
    def __init__(self, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)
        self.max_retries = max_retries

    async def process_query(self, query_key, query_data, point_context):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="rank_papers",
                full_text=query_data["full_text"],
                point_context=point_context,
                query_rationale=query_data["query_rationale"],
            )
            try:
                print(f"Processing queries for {point_context}...")
                response = await self.llm_api_handler.generate_cohere_content(prompt)
                print(f"Response: {response}")
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping query."
                    )
                    return None

                try:
                    # Extract the relevance score using the specified token format
                    relevance_score_match = re.search(
                        r"<<relevance>>(\d+\.\d+)<<relevance>>",
                        response,
                    )

                    if relevance_score_match:
                        relevance_score_str = relevance_score_match.group(1)
                        try:
                            relevance_score = float(relevance_score_str)
                            if relevance_score > 0.5:
                                logger.debug(f"Successfully processed query.")
                                return {
                                    "DOI": query_data["DOI"],
                                    "title": query_data["title"],
                                    "analysis": response,
                                    "relevance_score": relevance_score,
                                }
                            else:
                                logger.debug(
                                    f"Relevance score {relevance_score} is below the threshold. Skipping query."
                                )
                                return None
                        except ValueError:
                            logger.warning(
                                f"Extracted relevance score '{relevance_score_str}' is not a valid float. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"No relevance score found between <|relevance|> tokens in the response for query {query_key}. Response: {response}"
                        )
                        retry_count += 1
                except Exception as e:
                    logger.warning(
                        f"Error extracting relevance score for query {query_key}: {str(e)}. Retrying..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing query {query_key}: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for query {query_key}. Skipping query.")
        return None

    async def process_queries(self, input_json, point_context):
        tasks = []
        for query_key, query_data in input_json.items():
            if not query_data.get("DOI") or not query_data.get("title"):
                logger.warning(
                    f"Discarding query {query_key} due to missing DOI or title."
                )
                continue
            task = asyncio.create_task(
                self.process_query(query_key, query_data, point_context)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        output_json = {}
        for query_key, result in zip(input_json.keys(), results):
            if result and isinstance(result, dict):
                output_json[query_key] = result

        return output_json


async def main(input_json, point_context):
    async with aiohttp.ClientSession() as session:
        ranker = PaperRanker(session)
        logger.info("Starting paper ranking process...")
        output_json = await ranker.process_queries(input_json, point_context)
        logger.info("Paper ranking process completed.")
        return output_json


if __name__ == "__main__":
    input_json = {
        "query_1": {
            "DOI": "https://doi.org/10.1007/bf00281114",
            "authors": ["Jarrett Rj"],
            "citation_count": 156,
            "journal": "Diabetologia",
            "pdf_link": "https://link.springer.com/content/pdf/10.1007%2FBF00281114.pdf",
            "publication_year": 1984,
            "title": "Type 2 (non-insulin-dependent) diabetes mellitus and coronary heart disease ? chicken, egg or neither?",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query aims to understand the relationship between Type 2 diabetes and coronary heart disease in chickens.",
        },
        "query_2": {
            "DOI": "https://doi.org/10.1001/jamainternmed.2019.6969",
            "authors": [
                "Victor W. Zhong",
                "Linda Van Horn",
                "Philip Greenland",
                "Mercedes R. Carnethon",
                "Hongyan Ning",
                "John T. Wilkins",
                "Donald M. Lloyd‐Jones",
                "Norrina B. Allen",
            ],
            "citation_count": 221,
            "journal": "JAMA internal medicine",
            "pdf_link": "https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf",
            "publication_year": 2020,
            "title": "Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query investigates the associations between different types of meat intake, including poultry, and cardiovascular disease and mortality.",
        },
        "query_3": {
            "DOI": "https://doi.org/10.1016/s0034-5288(18)33737-8",
            "authors": ["S.F. Cueva", "H. Sillau", "Abel Valenzuela", "H. P. Ploog"],
            "citation_count": 181,
            "journal": "Research in Veterinary Science/Research in veterinary science",
            "publication_year": 1974,
            "title": "High Altitude Induced Pulmonary Hypertension and Right Heart Failure in Broiler Chickens",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query focuses on the effects of high altitude on pulmonary hypertension and right heart failure specifically in broiler chickens.",
        },
        "query_4": {
            "DOI": "https://doi.org/10.2307/1588087",
            "authors": ["Sherwin A. Hall", "Nicanor Machicao"],
            "citation_count": 58,
            "journal": "Avian diseases",
            "publication_year": 1968,
            "title": "Myocarditis in Broiler Chickens Reared at High Altitude",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query examines myocarditis in broiler chickens reared at high altitude, providing insight into heart disease in this specific context.",
        },
        "query_5": {
            "DOI": "https://doi.org/10.1038/srep14727",
            "authors": [
                "Huaguang Lu",
                "Yi Tang",
                "Patricia A. Dunn",
                "Eva Wallner-Pendleton",
                "Lin Lin",
                "Eric A. Knoll",
            ],
            "citation_count": 82,
            "journal": "Scientific reports",
            "pdf_link": "https://www.nature.com/articles/srep14727.pdf",
            "publication_year": 2015,
            "title": "Isolation and molecular characterization of newly emerging avian reovirus variants and novel strains in Pennsylvania, USA, 2011–2014",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query looks at the isolation and characterization of new avian reovirus variants and strains, which may have implications for chicken health.",
        },
    }
    point_context = "Heart disease in chickens."

    output_json = asyncio.run(main(input_json, point_context))
    print(json.dumps(output_json, indent=2))


# Contents of .\prompts.py
import re

scopus_search_guide = """
Syntax and Operators

Valid syntax for advanced search queries includes:

Field codes (e.g. TITLE, ABS, KEY, AUTH, AFFIL) to restrict searches to specific parts of documents
Boolean operators (AND, OR, AND NOT) to combine search terms
Proximity operators (W/n, PRE/n) to find words within a specified distance - W/n: Finds terms within "n" words of each other, regardless of order. Example: journal W/15 publishing finds articles where "journal" and "publishing" are within two words of each other. - PRE/n: Finds terms in the specified order and within "n" words of each other. Example: data PRE/50 analysis finds articles where "data" appears before "analysis" within three words. - To find terms in the same sentence, use 15. To find terms in the same paragraph, use 50 -
Quotation marks for loose/approximate phrase searches
Braces {{}} for exact phrase searches (without hte backslashes of course)
Wildcards (*) to capture variations of search terms
Invalid syntax includes:

Mixing different proximity operators (e.g. W/n and PRE/n) in the same expression
Using wildcards or proximity operators with exact phrase searches
Placing AND NOT before other Boolean operators
Using wildcards on their own without any search terms
Ideal Search Structure

An ideal advanced search query should:

Use field codes to focus the search on the most relevant parts of documents
Combine related concepts using AND and OR
Exclude irrelevant terms with AND NOT at the end
Employ quotation marks and braces appropriately for phrase searching
Include wildcards to capture variations of key terms (while avoiding mixing them with other operators)
Follow the proper order of precedence for operators
Complex searches should be built up systematically, with parentheses to group related expressions as needed. The information from the provided documents on syntax rules and operators should be applied rigorously.

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

Example Advanced Searches

{{
"query_1": "TITLE-ABS-KEY(("precision agriculture" OR "precision farming") AND ("machine learning" OR "AI") AND "water")",
"query_2": "TITLE-ABS-KEY((iot OR "internet of things") AND (irrigation OR watering) AND sensor*)",
"query_3": "TITLE-ABS-Key(("precision farming" OR "precision agriculture") AND ("deep learning" OR "neural networks") AND "water")",
"query_4": "TITLE-ABS-KEY((crop W/5 monitor*) AND "remote sensing" AND (irrigation OR water*))",
"query_5": "TITLE("precision irrigation" OR "variable rate irrigation" AND "machine learning")"
}}

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **. 

These example searches demonstrate different ways to effectively combine key concepts related to precision agriculture, irrigation, real-time monitoring, IoT, machine learning and related topics using advanced search operators. They make use of field codes, Boolean and proximity operators, phrase searching, and wildcards to construct targeted, comprehensive searches to surface the most relevant research. The topic focus is achieved through carefully chosen search terms covering the desired themes.
"""

alex_search_guide = """
Syntax and Operators
Valid syntax for advanced alex search queries includes:
Using quotation marks %22%22 for exact phrase matches
Adding a minus sign - before terms to exclude them
Employing the OR operator in all caps to find pages containing either term
Using the site%3A operator to limit results to a specific website
Applying the filetype%3A operator to find specific file formats like PDF, DOC, etc.
Adding the * wildcard as a placeholder for unknown words
`
Invalid syntax includes:
Putting a plus sign + before words (alex stopped supporting this)
Using other special characters like %3F, %24, %26, %23, etc. within search terms
Explicitly using the AND operator (alex's default behavior makes it redundant)

Ideal Search Structure
An effective alex search query should:
Start with the most important search terms
Use specific, descriptive keywords related to irrigation scheduling, management, and precision irrigation
Utilize exact phrases in %22quotes%22 for specific word combinations
Exclude irrelevant terms using the - minus sign
Connect related terms or synonyms with OR
Apply the * wildcard strategically for flexibility
Note:

By following these guidelines and using proper URL encoding, you can construct effective and accurate search queries for alex.

Searches should be concise yet precise, following the syntax rules carefully. 

Example Searches
{{
"query_1": "https://api.openalex.org/works?search=%22precision+irrigation%22+%2B%22soil+moisture+sensors%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_2": "https://api.openalex.org/works?search=%22machine+learning%22+%2B%22irrigation+management%22+%2B%22crop+water+demand+prediction%22&sort=relevance_score:desc&per-page=30",
"query_3": "https://api.openalex.org/works?search=%22IoT+sensors%22+%2B%22real-time%22+%2B%22soil+moisture+monitoring%22+%2B%22crop+water+stress%22&sort=relevance_score:desc&per-page=30",
"query_4": "https://api.openalex.org/works?search=%22remote+sensing%22+%2B%22vegetation+indices%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_5": "https://api.openalex.org/works?search=%22wireless+sensor+networks%22+%2B%22precision+agriculture%22+%2B%22variable+rate+irrigation%22+%2B%22irrigation+automation%22&sort=relevance_score:desc&per-page=30"
}}

These example searches demonstrate how to create targeted, effective alex searches. They focus on specific topics, exclude irrelevant results, allow synonym flexibility, and limit to relevant domains when needed. The search terms are carefully selected to balance relevance and specificity while avoiding being overly restrictive.  By combining relevant keywords, exact phrases, and operators, these searches help generate high-quality results for the given topics.
"""

core_search_guide = """
### CORE API Search Guide: Formulating Queries in JSON Format

This guide provides a structured approach to creating effective search queries using the CORE API. The guide emphasizes the JSON format to ensure clarity and precision in your search queries.

#### Syntax and Operators

**Valid Syntax for CORE API Queries:**
- **Field-specific searches**: Direct your query to search within specific fields like `title`, `author`, or `subject`.
- **Boolean Operators**: Use `AND`, `OR`, and `NOT` to combine or exclude terms.
- **Grouping**: Use parentheses `()` to structure the query and define the order of operations.
- **Range Queries**: Specify ranges for dates or numerical values with `>`, `<`, `>=`, `<=`.
- **Existence Check**: Use `_exists_` to filter results based on the presence of data in specified fields.

**Invalid Syntax:**
- **Inconsistencies in field names**: Ensure field names are correctly spelled and appropriate for the data type.
- **Improper boolean logic**: Avoid illogical combinations that nullify the search criteria (e.g., `AND NOT` used incorrectly).

#### Ideal Query Structure

Your search queries should:
1. **Use Field-Specific Filters**: Focus your search on the most relevant attributes.
2. **Combine Keywords Effectively**: Use logical operators to refine and broaden your searches.
3. **Employ Grouping and Range Queries** where complex relationships or specific time frames are needed.

#### Example Advanced Searches in JSON Format

Here are examples of structured queries formatted in JSON, demonstrating different ways to effectively combine search criteria using the CORE API:

```json
{
    "query_1": {
        "search_query": "climate change, water resources",
        "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
    },
    "query_2": {
        "search_query": "water scarcity, (hydrologist OR water expert)",
        "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
    },
    "query_3": {
        "search_query": "sea level rise, coastal erosion",
        "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
    },
    "query_4": {
        "search_query": "water conservation, climate change mitigation, environmental studies",
        "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
    },
    "query_5": {
        "search_query": "glacier melting, cryosphere",
        "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
    },
}
```

### Critical Considerations

- **Escape Characters**: When using JSON format, ensure that all double quotes inside JSON values are properly escaped using backslashes (`\"`) to prevent parsing errors.
- **Complexity**: As queries become more complex, ensure they are still readable and maintainable. Use whitespace and indentation in JSON to enhance clarity.

These examples illustrate how to utilize the CORE API's flexible query capabilities to target specific fields, combine search terms logically, and exclude irrelevant data. By following these guidelines and adapting the examples to your research needs, you can efficiently leverage the CORE API to access a vast range of academic materials.
"""


def remove_illegal_characters(text):
    if text is None:
        return ""
    illegal_chars = re.compile(r"[\000-\010]|[\013-\014]|[\016-\037]")
    return illegal_chars.sub("", str(text))


def get_prompt(template_name, **kwargs):

    prompts = {
        "generate_queries": """
<documents>
<document index="1">
<source>search_query_prompt.txt</source>
<document_content>
<instructions>
Review the user's main query: '{user_query}'. Break down this query into distinct sub-queries that address different aspects necessary to fully answer the main query. 
For each sub-query, provide a rationale explaining why it is essential. Format these sub-queries according to the directions in <search_guidance>. structure your response as a json with detailed search queries, each accompanied by its rationale. 
The output should adhere to this format:
{{
  "query_1": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic."
  }},
  "query_2": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources."
  }},
  ...
}}
**Note: Only generate as many sub-queries and rationales as necessary to thoroughly address the main query, up to a maximum of 10. Each sub-query must directly contribute to unraveling the main query's aspects.
</instructions>
<search_guidance>
{search_guidance}
</search_guidance>
</resources>
</document_content>
</document>
</documents>
""",
        "rank_papers": """
<instructions>
Analyze the paper's relevance to {point_context} from the {query_rationale} perspective. 

Begin your response with the relevance score between the following tokens:
<<relevance>>*.*<<relevance>>
The relevance score should be a decimal between 0.0 and 1.0, with 1.0 being the most relevant. If there is not enough information to determine relevance, assign a score of 0.0.
Examples:
- Correct: "<<relevance>>0.9<<relevance>>"
- Correct: "<<relevance>>0.3<<relevance>>"
- Correct: "<<relevance>>0.0<<relevance>>"
After providing the relevance score, include the following in your analysis:

Include in your analysis:
- Verbatim extracts: key terms, research questions, methods, results, tables, figures, quotes, conclusions
- Explanation of study purpose and objectives
- Relevance evaluation to the specified point
- Limitations for addressing the point
- Free-form extraction with relevant verbatim information (any format). Extract as much verbatim information as needed to support your analysis.

End with this JSON:
<response_format>
{{
  "inline_citation": "<author surname>, <year>",
  "apa_citation": "<full APA citation>",
  "study_location": "<city/region, country>", 
  "main_objective": "<main objective>",
  "technologies_used": "<technology 1>, <technology 2>, ...",
  "data_sources": "<data source 1>, <data source 2>, ...",
  "key_findings": "<key findings summary>"
}}
</response_format>
</instructions>

<full_text>
{full_text}
</full_text>
""",
        "synthesize_results": """
<prompt>
    <expert_description>
        You are an expert polymath skilled in deep analysis and synthesis of complex research information. Utilize the provided materials, as well as any other relevant data, facts, or discussions provided to you, to construct a detailed and technically rigorous response that is akin to a high-level research analysis. Include all pertinent data, facts, and figures that aid in constructing a comprehensive analysis. Your response should reflect the depth of understanding expected from a seasoned researcher or PhD holder, ensuring no details are overlooked.
    </expert_description>
    <user_query>
        {user_query}
    </user_query>
    <returned_results>
        {returned_results}
    </returned_results>
    <response_format>
        Provide a comprehensive, structured response that rigorously analyzes and interprets the data and insights from the provided research materials and any other relevant information you have been provided with. 
        Mention any pertinent data, facts, numbers, etc., ensuring they are properly attributed. Cite the research papers inline, including hyperlinks to the papers' DOI where necessary (you may need prepend with https://doi.org/ to make it a hyperlink). e.g [author et al 2021](). 
        Conclude with a full citation of all referenced papers. Structure your response in a clear, logical manner, focusing on technical accuracy and depth to thoroughly answer the user's query based on the provided data. 
        Keep your answer tightly focused on the user's query and only include relevant/pertinent information. Begin you answer without preamble.
    </response_format>
    <critical-points>
        - Only include and discuss sources that are directly relevant to the user query.
        - Sources which are not directly relevant to the user query should not be included in the response.
    </critical-points>
</prompt>

""",
    }
    try:
        return prompts[template_name].format(**kwargs)

    except KeyError as e:
        missing_key = str(e).strip("'")
        raise ValueError(
            f"Missing argument for template '{template_name}': {missing_key}"
        )


# Contents of .\scopus_search.py
"""
Scopus Search Program

This program performs searches on the Scopus API based on the provided JSON queries.
It retrieves relevant data for the first entry with successfully scraped full text,
including title, DOI, description, journal, authors, citation count, and full text.
The results are returned in the updated JSON format.

Usage:
    scopus_search = ScopusSearch(doi_scraper)
    updated_json = scopus_search.search_and_parse_json(input_json)

Parameters:
    - doi_scraper: A scraper object capable of retrieving full text content given a DOI.
    - input_json: A JSON object containing the search queries in the specified format.

Returns:
    - updated_json: The updated JSON object with the search results and additional data.
"""

import aiohttp
import asyncio
import json
import time
import logging
from collections import deque
from misc_utils import prepare_text_for_json, get_api_keys
from web_scraper import WebScraper

# Create a logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Create a console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Create a formatter
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)

# Add the console handler to the logger
logger.addHandler(console_handler)

# Optional: Create a file handler to log messages to a file
file_handler = logging.FileHandler("scopus.log")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


class ScopusSearch:
    def __init__(self, doi_scraper, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.base_url = "http://api.elsevier.com/content/search/scopus"
        self.request_times = deque(maxlen=6)
        self.scraper = doi_scraper
        self.session = session
        self.max_retries = max_retries

    async def search(self, query, count=25, view="COMPLETE", response_format="json"):
        headers = {
            "X-ELS-APIKey": self.api_keys["SCOPUS_API_KEY"],
            "Accept": (
                "application/json"
                if response_format == "json"
                else "application/atom+xml"
            ),
        }

        params = {
            "query": query["search_query"].replace("\\", ""),
            "count": count,
            "view": view,
        }

        retry_count = 0
        while retry_count < self.max_retries:
            try:
                # Ensure compliance with the rate limit
                while True:
                    current_time = time.time()
                    if (
                        not self.request_times
                        or current_time - self.request_times[0] >= 1
                    ):
                        self.request_times.append(current_time)
                        break
                    else:
                        await asyncio.sleep(0.2)

                async with self.session.get(
                    self.base_url, headers=headers, params=params
                ) as response:
                    if response.status == 200:
                        logger.info("Scopus API request successful.")
                        if response_format == "json":
                            return await response.json()
                        else:
                            return await response.text()
                    else:
                        logger.warning(
                            f"Scopus API request failed with status code: {response.status}"
                        )
                        return None
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                retry_count += 1
                wait_time = 2**retry_count
                logger.warning(
                    f"Error occurred while making Scopus API request: {e}. Retrying in {wait_time} seconds... (Attempt {retry_count}/{self.max_retries})"
                )
                await asyncio.sleep(wait_time)  # Exponential backoff

        logger.error(
            f"Max retries ({self.max_retries}) exceeded. Unable to fetch data from the Scopus API for query: {query}"
        )
        return None

    async def search_and_parse(self, query, query_id, count=25, view="COMPLETE"):
        try:
            results = await self.search(query, count, view, response_format="json")

            if (
                results is None
                or "search-results" not in results
                or "entry" not in results["search-results"]
            ):
                logger.warning(f"No results found for query: {query}")
                return {}
            else:
                for entry in results["search-results"]["entry"]:
                    title = entry.get("dc:title")
                    doi = entry.get("prism:doi")
                    description = entry.get("dc:description")
                    journal = entry.get("prism:publicationName")
                    citation_count = entry.get("citedby-count", "0")
                    authors = [
                        author.get("authname")
                        for author in entry.get("author", [])
                        if author.get("authname") is not None
                    ]

                    full_text = None
                    if doi:
                        logger.info(f"Scraping full text for DOI: {doi}")
                        try:
                            full_text = await self.scraper.get_url_content(doi)
                            full_text = await prepare_text_for_json(full_text)
                            logger.info(
                                f"Full text scraped successfully for DOI: {doi}"
                            )
                        except Exception as e:
                            logger.warning(
                                f"Error occurred while scraping full text for DOI: {doi}. Error: {e}"
                            )
                            continue

                    parsed_result = {
                        "search_query": query["search_query"],
                        "query_rationale": query["query_rationale"],
                        "title": title,
                        "DOI": doi,
                        "description": description,
                        "journal": journal,
                        "authors": authors,
                        "citation_count": citation_count,
                        "full_text": full_text or "",
                    }

                    return parsed_result

                logger.warning(f"No full text successfully scraped for query: {query}")
                return {}
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {query}. Error: {e}"
            )
            return {}

    async def search_and_parse_json(self, input_json):
        try:
            updated_json = {}
            for query_id, query in input_json.items():
                parsed_result = await self.search_and_parse(query, query_id)
                updated_json[query_id] = parsed_result
            return json.dumps(updated_json, ensure_ascii=False)
        except Exception as e:
            logger.error(
                f"An error occurred while processing the input JSON. Error: {e}"
            )
            return json.dumps({})


async def main():
    async with aiohttp.ClientSession() as session:
        doi_scraper = WebScraper(session)
        # Create an instance of the ScopusSearch class
        scopus_search = ScopusSearch(doi_scraper, session)

        # Example usage
        input_json = {
            "query_1": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND pathology)",
                "query_rationale": "This query is essential to understand the pathology of heart disease in chickens, providing a foundation for further investigation.",
            },
            "query_2": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND epidemiology)",
                "query_rationale": "This query is necessary to identify the prevalence and distribution of heart disease in chicken populations, informing strategies for disease control and prevention.",
            },
            "query_3": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND nutrition)",
                "query_rationale": "This query is important to explore the relationship between nutrition and heart disease in chickens, potentially identifying dietary factors that contribute to the development of the disease.",
            },
            "query_4": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND genetics)",
                "query_rationale": "This query is crucial to investigate the genetic factors that predispose chickens to heart disease, enabling the development of breeding programs that reduce the incidence of the disease.",
            },
            "query_5": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND diagnosis)",
                "query_rationale": "This query is essential to identify effective diagnostic methods for heart disease in chickens, ensuring accurate and timely detection of the disease.",
            },
        }

        # Call the search_and_parse_json method
        updated_json = await scopus_search.search_and_parse_json(input_json)
        print(updated_json)


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\synthesize_results.py
import asyncio
import logging
from llm_api_handler import LLM_APIHandler
from prompts import get_prompt
import aiohttp
from misc_utils import get_api_keys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryProcessor:
    def __init__(self, session):
        api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(api_keys, session)

    async def process_query(self, user_query, returned_results):
        prompt = get_prompt(
            template_name="synthesize_results",
            user_query=user_query,
            returned_results=returned_results,
        )

        try:
            logger.info(f"Processing query: {user_query}")
            response = await self.llm_api_handler.generate_openai_content(prompt)
            return response
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            return None


async def main():
    async with aiohttp.ClientSession() as session:
        query_processor = QueryProcessor(session)
        user_query = "What are the latest advancements in artificial intelligence?"
        returned_results = [
            "AI has made significant progress in fields such as computer vision and natural language processing.",
            "Deep learning techniques have revolutionized AI, enabling machines to learn from large datasets.",
            "AI is being applied in various domains, including healthcare, finance, and autonomous vehicles.",
        ]

        logger.info("Starting query processing...")
        response = await query_processor.process_query(user_query, returned_results)
        if response:
            logger.info(f"Response: {response}")
        else:
            logger.warning("Failed to generate a response.")
        logger.info("Query processing completed.")


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\user_interface_local.py
# utils/user_interface.py
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import gradio as gr
import json
import aiohttp
import logging
from get_search_queries import QueryGenerator
from scopus_search import ScopusSearch
from analyze_papers import PaperRanker
from synthesize_results import QueryProcessor
from web_scraper import WebScraper
from core_search import CORESearch
from misc_utils import get_api_keys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("research_query_processor.log"),
        logging.StreamHandler(),
    ],
)


class ResearchQueryProcessor:
    def __init__(self):
        self.api_keys = get_api_keys()
        self.session = None

    async def chatbot_response(self, message, history):
        async with aiohttp.ClientSession() as session:
            self.session = session
            logging.info(f"Received message: {message}")

            yield "Generating search queries..."
            query_generator = QueryGenerator(self.session)
            search_queries = await query_generator.generate_queries(message)
            logging.info(f"Generated search queries: {search_queries}")

            yield "Searching in Scopus..."
            doi_scraper = WebScraper(self.session)
            scopus_search = ScopusSearch(doi_scraper, self.session)
            search_results = await scopus_search.search_and_parse_json(search_queries)
            search_results = json.loads(search_results)
            logging.info(f"Scopus search results: {search_results}")

            yield "Analyzing papers..."
            paper_ranker = PaperRanker(self.session)
            analyzed_papers = await paper_ranker.process_queries(
                search_results, message
            )
            logging.info(f"Analyzed papers: {analyzed_papers}")

            yield "Synthesizing results..."
            query_processor = QueryProcessor(self.session)
            synthesized_results = await query_processor.process_query(
                message, analyzed_papers
            )
            logging.info(f"Synthesized results: {synthesized_results}")

            yield f"{synthesized_results}"


def create_app():
    processor = ResearchQueryProcessor()

    chat_interface = gr.ChatInterface(
        fn=processor.chatbot_response,
        title="Literature Review Agent",
        description="Enter your research query below.",
        theme=gr.themes.Soft(),
    )

    return chat_interface


# Run the app
if __name__ == "__main__":
    app = create_app()
    app.launch()


# Contents of .\web_scraper.py
import asyncio
import random
import aiohttp
from playwright.async_api import async_playwright
from fake_useragent import UserAgent
import logging
import sys


class WebScraper:
    def __init__(self, session, max_concurrent_tasks=120):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.user_agent = UserAgent()
        self.browser = None
        self.session = session
        self.logger = logging.getLogger(__name__)

    async def initialize(self):
        try:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        if self.browser:
            await self.browser.close()
            self.logger.info("Browser closed")

    async def scrape_url(self, url, max_retries=3):
        if not self.browser:
            await self.initialize()  # Ensure the browser is initialized

        retry_count = 0
        while retry_count < max_retries:
            try:
                context = await self.browser.new_context(
                    user_agent=self.user_agent.random,
                    viewport={"width": 1920, "height": 1080},
                    ignore_https_errors=True,
                )
                page = await context.new_page()
                await self.navigate_to_url(page, url)
                content = await self.extract_text_content(page)
                self.logger.info(f"Successfully scraped URL: {url}")
                await page.close()
                await context.close()
                return content
            except Exception as e:
                self.logger.error(
                    f"Error occurred while scraping URL: {url}. Error: {str(e)}"
                )
                retry_count += 1
                await asyncio.sleep(
                    random.uniform(1, 5)
                )  # Random delay between retries
            finally:
                try:
                    await page.close()
                    await context.close()
                except Exception as e:
                    self.logger.warning(
                        f"Error occurred while closing page or context: {str(e)}"
                    )
        self.logger.warning(f"Max retries exceeded for URL: {url}")
        return ""

    async def get_url_content(self, url):
        async with self.semaphore:
            return await self.scrape_url(url)

    async def navigate_to_url(self, page, url, max_retries=3):
        if not url.startswith("http"):
            url = f"https://doi.org/{url}"
        retry_count = 0
        while retry_count < max_retries:
            try:
                await page.goto(url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)  # Minor delay to ensure page loads completely
                return
            except Exception as e:
                self.logger.warning(
                    f"Retrying URL: {url}. Remaining retries: {max_retries - retry_count}"
                )
                retry_count += 1
                await asyncio.sleep(
                    random.uniform(1, 5)
                )  # Random delay between retries
        self.logger.error(
            f"Failed to navigate to URL: {url} after {max_retries} retries"
        )
        raise e

    async def extract_text_content(self, page):
        try:
            paragraphs = await page.query_selector_all("p")
            text_content = "".join(
                [await paragraph.inner_text() + "\n" for paragraph in paragraphs]
            )
            return text_content.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text content. Error: {str(e)}")
            return ""


# Usage
async def main():
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler("scraper.log"),
            logging.StreamHandler(sys.stdout),
        ],
    )

    async with aiohttp.ClientSession() as session:
        scraper = WebScraper(session=session)
        try:
            await scraper.initialize()
        except Exception as e:
            logging.error(f"Initialization failed: {e}")
            return  # Early return if initialization fails

        urls = [
            "10.1016/j.ifacol.2020.12.237",
            "10.1016/j.agwat.2023.108536",
            "10.1016/j.atech.2023.100251",
            "10.1016/j.atech.2023.100179",
            "10.1016/j.ifacol.2023.10.677",
            "10.1016/j.ifacol.2023.10.1655",
            "10.1016/j.ifacol.2023.10.667",
            "10.1002/cjce.24764",
            "10.3390/app13084734",
            "10.1016/j.atech.2022.100074",
            "10.1007/s10668-023-04028-9",
            "10.1109/IJCNN54540.2023.10191862",
            "10.1201/9780429290152-5",
            "10.1016/j.jprocont.2022.10.003",
            "10.1016/j.rser.2022.112790",
            "10.1007/s11269-022-03191-4",
            "10.3390/app12094235",
            "10.3390/w14060889",
            "10.3390/su14031304",
        ]

        scrape_tasks = []
        for url in urls:
            scrape_task = asyncio.create_task(scraper.get_url_content(url))
            scrape_tasks.append(scrape_task)

        scraped_contents = await asyncio.gather(*scrape_tasks)

        for url, content in zip(urls, scraped_contents):
            logging.info(f"Scraped content for URL: {url}")
            logging.info(f"Content: {content}")

        await scraper.close()


if __name__ == "__main__":
    asyncio.run(main())


# Contents of .\wsgi.py
# wsgi.py
from user_interface import create_app

app = create_app()
